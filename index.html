<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-07-27T01:30:00Z">07-27</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">UrduFake@FIRE2020: Shared Track on Fake News Identification in Urdu. (arXiv:2207.12406v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12406">
<div class="article-summary-box-inner">
<span><p>This paper gives the overview of the first shared task at FIRE 2020 on fake
news detection in the Urdu language. This is a binary classification task in
which the goal is to identify fake news using a dataset composed of 900
annotated news articles for training and 400 news articles for testing. The
dataset contains news in five domains: (i) Health, (ii) Sports, (iii) Showbiz,
(iv) Technology, and (v) Business. 42 teams from 6 different countries (India,
China, Egypt, Germany, Pakistan, and the UK) registered for the task. 9 teams
submitted their experimental results. The participants used various machine
learning methods ranging from feature-based traditional machine learning to
neural network techniques. The best performing system achieved an F-score value
of 0.90, showing that the BERT-based approach outperforms other machine
learning classifiers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Speaker Diarization that is Agnostic to Language, Overlap-Aware, and Tuning Free. (arXiv:2207.12504v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12504">
<div class="article-summary-box-inner">
<span><p>Podcasts are conversational in nature and speaker changes are frequent --
requiring speaker diarization for content understanding. We propose an
unsupervised technique for speaker diarization without relying on
language-specific components. The algorithm is overlap-aware and does not
require information about the number of speakers. Our approach shows 79%
improvement on purity scores (34% on F-score) against the Google Cloud Platform
solution on podcast data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DialCrowd 2.0: A Quality-Focused Dialog System Crowdsourcing Toolkit. (arXiv:2207.12551v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12551">
<div class="article-summary-box-inner">
<span><p>Dialog system developers need high-quality data to train, fine-tune and
assess their systems. They often use crowdsourcing for this since it provides
large quantities of data from many workers. However, the data may not be of
sufficiently good quality. This can be due to the way that the requester
presents a task and how they interact with the workers. This paper introduces
DialCrowd 2.0 to help requesters obtain higher quality data by, for example,
presenting tasks more clearly and facilitating effective communication with
workers. DialCrowd 2.0 guides developers in creating improved Human
Intelligence Tasks (HITs) and is directly applicable to the workflows used
currently by developers and researchers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Innovations in Neural Data-to-text Generation. (arXiv:2207.12571v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12571">
<div class="article-summary-box-inner">
<span><p>The neural boom that has sparked natural language processing (NLP) research
through the last decade has similarly led to significant innovations in
data-to-text generation (DTG). This survey offers a consolidated view into the
neural DTG paradigm with a structured examination of the approaches, benchmark
datasets, and evaluation protocols. This survey draws boundaries separating DTG
from the rest of the natural language generation (NLG) landscape, encompassing
an up-to-date synthesis of the literature, and highlighting the stages of
technological adoption from within and outside the greater NLG umbrella. With
this holistic view, we highlight promising avenues for DTG research that not
only focus on the design of linguistically capable systems but also systems
that exhibit fairness and accountability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WinoGAViL: Gamified Association Benchmark to Challenge Vision-and-Language Models. (arXiv:2207.12576v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12576">
<div class="article-summary-box-inner">
<span><p>While vision-and-language models perform well on tasks such as visual
question answering, they struggle when it comes to basic human commonsense
reasoning skills. In this work, we introduce WinoGAViL: an online game to
collect vision-and-language associations, (e.g., werewolves to a full moon),
used as a dynamic benchmark to evaluate state-of-the-art models. Inspired by
the popular card game Codenames, a spymaster gives a textual cue related to
several visual candidates, and another player has to identify them. Human
players are rewarded for creating associations that are challenging for a rival
AI model but still solvable by other human players. We use the game to collect
3.5K instances, finding that they are intuitive for humans (&gt;90% Jaccard index)
but challenging for state-of-the-art AI models, where the best model (ViLT)
achieves a score of 52%, succeeding mostly where the cue is visually salient.
Our analysis as well as the feedback we collect from players indicate that the
collected associations require diverse reasoning skills, including general
knowledge, common sense, abstraction, and more. We release the dataset, the
code and the interactive game, aiming to allow future data collection that can
be used to develop models with better association abilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Visual Representation from Modality-Shared Contrastive Language-Image Pre-training. (arXiv:2207.12661v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12661">
<div class="article-summary-box-inner">
<span><p>Large-scale multi-modal contrastive pre-training has demonstrated great
utility to learn transferable features for a range of downstream tasks by
mapping multiple modalities into a shared embedding space. Typically, this has
employed separate encoders for each modality. However, recent work suggests
that transformers can support learning across multiple modalities and allow
knowledge sharing. Inspired by this, we investigate a variety of
Modality-Shared Contrastive Language-Image Pre-training (MS-CLIP) frameworks.
More specifically, we question how many parameters of a transformer model can
be shared across modalities during contrastive pre-training, and rigorously
examine architectural design choices that position the proportion of parameters
shared along a spectrum. In studied conditions, we observe that a mostly
unified encoder for vision and language signals outperforms all other
variations that separate more parameters. Additionally, we find that
light-weight modality-specific parallel modules further improve performance.
Experimental results show that the proposed MS-CLIP approach outperforms
vanilla CLIP by up to 13\% relative in zero-shot ImageNet classification
(pre-trained on YFCC-100M), while simultaneously supporting a reduction of
parameters. In addition, our approach outperforms vanilla CLIP by 1.6 points in
linear probing on a collection of 24 downstream vision tasks. Furthermore, we
discover that sharing parameters leads to semantic concepts from different
modalities being encoded more closely in the embedding space, facilitating the
transferring of common semantic structure (e.g., attention patterns) from
language to vision. Code is available at
\href{https://github.com/Hxyou/MSCLIP}{URL}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Advanced Conditional Variational Autoencoders (A-CVAE): Towards interpreting open-domain conversation generation via disentangling latent feature representation. (arXiv:2207.12696v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12696">
<div class="article-summary-box-inner">
<span><p>Currently end-to-end deep learning based open-domain dialogue systems remain
black box models, making it easy to generate irrelevant contents with
data-driven models. Specifically, latent variables are highly entangled with
different semantics in the latent space due to the lack of priori knowledge to
guide the training. To address this problem, this paper proposes to harness the
generative model with a priori knowledge through a cognitive approach involving
mesoscopic scale feature disentanglement. Particularly, the model integrates
the macro-level guided-category knowledge and micro-level open-domain dialogue
data for the training, leveraging the priori knowledge into the latent space,
which enables the model to disentangle the latent variables within the
mesoscopic scale. Besides, we propose a new metric for open-domain dialogues,
which can objectively evaluate the interpretability of the latent space
distribution. Finally, we validate our model on different datasets and
experimentally demonstrate that our model is able to generate higher quality
and more interpretable dialogues than other models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Automated News Bias Classifier Using Caenorhabditis Elegans Inspired Recursive Feedback Network Architecture. (arXiv:2207.12724v1 [cs.NE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12724">
<div class="article-summary-box-inner">
<span><p>Traditional approaches to classify the political bias of news articles have
failed to generate accurate, generalizable results. Existing networks premised
on CNNs and DNNs lack a model to identify and extrapolate subtle indicators of
bias like word choice, context, and presentation. In this paper, we propose a
network architecture that achieves human-level accuracy in assigning bias
classifications to articles. The underlying model is based on a novel Mesh
Neural Network (MNN),this structure enables feedback and feedforward synaptic
connections between any two neurons in the mesh. The MNN ontains six network
configurations that utilize Bernoulli based random sampling, pre-trained DNNs,
and a network modelled after the C. Elegans nematode. The model is trained on
over ten-thousand articles scraped from AllSides.com which are labelled to
indicate political bias. The parameters of the network are then evolved using a
genetic algorithm suited to the feedback neural structure. Finally, the best
performing model is applied to five popular news sources in the United States
over a fifty-day trial to quantify political biases in the articles they
display. We hope our project can spur research into biological solutions for
NLP tasks and provide accurate tools for citizens to understand subtle biases
in the articles they consume.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controllable User Dialogue Act Augmentation for Dialogue State Tracking. (arXiv:2207.12757v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12757">
<div class="article-summary-box-inner">
<span><p>Prior work has demonstrated that data augmentation is useful for improving
dialogue state tracking. However, there are many types of user utterances,
while the prior method only considered the simplest one for augmentation,
raising the concern about poor generalization capability. In order to better
cover diverse dialogue acts and control the generation quality, this paper
proposes controllable user dialogue act augmentation (CUDA-DST) to augment user
utterances with diverse behaviors. With the augmented data, different state
trackers gain improvement and show better robustness, achieving the
state-of-the-art performance on MultiWOZ 2.1
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training Effective Neural Sentence Encoders from Automatically Mined Paraphrases. (arXiv:2207.12759v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12759">
<div class="article-summary-box-inner">
<span><p>Sentence embeddings are commonly used in text clustering and semantic
retrieval tasks. State-of-the-art sentence representation methods are based on
artificial neural networks fine-tuned on large collections of manually labeled
sentence pairs. Sufficient amount of annotated data is available for
high-resource languages such as English or Chinese. In less popular languages,
multilingual models have to be used, which offer lower performance. In this
publication, we address this problem by proposing a method for training
effective language-specific sentence encoders without manually labeled data.
Our approach is to automatically construct a dataset of paraphrase pairs from
sentence-aligned bilingual text corpora. We then use the collected data to
fine-tune a Transformer language model with an additional recurrent pooling
layer. Our sentence encoder can be trained in less than a day on a single
graphics card, achieving high performance on a diverse set of sentence-level
tasks. We evaluate our method on eight linguistic tasks in Polish, comparing it
with the best available multilingual sentence encoders.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Equivariant and Invariant Grounding for Video Question Answering. (arXiv:2207.12783v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12783">
<div class="article-summary-box-inner">
<span><p>Video Question Answering (VideoQA) is the task of answering the natural
language questions about a video. Producing an answer requires understanding
the interplay across visual scenes in video and linguistic semantics in
question. However, most leading VideoQA models work as black boxes, which make
the visual-linguistic alignment behind the answering process obscure. Such
black-box nature calls for visual explainability that reveals ``What part of
the video should the model look at to answer the question?''. Only a few works
present the visual explanations in a post-hoc fashion, which emulates the
target model's answering process via an additional method. Nonetheless, the
emulation struggles to faithfully exhibit the visual-linguistic alignment
during answering.
</p>
<p>Instead of post-hoc explainability, we focus on intrinsic interpretability to
make the answering process transparent. At its core is grounding the
question-critical cues as the causal scene to yield answers, while rolling out
the question-irrelevant information as the environment scene. Taking a causal
look at VideoQA, we devise a self-interpretable framework, Equivariant and
Invariant Grounding for Interpretable VideoQA (EIGV). Specifically, the
equivariant grounding encourages the answering to be sensitive to the semantic
changes in the causal scene and question; in contrast, the invariant grounding
enforces the answering to be insensitive to the changes in the environment
scene. By imposing them on the answering process, EIGV is able to distinguish
the causal scene from the environment information, and explicitly present the
visual-linguistic alignment. Extensive experiments on three benchmark datasets
justify the superiority of EIGV in terms of accuracy and visual
interpretability over the leading baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extreme compression of sentence-transformer ranker models: faster inference, longer battery life, and less storage on edge devices. (arXiv:2207.12852v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12852">
<div class="article-summary-box-inner">
<span><p>Modern search systems use several large ranker models with transformer
architectures. These models require large computational resources and are not
suitable for usage on devices with limited computational resources. Knowledge
distillation is a popular compression technique that can reduce the resource
needs of such models, where a large teacher model transfers knowledge to a
small student model. To drastically reduce memory requirements and energy
consumption, we propose two extensions for a popular sentence-transformer
distillation procedure: generation of an optimal size vocabulary and
dimensionality reduction of the embedding dimension of teachers prior to
distillation. We evaluate these extensions on two different types of ranker
models. This results in extremely compressed student models whose analysis on a
test dataset shows the significance and utility of our proposed extensions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Collaborative Filtering Recommender with Prompt-Based Sentiment Analysis. (arXiv:2207.12883v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12883">
<div class="article-summary-box-inner">
<span><p>Collaborative Filtering(CF) recommender is a crucial application in the
online market and ecommerce. However, CF recommender has been proven to suffer
from persistent problems related to sparsity of the user rating that will
further lead to a cold-start issue. Existing methods address the data sparsity
issue by applying token-level sentiment analysis that translate text review
into sentiment scores as a complement of the user rating. In this paper, we
attempt to optimize the sentiment analysis with advanced NLP models including
BERT and RoBERTa, and experiment on whether the CF recommender has been further
enhanced. We build the recommenders on the Amazon US Reviews dataset, and tune
the pretrained BERT and RoBERTa with the traditional fine-tuned paradigm as
well as the new prompt-based learning paradigm. Experimental result shows that
the recommender enhanced with the sentiment ratings predicted by the fine-tuned
RoBERTa has the best performance, and achieved 30.7% overall gain by comparing
MAP, NDCG and precision at K to the baseline recommender. Prompt-based learning
paradigm, although superior to traditional fine-tune paradigm in pure sentiment
analysis, fail to further improve the CF recommender.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning structures of the French clinical language:development and validation of word embedding models using 21 million clinical reports from electronic health records. (arXiv:2207.12940v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12940">
<div class="article-summary-box-inner">
<span><p>Background
</p>
<p>Clinical studies using real-world data may benefit from exploiting clinical
reports, a particularly rich albeit unstructured medium. To that end, natural
language processing can extract relevant information. Methods based on transfer
learning using pre-trained language models have achieved state-of-the-art
results in most NLP applications; however, publicly available models lack
exposure to speciality-languages, especially in the medical field.
</p>
<p>Objective
</p>
<p>We aimed to evaluate the impact of adapting a language model to French
clinical reports on downstream medical NLP tasks.
</p>
<p>Methods
</p>
<p>We leveraged a corpus of 21M clinical reports collected from August 2017 to
July 2021 at the Greater Paris University Hospitals (APHP) to produce two
CamemBERT architectures on speciality language: one retrained from scratch and
the other using CamemBERT as its initialisation. We used two French annotated
medical datasets to compare our language models to the original CamemBERT
network, evaluating the statistical significance of improvement with the
Wilcoxon test.
</p>
<p>Results
</p>
<p>Our models pretrained on clinical reports increased the average F1-score on
APMed (an APHP-specific task) by 3 percentage points to 91%, a statistically
significant improvement. They also achieved performance comparable to the
original CamemBERT on QUAERO. These results hold true for the fine-tuned and
from-scratch versions alike, starting from very few pre-training samples.
</p>
<p>Conclusions
</p>
<p>We confirm previous literature showing that adapting generalist pre-train
language models such as CamenBERT on speciality corpora improves their
performance for downstream clinical NLP tasks. Our results suggest that
retraining from scratch does not induce a statistically significant performance
gain compared to fine-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hansel: A Chinese Few-Shot and Zero-Shot Entity Linking Benchmark. (arXiv:2207.13005v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13005">
<div class="article-summary-box-inner">
<span><p>Modern Entity Linking (EL) systems entrench a popularity bias, yet there is
no dataset focusing on tail and emerging entities in languages other than
English. We present Hansel, a new benchmark in Chinese that fills the vacancy
of non-English few-shot and zero-shot EL challenges. The test set of Hansel is
human annotated and reviewed, created with a novel method for collecting
zero-shot EL datasets. It covers 10K diverse documents in news, social media
posts and other web articles, with Wikidata as its target Knowledge Base. We
demonstrate that the existing state-of-the-art EL system performs poorly on
Hansel (R@1 of 36.6% on Few-Shot). We then establish a strong baseline that
scores a R@1 of 46.2% on Few-Shot and 76.6% on Zero-Shot on our dataset. We
also show that our baseline achieves competitive results on TAC-KBP2015 Chinese
Entity Linking task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NewsStories: Illustrating articles with visual summaries. (arXiv:2207.13061v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13061">
<div class="article-summary-box-inner">
<span><p>Recent self-supervised approaches have used large-scale image-text datasets
to learn powerful representations that transfer to many tasks without
finetuning. These methods often assume that there is one-to-one correspondence
between its images and their (short) captions. However, many tasks require
reasoning about multiple images and long text narratives, such as describing
news articles with visual summaries. Thus, we explore a novel setting where the
goal is to learn a self-supervised visual-language representation that is
robust to varying text length and the number of images. In addition, unlike
prior work which assumed captions have a literal relation to the image, we
assume images only contain loose illustrative correspondence with the text. To
explore this problem, we introduce a large-scale multimodal dataset containing
over 31M articles, 22M images and 1M videos. We show that state-of-the-art
image-text alignment methods are not robust to longer narratives with multiple
images. Finally, we introduce an intuitive baseline that outperforms these
methods on zero-shot image-set retrieval by 10% on the GoodNews dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TI-CNN: Convolutional Neural Networks for Fake News Detection. (arXiv:1806.00749v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1806.00749">
<div class="article-summary-box-inner">
<span><p>With the development of social networks, fake news for various commercial and
political purposes has been appearing in large numbers and gotten widespread in
the online world. With deceptive words, people can get infected by the fake
news very easily and will share them without any fact-checking. For instance,
during the 2016 US president election, various kinds of fake news about the
candidates widely spread through both official news media and the online social
networks. These fake news is usually released to either smear the opponents or
support the candidate on their side. The erroneous information in the fake news
is usually written to motivate the voters' irrational emotion and enthusiasm.
Such kinds of fake news sometimes can bring about devastating effects, and an
important goal in improving the credibility of online social networks is to
identify the fake news timely. In this paper, we propose to study the fake news
detection problem. Automatic fake news identification is extremely hard, since
pure model based fact-checking for news is still an open problem, and few
existing models can be applied to solve the problem. With a thorough
investigation of a fake news data, lots of useful explicit features are
identified from both the text words and images used in the fake news. Besides
the explicit features, there also exist some hidden patterns in the words and
images used in fake news, which can be captured with a set of latent features
extracted via the multiple convolutional layers in our model. A model named as
TI-CNN (Text and Image information based Convolutinal Neural Network) is
proposed in this paper. By projecting the explicit and latent features into a
unified feature space, TI-CNN is trained with both the text and image
information simultaneously. Extensive experiments carried on the real-world
fake news datasets have demonstrate the effectiveness of TI-CNN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Advanced Semantics for Commonsense Knowledge Extraction. (arXiv:2011.00905v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.00905">
<div class="article-summary-box-inner">
<span><p>Commonsense knowledge (CSK) about concepts and their properties is useful for
AI applications such as robust chatbots. Prior works like ConceptNet, TupleKB
and others compiled large CSK collections, but are restricted in their
expressiveness to subject-predicate-object (SPO) triples with simple concepts
for S and monolithic strings for P and O. Also, these projects have either
prioritized precision or recall, but hardly reconcile these complementary
goals. This paper presents a methodology, called Ascent, to automatically build
a large-scale knowledge base (KB) of CSK assertions, with advanced
expressiveness and both better precision and recall than prior works. Ascent
goes beyond triples by capturing composite concepts with subgroups and aspects,
and by refining assertions with semantic facets. The latter are important to
express temporal and spatial validity of assertions and further qualifiers.
Ascent combines open information extraction with judicious cleaning using
language models. Intrinsic evaluation shows the superior size and quality of
the Ascent KB, and an extrinsic evaluation for QA-support tasks underlines the
benefits of Ascent. A web interface, data and code can be found at
https://ascent.mpi-inf.mpg.de/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On joint training with interfaces for spoken language understanding. (arXiv:2106.15919v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.15919">
<div class="article-summary-box-inner">
<span><p>Spoken language understanding (SLU) systems extract both text transcripts and
semantics associated with intents and slots from input speech utterances. SLU
systems usually consist of (1) an automatic speech recognition (ASR) module,
(2) an interface module that exposes relevant outputs from ASR, and (3) a
natural language understanding (NLU) module. Interfaces in SLU systems carry
information on text transcriptions or richer information like neural embeddings
from ASR to NLU. In this paper, we study how interfaces affect joint-training
for spoken language understanding. Most notably, we obtain the state-of-the-art
results on the publicly available 50-hr SLURP dataset. We first leverage
large-size pretrained ASR and NLU models that are connected by a text
interface, and then jointly train both models via a sequence loss function. For
scenarios where pretrained models are not utilized, the best results are
obtained through a joint sequence loss training using richer neural interfaces.
Finally, we show the overall diminishing impact of leveraging pretrained models
with increased training data size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Text Extractive Summarization Based on Graph and Pre-trained Language Model Attention. (arXiv:2110.04878v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04878">
<div class="article-summary-box-inner">
<span><p>Representing a text as a graph for obtaining automatic text summarization has
been investigated for over ten years. With the development of attention or
Transformer on natural language processing (NLP), it is possible to make a
connection between the graph and attention structure for a text. In this paper,
an attention matrix between the sentences of the whole text is adopted as a
weighted adjacent matrix of a fully connected graph of the text, which can be
produced through the pre-training language model. The GCN is further applied to
the text graph model for classifying each node and finding out the salient
sentences from the text. It is demonstrated by the experimental results on two
typical datasets that our proposed model can achieve a competitive result in
comparison with sate-of-the-art models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Guiding Visual Question Generation. (arXiv:2110.08226v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08226">
<div class="article-summary-box-inner">
<span><p>In traditional Visual Question Generation (VQG), most images have multiple
concepts (e.g. objects and categories) for which a question could be generated,
but models are trained to mimic an arbitrary choice of concept as given in
their training data. This makes training difficult and also poses issues for
evaluation -- multiple valid questions exist for most images but only one or a
few are captured by the human references. We present Guiding Visual Question
Generation - a variant of VQG which conditions the question generator on
categorical information based on expectations on the type of question and the
objects it should explore. We propose two variants: (i) an explicitly guided
model that enables an actor (human or automated) to select which objects and
categories to generate a question for; and (ii) an implicitly guided model that
learns which objects and categories to condition on, based on discrete latent
variables. The proposed models are evaluated on an answer-category augmented
VQA dataset and our quantitative results show a substantial improvement over
the current state of the art (over 9 BLEU-4 increase). Human evaluation
validates that guidance helps the generation of questions that are
grammatically coherent and relevant to the given image and objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Theories on Styles to their Transfer in Text: Bridging the Gap with a Hierarchical Survey. (arXiv:2110.15871v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15871">
<div class="article-summary-box-inner">
<span><p>Humans are naturally endowed with the ability to write in a particular style.
They can, for instance, re-phrase a formal letter in an informal way, convey a
literal message with the use of figures of speech or edit a novel by mimicking
the style of some well-known authors. Automating this form of creativity
constitutes the goal of style transfer. As a natural language generation task,
style transfer aims at rewriting existing texts, and specifically, it creates
paraphrases that exhibit some desired stylistic attributes. From a practical
perspective, it envisions beneficial applications, like chatbots that modulate
their communicative style to appear empathetic, or systems that automatically
simplify technical articles for a non-expert audience. Several style-aware
paraphrasing methods have attempted to tackle style transfer. A handful of
surveys give a methodological overview of the field, but they do not support
researchers to focus on specific styles. With this paper, we aim at providing a
comprehensive discussion of the styles that have received attention in the
transfer task. We organize them in a hierarchy, highlighting the challenges for
the definition of each of them, and pointing out gaps in the current research
landscape. The hierarchy comprises two main groups. One encompasses styles that
people modulate arbitrarily, along the lines of registers and genres. The other
group corresponds to unintentionally expressed styles, due to an author's
personal characteristics. Hence, our review shows how these groups relate to
one another, and where specific styles, including some that have not yet been
explored, belong in the hierarchy. Moreover, we summarize the methods employed
for different stylistic families, hinting researchers towards those that would
be the most fitting for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pay More Attention to History: A Context Modelling Strategy for Conversational Text-to-SQL. (arXiv:2112.08735v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08735">
<div class="article-summary-box-inner">
<span><p>Conversational text-to-SQL aims at converting multi-turn natural language
queries into their corresponding SQL (Structured Query Language)
representations. One of the most intractable problems of conversational
text-to-SQL is modelling the semantics of multi-turn queries and gathering the
proper information required for the current query. This paper shows that
explicitly modelling the semantic changes by adding each turn and the
summarization of the whole context can bring better performance on converting
conversational queries into SQLs. In particular, we propose two conversational
modelling tasks in both turn grain and conversation grain. These two tasks
simply work as auxiliary training tasks to help with multi-turn conversational
semantic parsing. We conducted empirical studies and achieved new
state-of-the-art results on the large-scale open-domain conversational
text-to-SQL dataset. The results demonstrate that the proposed mechanism
significantly improves the performance of multi-turn semantic parsing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Adaptive Deep Clustering Pipeline to Inform Text Labeling at Scale. (arXiv:2202.01211v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01211">
<div class="article-summary-box-inner">
<span><p>Mining the latent intentions from large volumes of natural language inputs is
a key step to help data analysts design and refine Intelligent Virtual
Assistants (IVAs) for customer service and sales support. We created a flexible
and scalable clustering pipeline within the Verint Intent Manager (VIM) that
integrates the fine-tuning of language models, a high performing k-NN library
and community detection techniques to help analysts quickly surface and
organize relevant user intentions from conversational texts. The fine-tuning
step is necessary because pre-trained language models cannot encode texts to
efficiently surface particular clustering structures when the target texts are
from an unseen domain or the clustering task is not topic detection. We
describe the pipeline and demonstrate its performance and ability to scale on
three real-world text mining tasks. As deployed in the VIM application, this
clustering pipeline produces high quality results, improving the performance of
data analysts and reducing the time it takes to surface intentions from
customer service data, thereby reducing the time it takes to build and deploy
IVAs in new domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Introducing the ICBe Dataset: Very High Recall and Precision Event Extraction from Narratives about International Crises. (arXiv:2202.07081v2 [stat.AP] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07081">
<div class="article-summary-box-inner">
<span><p>How do international crises unfold? We conceptualize of international
relations as a strategic chess game between adversaries and develop a
systematic way to measure pieces, moves, and gambits accurately and
consistently over a hundred years of history. We introduce a new ontology and
dataset of international events called ICBe based on a very high-quality corpus
of narratives from the International Crisis Behavior (ICB) Project. We
demonstrate that ICBe has higher coverage, recall, and precision than existing
state of the art datasets and conduct two detailed case studies of the Cuban
Missile Crisis (1962) and Crimea-Donbas Crisis (2014). We further introduce two
new event visualizations (event icongraphy and crisis maps), an automated
benchmark for measuring event recall using natural language processing
(sythnetic narratives), and an ontology reconstruction task for objectively
measuring event precision. We make the data, online appendix, replication
material, and visualizations of every historical episode available at a
companion website www.crisisevents.org and the github repository.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BioADAPT-MRC: Adversarial Learning-based Domain Adaptation Improves Biomedical Machine Reading Comprehension Task. (arXiv:2202.13174v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13174">
<div class="article-summary-box-inner">
<span><p>Biomedical machine reading comprehension (biomedical-MRC) aims to comprehend
complex biomedical narratives and assist healthcare professionals in retrieving
information from them. The high performance of modern neural network-based MRC
systems depends on high-quality, large-scale, human-annotated training
datasets. In the biomedical domain, a crucial challenge in creating such
datasets is the requirement for domain knowledge, inducing the scarcity of
labeled data and the need for transfer learning from the labeled
general-purpose (source) domain to the biomedical (target) domain. However,
there is a discrepancy in marginal distributions between the general-purpose
and biomedical domains due to the variances in topics. Therefore,
direct-transferring of learned representations from a model trained on a
general-purpose domain to the biomedical domain can hurt the model's
performance. We present an adversarial learning-based domain adaptation
framework for the biomedical machine reading comprehension task (BioADAPT-MRC),
a neural network-based method to address the discrepancies in the marginal
distributions between the general and biomedical domain datasets. BioADAPT-MRC
relaxes the need for generating pseudo labels for training a well-performing
biomedical-MRC model. We extensively evaluate the performance of BioADAPT-MRC
by comparing it with the best existing methods on three widely used benchmark
biomedical-MRC datasets -- BioASQ-7b, BioASQ-8b, and BioASQ-9b. Our results
suggest that without using any synthetic or human-annotated data from the
biomedical domain, BioADAPT-MRC can achieve state-of-the-art performance on
these datasets. Availability: BioADAPT-MRC is freely available as an
open-source project at \url{https://github.com/mmahbub/BioADAPT-MRC}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PACS: A Dataset for Physical Audiovisual CommonSense Reasoning. (arXiv:2203.11130v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11130">
<div class="article-summary-box-inner">
<span><p>In order for AI to be safely deployed in real-world scenarios such as
hospitals, schools, and the workplace, it must be able to robustly reason about
the physical world. Fundamental to this reasoning is physical common sense:
understanding the physical properties and affordances of available objects, how
they can be manipulated, and how they interact with other objects. Physical
commonsense reasoning is fundamentally a multi-sensory task, since physical
properties are manifested through multiple modalities - two of them being
vision and acoustics. Our paper takes a step towards real-world physical
commonsense reasoning by contributing PACS: the first audiovisual benchmark
annotated for physical commonsense attributes. PACS contains 13,400
question-answer pairs, involving 1,377 unique physical commonsense questions
and 1,526 videos. Our dataset provides new opportunities to advance the
research field of physical reasoning by bringing audio as a core component of
this multimodal problem. Using PACS, we evaluate multiple state-of-the-art
models on our new challenging task. While some models show promising results
(70% accuracy), they all fall short of human performance (95% accuracy). We
conclude the paper by demonstrating the importance of multimodal reasoning and
providing possible avenues for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing Speech Emotion Recognition Transformers for Linguistic Knowledge. (arXiv:2204.00400v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00400">
<div class="article-summary-box-inner">
<span><p>Large, pre-trained neural networks consisting of self-attention layers
(transformers) have recently achieved state-of-the-art results on several
speech emotion recognition (SER) datasets. These models are typically
pre-trained in self-supervised manner with the goal to improve automatic speech
recognition performance -- and thus, to understand linguistic information. In
this work, we investigate the extent in which this information is exploited
during SER fine-tuning. Using a reproducible methodology based on open-source
tools, we synthesise prosodically neutral speech utterances while varying the
sentiment of the text. Valence predictions of the transformer model are very
reactive to positive and negative sentiment content, as well as negations, but
not to intensifiers or reducers, while none of those linguistic features impact
arousal or dominance. These findings show that transformers can successfully
leverage linguistic information to improve their valence predictions, and that
linguistic analysis should be included in their testing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modern Baselines for SPARQL Semantic Parsing. (arXiv:2204.12793v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12793">
<div class="article-summary-box-inner">
<span><p>In this work, we focus on the task of generating SPARQL queries from natural
language questions, which can then be executed on Knowledge Graphs (KGs). We
assume that gold entity and relations have been provided, and the remaining
task is to arrange them in the right order along with SPARQL vocabulary, and
input tokens to produce the correct SPARQL query. Pre-trained Language Models
(PLMs) have not been explored in depth on this task so far, so we experiment
with BART, T5 and PGNs (Pointer Generator Networks) with BERT embeddings,
looking for new baselines in the PLM era for this task, on DBpedia and Wikidata
KGs. We show that T5 requires special input tokenisation, but produces state of
the art performance on LC-QuAD 1.0 and LC-QuAD 2.0 datasets, and outperforms
task-specific models from previous works. Moreover, the methods enable semantic
parsing for questions where a part of the input needs to be copied to the
output query, thus enabling a new paradigm in KG semantic parsing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language statistics at different spatial, temporal, and grammatical scales. (arXiv:2207.00709v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00709">
<div class="article-summary-box-inner">
<span><p>Statistical linguistics has advanced considerably in recent decades as data
has become available. This has allowed researchers to study how statistical
properties of languages change over time. In this work, we use data from
Twitter to explore English and Spanish considering the rank diversity at
different scales: temporal (from 3 to 96 hour intervals), spatial (from 3km to
3000+km radii), and grammatical (from monograms to pentagrams). We find that
all three scales are relevant. However, the greatest changes come from
variations in the grammatical scale. At the lowest grammatical scale
(monograms), the rank diversity curves are most similar, independently on the
values of other scales, languages, and countries. As the grammatical scale
grows, the rank diversity curves vary more depending on the temporal and
spatial scales, as well as on the language and country. We also study the
statistics of Twitter-specific tokens: emojis, hashtags, and user mentions.
These particular type of tokens show a sigmoid kind of behaviour as a rank
diversity function. Our results are helpful to quantify aspects of language
statistics that seem universal and what may lead to variations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action. (arXiv:2207.04429v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04429">
<div class="article-summary-box-inner">
<span><p>Goal-conditioned policies for robotic navigation can be trained on large,
unannotated datasets, providing for good generalization to real-world settings.
However, particularly in vision-based settings where specifying goals requires
an image, this makes for an unnatural interface. Language provides a more
convenient modality for communication with robots, but contemporary methods
typically require expensive supervision, in the form of trajectories annotated
with language descriptions. We present a system, LM-Nav, for robotic navigation
that enjoys the benefits of training on unannotated large datasets of
trajectories, while still providing a high-level interface to the user. Instead
of utilizing a labeled instruction following dataset, we show that such a
system can be constructed entirely out of pre-trained models for navigation
(ViNG), image-language association (CLIP), and language modeling (GPT-3),
without requiring any fine-tuning or language-annotated robot data. We
instantiate LM-Nav on a real-world mobile robot and demonstrate long-horizon
navigation through complex, outdoor environments from natural language
instructions. For videos of our experiments, code release, and an interactive
Colab notebook that runs in your browser, please check out our project page
https://sites.google.com/view/lmnav
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sockeye 3: Fast Neural Machine Translation with PyTorch. (arXiv:2207.05851v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05851">
<div class="article-summary-box-inner">
<span><p>Sockeye 3 is the latest version of the Sockeye toolkit for Neural Machine
Translation (NMT). Now based on PyTorch, Sockeye 3 provides faster model
implementations and more advanced features with a further streamlined codebase.
This enables broader experimentation with faster iteration, efficient training
of stronger and faster models, and the flexibility to move new ideas quickly
from research to production. When running comparable models, Sockeye 3 is up to
126% faster than other PyTorch implementations on GPUs and up to 292% faster on
CPUs. Sockeye 3 is open source software released under the Apache 2.0 license.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FFTc: An MLIR Dialect for Developing HPC Fast Fourier Transform Libraries. (arXiv:2207.06803v2 [cs.MS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06803">
<div class="article-summary-box-inner">
<span><p>Discrete Fourier Transform (DFT) libraries are one of the most critical
software components for scientific computing. Inspired by FFTW, a widely used
library for DFT HPC calculations, we apply compiler technologies for the
development of HPC Fourier transform libraries. In this work, we introduce
FFTc, a domain-specific language, based on Multi-Level Intermediate
Representation (MLIR), for expressing Fourier Transform algorithms. We present
the initial design, implementation, and preliminary results of FFTc.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Color Coding of Large Value Ranges Applied to Meteorological Data. (arXiv:2207.12399v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12399">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel color scheme designed to address the challenge of
visualizing data series with large value ranges, where scale transformation
provides limited support. We focus on meteorological data, where the presence
of large value ranges is common. We apply our approach to meteorological
scatterplots, as one of the most common plots used in this domain area. Our
approach leverages the numerical representation of mantissa and exponent of the
values to guide the design of novel "nested" color schemes, able to emphasize
differences between magnitudes. Our user study evaluates the new designs, the
state of the art color scales and representative color schemes used in the
analysis of meteorological data: ColorCrafter, Viridis, and Rainbow. We assess
accuracy, time and confidence in the context of discrimination (comparison) and
interpretation (reading) tasks. Our proposed color scheme significantly
outperforms the others in interpretation tasks, while showing comparable
performances in discrimination tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Shape Sequence of Human Comparison and Classification using Current and Varifolds. (arXiv:2207.12485v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12485">
<div class="article-summary-box-inner">
<span><p>In this paper we address the task of the comparison and the classification of
3D shape sequences of human. The non-linear dynamics of the human motion and
the changing of the surface parametrization over the time make this task very
challenging. To tackle this issue, we propose to embed the 3D shape sequences
in an infinite dimensional space, the space of varifolds, endowed with an inner
product that comes from a given positive definite kernel. More specifically,
our approach involves two steps: 1) the surfaces are represented as varifolds,
this representation induces metrics equivariant to rigid motions and invariant
to parametrization; 2) the sequences of 3D shapes are represented by Gram
matrices derived from their infinite dimensional Hankel matrices. The problem
of comparison of two 3D sequences of human is formulated as a comparison of two
Gram-Hankel matrices. Extensive experiments on CVSSP3D and Dyna datasets show
that our method is competitive with state-of-the-art in 3D human sequence
motion retrieval. Code for the experiments is available at
https://github.com/CRISTAL-3DSAM/HumanComparisonVarifolds.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeuriCam: Video Super-Resolution and Colorization Using Key Frames. (arXiv:2207.12496v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12496">
<div class="article-summary-box-inner">
<span><p>We present NeuriCam, a key-frame video super-resolution and colorization
based system, to achieve low-power video capture from dual-mode IOT cameras.
Our idea is to design a dual-mode camera system where the first mode is low
power (1.1~mW) but only outputs gray-scale, low resolution and noisy video and
the second mode consumes much higher power (100~mW) but outputs color and
higher resolution images. To reduce total energy consumption, we heavily duty
cycle the high power mode to output an image only once every second. The data
from this camera system is then wirelessly streamed to a nearby plugged-in
gateway, where we run our real-time neural network decoder to reconstruct a
higher resolution color video. To achieve this, we introduce an attention
feature filter mechanism that assigns different weights to different features,
based on the correlation between the feature map and contents of the input
frame at each spatial location. We design a wireless hardware prototype using
off-the-shelf cameras and address practical issues including packet loss and
perspective mismatch. Our evaluation shows that our dual-camera hardware
reduces camera energy consumption while achieving an average gray-scale PSNR
gain of 3.7~dB over prior video super resolution methods and 5.6~dB RGB gain
over existing color propagation methods. Open-source code:
https://github.com/vb000/NeuriCam.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep learning-based algorithm for assessment of knee osteoarthritis severity in radiographs matches performance of radiologists. (arXiv:2207.12521v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12521">
<div class="article-summary-box-inner">
<span><p>A fully-automated deep learning algorithm matched performance of radiologists
in assessment of knee osteoarthritis severity in radiographs using the
Kellgren-Lawrence grading system.
</p>
<p>To develop an automated deep learning-based algorithm that jointly uses
Posterior-Anterior (PA) and Lateral (LAT) views of knee radiographs to assess
knee osteoarthritis severity according to the Kellgren-Lawrence grading system.
</p>
<p>We used a dataset of 9739 exams from 2802 patients from Multicenter
Osteoarthritis Study (MOST). The dataset was divided into a training set of
2040 patients, a validation set of 259 patients and a test set of 503 patients.
A novel deep learning-based method was utilized for assessment of knee OA in
two steps: (1) localization of knee joints in the images, (2) classification
according to the KL grading system. Our method used both PA and LAT views as
the input to the model. The scores generated by the algorithm were compared to
the grades provided in the MOST dataset for the entire test set as well as
grades provided by 5 radiologists at our institution for a subset of the test
set.
</p>
<p>The model obtained a multi-class accuracy of 71.90% on the entire test set
when compared to the ratings provided in the MOST dataset. The quadratic
weighted Kappa coefficient for this set was 0.9066. The average quadratic
weighted Kappa between all pairs of radiologists from our institution who took
a part of study was 0.748. The average quadratic-weighted Kappa between the
algorithm and the radiologists at our institution was 0.769.
</p>
<p>The proposed model performed demonstrated equivalency of KL classification to
MSK radiologists, but clearly superior reproducibility. Our model also agreed
with radiologists at our institution to the same extent as the radiologists
with each other. The algorithm could be used to provide reproducible assessment
of knee osteoarthritis severity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Trainability Preserving Neural Structured Pruning. (arXiv:2207.12534v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12534">
<div class="article-summary-box-inner">
<span><p>Several recent works empirically find finetuning learning rate is critical to
the final performance in neural network structured pruning. Further researches
find that the network trainability broken by pruning answers for it, thus
calling for an urgent need to recover trainability before finetuning. Existing
attempts propose to exploit weight orthogonalization to achieve dynamical
isometry for improved trainability. However, they only work for linear MLP
networks. How to develop a filter pruning method that maintains or recovers
trainability and is scalable to modern deep networks remains elusive. In this
paper, we present trainability preserving pruning (TPP), a regularization-based
structured pruning method that can effectively maintain trainability during
sparsification. Specifically, TPP regularizes the gram matrix of convolutional
kernels so as to de-correlate the pruned filters from the kept filters. Beside
the convolutional layers, we also propose to regularize the BN parameters for
better preserving trainability. Empirically, TPP can compete with the
ground-truth dynamical isometry recovery method on linear MLP networks. On
non-linear networks (ResNet56/VGG19, CIFAR datasets), it outperforms the other
counterpart solutions by a large margin. Moreover, TPP can also work
effectively with modern deep networks (ResNets) on ImageNet, delivering
encouraging performance in comparison to many top-performing filter pruning
methods. To our best knowledge, this is the first approach that effectively
maintains trainability during pruning for the large-scale deep neural networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Leak: Membership Inference Attacks Against Semi-supervised Learning. (arXiv:2207.12535v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12535">
<div class="article-summary-box-inner">
<span><p>Semi-supervised learning (SSL) leverages both labeled and unlabeled data to
train machine learning (ML) models. State-of-the-art SSL methods can achieve
comparable performance to supervised learning by leveraging much fewer labeled
data. However, most existing works focus on improving the performance of SSL.
In this work, we take a different angle by studying the training data privacy
of SSL. Specifically, we propose the first data augmentation-based membership
inference attacks against ML models trained by SSL. Given a data sample and the
black-box access to a model, the goal of membership inference attack is to
determine whether the data sample belongs to the training dataset of the model.
Our evaluation shows that the proposed attack can consistently outperform
existing membership inference attacks and achieves the best performance against
the model trained by SSL. Moreover, we uncover that the reason for membership
leakage in SSL is different from the commonly believed one in supervised
learning, i.e., overfitting (the gap between training and testing accuracy). We
observe that the SSL model is well generalized to the testing data (with almost
0 overfitting) but ''memorizes'' the training data by giving a more confident
prediction regardless of its correctness. We also explore early stopping as a
countermeasure to prevent membership inference attacks against SSL. The results
show that early stopping can mitigate the membership inference attack, but with
the cost of model's utility degradation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Live Stream Temporally Embedded 3D Human Body Pose and Shape Estimation. (arXiv:2207.12537v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12537">
<div class="article-summary-box-inner">
<span><p>3D Human body pose and shape estimation within a temporal sequence can be
quite critical for understanding human behavior. Despite the significant
progress in human pose estimation in the recent years, which are often based on
single images or videos, human motion estimation on live stream videos is still
a rarely-touched area considering its special requirements for real-time output
and temporal consistency. To address this problem, we present a temporally
embedded 3D human body pose and shape estimation (TePose) method to improve the
accuracy and temporal consistency of pose estimation in live stream videos.
TePose uses previous predictions as a bridge to feedback the error for better
estimation in the current frame and to learn the correspondence between data
frames and predictions in the history. A multi-scale spatio-temporal graph
convolutional network is presented as the motion discriminator for adversarial
training using datasets without any 3D labeling. We propose a sequential data
loading strategy to meet the special start-to-end data processing requirement
of live stream. We demonstrate the importance of each proposed module with
extensive experiments. The results show the effectiveness of TePose on
widely-used human pose benchmarks with state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inter-Frame Compression for Dynamic Point Cloud Geometry Coding. (arXiv:2207.12554v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12554">
<div class="article-summary-box-inner">
<span><p>Efficient point cloud compression is essential for applications like virtual
and mixed reality, autonomous driving, and cultural heritage. In this paper, we
propose a deep learning-based inter-frame encoding scheme for dynamic point
cloud geometry compression. We propose a lossy geometry compression scheme that
predicts the latent representation of the current frame using the previous
frame by employing a novel prediction network. Our proposed network utilizes
sparse convolutions with hierarchical multiscale 3D feature learning to encode
the current frame using the previous frame. We employ convolution on target
coordinates to map the latent representation of the previous frame to the
downsampled coordinates of the current frame to predict the current frame's
feature embedding. Our framework transmits the residual of the predicted
features and the actual features by compressing them using a learned
probabilistic factorized entropy model. At the receiver, the decoder
hierarchically reconstructs the current frame by progressively rescaling the
feature embedding. We compared our model to the state-of-the-art Video-based
Point Cloud Compression (V-PCC) and Geometry-based Point Cloud Compression
(G-PCC) schemes standardized by the Moving Picture Experts Group (MPEG). Our
method achieves more than 91% BD-Rate Bjontegaard Delta Rate) reduction against
G-PCC, more than 62% BD-Rate reduction against V-PCC intra-frame encoding mode,
and more than 52% BD-Rate savings against V-PCC P-frame-based inter-frame
encoding mode using HEVC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Static Hand Gesture Recognition for American Sign Language using Neuromorphic Hardware. (arXiv:2207.12559v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12559">
<div class="article-summary-box-inner">
<span><p>In this paper, we develop four spiking neural network (SNN) models for two
static American Sign Language (ASL) hand gesture classification tasks, i.e.,
the ASL Alphabet and ASL Digits. The SNN models are deployed on Intel's
neuromorphic platform, Loihi, and then compared against equivalent deep neural
network (DNN) models deployed on an edge computing device, the Intel Neural
Compute Stick 2 (NCS2). We perform a comprehensive comparison between the two
systems in terms of accuracy, latency, power consumption, and energy. The best
DNN model achieves an accuracy of 99.6% on the ASL Alphabet dataset, whereas
the best performing SNN model has an accuracy of 99.44%. For the ASL-Digits
dataset, the best SNN model outperforms all of its DNN counterparts with 99.52%
accuracy. Moreover, our obtained experimental results show that the Loihi
neuromorphic hardware implementations achieve up to 14.67x and 4.09x reduction
in power consumption and energy, respectively, when compared to NCS2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Seeing Far in the Dark with Patterned Flash. (arXiv:2207.12570v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12570">
<div class="article-summary-box-inner">
<span><p>Flash illumination is widely used in imaging under low-light environments.
However, illumination intensity falls off with propagation distance
quadratically, which poses significant challenges for flash imaging at a long
distance. We propose a new flash technique, named ``patterned flash'', for
flash imaging at a long distance. Patterned flash concentrates optical power
into a dot array. Compared with the conventional uniform flash where the signal
is overwhelmed by the noise everywhere, patterned flash provides stronger
signals at sparsely distributed points across the field of view to ensure the
signals at those points stand out from the sensor noise. This enables
post-processing to resolve important objects and details. Additionally, the
patterned flash projects texture onto the scene, which can be treated as a
structured light system for depth perception. Given the novel system, we
develop a joint image reconstruction and depth estimation algorithm with a
convolutional neural network. We build a hardware prototype and test the
proposed flash technique on various scenes. The experimental results
demonstrate that our patterned flash has significantly better performance at
long distances in low-light environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Translating a Visual LEGO Manual to a Machine-Executable Plan. (arXiv:2207.12572v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12572">
<div class="article-summary-box-inner">
<span><p>We study the problem of translating an image-based, step-by-step assembly
manual created by human designers into machine-interpretable instructions. We
formulate this problem as a sequential prediction task: at each step, our model
reads the manual, locates the components to be added to the current shape, and
infers their 3D poses. This task poses the challenge of establishing a 2D-3D
correspondence between the manual image and the real 3D object, and 3D pose
estimation for unseen 3D objects, since a new component to be added in a step
can be an object built from previous steps. To address these two challenges, we
present a novel learning-based framework, the Manual-to-Executable-Plan Network
(MEPNet), which reconstructs the assembly steps from a sequence of manual
images. The key idea is to integrate neural 2D keypoint detection modules and
2D-3D projection algorithms for high-precision prediction and strong
generalization to unseen components. The MEPNet outperforms existing methods on
three newly collected LEGO manual datasets and a Minecraft house dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WinoGAViL: Gamified Association Benchmark to Challenge Vision-and-Language Models. (arXiv:2207.12576v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12576">
<div class="article-summary-box-inner">
<span><p>While vision-and-language models perform well on tasks such as visual
question answering, they struggle when it comes to basic human commonsense
reasoning skills. In this work, we introduce WinoGAViL: an online game to
collect vision-and-language associations, (e.g., werewolves to a full moon),
used as a dynamic benchmark to evaluate state-of-the-art models. Inspired by
the popular card game Codenames, a spymaster gives a textual cue related to
several visual candidates, and another player has to identify them. Human
players are rewarded for creating associations that are challenging for a rival
AI model but still solvable by other human players. We use the game to collect
3.5K instances, finding that they are intuitive for humans (&gt;90% Jaccard index)
but challenging for state-of-the-art AI models, where the best model (ViLT)
achieves a score of 52%, succeeding mostly where the cue is visually salient.
Our analysis as well as the feedback we collect from players indicate that the
collected associations require diverse reasoning skills, including general
knowledge, common sense, abstraction, and more. We release the dataset, the
code and the interactive game, aiming to allow future data collection that can
be used to develop models with better association abilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compiler-Aware Neural Architecture Search for On-Mobile Real-time Super-Resolution. (arXiv:2207.12577v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12577">
<div class="article-summary-box-inner">
<span><p>Deep learning-based super-resolution (SR) has gained tremendous popularity in
recent years because of its high image quality performance and wide application
scenarios. However, prior methods typically suffer from large amounts of
computations and huge power consumption, causing difficulties for real-time
inference, especially on resource-limited platforms such as mobile devices. To
mitigate this, we propose a compiler-aware SR neural architecture search (NAS)
framework that conducts depth search and per-layer width search with adaptive
SR blocks. The inference speed is directly taken into the optimization along
with the SR loss to derive SR models with high image quality while satisfying
the real-time inference requirement. Instead of measuring the speed on mobile
devices at each iteration during the search process, a speed model incorporated
with compiler optimizations is leveraged to predict the inference latency of
the SR block with various width configurations for faster convergence. With the
proposed framework, we achieve real-time SR inference for implementing 720p
resolution with competitive SR performance (in terms of PSNR and SSIM) on
GPU/DSP of mobile platforms (Samsung Galaxy S21).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RenderNet: Visual Relocalization Using Virtual Viewpoints in Large-Scale Indoor Environments. (arXiv:2207.12579v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12579">
<div class="article-summary-box-inner">
<span><p>Visual relocalization has been a widely discussed problem in 3D vision: given
a pre-constructed 3D visual map, the 6 DoF (Degrees-of-Freedom) pose of a query
image is estimated. Relocalization in large-scale indoor environments enables
attractive applications such as augmented reality and robot navigation.
However, appearance changes fast in such environments when the camera moves,
which is challenging for the relocalization system. To address this problem, we
propose a virtual view synthesis-based approach, RenderNet, to enrich the
database and refine poses regarding this particular scenario. Instead of
rendering real images which requires high-quality 3D models, we opt to directly
render the needed global and local features of virtual viewpoints and apply
them in the subsequent image retrieval and feature matching operations
respectively. The proposed method can largely improve the performance in
large-scale indoor environments, e.g., achieving an improvement of 7.1\% and
12.2\% on the Inloc dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TGCF: Texture guided color fusion for impressionism oil painting style rendering. (arXiv:2207.12585v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12585">
<div class="article-summary-box-inner">
<span><p>As a major branch of Non-Photorealistic Rendering (NPR), image stylization
mainly uses the computer algorithms to render a photo into an artistic
painting. Recent work has shown that the extraction of style information such
as stroke texture and color of the target style image is the key to image
stylization. Given its stroke texture and color characteristics, a new stroke
rendering method is proposed, which fully considers the tonal characteristics
and the representative color of the original oil painting, in order to fit the
tone of the original oil painting image into the stylized image and make it
close to the artist's creative effect. The experiments have validated the
efficacy of the proposed model. This method would be more suitable for the
works of pointillism painters with a relatively uniform sense of direction,
especially for natural scenes. When the original painting brush strokes have a
clearer sense of direction, using this method to simulate brushwork texture
features can be less satisfactory.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large-displacement 3D Object Tracking with Hybrid Non-local Optimization. (arXiv:2207.12620v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12620">
<div class="article-summary-box-inner">
<span><p>Optimization-based 3D object tracking is known to be precise and fast, but
sensitive to large inter-frame displacements. In this paper we propose a fast
and effective non-local 3D tracking method. Based on the observation that
erroneous local minimum are mostly due to the out-of-plane rotation, we propose
a hybrid approach combining non-local and local optimizations for different
parameters, resulting in efficient non-local search in the 6D pose space. In
addition, a precomputed robust contour-based tracking method is proposed for
the pose optimization. By using long search lines with multiple candidate
correspondences, it can adapt to different frame displacements without the need
of coarse-to-fine search. After the pre-computation, pose updates can be
conducted very fast, enabling the non-local optimization to run in real time.
Our method outperforms all previous methods for both small and large
displacements. For large displacements, the accuracy is greatly improved
($81.7\% \;\text{v.s.}\; 19.4\%$). At the same time, real-time speed ($&gt;$50fps)
can be achieved with only CPU. The source code is available at
\url{https://github.com/cvbubbles/nonlocal-3dtracking}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Attention Network for Compressed Video Referring Object Segmentation. (arXiv:2207.12622v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12622">
<div class="article-summary-box-inner">
<span><p>Referring video object segmentation aims to segment the object referred by a
given language expression. Existing works typically require compressed video
bitstream to be decoded to RGB frames before being segmented, which increases
computation and storage requirements and ultimately slows the inference down.
This may hamper its application in real-world computing resource limited
scenarios, such as autonomous cars and drones. To alleviate this problem, in
this paper, we explore the referring object segmentation task on compressed
videos, namely on the original video data flow. Besides the inherent difficulty
of the video referring object segmentation task itself, obtaining
discriminative representation from compressed video is also rather challenging.
To address this problem, we propose a multi-attention network which consists of
dual-path dual-attention module and a query-based cross-modal Transformer
module. Specifically, the dual-path dual-attention module is designed to
extract effective representation from compressed data in three modalities,
i.e., I-frame, Motion Vector and Residual. The query-based cross-modal
Transformer firstly models the correlation between linguistic and visual
modalities, and then the fused multi-modality features are used to guide object
queries to generate a content-aware dynamic kernel and to predict final
segmentation masks. Different from previous works, we propose to learn just one
kernel, which thus removes the complicated post mask-matching procedure of
existing methods. Extensive promising experimental results on three challenging
datasets show the effectiveness of our method compared against several
state-of-the-art methods which are proposed for processing RGB data. Source
code is available at: https://github.com/DexiangHong/MANet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Hierarchy Aware Features for Reducing Mistake Severity. (arXiv:2207.12646v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12646">
<div class="article-summary-box-inner">
<span><p>Label hierarchies are often available apriori as part of biological taxonomy
or language datasets WordNet. Several works exploit these to learn hierarchy
aware features in order to improve the classifier to make semantically
meaningful mistakes while maintaining or reducing the overall error. In this
paper, we propose a novel approach for learning Hierarchy Aware Features (HAF)
that leverages classifiers at each level of the hierarchy that are constrained
to generate predictions consistent with the label hierarchy. The classifiers
are trained by minimizing a Jensen-Shannon Divergence with target soft labels
obtained from the fine-grained classifiers. Additionally, we employ a simple
geometric loss that constrains the feature space geometry to capture the
semantic structure of the label space. HAF is a training time approach that
improves the mistakes while maintaining top-1 error, thereby, addressing the
problem of cross-entropy loss that treats all mistakes as equal. We evaluate
HAF on three hierarchical datasets and achieve state-of-the-art results on the
iNaturalist-19 and CIFAR-100 datasets. The source code is available at
https://github.com/07Agarg/HAF
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Modal Causal Relational Reasoning for Event-Level Visual Question Answering. (arXiv:2207.12647v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12647">
<div class="article-summary-box-inner">
<span><p>Existing visual question answering methods tend to capture the spurious
correlations from visual and linguistic modalities, and fail to discover the
true casual mechanism that facilitates reasoning truthfully based on the
dominant visual evidence and the correct question intention. Additionally, the
existing methods usually ignore the complex event-level understanding in
multi-modal settings that requires a strong cognitive capability of causal
inference to jointly model cross-modal event temporality, causality, and
dynamics. In this work, we focus on event-level visual question answering from
a new perspective, i.e., cross-modal causal relational reasoning, by
introducing causal intervention methods to mitigate the spurious correlations
and discover the true causal structures for the integration of visual and
linguistic modalities. Specifically, we propose a novel event-level visual
question answering framework named Cross-Modal Causal RelatIonal Reasoning
(CMCIR), to achieve robust casuality-aware visual-linguistic question
answering. To uncover the causal structures for visual and linguistic
modalities, the novel Causality-aware Visual-Linguistic Reasoning (CVLR) module
is proposed to collaboratively disentangle the visual and linguistic spurious
correlations via elaborately designed front-door and back-door causal
intervention modules. To discover the fine-grained interactions between
linguistic semantics and spatial-temporal representations, we build a novel
Spatial-Temporal Transformer (STT) that builds the multi-modal co-occurrence
interactions between visual and linguistic content. Extensive experiments on
large-scale event-level urban dataset SUTD-TrafficQA and three benchmark
real-world datasets TGIF-QA, MSVD-QA, and MSRVTT-QA demonstrate the
effectiveness of our CMCIR for discovering visual-linguistic causal structures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient and Accurate Skeleton-Based Two-Person Interaction Recognition Using Inter- and Intra-body Graphs. (arXiv:2207.12648v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12648">
<div class="article-summary-box-inner">
<span><p>Skeleton-based two-person interaction recognition has been gaining increasing
attention as advancements are made in pose estimation and graph convolutional
networks. Although the accuracy has been gradually improving, the increasing
computational complexity makes it more impractical for a real-world
environment. There is still room for accuracy improvement as the conventional
methods do not fully represent the relationship between inter-body joints. In
this paper, we propose a lightweight model for accurately recognizing
two-person interactions. In addition to the architecture, which incorporates
middle fusion, we introduce a factorized convolution technique to reduce the
weight parameters of the model. We also introduce a network stream that
accounts for relative distance changes between inter-body joints to improve
accuracy. Experiments using two large-scale datasets, NTU RGB+D 60 and 120,
show that our method simultaneously achieved the highest accuracy and
relatively low computational complexity compared with the conventional methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Asymmetric Scalable Cross-modal Hashing. (arXiv:2207.12650v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12650">
<div class="article-summary-box-inner">
<span><p>Cross-modal hashing is a successful method to solve large-scale multimedia
retrieval issue. A lot of matrix factorization-based hashing methods are
proposed. However, the existing methods still struggle with a few problems,
such as how to generate the binary codes efficiently rather than directly relax
them to continuity. In addition, most of the existing methods choose to use an
$n\times n$ similarity matrix for optimization, which makes the memory and
computation unaffordable. In this paper we propose a novel Asymmetric Scalable
Cross-Modal Hashing (ASCMH) to address these issues. It firstly introduces a
collective matrix factorization to learn a common latent space from the
kernelized features of different modalities, and then transforms the similarity
matrix optimization to a distance-distance difference problem minimization with
the help of semantic labels and common latent space. Hence, the computational
complexity of the $n\times n$ asymmetric optimization is relieved. In the
generation of hash codes we also employ an orthogonal constraint of label
information, which is indispensable for search accuracy. So the redundancy of
computation can be much reduced. For efficient optimization and scalable to
large-scale datasets, we adopt the two-step approach rather than optimizing
simultaneously. Extensive experiments on three benchmark datasets: Wiki,
MIRFlickr-25K, and NUS-WIDE, demonstrate that our ASCMH outperforms the
state-of-the-art cross-modal hashing methods in terms of accuracy and
efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Deep Learning Assist Automatic Identification of Layered Pigments From XRF Data?. (arXiv:2207.12651v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12651">
<div class="article-summary-box-inner">
<span><p>X-ray fluorescence spectroscopy (XRF) plays an important role for elemental
analysis in a wide range of scientific fields, especially in cultural heritage.
XRF imaging, which uses a raster scan to acquire spectra across artworks,
provides the opportunity for spatial analysis of pigment distributions based on
their elemental composition. However, conventional XRF-based pigment
identification relies on time-consuming elemental mapping by expert
interpretations of measured spectra. To reduce the reliance on manual work,
recent studies have applied machine learning techniques to cluster similar XRF
spectra in data analysis and to identify the most likely pigments.
Nevertheless, it is still challenging for automatic pigment identification
strategies to directly tackle the complex structure of real paintings, e.g.
pigment mixtures and layered pigments. In addition, pixel-wise pigment
identification based on XRF imaging remains an obstacle due to the high noise
level compared with averaged spectra. Therefore, we developed a
deep-learning-based end-to-end pigment identification framework to fully
automate the pigment identification process. In particular, it offers high
sensitivity to the underlying pigments and to the pigments with a low
concentration, therefore enabling satisfying results in mapping the pigments
based on single-pixel XRF spectrum. As case studies, we applied our framework
to lab-prepared mock-up paintings and two 19th-century paintings: Paul
Gauguin's Po\`emes Barbares (1896) that contains layered pigments with an
underlying painting, and Paul Cezanne's The Bathers (1899-1904). The pigment
identification results demonstrated that our model achieved comparable results
to the analysis by elemental mapping, suggesting the generalizability and
stability of our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ProposalContrast: Unsupervised Pre-training for LiDAR-based 3D Object Detection. (arXiv:2207.12654v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12654">
<div class="article-summary-box-inner">
<span><p>Existing approaches for unsupervised point cloud pre-training are constrained
to either scene-level or point/voxel-level instance discrimination. Scene-level
methods tend to lose local details that are crucial for recognizing the road
objects, while point/voxel-level methods inherently suffer from limited
receptive field that is incapable of perceiving large objects or context
environments. Considering region-level representations are more suitable for 3D
object detection, we devise a new unsupervised point cloud pre-training
framework, called ProposalContrast, that learns robust 3D representations by
contrasting region proposals. Specifically, with an exhaustive set of region
proposals sampled from each point cloud, geometric point relations within each
proposal are modeled for creating expressive proposal representations. To
better accommodate 3D detection properties, ProposalContrast optimizes with
both inter-cluster and inter-proposal separation, i.e., sharpening the
discriminativeness of proposal representations across semantic classes and
object instances. The generalizability and transferability of ProposalContrast
are verified on various 3D detectors (i.e., PV-RCNN, CenterPoint, PointPillars
and PointRCNN) and datasets (i.e., KITTI, Waymo and ONCE).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-supervised 3D Object Detection with Proficient Teachers. (arXiv:2207.12655v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12655">
<div class="article-summary-box-inner">
<span><p>Dominated point cloud-based 3D object detectors in autonomous driving
scenarios rely heavily on the huge amount of accurately labeled samples,
however, 3D annotation in the point cloud is extremely tedious, expensive and
time-consuming. To reduce the dependence on large supervision, semi-supervised
learning (SSL) based approaches have been proposed. The Pseudo-Labeling
methodology is commonly used for SSL frameworks, however, the low-quality
predictions from the teacher model have seriously limited its performance. In
this work, we propose a new Pseudo-Labeling framework for semi-supervised 3D
object detection, by enhancing the teacher model to a proficient one with
several necessary designs. First, to improve the recall of pseudo labels, a
Spatialtemporal Ensemble (STE) module is proposed to generate sufficient seed
boxes. Second, to improve the precision of recalled boxes, a Clusteringbased
Box Voting (CBV) module is designed to get aggregated votes from the clustered
seed boxes. This also eliminates the necessity of sophisticated thresholds to
select pseudo labels. Furthermore, to reduce the negative influence of wrongly
pseudo-labeled samples during the training, a soft supervision signal is
proposed by considering Box-wise Contrastive Learning (BCL). The effectiveness
of our model is verified on both ONCE and Waymo datasets. For example, on ONCE,
our approach significantly improves the baseline by 9.51 mAP. Moreover, with
half annotations, our model outperforms the oracle model with full annotations
on Waymo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph Neural Network and Spatiotemporal Transformer Attention for 3D Video Object Detection from Point Clouds. (arXiv:2207.12659v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12659">
<div class="article-summary-box-inner">
<span><p>Previous works for LiDAR-based 3D object detection mainly focus on the
single-frame paradigm. In this paper, we propose to detect 3D objects by
exploiting temporal information in multiple frames, i.e., the point cloud
videos. We empirically categorize the temporal information into short-term and
long-term patterns. To encode the short-term data, we present a Grid Message
Passing Network (GMPNet), which considers each grid (i.e., the grouped points)
as a node and constructs a k-NN graph with the neighbor grids. To update
features for a grid, GMPNet iteratively collects information from its
neighbors, thus mining the motion cues in grids from nearby frames. To further
aggregate the long-term frames, we propose an Attentive Spatiotemporal
Transformer GRU (AST-GRU), which contains a Spatial Transformer Attention (STA)
module and a Temporal Transformer Attention (TTA) module. STA and TTA enhance
the vanilla GRU to focus on small objects and better align the moving objects.
Our overall framework supports both online and offline video object detection
in point clouds. We implement our algorithm based on prevalent anchor-based and
anchor-free detectors. The evaluation results on the challenging nuScenes
benchmark show the superior performance of our method, achieving the 1st on the
leaderboard without any bells and whistles, by the time the paper is submitted.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Visual Representation from Modality-Shared Contrastive Language-Image Pre-training. (arXiv:2207.12661v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12661">
<div class="article-summary-box-inner">
<span><p>Large-scale multi-modal contrastive pre-training has demonstrated great
utility to learn transferable features for a range of downstream tasks by
mapping multiple modalities into a shared embedding space. Typically, this has
employed separate encoders for each modality. However, recent work suggests
that transformers can support learning across multiple modalities and allow
knowledge sharing. Inspired by this, we investigate a variety of
Modality-Shared Contrastive Language-Image Pre-training (MS-CLIP) frameworks.
More specifically, we question how many parameters of a transformer model can
be shared across modalities during contrastive pre-training, and rigorously
examine architectural design choices that position the proportion of parameters
shared along a spectrum. In studied conditions, we observe that a mostly
unified encoder for vision and language signals outperforms all other
variations that separate more parameters. Additionally, we find that
light-weight modality-specific parallel modules further improve performance.
Experimental results show that the proposed MS-CLIP approach outperforms
vanilla CLIP by up to 13\% relative in zero-shot ImageNet classification
(pre-trained on YFCC-100M), while simultaneously supporting a reduction of
parameters. In addition, our approach outperforms vanilla CLIP by 1.6 points in
linear probing on a collection of 24 downstream vision tasks. Furthermore, we
discover that sharing parameters leads to semantic concepts from different
modalities being encoded more closely in the embedding space, facilitating the
transferring of common semantic structure (e.g., attention patterns) from
language to vision. Code is available at
\href{https://github.com/Hxyou/MSCLIP}{URL}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Kendall Shape Space Approach to 3D Shape Estimation from 2D Landmarks. (arXiv:2207.12687v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12687">
<div class="article-summary-box-inner">
<span><p>3D shapes provide substantially more information than 2D images. However, the
acquisition of 3D shapes is sometimes very difficult or even impossible in
comparison with acquiring 2D images, making it necessary to derive the 3D shape
from 2D images. Although this is, in general, a mathematically ill-posed
problem, it might be solved by constraining the problem formulation using prior
information. Here, we present a new approach based on Kendall's shape space to
reconstruct 3D shapes from single monocular 2D images. The work is motivated by
an application to study the feeding behavior of the basking shark, an
endangered species whose massive size and mobility render 3D shape data nearly
impossible to obtain, hampering understanding of their feeding behaviors and
ecology. 2D images of these animals in feeding position, however, are readily
available. We compare our approach with state-of-the-art shape-based
approaches, both on human stick models and on shark head skeletons. Using a
small set of training shapes, we show that the Kendall shape space approach is
substantially more robust than previous methods and results in plausible
shapes. This is essential for the motivating application in which specimens are
rare and therefore only few training shapes are available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CENet: Toward Concise and Efficient LiDAR Semantic Segmentation for Autonomous Driving. (arXiv:2207.12691v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12691">
<div class="article-summary-box-inner">
<span><p>Accurate and fast scene understanding is one of the challenging task for
autonomous driving, which requires to take full advantage of LiDAR point clouds
for semantic segmentation. In this paper, we present a \textbf{concise} and
\textbf{efficient} image-based semantic segmentation network, named
\textbf{CENet}. In order to improve the descriptive power of learned features
and reduce the computational as well as time complexity, our CENet integrates
the convolution with larger kernel size instead of MLP, carefully-selected
activation functions, and multiple auxiliary segmentation heads with
corresponding loss functions into architecture. Quantitative and qualitative
experiments conducted on publicly available benchmarks, SemanticKITTI and
SemanticPOSS, demonstrate that our pipeline achieves much better mIoU and
inference performance compared with state-of-the-art models. The code will be
available at https://github.com/huixiancheng/CENet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparison of Deep Learning and Machine Learning Models and Frameworks for Skin Lesion Classification. (arXiv:2207.12715v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12715">
<div class="article-summary-box-inner">
<span><p>The incidence rate for skin cancer has been steadily increasing throughout
the world, leading to it being a serious issue. Diagnosis at an early stage has
the potential to drastically reduce the harm caused by the disease, however,
the traditional biopsy is a labor-intensive and invasive procedure. In
addition, numerous rural communities do not have easy access to hospitals and
do not prefer visiting one for what they feel might be a minor issue. Using
machine learning and deep learning for skin cancer classification can increase
accessibility and reduce the discomforting procedures involved in the
traditional lesion detection process. These models can be wrapped in web or
mobile apps and serve a greater population. In this paper, two such models are
tested on the benchmark HAM10000 dataset of common skin lesions. They are
Random Forest with Stratified K-Fold Validation, and MobileNetV2 (throughout
the rest of the paper referred to as MobileNet). The MobileNet model was
trained separately using both TensorFlow and PyTorch frameworks. A side-by-side
comparison of both deep learning and machine learning models and a comparison
of the same deep learning model on different frameworks for skin lesion
diagnosis in a resource-constrained mobile environment has not been conducted
before. The results indicate that each of these models fares better at
different classification tasks. For greater overall recall, accuracy, and
detection of malignant melanoma, the TensorFlow MobileNet was the better
choice. However, for detecting noncancerous skin lesions, the PyTorch MobileNet
proved to be better. Random Forest was the better algorithm when it came to
having a low computational cost with moderate correctness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MV-FCOS3D++: Multi-View Camera-Only 4D Object Detection with Pretrained Monocular Backbones. (arXiv:2207.12716v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12716">
<div class="article-summary-box-inner">
<span><p>In this technical report, we present our solution, dubbed MV-FCOS3D++, for
the Camera-Only 3D Detection track in Waymo Open Dataset Challenge 2022. For
multi-view camera-only 3D detection, methods based on bird-eye-view or 3D
geometric representations can leverage the stereo cues from overlapped regions
between adjacent views and directly perform 3D detection without hand-crafted
post-processing. However, it lacks direct semantic supervision for 2D
backbones, which can be complemented by pretraining simple monocular-based
detectors. Our solution is a multi-view framework for 4D detection following
this paradigm. It is built upon a simple monocular detector FCOS3D++,
pretrained only with object annotations of Waymo, and converts multi-view
features to a 3D grid space to detect 3D objects thereon. A dual-path neck for
single-frame understanding and temporal stereo matching is devised to
incorporate multi-frame information. Our method finally achieves 49.75% mAPL
with a single model and wins 2nd place in the WOD challenge, without any
LiDAR-based depth supervision during training. The code will be released at
https://github.com/Tai-Wang/Depth-from-Motion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Convolutional neural networks and multi-threshold analysis for contamination detection in the apparel industry. (arXiv:2207.12720v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12720">
<div class="article-summary-box-inner">
<span><p>Quality control of apparel items is mandatory in modern textile industry, as
consumer's awareness and expectations about the highest possible standard is
constantly increasing in favor of sustainable and ethical textile products.
Such a level of quality is achieved by checking the product throughout its life
cycle, from raw materials to boxed stock. Checks may include color shading
tests, fasteners fatigue tests, fabric weigh tests, contamination tests, etc.
This work deals specifically with the automatic detection of contaminations
given by small parts in the finished product such as raw material like little
stones and plastic bits or materials from the construction process, like a
whole needle or a clip. Identification is performed by a two-level processing
of X-ray images of the items: in the first, a multi-threshold analysis
recognizes the contaminations by gray level and shape attributes; the second
level consists of a deep learning classifier that has been trained to
distinguish between true positives and false positives. The automatic detector
was successfully deployed in an actual production plant, since the results
satisfy the technical specification of the process, namely a number of false
negatives smaller than 3% and a number of false positives smaller than 15%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">$\textbf{P$^2$A}$: A Dataset and Benchmark for Dense Action Detection from Table Tennis Match Broadcasting Videos. (arXiv:2207.12730v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12730">
<div class="article-summary-box-inner">
<span><p>While deep learning has been widely used for video analytics, such as video
classification and action detection, dense action detection with fast-moving
subjects from sports videos is still challenging. In this work, we release yet
another sports video dataset $\textbf{P$^2$A}$ for $\underline{P}$ing
$\underline{P}$ong-$\underline{A}$ction detection, which consists of 2,721
video clips collected from the broadcasting videos of professional table tennis
matches in World Table Tennis Championships and Olympiads. We work with a crew
of table tennis professionals and referees to obtain fine-grained action labels
(in 14 classes) for every ping-pong action that appeared in the dataset and
formulate two sets of action detection problems - action localization and
action recognition. We evaluate a number of commonly-seen action recognition
(e.g., TSM, TSN, Video SwinTransformer, and Slowfast) and action localization
models (e.g., BSN, BSN++, BMN, TCANet), using $\textbf{P$^2$A}$ for both
problems, under various settings. These models can only achieve 48% area under
the AR-AN curve for localization and 82% top-one accuracy for recognition since
the ping-pong actions are dense with fast-moving subjects but broadcasting
videos are with only 25 FPS. The results confirm that $\textbf{P$^2$A}$ is
still a challenging task and can be used as a benchmark for action detection
from videos.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distribution Learning Based on Evolutionary Algorithm Assisted Deep Neural Networks for Imbalanced Image Classification. (arXiv:2207.12744v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12744">
<div class="article-summary-box-inner">
<span><p>To address the trade-off problem of quality-diversity for the generated
images in imbalanced classification tasks, we research on over-sampling based
methods at the feature level instead of the data level and focus on searching
the latent feature space for optimal distributions. On this basis, we propose
an iMproved Estimation Distribution Algorithm based Latent featUre Distribution
Evolution (MEDA_LUDE) algorithm, where a joint learning procedure is programmed
to make the latent features both optimized and evolved by the deep neural
networks and the evolutionary algorithm, respectively. We explore the effect of
the Large-margin Gaussian Mixture (L-GM) loss function on distribution learning
and design a specialized fitness function based on the similarities among
samples to increase diversity. Extensive experiments on benchmark based
imbalanced datasets validate the effectiveness of our proposed algorithm, which
can generate images with both quality and diversity. Furthermore, the MEDA_LUDE
algorithm is also applied to the industrial field and successfully alleviates
the imbalanced issue in fabric defect classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Criteria Comparative Learning for Real-scene Image Super-Resolution. (arXiv:2207.12767v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12767">
<div class="article-summary-box-inner">
<span><p>Real-scene image super-resolution aims to restore real-world low-resolution
images into their high-quality versions. A typical RealSR framework usually
includes the optimization of multiple criteria which are designed for different
image properties, by making the implicit assumption that the ground-truth
images can provide a good trade-off between different criteria. However, this
assumption could be easily violated in practice due to the inherent contrastive
relationship between different image properties. Contrastive learning (CL)
provides a promising recipe to relieve this problem by learning discriminative
features using the triplet contrastive losses. Though CL has achieved
significant success in many computer vision tasks, it is non-trivial to
introduce CL to RealSR due to the difficulty in defining valid positive image
pairs in this case. Inspired by the observation that the contrastive
relationship could also exist between the criteria, in this work, we propose a
novel training paradigm for RealSR, named Criteria Comparative Learning
(Cria-CL), by developing contrastive losses defined on criteria instead of
image patches. In addition, a spatial projector is proposed to obtain a good
view for Cria-CL in RealSR. Our experiments demonstrate that compared with the
typical weighted regression strategy, our method achieves a significant
improvement under similar parameter settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Study on the Use of Edge TPUs for Eye Fundus Image Segmentation. (arXiv:2207.12770v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12770">
<div class="article-summary-box-inner">
<span><p>Medical image segmentation can be implemented using Deep Learning methods
with fast and efficient segmentation networks. Single-board computers (SBCs)
are difficult to use to train deep networks due to their memory and processing
limitations. Specific hardware such as Google's Edge TPU makes them suitable
for real time predictions using complex pre-trained networks. In this work, we
study the performance of two SBCs, with and without hardware acceleration for
fundus image segmentation, though the conclusions of this study can be applied
to the segmentation by deep neural networks of other types of medical images.
To test the benefits of hardware acceleration, we use networks and datasets
from a previous published work and generalize them by testing with a dataset
with ultrasound thyroid images. We measure prediction times in both SBCs and
compare them with a cloud based TPU system. The results show the feasibility of
Machine Learning accelerated SBCs for optic disc and cup segmentation obtaining
times below 25 milliseconds per image using Edge TPUs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Static and Dynamic Concepts for Self-supervised Video Representation Learning. (arXiv:2207.12795v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12795">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel learning scheme for self-supervised video
representation learning. Motivated by how humans understand videos, we propose
to first learn general visual concepts then attend to discriminative local
areas for video understanding. Specifically, we utilize static frame and frame
difference to help decouple static and dynamic concepts, and respectively align
the concept distributions in latent space. We add diversity and fidelity
regularizations to guarantee that we learn a compact set of meaningful
concepts. Then we employ a cross-attention mechanism to aggregate detailed
local features of different concepts, and filter out redundant concepts with
low activations to perform local concept contrast. Extensive experiments
demonstrate that our method distills meaningful static and dynamic concepts to
guide video understanding, and obtains state-of-the-art results on UCF-101,
HMDB-51, and Diving-48.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Class-Aware Universum Inspired Re-Balance Learning for Long-Tailed Recognition. (arXiv:2207.12808v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12808">
<div class="article-summary-box-inner">
<span><p>Data augmentation for minority classes is an effective strategy for
long-tailed recognition, thus developing a large number of methods. Although
these methods all ensure the balance in sample quantity, the quality of the
augmented samples is not always satisfactory for recognition, being prone to
such problems as over-fitting, lack of diversity, semantic drift, etc. For
these issues, we propose the Class-aware Universum Inspired Re-balance
Learning(CaUIRL) for long-tailed recognition, which endows the Universum with
class-aware ability to re-balance individual minority classes from both sample
quantity and quality. In particular, we theoretically prove that the
classifiers learned by CaUIRL are consistent with those learned under the
balanced condition from a Bayesian perspective. In addition, we further develop
a higher-order mixup approach, which can automatically generate class-aware
Universum(CaU) data without resorting to any external data. Unlike the
traditional Universum, such generated Universum additionally takes the domain
similarity, class separability, and sample diversity into account. Extensive
experiments on benchmark datasets demonstrate the surprising advantages of our
method, especially the top1 accuracy in minority classes is improved by 1.9% 6%
compared to the state-of-the-art method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bodily Behaviors in Social Interaction: Novel Annotations and State-of-the-Art Evaluation. (arXiv:2207.12817v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12817">
<div class="article-summary-box-inner">
<span><p>Body language is an eye-catching social signal and its automatic analysis can
significantly advance artificial intelligence systems to understand and
actively participate in social interactions. While computer vision has made
impressive progress in low-level tasks like head and body pose estimation, the
detection of more subtle behaviors such as gesturing, grooming, or fumbling is
not well explored. In this paper we present BBSI, the first set of annotations
of complex Bodily Behaviors embedded in continuous Social Interactions in a
group setting. Based on previous work in psychology, we manually annotated 26
hours of spontaneous human behavior in the MPIIGroupInteraction dataset with 15
distinct body language classes. We present comprehensive descriptive statistics
on the resulting dataset as well as results of annotation quality evaluations.
For automatic detection of these behaviors, we adapt the Pyramid Dilated
Attention Network (PDAN), a state-of-the-art approach for human action
detection. We perform experiments using four variants of spatial-temporal
features as input to PDAN: Two-Stream Inflated 3D CNN, Temporal Segment
Networks, Temporal Shift Module and Swin Transformer. Results are promising and
indicate a great room for improvement in this difficult task. Representing a
key piece in the puzzle towards automatic understanding of social behavior,
BBSI is fully available to the research community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">S-Prompts Learning with Pre-trained Transformers: An Occam's Razor for Domain Incremental Learning. (arXiv:2207.12819v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12819">
<div class="article-summary-box-inner">
<span><p>State-of-the-art deep neural networks are still struggling to address the
catastrophic forgetting problem in continual learning. In this paper, we
propose one simple paradigm (named as S-Prompting) and two concrete approaches
to highly reduce the forgetting degree in one of the most typical continual
learning scenarios, i.e., domain increment learning (DIL). The key idea of the
paradigm is to learn prompts independently across domains with pre-trained
transformers, avoiding the use of exemplars that commonly appear in
conventional methods. This results in a win-win game where the prompting can
achieve the best for each domain. The independent prompting across domains only
requests one single cross-entropy loss for training and one simple K-NN
operation as a domain identifier for inference. The learning paradigm derives
an image prompt learning approach and a brand-new language-image prompt
learning approach. Owning an excellent scalability (0.03% parameter increase
per domain), the best of our approaches achieves a remarkable relative
improvement (an average of about 30%) over the best of the state-of-the-art
exemplar-free methods for three standard DIL tasks, and even surpasses the best
of them relatively by about 6% in average when they use exemplars.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compositional Human-Scene Interaction Synthesis with Semantic Control. (arXiv:2207.12824v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12824">
<div class="article-summary-box-inner">
<span><p>Synthesizing natural interactions between virtual humans and their 3D
environments is critical for numerous applications, such as computer games and
AR/VR experiences. Our goal is to synthesize humans interacting with a given 3D
scene controlled by high-level semantic specifications as pairs of action
categories and object instances, e.g., "sit on the chair". The key challenge of
incorporating interaction semantics into the generation framework is to learn a
joint representation that effectively captures heterogeneous information,
including human body articulation, 3D object geometry, and the intent of the
interaction. To address this challenge, we design a novel transformer-based
generative model, in which the articulated 3D human body surface points and 3D
objects are jointly encoded in a unified latent space, and the semantics of the
interaction between the human and objects are embedded via positional encoding.
Furthermore, inspired by the compositional nature of interactions that humans
can simultaneously interact with multiple objects, we define interaction
semantics as the composition of varying numbers of atomic action-object pairs.
Our proposed generative model can naturally incorporate varying numbers of
atomic interactions, which enables synthesizing compositional human-scene
interactions without requiring composite interaction data. We extend the PROX
dataset with interaction semantic labels and scene instance segmentation to
evaluate our method and demonstrate that our method can generate realistic
human-scene interactions with semantic control. Our perceptual study shows that
our synthesized virtual humans can naturally interact with 3D scenes,
considerably outperforming existing methods. We name our method COINS, for
COmpositional INteraction Synthesis with Semantic Control. Code and data are
available at https://github.com/zkf1997/COINS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal-GuideNet: Gaze-Probe Bidirectional Guidance in Obstetric Ultrasound Scanning. (arXiv:2207.12833v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12833">
<div class="article-summary-box-inner">
<span><p>Eye trackers can provide visual guidance to sonographers during ultrasound
(US) scanning. Such guidance is potentially valuable for less experienced
operators to improve their scanning skills on how to manipulate the probe to
achieve the desired plane. In this paper, a multimodal guidance approach
(Multimodal-GuideNet) is proposed to capture the stepwise dependency between a
real-world US video signal, synchronized gaze, and probe motion within a
unified framework. To understand the causal relationship between gaze movement
and probe motion, our model exploits multitask learning to jointly learn two
related tasks: predicting gaze movements and probe signals that an experienced
sonographer would perform in routine obstetric scanning. The two tasks are
associated by a modality-aware spatial graph to detect the co-occurrence among
the multi-modality inputs and share useful cross-modal information. Instead of
a deterministic scanning path, Multimodal-GuideNet allows for scanning
diversity by estimating the probability distribution of real scans. Experiments
performed with three typical obstetric scanning examinations show that the new
approach outperforms single-task learning for both probe motion guidance and
gaze movement prediction. Multimodal-GuideNet also provides a visual guidance
signal with an error rate of less than 10 pixels for a 224x288 US image.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KinePose: A temporally optimized inverse kinematics technique for 6DOF human pose estimation with biomechanical constraints. (arXiv:2207.12841v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12841">
<div class="article-summary-box-inner">
<span><p>Computer vision/deep learning-based 3D human pose estimation methods aim to
localize human joints from images and videos. Pose representation is normally
limited to 3D joint positional/translational degrees of freedom (3DOFs),
however, a further three rotational DOFs (6DOFs) are required for many
potential biomechanical applications. Positional DOFs are insufficient to
analytically solve for joint rotational DOFs in a 3D human skeletal model.
Therefore, we propose a temporal inverse kinematics (IK) optimization technique
to infer joint orientations throughout a biomechanically informed, and
subject-specific kinematic chain. For this, we prescribe link directions from a
position-based 3D pose estimate. Sequential least squares quadratic programming
is used to solve a minimization problem that involves both frame-based pose
terms, and a temporal term. The solution space is constrained using joint DOFs,
and ranges of motion (ROMs). We generate 3D pose motion sequences to assess the
IK approach both for general accuracy, and accuracy in boundary cases. Our
temporal algorithm achieves 6DOF pose estimates with low Mean Per Joint Angular
Separation (MPJAS) errors (3.7{\deg}/joint overall, &amp; 1.6{\deg}/joint for lower
limbs). With frame-by-frame IK we obtain low errors in the case of bent elbows
and knees, however, motion sequences with phases of extended/straight limbs
results in ambiguity in twist angle. With temporal IK, we reduce ambiguity for
these poses, resulting in lower average errors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Domain Adaptation for Video Transformers in Action Recognition. (arXiv:2207.12842v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12842">
<div class="article-summary-box-inner">
<span><p>Over the last few years, Unsupervised Domain Adaptation (UDA) techniques have
acquired remarkable importance and popularity in computer vision. However, when
compared to the extensive literature available for images, the field of videos
is still relatively unexplored. On the other hand, the performance of a model
in action recognition is heavily affected by domain shift. In this paper, we
propose a simple and novel UDA approach for video action recognition. Our
approach leverages recent advances on spatio-temporal transformers to build a
robust source model that better generalises to the target domain. Furthermore,
our architecture learns domain invariant features thanks to the introduction of
a novel alignment loss term derived from the Information Bottleneck principle.
We report results on two video action recognition benchmarks for UDA, showing
state-of-the-art performance on HMDB$\leftrightarrow$UCF, as well as on
Kinetics$\rightarrow$NEC-Drone, which is more challenging. This demonstrates
the effectiveness of our method in handling different levels of domain shift.
The source code is available at https://github.com/vturrisi/UDAVT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bessel Equivariant Networks for Inversion of Transmission Effects in Multi-Mode Optical Fibres. (arXiv:2207.12849v1 [physics.optics])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12849">
<div class="article-summary-box-inner">
<span><p>We develop a new type of model for solving the task of inverting the
transmission effects of multi-mode optical fibres through the construction of
an $\mathrm{SO}^{+}(2,1)$-equivariant neural network. This model takes
advantage of the of the azimuthal correlations known to exist in fibre speckle
patterns and naturally accounts for the difference in spatial arrangement
between input and speckle patterns. In addition, we use a second
post-processing network to remove circular artifacts, fill gaps, and sharpen
the images, which is required due to the nature of optical fibre transmission.
This two stage approach allows for the inspection of the predicted images
produced by the more robust physically motivated equivariant model, which could
be useful in a safety-critical application, or by the output of both models,
which produces high quality images. Further, this model can scale to previously
unachievable resolutions of imaging with multi-mode optical fibres and is
demonstrated on $256 \times 256$ pixel images. This is a result of improving
the trainable parameter requirement from $\mathcal{O}(N^4)$ to
$\mathcal{O}(m)$, where $N$ is pixel size and $m$ is number of fibre modes.
Finally, this model generalises to new images, outside of the set of training
data classes, better than previous models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Smart City Security: Violence and Weaponized Violence Detection using DCNN. (arXiv:2207.12850v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12850">
<div class="article-summary-box-inner">
<span><p>In this ever connected society, CCTVs have had a pivotal role in enforcing
safety and security of the citizens by recording unlawful activities for the
authorities to take actions. In a smart city context, using Deep Convolutional
Neural Networks (DCNN) to detection violence and weaponized violence from CCTV
videos will provide an additional layer of security by ensuring real-time
detection around the clock. In this work, we introduced a new specialised
dataset by gathering real CCTV footage of both weaponized and non-weaponized
violence as well as non-violence videos from YouTube. We also proposed a novel
approach in merging consecutive video frames into a single salient image which
will then be the input to the DCNN. Results from multiple DCNN architectures
have proven the effectiveness of our method by having the highest accuracy of
99\%. We also take into consideration the efficiency of our methods through
several parameter trade-offs to ensure smart city sustainability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visually explaining 3D-CNN predictions for video classification with an adaptive occlusion sensitivity analysis. (arXiv:2207.12859v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12859">
<div class="article-summary-box-inner">
<span><p>This paper proposes a method for visually explaining the decision-making
process of 3D convolutional neural networks (CNN) with a temporal extension of
occlusion sensitivity analysis. The key idea here is to occlude a specific
volume of data by a 3D mask in an input 3D temporal-spatial data space and then
measure the change degree in the output score. The occluded volume data that
produces a larger change degree is regarded as a more critical element for
classification. However, while the occlusion sensitivity analysis is commonly
used to analyze single image classification, it is not so straightforward to
apply this idea to video classification as a simple fixed cuboid cannot deal
with the motions. To this end, we adapt the shape of a 3D occlusion mask to
complicated motions of target objects. Our flexible mask adaptation is
performed by considering the temporal continuity and spatial co-occurrence of
the optical flows extracted from the input video data. We further propose to
approximate our method by using the first-order partial derivative of the score
with respect to an input image to reduce its computational cost. We demonstrate
the effectiveness of our method through various and extensive comparisons with
the conventional methods in terms of the deletion/insertion metric and the
pointing metric on the UCF-101. The code is available at:
https://github.com/uchiyama33/AOSA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FRIB: Low-poisoning Rate Invisible Backdoor Attack based on Feature Repair. (arXiv:2207.12863v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12863">
<div class="article-summary-box-inner">
<span><p>During the generation of invisible backdoor attack poisoned data, the feature
space transformation operation tends to cause the loss of some poisoned
features and weakens the mapping relationship between source images with
triggers and target labels, resulting in the need for a higher poisoning rate
to achieve the corresponding backdoor attack success rate. To solve the above
problems, we propose the idea of feature repair for the first time and
introduce the blind watermark technique to repair the poisoned features lost
during the generation of poisoned data. Under the premise of ensuring
consistent labeling, we propose a low-poisoning rate invisible backdoor attack
based on feature repair, named FRIB. Benefiting from the above design concept,
the new method enhances the mapping relationship between the source images with
triggers and the target labels, and increases the degree of misleading DNNs,
thus achieving a high backdoor attack success rate with a very low poisoning
rate. Ultimately, the detailed experimental results show that the goal of
achieving a high success rate of backdoor attacks with a very low poisoning
rate is achieved on all MNIST, CIFAR10, GTSRB, and ImageNet datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Implementation Of Tiny Machine Learning Models On Arduino 33 BLE For Gesture And Speech Recognition. (arXiv:2207.12866v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12866">
<div class="article-summary-box-inner">
<span><p>In this article gesture recognition and speech recognition applications are
implemented on embedded systems with Tiny Machine Learning (TinyML). It
features 3-axis accelerometer, 3-axis gyroscope and 3-axis magnetometer. The
gesture recognition,provides an innovative approach nonverbal communication. It
has wide applications in human-computer interaction and sign language. Here in
the implementation of hand gesture recognition, TinyML model is trained and
deployed from EdgeImpulse framework for hand gesture recognition and based on
the hand movements, Arduino Nano 33 BLE device having 6-axis IMU can find out
the direction of movement of hand. The Speech is a mode of communication.
Speech recognition is a way by which the statements or commands of human speech
is understood by the computer which reacts accordingly. The main aim of speech
recognition is to achieve communication between man and machine. Here in the
implementation of speech recognition, TinyML model is trained and deployed from
EdgeImpulse framework for speech recognition and based on the keywords
pronounced by human, Arduino Nano 33 BLE device having built-in microphone can
make an RGB LED glow like red, green or blue based on keyword pronounced. The
results of each application are obtained and listed in the results section and
given the analysis upon the results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalized Probabilistic U-Net for medical image segementation. (arXiv:2207.12872v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12872">
<div class="article-summary-box-inner">
<span><p>We propose the Generalized Probabilistic U-Net, which extends the
Probabilistic U-Net by allowing more general forms of the Gaussian distribution
as the latent space distribution that can better approximate the uncertainty in
the reference segmentations. We study the effect the choice of latent space
distribution has on capturing the uncertainty in the reference segmentations
using the LIDC-IDRI dataset. We show that the choice of distribution affects
the sample diversity of the predictions and their overlap with respect to the
reference segmentations. For the LIDC-IDRI dataset, we show that using a
mixture of Gaussians results in a statistically significant improvement in the
generalized energy distance (GED) metric with respect to the standard
Probabilistic U-Net. We have made our implementation available at
https://github.com/ishaanb92/GeneralizedProbabilisticUNet
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detection of road traffic crashes based on collision estimation. (arXiv:2207.12886v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12886">
<div class="article-summary-box-inner">
<span><p>This paper introduces a framework based on computer vision that can detect
road traffic crashes (RCTs) by using the installed surveillance/CCTV camera and
report them to the emergency in real-time with the exact location and time of
occurrence of the accident. The framework is built of five modules. We start
with the detection of vehicles by using YOLO architecture; The second module is
the tracking of vehicles using MOSSE tracker, Then the third module is a new
approach to detect accidents based on collision estimation. Then the fourth
module for each vehicle, we detect if there is a car accident or not based on
the violent flow descriptor (ViF) followed by an SVM classifier for crash
prediction. Finally, in the last stage, if there is a car accident, the system
will send a notification to the emergency by using a GPS module that provides
us with the location, time, and date of the accident to be sent to the
emergency with the help of the GSM module. The main objective is to achieve
higher accuracy with fewer false alarms and to implement a simple system based
on pipelining technique.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LaKo: Knowledge-driven Visual Question Answering via Late Knowledge-to-Text Injection. (arXiv:2207.12888v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12888">
<div class="article-summary-box-inner">
<span><p>Visual question answering (VQA) often requires an understanding of visual
concepts and language semantics, which relies on external knowledge. Most
existing methods exploit pre-trained language models or/and unstructured text,
but the knowledge in these resources are often incomplete and noisy. Some
methods prefer to use knowledge graphs (KGs) which often have intensive
structured knowledge, but the research is still quite preliminary. In this
paper, we propose LaKo, a knowledge-driven VQA method via Late
Knowledge-to-text Injection. To effectively incorporate an external KG, we
transfer triples into text and propose a late injection mechanism. Finally we
address VQA as a text generation task with an effective encoder-decoder
paradigm. In the evaluation with OKVQA datasets, our method achieves
state-of-the-art results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Modality Image Registration using a Training-Time Privileged Third Modality. (arXiv:2207.12901v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12901">
<div class="article-summary-box-inner">
<span><p>In this work, we consider the task of pairwise cross-modality image
registration, which may benefit from exploiting additional images available
only at training time from an additional modality that is different to those
being registered. As an example, we focus on aligning intra-subject
multiparametric Magnetic Resonance (mpMR) images, between T2-weighted (T2w)
scans and diffusion-weighted scans with high b-value (DWI$_{high-b}$). For the
application of localising tumours in mpMR images, diffusion scans with zero
b-value (DWI$_{b=0}$) are considered easier to register to T2w due to the
availability of corresponding features. We propose a learning from privileged
modality algorithm, using a training-only imaging modality DWI$_{b=0}$, to
support the challenging multi-modality registration problems. We present
experimental results based on 369 sets of 3D multiparametric MRI images from
356 prostate cancer patients and report, with statistical significance, a
lowered median target registration error of 4.34 mm, when registering the
holdout DWI$_{high-b}$ and T2w image pairs, compared with that of 7.96 mm
before registration. Results also show that the proposed learning-based
registration networks enabled efficient registration with comparable or better
accuracy, compared with a classical iterative algorithm and other tested
learning-based methods with/without the additional modality. These compared
algorithms also failed to produce any significantly improved alignment between
DWI$_{high-b}$ and T2w in this challenging application.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AlignSDF: Pose-Aligned Signed Distance Fields for Hand-Object Reconstruction. (arXiv:2207.12909v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12909">
<div class="article-summary-box-inner">
<span><p>Recent work achieved impressive progress towards joint reconstruction of
hands and manipulated objects from monocular color images. Existing methods
focus on two alternative representations in terms of either parametric meshes
or signed distance fields (SDFs). On one side, parametric models can benefit
from prior knowledge at the cost of limited shape deformations and mesh
resolutions. Mesh models, hence, may fail to precisely reconstruct details such
as contact surfaces of hands and objects. SDF-based methods, on the other side,
can represent arbitrary details but are lacking explicit priors. In this work
we aim to improve SDF models using priors provided by parametric
representations. In particular, we propose a joint learning framework that
disentangles the pose and the shape. We obtain hand and object poses from
parametric models and use them to align SDFs in 3D space. We show that such
aligned SDFs better focus on reconstructing shape details and improve
reconstruction accuracy both for hands and objects. We evaluate our method and
demonstrate significant improvements over the state of the art on the
challenging ObMan and DexYCB benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Guide to Image and Video based Small Object Detection using Deep Learning : Case Study of Maritime Surveillance. (arXiv:2207.12926v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12926">
<div class="article-summary-box-inner">
<span><p>Small object detection (SOD) in optical images and videos is a challenging
problem that even state-of-the-art generic object detection methods fail to
accurately localize and identify such objects. Typically, small objects appear
in real-world due to large camera-object distance. Because small objects occupy
only a small area in the input image (e.g., less than 10%), the information
extracted from such a small area is not always rich enough to support decision
making. Multidisciplinary strategies are being developed by researchers working
at the interface of deep learning and computer vision to enhance the
performance of SOD deep learning based methods. In this paper, we provide a
comprehensive review of over 160 research papers published between 2017 and
2022 in order to survey this growing subject. This paper summarizes the
existing literature and provide a taxonomy that illustrates the broad picture
of current research. We investigate how to improve the performance of small
object detection in maritime environments, where increasing performance is
critical. By establishing a connection between generic and maritime SOD
research, future directions have been identified. In addition, the popular
datasets that have been used for SOD for generic and maritime applications are
discussed, and also well-known evaluation metrics for the state-of-the-art
methods on some of the datasets are provided.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Reliable Online Method for Joint Estimation of Focal Length and Camera Rotation. (arXiv:2207.12934v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12934">
<div class="article-summary-box-inner">
<span><p>Linear perspectivecues deriving from regularities of the built environment
can be used to recalibrate both intrinsic and extrinsic camera parameters
online, but these estimates can be unreliable due to irregularities in the
scene, uncertainties in line segment estimation and background clutter. Here we
address this challenge through four initiatives. First, we use the PanoContext
panoramic image dataset [27] to curate a novel and realistic dataset of planar
projections over a broad range of scenes, focal lengths and camera poses.
Second, we use this novel dataset and the YorkUrbanDB [4] to systematically
evaluate the linear perspective deviation measures frequently found in the
literature and show that the choice of deviation measure and likelihood model
has a huge impact on reliability. Third, we use these findings to create a
novel system for online camera calibration we call fR, and show that it
outperforms the prior state of the art, substantially reducing error in
estimated camera rotation and focal length. Our fourth contribution is a novel
and efficient approach to estimating uncertainty that can dramatically improve
online reliability for performance-critical applications by strategically
selecting which frames to use for recalibration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Segmentation for Autonomous Driving: Model Evaluation, Dataset Generation, Perspective Comparison, and Real-Time Capability. (arXiv:2207.12939v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12939">
<div class="article-summary-box-inner">
<span><p>Environmental perception is an important aspect within the field of
autonomous vehicles that provides crucial information about the driving domain,
including but not limited to identifying clear driving areas and surrounding
obstacles. Semantic segmentation is a widely used perception method for
self-driving cars that associates each pixel of an image with a predefined
class. In this context, several segmentation models are evaluated regarding
accuracy and efficiency. Experimental results on the generated dataset confirm
that the segmentation model FasterSeg is fast enough to be used in realtime on
lowpower computational (embedded) devices in self-driving cars. A simple method
is also introduced to generate synthetic training data for the model. Moreover,
the accuracy of the first-person perspective and the bird's eye view
perspective are compared. For a $320 \times 256$ input in the first-person
perspective, FasterSeg achieves $65.44\,\%$ mean Intersection over Union
(mIoU), and for a $320 \times 256$ input from the bird's eye view perspective,
FasterSeg achieves $64.08\,\%$ mIoU. Both perspectives achieve a frame rate of
$247.11$ Frames per Second (FPS) on the NVIDIA Jetson AGX Xavier. Lastly, the
frame rate and the accuracy with respect to the arithmetic 16-bit Floating
Point (FP16) and 32-bit Floating Point (FP32) of both perspectives are measured
and compared on the target hardware.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Generalizable Latent Representations for Novel Degradations in Super Resolution. (arXiv:2207.12941v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12941">
<div class="article-summary-box-inner">
<span><p>Typical methods for blind image super-resolution (SR) focus on dealing with
unknown degradations by directly estimating them or learning the degradation
representations in a latent space. A potential limitation of these methods is
that they assume the unknown degradations can be simulated by the integration
of various handcrafted degradations (e.g., bicubic downsampling), which is not
necessarily true. The real-world degradations can be beyond the simulation
scope by the handcrafted degradations, which are referred to as novel
degradations. In this work, we propose to learn a latent representation space
for degradations, which can be generalized from handcrafted (base) degradations
to novel degradations. The obtained representations for a novel degradation in
this latent space are then leveraged to generate degraded images consistent
with the novel degradation to compose paired training data for SR model.
Furthermore, we perform variational inference to match the posterior of
degradations in latent representation space with a prior distribution (e.g.,
Gaussian distribution). Consequently, we are able to sample more high-quality
representations for a novel degradation to augment the training data for SR
model. We conduct extensive experiments on both synthetic and real-world
datasets to validate the effectiveness and advantages of our method for blind
super-resolution with novel degradations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AMF: Adaptable Weighting Fusion with Multiple Fine-tuning for Image Classification. (arXiv:2207.12944v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12944">
<div class="article-summary-box-inner">
<span><p>Fine-tuning is widely applied in image classification tasks as a transfer
learning approach. It re-uses the knowledge from a source task to learn and
obtain a high performance in target tasks. Fine-tuning is able to alleviate the
challenge of insufficient training data and expensive labelling of new data.
However, standard fine-tuning has limited performance in complex data
distributions. To address this issue, we propose the Adaptable Multi-tuning
method, which adaptively determines each data sample's fine-tuning strategy. In
this framework, multiple fine-tuning settings and one policy network are
defined. The policy network in Adaptable Multi-tuning can dynamically adjust to
an optimal weighting to feed different samples into models that are trained
using different fine-tuning strategies. Our method outperforms the standard
fine-tuning approach by 1.69%, 2.79% on the datasets FGVC-Aircraft, and
Describable Texture, yielding comparable performance on the datasets Stanford
Cars, CIFAR-10, and Fashion-MNIST.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextual Text Block Detection towards Scene Text Understanding. (arXiv:2207.12955v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12955">
<div class="article-summary-box-inner">
<span><p>Most existing scene text detectors focus on detecting characters or words
that only capture partial text messages due to missing contextual information.
For a better understanding of text in scenes, it is more desired to detect
contextual text blocks (CTBs) which consist of one or multiple integral text
units (e.g., characters, words, or phrases) in natural reading order and
transmit certain complete text messages. This paper presents contextual text
detection, a new setup that detects CTBs for better understanding of texts in
scenes. We formulate the new setup by a dual detection task which first detects
integral text units and then groups them into a CTB. To this end, we design a
novel scene text clustering technique that treats integral text units as tokens
and groups them (belonging to the same CTB) into an ordered token sequence. In
addition, we create two datasets SCUT-CTW-Context and ReCTS-Context to
facilitate future research, where each CTB is well annotated by an ordered
sequence of integral text units. Further, we introduce three metrics that
measure contextual text detection in local accuracy, continuity, and global
accuracy. Extensive experiments show that our method accurately detects CTBs
which effectively facilitates downstream tasks such as text classification and
translation. The project is available at
https://sg-vilab.github.io/publication/xue2022contextual/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incremental Few-Shot Semantic Segmentation via Embedding Adaptive-Update and Hyper-class Representation. (arXiv:2207.12964v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12964">
<div class="article-summary-box-inner">
<span><p>Incremental few-shot semantic segmentation (IFSS) targets at incrementally
expanding model's capacity to segment new class of images supervised by only a
few samples. However, features learned on old classes could significantly
drift, causing catastrophic forgetting. Moreover, few samples for pixel-level
segmentation on new classes lead to notorious overfitting issues in each
learning session. In this paper, we explicitly represent class-based knowledge
for semantic segmentation as a category embedding and a hyper-class embedding,
where the former describes exclusive semantical properties, and the latter
expresses hyper-class knowledge as class-shared semantic properties. Aiming to
solve IFSS problems, we present EHNet, i.e., Embedding adaptive-update and
Hyper-class representation Network from two aspects. First, we propose an
embedding adaptive-update strategy to avoid feature drift, which maintains old
knowledge by hyper-class representation, and adaptively update category
embeddings with a class-attention scheme to involve new classes learned in
individual sessions. Second, to resist overfitting issues caused by few
training samples, a hyper-class embedding is learned by clustering all category
embeddings for initialization and aligned with category embedding of the new
class for enhancement, where learned knowledge assists to learn new knowledge,
thus alleviating performance dependence on training data scale. Significantly,
these two designs provide representation capability for classes with sufficient
semantics and limited biases, enabling to perform segmentation tasks requiring
high semantic dependence. Experiments on PASCAL-5i and COCO datasets show that
EHNet achieves new state-of-the-art performance with remarkable advantages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nondestructive Quality Control in Powder Metallurgy using Hyperspectral Imaging. (arXiv:2207.12966v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12966">
<div class="article-summary-box-inner">
<span><p>Measuring the purity in the metal powder is critical for preserving the
quality of additive manufacturing products. Contamination is one of the most
headache problems which can be caused by multiple reasons and lead to the
as-built components cracking and malfunctions. Existing methods for
metallurgical condition assessment are mostly time-consuming and mainly focus
on the physical integrity of structure rather than material composition.
Through capturing spectral data from a wide frequency range along with the
spatial information, hyperspectral imaging (HSI) can detect minor differences
in terms of temperature, moisture and chemical composition. Therefore, HSI can
provide a unique way to tackle this challenge. In this paper, with the use of a
near-infrared HSI camera, applications of HSI for the non-destructive
inspection of metal powders are introduced. Technical assumptions and solutions
on three step-by-step case studies are presented in detail, including powder
characterization, contamination detection, and band selection analysis.
Experimental results have fully demonstrated the great potential of HSI and
related AI techniques for NDT of powder metallurgy, especially the potential to
satisfy the industrial manufacturing environment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransFiner: A Full-Scale Refinement Approach for Multiple Object Tracking. (arXiv:2207.12967v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12967">
<div class="article-summary-box-inner">
<span><p>Multiple object tracking (MOT) is the task containing detection and
association. Plenty of trackers have achieved competitive performance.
Unfortunately, for the lack of informative exchange on these subtasks, they are
often biased toward one of the two and remain underperforming in complex
scenarios, such as the expected false negatives and mistaken trajectories of
targets when passing each other. In this paper, we propose TransFiner, a
transformer-based post-refinement approach for MOT. It is a generic attachment
framework that leverages the images and tracking results (locations and class
predictions) from the original tracker as inputs, which are then used to launch
TransFiner powerfully. Moreover, TransFiner depends on query pairs, which
produce pairs of detection and motion through the fusion decoder and achieve
comprehensive tracking improvement. We also provide targeted refinement by
labeling query pairs according to different refinement levels. Experiments show
that our design is effective, on the MOT17 benchmark, we elevate the
CenterTrack from 67.8% MOTA and 64.7% IDF1 to 71.5% MOTA and 66.8% IDF1.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tracking Every Thing in the Wild. (arXiv:2207.12978v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12978">
<div class="article-summary-box-inner">
<span><p>Current multi-category Multiple Object Tracking (MOT) metrics use class
labels to group tracking results for per-class evaluation. Similarly, MOT
methods typically only associate objects with the same class predictions. These
two prevalent strategies in MOT implicitly assume that the classification
performance is near-perfect. However, this is far from the case in recent
large-scale MOT datasets, which contain large numbers of classes with many rare
or semantically similar categories. Therefore, the resulting inaccurate
classification leads to sub-optimal tracking and inadequate benchmarking of
trackers. We address these issues by disentangling classification from
tracking. We introduce a new metric, Track Every Thing Accuracy (TETA),
breaking tracking measurement into three sub-factors: localization,
association, and classification, allowing comprehensive benchmarking of
tracking performance even under inaccurate classification. TETA also deals with
the challenging incomplete annotation problem in large-scale tracking datasets.
We further introduce a Track Every Thing tracker (TETer), that performs
association using Class Exemplar Matching (CEM). Our experiments show that TETA
evaluates trackers more comprehensively, and TETer achieves significant
improvements on the challenging large-scale datasets BDD100K and TAO compared
to the state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient One Pass Self-distillation with Zipf's Label Smoothing. (arXiv:2207.12980v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12980">
<div class="article-summary-box-inner">
<span><p>Self-distillation exploits non-uniform soft supervision from itself during
training and improves performance without any runtime cost. However, the
overhead during training is often overlooked, and yet reducing time and memory
overhead during training is increasingly important in the giant models' era.
This paper proposes an efficient self-distillation method named Zipf's Label
Smoothing (Zipf's LS), which uses the on-the-fly prediction of a network to
generate soft supervision that conforms to Zipf distribution without using any
contrastive samples or auxiliary parameters. Our idea comes from an empirical
observation that when the network is duly trained the output values of a
network's final softmax layer, after sorting by the magnitude and averaged
across samples, should follow a distribution reminiscent to Zipf's Law in the
word frequency statistics of natural languages. By enforcing this property on
the sample level and throughout the whole training period, we find that the
prediction accuracy can be greatly improved. Using ResNet50 on the INAT21
fine-grained classification dataset, our technique achieves +3.61% accuracy
gain compared to the vanilla baseline, and 0.88% more gain against the previous
label smoothing or self-distillation strategies. The implementation is publicly
available at https://github.com/megvii-research/zipfls.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explaining Deep Neural Networks for Point Clouds using Gradient-based Visualisations. (arXiv:2207.12984v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12984">
<div class="article-summary-box-inner">
<span><p>Explaining decisions made by deep neural networks is a rapidly advancing
research topic. In recent years, several approaches have attempted to provide
visual explanations of decisions made by neural networks designed for
structured 2D image input data. In this paper, we propose a novel approach to
generate coarse visual explanations of networks designed to classify
unstructured 3D data, namely point clouds. Our method uses gradients flowing
back to the final feature map layers and maps these values as contributions of
the corresponding points in the input point cloud. Due to dimensionality
disagreement and lack of spatial consistency between input points and final
feature maps, our approach combines gradients with points dropping to compute
explanations of different parts of the point cloud iteratively. The generality
of our approach is tested on various point cloud classification networks,
including 'single object' networks PointNet, PointNet++, DGCNN, and a 'scene'
network VoteNet. Our method generates symmetric explanation maps that highlight
important regions and provide insight into the decision-making process of
network architectures. We perform an exhaustive evaluation of trust and
interpretability of our explanation method against comparative approaches using
quantitative, quantitative and human studies. All our code is implemented in
PyTorch and will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Monocular 3D Object Detection with Depth from Motion. (arXiv:2207.12988v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12988">
<div class="article-summary-box-inner">
<span><p>Perceiving 3D objects from monocular inputs is crucial for robotic systems,
given its economy compared to multi-sensor settings. It is notably difficult as
a single image can not provide any clues for predicting absolute depth values.
Motivated by binocular methods for 3D object detection, we take advantage of
the strong geometry structure provided by camera ego-motion for accurate object
depth estimation and detection. We first make a theoretical analysis on this
general two-view case and notice two challenges: 1) Cumulative errors from
multiple estimations that make the direct prediction intractable; 2) Inherent
dilemmas caused by static cameras and matching ambiguity. Accordingly, we
establish the stereo correspondence with a geometry-aware cost volume as the
alternative for depth estimation and further compensate it with monocular
understanding to address the second problem. Our framework, named Depth from
Motion (DfM), then uses the established geometry to lift 2D image features to
the 3D space and detects 3D objects thereon. We also present a pose-free DfM to
make it usable when the camera pose is unavailable. Our framework outperforms
state-of-the-art methods by a large margin on the KITTI benchmark. Detailed
quantitative and qualitative analyses also validate our theoretical
conclusions. The code will be released at
https://github.com/Tai-Wang/Depth-from-Motion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">V$^2$L: Leveraging Vision and Vision-language Models into Large-scale Product Retrieval. (arXiv:2207.12994v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12994">
<div class="article-summary-box-inner">
<span><p>Product retrieval is of great importance in the ecommerce domain. This paper
introduces our 1st-place solution in eBay eProduct Visual Search Challenge
(FGVC9), which is featured for an ensemble of about 20 models from vision
models and vision-language models. While model ensemble is common, we show that
combining the vision models and vision-language models brings particular
benefits from their complementarity and is a key factor to our superiority.
Specifically, for the vision models, we use a two-stage training pipeline which
first learns from the coarse labels provided in the training set and then
conducts fine-grained self-supervised training, yielding a coarse-to-fine
metric learning manner. For the vision-language models, we use the textual
description of the training image as the supervision signals for fine-tuning
the image-encoder (feature extractor). With these designs, our solution
achieves 0.7623 MAR@10, ranking the first place among all the competitors. The
code is available at: \href{https://github.com/WangWenhao0716/V2L}{V$^2$L}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust and Efficient Segmentation of Cross-domain Medical Images. (arXiv:2207.12995v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12995">
<div class="article-summary-box-inner">
<span><p>Efficient medical image segmentation aims to provide accurate pixel-wise
prediction for the medical images with the lightweight implementation
framework. However, lightweight frameworks generally fail to achieve high
performance, and suffer from the poor generalizable ability on cross-domain
tasks.In this paper, we propose a generalizable knowledge distillation method
for robust and efficient segmentation of cross-domain medical images.
Primarily, we propose the Model-Specific Alignment Networks (MSAN) to provide
the domain-invariant representations which are regularized by a Pre-trained
Semantic AutoEncoder (P-SAE). Meanwhile, a customized Alignment Consistency
Training (ACT) strategy is designed to promote the MSAN training. With the
domain-invariant representative vectors in MSAN, we propose two generalizable
knowledge distillation schemes, Dual Contrastive Graph Distillation (DCGD) and
Domain-Invariant Cross Distillation (DICD). Specifically, in DCGD, two types of
implicit contrastive graphs are designed to represent the intra-coupling and
inter-coupling semantic correlations from the perspective of data distribution.
In DICD, the domain-invariant semantic vectors from the two models (i.e.,
teacher and student) are leveraged to cross-reconstruct features by the header
exchange of MSAN, which achieves generalizable improvement for both the encoder
and decoder in the student model. Furthermore, a metric named Fr\'echet
Semantic Distance (FSD) is tailored to verify the effectiveness of the
regularized domain-invariant features. Extensive experiments conducted on the
Liver and Retinal Vessel Segmentation datasets demonstrate the priority of our
method, in terms of performance and generalization on lightweight frameworks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topological Optimized Convolutional Visual Recurrent Network for Brain Tumor Segmentation and Classification. (arXiv:2207.13021v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13021">
<div class="article-summary-box-inner">
<span><p>In today's world of health care, brain tumor (BT) detection has become a
common occurrence. However, the manual BT classification approach is
time-consuming and only available at a few diagnostic centres. So Deep
Convolutional Neural Network (DCNN) is introduced in the medical field for
making accurate diagnoses and aiding in the patient's treatment before surgery.
But these networks have problems such as overfitting and being unable to
extract necessary features for classification. To overcome these problems, we
developed the TDA-IPH and Convolutional Transfer learning and Visual Recurrent
learning with Elephant Herding Optimization hyper-parameter tuning (CTVR-EHO)
models for BT segmentation and classification. Initially, the Topological Data
Analysis based Improved Persistent Homology (TDA-IPH) is designed to segment
the BT image. Then, from the segmented image, features are extracted
simultaneously using TL via the AlexNet model and Bidirectional Visual Long
Short Term Memory (Bi-VLSTM). Elephant Herding Optimization (EHO) is used to
tune the hyper parameters of both networks to get an optimal result. Finally,
extracted features are concatenated and classified using the softmax activation
layer. The simulation result of this proposed CTVR-EHO and TDA-IPH method is
analysed based on some metrics such as precision, accuracy, recall, loss, and F
score. When compared to other existing BT segmentation and classification
models, the proposed CTVR-EHO and TDA-IPH approaches show high accuracy
(99.8%), high recall (99.23%), high precision (99.67%), and high F score
(99.59%).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pseudo-Pair based Self-Similarity Learning for Unsupervised Person Re-identification. (arXiv:2207.13035v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13035">
<div class="article-summary-box-inner">
<span><p>Person re-identification (re-ID) is of great importance to video surveillance
systems by estimating the similarity between a pair of cross-camera person
shorts. Current methods for estimating such similarity require a large number
of labeled samples for supervised training. In this paper, we present a
pseudo-pair based self-similarity learning approach for unsupervised person
re-ID without human annotations. Unlike conventional unsupervised re-ID methods
that use pseudo labels based on global clustering, we construct patch surrogate
classes as initial supervision, and propose to assign pseudo labels to images
through the pairwise gradient-guided similarity separation. This can cluster
images in pseudo pairs, and the pseudos can be updated during training. Based
on pseudo pairs, we propose to improve the generalization of similarity
function via a novel self-similarity learning:it learns local discriminative
features from individual images via intra-similarity, and discovers the patch
correspondence across images via inter-similarity. The intra-similarity
learning is based on channel attention to detect diverse local features from an
image. The inter-similarity learning employs a deformable convolution with a
non-local block to align patches for cross-image similarity. Experimental
results on several re-ID benchmark datasets demonstrate the superiority of the
proposed method over the state-of-the-arts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Resolution-Adaptive Representations for Cross-Resolution Person Re-Identification. (arXiv:2207.13037v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13037">
<div class="article-summary-box-inner">
<span><p>The cross-resolution person re-identification (CRReID) problem aims to match
low-resolution (LR) query identity images against high resolution (HR) gallery
images. It is a challenging and practical problem since the query images often
suffer from resolution degradation due to the different capturing conditions
from real-world cameras. To address this problem, state-of-the-art (SOTA)
solutions either learn the resolution-invariant representation or adopt
super-resolution (SR) module to recover the missing information from the LR
query. This paper explores an alternative SR-free paradigm to directly compare
HR and LR images via a dynamic metric, which is adaptive to the resolution of a
query image. We realize this idea by learning resolution-adaptive
representations for cross-resolution comparison. Specifically, we propose two
resolution-adaptive mechanisms. The first one disentangles the
resolution-specific information into different sub-vectors in the penultimate
layer of the deep neural networks, and thus creates a varying-length
representation. To better extract resolution-dependent information, we further
propose to learn resolution-adaptive masks for intermediate residual feature
blocks. A novel progressive learning strategy is proposed to train those masks
properly. These two mechanisms are combined to boost the performance of CRReID.
Experimental results show that the proposed method is superior to existing
approaches and achieves SOTA performance on multiple CRReID benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text-Guided Synthesis of Artistic Images with Retrieval-Augmented Diffusion Models. (arXiv:2207.13038v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13038">
<div class="article-summary-box-inner">
<span><p>Novel architectures have recently improved generative image synthesis leading
to excellent visual quality in various tasks. Of particular note is the field
of ``AI-Art'', which has seen unprecedented growth with the emergence of
powerful multimodal models such as CLIP. By combining speech and image
synthesis models, so-called ``prompt-engineering'' has become established, in
which carefully selected and composed sentences are used to achieve a certain
visual style in the synthesized image. In this note, we present an alternative
approach based on retrieval-augmented diffusion models (RDMs). In RDMs, a set
of nearest neighbors is retrieved from an external database during training for
each training instance, and the diffusion model is conditioned on these
informative samples. During inference (sampling), we replace the retrieval
database with a more specialized database that contains, for example, only
images of a particular visual style. This provides a novel way to prompt a
general trained model after training and thereby specify a particular visual
style. As shown by our experiments, this approach is superior to specifying the
visual style within the text prompt. We open-source code and model weights at
https://github.com/CompVis/latent-diffusion .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient High-Resolution Deep Learning: A Survey. (arXiv:2207.13050v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13050">
<div class="article-summary-box-inner">
<span><p>Cameras in modern devices such as smartphones, satellites and medical
equipment are capable of capturing very high resolution images and videos. Such
high-resolution data often need to be processed by deep learning models for
cancer detection, automated road navigation, weather prediction, surveillance,
optimizing agricultural processes and many other applications. Using
high-resolution images and videos as direct inputs for deep learning models
creates many challenges due to their high number of parameters, computation
cost, inference latency and GPU memory consumption. Simple approaches such as
resizing the images to a lower resolution are common in the literature,
however, they typically significantly decrease accuracy. Several works in the
literature propose better alternatives in order to deal with the challenges of
high-resolution data and improve accuracy and speed while complying with
hardware limitations and time restrictions. This survey describes such
efficient high-resolution deep learning methods, summarizes real-world
applications of high-resolution deep learning, and provides comprehensive
information about available high-resolution datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NewsStories: Illustrating articles with visual summaries. (arXiv:2207.13061v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13061">
<div class="article-summary-box-inner">
<span><p>Recent self-supervised approaches have used large-scale image-text datasets
to learn powerful representations that transfer to many tasks without
finetuning. These methods often assume that there is one-to-one correspondence
between its images and their (short) captions. However, many tasks require
reasoning about multiple images and long text narratives, such as describing
news articles with visual summaries. Thus, we explore a novel setting where the
goal is to learn a self-supervised visual-language representation that is
robust to varying text length and the number of images. In addition, unlike
prior work which assumed captions have a literal relation to the image, we
assume images only contain loose illustrative correspondence with the text. To
explore this problem, we introduce a large-scale multimodal dataset containing
over 31M articles, 22M images and 1M videos. We show that state-of-the-art
image-text alignment methods are not robust to longer narratives with multiple
images. Finally, we introduce an intuitive baseline that outperforms these
methods on zero-shot image-set retrieval by 10% on the GoodNews dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Manipulations Beyond Faces: A Dataset with Human-Machine Analysis. (arXiv:2207.13064v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13064">
<div class="article-summary-box-inner">
<span><p>As tools for content editing mature, and artificial intelligence (AI) based
algorithms for synthesizing media grow, the presence of manipulated content
across online media is increasing. This phenomenon causes the spread of
misinformation, creating a greater need to distinguish between "real'' and
"manipulated'' content. To this end, we present VideoSham, a dataset consisting
of 826 videos (413 real and 413 manipulated). Many of the existing deepfake
datasets focus exclusively on two types of facial manipulations -- swapping
with a different subject's face or altering the existing face. VideoSham, on
the other hand, contains more diverse, context-rich, and human-centric,
high-resolution videos manipulated using a combination of 6 different spatial
and temporal attacks. Our analysis shows that state-of-the-art manipulation
detection algorithms only work for a few specific attacks and do not scale well
on VideoSham. We performed a user study on Amazon Mechanical Turk with 1200
participants to understand if they can differentiate between the real and
manipulated videos in VideoSham. Finally, we dig deeper into the strengths and
weaknesses of performances by humans and SOTA-algorithms to identify gaps that
need to be filled with better AI algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeFakePro: Decentralized DeepFake Attacks Detection using ENF Authentication. (arXiv:2207.13070v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13070">
<div class="article-summary-box-inner">
<span><p>Advancements in generative models, like Deepfake allows users to imitate a
targeted person and manipulate online interactions. It has been recognized that
disinformation may cause disturbance in society and ruin the foundation of
trust. This article presents DeFakePro, a decentralized consensus
mechanism-based Deepfake detection technique in online video conferencing
tools. Leveraging Electrical Network Frequency (ENF), an environmental
fingerprint embedded in digital media recording, affords a consensus mechanism
design called Proof-of-ENF (PoENF) algorithm. The similarity in ENF signal
fluctuations is utilized in the PoENF algorithm to authenticate the media
broadcasted in conferencing tools. By utilizing the video conferencing setup
with malicious participants to broadcast deep fake video recordings to other
participants, the DeFakePro system verifies the authenticity of the incoming
media in both audio and video channels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DETRs with Hybrid Matching. (arXiv:2207.13080v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13080">
<div class="article-summary-box-inner">
<span><p>One-to-one set matching is a key design for DETR to establish its end-to-end
capability, so that object detection does not require a hand-crafted NMS
(non-maximum suppression) method to remove duplicate detections. This
end-to-end signature is important for the versatility of DETR, and it has been
generalized to a wide range of visual problems, including instance/semantic
segmentation, human pose estimation, and point cloud/multi-view-images based
detection, etc. However, we note that because there are too few queries
assigned as positive samples, the one-to-one set matching significantly reduces
the training efficiency of positive samples. This paper proposes a simple yet
effective method based on a hybrid matching scheme that combines the original
one-to-one matching branch with auxiliary queries that use one-to-many matching
loss during training. This hybrid strategy has been shown to significantly
improve training efficiency and improve accuracy. In inference, only the
original one-to-one match branch is used, thus maintaining the end-to-end merit
and the same inference efficiency of DETR. The method is named
$\mathcal{H}$-DETR, and it shows that a wide range of representative DETR
methods can be consistently improved across a wide range of visual tasks,
including Deformable-DETR, 3DETR/PETRv2, PETR, and TransTrack, among others.
Code will be available at: https://github.com/HDETR
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task Agnostic and Post-hoc Unseen Distribution Detection. (arXiv:2207.13083v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13083">
<div class="article-summary-box-inner">
<span><p>Despite the recent advances in out-of-distribution(OOD) detection, anomaly
detection, and uncertainty estimation tasks, there do not exist a task-agnostic
and post-hoc approach. To address this limitation, we design a novel
clustering-based ensembling method, called Task Agnostic and Post-hoc Unseen
Distribution Detection (TAPUDD) that utilizes the features extracted from the
model trained on a specific task. Explicitly, it comprises of TAP-Mahalanobis,
which clusters the training datasets' features and determines the minimum
Mahalanobis distance of the test sample from all clusters. Further, we propose
the Ensembling module that aggregates the computation of iterative
TAP-Mahalanobis for a different number of clusters to provide reliable and
efficient cluster computation. Through extensive experiments on synthetic and
real-world datasets, we observe that our approach can detect unseen samples
effectively across diverse tasks and performs better or on-par with the
existing baselines. To this end, we eliminate the necessity of determining the
optimal value of the number of clusters and demonstrate that our method is more
viable for large-scale classification tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Group DETR: Fast Training Convergence with Decoupled One-to-Many Label Assignment. (arXiv:2207.13085v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.13085">
<div class="article-summary-box-inner">
<span><p>Detection Transformer (DETR) relies on One-to-One label assignment, i.e.,
assigning one ground-truth (gt) object to only one positive object query, for
end-to-end object detection and lacks the capability of exploiting multiple
positive queries. We present a novel DETR training approach, named {\em Group
DETR}, to support multiple positive queries. To be specific, we decouple the
positives into multiple independent groups and keep only one positive per gt
object in each group. We make simple modifications during training: (i) adopt
$K$ groups of object queries; (ii) conduct decoder self-attention on each group
of object queries with the same parameters; (iii) perform One-to-One label
assignment for each group, leading to $K$ positive object queries for each gt
object. In inference, we only use one group of object queries, making no
modifications to both architecture and processes. We validate the effectiveness
of the proposed approach on DETR variants, including Conditional DETR,
DAB-DETR, DN-DETR, and DINO.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What you get is not always what you see: pitfalls in solar array assessment using overhead imagery. (arXiv:1902.10895v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1902.10895">
<div class="article-summary-box-inner">
<span><p>Effective integration planning for small, distributed solar photovoltaic (PV)
arrays into electric power grids requires access to high quality data: the
location and power capacity of individual solar PV arrays. Unfortunately,
national databases of small-scale solar PV do not exist; those that do are
limited in their spatial resolution, typically aggregated up to state or
national levels. While several promising approaches for solar PV detection have
been published, strategies for evaluating the performance of these models are
often highly heterogeneous from study to study. The resulting comparison of
these methods for practical applications for energy assessments becomes
challenging and may imply that the reported performance evaluations are overly
optimistic. The heterogeneity comes in many forms, each of which we explore in
this work: the level of spatial aggregation, the validation of ground truth,
inconsistencies in the training and validation datasets, and the degree of
diversity of the locations and sensors from which the training and validation
data originate. For each, we discuss emerging practices from the literature to
address them or suggest directions of future research. As part of our
investigation, we evaluate solar PV identification performance in two large
regions. Our findings suggest that traditional performance evaluation of the
automated identification of solar PV from satellite imagery may be optimistic
due to common limitations in the validation process. The takeaways from this
work are intended to inform and catalyze the large-scale practical application
of automated solar PV assessment techniques by energy researchers and
professionals.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse Signal Models for Data Augmentation in Deep Learning ATR. (arXiv:2012.09284v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.09284">
<div class="article-summary-box-inner">
<span><p>Automatic Target Recognition (ATR) algorithms classify a given Synthetic
Aperture Radar (SAR) image into one of the known target classes using a set of
training images available for each class. Recently, learning methods have shown
to achieve state-of-the-art classification accuracy if abundant training data
is available, sampled uniformly over the classes, and their poses. In this
paper, we consider the task of ATR with a limited set of training images. We
propose a data augmentation approach to incorporate domain knowledge and
improve the generalization power of a data-intensive learning algorithm, such
as a Convolutional neural network (CNN). The proposed data augmentation method
employs a limited persistence sparse modeling approach, capitalizing on
commonly observed characteristics of wide-angle synthetic aperture radar (SAR)
imagery. Specifically, we exploit the sparsity of the scattering centers in the
spatial domain and the smoothly-varying structure of the scattering
coefficients in the azimuthal domain to solve the ill-posed problem of
over-parametrized model fitting. Using this estimated model, we synthesize new
images at poses and sub-pixel translations not available in the given data to
augment CNN's training data. The experimental results show that for the
training data starved region, the proposed method provides a significant gain
in the resulting ATR algorithm's generalization performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Object Detection via Probabilistic Ensembling. (arXiv:2104.02904v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.02904">
<div class="article-summary-box-inner">
<span><p>Object detection with multimodal inputs can improve many safety-critical
systems such as autonomous vehicles (AVs). Motivated by AVs that operate in
both day and night, we study multimodal object detection with RGB and thermal
cameras, since the latter provides much stronger object signatures under poor
illumination. We explore strategies for fusing information from different
modalities. Our key contribution is a probabilistic ensembling technique,
ProbEn, a simple non-learned method that fuses together detections from
multi-modalities. We derive ProbEn from Bayes' rule and first principles that
assume conditional independence across modalities. Through probabilistic
marginalization, ProbEn elegantly handles missing modalities when detectors do
not fire on the same object. Importantly, ProbEn also notably improves
multimodal detection even when the conditional independence assumption does not
hold, e.g., fusing outputs from other fusion methods (both off-the-shelf and
trained in-house). We validate ProbEn on two benchmarks containing both aligned
(KAIST) and unaligned (FLIR) multimodal images, showing that ProbEn outperforms
prior work by more than 13% in relative performance!
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Attraction and Contrastive Repulsion for Representation Learning. (arXiv:2105.03746v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.03746">
<div class="article-summary-box-inner">
<span><p>Contrastive learning (CL) methods effectively learn data representations
without label supervision, where the encoder contrasts each positive sample
over multiple negative samples via a one-vs-many softmax cross-entropy loss. By
leveraging large amounts of unlabeled image data, recent CL methods have
achieved promising results when pre-trained on ImageNet, a well-curated data
set with balanced image classes. However, they tend to yield worse performance
when pre-trained on images in the wild. In this paper, to further improve the
performance of CL and enhance its robustness on uncurated data sets, we propose
a doubly CL strategy that contrasts the positive (negative) samples of a query
within themselves before deciding how strongly to pull (push) them. We realize
this strategy with contrastive attraction and contrastive repulsion (CACR),
which makes the query not only exert a greater force to attract more distant
positive samples but also do so to repel closer negative samples. Theoretical
analysis reveals that CACR generalizes CL's behavior by taking into
consideration the differences between the distributions of the
positive/negative samples, which in general are sampled independently of the
query, and their true conditional distributions given the query. We demonstrate
this unique intra-positive attraction and intra-negative repulsion mechanism,
which helps remove the need to assume uniform prior distributions on both the
data and their latent representation, is particularly beneficial when data sets
are less curated. Extensive large-scale experiments on a number of standard
vision tasks show that CACR not only consistently outperforms existing CL
methods on benchmark data sets in representation learning, but also shows
better robustness when generalized to pre-training on imbalanced image data
sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Matching Visual Features to Hierarchical Semantic Topics for Image Paragraph Captioning. (arXiv:2105.04143v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.04143">
<div class="article-summary-box-inner">
<span><p>Observing a set of images and their corresponding paragraph-captions, a
challenging task is to learn how to produce a semantically coherent paragraph
to describe the visual content of an image. Inspired by recent successes in
integrating semantic topics into this task, this paper develops a plug-and-play
hierarchical-topic-guided image paragraph generation framework, which couples a
visual extractor with a deep topic model to guide the learning of a language
model. To capture the correlations between the image and text at multiple
levels of abstraction and learn the semantic topics from images, we design a
variational inference network to build the mapping from image features to
textual captions. To guide the paragraph generation, the learned hierarchical
topics and visual features are integrated into the language model, including
Long Short-Term Memory (LSTM) and Transformer, and jointly optimized.
Experiments on public datasets demonstrate that the proposed models, which are
competitive with many state-of-the-art approaches in terms of standard
evaluation metrics, can be used to both distill interpretable multi-layer
semantic topics and generate diverse and coherent captions. We release our code
at https://github.com/DandanGuo1993/VTCM-based-image-paragraph-caption.git
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Heterogeneous Contrastive Learning. (arXiv:2105.09401v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.09401">
<div class="article-summary-box-inner">
<span><p>With the advent of big data across multiple high-impact applications, we are
often facing the challenge of complex heterogeneity. The newly collected data
usually consist of multiple modalities and are characterized with multiple
labels, thus exhibiting the co-existence of multiple types of heterogeneity.
Although state-of-the-art techniques are good at modeling complex heterogeneity
with sufficient label information, such label information can be quite
expensive to obtain in real applications. Recently, researchers pay great
attention to contrastive learning due to its prominent performance by utilizing
rich unlabeled data. However, existing work on contrastive learning is not able
to address the problem of false negative pairs, i.e., some `negative' pairs may
have similar representations if they have the same label. To overcome the
issues, in this paper, we propose a unified heterogeneous learning framework,
which combines both the weighted unsupervised contrastive loss and the weighted
supervised contrastive loss to model multiple types of heterogeneity. We first
provide a theoretical analysis showing that the vanilla contrastive learning
loss easily leads to the sub-optimal solution in the presence of false negative
pairs, whereas the proposed weighted loss could automatically adjust the weight
based on the similarity of the learned representations to mitigate this issue.
Experimental results on real-world data sets demonstrate the effectiveness and
the efficiency of the proposed framework modeling multiple types of
heterogeneity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pose Refinement with Joint Optimization of Visual Points and Lines. (arXiv:2110.03940v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03940">
<div class="article-summary-box-inner">
<span><p>High-precision camera re-localization technology in a pre-established 3D
environment map is the basis for many tasks, such as Augmented Reality,
Robotics and Autonomous Driving. The point-based visual re-localization
approaches are well-developed in recent decades, but are insufficient in some
feature-less cases. In this paper, we design a complete pipeline for camera
pose refinement with points and lines, which contains the innovatively designed
line extracting CNN named VLSE, the line matching and the pose optimization
approaches. We adopt a novel line representation and customize a hybrid
convolution block based on the Stacked Hourglass network, to detect accurate
and stable line features on images. Then we apply a geometric-based strategy to
obtain precise 2D-3D line correspondences using epipolar constraint and
reprojection filtering. A following point-line joint cost function is
constructed to optimize the camera pose with the initial coarse pose from the
pure point-based localization. Sufficient experiments are conducted on open
datasets, i.e, line extractor on Wireframe and YorkUrban, localization
performance on InLoc duc1 and duc2, to confirm the effectiveness of our
point-line joint pose optimization method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Making Corgis Important for Honeycomb Classification: Adversarial Attacks on Concept-based Explainability Tools. (arXiv:2110.07120v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07120">
<div class="article-summary-box-inner">
<span><p>Methods for model explainability have become increasingly critical for
testing the fairness and soundness of deep learning. Concept-based
interpretability techniques, which use a small set of human-interpretable
concept exemplars in order to measure the influence of a concept on a model's
internal representation of input, are an important thread in this line of
research. In this work we show that these explainability methods can suffer the
same vulnerability to adversarial attacks as the models they are meant to
analyze. We demonstrate this phenomenon on two well-known concept-based
interpretability methods: TCAV and faceted feature visualization. We show that
by carefully perturbing the examples of the concept that is being investigated,
we can radically change the output of the interpretability method. The attacks
that we propose can either induce positive interpretations (polka dots are an
important concept for a model when classifying zebras) or negative
interpretations (stripes are not an important factor in identifying images of a
zebra). Our work highlights the fact that in safety-critical applications,
there is need for security around not only the machine learning pipeline but
also the model interpretation process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Guiding Visual Question Generation. (arXiv:2110.08226v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08226">
<div class="article-summary-box-inner">
<span><p>In traditional Visual Question Generation (VQG), most images have multiple
concepts (e.g. objects and categories) for which a question could be generated,
but models are trained to mimic an arbitrary choice of concept as given in
their training data. This makes training difficult and also poses issues for
evaluation -- multiple valid questions exist for most images but only one or a
few are captured by the human references. We present Guiding Visual Question
Generation - a variant of VQG which conditions the question generator on
categorical information based on expectations on the type of question and the
objects it should explore. We propose two variants: (i) an explicitly guided
model that enables an actor (human or automated) to select which objects and
categories to generate a question for; and (ii) an implicitly guided model that
learns which objects and categories to condition on, based on discrete latent
variables. The proposed models are evaluated on an answer-category augmented
VQA dataset and our quantitative results show a substantial improvement over
the current state of the art (over 9 BLEU-4 increase). Human evaluation
validates that guidance helps the generation of questions that are
grammatically coherent and relevant to the given image and objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Iterative Teaching by Label Synthesis. (arXiv:2110.14432v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14432">
<div class="article-summary-box-inner">
<span><p>In this paper, we consider the problem of iterative machine teaching, where a
teacher provides examples sequentially based on the current iterative learner.
In contrast to previous methods that have to scan over the entire pool and
select teaching examples from it in each iteration, we propose a label
synthesis teaching framework where the teacher randomly selects input teaching
examples (e.g., images) and then synthesizes suitable outputs (e.g., labels)
for them. We show that this framework can avoid costly example selection while
still provably achieving exponential teachability. We propose multiple novel
teaching algorithms in this framework. Finally, we empirically demonstrate the
value of our framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sliced Recursive Transformer. (arXiv:2111.05297v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.05297">
<div class="article-summary-box-inner">
<span><p>We present a neat yet effective recursive operation on vision transformers
that can improve parameter utilization without involving additional parameters.
This is achieved by sharing weights across the depth of transformer networks.
The proposed method can obtain a substantial gain (~2%) simply using naive
recursive operation, requires no special or sophisticated knowledge for
designing principles of networks, and introduces minimal computational overhead
to the training procedure. To reduce the additional computation caused by
recursive operation while maintaining the superior accuracy, we propose an
approximating method through multiple sliced group self-attentions across
recursive layers which can reduce the cost consumption by 10~30% with minimal
performance loss. We call our model Sliced Recursive Transformer (SReT), a
novel and parameter-efficient vision transformer design that is compatible with
a broad range of other designs for efficient ViT architectures. Our best model
establishes significant improvement on ImageNet-1K over state-of-the-art
methods while containing fewer parameters. The proposed weight sharing
mechanism by sliced recursion structure allows us to build a transformer with
more than 100 or even 1000 shared layers with ease while keeping a compact size
(13~15M), to avoid optimization difficulties when the model is too large. The
flexible scalability has shown great potential for scaling up models and
constructing extremely deep vision transformers. Code is available at
https://github.com/szq0214/SReT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stacked BNAS: Rethinking Broad Convolutional Neural Network for Neural Architecture Search. (arXiv:2111.07722v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.07722">
<div class="article-summary-box-inner">
<span><p>Different from other deep scalable architecture based NAS approaches, Broad
Neural Architecture Search (BNAS) proposes a broad one which consists of
convolution and enhancement blocks, dubbed Broad Convolutional Neural Network
(BCNN) as search space for amazing efficiency improvement. BCNN reuses the
topologies of cells in convolution block, so that BNAS can employ few cells for
efficient search. Moreover, multi-scale feature fusion and knowledge embedding
are proposed to improve the performance of BCNN with shallow topology. However,
BNAS suffers some drawbacks: 1) insufficient representation diversity for
feature fusion and enhancement, and 2) time consuming of knowledge embedding
design by human expert.
</p>
<p>In this paper, we propose Stacked BNAS whose search space is a developed
broad scalable architecture named Stacked BCNN, with better performance than
BNAS. On the one hand, Stacked BCNN treats mini-BCNN as the basic block to
preserve comprehensive representation and deliver powerful feature extraction
ability. On the other hand, we propose Knowledge Embedding Search (KES) to
learn appropriate knowledge embeddings. Experimental results show that 1)
Stacked BNAS obtains better performance than BNAS, 2) KES contributes to reduce
the parameters of learned architecture with satisfactory performance, and 3)
Stacked BNAS delivers state-of-the-art efficiency of 0.02 GPU days.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TnT Attacks! Universal Naturalistic Adversarial Patches Against Deep Neural Network Systems. (arXiv:2111.09999v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.09999">
<div class="article-summary-box-inner">
<span><p>Deep neural networks are vulnerable to attacks from adversarial inputs and,
more recently, Trojans to misguide or hijack the model's decision. We expose
the existence of an intriguing class of spatially bounded, physically
realizable, adversarial examples -- Universal NaTuralistic adversarial paTches
-- we call TnTs, by exploring the superset of the spatially bounded adversarial
example space and the natural input space within generative adversarial
networks. Now, an adversary can arm themselves with a patch that is
naturalistic, less malicious-looking, physically realizable, highly effective
achieving high attack success rates, and universal. A TnT is universal because
any input image captured with a TnT in the scene will: i) misguide a network
(untargeted attack); or ii) force the network to make a malicious decision
(targeted attack). Interestingly, now, an adversarial patch attacker has the
potential to exert a greater level of control -- the ability to choose a
location-independent, natural-looking patch as a trigger in contrast to being
constrained to noisy perturbations -- an ability is thus far shown to be only
possible with Trojan attack methods needing to interfere with the model
building processes to embed a backdoor at the risk discovery; but, still
realize a patch deployable in the physical world. Through extensive experiments
on the large-scale visual classification task, ImageNet with evaluations across
its entire validation set of 50,000 images, we demonstrate the realistic threat
from TnTs and the robustness of the attack. We show a generalization of the
attack to create patches achieving higher attack success rates than existing
state-of-the-art methods. Our results show the generalizability of the attack
to different visual classification tasks (CIFAR-10, GTSRB, PubFig) and multiple
state-of-the-art deep neural networks such as WideResnet50, Inception-V3 and
VGG-16.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-slimmed Vision Transformer. (arXiv:2111.12624v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12624">
<div class="article-summary-box-inner">
<span><p>Vision transformers (ViTs) have become the popular structures and
outperformed convolutional neural networks (CNNs) on various vision tasks.
However, such powerful transformers bring a huge computation burden, because of
the exhausting token-to-token comparison. The previous works focus on dropping
insignificant tokens to reduce the computational cost of ViTs. But when the
dropping ratio increases, this hard manner will inevitably discard the vital
tokens, which limits its efficiency. To solve the issue, we propose a generic
self-slimmed learning approach for vanilla ViTs, namely SiT. Specifically, we
first design a novel Token Slimming Module (TSM), which can boost the inference
efficiency of ViTs by dynamic token aggregation. As a general method of token
hard dropping, our TSM softly integrates redundant tokens into fewer
informative ones. It can dynamically zoom visual attention without cutting off
discriminative token relations in the images, even with a high slimming ratio.
Furthermore, we introduce a concise Feature Recalibration Distillation (FRD)
framework, wherein we design a reverse version of TSM (RTSM) to recalibrate the
unstructured token in a flexible auto-encoder manner. Due to the similar
structure between teacher and student, our FRD can effectively leverage
structure knowledge for better convergence. Finally, we conduct extensive
experiments to evaluate our SiT. It demonstrates that our method can speed up
ViTs by 1.7x with negligible accuracy drop, and even speed up ViTs by 3.6x
while maintaining 97% of their performance. Surprisingly, by simply arming
LV-ViT with our SiT, we achieve new state-of-the-art performance on ImageNet.
Code is available at https://github.com/Sense-X/SiT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Token Sampling For Efficient Vision Transformers. (arXiv:2111.15667v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.15667">
<div class="article-summary-box-inner">
<span><p>While state-of-the-art vision transformer models achieve promising results in
image classification, they are computationally expensive and require many
GFLOPs. Although the GFLOPs of a vision transformer can be decreased by
reducing the number of tokens in the network, there is no setting that is
optimal for all input images. In this work, we therefore introduce a
differentiable parameter-free Adaptive Token Sampler (ATS) module, which can be
plugged into any existing vision transformer architecture. ATS empowers vision
transformers by scoring and adaptively sampling significant tokens. As a
result, the number of tokens is not constant anymore and varies for each input
image. By integrating ATS as an additional layer within the current transformer
blocks, we can convert them into much more efficient vision transformers with
an adaptive number of tokens. Since ATS is a parameter-free module, it can be
added to the off-the-shelf pre-trained vision transformers as a plug and play
module, thus reducing their GFLOPs without any additional training. Moreover,
due to its differentiable design, one can also train a vision transformer
equipped with ATS. We evaluate the efficiency of our module in both image and
video classification tasks by adding it to multiple SOTA vision transformers.
Our proposed module improves the SOTA by reducing their computational costs
(GFLOPs) by 2X, while preserving their accuracy on the ImageNet, Kinetics-400,
and Kinetics-600 datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Event-guided Deblurring of Unknown Exposure Time Videos. (arXiv:2112.06988v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06988">
<div class="article-summary-box-inner">
<span><p>Motion deblurring is a highly ill-posed problem due to the loss of motion
information in the blur degradation process. Since event cameras can capture
apparent motion with a high temporal resolution, several attempts have explored
the potential of events for guiding deblurring. These methods generally assume
that the exposure time is the same as the reciprocal of the video frame rate.
However, this is not true in real situations, and the exposure time might be
unknown and dynamically varies depending on the video shooting
environment(e.g., illumination condition). In this paper, we address the
event-guided motion deblurring assuming dynamically variable unknown exposure
time of the frame-based camera. To this end, we first derive a new formulation
for event-guided motion deblurring by considering the exposure and readout time
in the video frame acquisition process. We then propose a novel end-to-end
learning framework for event-guided motion deblurring. In particular, we design
a novel Exposure Time-based Event Selection(ETES) module to selectively use
event features by estimating the cross-modal correlation between the features
from blurred frames and the events. Moreover, we propose a feature fusion
module to fuse the selected features from events and blur frames effectively.
We conduct extensive experiments on various datasets and demonstrate that our
method achieves state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TAFIM: Targeted Adversarial Attacks against Facial Image Manipulations. (arXiv:2112.09151v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09151">
<div class="article-summary-box-inner">
<span><p>Face manipulation methods can be misused to affect an individual's privacy or
to spread disinformation. To this end, we introduce a novel data-driven
approach that produces image-specific perturbations which are embedded in the
original images. The key idea is that these protected images prevent face
manipulation by causing the manipulation model to produce a predefined
manipulation target (uniformly colored output image in our case) instead of the
actual manipulation. In addition, we propose to leverage differentiable
compression approximation, hence making generated perturbations robust to
common image compression. In order to prevent against multiple manipulation
methods simultaneously, we further propose a novel attention-based fusion of
manipulation-specific perturbations. Compared to traditional adversarial
attacks that optimize noise patterns for each image individually, our
generalized model only needs a single forward pass, thus running orders of
magnitude faster and allowing for easy integration in image processing stacks,
even on resource-constrained devices like smartphones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Learning of Semantic and Visual Representations for Text Tracking. (arXiv:2112.14976v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14976">
<div class="article-summary-box-inner">
<span><p>Semantic representation is of great benefit to the video text tracking(VTT)
task that requires simultaneously classifying, detecting, and tracking texts in
the video. Most existing approaches tackle this task by appearance similarity
in continuous frames, while ignoring the abundant semantic features. In this
paper, we explore to robustly track video text with contrastive learning of
semantic and visual representations. Correspondingly, we present an end-to-end
video text tracker with Semantic and Visual Representations(SVRep), which
detects and tracks texts by exploiting the visual and semantic relationships
between different texts in a video sequence. Besides, with a light-weight
architecture, SVRep achieves state-of-the-art performance while maintaining
competitive inference speed. Specifically, with a backbone of ResNet-18, SVRep
achieves an ${\rm ID_{F1}}$ of $\textbf{65.9\%}$, running at $\textbf{16.7}$
FPS, on the ICDAR2015(video) dataset with $\textbf{8.6\%}$ improvement than the
previous state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Head2Toe: Utilizing Intermediate Representations for Better Transfer Learning. (arXiv:2201.03529v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03529">
<div class="article-summary-box-inner">
<span><p>Transfer-learning methods aim to improve performance in a data-scarce target
domain using a model pretrained on a data-rich source domain. A cost-efficient
strategy, linear probing, involves freezing the source model and training a new
classification head for the target domain. This strategy is outperformed by a
more costly but state-of-the-art method -- fine-tuning all parameters of the
source model to the target domain -- possibly because fine-tuning allows the
model to leverage useful information from intermediate layers which is
otherwise discarded by the later pretrained layers. We explore the hypothesis
that these intermediate layers might be directly exploited. We propose a
method, Head-to-Toe probing (Head2Toe), that selects features from all layers
of the source model to train a classification head for the target-domain. In
evaluations on the VTAB-1k, Head2Toe matches performance obtained with
fine-tuning on average while reducing training and storage cost hundred folds
or more, but critically, for out-of-distribution transfer, Head2Toe outperforms
fine-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PT4AL: Using Self-Supervised Pretext Tasks for Active Learning. (arXiv:2201.07459v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07459">
<div class="article-summary-box-inner">
<span><p>Labeling a large set of data is expensive. Active learning aims to tackle
this problem by asking to annotate only the most informative data from the
unlabeled set. We propose a novel active learning approach that utilizes
self-supervised pretext tasks and a unique data sampler to select data that are
both difficult and representative. We discover that the loss of a simple
self-supervised pretext task, such as rotation prediction, is closely
correlated to the downstream task loss. Before the active learning iterations,
the pretext task learner is trained on the unlabeled set, and the unlabeled
data are sorted and split into batches by their pretext task losses. In each
active learning iteration, the main task model is used to sample the most
uncertain data in a batch to be annotated. We evaluate our method on various
image classification and segmentation benchmarks and achieve compelling
performances on CIFAR10, Caltech-101, ImageNet, and Cityscapes. We further show
that our method performs well on imbalanced datasets, and can be an effective
solution to the cold-start problem where active learning performance is
affected by the randomly sampled initial labeled set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image reconstruction algorithms in radio interferometry: from handcrafted to learned regularization denoisers. (arXiv:2202.12959v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12959">
<div class="article-summary-box-inner">
<span><p>We introduce a new class of iterative image reconstruction algorithms for
radio interferometry, at the interface of convex optimization and deep
learning, inspired by plug-and-play methods. The approach consists in learning
a prior image model by training a deep neural network (DNN) as a denoiser, and
substituting it for the handcrafted proximal regularization operator of an
optimization algorithm. The proposed AIRI (``AI for Regularization in
radio-interferometric Imaging'') framework, for imaging complex intensity
structure with diffuse and faint emission from visibility data, inherits the
robustness and interpretability of optimization, and the learning power and
speed of networks. Our approach relies on three steps. Firstly, we design a low
dynamic range training database from optical intensity images. Secondly, we
train a DNN denoiser at a noise level inferred from the signal-to-noise ratio
of the data. We use training losses enhanced with a nonexpansiveness term
ensuring algorithm convergence, and including on-the-fly database dynamic range
enhancement via exponentiation. Thirdly, we plug the learned denoiser into the
forward-backward optimization algorithm, resulting in a simple iterative
structure alternating a denoising step with a gradient-descent data-fidelity
step. We have validated AIRI against CLEAN, optimization algorithms of the SARA
family, and a DNN trained to reconstruct the image directly from visibility
data. Simulation results show that AIRI is competitive in imaging quality with
SARA and its unconstrained forward-backward-based version uSARA, while
providing significant acceleration. CLEAN remains faster but offers lower
quality. The end-to-end DNN offers further acceleration, but with far lower
quality than AIRI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Wilderness Characteristics Using Explainable Machine Learning in Satellite Imagery. (arXiv:2203.00379v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00379">
<div class="article-summary-box-inner">
<span><p>Wilderness areas offer important ecological and social benefits and there are
urgent reasons to discover where their positive characteristics and ecological
functions are present and able to flourish. We apply a novel explainable
machine learning technique to satellite images which show wild and
anthropogenic areas in Fennoscandia. Occluding certain activations in an
interpretable artificial neural network we complete a comprehensive sensitivity
analysis regarding wild and anthropogenic characteristics. This enables us to
predict detailed and high-resolution sensitivity maps highlighting these
characteristics. Our artificial neural network provides an interpretable
activation space increasing confidence in our method. Within the activation
space, regions are semantically arranged. Our approach advances explainable
machine learning for remote sensing, offers opportunities for comprehensive
analyses of existing wilderness, and has practical relevance for conservation
efforts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Matters: A Weakly Supervised Vision-Language Pre-training Approach for Scene Text Detection and Spotting. (arXiv:2203.03911v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03911">
<div class="article-summary-box-inner">
<span><p>Recently, Vision-Language Pre-training (VLP) techniques have greatly
benefited various vision-language tasks by jointly learning visual and textual
representations, which intuitively helps in Optical Character Recognition (OCR)
tasks due to the rich visual and textual information in scene text images.
However, these methods cannot well cope with OCR tasks because of the
difficulty in both instance-level text encoding and image-text pair acquisition
(i.e. images and captured texts in them). This paper presents a weakly
supervised pre-training method, oCLIP, which can acquire effective scene text
representations by jointly learning and aligning visual and textual
information. Our network consists of an image encoder and a character-aware
text encoder that extract visual and textual features, respectively, as well as
a visual-textual decoder that models the interaction among textual and visual
features for learning effective scene text representations. With the learning
of textual features, the pre-trained model can attend texts in images well with
character awareness. Besides, these designs enable the learning from weakly
annotated texts (i.e. partial texts in images without text bounding boxes)
which mitigates the data annotation constraint greatly. Experiments over the
weakly annotated images in ICDAR2019-LSVT show that our pre-trained model
improves F-score by +2.5\% and +4.8\% while transferring its weights to other
text detection and spotting networks, respectively. In addition, the proposed
method outperforms existing pre-training techniques consistently across
multiple public datasets (e.g., +3.2\% and +1.3\% for Total-Text and CTW1500).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ParC-Net: Position Aware Circular Convolution with Merits from ConvNets and Transformer. (arXiv:2203.03952v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03952">
<div class="article-summary-box-inner">
<span><p>Recently, vision transformers started to show impressive results which
outperform large convolution based models significantly. However, in the area
of small models for mobile or resource constrained devices, ConvNet still has
its own advantages in both performance and model complexity. We propose
ParC-Net, a pure ConvNet based backbone model that further strengthens these
advantages by fusing the merits of vision transformers into ConvNets.
Specifically, we propose position aware circular convolution (ParC), a
light-weight convolution op which boasts a global receptive field while
producing location sensitive features as in local convolutions. We combine the
ParCs and squeeze-exictation ops to form a meta-former like model block, which
further has the attention mechanism like transformers. The aforementioned block
can be used in plug-and-play manner to replace relevant blocks in ConvNets or
transformers. Experiment results show that the proposed ParC-Net achieves
better performance than popular light-weight ConvNets and vision transformer
based models in common vision tasks and datasets, while having fewer parameters
and faster inference speed. For classification on ImageNet-1k, ParC-Net
achieves 78.6% top-1 accuracy with about 5.0 million parameters, saving 11%
parameters and 13% computational cost but gaining 0.2% higher accuracy and 23%
faster inference speed (on ARM based Rockchip RK3288) compared with MobileViT,
and uses only 0.5 times parameters but gaining 2.7% accuracy compared with
DeIT. On MS-COCO object detection and PASCAL VOC segmentation tasks, ParC-Net
also shows better performance. Source code is available at
https://github.com/hkzhang91/ParC-Net
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SynthStrip: Skull-Stripping for Any Brain Image. (arXiv:2203.09974v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09974">
<div class="article-summary-box-inner">
<span><p>The removal of non-brain signal from magnetic resonance imaging (MRI) data,
known as skull-stripping, is an integral component of many neuroimage analysis
streams. Despite their abundance, popular classical skull-stripping methods are
usually tailored to images with specific acquisition properties, namely
near-isotropic resolution and T1-weighted (T1w) MRI contrast, which are
prevalent in research settings. As a result, existing tools tend to adapt
poorly to other image types, such as stacks of thick slices acquired with fast
spin-echo (FSE) MRI that are common in the clinic. While learning-based
approaches for brain extraction have gained traction in recent years, these
methods face a similar burden, as they are only effective for image types seen
during the training procedure. To achieve robust skull-stripping across a
landscape of imaging protocols, we introduce SynthStrip, a rapid,
learning-based brain-extraction tool. By leveraging anatomical segmentations to
generate an entirely synthetic training dataset with anatomies, intensity
distributions, and artifacts that far exceed the realistic range of medical
images, SynthStrip learns to successfully generalize to a variety of real
acquired brain images, removing the need for training data with target
contrasts. We demonstrate the efficacy of SynthStrip for a diverse set of image
acquisitions and resolutions across subject populations, ranging from newborn
to adult. We show substantial improvements in accuracy over popular
skull-stripping baselines -- all with a single trained model. Our method and
labeled evaluation data are available at https://w3id.org/synthstrip.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PACS: A Dataset for Physical Audiovisual CommonSense Reasoning. (arXiv:2203.11130v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11130">
<div class="article-summary-box-inner">
<span><p>In order for AI to be safely deployed in real-world scenarios such as
hospitals, schools, and the workplace, it must be able to robustly reason about
the physical world. Fundamental to this reasoning is physical common sense:
understanding the physical properties and affordances of available objects, how
they can be manipulated, and how they interact with other objects. Physical
commonsense reasoning is fundamentally a multi-sensory task, since physical
properties are manifested through multiple modalities - two of them being
vision and acoustics. Our paper takes a step towards real-world physical
commonsense reasoning by contributing PACS: the first audiovisual benchmark
annotated for physical commonsense attributes. PACS contains 13,400
question-answer pairs, involving 1,377 unique physical commonsense questions
and 1,526 videos. Our dataset provides new opportunities to advance the
research field of physical reasoning by bringing audio as a core component of
this multimodal problem. Using PACS, we evaluate multiple state-of-the-art
models on our new challenging task. While some models show promising results
(70% accuracy), they all fall short of human performance (95% accuracy). We
conclude the paper by demonstrating the importance of multimodal reasoning and
providing possible avenues for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Semantic Segmentation Grounded in Visual Concepts. (arXiv:2203.13868v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13868">
<div class="article-summary-box-inner">
<span><p>Unsupervised semantic segmentation requires assigning a label to every pixel
without any human annotations. Despite recent advances in self-supervised
representation learning for individual images, unsupervised semantic
segmentation with pixel-level representations is still a challenging task and
remains underexplored. In this work, we propose a self-supervised pixel
representation learning method for semantic segmentation by using visual
concepts (i.e., groups of pixels with semantic meanings, such as parts,
objects, and scenes) extracted from images. To guide self-supervised learning,
we leverage three types of relationships between pixels and concepts, including
the relationships between pixels and local concepts, local and global concepts,
as well as the co-occurrence of concepts. We evaluate the learned pixel
embeddings and visual concepts on three datasets, including PASCAL VOC 2012,
COCO 2017, and DAVIS 2017. Our results show that the proposed method gains
consistent and substantial improvements over recent unsupervised semantic
segmentation approaches, and also demonstrate that visual concepts can reveal
insights into image datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-distillation Augmented Masked Autoencoders for Histopathological Image Classification. (arXiv:2203.16983v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16983">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning (SSL) has drawn increasing attention in pathological
image analysis in recent years. Compared to contrastive learning which requires
careful design, masked autoencoders (MAE) building SSL from a generative
paradigm probably is a simpler method. In this paper, we introduce MAE and
verify the effect of visible patches for pathological image classification.
Based on it, a novel SD-MAE model is proposed to enable a self-distillation
augmented SSL on top of the raw MAE. Besides the reconstruction loss on masked
image patches, SD-MAE further imposes the self-distillation loss on visible
patches. It transfers knowledge brought by the global attention of the decoder
to the encoder which only uses local attention. We apply SD-MAE on two public
pathological image datasets. Experiments demonstrate that SD-MAE performs
highly competitive when compared with other SSL methods. Our code will be
released soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structured Graph Variational Autoencoders for Indoor Furniture layout Generation. (arXiv:2204.04867v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04867">
<div class="article-summary-box-inner">
<span><p>We present a structured graph variational autoencoder for generating the
layout of indoor 3D scenes. Given the room type (e.g., living room or library)
and the room layout (e.g., room elements such as floor and walls), our
architecture generates a collection of objects (e.g., furniture items such as
sofa, table and chairs) that is consistent with the room type and layout. This
is a challenging problem because the generated scene should satisfy multiple
constrains, e.g., each object must lie inside the room and two objects cannot
occupy the same volume. To address these challenges, we propose a deep
generative model that encodes these relationships as soft constraints on an
attributed graph (e.g., the nodes capture attributes of room and furniture
elements, such as class, pose and size, and the edges capture geometric
relationships such as relative orientation). The architecture consists of a
graph encoder that maps the input graph to a structured latent space, and a
graph decoder that generates a furniture graph, given a latent code and the
room graph. The latent space is modeled with auto-regressive priors, which
facilitates the generation of highly structured scenes. We also propose an
efficient training procedure that combines matching and constrained learning.
Experiments on the 3D-FRONT dataset show that our method produces scenes that
are diverse and are adapted to the room layout.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MINSU (Mobile Inventory And Scanning Unit):Computer Vision and AI. (arXiv:2204.06681v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06681">
<div class="article-summary-box-inner">
<span><p>The MINSU(Mobile Inventory and Scanning Unit) algorithm uses the
computational vision analysis method to record the residual quantity/fullness
of the cabinet. To do so, it goes through a five-step method: object detection,
foreground subtraction, K-means clustering, percentage estimation, and
counting. The input image goes through the object detection method to analyze
the specific position of the cabinets in terms of coordinates. After doing so,
it goes through the foreground subtraction method to make the image more
focus-able to the cabinet itself by removing the background (some manual work
may have to be done such as selecting the parts that were not grab cut by the
algorithm). In the K-means clustering method, the multi-colored image turns
into a 3 colored monotonous image for quicker and more accurate analysis. At
last, the image goes through percentage estimation and counting. In these two
methods, the proportion that the material inside the cabinet is found in
percentage which then is used to approximate the number of materials inside.
Had this project been successful, the residual quantity management could solve
the problem addressed earlier in the introduction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differentiable Zooming for Multiple Instance Learning on Whole-Slide Images. (arXiv:2204.12454v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12454">
<div class="article-summary-box-inner">
<span><p>Multiple Instance Learning (MIL) methods have become increasingly popular for
classifying giga-pixel sized Whole-Slide Images (WSIs) in digital pathology.
Most MIL methods operate at a single WSI magnification, by processing all the
tissue patches. Such a formulation induces high computational requirements, and
constrains the contextualization of the WSI-level representation to a single
scale. A few MIL methods extend to multiple scales, but are computationally
more demanding. In this paper, inspired by the pathological diagnostic process,
we propose ZoomMIL, a method that learns to perform multi-level zooming in an
end-to-end manner. ZoomMIL builds WSI representations by aggregating
tissue-context information from multiple magnifications. The proposed method
outperforms the state-of-the-art MIL methods in WSI classification on two large
datasets, while significantly reducing the computational demands with regard to
Floating-Point Operations (FLOPs) and processing time by up to 40x.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HRDA: Context-Aware High-Resolution Domain-Adaptive Semantic Segmentation. (arXiv:2204.13132v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13132">
<div class="article-summary-box-inner">
<span><p>Unsupervised domain adaptation (UDA) aims to adapt a model trained on the
source domain (e.g. synthetic data) to the target domain (e.g. real-world data)
without requiring further annotations on the target domain. This work focuses
on UDA for semantic segmentation as real-world pixel-wise annotations are
particularly expensive to acquire. As UDA methods for semantic segmentation are
usually GPU memory intensive, most previous methods operate only on downscaled
images. We question this design as low-resolution predictions often fail to
preserve fine details. The alternative of training with random crops of
high-resolution images alleviates this problem but falls short in capturing
long-range, domain-robust context information. Therefore, we propose HRDA, a
multi-resolution training approach for UDA, that combines the strengths of
small high-resolution crops to preserve fine segmentation details and large
low-resolution crops to capture long-range context dependencies with a learned
scale attention, while maintaining a manageable GPU memory footprint. HRDA
enables adapting small objects and preserving fine segmentation details. It
significantly improves the state-of-the-art performance by 5.5 mIoU for
GTA-to-Cityscapes and 4.9 mIoU for Synthia-to-Cityscapes, resulting in
unprecedented 73.8 and 65.8 mIoU, respectively. The implementation is available
at https://github.com/lhoyer/HRDA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Extrapolation in Space and Time. (arXiv:2205.02084v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02084">
<div class="article-summary-box-inner">
<span><p>Novel view synthesis (NVS) and video prediction (VP) are typically considered
disjoint tasks in computer vision. However, they can both be seen as ways to
observe the spatial-temporal world: NVS aims to synthesize a scene from a new
point of view, while VP aims to see a scene from a new point of time. These two
tasks provide complementary signals to obtain a scene representation, as
viewpoint changes from spatial observations inform depth, and temporal
observations inform the motion of cameras and individual objects. Inspired by
these observations, we propose to study the problem of Video Extrapolation in
Space and Time (VEST). We propose a model that leverages the self-supervision
and the complementary cues from both tasks, while existing methods can only
solve one of them. Experiments show that our method achieves performance better
than or comparable to several state-of-the-art NVS and VP methods on indoor and
outdoor real-world datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HULC: 3D Human Motion Capture with Pose Manifold Sampling and Dense Contact Guidance. (arXiv:2205.05677v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05677">
<div class="article-summary-box-inner">
<span><p>Marker-less monocular 3D human motion capture (MoCap) with scene interactions
is a challenging research topic relevant for extended reality, robotics and
virtual avatar generation. Due to the inherent depth ambiguity of monocular
settings, 3D motions captured with existing methods often contain severe
artefacts such as incorrect body-scene inter-penetrations, jitter and body
floating. To tackle these issues, we propose HULC, a new approach for 3D human
MoCap which is aware of the scene geometry. HULC estimates 3D poses and dense
body-environment surface contacts for improved 3D localisations, as well as the
absolute scale of the subject. Furthermore, we introduce a 3D pose trajectory
optimisation based on a novel pose manifold sampling that resolves erroneous
body-environment inter-penetrations. Although the proposed method requires less
structured inputs compared to existing scene-aware monocular MoCap algorithms,
it produces more physically-plausible poses: HULC significantly and
consistently outperforms the existing approaches in various experiments and on
different metrics. Project page: https://vcai.mpi-inf.mpg.de/projects/HULC/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Image Representation Learning with Deep Latent Particles. (arXiv:2205.15821v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15821">
<div class="article-summary-box-inner">
<span><p>We propose a new representation of visual data that disentangles object
position from appearance. Our method, termed Deep Latent Particles (DLP),
decomposes the visual input into low-dimensional latent ``particles'', where
each particle is described by its spatial location and features of its
surrounding region. To drive learning of such representations, we follow a
VAE-based approach and introduce a prior for particle positions based on a
spatial-softmax architecture, and a modification of the evidence lower bound
loss inspired by the Chamfer distance between particles. We demonstrate that
our DLP representations are useful for downstream tasks such as unsupervised
keypoint (KP) detection, image manipulation, and video prediction for scenes
composed of multiple dynamic objects. In addition, we show that our
probabilistic interpretation of the problem naturally provides uncertainty
estimates for particle locations, which can be used for model selection, among
other tasks. Videos and code are available:
https://taldatech.github.io/deep-latent-particles-web/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Box2Mask: Weakly Supervised 3D Semantic Instance Segmentation Using Bounding Boxes. (arXiv:2206.01203v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01203">
<div class="article-summary-box-inner">
<span><p>Current 3D segmentation methods heavily rely on large-scale point-cloud
datasets, which are notoriously laborious to annotate. Few attempts have been
made to circumvent the need for dense per-point annotations. In this work, we
look at weakly-supervised 3D semantic instance segmentation. The key idea is to
leverage 3D bounding box labels which are easier and faster to annotate.
Indeed, we show that it is possible to train dense segmentation models using
only bounding box labels. At the core of our method, \name{}, lies a deep
model, inspired by classical Hough voting, that directly votes for bounding box
parameters, and a clustering method specifically tailored to bounding box
votes. This goes beyond commonly used center votes, which would not fully
exploit the bounding box annotations. On ScanNet test, our weakly supervised
model attains leading performance among other weakly supervised approaches (+18
mAP@50). Remarkably, it also achieves 97% of the mAP@50 score of current fully
supervised models. To further illustrate the practicality of our work, we train
Box2Mask on the recently released ARKitScenes dataset which is annotated with
3D bounding boxes only, and show, for the first time, compelling 3D instance
segmentation masks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compositional Visual Generation with Composable Diffusion Models. (arXiv:2206.01714v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01714">
<div class="article-summary-box-inner">
<span><p>Large text-guided diffusion models, such as DALLE-2, are able to generate
stunning photorealistic images given natural language descriptions. While such
models are highly flexible, they struggle to understand the composition of
certain concepts, such as confusing the attributes of different objects or
relations between objects. In this paper, we propose an alternative structured
approach for compositional generation using diffusion models. An image is
generated by composing a set of diffusion models, with each of them modeling a
certain component of the image. To do this, we interpret diffusion models as
energy-based models in which the data distributions defined by the energy
functions may be explicitly combined. The proposed method can generate scenes
at test time that are substantially more complex than those seen in training,
composing sentence descriptions, object relations, human facial attributes, and
even generalizing to new combinations that are rarely seen in the real world.
We further illustrate how our approach may be used to compose pre-trained
text-guided diffusion models and generate photorealistic images containing all
the details described in the input descriptions, including the binding of
certain object attributes that have been shown difficult for DALLE-2. These
results point to the effectiveness of the proposed method in promoting
structured generalization for visual generation. Project page:
https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robustness Implies Generalization via Data-Dependent Generalization Bounds. (arXiv:2206.13497v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.13497">
<div class="article-summary-box-inner">
<span><p>This paper proves that robustness implies generalization via data-dependent
generalization bounds. As a result, robustness and generalization are shown to
be connected closely in a data-dependent manner. Our bounds improve previous
bounds in two directions, to solve an open problem that has seen little
development since 2010. The first is to reduce the dependence on the covering
number. The second is to remove the dependence on the hypothesis space. We
present several examples, including ones for lasso and deep learning, in which
our bounds are provably preferable. The experiments on real-world data and
theoretical models demonstrate near-exponential improvements in various
situations. To achieve these improvements, we do not require additional
assumptions on the unknown distribution; instead, we only incorporate an
observable and computable property of the training samples. A key technical
innovation is an improved concentration bound for multinomial random variables
that is of independent interest beyond robustness and generalization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Autonomous Intraluminal Navigation of a Soft Robot using Deep-Learning-based Visual Servoing. (arXiv:2207.00401v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00401">
<div class="article-summary-box-inner">
<span><p>Navigation inside luminal organs is an arduous task that requires
non-intuitive coordination between the movement of the operator's hand and the
information obtained from the endoscopic video. The development of tools to
automate certain tasks could alleviate the physical and mental load of doctors
during interventions, allowing them to focus on diagnosis and decision-making
tasks. In this paper, we present a synergic solution for intraluminal
navigation consisting of a 3D printed endoscopic soft robot that can move
safely inside luminal structures. Visual servoing, based on Convolutional
Neural Networks (CNNs) is used to achieve the autonomous navigation task. The
CNN is trained with phantoms and in-vivo data to segment the lumen, and a
model-less approach is presented to control the movement in constrained
environments. The proposed robot is validated in anatomical phantoms in
different path configurations. We analyze the movement of the robot using
different metrics such as task completion time, smoothness, error in the
steady-state, and mean and maximum error. We show that our method is suitable
to navigate safely in hollow environments and conditions which are different
than the ones the network was originally trained on.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accurate Ground-Truth Depth Image Generation via Overfit Training of Point Cloud Registration using Local Frame Sets. (arXiv:2207.07016v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07016">
<div class="article-summary-box-inner">
<span><p>Accurate three-dimensional perception is a fundamental task in several
computer vision applications. Recently, commercial RGB-depth (RGB-D) cameras
have been widely adopted as single-view depth-sensing devices owing to their
efficient depth-sensing abilities. However, the depth quality of most RGB-D
sensors remains insufficient owing to the inherent noise from a single-view
environment. Recently, several studies have focused on the single-view depth
enhancement of RGB-D cameras. Recent research has proposed deep-learning-based
approaches that typically train networks using high-quality supervised depth
datasets, which indicates that the quality of the ground-truth (GT) depth
dataset is a top-most important factor for accurate system; however, such
high-quality GT datasets are difficult to obtain. In this study, we developed a
novel method for high-quality GT depth generation based on an RGB-D stream
dataset. First, we defined consecutive depth frames in a local spatial region
as a local frame set. Then, the depth frames were aligned to a certain frame in
the local frame set using an unsupervised point cloud registration scheme. The
registration parameters were trained based on an overfit-training scheme, which
was primarily used to construct a single GT depth image for each frame set. The
final GT depth dataset was constructed using several local frame sets, and each
local frame set was trained independently. The primary advantage of this study
is that a high-quality GT depth dataset can be constructed under various
scanning environments using only the RGB-D stream dataset. Moreover, our
proposed method can be used as a new benchmark GT dataset for accurate
performance evaluations. We evaluated our GT dataset on previously benchmarked
GT depth datasets and demonstrated that our method is superior to
state-of-the-art depth enhancement frameworks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Injecting 3D Perception of Controllable NeRF-GAN into StyleGAN for Editable Portrait Image Synthesis. (arXiv:2207.10257v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.10257">
<div class="article-summary-box-inner">
<span><p>Over the years, 2D GANs have achieved great successes in photorealistic
portrait generation. However, they lack 3D understanding in the generation
process, thus they suffer from multi-view inconsistency problem. To alleviate
the issue, many 3D-aware GANs have been proposed and shown notable results, but
3D GANs struggle with editing semantic attributes. The controllability and
interpretability of 3D GANs have not been much explored. In this work, we
propose two solutions to overcome these weaknesses of 2D GANs and 3D-aware
GANs. We first introduce a novel 3D-aware GAN, SURF-GAN, which is capable of
discovering semantic attributes during training and controlling them in an
unsupervised manner. After that, we inject the prior of SURF-GAN into StyleGAN
to obtain a high-fidelity 3D-controllable generator. Unlike existing
latent-based methods allowing implicit pose control, the proposed
3D-controllable StyleGAN enables explicit pose control over portrait
generation. This distillation allows direct compatibility between 3D control
and many StyleGAN-based techniques (e.g., inversion and stylization), and also
brings an advantage in terms of computational resources. Our codes are
available at https://github.com/jgkwak95/SURF-GAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatial-temporal Analysis for Automated Concrete Workability Estimation. (arXiv:2207.11635v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11635">
<div class="article-summary-box-inner">
<span><p>Concrete workability measure is mostly determined based on subjective
assessment of a certified assessor with visual inspections. The potential human
error in measuring the workability and the resulting unnecessary adjustments
for the workability is a major challenge faced by the construction industry,
leading to significant costs, material waste and delay. In this paper, we try
to apply computer vision techniques to observe the concrete mixing process and
estimate the workability. Specifically, we collected the video data and then
built three different deep neural networks for spatial-temporal regression. The
pilot study demonstrates a practical application with computer vision
techniques to estimate the concrete workability during the mixing process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sub-Aperture Feature Adaptation in Single Image Super-resolution Model for Light Field Imaging. (arXiv:2207.11894v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11894">
<div class="article-summary-box-inner">
<span><p>With the availability of commercial Light Field (LF) cameras, LF imaging has
emerged as an up and coming technology in computational photography. However,
the spatial resolution is significantly constrained in commercial microlens
based LF cameras because of the inherent multiplexing of spatial and angular
information. Therefore, it becomes the main bottleneck for other applications
of light field cameras. This paper proposes an adaptation module in a
pretrained Single Image Super Resolution (SISR) network to leverage the
powerful SISR model instead of using highly engineered light field imaging
domain specific Super Resolution models. The adaption module consists of a Sub
aperture Shift block and a fusion block. It is an adaptation in the SISR
network to further exploit the spatial and angular information in LF images to
improve the super resolution performance. Experimental validation shows that
the proposed method outperforms existing light field super resolution
algorithms. It also achieves PSNR gains of more than 1 dB across all the
datasets as compared to the same pretrained SISR models for scale factor 2, and
PSNR gains 0.6 to 1 dB for scale factor 4.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimal Boxes: Boosting End-to-End Scene Text Recognition by Adjusting Annotated Bounding Boxes via Reinforcement Learning. (arXiv:2207.11934v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11934">
<div class="article-summary-box-inner">
<span><p>Text detection and recognition are essential components of a modern OCR
system. Most OCR approaches attempt to obtain accurate bounding boxes of text
at the detection stage, which is used as the input of the text recognition
stage. We observe that when using tight text bounding boxes as input, a text
recognizer frequently fails to achieve optimal performance due to the
inconsistency between bounding boxes and deep representations of text
recognition. In this paper, we propose Box Adjuster, a reinforcement
learning-based method for adjusting the shape of each text bounding box to make
it more compatible with text recognition models. Additionally, when dealing
with cross-domain problems such as synthetic-to-real, the proposed method
significantly reduces mismatches in domain distribution between the source and
target domains. Experiments demonstrate that the performance of end-to-end text
recognition systems can be improved when using the adjusted bounding boxes as
the ground truths for training. Specifically, on several benchmark datasets for
scene text understanding, the proposed method outperforms state-of-the-art text
spotters by an average of 2.0% F-Score on end-to-end text recognition tasks and
4.6% F-Score on domain adaptation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RA-Depth: Resolution Adaptive Self-Supervised Monocular Depth Estimation. (arXiv:2207.11984v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11984">
<div class="article-summary-box-inner">
<span><p>Existing self-supervised monocular depth estimation methods can get rid of
expensive annotations and achieve promising results. However, these methods
suffer from severe performance degradation when directly adopting a model
trained on a fixed resolution to evaluate at other different resolutions. In
this paper, we propose a resolution adaptive self-supervised monocular depth
estimation method (RA-Depth) by learning the scale invariance of the scene
depth. Specifically, we propose a simple yet efficient data augmentation method
to generate images with arbitrary scales for the same scene. Then, we develop a
dual high-resolution network that uses the multi-path encoder and decoder with
dense interactions to aggregate multi-scale features for accurate depth
inference. Finally, to explicitly learn the scale invariance of the scene
depth, we formulate a cross-scale depth consistency loss on depth predictions
with different scales. Extensive experiments on the KITTI, Make3D and NYU-V2
datasets demonstrate that RA-Depth not only achieves state-of-the-art
performance, but also exhibits a good ability of resolution adaptation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Siamese Transformer Network for Single Object Tracking on Point Clouds. (arXiv:2207.11995v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11995">
<div class="article-summary-box-inner">
<span><p>Siamese network based trackers formulate 3D single object tracking as
cross-correlation learning between point features of a template and a search
area. Due to the large appearance variation between the template and search
area during tracking, how to learn the robust cross correlation between them
for identifying the potential target in the search area is still a challenging
problem. In this paper, we explicitly use Transformer to form a 3D Siamese
Transformer network for learning robust cross correlation between the template
and the search area of point clouds. Specifically, we develop a Siamese point
Transformer network to learn shape context information of the target. Its
encoder uses self-attention to capture non-local information of point clouds to
characterize the shape information of the object, and the decoder utilizes
cross-attention to upsample discriminative point features. After that, we
develop an iterative coarse-to-fine correlation network to learn the robust
cross correlation between the template and the search area. It formulates the
cross-feature augmentation to associate the template with the potential target
in the search area via cross attention. To further enhance the potential
target, it employs the ego-feature augmentation that applies self-attention to
the local k-NN graph of the feature space to aggregate target features.
Experiments on the KITTI, nuScenes, and Waymo datasets show that our method
achieves state-of-the-art performance on the 3D single object tracking task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Riemannian Geometry Approach for Minimizing Distortion and its Applications. (arXiv:2207.12038v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12038">
<div class="article-summary-box-inner">
<span><p>Given an affine transformation $T$, we define its Fisher distortion
$Dist_F(T)$. We show that the Fisher distortion has Riemannian metric structure
and provide an algorithm for finding mean distorting transformation -- namely
-- for a given set $\{T_{i}\}_{i=1}^N$ of affine transformations, find an
affine transformation $T$ that minimize the overall distortion
$\sum_{i=1}^NDist_F^{2}(T^{-1}T_{i}).$ The mean distorting transformation can
be useful in some fields -- in particular, we apply it for rendering affine
panoramas.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Decorrelation with Potential Energy Ranking. (arXiv:2207.12194v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.12194">
<div class="article-summary-box-inner">
<span><p>Machine learning systems, especially the methods based on deep learning,
enjoy great success in modern computer vision tasks under experimental
settings. Generally, these classic deep learning methods are built on the
\emph{i.i.d.} assumption, supposing the training and test data are drawn from a
similar distribution independently and identically. However, the aforementioned
\emph{i.i.d.} assumption is in general unavailable in the real-world scenario,
and as a result, leads to sharp performance decay of deep learning algorithms.
Behind this, domain shift is one of the primary factors to be blamed. In order
to tackle this problem, we propose using \textbf{Po}tential \textbf{E}nergy
\textbf{R}anking (PoER) to decouple the object feature and the domain feature
(\emph{i.e.,} appearance feature) in given images, promoting the learning of
label-discriminative features while filtering out the irrelevant correlations
between the objects and the background. PoER helps the neural networks to
capture label-related features which contain the domain information first in
shallow layers and then distills the label-discriminative representations out
progressively, enforcing the neural networks to be aware of the characteristic
of objects and background which is vital to the generation of domain-invariant
features. PoER reports superior performance on domain generalization
benchmarks, improving the average top-1 accuracy by at least 1.20\% compared to
the existing methods. Moreover, we use PoER in the ECCV 2022 NICO
Challenge\footnote{https://nicochallenge.com}, achieving top place with only a
vanilla ResNet-18. The code has been made available at
https://github.com/ForeverPs/PoER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Face Deblurring using Dual Camera Fusion on Mobile Phones. (arXiv:2207.11617v1 [cs.CV] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.11617">
<div class="article-summary-box-inner">
<span><p>Motion blur of fast-moving subjects is a longstanding problem in photography
and very common on mobile phones due to limited light collection efficiency,
particularly in low-light conditions. While we have witnessed great progress in
image deblurring in recent years, most methods require significant
computational power and have limitations in processing high-resolution photos
with severe local motions. To this end, we develop a novel face deblurring
system based on the dual camera fusion technique for mobile phones. The system
detects subject motion to dynamically enable a reference camera, e.g.,
ultrawide angle camera commonly available on recent premium phones, and
captures an auxiliary photo with faster shutter settings. While the main shot
is low noise but blurry, the reference shot is sharp but noisy. We learn ML
models to align and fuse these two shots and output a clear photo without
motion blur. Our algorithm runs efficiently on Google Pixel 6, which takes 463
ms overhead per shot. Our experiments demonstrate the advantage and robustness
of our system against alternative single-image, multi-frame, face-specific, and
video deblurring algorithms as well as commercial products. To the best of our
knowledge, our work is the first mobile solution for face motion deblurring
that works reliably and robustly over thousands of images in diverse motion and
lighting conditions.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-07-27 23:08:12.790207755 UTC">2022-07-27 23:08:12 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>