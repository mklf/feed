<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-14T01:30:00Z">09-14</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Reference-Centric Models for Grounded Collaborative Dialogue. (arXiv:2109.05042v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05042">
<div class="article-summary-box-inner">
<span><p>We present a grounded neural dialogue model that successfully collaborates
with people in a partially-observable reference game. We focus on a setting
where two agents each observe an overlapping part of a world context and need
to identify and agree on some object they share. Therefore, the agents should
pool their information and communicate pragmatically to solve the task. Our
dialogue agent accurately grounds referents from the partner's utterances using
a structured reference resolver, conditions on these referents using a
recurrent memory, and uses a pragmatic generation procedure to ensure the
partner can resolve the references the agent produces. We evaluate on the
OneCommon spatial grounding dialogue task (Udagawa and Aizawa 2019), involving
a number of dots arranged on a board with continuously varying positions,
sizes, and shades. Our agent substantially outperforms the previous state of
the art for the task, obtaining a 20% relative improvement in successful task
completion in self-play evaluations and a 50% relative improvement in success
in human evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Entity-Based Knowledge Conflicts in Question Answering. (arXiv:2109.05052v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05052">
<div class="article-summary-box-inner">
<span><p>Knowledge-dependent tasks typically use two sources of knowledge: parametric,
learned at training time, and contextual, given as a passage at inference time.
To understand how models use these sources together, we formalize the problem
of knowledge conflicts, where the contextual information contradicts the
learned information. Analyzing the behaviour of popular models, we measure
their over-reliance on memorized information (the cause of hallucinations), and
uncover important factors that exacerbate this behaviour. Lastly, we propose a
simple method to mitigate over-reliance on parametric knowledge, which
minimizes hallucination, and improves out-of-distribution generalization by
4%-7%. Our findings demonstrate the importance for practitioners to evaluate
model tendency to hallucinate rather than read, and show that our mitigation
strategy encourages generalization to evolving information (i.e.,
time-dependent queries). To encourage these practices, we have released our
framework for generating knowledge conflicts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speaker Turn Modeling for Dialogue Act Classification. (arXiv:2109.05056v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05056">
<div class="article-summary-box-inner">
<span><p>Dialogue Act (DA) classification is the task of classifying utterances with
respect to the function they serve in a dialogue. Existing approaches to DA
classification model utterances without incorporating the turn changes among
speakers throughout the dialogue, therefore treating it no different than
non-interactive written text. In this paper, we propose to integrate the turn
changes in conversations among speakers when modeling DAs. Specifically, we
learn conversation-invariant speaker turn embeddings to represent the speaker
turns in a conversation; the learned speaker turn embeddings are then merged
with the utterance embeddings for the downstream task of DA classification.
With this simple yet effective mechanism, our model is able to capture the
semantics from the dialogue content while accounting for different speaker
turns in a conversation. Validation on three benchmark public datasets
demonstrates superior performance of our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FBERT: A Neural Transformer for Identifying Offensive Content. (arXiv:2109.05074v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05074">
<div class="article-summary-box-inner">
<span><p>Transformer-based models such as BERT, XLNET, and XLM-R have achieved
state-of-the-art performance across various NLP tasks including the
identification of offensive language and hate speech, an important problem in
social media. In this paper, we present fBERT, a BERT model retrained on SOLID,
the largest English offensive language identification corpus available with
over $1.4$ million offensive instances. We evaluate fBERT's performance on
identifying offensive content on multiple English datasets and we test several
thresholds for selecting instances from SOLID. The fBERT model will be made
freely available to the community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Self-Disclosure In Neural Dialog Models By Candidate Re-ranking. (arXiv:2109.05090v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05090">
<div class="article-summary-box-inner">
<span><p>Neural language modelling has progressed the state-of-the-art in different
downstream Natural Language Processing (NLP) tasks. One such area is of
open-domain dialog modelling, neural dialog models based on GPT-2 such as
DialoGPT have shown promising performance in single-turn conversation. However,
such (neural) dialog models have been criticized for generating responses which
although may have relevance to the previous human response, tend to quickly
dissipate human interest and descend into trivial conversation. One reason for
such performance is the lack of explicit conversation strategy being employed
in human-machine conversation. Humans employ a range of conversation strategies
while engaging in a conversation, one such key social strategies is
Self-disclosure(SD). A phenomenon of revealing information about one-self to
others. Social penetration theory (SPT) proposes that communication between two
people moves from shallow to deeper levels as the relationship progresses
primarily through self-disclosure. Disclosure helps in creating rapport among
the participants engaged in a conversation. In this paper, Self-disclosure
enhancement architecture (SDEA) is introduced utilizing Self-disclosure Topic
Model (SDTM) during inference stage of a neural dialog model to re-rank
response candidates to enhance self-disclosure in single-turn responses from
from the model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models. (arXiv:2109.05093v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05093">
<div class="article-summary-box-inner">
<span><p>Large pre-trained language models for textual data have an unconstrained
output space; at each decoding step, they can produce any of 10,000s of
sub-word tokens. When fine-tuned to target constrained formal languages like
SQL, these models often generate invalid code, rendering it unusable. We
propose PICARD (code and trained models available at
https://github.com/ElementAI/picard), a method for constraining auto-regressive
decoders of language models through incremental parsing. PICARD helps to find
valid output sequences by rejecting inadmissible tokens at each decoding step.
On the challenging Spider and CoSQL text-to-SQL translation tasks, we show that
PICARD transforms fine-tuned T5 models with passable performance into
state-of-the-art solutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HypoGen: Hyperbole Generation with Commonsense and Counterfactual Knowledge. (arXiv:2109.05097v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05097">
<div class="article-summary-box-inner">
<span><p>A hyperbole is an intentional and creative exaggeration not to be taken
literally. Despite its ubiquity in daily life, the computational explorations
of hyperboles are scarce. In this paper, we tackle the under-explored and
challenging task: sentence-level hyperbole generation. We start with a
representative syntactic pattern for intensification and systematically study
the semantic (commonsense and counterfactual) relationships between each
component in such hyperboles. Next, we leverage the COMeT and reverse COMeT
models to do commonsense and counterfactual inference. We then generate
multiple hyperbole candidates based on our findings from the pattern, and train
neural classifiers to rank and select high-quality hyperboles. Automatic and
human evaluations show that our generation method is able to generate
hyperboles creatively with high success rate and intensity scores.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Zero-shot Commonsense Reasoning with Self-supervised Refinement of Language Models. (arXiv:2109.05105v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05105">
<div class="article-summary-box-inner">
<span><p>Can we get existing language models and refine them for zero-shot commonsense
reasoning? This paper presents an initial study exploring the feasibility of
zero-shot commonsense reasoning for the Winograd Schema Challenge by
formulating the task as self-supervised refinement of a pre-trained language
model. In contrast to previous studies that rely on fine-tuning annotated
datasets, we seek to boost conceptualization via loss landscape refinement. To
this end, we propose a novel self-supervised learning approach that refines the
language model utilizing a set of linguistic perturbations of similar concept
relationships. Empirical analysis of our conceptually simple framework
demonstrates the viability of zero-shot commonsense reasoning on multiple
benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attention-based Contrastive Learning for Winograd Schemas. (arXiv:2109.05108v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05108">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning has recently attracted considerable attention in the
NLP community for its ability to learn discriminative features using a
contrastive objective. This paper investigates whether contrastive learning can
be extended to Transfomer attention to tackling the Winograd Schema Challenge.
To this end, we propose a novel self-supervised framework, leveraging a
contrastive loss directly at the level of self-attention. Experimental analysis
of our attention-based models on multiple datasets demonstrates superior
commonsense reasoning capabilities. The proposed approach outperforms all
comparable unsupervised approaches while occasionally surpassing supervised
ones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved Latent Tree Induction with Distant Supervision via Span Constraints. (arXiv:2109.05112v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05112">
<div class="article-summary-box-inner">
<span><p>For over thirty years, researchers have developed and analyzed methods for
latent tree induction as an approach for unsupervised syntactic parsing.
Nonetheless, modern systems still do not perform well enough compared to their
supervised counterparts to have any practical use as structural annotation of
text. In this work, we present a technique that uses distant supervision in the
form of span constraints (i.e. phrase bracketing) to improve performance in
unsupervised constituency parsing. Using a relatively small number of span
constraints we can substantially improve the output from DIORA, an already
competitive unsupervised parsing system. Compared with full parse tree
annotation, span constraints can be acquired with minimal effort, such as with
a lexicon derived from Wikipedia, to find exact text matches. Our experiments
show span constraints based on entities improves constituency parsing on
English WSJ Penn Treebank by more than 5 F1. Furthermore, our method extends to
any domain where span constraints are easily attainable, and as a case study we
demonstrate its effectiveness by parsing biomedical text from the CRAFT
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Partially-supervised novel object captioning leveraging context from paired data. (arXiv:2109.05115v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05115">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose an approach to improve image captioning solutions
for images with novel objects that do not have caption labels in the training
dataset. Our approach is agnostic to model architecture, and primarily focuses
on training technique that uses existing fully paired image-caption data and
the images with only the novel object detection labels (partially paired data).
We create synthetic paired captioning data for these novel objects by
leveraging context from existing image-caption pairs. We further re-use these
partially paired images with novel objects to create pseudo-label captions that
are used to fine-tune the captioning model. Using a popular captioning model
(Up-Down) as baseline, our approach achieves state-of-the-art results on
held-out MS COCO out-of-domain test split, and improves F1 metric and CIDEr for
novel object images by 75.8 and 26.6 points respectively, compared to baseline
model that does not use partially paired images during training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MURAL: Multimodal, Multitask Retrieval Across Languages. (arXiv:2109.05125v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05125">
<div class="article-summary-box-inner">
<span><p>Both image-caption pairs and translation pairs provide the means to learn
deep representations of and connections between languages. We use both types of
pairs in MURAL (MUltimodal, MUltitask Representations Across Languages), a dual
encoder that solves two tasks: 1) image-text matching and 2) translation pair
matching. By incorporating billions of translation pairs, MURAL extends ALIGN
(Jia et al. PMLR'21)--a state-of-the-art dual encoder learned from 1.8 billion
noisy image-text pairs. When using the same encoders, MURAL's performance
matches or exceeds ALIGN's cross-modal retrieval performance on well-resourced
languages across several datasets. More importantly, it considerably improves
performance on under-resourced languages, showing that text-text learning can
overcome a paucity of image-caption examples for these languages. On the
Wikipedia Image-Text dataset, for example, MURAL-base improves zero-shot mean
recall by 8.1% on average for eight under-resourced languages and by 6.8% on
average when fine-tuning. We additionally show that MURAL's text
representations cluster not only with respect to genealogical connections but
also based on areal linguistics, such as the Balkan Sprachbund.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">D-REX: Dialogue Relation Extraction with Explanations. (arXiv:2109.05126v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05126">
<div class="article-summary-box-inner">
<span><p>Existing research studies on cross-sentence relation extraction in long-form
multi-party conversations aim to improve relation extraction without
considering the explainability of such methods. This work addresses that gap by
focusing on extracting explanations that indicate that a relation exists while
using only partially labeled data. We propose our model-agnostic framework,
D-REX, a policy-guided semi-supervised algorithm that explains and ranks
relations. We frame relation extraction as a re-ranking task and include
relation- and entity-specific explanations as an intermediate step of the
inference process. We find that about 90% of the time, human annotators prefer
D-REX's explanations over a strong BERT-based joint relation extraction and
explanation model. Finally, our evaluations on a dialogue relation extraction
dataset show that our method is simple yet effective and achieves a
state-of-the-art F1 score on relation extraction, improving upon existing
methods by 13.5%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Refocusing on Relevance: Personalization in NLG. (arXiv:2109.05140v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05140">
<div class="article-summary-box-inner">
<span><p>Many NLG tasks such as summarization, dialogue response, or open domain
question answering focus primarily on a source text in order to generate a
target response. This standard approach falls short, however, when a user's
intent or context of work is not easily recoverable based solely on that source
text -- a scenario that we argue is more of the rule than the exception. In
this work, we argue that NLG systems in general should place a much higher
level of emphasis on making use of additional context, and suggest that
relevance (as used in Information Retrieval) be thought of as a crucial tool
for designing user-oriented text-generating tasks. We further discuss possible
harms and hazards around such personalization, and argue that value-sensitive
design represents a crucial path forward through these challenges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extract, Integrate, Compete: Towards Verification Style Reading Comprehension. (arXiv:2109.05149v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05149">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a new verification style reading comprehension
dataset named VGaokao from Chinese Language tests of Gaokao. Different from
existing efforts, the new dataset is originally designed for native speakers'
evaluation, thus requiring more advanced language understanding skills. To
address the challenges in VGaokao, we propose a novel Extract-Integrate-Compete
approach, which iteratively selects complementary evidence with a novel query
updating mechanism and adaptively distills supportive evidence, followed by a
pairwise competition to push models to learn the subtle difference among
similar text pieces. Experiments show that our methods outperform various
baselines on VGaokao with retrieved complementary evidence, while having the
merits of efficiency and explainability. Our dataset and code are released for
further research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Natural SQL: Making SQL Easier to Infer from Natural Language Specifications. (arXiv:2109.05153v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05153">
<div class="article-summary-box-inner">
<span><p>Addressing the mismatch between natural language descriptions and the
corresponding SQL queries is a key challenge for text-to-SQL translation. To
bridge this gap, we propose an SQL intermediate representation (IR) called
Natural SQL (NatSQL). Specifically, NatSQL preserves the core functionalities
of SQL, while it simplifies the queries as follows: (1) dispensing with
operators and keywords such as GROUP BY, HAVING, FROM, JOIN ON, which are
usually hard to find counterparts for in the text descriptions; (2) removing
the need for nested subqueries and set operators; and (3) making schema linking
easier by reducing the required number of schema items. On Spider, a
challenging text-to-SQL benchmark that contains complex and nested SQL queries,
we demonstrate that NatSQL outperforms other IRs, and significantly improves
the performance of several previous SOTA models. Furthermore, for existing
models that do not support executable SQL generation, NatSQL easily enables
them to generate executable SQL queries, and achieves the new state-of-the-art
execution accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Underexplored Limitations of Cross-Domain Text-to-SQL Generalization. (arXiv:2109.05157v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05157">
<div class="article-summary-box-inner">
<span><p>Recently, there has been significant progress in studying neural networks for
translating text descriptions into SQL queries under the zero-shot cross-domain
setting. Despite achieving good performance on some public benchmarks, we
observe that existing text-to-SQL models do not generalize when facing domain
knowledge that does not frequently appear in the training data, which may
render the worse prediction performance for unseen domains. In this work, we
investigate the robustness of text-to-SQL models when the questions require
rarely observed domain knowledge. In particular, we define five types of domain
knowledge and introduce Spider-DK (DK is the abbreviation of domain knowledge),
a human-curated dataset based on the Spider benchmark for text-to-SQL
translation. NL questions in Spider-DK are selected from Spider, and we modify
some samples by adding domain knowledge that reflects real-world question
paraphrases. We demonstrate that the prediction accuracy dramatically drops on
samples that require such domain knowledge, even if the domain knowledge
appears in the training set, and the model provides the correct predictions for
related training samples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StreamHover: Livestream Transcript Summarization and Annotation. (arXiv:2109.05160v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05160">
<div class="article-summary-box-inner">
<span><p>With the explosive growth of livestream broadcasting, there is an urgent need
for new summarization technology that enables us to create a preview of
streamed content and tap into this wealth of knowledge. However, the problem is
nontrivial due to the informal nature of spoken language. Further, there has
been a shortage of annotated datasets that are necessary for transcript
summarization. In this paper, we present StreamHover, a framework for
annotating and summarizing livestream transcripts. With a total of over 500
hours of videos annotated with both extractive and abstractive summaries, our
benchmark dataset is significantly larger than currently existing annotated
corpora. We explore a neural extractive summarization model that leverages
vector-quantized variational autoencoder to learn latent vector representations
of spoken utterances and identify salient utterances from the transcripts to
form summaries. We show that our model generalizes better and improves
performance over strong baselines. The results of this study provide an avenue
for future research to improve summarization solutions for efficient browsing
of livestreams.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Categorization of Social Knowledge for Commonsense Question Answering. (arXiv:2109.05168v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05168">
<div class="article-summary-box-inner">
<span><p>Large pre-trained language models (PLMs) have led to great success on various
commonsense question answering (QA) tasks in an end-to-end fashion. However,
little attention has been paid to what commonsense knowledge is needed to
deeply characterize these QA tasks. In this work, we proposed to categorize the
semantics needed for these tasks using the SocialIQA as an example. Building
upon our labeled social knowledge categories dataset on top of SocialIQA, we
further train neural QA models to incorporate such social knowledge categories
and relation information from a knowledge base. Unlike previous work, we
observe our models with semantic categorizations of social knowledge can
achieve comparable performance with a relatively simple model and smaller size
compared to other complex approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">College Student Retention Risk Analysis From Educational Database using Multi-Task Multi-Modal Neural Fusion. (arXiv:2109.05178v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05178">
<div class="article-summary-box-inner">
<span><p>We develop a Multimodal Spatiotemporal Neural Fusion network for Multi-Task
Learning (MSNF-MTCL) to predict 5 important students' retention risks: future
dropout, next semester dropout, type of dropout, duration of dropout and cause
of dropout. First, we develop a general purpose multi-modal neural fusion
network model MSNF for learning students' academic information representation
by fusing spatial and temporal unstructured advising notes with spatiotemporal
structured data. MSNF combines a Bidirectional Encoder Representations from
Transformers (BERT)-based document embedding framework to represent each
advising note, Long-Short Term Memory (LSTM) network to model temporal advising
note embeddings, LSTM network to model students' temporal performance variables
and students' static demographics altogether. The final fused representation
from MSNF has been utilized on a Multi-Task Cascade Learning (MTCL) model
towards building MSNF-MTCL for predicting 5 student retention risks. We
evaluate MSNFMTCL on a large educational database consists of 36,445 college
students over 18 years period of time that provides promising performances
comparing with the nearest state-of-art models. Additionally, we test the
fairness of such model given the existence of biases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Asking Questions Like Educational Experts: Automatically Generating Question-Answer Pairs on Real-World Examination Data. (arXiv:2109.05179v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05179">
<div class="article-summary-box-inner">
<span><p>Generating high quality question-answer pairs is a hard but meaningful task.
Although previous works have achieved great results on answer-aware question
generation, it is difficult to apply them into practical application in the
education field. This paper for the first time addresses the question-answer
pair generation task on the real-world examination data, and proposes a new
unified framework on RACE. To capture the important information of the input
passage we first automatically generate(rather than extracting) keyphrases,
thus this task is reduced to keyphrase-question-answer triplet joint
generation. Accordingly, we propose a multi-agent communication model to
generate and optimize the question and keyphrases iteratively, and then apply
the generated question and keyphrases to guide the generation of answers. To
establish a solid benchmark, we build our model on the strong generative
pre-training model. Experimental results show that our model makes great
breakthroughs in the question-answer pair generation task. Moreover, we make a
comprehensive analysis on our model, suggesting new directions for this
challenging task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speaker-Oriented Latent Structures for Dialogue-Based Relation Extraction. (arXiv:2109.05182v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05182">
<div class="article-summary-box-inner">
<span><p>Dialogue-based relation extraction (DiaRE) aims to detect the structural
information from unstructured utterances in dialogues. Existing relation
extraction models may be unsatisfactory under such a conversational setting,
due to the entangled logic and information sparsity issues in utterances
involving multiple speakers. To this end, we introduce SOLS, a novel model
which can explicitly induce speaker-oriented latent structures for better
DiaRE. Specifically, we learn latent structures to capture the relationships
among tokens beyond the utterance boundaries, alleviating the entangled logic
issue. During the learning process, our speaker-specific regularization method
progressively highlights speaker-related key clues and erases the irrelevant
ones, alleviating the information sparsity issue. Experiments on three public
datasets demonstrate the effectiveness of our proposed approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MOMENTA: A Multimodal Framework for Detecting Harmful Memes and Their Targets. (arXiv:2109.05184v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05184">
<div class="article-summary-box-inner">
<span><p>Internet memes have become powerful means to transmit political,
psychological, and socio-cultural ideas. Although memes are typically humorous,
recent days have witnessed an escalation of harmful memes used for trolling,
cyberbullying, and abusing social entities. Detecting such harmful memes is
challenging as they can be highly satirical and cryptic. Moreover, while
previous work has focused on specific aspects of memes such as hate speech and
propaganda, there has been little work on harm in general, and only one
specialized dataset for it. Here, we focus on bridging this gap. In particular,
we aim to solve two novel tasks: detecting harmful memes and identifying the
social entities they target. We further extend the recently released HarMeme
dataset to generalize on two prevalent topics - COVID-19 and US politics and
name the two datasets as Harm-C and Harm-P, respectively. We then propose
MOMENTA (MultimOdal framework for detecting harmful MemEs aNd Their tArgets), a
novel multimodal (text + image) deep neural model, which uses global and local
perspectives to detect harmful memes. MOMENTA identifies the object proposals
and attributes and uses a multimodal model to perceive the comprehensive
context in which the objects and the entities are portrayed in a given meme.
MOMENTA is interpretable and generalizable, and it outperforms numerous
baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Total Recall: a Customized Continual Learning Method for Neural Semantic Parsers. (arXiv:2109.05186v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05186">
<div class="article-summary-box-inner">
<span><p>This paper investigates continual learning for semantic parsing. In this
setting, a neural semantic parser learns tasks sequentially without accessing
full training data from previous tasks. Direct application of the SOTA
continual learning algorithms to this problem fails to achieve comparable
performance with re-training models with all seen tasks because they have not
considered the special properties of structured outputs yielded by semantic
parsers. Therefore, we propose TotalRecall, a continual learning method
designed for neural semantic parsers from two aspects: i) a sampling method for
memory replay that diversifies logical form templates and balances
distributions of parse actions in a memory; ii) a two-stage training method
that significantly improves generalization capability of the parsers across
tasks. We conduct extensive experiments to study the research problems involved
in continual semantic parsing and demonstrate that a neural semantic parser
trained with TotalRecall achieves superior performance than the one trained
directly with the SOTA continual learning algorithms and achieve a 3-6 times
speedup compared to re-training from scratch. Code and datasets are available
at: https://github.com/zhuang-li/cl_nsp.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TopicRefine: Joint Topic Prediction and Dialogue Response Generation for Multi-turn End-to-End Dialogue System. (arXiv:2109.05187v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05187">
<div class="article-summary-box-inner">
<span><p>A multi-turn dialogue always follows a specific topic thread, and topic shift
at the discourse level occurs naturally as the conversation progresses,
necessitating the model's ability to capture different topics and generate
topic-aware responses. Previous research has either predicted the topic first
and then generated the relevant response, or simply applied the attention
mechanism to all topics, ignoring the joint distribution of the topic
prediction and response generation models and resulting in uncontrollable and
unrelated responses. In this paper, we propose a joint framework with a topic
refinement mechanism to learn these two tasks simultaneously. Specifically, we
design a three-pass iteration mechanism to generate coarse response first, then
predict corresponding topics, and finally generate refined response conditioned
on predicted topics. Moreover, we utilize GPT2DoubleHeads and BERT for the
topic prediction task respectively, aiming to investigate the effects of joint
learning and the understanding ability of GPT model. Experimental results
demonstrate that our proposed framework achieves new state-of-the-art
performance at response generation task and the great potential understanding
capability of GPT model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Eliciting Knowledge from Language Models for Event Extraction. (arXiv:2109.05190v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05190">
<div class="article-summary-box-inner">
<span><p>Eliciting knowledge contained in language models via prompt-based learning
has shown great potential in many natural language processing tasks, such as
text classification and generation. Whereas, the applications for more complex
tasks such as event extraction are less studied, since the design of prompt is
not straightforward due to the complicated types and arguments. In this paper,
we explore to elicit the knowledge from pre-trained language models for event
trigger detection and argument extraction. Specifically, we present various
joint trigger/argument prompt methods, which can elicit more complementary
knowledge by modeling the interactions between different triggers or arguments.
The experimental results on the benchmark dataset, namely ACE2005, show the
great advantages of our proposed approach. In particular, our approach is
superior to the recent advanced methods in the few-shot scenario where only a
few samples are used for training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Multi-modal Summarization. (arXiv:2109.05199v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05199">
<div class="article-summary-box-inner">
<span><p>The new era of technology has brought us to the point where it is convenient
for people to share their opinions over an abundance of platforms. These
platforms have a provision for the users to express themselves in multiple
forms of representations, including text, images, videos, and audio. This,
however, makes it difficult for users to obtain all the key information about a
topic, making the task of automatic multi-modal summarization (MMS) essential.
In this paper, we present a comprehensive survey of the existing research in
the area of MMS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncovering Main Causalities for Long-tailed Information Extraction. (arXiv:2109.05213v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05213">
<div class="article-summary-box-inner">
<span><p>Information Extraction (IE) aims to extract structural information from
unstructured texts. In practice, long-tailed distributions caused by the
selection bias of a dataset, may lead to incorrect correlations, also known as
spurious correlations, between entities and labels in the conventional
likelihood models. This motivates us to propose counterfactual IE (CFIE), a
novel framework that aims to uncover the main causalities behind data in the
view of causal inference. Specifically, 1) we first introduce a unified
structural causal model (SCM) for various IE tasks, describing the
relationships among variables; 2) with our SCM, we then generate
counterfactuals based on an explicit language structure to better calculate the
direct causal effect during the inference stage; 3) we further propose a novel
debiasing approach to yield more robust predictions. Experiments on three IE
tasks across five public datasets show the effectiveness of our CFIE model in
mitigating the spurious correlation issues.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Empirical Analysis of Training Strategies of Transformer-based Japanese Chit-chat Systems. (arXiv:2109.05217v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05217">
<div class="article-summary-box-inner">
<span><p>In recent years, several high-performance conversational systems have been
proposed based on the Transformer encoder-decoder model. Although previous
studies analyzed the effects of the model parameters and the decoding method on
subjective dialogue evaluations with overall metrics, they did not analyze how
the differences of fine-tuning datasets affect on user's detailed impression.
In addition, the Transformer-based approach has only been verified for English,
not for such languages with large inter-language distances as Japanese. In this
study, we develop large-scale Transformer-based Japanese dialogue models and
Japanese chit-chat datasets to examine the effectiveness of the
Transformer-based approach for building chit-chat dialogue systems. We
evaluated and analyzed the impressions of human dialogues in different
fine-tuning datasets, model parameters, and the use of additional information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdaK-NER: An Adaptive Top-K Approach for Named Entity Recognition with Incomplete Annotations. (arXiv:2109.05233v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05233">
<div class="article-summary-box-inner">
<span><p>State-of-the-art Named Entity Recognition(NER) models rely heavily on large
amountsof fully annotated training data. However, ac-cessible data are often
incompletely annotatedsince the annotators usually lack comprehen-sive
knowledge in the target domain. Normallythe unannotated tokens are regarded as
non-entities by default, while we underline thatthese tokens could either be
non-entities orpart of any entity. Here, we study NER mod-eling with incomplete
annotated data whereonly a fraction of the named entities are la-beled, and the
unlabeled tokens are equiva-lently multi-labeled by every possible label.Taking
multi-labeled tokens into account, thenumerous possible paths can distract the
train-ing model from the gold path (ground truthlabel sequence), and thus
hinders the learn-ing ability. In this paper, we propose AdaK-NER, named the
adaptive top-Kapproach, tohelp the model focus on a smaller feasible re-gion
where the gold path is more likely to belocated. We demonstrate the superiority
ofour approach through extensive experimentson both English and Chinese
datasets, aver-agely improving 2% in F-score on the CoNLL-2003 and over 10% on
two Chinese datasetscompared with the prior state-of-the-art works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prior Omission of Dissimilar Source Domain(s) for Cost-Effective Few-Shot Learning. (arXiv:2109.05234v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05234">
<div class="article-summary-box-inner">
<span><p>Few-shot slot tagging is an emerging research topic in the field of Natural
Language Understanding (NLU). With sufficient annotated data from source
domains, the key challenge is how to train and adapt the model to another
target domain which only has few labels. Conventional few-shot approaches use
all the data from the source domains without considering inter-domain relations
and implicitly assume each sample in the domain contributes equally. However,
our experiments show that the data distribution bias among different domains
will significantly affect the adaption performance. Moreover, transferring
knowledge from dissimilar domains will even introduce some extra noises so that
affect the performance of models. To tackle this problem, we propose an
effective similarity-based method to select data from the source domains. In
addition, we propose a Shared-Private Network (SP-Net) for the few-shot slot
tagging task. The words from the same class would have some shared features. We
extract those shared features from the limited annotated data on the target
domain and merge them together as the label embedding to help us predict other
unlabelled data on the target domain. The experiment shows that our method
outperforms the state-of-the-art approaches with fewer source data. The result
also proves that some training data from dissimilar sources are redundant and
even negative for the adaption.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Universal Simultaneous Machine Translation with Mixture-of-Experts Wait-k Policy. (arXiv:2109.05238v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05238">
<div class="article-summary-box-inner">
<span><p>Simultaneous machine translation (SiMT) generates translation before reading
the entire source sentence and hence it has to trade off between translation
quality and latency. To fulfill the requirements of different translation
quality and latency in practical applications, the previous methods usually
need to train multiple SiMT models for different latency levels, resulting in
large computational costs. In this paper, we propose a universal SiMT model
with Mixture-of-Experts Wait-k Policy to achieve the best translation quality
under arbitrary latency with only one trained model. Specifically, our method
employs multi-head attention to accomplish the mixture of experts where each
head is treated as a wait-k expert with its own waiting words number, and given
a test latency and source inputs, the weights of the experts are accordingly
adjusted to produce the best translation. Experiments on three datasets show
that our method outperforms all the strong baselines under different latency,
including the state-of-the-art adaptive policy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Concentrated Cross-Attention for Neural Machine Translation with Gaussian Mixture Model. (arXiv:2109.05244v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05244">
<div class="article-summary-box-inner">
<span><p>Cross-attention is an important component of neural machine translation
(NMT), which is always realized by dot-product attention in previous methods.
However, dot-product attention only considers the pair-wise correlation between
words, resulting in dispersion when dealing with long sentences and neglect of
source neighboring relationships. Inspired by linguistics, the above issues are
caused by ignoring a type of cross-attention, called concentrated attention,
which focuses on several central words and then spreads around them. In this
work, we apply Gaussian Mixture Model (GMM) to model the concentrated attention
in cross-attention. Experiments and analyses we conducted on three datasets
show that the proposed method outperforms the baseline and has significant
improvement on alignment quality, N-gram accuracy, and long sentence
translation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Qualitative and Quantitative Analysis of Diversity in Cross-document Coreference Resolution Datasets. (arXiv:2109.05250v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05250">
<div class="article-summary-box-inner">
<span><p>Cross-document coreference resolution (CDCR) datasets, such as ECB+, contain
manually annotated event-centric mentions of events and entities that form
coreference chains with identity relations. ECB+ is a state-of-the-art CDCR
dataset that focuses on the resolution of events and their descriptive
attributes, i.e., actors, location, and date-time. NewsWCL50 is a dataset that
annotates coreference chains of both events and entities with a strong variance
of word choice and more loosely-related coreference anaphora, e.g., bridging or
near-identity relations. In this paper, we qualitatively and quantitatively
compare annotation schemes of ECB+ and NewsWCL50 with multiple criteria. We
propose a phrasing diversity metric (PD) that compares lexical diversity within
coreference chains on a more detailed level than previously proposed metric,
e.g., a number of unique lemmas. We discuss the different tasks that both CDCR
datasets create, i.e., lexical disambiguation and lexical diversity challenges,
and propose a direction for further CDCR evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">XCoref: Cross-document Coreference Resolution in the Wild. (arXiv:2109.05252v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05252">
<div class="article-summary-box-inner">
<span><p>Datasets and methods for cross-document coreference resolution (CDCR) focus
on events or entities with strict coreference relations. They lack, however,
annotating and resolving coreference mentions with more abstract or loose
relations that may occur when news articles report about controversial and
polarized events. Bridging and loose coreference relations trigger associations
that may lead to exposing news readers to bias by word choice and labeling. For
example, coreferential mentions of "direct talks between U.S. President Donald
Trump and Kim" such as "an extraordinary meeting following months of heated
rhetoric" or "great chance to solve a world problem" form a more positive
perception of this event. A step towards bringing awareness of bias by word
choice and labeling is the reliable resolution of coreferences with high
lexical diversity. We propose an unsupervised method named XCoref, which is a
CDCR method that capably resolves not only previously prevalent entities, such
as persons, e.g., "Donald Trump," but also abstractly defined concepts, such as
groups of persons, "caravan of immigrants," events and actions, e.g., "marching
to the U.S. border." In an extensive evaluation, we compare the proposed XCoref
to a state-of-the-art CDCR method and a previous method TCA that resolves such
complex coreference relations and find that XCoref outperforms these methods.
Outperforming an established CDCR model shows that the new CDCR models need to
be evaluated on semantically complex mentions with more loose coreference
relations to indicate their applicability of models to resolve mentions in the
"wild" of political news articles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Translation via Grafting Pre-trained Language Models. (arXiv:2109.05256v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05256">
<div class="article-summary-box-inner">
<span><p>Can pre-trained BERT for one language and GPT for another be glued together
to translate texts? Self-supervised training using only monolingual data has
led to the success of pre-trained (masked) language models in many NLP tasks.
However, directly connecting BERT as an encoder and GPT as a decoder can be
challenging in machine translation, for GPT-like models lack a cross-attention
component that is needed in seq2seq decoders. In this paper, we propose
Graformer to graft separately pre-trained (masked) language models for machine
translation. With monolingual data for pre-training and parallel data for
grafting training, we maximally take advantage of the usage of both types of
data. Experiments on 60 directions show that our method achieves average
improvements of 5.8 BLEU in x2en and 2.9 BLEU in en2x directions comparing with
the multilingual Transformer of the same size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COSMic: A Coherence-Aware Generation Metric for Image Descriptions. (arXiv:2109.05281v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05281">
<div class="article-summary-box-inner">
<span><p>Developers of text generation models rely on automated evaluation metrics as
a stand-in for slow and expensive manual evaluations. However, image captioning
metrics have struggled to give accurate learned estimates of the semantic and
pragmatic success of output text. We address this weakness by introducing the
first discourse-aware learned generation metric for evaluating image
descriptions. Our approach is inspired by computational theories of discourse
for capturing information goals using coherence. We present a dataset of
image$\unicode{x2013}$description pairs annotated with coherence relations. We
then train a coherence-aware metric on a subset of the Conceptual Captions
dataset and measure its effectiveness$\unicode{x2014}$its ability to predict
human ratings of output captions$\unicode{x2014}$on a test set composed of
out-of-domain images. We demonstrate a higher Kendall Correlation Coefficient
for our proposed metric with the human judgments for the results of a number of
state-of-the-art coherence-aware caption generation models when compared to
several other metrics including recently proposed learned metrics such as
BLEURT and BERTScore.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What's in a Name? Answer Equivalence For Open-Domain Question Answering. (arXiv:2109.05289v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05289">
<div class="article-summary-box-inner">
<span><p>A flaw in QA evaluation is that annotations often only provide one gold
answer. Thus, model predictions semantically equivalent to the answer but
superficially different are considered incorrect. This work explores mining
alias entities from knowledge bases and using them as additional gold answers
(i.e., equivalent answers). We incorporate answers for two settings: evaluation
with additional answers and model training with equivalent answers. We analyse
three QA benchmarks: Natural Questions, TriviaQA, and SQuAD. Answer expansion
increases the exact match score on all datasets for evaluation, while
incorporating it helps model training over real-world datasets. We ensure the
additional answers are valid through a human post hoc evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Looking for Confirmations: An Effective and Human-Like Visual Dialogue Strategy. (arXiv:2109.05312v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05312">
<div class="article-summary-box-inner">
<span><p>Generating goal-oriented questions in Visual Dialogue tasks is a challenging
and long-standing problem. State-Of-The-Art systems are shown to generate
questions that, although grammatically correct, often lack an effective
strategy and sound unnatural to humans. Inspired by the cognitive literature on
information search and cross-situational word learning, we design Confirm-it, a
model based on a beam search re-ranking algorithm that guides an effective
goal-oriented strategy by asking questions that confirm the model's conjecture
about the referent. We take the GuessWhat?! game as a case-study. We show that
dialogues generated by Confirm-it are more natural and effective than beam
search decoding without re-ranking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bayesian Topic Regression for Causal Inference. (arXiv:2109.05317v1 [stat.ML])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05317">
<div class="article-summary-box-inner">
<span><p>Causal inference using observational text data is becoming increasingly
popular in many research areas. This paper presents the Bayesian Topic
Regression (BTR) model that uses both text and numerical information to model
an outcome variable. It allows estimation of both discrete and continuous
treatment effects. Furthermore, it allows for the inclusion of additional
numerical confounding factors next to text data. To this end, we combine a
supervised Bayesian topic model with a Bayesian regression framework and
perform supervised representation learning for the text features jointly with
the regression parameter training, respecting the Frisch-Waugh-Lovell theorem.
Our paper makes two main contributions. First, we provide a regression
framework that allows causal inference in settings when both text and numerical
confounders are of relevance. We show with synthetic and semi-synthetic
datasets that our joint approach recovers ground truth with lower bias than any
benchmark model, when text and numerical features are correlated. Second,
experiments on two real-world datasets demonstrate that a joint and supervised
learning strategy also yields superior prediction results compared to
strategies that estimate regression weights for text and non-text features
separately, being even competitive with more complex deep neural networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Latent Hatred: A Benchmark for Understanding Implicit Hate Speech. (arXiv:2109.05322v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05322">
<div class="article-summary-box-inner">
<span><p>Hate speech has grown significantly on social media, causing serious
consequences for victims of all demographics. Despite much attention being paid
to characterize and detect discriminatory speech, most work has focused on
explicit or overt hate speech, failing to address a more pervasive form based
on coded or indirect language. To fill this gap, this work introduces a
theoretically-justified taxonomy of implicit hate speech and a benchmark corpus
with fine-grained labels for each message and its implication. We present
systematic analyses of our dataset using contemporary baselines to detect and
explain implicit hate speech, and we discuss key features that challenge
existing models. This dataset will continue to serve as a useful benchmark for
understanding this multifaceted issue.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">To Protect and To Serve? Analyzing Entity-Centric Framing of Police Violence. (arXiv:2109.05325v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05325">
<div class="article-summary-box-inner">
<span><p>Framing has significant but subtle effects on public opinion and policy. We
propose an NLP framework to measure entity-centric frames. We use it to
understand media coverage on police violence in the United States in a new
Police Violence Frames Corpus of 82k news articles spanning 7k police killings.
Our work uncovers more than a dozen framing devices and reveals significant
differences in the way liberal and conservative news sources frame both the
issue of police violence and the entities involved. Conservative sources
emphasize when the victim is armed or attacking an officer and are more likely
to mention the victim's criminal record. Liberal sources focus more on the
underlying systemic injustice, highlighting the victim's race and that they
were unarmed. We discover temporary spikes in these injustice frames near
high-profile shooting events, and finally, we show protest volume correlates
with and precedes media framing decisions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Objective Metric for Explainable AI: How and Why to Estimate the Degree of Explainability. (arXiv:2109.05327v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05327">
<div class="article-summary-box-inner">
<span><p>Numerous government initiatives (e.g. the EU with GDPR) are coming to the
conclusion that the increasing complexity of modern software systems must be
contrasted with some Rights to Explanation and metrics for the Impact
Assessment of these tools, that allow humans to understand and oversee the
output of Automated Decision Making systems. Explainable AI was born as a
pathway to allow humans to explore and understand the inner working of complex
systems. But establishing what is an explanation and objectively evaluating
explainability, are not trivial tasks. With this paper, we present a new
model-agnostic metric to measure the Degree of eXplainability of correct
information in an objective way, exploiting a specific model from Ordinary
Language Philosophy called the Achinstein's Theory of Explanations. In order to
understand whether this metric is actually behaving as explainability is
expected to, we designed a few experiments and a user-study on two realistic
AI-based systems for healthcare and finance, involving famous AI technology
including Artificial Neural Networks and TreeSHAP. The results we obtained are
very encouraging, suggesting that our proposed metric for measuring the Degree
of eXplainability is robust on several scenarios and it can be eventually
exploited for a lawful Impact Assessment of an Automated Decision Making
system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling and Acceleration of Three-dimensional Structure Determination for Single-Particle Imaging Experiments with SpiniFEL. (arXiv:2109.05339v1 [physics.comp-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05339">
<div class="article-summary-box-inner">
<span><p>The Linac Coherent Light Source (LCLS) is an X- ray free electron laser
(XFEL) facility enabling the study of the structure and dynamics of single
macromolecules. A major upgrade will bring the repetition rate of the X-ray
source from 120 to 1 million pulses per second. Exascale high performance
computing (HPC) capabilities will be required to process the corresponding data
rates. We present SpiniFEL, an application used for structure determination of
proteins from single-particle imaging (SPI) experiments. An emerging technique
for imaging individual proteins and other large molecular complexes by
outrunning radiation damage, SPI breaks free from the need for crystallization
(which is difficult for some proteins) and allows for imaging molecular
dynamics at near ambient conditions. SpiniFEL is being developed to run on
supercomputers in near real-time while an experiment is taking place, so that
the feedback about the data can guide the data collection strategy. We describe
here how we reformulated the mathematical framework for parallelizable
implementation and accelerated the most compute intensive parts of the
application. We also describe the use of Pygion, a Python interface for the
Legion task-based programming model and compare to our existing MPI+GPU
implementation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HYDRA -- Hyper Dependency Representation Attentions. (arXiv:2109.05349v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05349">
<div class="article-summary-box-inner">
<span><p>Attention is all we need as long as we have enough data. Even so, it is
sometimes not easy to determine how much data is enough while the models are
becoming larger and larger. In this paper, we propose HYDRA heads, lightweight
pretrained linguistic self-attention heads to inject knowledge into transformer
models without pretraining them again. Our approach is a balanced paradigm
between leaving the models to learn unsupervised and forcing them to conform to
linguistic knowledge rigidly as suggested in previous studies. Our experiment
proves that the approach is not only the boost performance of the model but
also lightweight and architecture friendly. We empirically verify our framework
on benchmark datasets to show the contribution of linguistic knowledge to a
transformer model. This is a promising result for a new approach to
transferring knowledge from linguistic resources into transformer-based models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning from Language Description: Low-shot Named Entity Recognition via Decomposed Framework. (arXiv:2109.05357v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05357">
<div class="article-summary-box-inner">
<span><p>In this work, we study the problem of named entity recognition (NER) in a low
resource scenario, focusing on few-shot and zero-shot settings. Built upon
large-scale pre-trained language models, we propose a novel NER framework,
namely SpanNER, which learns from natural language supervision and enables the
identification of never-seen entity classes without using in-domain labeled
data. We perform extensive experiments on 5 benchmark datasets and evaluate the
proposed method in the few-shot learning, domain transfer and zero-shot
learning settings. The experimental results show that the proposed method can
bring 10%, 23% and 26% improvements in average over the best baselines in
few-shot learning, domain transfer and zero-shot learning settings
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Implicit Premise Generation with Discourse-aware Commonsense Knowledge Models. (arXiv:2109.05358v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05358">
<div class="article-summary-box-inner">
<span><p>Enthymemes are defined as arguments where a premise or conclusion is left
implicit. We tackle the task of generating the implicit premise in an
enthymeme, which requires not only an understanding of the stated conclusion
and premise but also additional inferences that could depend on commonsense
knowledge. The largest available dataset for enthymemes (Habernal et al., 2018)
consists of 1.7k samples, which is not large enough to train a neural text
generation model. To address this issue, we take advantage of a similar task
and dataset: Abductive reasoning in narrative text (Bhagavatula et al., 2020).
However, we show that simply using a state-of-the-art seq2seq model fine-tuned
on this data might not generate meaningful implicit premises associated with
the given enthymemes. We demonstrate that encoding discourse-aware commonsense
during fine-tuning improves the quality of the generated implicit premises and
outperforms all other baselines both in automatic and human evaluations on
three different datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COMBO: State-of-the-Art Morphosyntactic Analysis. (arXiv:2109.05361v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05361">
<div class="article-summary-box-inner">
<span><p>We introduce COMBO - a fully neural NLP system for accurate part-of-speech
tagging, morphological analysis, lemmatisation, and (enhanced) dependency
parsing. It predicts categorical morphosyntactic features whilst also exposes
their vector representations, extracted from hidden layers. COMBO is an easy to
install Python package with automatically downloadable pre-trained models for
over 40 languages. It maintains a balance between efficiency and quality. As it
is an end-to-end system and its modules are jointly trained, its training is
competitively fast. As its models are optimised for accuracy, they achieve
often better prediction quality than SOTA. The COMBO library is available at:
https://gitlab.clarin-pl.eu/syntactic-tools/combo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modular Self-Supervision for Document-Level Relation Extraction. (arXiv:2109.05362v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05362">
<div class="article-summary-box-inner">
<span><p>Extracting relations across large text spans has been relatively
underexplored in NLP, but it is particularly important for high-value domains
such as biomedicine, where obtaining high recall of the latest findings is
crucial for practical applications. Compared to conventional information
extraction confined to short text spans, document-level relation extraction
faces additional challenges in both inference and learning. Given longer text
spans, state-of-the-art neural architectures are less effective and
task-specific self-supervision such as distant supervision becomes very noisy.
In this paper, we propose decomposing document-level relation extraction into
relation detection and argument resolution, taking inspiration from Davidsonian
semantics. This enables us to incorporate explicit discourse modeling and
leverage modular self-supervision for each sub-problem, which is less
noise-prone and can be further refined end-to-end via variational EM. We
conduct a thorough evaluation in biomedical machine reading for precision
oncology, where cross-paragraph relation mentions are prevalent. Our method
outperforms prior state of the art, such as multi-scale learning and graph
neural networks, by over 20 absolute F1 points. The gain is particularly
pronounced among the most challenging relation instances whose arguments never
co-occur in a paragraph.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Impact of Positional Encodings on Multilingual Compression. (arXiv:2109.05388v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05388">
<div class="article-summary-box-inner">
<span><p>In order to preserve word-order information in a non-autoregressive setting,
transformer architectures tend to include positional knowledge, by (for
instance) adding positional encodings to token embeddings. Several
modifications have been proposed over the sinusoidal positional encodings used
in the original transformer architecture; these include, for instance,
separating position encodings and token embeddings, or directly modifying
attention weights based on the distance between word pairs. We first show that
surprisingly, while these modifications tend to improve monolingual language
models, none of them result in better multilingual language models. We then
answer why that is: Sinusoidal encodings were explicitly designed to facilitate
compositionality by allowing linear projections over arbitrary time steps.
Higher variances in multilingual training distributions requires higher
compression, in which case, compositionality becomes indispensable. Learned
absolute positional encodings (e.g., in mBERT) tend to approximate sinusoidal
embeddings in multilingual settings, but more complex positional encoding
architectures lack the inductive bias to effectively learn compositionality and
cross-lingual alignment. In other words, while sinusoidal positional encodings
were originally designed for monolingual applications, they are particularly
useful in multilingual language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Table Content for Zero-shot Text-to-SQL with Meta-Learning. (arXiv:2109.05395v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05395">
<div class="article-summary-box-inner">
<span><p>Single-table text-to-SQL aims to transform a natural language question into a
SQL query according to one single table. Recent work has made promising
progress on this task by pre-trained language models and a multi-submodule
framework. However, zero-shot table, that is, the invisible table in the
training set, is currently the most critical bottleneck restricting the
application of existing approaches to real-world scenarios. Although some work
has utilized auxiliary tasks to help handle zero-shot tables, expensive extra
manual annotation limits their practicality. In this paper, we propose a new
approach for the zero-shot text-to-SQL task which does not rely on any
additional manual annotations. Our approach consists of two parts. First, we
propose a new model that leverages the abundant information of table content to
help establish the mapping between questions and zero-shot tables. Further, we
propose a simple but efficient meta-learning strategy to train our model. The
strategy utilizes the two-step gradient update to force the model to learn a
generalization ability towards zero-shot tables. We conduct extensive
experiments on a public open-domain text-to-SQL dataset WikiSQL and a
domain-specific dataset ESQL. Compared to existing approaches using the same
pre-trained model, our approach achieves significant improvements on both
datasets. Compared to the larger pre-trained model and the tabular-specific
pre-trained model, our approach is still competitive. More importantly, on the
zero-shot subsets of both the datasets, our approach further increases the
improvements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Guiding Topic Flows in the Generative Chatbot by Enhancing the ConceptNet with the Conversation Corpora. (arXiv:2109.05406v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05406">
<div class="article-summary-box-inner">
<span><p>Human conversations consist of reasonable and natural topic flows, which are
observed as the shifts of the mentioned concepts across utterances. Previous
chatbots that incorporate the external commonsense knowledge graph prove that
modeling the concept shifts can effectively alleviate the dull and
uninformative response dilemma. However, there still exists a gap between the
concept relations in the natural conversation and those in the external
commonsense knowledge graph, which is an issue to solve. Specifically, the
concept relations in the external commonsense knowledge graph are not
intuitively built from the conversational scenario but the world knowledge,
which makes them insufficient for the chatbot construction. To bridge the above
gap, we propose the method to supply more concept relations extracted from the
conversational corpora and reconstruct an enhanced concept graph for the
chatbot construction. In addition, we present a novel, powerful, and fast graph
encoding architecture named the Edge-Transformer to replace the traditional GNN
architecture. Experimental results on the Reddit conversation dataset indicate
our proposed method significantly outperforms strong baseline systems and
achieves new SOTA results. Further analysis individually proves the
effectiveness of the enhanced concept graph and the Edge-Transformer
architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pairwise Supervised Contrastive Learning of Sentence Representations. (arXiv:2109.05424v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05424">
<div class="article-summary-box-inner">
<span><p>Many recent successes in sentence representation learning have been achieved
by simply fine-tuning on the Natural Language Inference (NLI) datasets with
triplet loss or siamese loss. Nevertheless, they share a common weakness:
sentences in a contradiction pair are not necessarily from different semantic
categories. Therefore, optimizing the semantic entailment and contradiction
reasoning objective alone is inadequate to capture the high-level semantic
structure. The drawback is compounded by the fact that the vanilla siamese or
triplet losses only learn from individual sentence pairs or triplets, which
often suffer from bad local optima. In this paper, we propose PairSupCon, an
instance discrimination based approach aiming to bridge semantic entailment and
contradiction understanding with high-level categorical concept encoding. We
evaluate PairSupCon on various downstream tasks that involve understanding
sentence semantics at different granularities. We outperform the previous
state-of-the-art method with $10\%$--$13\%$ averaged improvement on eight
clustering tasks, and $5\%$--$6\%$ averaged improvement on seven semantic
textual similarity (STS) tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Not All Negatives are Equal: Label-Aware Contrastive Loss for Fine-grained Text Classification. (arXiv:2109.05427v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05427">
<div class="article-summary-box-inner">
<span><p>Fine-grained classification involves dealing with datasets with larger number
of classes with subtle differences between them. Guiding the model to focus on
differentiating dimensions between these commonly confusable classes is key to
improving performance on fine-grained tasks. In this work, we analyse the
contrastive fine-tuning of pre-trained language models on two fine-grained text
classification tasks, emotion classification and sentiment analysis. We
adaptively embed class relationships into a contrastive objective function to
help differently weigh the positives and negatives, and in particular,
weighting closely confusable negatives more than less similar negative
examples. We find that Label-aware Contrastive Loss outperforms previous
contrastive methods, in the presence of larger number and/or more confusable
classes, and helps models to produce output distributions that are more
differentiated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are Gender-Neutral Queries Really Gender-Neutral? Mitigating Gender Bias in Image Search. (arXiv:2109.05433v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05433">
<div class="article-summary-box-inner">
<span><p>Internet search affects people's cognition of the world, so mitigating biases
in search results and learning fair models is imperative for social good. We
study a unique gender bias in image search in this work: the search images are
often gender-imbalanced for gender-neutral natural language queries. We
diagnose two typical image search models, the specialized model trained on
in-domain datasets and the generalized representation model pre-trained on
massive image and text data across the internet. Both models suffer from severe
gender bias. Therefore, we introduce two novel debiasing approaches: an
in-processing fair sampling method to address the gender imbalance issue for
training models, and a post-processing feature clipping method base on mutual
information to debias multimodal representations of pre-trained models.
Extensive experiments on MS-COCO and Flickr30K benchmarks show that our methods
significantly reduce the gender bias in image search models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"Let Your Characters Tell Their Story": A Dataset for Character-Centric Narrative Understanding. (arXiv:2109.05438v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05438">
<div class="article-summary-box-inner">
<span><p>When reading a literary piece, readers often make inferences about various
characters' roles, personalities, relationships, intents, actions, etc. While
humans can readily draw upon their past experiences to build such a
character-centric view of the narrative, understanding characters in narratives
can be a challenging task for machines. To encourage research in this field of
character-centric narrative understanding, we present LiSCU -- a new dataset of
literary pieces and their summaries paired with descriptions of characters that
appear in them. We also introduce two new tasks on LiSCU: Character
Identification and Character Description Generation. Our experiments with
several pre-trained language models adapted for these tasks demonstrate that
there is a need for better models of narrative comprehension.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Conversational Search for Online Shopping with Utterance Transfer. (arXiv:2109.05460v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05460">
<div class="article-summary-box-inner">
<span><p>Successful conversational search systems can present natural, adaptive and
interactive shopping experience for online shopping customers. However,
building such systems from scratch faces real word challenges from both
imperfect product schema/knowledge and lack of training dialog data.In this
work we first propose ConvSearch, an end-to-end conversational search system
that deeply combines the dialog system with search. It leverages the text
profile to retrieve products, which is more robust against imperfect product
schema/knowledge compared with using product attributes alone. We then address
the lack of data challenges by proposing an utterance transfer approach that
generates dialogue utterances by using existing dialog from other domains, and
leveraging the search behavior data from e-commerce retailer. With utterance
transfer, we introduce a new conversational search dataset for online shopping.
Experiments show that our utterance transfer method can significantly improve
the availability of training dialogue data without crowd-sourcing, and the
conversational search system significantly outperformed the best tested
baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Logic Traps in Evaluating Post-hoc Interpretations. (arXiv:2109.05463v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05463">
<div class="article-summary-box-inner">
<span><p>Post-hoc interpretation aims to explain a trained model and reveal how the
model arrives at a decision. Though research on post-hoc interpretations has
developed rapidly, one growing pain in this field is the difficulty in
evaluating interpretations. There are some crucial logic traps behind existing
evaluation methods, which are ignored by most works. In this opinion piece, we
summarize four kinds evaluation methods and point out the corresponding logic
traps behind them. We argue that we should be clear about these traps rather
than ignore them and draw conclusions assertively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Task Difficulty for Few-Shot Relation Extraction. (arXiv:2109.05473v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05473">
<div class="article-summary-box-inner">
<span><p>Few-shot relation extraction (FSRE) focuses on recognizing novel relations by
learning with merely a handful of annotated instances. Meta-learning has been
widely adopted for such a task, which trains on randomly generated few-shot
tasks to learn generic data representations. Despite impressive results
achieved, existing models still perform suboptimally when handling hard FSRE
tasks, where the relations are fine-grained and similar to each other. We argue
this is largely because existing models do not distinguish hard tasks from easy
ones in the learning process. In this paper, we introduce a novel approach
based on contrastive learning that learns better representations by exploiting
relation label information. We further design a method that allows the model to
adaptively learn how to focus on hard tasks. Experiments on two standard
datasets demonstrate the effectiveness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stylistic Retrieval-based Dialogue System with Unparallel Training Data. (arXiv:2109.05477v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05477">
<div class="article-summary-box-inner">
<span><p>The ability of a dialog system to express consistent language style during
conversations has a direct, positive impact on its usability and on user
satisfaction. Although previous studies have demonstrated that style transfer
is feasible with a large amount of parallel data, it is often impossible to
collect such data for different styles. In this paper, instead of manually
constructing conversation data with a certain style, we propose a flexible
framework that adapts a generic retrieval-based dialogue system to mimic the
language style of a specified persona without any parallel data. Our approach
is based on automatic generation of stylized data by learning the usage of
jargon, and then rewriting the generic conversations to a stylized one by
incorporating the jargon. In experiments we implemented dialogue systems with
five distinct language styles, and the result shows our framework significantly
outperforms baselines in terms of the average score of responses' relevance and
style degree, and content diversity. A/B testing on a commercial chatbot shows
that users are more satisfied with our system. This study demonstrates the
feasibility of building stylistic dialogue systems by simple data augmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Enhanced Fine-Tuning for Better Handling Unseen Entities in Dialogue Generation. (arXiv:2109.05487v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05487">
<div class="article-summary-box-inner">
<span><p>Although pre-training models have achieved great success in dialogue
generation, their performance drops dramatically when the input contains an
entity that does not appear in pre-training and fine-tuning datasets (unseen
entity). To address this issue, existing methods leverage an external knowledge
base to generate appropriate responses. In real-world scenario, the entity may
not be included by the knowledge base or suffer from the precision of knowledge
retrieval. To deal with this problem, instead of introducing knowledge base as
the input, we force the model to learn a better semantic representation by
predicting the information in the knowledge base, only based on the input
context. Specifically, with the help of a knowledge base, we introduce two
auxiliary training objectives: 1) Interpret Masked Word, which conjectures the
meaning of the masked entity given the context; 2) Hypernym Generation, which
predicts the hypernym of the entity based on the context. Experiment results on
two dialogue corpus verify the effectiveness of our methods under both
knowledge available and unavailable settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Domain Adaptation Schemes for Building ASR in Low-resource Languages. (arXiv:2109.05494v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05494">
<div class="article-summary-box-inner">
<span><p>Building an automatic speech recognition (ASR) system from scratch requires a
large amount of annotated speech data, which is difficult to collect in many
languages. However, there are cases where the low-resource language shares a
common acoustic space with a high-resource language having enough annotated
data to build an ASR. In such cases, we show that the domain-independent
acoustic models learned from the high-resource language through unsupervised
domain adaptation (UDA) schemes can enhance the performance of the ASR in the
low-resource language. We use the specific example of Hindi in the source
domain and Sanskrit in the target domain. We explore two architectures: i)
domain adversarial training using gradient reversal layer (GRL) and ii) domain
separation networks (DSN). The GRL and DSN architectures give absolute
improvements of 6.71% and 7.32%, respectively, in word error rate over the
baseline deep neural network model when trained on just 5.5 hours of data in
the target domain. We also show that choosing a proper language (Telugu) in the
source domain can bring further improvement. The results suggest that UDA
schemes can be helpful in the development of ASR systems for low-resource
languages, mitigating the hassle of collecting large amounts of annotated
speech data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TEASEL: A Transformer-Based Speech-Prefixed Language Model. (arXiv:2109.05522v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05522">
<div class="article-summary-box-inner">
<span><p>Multimodal language analysis is a burgeoning field of NLP that aims to
simultaneously model a speaker's words, acoustical annotations, and facial
expressions. In this area, lexicon features usually outperform other modalities
because they are pre-trained on large corpora via Transformer-based models.
Despite their strong performance, training a new self-supervised learning (SSL)
Transformer on any modality is not usually attainable due to insufficient data,
which is the case in multimodal language learning. This work proposes a
Transformer-Based Speech-Prefixed Language Model called TEASEL to approach the
mentioned constraints without training a complete Transformer model. TEASEL
model includes speech modality as a dynamic prefix besides the textual modality
compared to a conventional language model. This method exploits a conventional
pre-trained language model as a cross-modal Transformer model. We evaluated
TEASEL for the multimodal sentiment analysis task defined by CMU-MOSI dataset.
Extensive experiments show that our model outperforms unimodal baseline
language models by 4% and outperforms the current multimodal state-of-the-art
(SoTA) model by 1% in F1-score. Additionally, our proposed method is 72%
smaller than the SoTA model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Constructing Phrase-level Semantic Labels to Form Multi-Grained Supervision for Image-Text Retrieval. (arXiv:2109.05523v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05523">
<div class="article-summary-box-inner">
<span><p>Existing research for image text retrieval mainly relies on sentence-level
supervision to distinguish matched and mismatched sentences for a query image.
However, semantic mismatch between an image and sentences usually happens in
finer grain, i.e., phrase level. In this paper, we explore to introduce
additional phrase-level supervision for the better identification of mismatched
units in the text. In practice, multi-grained semantic labels are automatically
constructed for a query image in both sentence-level and phrase-level. We
construct text scene graphs for the matched sentences and extract entities and
triples as the phrase-level labels. In order to integrate both supervision of
sentence-level and phrase-level, we propose Semantic Structure Aware Multimodal
Transformer (SSAMT) for multi-modal representation learning. Inside the SSAMT,
we utilize different kinds of attention mechanisms to enforce interactions of
multi-grain semantic units in both sides of vision and language. For the
training, we propose multi-scale matching losses from both global and local
perspectives, and penalize mismatched phrases. Experimental results on MS-COCO
and Flickr30K show the effectiveness of our approach compared to some
state-of-the-art models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extracting Event Temporal Relations via Hyperbolic Geometry. (arXiv:2109.05527v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05527">
<div class="article-summary-box-inner">
<span><p>Detecting events and their evolution through time is a crucial task in
natural language understanding. Recent neural approaches to event temporal
relation extraction typically map events to embeddings in the Euclidean space
and train a classifier to detect temporal relations between event pairs.
However, embeddings in the Euclidean space cannot capture richer asymmetric
relations such as event temporal relations. We thus propose to embed events
into hyperbolic spaces, which are intrinsically oriented at modeling
hierarchical structures. We introduce two approaches to encode events and their
temporal relations in hyperbolic spaces. One approach leverages hyperbolic
embeddings to directly infer event relations through simple geometrical
operations. In the second one, we devise an end-to-end architecture composed of
hyperbolic neural units tailored for the temporal relation extraction task.
Thorough experimental assessments on widely used datasets have shown the
benefits of revisiting the tasks on a different geometrical space, resulting in
state-of-the-art performance on several standard metrics. Finally, the ablation
study and several qualitative analyses highlighted the rich event semantics
implicitly encoded into hyperbolic spaces.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Good-Enough Example Extrapolation. (arXiv:2109.05602v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05602">
<div class="article-summary-box-inner">
<span><p>This paper asks whether extrapolating the hidden space distribution of text
examples from one class onto another is a valid inductive bias for data
augmentation. To operationalize this question, I propose a simple data
augmentation protocol called "good-enough example extrapolation" (GE3). GE3 is
lightweight and has no hyperparameters. Applied to three text classification
datasets for various data imbalance scenarios, GE3 improves performance more
than upsampling and other hidden-space data augmentation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Levenshtein Training for Word-level Quality Estimation. (arXiv:2109.05611v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05611">
<div class="article-summary-box-inner">
<span><p>We propose a novel scheme to use the Levenshtein Transformer to perform the
task of word-level quality estimation. A Levenshtein Transformer is a natural
fit for this task: trained to perform decoding in an iterative manner, a
Levenshtein Transformer can learn to post-edit without explicit supervision. To
further minimize the mismatch between the translation task and the word-level
QE task, we propose a two-stage transfer learning procedure on both augmented
data and human post-editing data. We also propose heuristics to construct
reference labels that are compatible with subword-level finetuning and
inference. Results on WMT 2020 QE shared task dataset show that our proposed
method has superior data efficiency under the data-constrained setting and
competitive performance under the unconstrained setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RockNER: A Simple Method to Create Adversarial Examples for Evaluating the Robustness of Named Entity Recognition Models. (arXiv:2109.05620v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05620">
<div class="article-summary-box-inner">
<span><p>To audit the robustness of named entity recognition (NER) models, we propose
RockNER, a simple yet effective method to create natural adversarial examples.
Specifically, at the entity level, we replace target entities with other
entities of the same semantic class in Wikidata; at the context level, we use
pre-trained language models (e.g., BERT) to generate word substitutions.
Together, the two levels of attack produce natural adversarial examples that
result in a shifted distribution from the training data on which our target
models have been trained. We apply the proposed method to the OntoNotes dataset
and create a new benchmark named OntoRock for evaluating the robustness of
existing NER models via a systematic evaluation protocol. Our experiments and
analysis reveal that even the best model has a significant performance drop,
and these models seem to memorize in-domain entity patterns instead of
reasoning from the context. Our work also studies the effects of a few simple
data augmentation methods to improve the robustness of NER models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DialBERT: A Hierarchical Pre-Trained Model for Conversation Disentanglement. (arXiv:2004.03760v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.03760">
<div class="article-summary-box-inner">
<span><p>Disentanglement is a problem in which multiple conversations occur in the
same channel simultaneously, and the listener should decide which utterance is
part of the conversation he will respond to. We propose a new model, named
Dialogue BERT (DialBERT), which integrates local and global semantics in a
single stream of messages to disentangle the conversations that mixed together.
We employ BERT to capture the matching information in each utterance pair at
the utterance-level, and use a BiLSTM to aggregate and incorporate the
context-level information. With only a 3% increase in parameters, a 12%
improvement has been attained in comparison to BERT, based on the F1-Score. The
model achieves a state-of-the-art result on the a new dataset proposed by IBM
and surpasses previous work by a substantial margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fighting the COVID-19 Infodemic: Modeling the Perspective of Journalists, Fact-Checkers, Social Media Platforms, Policy Makers, and the Society. (arXiv:2005.00033v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.00033">
<div class="article-summary-box-inner">
<span><p>With the emergence of the COVID-19 pandemic, the political and the medical
aspects of disinformation merged as the problem got elevated to a whole new
level to become the first global infodemic. Fighting this infodemic has been
declared one of the most important focus areas of the World Health
Organization, with dangers ranging from promoting fake cures, rumors, and
conspiracy theories to spreading xenophobia and panic. Ad-dressing the issue
requires solving a number of challenging problems such as identifying messages
containing claims, determining their check-worthiness and factuality, and their
potential to do harm as well as the nature of that harm, to mention just a few.
To address this gap, we release a large dataset of 16K manually annotated
tweets for fine-grained disinformation analysis that (i) focuses on COVID-19,
(ii) combines the perspectives and the interests of journalists, fact-checkers,
social media platforms, policy makers, and society, and (iii) covers Arabic,
Bulgarian, Dutch, and English. Finally, we show strong evaluation results using
pretrained Transformers, thus con-firming the practical utility of the dataset
in monolingual vs. multilingual, and single task vs. multitask settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MedLatinEpi and MedLatinLit: Two Datasets for the Computational Authorship Analysis of Medieval Latin Texts. (arXiv:2006.12289v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.12289">
<div class="article-summary-box-inner">
<span><p>We present and make available MedLatinEpi and MedLatinLit, two datasets of
medieval Latin texts to be used in research on computational authorship
analysis. MedLatinEpi and MedLatinLit consist of 294 and 30 curated texts,
respectively, labelled by author; MedLatinEpi texts are of epistolary nature,
while MedLatinLit texts consist of literary comments and treatises about
various subjects. As such, these two datasets lend themselves to supporting
research in authorship analysis tasks, such as authorship attribution,
authorship verification, or same-author verification. Along with the datasets
we provide experimental results, obtained on these datasets, for the authorship
verification task, i.e., the task of predicting whether a text of unknown
authorship was written by a candidate author or not. We also make available the
source code of the authorship verification system we have used, thus allowing
our experiments to be reproduced, and to be used as baselines, by other
researchers. We also describe the application of the above authorship
verification system, using these datasets as training data, for investigating
the authorship of two medieval epistles whose authorship has been disputed by
scholars.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OSCaR: Orthogonal Subspace Correction and Rectification of Biases in Word Embeddings. (arXiv:2007.00049v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.00049">
<div class="article-summary-box-inner">
<span><p>Language representations are known to carry stereotypical biases and, as a
result, lead to biased predictions in downstream tasks. While existing methods
are effective at mitigating biases by linear projection, such methods are too
aggressive: they not only remove bias, but also erase valuable information from
word embeddings. We develop new measures for evaluating specific information
retention that demonstrate the tradeoff between bias removal and information
retention. To address this challenge, we propose OSCaR (Orthogonal Subspace
Correction and Rectification), a bias-mitigating method that focuses on
disentangling biased associations between concepts instead of removing concepts
wholesale. Our experiments on gender biases show that OSCaR is a well-balanced
approach that ensures that semantic information is retained in the embeddings
and bias is also effectively mitigated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Robustness and Bias Analysis of BERT-based Relation Extraction. (arXiv:2009.06206v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.06206">
<div class="article-summary-box-inner">
<span><p>Fine-tuning pre-trained models have achieved impressive performance on
standard natural language processing benchmarks. However, the resultant model
generalizability remains poorly understood. We do not know, for example, how
excellent performance can lead to the perfection of generalization models. In
this study, we analyze a fine-tuned BERT model from different perspectives
using relation extraction. We also characterize the differences in
generalization techniques according to our proposed improvements. From
empirical experimentation, we find that BERT suffers a bottleneck in terms of
robustness by way of randomizations, adversarial and counterfactual tests, and
biases (i.e., selection and semantic). These findings highlight opportunities
for future improvements. Our open-sourced testbed DiagnoseRE is available in
\url{https://github.com/zjunlp/DiagnoseRE}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GraphCodeBERT: Pre-training Code Representations with Data Flow. (arXiv:2009.08366v4 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.08366">
<div class="article-summary-box-inner">
<span><p>Pre-trained models for programming language have achieved dramatic empirical
improvements on a variety of code-related tasks such as code search, code
completion, code summarization, etc. However, existing pre-trained models
regard a code snippet as a sequence of tokens, while ignoring the inherent
structure of code, which provides crucial code semantics and would enhance the
code understanding process. We present GraphCodeBERT, a pre-trained model for
programming language that considers the inherent structure of code. Instead of
taking syntactic-level structure of code like abstract syntax tree (AST), we
use data flow in the pre-training stage, which is a semantic-level structure of
code that encodes the relation of "where-the-value-comes-from" between
variables. Such a semantic-level structure is neat and does not bring an
unnecessarily deep hierarchy of AST, the property of which makes the model more
efficient. We develop GraphCodeBERT based on Transformer. In addition to using
the task of masked language modeling, we introduce two structure-aware
pre-training tasks. One is to predict code structure edges, and the other is to
align representations between source code and code structure. We implement the
model in an efficient way with a graph-guided masked attention function to
incorporate the code structure. We evaluate our model on four tasks, including
code search, clone detection, code translation, and code refinement. Results
show that code structure and newly introduced pre-training tasks can improve
GraphCodeBERT and achieves state-of-the-art performance on the four downstream
tasks. We further show that the model prefers structure-level attentions over
token-level attentions in the task of code search.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Program Enhanced Fact Verification with Verbalization and Graph Attention Network. (arXiv:2010.03084v6 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.03084">
<div class="article-summary-box-inner">
<span><p>Performing fact verification based on structured data is important for many
real-life applications and is a challenging research problem, particularly when
it involves both symbolic operations and informal inference based on language
understanding. In this paper, we present a Program-enhanced Verbalization and
Graph Attention Network (ProgVGAT) to integrate programs and execution into
textual inference models. Specifically, a verbalization with program execution
model is proposed to accumulate evidences that are embedded in operations over
the tables. Built on that, we construct the graph attention verification
networks, which are designed to fuse different sources of evidences from
verbalized program execution, program structures, and the original statements
and tables, to make the final verification decision. To support the above
framework, we propose a program selection module optimized with a new training
strategy based on margin loss, to produce more accurate programs, which is
shown to be effective in enhancing the final verification results. Experimental
results show that the proposed framework achieves the new state-of-the-art
performance, a 74.4% accuracy, on the benchmark dataset TABFACT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DuoRAT: Towards Simpler Text-to-SQL Models. (arXiv:2010.11119v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.11119">
<div class="article-summary-box-inner">
<span><p>Recent neural text-to-SQL models can effectively translate natural language
questions to corresponding SQL queries on unseen databases. Working mostly on
the Spider dataset, researchers have proposed increasingly sophisticated
solutions to the problem. Contrary to this trend, in this paper we focus on
simplifications. We begin by building DuoRAT, a re-implementation of the
state-of-the-art RAT-SQL model that unlike RAT-SQL is using only relation-aware
or vanilla transformers as the building blocks. We perform several ablation
experiments using DuoRAT as the baseline model. Our experiments confirm the
usefulness of some techniques and point out the redundancy of others, including
structural SQL features and features that link the question with the schema.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Paraphrasing with Pretrained Language Models. (arXiv:2010.12885v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.12885">
<div class="article-summary-box-inner">
<span><p>Paraphrase generation has benefited extensively from recent progress in the
designing of training objectives and model architectures. However, previous
explorations have largely focused on supervised methods, which require a large
amount of labeled data that is costly to collect. To address this drawback, we
adopt a transfer learning approach and propose a training pipeline that enables
pre-trained language models to generate high-quality paraphrases in an
unsupervised setting. Our recipe consists of task-adaptation, self-supervision,
and a novel decoding algorithm named Dynamic Blocking (DB). To enforce a
surface form dissimilar from the input, whenever the language model emits a
token contained in the source sequence, DB prevents the model from outputting
the subsequent source token for the next generation step. We show with
automatic and human evaluations that our approach achieves state-of-the-art
performance on both the Quora Question Pair (QQP) and the ParaNMT datasets and
is robust to domain shift between the two datasets of distinct distributions.
We also demonstrate that our model transfers to paraphrasing in other languages
without any additional finetuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-task Language Modeling for Improving Speech Recognition of Rare Words. (arXiv:2011.11715v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.11715">
<div class="article-summary-box-inner">
<span><p>End-to-end automatic speech recognition (ASR) systems are increasingly
popular due to their relative architectural simplicity and competitive
performance. However, even though the average accuracy of these systems may be
high, the performance on rare content words often lags behind hybrid ASR
systems. To address this problem, second-pass rescoring is often applied
leveraging upon language modeling. In this paper, we propose a second-pass
system with multi-task learning, utilizing semantic targets (such as intent and
slot prediction) to improve speech recognition performance. We show that our
rescoring model trained with these additional tasks outperforms the baseline
rescoring model, trained with only the language modeling task, by 1.4% on a
general test and by 2.6% on a rare word test set in terms of word-error-rate
relative (WERR). Our best ASR system with multi-task LM shows 4.6% WERR
deduction compared with RNN Transducer only ASR baseline for rare words
recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Guided Image Captioning Performance across Domains. (arXiv:2012.02339v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.02339">
<div class="article-summary-box-inner">
<span><p>Image captioning models generally lack the capability to take into account
user interest, and usually default to global descriptions that try to balance
readability, informativeness, and information overload. On the other hand, VQA
models generally lack the ability to provide long descriptive answers, while
expecting the textual question to be quite precise. We present a method to
control the concepts that an image caption should focus on, using an additional
input called the guiding text that refers to either groundable or ungroundable
concepts in the image. Our model consists of a Transformer-based multimodal
encoder that uses the guiding text together with global and object-level image
features to derive early-fusion representations used to generate the guided
caption. While models trained on Visual Genome data have an in-domain advantage
of fitting well when guided with automatic object labels, we find that guided
captioning models trained on Conceptual Captions generalize better on
out-of-domain images and guiding texts. Our human-evaluation results indicate
that attempting in-the-wild guided image captioning requires access to large,
unrestricted-domain training datasets, and that increased style diversity (even
without increasing the number of unique tokens) is a key factor for improved
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Sense Language Modelling. (arXiv:2012.05776v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.05776">
<div class="article-summary-box-inner">
<span><p>The effectiveness of a language model is influenced by its token
representations, which must encode contextual information and handle the same
word form having a plurality of meanings (polysemy). Currently, none of the
common language modelling architectures explicitly model polysemy. We propose a
language model which not only predicts the next word, but also its sense in
context. We argue that this higher prediction granularity may be useful for end
tasks such as assistive writing, and allow for more a precise linking of
language models with knowledge bases. We find that multi-sense language
modelling requires architectures that go beyond standard language models, and
here propose a structured prediction framework that decomposes the task into a
word followed by a sense prediction task. To aid sense prediction, we utilise a
Graph Attention Network, which encodes definitions and example uses of word
senses. Overall, we find that multi-sense language modelling is a highly
challenging task, and suggest that future work focus on the creation of more
annotated training datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">YASO: A Targeted Sentiment Analysis Evaluation Dataset for Open-Domain Reviews. (arXiv:2012.14541v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.14541">
<div class="article-summary-box-inner">
<span><p>Current TSA evaluation in a cross-domain setup is restricted to the small set
of review domains available in existing datasets. Such an evaluation is
limited, and may not reflect true performance on sites like Amazon or Yelp that
host diverse reviews from many domains. To address this gap, we present YASO -
a new TSA evaluation dataset of open-domain user reviews. YASO contains 2,215
English sentences from dozens of review domains, annotated with target terms
and their sentiment. Our analysis verifies the reliability of these
annotations, and explores the characteristics of the collected data. Benchmark
results using five contemporary TSA systems show there is ample room for
improvement on this challenging new dataset. YASO is available at
https://github.com/IBM/yaso-tsa.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ECONET: Effective Continual Pretraining of Language Models for Event Temporal Reasoning. (arXiv:2012.15283v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.15283">
<div class="article-summary-box-inner">
<span><p>While pre-trained language models (PTLMs) have achieved noticeable success on
many NLP tasks, they still struggle for tasks that require event temporal
reasoning, which is essential for event-centric applications. We present a
continual pre-training approach that equips PTLMs with targeted knowledge about
event temporal relations. We design self-supervised learning objectives to
recover masked-out event and temporal indicators and to discriminate sentences
from their corrupted counterparts (where event or temporal indicators got
replaced). By further pre-training a PTLM with these objectives jointly, we
reinforce its attention to event and temporal information, yielding enhanced
capability on event temporal reasoning. This effective continual pre-training
framework for event temporal reasoning (ECONET) improves the PTLMs' fine-tuning
performances across five relation extraction and question answering tasks and
achieves new or on-par state-of-the-art performances in most of our downstream
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning From How Human Correct For Data-Centric Deep Learning. (arXiv:2102.00225v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.00225">
<div class="article-summary-box-inner">
<span><p>In industry NLP application, our manually labeled data has a certain number
of noisy data. We present a simple method to find the noisy data and relabel
them manually, meanwhile we collect the correction information. Then we present
novel method to incorporate the human correction information into deep learning
model. Human know how to correct noisy data. So the correction information can
be inject into deep learning model. We do the experiment on our own text
classification dataset, which is manually labeled, because we relabel the noisy
data in our dataset for our industry application. The experiment result shows
that our method improve the classification accuracy from 91.7% to 92.5%. The
91.7% baseline is based on BERT training on the corrected dataset, which is
hard to surpass.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Transformer Modifications Transfer Across Implementations and Applications?. (arXiv:2102.11972v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.11972">
<div class="article-summary-box-inner">
<span><p>The research community has proposed copious modifications to the Transformer
architecture since it was introduced over three years ago, relatively few of
which have seen widespread adoption. In this paper, we comprehensively evaluate
many of these modifications in a shared experimental setting that covers most
of the common uses of the Transformer in natural language processing.
Surprisingly, we find that most modifications do not meaningfully improve
performance. Furthermore, most of the Transformer variants we found beneficial
were either developed in the same codebase that we used or are relatively minor
changes. We conjecture that performance improvements may strongly depend on
implementation details and correspondingly make some recommendations for
improving the generality of experimental results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging pre-trained representations to improve access to untranscribed speech from endangered languages. (arXiv:2103.14583v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.14583">
<div class="article-summary-box-inner">
<span><p>Pre-trained speech representations like wav2vec 2.0 are a powerful tool for
automatic speech recognition (ASR). Yet many endangered languages lack
sufficient data for pre-training such models, or are predominantly oral
vernaculars without a standardised writing system, precluding fine-tuning.
Query-by-example spoken term detection (QbE-STD) offers an alternative for
iteratively indexing untranscribed speech corpora by locating spoken query
terms. Using data from 7 Australian Aboriginal languages and a regional variety
of Dutch, all of which are endangered or vulnerable, we show that QbE-STD can
be improved by leveraging representations developed for ASR (wav2vec 2.0: the
English monolingual model and XLSR53 multilingual model). Surprisingly, the
English model outperformed the multilingual model on 4 Australian language
datasets, raising questions around how to optimally leverage self-supervised
speech representations for QbE-STD. Nevertheless, we find that wav2vec 2.0
representations (either English or XLSR53) offer large improvements (56-86%
relative) over state-of-the-art approaches on our endangered language datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Convex Aggregation for Opinion Summarization. (arXiv:2104.01371v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01371">
<div class="article-summary-box-inner">
<span><p>Recent advances in text autoencoders have significantly improved the quality
of the latent space, which enables models to generate grammatical and
consistent text from aggregated latent vectors. As a successful application of
this property, unsupervised opinion summarization models generate a summary by
decoding the aggregated latent vectors of inputs. More specifically, they
perform the aggregation via simple average. However, little is known about how
the vector aggregation step affects the generation quality. In this study, we
revisit the commonly used simple average approach by examining the latent space
and generated summaries. We found that text autoencoders tend to generate
overly generic summaries from simply averaged latent vectors due to an
unexpected $L_2$-norm shrinkage in the aggregated latent vectors, which we
refer to as summary vector degeneration. To overcome this issue, we develop a
framework Coop, which searches input combinations for the latent vector
aggregation using input-output word overlap. Experimental results show that
Coop successfully alleviates the summary vector degeneration issue and
establishes new state-of-the-art performance on two opinion summarization
benchmarks. Code is available at \url{https://github.com/megagonlabs/coop}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Role of BERT Token Representations to Explain Sentence Probing Results. (arXiv:2104.01477v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01477">
<div class="article-summary-box-inner">
<span><p>Several studies have been carried out on revealing linguistic features
captured by BERT. This is usually achieved by training a diagnostic classifier
on the representations obtained from different layers of BERT. The subsequent
classification accuracy is then interpreted as the ability of the model in
encoding the corresponding linguistic property. Despite providing insights,
these studies have left out the potential role of token representations. In
this paper, we provide a more in-depth analysis on the representation space of
BERT in search for distinct and meaningful subspaces that can explain the
reasons behind these probing results. Based on a set of probing tasks and with
the help of attribution methods we show that BERT tends to encode meaningful
knowledge in specific token representations (which are often ignored in
standard classification setups), allowing the model to detect syntactic and
semantic abnormalities, and to distinctively separate grammatical number and
tense subspaces.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pushing the Limits of Non-Autoregressive Speech Recognition. (arXiv:2104.03416v4 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.03416">
<div class="article-summary-box-inner">
<span><p>We combine recent advancements in end-to-end speech recognition to
non-autoregressive automatic speech recognition. We push the limits of
non-autoregressive state-of-the-art results for multiple datasets: LibriSpeech,
Fisher+Switchboard and Wall Street Journal. Key to our recipe, we leverage CTC
on giant Conformer neural network architectures with SpecAugment and wav2vec2
pre-training. We achieve 1.8%/3.6% WER on LibriSpeech test/test-other sets,
5.1%/9.8% WER on Switchboard, and 3.4% on the Wall Street Journal, all without
a language model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reducing Discontinuous to Continuous Parsing with Pointer Network Reordering. (arXiv:2104.06239v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06239">
<div class="article-summary-box-inner">
<span><p>Discontinuous constituent parsers have always lagged behind continuous
approaches in terms of accuracy and speed, as the presence of constituents with
discontinuous yield introduces extra complexity to the task. However, a
discontinuous tree can be converted into a continuous variant by reordering
tokens. Based on that, we propose to reduce discontinuous parsing to a
continuous problem, which can then be directly solved by any off-the-shelf
continuous parser. To that end, we develop a Pointer Network capable of
accurately generating the continuous token arrangement for a given input
sentence and define a bijective function to recover the original order.
Experiments on the main benchmarks with two continuous parsers prove that our
approach is on par in accuracy with purely discontinuous state-of-the-art
algorithms, but considerably faster.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Interpretable Clauses Semantically using Pretrained Word Representation. (arXiv:2104.06901v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06901">
<div class="article-summary-box-inner">
<span><p>Tsetlin Machine (TM) is an interpretable pattern recognition algorithm based
on propositional logic, which has demonstrated competitive performance in many
Natural Language Processing (NLP) tasks, including sentiment analysis, text
classification, and Word Sense Disambiguation. To obtain human-level
interpretability, legacy TM employs Boolean input features such as bag-of-words
(BOW). However, the BOW representation makes it difficult to use any
pre-trained information, for instance, word2vec and GloVe word representations.
This restriction has constrained the performance of TM compared to deep neural
networks (DNNs) in NLP. To reduce the performance gap, in this paper, we
propose a novel way of using pre-trained word representations for TM. The
approach significantly enhances the performance and interpretability of TM. We
achieve this by extracting semantically related words from pre-trained word
representations as input features to the TM. Our experiments show that the
accuracy of the proposed approach is significantly higher than the previous
BOW-based TM, reaching the level of DNN-based models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Disentangling Representations of Text by Masking Transformers. (arXiv:2104.07155v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07155">
<div class="article-summary-box-inner">
<span><p>Representations from large pretrained models such as BERT encode a range of
features into monolithic vectors, affording strong predictive accuracy across a
multitude of downstream tasks. In this paper we explore whether it is possible
to learn disentangled representations by identifying existing subnetworks
within pretrained models that encode distinct, complementary aspect
representations. Concretely, we learn binary masks over transformer weights or
hidden units to uncover subsets of features that correlate with a specific
factor of variation; this eliminates the need to train a disentangled model
from scratch for a particular task. We evaluate this method with respect to its
ability to disentangle representations of sentiment from genre in movie
reviews, "toxicity" from dialect in Tweets, and syntax from semantics.
</p>
<p>By combining masking with magnitude pruning we find that we can identify
sparse subnetworks within BERT that strongly encode particular aspects (e.g.,
toxicity) while only weakly encoding others (e.g., race). Moreover, despite
only learning masks, we find that disentanglement-via-masking performs as well
as -- and often better than -- previously proposed methods based on variational
autoencoders and adversarial training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Domain Label-Adaptive Stance Detection. (arXiv:2104.07467v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07467">
<div class="article-summary-box-inner">
<span><p>Stance detection concerns the classification of a writer's viewpoint towards
a target. There are different task variants, e.g., stance of a tweet vs. a full
article, or stance with respect to a claim vs. an (implicit) topic. Moreover,
task definitions vary, which includes the label inventory, the data collection,
and the annotation protocol. All these aspects hinder cross-domain studies, as
they require changes to standard domain adaptation approaches. In this paper,
we perform an in-depth analysis of 16 stance detection datasets, and we explore
the possibility for cross-domain learning from them. Moreover, we propose an
end-to-end unsupervised framework for out-of-domain prediction of unseen,
user-defined labels. In particular, we combine domain adaptation techniques
such as mixture of experts and domain-adversarial training with label
embeddings, and we demonstrate sizable performance gains over strong baselines,
both (i) in-domain, i.e., for seen targets, and (ii) out-of-domain, i.e., for
unseen targets. Finally, we perform an exhaustive analysis of the cross-domain
results, and we highlight the important factors influencing the model
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward Deconfounding the Influence of Entity Demographics for Question Answering Accuracy. (arXiv:2104.07571v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07571">
<div class="article-summary-box-inner">
<span><p>The goal of question answering (QA) is to answer any question. However, major
QA datasets have skewed distributions over gender, profession, and nationality.
Despite that skew, model accuracy analysis reveals little evidence that
accuracy is lower for people based on gender or nationality; instead, there is
more variation on professions (question topic). But QA's lack of representation
could itself hide evidence of bias, necessitating QA datasets that better
represent global diversity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Polarized Topics Using Partisanship-aware Contextualized Topic Embeddings. (arXiv:2104.07814v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07814">
<div class="article-summary-box-inner">
<span><p>Growing polarization of the news media has been blamed for fanning
disagreement, controversy and even violence. Early identification of polarized
topics is thus an urgent matter that can help mitigate conflict. However,
accurate measurement of topic-wise polarization is still an open research
challenge. To address this gap, we propose Partisanship-aware Contextualized
Topic Embeddings (PaCTE), a method to automatically detect polarized topics
from partisan news sources. Specifically, utilizing a language model that has
been finetuned on recognizing partisanship of the news articles, we represent
the ideology of a news corpus on a topic by corpus-contextualized topic
embedding and measure the polarization using cosine distance. We apply our
method to a dataset of news articles about the COVID-19 pandemic. Extensive
experiments on different news sources and topics demonstrate the efficacy of
our method to capture topical polarization, as indicated by its effectiveness
of retrieving the most polarized topics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Matching-oriented Product Quantization For Ad-hoc Retrieval. (arXiv:2104.07858v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07858">
<div class="article-summary-box-inner">
<span><p>Product quantization (PQ) is a widely used technique for ad-hoc retrieval.
Recent studies propose supervised PQ, where the embedding and quantization
models can be jointly trained with supervised learning. However, there is a
lack of appropriate formulation of the joint training objective; thus, the
improvements over previous non-supervised baselines are limited in reality. In
this work, we propose the Matching-oriented Product Quantization (MoPQ), where
a novel objective Multinoulli Contrastive Loss (MCL) is formulated. With the
minimization of MCL, we are able to maximize the matching probability of query
and ground-truth key, which contributes to the optimal retrieval accuracy.
Given that the exact computation of MCL is intractable due to the demand of
vast contrastive samples, we further propose the Differentiable Cross-device
Sampling (DCS), which significantly augments the contrastive samples for
precise approximation of MCL. We conduct extensive experimental studies on four
real-world datasets, whose results verify the effectiveness of MoPQ. The code
is available at https://github.com/microsoft/MoPQ.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Counter-Interference Adapter for Multilingual Machine Translation. (arXiv:2104.08154v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08154">
<div class="article-summary-box-inner">
<span><p>Developing a unified multilingual model has long been a pursuit for machine
translation. However, existing approaches suffer from performance degradation
-- a single multilingual model is inferior to separately trained bilingual ones
on rich-resource languages. We conjecture that such a phenomenon is due to
interference caused by joint training with multiple languages. To accommodate
the issue, we propose CIAT, an adapted Transformer model with a small parameter
overhead for multilingual machine translation. We evaluate CIAT on multiple
benchmark datasets, including IWSLT, OPUS-100, and WMT. Experiments show that
CIAT consistently outperforms strong multilingual baselines on 64 of total 66
language directions, 42 of which see above 0.5 BLEU improvement. Our code is
available at \url{https://github.com/Yaoming95/CIAT}~.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ESTER: A Machine Reading Comprehension Dataset for Event Semantic Relation Reasoning. (arXiv:2104.08350v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08350">
<div class="article-summary-box-inner">
<span><p>Understanding how events are semantically related to each other is the
essence of reading comprehension. Recent event-centric reading comprehension
datasets focus mostly on event arguments or temporal relations. While these
tasks partially evaluate machines' ability of narrative understanding,
human-like reading comprehension requires the capability to process event-based
information beyond arguments and temporal reasoning. For example, to understand
causality between events, we need to infer motivation or purpose; to establish
event hierarchy, we need to understand the composition of events. To facilitate
these tasks, we introduce ESTER, a comprehensive machine reading comprehension
(MRC) dataset for Event Semantic Relation Reasoning. The dataset leverages
natural language queries to reason about the five most common event semantic
relations, provides more than 6K questions and captures 10.1K event relation
pairs. Experimental results show that the current SOTA systems achieve 22.1%,
63.3%, and 83.5% for token-based exact-match, F1, and event-based HIT@1 scores,
which are all significantly below human performances (36.0%, 79.6%, 100%
respectively), highlighting our dataset as a challenging benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MT6: Multilingual Pretrained Text-to-Text Transformer with Translation Pairs. (arXiv:2104.08692v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08692">
<div class="article-summary-box-inner">
<span><p>Multilingual T5 (mT5) pretrains a sequence-to-sequence model on massive
monolingual texts, which has shown promising results on many cross-lingual
tasks. In this paper, we improve multilingual text-to-text transfer Transformer
with translation pairs (mT6). Specifically, we explore three cross-lingual
text-to-text pre-training tasks, namely, machine translation, translation pair
span corruption, and translation span corruption. In addition, we propose a
partially non-autoregressive objective for text-to-text pre-training. We
evaluate the methods on eight multilingual benchmark datasets, including
sentence classification, named entity recognition, question answering, and
abstractive summarization. Experimental results show that the proposed mT6
improves cross-lingual transferability over mT5.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GooAQ: Open Question Answering with Diverse Answer Types. (arXiv:2104.08727v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08727">
<div class="article-summary-box-inner">
<span><p>While day-to-day questions come with a variety of answer types, the current
question-answering (QA) literature has failed to adequately address the answer
diversity of questions. To this end, we present GooAQ, a large-scale dataset
with a variety of answer types. This dataset contains over 5 million questions
and 3 million answers collected from Google. GooAQ questions are collected
semi-automatically from the Google search engine using its autocomplete
feature. This results in naturalistic questions of practical interest that are
nonetheless short and expressed using simple language. GooAQ answers are mined
from Google's responses to our collected questions, specifically from the
answer boxes in the search results. This yields a rich space of answer types,
containing both textual answers (short and long) as well as more structured
ones such as collections. We benchmarkT5 models on GooAQ and observe that: (a)
in line with recent work, LM's strong performance on GooAQ's short-answer
questions heavily benefit from annotated data; however, (b) their quality in
generating coherent and accurate responses for questions requiring long
responses (such as 'how' and 'why' questions) is less reliant on observing
annotated data and mainly supported by their pre-training. We release GooAQ to
facilitate further research on improving QA with diverse response types.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can NLI Models Verify QA Systems' Predictions?. (arXiv:2104.08731v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08731">
<div class="article-summary-box-inner">
<span><p>To build robust question answering systems, we need the ability to verify
whether answers to questions are truly correct, not just "good enough" in the
context of imperfect QA datasets. We explore the use of natural language
inference (NLI) as a way to achieve this goal, as NLI inherently requires the
premise (document context) to contain all necessary information to support the
hypothesis (proposed answer to the question). We leverage large pre-trained
models and recent prior datasets to construct powerful question converter and
decontextualization modules, which can reformulate QA instances as
premise-hypothesis pairs with very high reliability. Then, by combining
standard NLI datasets with NLI examples automatically derived from QA training
data, we can train NLI models to judge the correctness of QA models' proposed
answers. We show that our NLI approach can generally improve the confidence
estimation of a QA model across different domains, evaluated in a selective QA
setting. Careful manual analysis over the predictions of our NLI model shows
that it can further identify cases where the QA model produces the right answer
for the wrong reason, or where the answer cannot be verified as addressing all
aspects of the question.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling End-to-End Models for Large-Scale Multilingual ASR. (arXiv:2104.14830v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.14830">
<div class="article-summary-box-inner">
<span><p>Building ASR models across many languages is a challenging multi-task
learning problem due to large variations and heavily unbalanced data. Existing
work has shown positive transfer from high resource to low resource languages.
However, degradations on high resource languages are commonly observed due to
interference from the heterogeneous multilingual data and reduction in
per-language capacity. We conduct a capacity study on a 15-language task, with
the amount of data per language varying from 7.6K to 53.5K hours. We adopt
GShard [1] to efficiently scale up to 10B parameters. Empirically, we find that
(1) scaling the number of model parameters is an effective way to solve the
capacity bottleneck - our 500M-param model already outperforms monolingual
baselines and scaling it to 1B and 10B brought further quality gains; (2)
larger models are not only more data efficient, but also more efficient in
terms of training cost as measured in TPU days - the 1B-param model reaches the
same accuracy at 34% of training time as the 500M-param model; (3) given a
fixed capacity budget, adding depth works better than width and large encoders
do better than large decoders; (4) with continuous training, they can be
adapted to new languages and domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Classifying Continuous Constraint Satisfaction problems. (arXiv:2106.02397v2 [cs.CC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02397">
<div class="article-summary-box-inner">
<span><p>A continuous constraint satisfaction problem (CCSP) is a constraint
satisfaction problem (CSP) with a domain $U \subset \mathbb{R}$. We engage in a
systematic study to classify CCSPs that are complete of the Existential Theory
of the Reals, i.e., ER-complete. To define this class, we first consider the
problem ETR, which also stands for Existential Theory of the Reals. In an
instance of this problem we are given some sentence of the form $\exists x_1,
\ldots, x_n \in \mathbb{R} : \Phi(x_1, \ldots, x_n)$, where $\Phi$ is a
well-formed quantifier-free formula consisting of the symbols $\{0, 1, +,
\cdot, \geq, &gt;, \wedge, \vee, \neg\}$, the goal is to check whether this
sentence is true. Now the class ER is the family of all problems that admit a
polynomial-time reduction to ETR. It is known that NP $\subseteq$ ER
$\subseteq$ PSPACE.
</p>
<p>We restrict our attention on CCSPs with addition constraints ($x + y = z$)
and some other mild technical condition. Previously, it was shown that
multiplication constraints ($x \cdot y = z$), squaring constraints ($x^2 = y$),
or inversion constraints ($x\cdot y = 1$) are sufficient to establish
ER-completeness. We extend this in the strongest possible sense for equality
constraints as follows. We show that CCSPs (with addition constraints and some
other mild technical condition) that have any one well-behaved curved equality
constraint ($f(x,y) = 0$) are ER-complete. We further extend our results to
inequality constraints. We show that any well-behaved convexly curved and any
well-behaved concavely curved inequality constraint ($f(x,y) \geq 0$ and
$g(x,y) \geq 0$) imply ER-completeness on the class of such CCSPs.
</p>
<p>We apply our findings to geometric packing and answer an open question by
Abrahamsen et al. [FOCS 2020]. Namely, we establish ER-completeness of packing
convex pieces into a square container under rotations and translations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Pretrained Cross-Lingual Language Models via Self-Labeled Word Alignment. (arXiv:2106.06381v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06381">
<div class="article-summary-box-inner">
<span><p>The cross-lingual language models are typically pretrained with masked
language modeling on multilingual text or parallel sentences. In this paper, we
introduce denoising word alignment as a new cross-lingual pre-training task.
Specifically, the model first self-labels word alignments for parallel
sentences. Then we randomly mask tokens in a bitext pair. Given a masked token,
the model uses a pointer network to predict the aligned token in the other
language. We alternately perform the above two steps in an
expectation-maximization manner. Experimental results show that our method
improves cross-lingual transferability on various datasets, especially on the
token-level tasks, such as question answering, and structured prediction.
Moreover, the model can serve as a pretrained word aligner, which achieves
reasonably low error rates on the alignment benchmarks. The code and pretrained
parameters are available at https://github.com/CZWin32768/XLM-Align.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WeChat Neural Machine Translation Systems for WMT21. (arXiv:2108.02401v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02401">
<div class="article-summary-box-inner">
<span><p>This paper introduces WeChat AI's participation in WMT 2021 shared news
translation task on English-&gt;Chinese, English-&gt;Japanese, Japanese-&gt;English and
English-&gt;German. Our systems are based on the Transformer (Vaswani et al.,
2017) with several novel and effective variants. In our experiments, we employ
data filtering, large-scale synthetic data generation (i.e., back-translation,
knowledge distillation, forward-translation, iterative in-domain knowledge
transfer), advanced finetuning approaches, and boosted Self-BLEU based model
ensemble. Our constrained systems achieve 36.9, 46.9, 27.8 and 31.3
case-sensitive BLEU scores on English-&gt;Chinese, English-&gt;Japanese,
Japanese-&gt;English and English-&gt;German, respectively. The BLEU scores of
English-&gt;Chinese, English-&gt;Japanese and Japanese-&gt;English are the highest among
all submissions, and that of English-&gt;German is the highest among all
constrained submissions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-Grained Element Identification in Complaint Text of Internet Fraud. (arXiv:2108.08676v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08676">
<div class="article-summary-box-inner">
<span><p>Existing system dealing with online complaint provides a final decision
without explanations. We propose to analyse the complaint text of internet
fraud in a fine-grained manner. Considering the complaint text includes
multiple clauses with various functions, we propose to identify the role of
each clause and classify them into different types of fraud element. We
construct a large labeled dataset originated from a real finance service
platform. We build an element identification model on top of BERT and propose
additional two modules to utilize the context of complaint text for better
element label classification, namely, global context encoder and label refiner.
Experimental results show the effectiveness of our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Just Say No: Analyzing the Stance of Neural Dialogue Generation in Offensive Contexts. (arXiv:2108.11830v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11830">
<div class="article-summary-box-inner">
<span><p>Dialogue models trained on human conversations inadvertently learn to
generate toxic responses. In addition to producing explicitly offensive
utterances, these models can also implicitly insult a group or individual by
aligning themselves with an offensive statement. To better understand the
dynamics of contextually offensive language, we investigate the stance of
dialogue model responses in offensive Reddit conversations. Specifically, we
create ToxiChat, a crowd-annotated dataset of 2,000 Reddit threads and model
responses labeled with offensive language and stance. Our analysis reveals that
42% of human responses agree with toxic comments, whereas only 13% agree with
safe comments. This undesirable behavior is learned by neural dialogue models,
such as DialoGPT, which we show are two times more likely to agree with
offensive comments. To enable automatic detection of offensive language, we
fine-tuned transformer-based classifiers on ToxiChat that achieve 0.71 F1 for
offensive labels and 0.53 Macro-F1 for stance labels. Finally, we quantify the
effectiveness of controllable text generation (CTG) methods to mitigate the
tendency of neural dialogue models to agree with offensive comments. Compared
to the baseline, our best CTG model achieves a 19% reduction in agreement with
offensive comments and produces 29% fewer offensive replies. Our work
highlights the need for further efforts to characterize and analyze
inappropriate behavior in dialogue models, in order to help make them safer.
Our code and corpus are available at https://github.com/abaheti95/ToxiChat .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Harms of Gender Exclusivity and Challenges in Non-Binary Representation in Language Technologies. (arXiv:2108.12084v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12084">
<div class="article-summary-box-inner">
<span><p>Gender is widely discussed in the context of language tasks and when
examining the stereotypes propagated by language models. However, current
discussions primarily treat gender as binary, which can perpetuate harms such
as the cyclical erasure of non-binary gender identities. These harms are driven
by model and dataset biases, which are consequences of the non-recognition and
lack of understanding of non-binary genders in society. In this paper, we
explain the complexity of gender and language around it, and survey non-binary
persons to understand harms associated with the treatment of gender as binary
in English language technologies. We also detail how current language
representations (e.g., GloVe, BERT) capture and perpetuate these harms and
related challenges that need to be acknowledged and addressed for
representations to equitably encode gender information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Partition Filter Network for Joint Entity and Relation Extraction. (arXiv:2108.12202v8 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12202">
<div class="article-summary-box-inner">
<span><p>In joint entity and relation extraction, existing work either sequentially
encode task-specific features, leading to an imbalance in inter-task feature
interaction where features extracted later have no direct contact with those
that come first. Or they encode entity features and relation features in a
parallel manner, meaning that feature representation learning for each task is
largely independent of each other except for input sharing. We propose a
partition filter network to model two-way interaction between tasks properly,
where feature encoding is decomposed into two steps: partition and filter. In
our encoder, we leverage two gates: entity and relation gate, to segment
neurons into two task partitions and one shared partition. The shared partition
represents inter-task information valuable to both tasks and is evenly shared
across two tasks to ensure proper two-way interaction. The task partitions
represent intra-task information and are formed through concerted efforts of
both gates, making sure that encoding of task-specific features is dependent
upon each other. Experiment results on six public datasets show that our model
performs significantly better than previous approaches. In addition, contrary
to what previous work has claimed, our auxiliary experiments suggest that
relation prediction is contributory to named entity prediction in a
non-negligible way. The source code can be found at
https://github.com/Coopercoppers/PFN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentence Bottleneck Autoencoders from Transformer Language Models. (arXiv:2109.00055v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00055">
<div class="article-summary-box-inner">
<span><p>Representation learning for text via pretraining a language model on a large
corpus has become a standard starting point for building NLP systems. This
approach stands in contrast to autoencoders, also trained on raw text, but with
the objective of learning to encode each input as a vector that allows full
reconstruction. Autoencoders are attractive because of their latent space
structure and generative properties. We therefore explore the construction of a
sentence-level autoencoder from a pretrained, frozen transformer language
model. We adapt the masked language modeling objective as a generative,
denoising one, while only training a sentence bottleneck and a single-layer
modified transformer decoder. We demonstrate that the sentence representations
discovered by our model achieve better quality than previous methods that
extract representations from pretrained transformers on text similarity tasks,
style transfer (an example of controlled generation), and single-sentence
classification tasks in the GLUE benchmark, while using fewer parameters than
large pretrained models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Improving Adversarial Training of NLP Models. (arXiv:2109.00544v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00544">
<div class="article-summary-box-inner">
<span><p>Adversarial training, a method for learning robust deep neural networks,
constructs adversarial examples during training. However, recent methods for
generating NLP adversarial examples involve combinatorial search and expensive
sentence encoders for constraining the generated instances. As a result, it
remains challenging to use vanilla adversarial training to improve NLP models'
performance, and the benefits are mainly uninvestigated. This paper proposes a
simple and improved vanilla adversarial training process for NLP models, which
we name Attacking to Training (A2T). The core part of A2T is a new and cheaper
word substitution attack optimized for vanilla adversarial training. We use A2T
to train BERT and RoBERTa models on IMDB, Rotten Tomatoes, Yelp, and SNLI
datasets. Our results empirically show that it is possible to train robust NLP
models using a much cheaper adversary. We demonstrate that vanilla adversarial
training with A2T can improve an NLP model's robustness to the attack it was
originally trained with and also defend the model against other types of word
substitution attacks. Furthermore, we show that A2T can improve NLP models'
standard accuracy, cross-domain generalization, and interpretability. Code is
available at https://github.com/QData/Textattack-A2T .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FewshotQA: A simple framework for few-shot learning of question answering tasks using pre-trained text-to-text models. (arXiv:2109.01951v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01951">
<div class="article-summary-box-inner">
<span><p>The task of learning from only a few examples (called a few-shot setting) is
of key importance and relevance to a real-world setting. For question answering
(QA), the current state-of-the-art pre-trained models typically need
fine-tuning on tens of thousands of examples to obtain good results. Their
performance degrades significantly in a few-shot setting (&lt; 100 examples). To
address this, we propose a simple fine-tuning framework that leverages
pre-trained text-to-text models and is directly aligned with their pre-training
framework. Specifically, we construct the input as a concatenation of the
question, a mask token representing the answer span and a context. Given this
input, the model is fine-tuned using the same objective as that of its
pre-training objective. Through experimental studies on various few-shot
configurations, we show that this formulation leads to significant gains on
multiple QA benchmarks (an absolute gain of 34.2 F1 points on average when
there are only 16 training examples). The gains extend further when used with
larger models (Eg:- 72.3 F1 on SQuAD using BART-large with only 32 examples)
and translate well to a multilingual setting . On the multilingual TydiQA
benchmark, our model outperforms the XLM-Roberta-large by an absolute margin of
upto 40 F1 points and an average of 33 F1 points in a few-shot setting (&lt;= 64
training examples). We conduct detailed ablation studies to analyze factors
contributing to these gains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Teaching Autoregressive Language Models Complex Tasks By Demonstration. (arXiv:2109.02102v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02102">
<div class="article-summary-box-inner">
<span><p>This paper demonstrates that by fine-tuning an autoregressive language model
(GPT-Neo) on appropriately structured step-by-step demonstrations, it is
possible to teach it to execute a mathematical task that has previously proved
difficult for Transformers - longhand modulo operations - with a relatively
small number of examples. Specifically, we fine-tune GPT-Neo to solve the
numbers__div_remainder task from the DeepMind Mathematics Dataset; Saxton et
al. (<a href="/abs/1904.01557">arXiv:1904.01557</a>) reported below 40% accuracy on this task with 2 million
training examples. We show that after fine-tuning on 200 appropriately
structured demonstrations of solving long division problems and reporting the
remainders, the smallest available GPT-Neo model achieves over 80% accuracy.
This is achieved by constructing an appropriate dataset for fine-tuning, with
no changes to the learning algorithm. These results suggest that fine-tuning
autoregressive language models on small sets of well-crafted demonstrations may
be a useful paradigm for enabling individuals without training in machine
learning to coax such models to perform some kinds of complex multi-step tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Low-Resource Dialogue Summarization with Domain-Agnostic Multi-Source Pretraining. (arXiv:2109.04080v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04080">
<div class="article-summary-box-inner">
<span><p>With the rapid increase in the volume of dialogue data from daily life, there
is a growing demand for dialogue summarization. Unfortunately, training a large
summarization model is generally infeasible due to the inadequacy of dialogue
data with annotated summaries. Most existing works for low-resource dialogue
summarization directly pretrain models in other domains, e.g., the news domain,
but they generally neglect the huge difference between dialogues and
conventional articles. To bridge the gap between out-of-domain pretraining and
in-domain fine-tuning, in this work, we propose a multi-source pretraining
paradigm to better leverage the external summary data. Specifically, we exploit
large-scale in-domain non-summary data to separately pretrain the dialogue
encoder and the summary decoder. The combined encoder-decoder model is then
pretrained on the out-of-domain summary data using adversarial critics, aiming
to facilitate domain-agnostic summarization. The experimental results on two
public datasets show that with only limited training data, our approach
achieves competitive performance and generalizes well in different dialogue
scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lexico-semantic and affective modelling of Spanish poetry: A semi-supervised learning approach. (arXiv:2109.04152v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04152">
<div class="article-summary-box-inner">
<span><p>Text classification tasks have improved substantially during the last years
by the usage of transformers. However, the majority of researches focus on
prose texts, with poetry receiving less attention, specially for Spanish
language. In this paper, we propose a semi-supervised learning approach for
inferring 21 psychological categories evoked by a corpus of 4572 sonnets, along
with 10 affective and lexico-semantic multiclass ones. The subset of poems used
for training an evaluation includes 270 sonnets. With our approach, we achieve
an AUC beyond 0.7 for 76% of the psychological categories, and an AUC over 0.65
for 60% on the multiclass ones. The sonnets are modelled using transformers,
through sentence embeddings, along with lexico-semantic and affective features,
obtained by using external lexicons. Consequently, we see that this approach
provides an AUC increase of up to 0.12, as opposed to using transformers alone.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning with Different Amounts of Annotation: From Zero to Many Labels. (arXiv:2109.04408v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04408">
<div class="article-summary-box-inner">
<span><p>Training NLP systems typically assumes access to annotated data that has a
single human label per example. Given imperfect labeling from annotators and
inherent ambiguity of language, we hypothesize that single label is not
sufficient to learn the spectrum of language interpretation. We explore new
annotation distribution schemes, assigning multiple labels per example for a
small subset of training examples. Introducing such multi label examples at the
cost of annotating fewer examples brings clear gains on natural language
inference task and entity typing task, even when we simply first train with a
single label data and then fine tune with multi label examples. Extending a
MixUp data augmentation framework, we propose a learning algorithm that can
learn from training examples with different amount of annotation (with zero,
one, or multiple labels). This algorithm efficiently combines signals from
uneven training data and brings additional gains in low annotation budget and
cross domain settings. Together, our method achieves consistent gains in two
tasks, suggesting distributing labels unevenly among training examples can be
beneficial for many NLP tasks.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">A Deep Learning-Based Unified Framework for Red Lesions Detection on Retinal Fundus Images. (arXiv:2109.05021v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05021">
<div class="article-summary-box-inner">
<span><p>Red-lesions, i.e., microaneurysms (MAs) and hemorrhages (HMs), are the early
signs of diabetic retinopathy (DR). The automatic detection of MAs and HMs on
retinal fundus images is a challenging task. Most of the existing methods
detect either only MAs or only HMs because of the difference in their texture,
sizes, and morphology. Though some methods detect both MAs and HMs, they suffer
from the curse of dimensionality of shape and colors features and fail to
detect all shape variations of HMs such as flame-shaped HM. Leveraging the
progress in deep learning, we proposed a two-stream red lesions detection
system dealing simultaneously with small and large red lesions. For this
system, we introduced a new ROIs candidates generation method for large red
lesions fundus images; it is based on blood vessel segmentation and
morphological operations, and reduces the computational complexity, and
enhances the detection accuracy by generating a small number of potential
candidates. For detection, we adapted the Faster RCNN framework with two
streams. We used pre-trained VGGNet as a bone model and carried out several
extensive experiments to tune it for vessels segmentation and candidates
generation, and finally learning the appropriate mapping, which yields better
detection of the red lesions comparing with the state-of-the-art methods. The
experimental results validated the effectiveness of the system in the detection
of both MAs and HMs; the method yields higher performance for per lesion
detection according to sensitivity under 4 FPIs on DiaretDB1-MA and
DiaretDB1-HM datasets, and 1 FPI on e-ophtha and ROCh datasets than the state
of the art methods w.r.t. various evaluation metrics. For DR screening, the
system outperforms other methods on DiaretDB1-MA, DiaretDB1-HM, and e-ophtha
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-time multimodal image registration with partial intraoperative point-set data. (arXiv:2109.05023v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05023">
<div class="article-summary-box-inner">
<span><p>We present Free Point Transformer (FPT) - a deep neural network architecture
for non-rigid point-set registration. Consisting of two modules, a global
feature extraction module and a point transformation module, FPT does not
assume explicit constraints based on point vicinity, thereby overcoming a
common requirement of previous learning-based point-set registration methods.
FPT is designed to accept unordered and unstructured point-sets with a variable
number of points and uses a "model-free" approach without heuristic
constraints. Training FPT is flexible and involves minimizing an intuitive
unsupervised loss function, but supervised, semi-supervised, and partially- or
weakly-supervised training are also supported. This flexibility makes FPT
amenable to multimodal image registration problems where the ground-truth
deformations are difficult or impossible to measure. In this paper, we
demonstrate the application of FPT to non-rigid registration of prostate
magnetic resonance (MR) imaging and sparsely-sampled transrectal ultrasound
(TRUS) images. The registration errors were 4.71 mm and 4.81 mm for complete
TRUS imaging and sparsely-sampled TRUS imaging, respectively. The results
indicate superior accuracy to the alternative rigid and non-rigid registration
algorithms tested and substantially lower computation time. The rapid inference
possible with FPT makes it particularly suitable for applications where
real-time registration is beneficial.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Medulloblastoma Tumor Classification using Deep Transfer Learning with Multi-Scale EfficientNets. (arXiv:2109.05025v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05025">
<div class="article-summary-box-inner">
<span><p>Medulloblastoma (MB) is the most common malignant brain tumor in childhood.
The diagnosis is generally based on the microscopic evaluation of
histopathological tissue slides. However, visual-only assessment of
histopathological patterns is a tedious and time-consuming task and is also
affected by observer variability. Hence, automated MB tumor classification
could assist pathologists by promoting consistency and robust quantification.
Recently, convolutional neural networks (CNNs) have been proposed for this
task, while transfer learning has shown promising results. In this work, we
propose an end-to-end MB tumor classification and explore transfer learning
with various input sizes and matching network dimensions. We focus on
differentiating between the histological subtypes classic and
desmoplastic/nodular. For this purpose, we systematically evaluate recently
proposed EfficientNets, which uniformly scale all dimensions of a CNN. Using a
data set with 161 cases, we demonstrate that pre-trained EfficientNets with
larger input resolutions lead to significant performance improvements compared
to commonly used pre-trained CNN architectures. Also, we highlight the
importance of transfer learning, when using such large architectures. Overall,
our best performing method achieves an F1-Score of 80.1%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Instance-Conditioned GAN. (arXiv:2109.05070v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05070">
<div class="article-summary-box-inner">
<span><p>Generative Adversarial Networks (GANs) can generate near photo realistic
images in narrow domains such as human faces. Yet, modeling complex
distributions of datasets such as ImageNet and COCO-Stuff remains challenging
in unconditional settings. In this paper, we take inspiration from kernel
density estimation techniques and introduce a non-parametric approach to
modeling distributions of complex datasets. We partition the data manifold into
a mixture of overlapping neighborhoods described by a datapoint and its nearest
neighbors, and introduce a model, called instance-conditioned GAN (IC-GAN),
which learns the distribution around each datapoint. Experimental results on
ImageNet and COCO-Stuff show that IC-GAN significantly improves over
unconditional models and unsupervised data partitioning baselines. Moreover, we
show that IC-GAN can effortlessly transfer to datasets not seen during training
by simply changing the conditioning instances, and still generate realistic
images. Finally, we extend IC-GAN to the class-conditional case and show
semantically controllable generation and competitive quantitative results on
ImageNet; while improving over BigGAN on ImageNet-LT. We will opensource our
code and trained models to reproduce the reported results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A semi-supervised self-training method to develop assistive intelligence for segmenting multiclass bridge elements from inspection videos. (arXiv:2109.05078v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05078">
<div class="article-summary-box-inner">
<span><p>Bridge inspection is an important step in preserving and rehabilitating
transportation infrastructure for extending their service lives. The
advancement of mobile robotic technology allows the rapid collection of a large
amount of inspection video data. However, the data are mainly images of complex
scenes, wherein a bridge of various structural elements mix with a cluttered
background. Assisting bridge inspectors in extracting structural elements of
bridges from the big complex video data, and sorting them out by classes, will
prepare inspectors for the element-wise inspection to determine the condition
of bridges. This paper is motivated to develop an assistive intelligence model
for segmenting multiclass bridge elements from inspection videos captured by an
aerial inspection platform. With a small initial training dataset labeled by
inspectors, a Mask Region-based Convolutional Neural Network (Mask R-CNN)
pre-trained on a large public dataset was transferred to the new task of
multiclass bridge element segmentation. Besides, the temporal coherence
analysis attempts to recover false negatives and identify the weakness that the
neural network can learn to improve. Furthermore, a semi-supervised
self-training (S$^3$T) method was developed to engage experienced inspectors in
refining the network iteratively. Quantitative and qualitative results from
evaluating the developed deep neural network demonstrate that the proposed
method can utilize a small amount of time and guidance from experienced
inspectors (3.58 hours for labeling 66 images) to build the network of
excellent performance (91.8% precision, 93.6% recall, and 92.7% f1-score).
Importantly, the paper illustrates an approach to leveraging the domain
knowledge and experiences of bridge professionals into computational
intelligence models to efficiently adapt the models to varied bridges in the
National Bridge Inventory.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Preliminary Wildfire Detection Using State-of-the-art PTZ (Pan, Tilt, Zoom) Camera Technology and Convolutional Neural Networks. (arXiv:2109.05083v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05083">
<div class="article-summary-box-inner">
<span><p>Wildfires are uncontrolled fires in the environment that can be caused by
humans or nature. In 2020 alone, wildfires in California have burned 4.2
million acres, damaged 10,500 buildings or structures, and killed more than 31
people, exacerbated by climate change and a rise in average global
temperatures. This also means there has been an increase in the costs of
extinguishing these treacherous wildfires. The objective of the research is to
detect forest fires in their earlier stages to prevent them from spreading,
prevent them from causing damage to a variety of things, and most importantly,
reduce or eliminate the chances of someone dying from a wildfire. A fire
detection system should be efficient and accurate with respect to extinguishing
wildfires in their earlier stages to prevent the spread of them along with
their consequences. Computer Vision is potentially a more reliable, fast, and
widespread method we need. The current research in the field of preliminary
fire detection has several problems related to unrepresentative data being used
to train models and their existing varied amounts of label imbalance in the
classes of their dataset. We propose a more representative and evenly
distributed data through better settings, lighting, atmospheres, etc., and
class distribution in the entire dataset. After thoroughly examining the
results of this research, it can be inferred that they supported the datasets
strengths by being a viable resource when tested in the real world on
unfamiliar data. This is evident since as the model trains on the dataset, it
is able to generalize on it, hence confirming this is a viable Machine Learning
setting that has practical impact.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Partially-supervised novel object captioning leveraging context from paired data. (arXiv:2109.05115v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05115">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose an approach to improve image captioning solutions
for images with novel objects that do not have caption labels in the training
dataset. Our approach is agnostic to model architecture, and primarily focuses
on training technique that uses existing fully paired image-caption data and
the images with only the novel object detection labels (partially paired data).
We create synthetic paired captioning data for these novel objects by
leveraging context from existing image-caption pairs. We further re-use these
partially paired images with novel objects to create pseudo-label captions that
are used to fine-tune the captioning model. Using a popular captioning model
(Up-Down) as baseline, our approach achieves state-of-the-art results on
held-out MS COCO out-of-domain test split, and improves F1 metric and CIDEr for
novel object images by 75.8 and 26.6 points respectively, compared to baseline
model that does not use partially paired images during training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Co-Correcting: Noise-tolerant Medical Image Classification via mutual Label Correction. (arXiv:2109.05159v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05159">
<div class="article-summary-box-inner">
<span><p>With the development of deep learning, medical image classification has been
significantly improved. However, deep learning requires massive data with
labels. While labeling the samples by human experts is expensive and
time-consuming, collecting labels from crowd-sourcing suffers from the noises
which may degenerate the accuracy of classifiers. Therefore, approaches that
can effectively handle label noises are highly desired. Unfortunately, recent
progress on handling label noise in deep learning has gone largely unnoticed by
the medical image. To fill the gap, this paper proposes a noise-tolerant
medical image classification framework named Co-Correcting, which significantly
improves classification accuracy and obtains more accurate labels through
dual-network mutual learning, label probability estimation, and curriculum
label correcting. On two representative medical image datasets and the MNIST
dataset, we test six latest Learning-with-Noisy-Labels methods and conduct
comparative studies. The experiments show that Co-Correcting achieves the best
accuracy and generalization under different noise ratios in various tasks. Our
project can be found at: https://github.com/JiarunLiu/Co-Correcting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Self-Supervised Deep Framework for Reference Bony Shape Estimation in Orthognathic Surgical Planning. (arXiv:2109.05191v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05191">
<div class="article-summary-box-inner">
<span><p>Virtual orthognathic surgical planning involves simulating surgical
corrections of jaw deformities on 3D facial bony shape models. Due to the lack
of necessary guidance, the planning procedure is highly experience-dependent
and the planning results are often suboptimal. A reference facial bony shape
model representing normal anatomies can provide an objective guidance to
improve planning accuracy. Therefore, we propose a self-supervised deep
framework to automatically estimate reference facial bony shape models. Our
framework is an end-to-end trainable network, consisting of a simulator and a
corrector. In the training stage, the simulator maps jaw deformities of a
patient bone to a normal bone to generate a simulated deformed bone. The
corrector then restores the simulated deformed bone back to normal. In the
inference stage, the trained corrector is applied to generate a
patient-specific normal-looking reference bone from a real deformed bone. The
proposed framework was evaluated using a clinical dataset and compared with a
state-of-the-art method that is based on a supervised point-cloud network.
Experimental results show that the estimated shape models given by our approach
are clinically acceptable and significantly more accurate than that of the
competing method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Follow the Curve: Robotic-Ultrasound Navigation with Learning Based Localization of Spinous Processes for Scoliosis Assessment. (arXiv:2109.05196v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05196">
<div class="article-summary-box-inner">
<span><p>The scoliosis progression in adolescents requires close monitoring to timely
take treatment measures. Ultrasound imaging is a radiation-free and low-cost
alternative in scoliosis assessment to X-rays, which are typically used in
clinical practice. However, ultrasound images are prone to speckle noises,
making it challenging for sonographers to detect bony features and follow the
spine's curvature. This paper introduces a robotic-ultrasound approach for
spinal curvature tracking and automatic navigation. A fully connected network
with deconvolutional heads is developed to locate the spinous process
efficiently with real-time ultrasound images. We use this machine
learning-based method to guide the motion of the robot-held ultrasound probe
and follow the spinal curvature while capturing ultrasound images and
correspondent position. We developed a new force-driven controller that
automatically adjusts the probe's pose relative to the skin surface to ensure a
good acoustic coupling between the probe and skin. After the scanning, the
acquired data is used to reconstruct the coronal spinal image, where the
deformity of the scoliosis spine can be assessed and measured. To evaluate the
performance of our methodology, we conducted an experimental study with human
subjects where the deviations from the image center during the robotized
procedure are compared to that obtained from manual scanning. The angles of
spinal deformity measured on spinal reconstruction images were similar for both
methods, implying that they equally reflect human anatomy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conditional Generation of Synthetic Geospatial Images from Pixel-level and Feature-level Inputs. (arXiv:2109.05201v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05201">
<div class="article-summary-box-inner">
<span><p>Training robust supervised deep learning models for many geospatial
applications of computer vision is difficult due to dearth of class-balanced
and diverse training data. Conversely, obtaining enough training data for many
applications is financially prohibitive or may be infeasible, especially when
the application involves modeling rare or extreme events. Synthetically
generating data (and labels) using a generative model that can sample from a
target distribution and exploit the multi-scale nature of images can be an
inexpensive solution to address scarcity of labeled data. Towards this goal, we
present a deep conditional generative model, called VAE-Info-cGAN, that
combines a Variational Autoencoder (VAE) with a conditional Information
Maximizing Generative Adversarial Network (InfoGAN), for synthesizing
semantically rich images simultaneously conditioned on a pixel-level condition
(PLC) and a macroscopic feature-level condition (FLC). Dimensionally, the PLC
can only vary in the channel dimension from the synthesized image and is meant
to be a task-specific input. The FLC is modeled as an attribute vector in the
latent space of the generated image which controls the contributions of various
characteristic attributes germane to the target distribution. Experiments on a
GPS trajectories dataset show that the proposed model can accurately generate
various forms of spatiotemporal aggregates across different geographic
locations while conditioned only on a raster representation of the road
network. The primary intended application of the VAE-Info-cGAN is synthetic
data (and label) generation for targeted data augmentation for computer
vision-based modeling of problems relevant to geospatial analysis and remote
sensing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Quantization with Code Memory for Unsupervised Image Retrieval. (arXiv:2109.05205v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05205">
<div class="article-summary-box-inner">
<span><p>The high efficiency in computation and storage makes hashing (including
binary hashing and quantization) a common strategy in large-scale retrieval
systems. To alleviate the reliance on expensive annotations, unsupervised deep
hashing becomes an important research problem. This paper provides a novel
solution to unsupervised deep quantization, namely Contrastive Quantization
with Code Memory (MeCoQ). Different from existing reconstruction-based
strategies, we learn unsupervised binary descriptors by contrastive learning,
which can better capture discriminative visual semantics. Besides, we uncover
that codeword diversity regularization is critical to prevent contrastive
learning-based quantization from model degeneration. Moreover, we introduce a
novel quantization code memory module that boosts contrastive learning with
lower feature drift than conventional feature memories. Extensive experiments
on benchmark datasets show that MeCoQ outperforms state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pyramid Hybrid Pooling Quantization for Efficient Fine-Grained Image Retrieval. (arXiv:2109.05206v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05206">
<div class="article-summary-box-inner">
<span><p>Deep hashing approaches, including deep quantization and deep binary hashing,
have become a common solution to large-scale image retrieval due to high
computation and storage efficiency. Most existing hashing methods can not
produce satisfactory results for fine-grained retrieval, because they usually
adopt the outputs of the last CNN layer to generate binary codes, which is less
effective to capture subtle but discriminative visual details. To improve
fine-grained image hashing, we propose Pyramid Hybrid Pooling Quantization
(PHPQ). Specifically, we propose a Pyramid Hybrid Pooling (PHP) module to
capture and preserve fine-grained semantic information from multi-level
features. Besides, we propose a learnable quantization module with a partial
attention mechanism, which helps to optimize the most relevant codewords and
improves the quantization. Comprehensive experiments demonstrate that PHPQ
outperforms state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RobustART: Benchmarking Robustness on Architecture Design and Training Techniques. (arXiv:2109.05211v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05211">
<div class="article-summary-box-inner">
<span><p>Deep neural networks (DNNs) are vulnerable to adversarial noises, which
motivates the benchmark of model robustness. Existing benchmarks mainly focus
on evaluating the defenses, but there are no comprehensive studies of how
architecture design and general training techniques affect robustness.
Comprehensively benchmarking their relationships will be highly beneficial for
better understanding and developing robust DNNs. Thus, we propose RobustART,
the first comprehensive Robustness investigation benchmark on ImageNet
(including open-source toolkit, pre-trained model zoo, datasets, and analyses)
regarding ARchitecture design (44 human-designed off-the-shelf architectures
and 1200+ networks from neural architecture search) and Training techniques
(10+ general techniques, e.g., data augmentation) towards diverse noises
(adversarial, natural, and system noises). Extensive experiments revealed and
substantiated several insights for the first time, for example: (1) adversarial
training largely improves the clean accuracy and all types of robustness for
Transformers and MLP-Mixers; (2) with comparable sizes, CNNs &gt; Transformers &gt;
MLP-Mixers on robustness against natural and system noises; Transformers &gt;
MLP-Mixers &gt; CNNs on adversarial robustness; (3) for some light-weight
architectures (e.g., EfficientNet, MobileNetV2, and MobileNetV3), increasing
model sizes or using extra training data cannot improve robustness. Our
benchmark <a href="http://robust.art/">this http URL</a> : (1) presents an open-source platform for
conducting comprehensive evaluation on diverse robustness types; (2) provides a
variety of pre-trained models with different training techniques to facilitate
robustness evaluation; (3) proposes a new view to better understand the
mechanism towards designing robust DNN architectures, backed up by the
analysis. We will continuously contribute to building this ecosystem for the
community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bornon: Bengali Image Captioning with Transformer-based Deep learning approach. (arXiv:2109.05218v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05218">
<div class="article-summary-box-inner">
<span><p>Image captioning using Encoder-Decoder based approach where CNN is used as
the Encoder and sequence generator like RNN as Decoder has proven to be very
effective. However, this method has a drawback that is sequence needs to be
processed in order. To overcome this drawback some researcher has utilized the
Transformer model to generate captions from images using English datasets.
However, none of them generated captions in Bengali using the transformer
model. As a result, we utilized three different Bengali datasets to generate
Bengali captions from images using the Transformer model. Additionally, we
compared the performance of the transformer-based model with a visual
attention-based Encoder-Decoder approach. Finally, we compared the result of
the transformer-based model with other models that employed different Bengali
image captioning datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Convolutional Hough Matching Networks for Robust and Efficient Visual Correspondence. (arXiv:2109.05221v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05221">
<div class="article-summary-box-inner">
<span><p>Despite advances in feature representation, leveraging geometric relations is
crucial for establishing reliable visual correspondences under large variations
of images. In this work we introduce a Hough transform perspective on
convolutional matching and propose an effective geometric matching algorithm,
dubbed Convolutional Hough Matching (CHM). The method distributes similarities
of candidate matches over a geometric transformation space and evaluates them
in a convolutional manner. We cast it into a trainable neural layer with a
semi-isotropic high-dimensional kernel, which learns non-rigid matching with a
small number of interpretable parameters. To further improve the efficiency of
high-dimensional voting, we also propose to use an efficient kernel
decomposition with center-pivot neighbors, which significantly sparsifies the
proposed semi-isotropic kernels without performance degradation. To validate
the proposed techniques, we develop the neural network with CHM layers that
perform convolutional matching in the space of translation and scaling. Our
method sets a new state of the art on standard benchmarks for semantic visual
correspondence, proving its strong robustness to challenging intra-class
variations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Computer Vision Techniques for Urban Mobility on Large-Scale, Unconstrained Roads. (arXiv:2109.05226v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05226">
<div class="article-summary-box-inner">
<span><p>Conventional approaches for addressing road safety rely on manual
interventions or immobile CCTV infrastructure. Such methods are expensive in
enforcing compliance to traffic rules and do not scale to large road networks.
This paper proposes a simple mobile imaging setup to address several common
problems in road safety at scale. We use recent computer vision techniques to
identify possible irregularities on roads, the absence of street lights, and
defective traffic signs using videos from a moving camera-mounted vehicle.
Beyond the inspection of static road infrastructure, we also demonstrate the
mobile imaging solution's applicability to spot traffic violations. Before
deploying our system in the real-world, we investigate the strengths and
shortcomings of computer vision techniques on thirteen condition-based
hierarchical labels. These conditions include different timings, road type,
traffic density, and state of road damage. Our demonstrations are then carried
out on 2000 km of unconstrained road scenes, captured across an entire city.
Through this, we quantitatively measure the overall safety of roads in the city
through carefully constructed metrics. We also show an interactive dashboard
for visually inspecting and initiating action in a time, labor and
cost-efficient manner. Code, models, and datasets used in this work will be
publicly released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Statistical Representation with Joint Deep Embedded Clustering. (arXiv:2109.05232v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05232">
<div class="article-summary-box-inner">
<span><p>One of the most promising approaches for unsupervised learning is combining
deep representation learning and deep clustering. Some recent works propose to
simultaneously learn representation using deep neural networks and perform
clustering by defining a clustering loss on top of embedded features. However,
these approaches are sensitive to imbalanced data and out-of-distribution
samples. Hence, these methods optimize clustering by pushing data close to
randomly initialized cluster centers. This is problematic when the number of
instances varies largely in different classes or a cluster with few samples has
less chance to be assigned a good centroid. To overcome these limitations, we
introduce StatDEC, a new unsupervised framework for joint statistical
representation learning and clustering. StatDEC simultaneously trains two deep
learning models, a deep statistics network that captures the data distribution,
and a deep clustering network that learns embedded features and performs
clustering by explicitly defining a clustering loss. Specifically, the
clustering network and representation network both take advantage of our
proposed statistics pooling layer that represents mean, variance, and
cardinality to handle the out-of-distribution samples as well as a class
imbalance. Our experiments show that using these representations, one can
considerably improve results on imbalanced image clustering across a variety of
image datasets. Moreover, the learned representations generalize well when
transferred to the out-of-distribution dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Class-Distribution-Aware Calibration for Long-Tailed Visual Recognition. (arXiv:2109.05263v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05263">
<div class="article-summary-box-inner">
<span><p>Despite impressive accuracy, deep neural networks are often miscalibrated and
tend to overly confident predictions. Recent techniques like temperature
scaling (TS) and label smoothing (LS) show effectiveness in obtaining a
well-calibrated model by smoothing logits and hard labels with scalar factors,
respectively. However, the use of uniform TS or LS factor may not be optimal
for calibrating models trained on a long-tailed dataset where the model
produces overly confident probabilities for high-frequency classes. In this
study, we propose class-distribution-aware TS (CDA-TS) and LS (CDA-LS) by
incorporating class frequency information in model calibration in the context
of long-tailed distribution. In CDA-TS, the scalar temperature value is
replaced with the CDA temperature vector encoded with class frequency to
compensate for the over-confidence. Similarly, CDA-LS uses a vector smoothing
factor and flattens the hard labels according to their corresponding class
distribution. We also integrate CDA optimal temperature vector with
distillation loss, which reduces miscalibration in self-distillation (SD). We
empirically show that class-distribution-aware TS and LS can accommodate the
imbalanced data distribution yielding superior performance in both calibration
error and predictive accuracy. We also observe that SD with an extremely
imbalanced dataset is less effective in terms of calibration performance. Code
is available in https://github.com/mobarakol/Class-Distribution-Aware-TS-LS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RVMDE: Radar Validated Monocular Depth Estimation for Robotics. (arXiv:2109.05265v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05265">
<div class="article-summary-box-inner">
<span><p>Stereoscopy exposits a natural perception of distance in a scene, and its
manifestation in 3D world understanding is an intuitive phenomenon. However, an
innate rigid calibration of binocular vision sensors is crucial for accurate
depth estimation. Alternatively, a monocular camera alleviates the limitation
at the expense of accuracy in estimating depth, and the challenge exacerbates
in harsh environmental conditions. Moreover, an optical sensor often fails to
acquire vital signals in harsh environments, and radar is used instead, which
gives coarse but more accurate signals. This work explores the utility of
coarse signals from radar when fused with fine-grained data from a monocular
camera for depth estimation in harsh environmental conditions. A variant of
feature pyramid network (FPN) extensively operates on fine-grained image
features at multiple scales with a fewer number of parameters. FPN feature maps
are fused with sparse radar features extracted with a Convolutional neural
network. The concatenated hierarchical features are used to predict the depth
with ordinal regression. We performed experiments on the nuScenes dataset, and
the proposed architecture stays on top in quantitative evaluations with reduced
parameters and faster inference. The depth estimation results suggest that the
proposed techniques can be used as an alternative to stereo depth estimation in
critical applications in robotics and self-driving cars. The source code will
be available in the following: \url{https://github.com/MI-Hussain/RVMDE}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COSMic: A Coherence-Aware Generation Metric for Image Descriptions. (arXiv:2109.05281v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05281">
<div class="article-summary-box-inner">
<span><p>Developers of text generation models rely on automated evaluation metrics as
a stand-in for slow and expensive manual evaluations. However, image captioning
metrics have struggled to give accurate learned estimates of the semantic and
pragmatic success of output text. We address this weakness by introducing the
first discourse-aware learned generation metric for evaluating image
descriptions. Our approach is inspired by computational theories of discourse
for capturing information goals using coherence. We present a dataset of
image$\unicode{x2013}$description pairs annotated with coherence relations. We
then train a coherence-aware metric on a subset of the Conceptual Captions
dataset and measure its effectiveness$\unicode{x2014}$its ability to predict
human ratings of output captions$\unicode{x2014}$on a test set composed of
out-of-domain images. We demonstrate a higher Kendall Correlation Coefficient
for our proposed metric with the human judgments for the results of a number of
state-of-the-art coherence-aware caption generation models when compared to
several other metrics including recently proposed learned metrics such as
BLEURT and BERTScore.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual-view Snapshot Compressive Imaging via Optical Flow Aided Recurrent Neural Network. (arXiv:2109.05287v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05287">
<div class="article-summary-box-inner">
<span><p>Dual-view snapshot compressive imaging (SCI) aims to capture videos from two
field-of-views (FoVs) using a 2D sensor (detector) in a single snapshot,
achieving joint FoV and temporal compressive sensing, and thus enjoying the
advantages of low-bandwidth, low-power, and low-cost. However, it is
challenging for existing model-based decoding algorithms to reconstruct each
individual scene, which usually require exhaustive parameter tuning with
extremely long running time for large scale data. In this paper, we propose an
optical flow-aided recurrent neural network for dual video SCI systems, which
provides high-quality decoding in seconds. Firstly, we develop a diversity
amplification method to enlarge the differences between scenes of two FoVs, and
design a deep convolutional neural network with dual branches to separate
different scenes from the single measurement. Secondly, we integrate the
bidirectional optical flow extracted from adjacent frames with the recurrent
neural network to jointly reconstruct each video in a sequential manner.
Extensive results on both simulation and real data demonstrate the superior
performance of our proposed model in a short inference time. The code and data
are available at https://github.com/RuiyingLu/OFaNet-for-Dual-view-SCI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BGT-Net: Bidirectional GRU Transformer Network for Scene Graph Generation. (arXiv:2109.05346v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05346">
<div class="article-summary-box-inner">
<span><p>Scene graphs are nodes and edges consisting of objects and object-object
relationships, respectively. Scene graph generation (SGG) aims to identify the
objects and their relationships. We propose a bidirectional GRU (BiGRU)
transformer network (BGT-Net) for the scene graph generation for images. This
model implements novel object-object communication to enhance the object
information using a BiGRU layer. Thus, the information of all objects in the
image is available for the other objects, which can be leveraged later in the
object prediction step. This object information is used in a transformer
encoder to predict the object class as well as to create object-specific edge
information via the use of another transformer encoder. To handle the dataset
bias induced by the long-tailed relationship distribution, softening with a
log-softmax function and adding a bias adaptation term to regulate the bias for
every relation prediction individually showed to be an effective approach. We
conducted an elaborate study on experiments and ablations using open-source
datasets, i.e., Visual Genome, Open-Images, and Visual Relationship Detection
datasets, demonstrating the effectiveness of the proposed model over state of
the art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepPyram: Enabling Pyramid View and Deformable Pyramid Reception for Semantic Segmentation in Cataract Surgery Videos. (arXiv:2109.05352v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05352">
<div class="article-summary-box-inner">
<span><p>Semantic segmentation in cataract surgery has a wide range of applications
contributing to surgical outcome enhancement and clinical risk reduction.
However, the varying issues in segmenting the different relevant instances make
the designation of a unique network quite challenging. This paper proposes a
semantic segmentation network termed as DeepPyram that can achieve superior
performance in segmenting relevant objects in cataract surgery videos with
varying issues. This superiority mainly originates from three modules: (i)
Pyramid View Fusion, which provides a varying-angle global view of the
surrounding region centering at each pixel position in the input convolutional
feature map; (ii) Deformable Pyramid Reception, which enables a wide deformable
receptive field that can adapt to geometric transformations in the object of
interest; and (iii) Pyramid Loss that adaptively supervises multi-scale
semantic feature maps. These modules can effectively boost semantic
segmentation performance, especially in the case of transparency,
deformability, scalability, and blunt edges in objects. The proposed approach
is evaluated using four datasets of cataract surgery for objects with different
contextual features and compared with thirteen state-of-the-art segmentation
networks. The experimental results confirm that DeepPyram outperforms the rival
approaches without imposing additional trainable parameters. Our comprehensive
ablation study further proves the effectiveness of the proposed modules.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Border-SegGCN: Improving Semantic Segmentation by Refining the Border Outline using Graph Convolutional Network. (arXiv:2109.05353v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05353">
<div class="article-summary-box-inner">
<span><p>We present Border-SegGCN, a novel architecture to improve semantic
segmentation by refining the border outline using graph convolutional networks
(GCN). The semantic segmentation network such as Unet or DeepLabV3+ is used as
a base network to have pre-segmented output. This output is converted into a
graphical structure and fed into the GCN to improve the border pixel prediction
of the pre-segmented output. We explored and studied the factors such as border
thickness, number of edges for a node, and the number of features to be fed
into the GCN by performing experiments. We demonstrate the effectiveness of the
Border-SegGCN on the CamVid and Carla dataset, achieving a test set performance
of 81.96% without any post-processing on CamVid dataset. It is higher than the
reported state of the art mIoU achieved on CamVid dataset by 0.404%
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sickle Cell Disease Severity Prediction from Percoll Gradient Images using Graph Convolutional Networks. (arXiv:2109.05372v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05372">
<div class="article-summary-box-inner">
<span><p>Sickle cell disease (SCD) is a severe genetic hemoglobin disorder that
results in premature destruction of red blood cells. Assessment of the severity
of the disease is a challenging task in clinical routine since the causes of
broad variance in SCD manifestation despite the common genetic cause remain
unclear. Identification of the biomarkers that would predict the severity grade
is of importance for prognosis and assessment of responsiveness of patients to
therapy. Detection of the changes in red blood cell (RBC) density through
separation of Percoll density gradient could be such marker as it allows to
resolve intercellular differences and follow the most damaged dense cells prone
to destruction and vaso-occlusion. Quantification of the images obtained from
the distribution of RBCs in Percoll gradient and interpretation of the obtained
is an important prerequisite for establishment of this approach. Here, we
propose a novel approach combining a graph convolutional network, a
convolutional neural network, fast Fourier transform, and recursive feature
elimination to predict the severity of SCD directly from a Percoll image. Two
important but expensive laboratory blood test parameters measurements are used
for training the graph convolutional network. To make the model independent
from such tests during prediction, the two parameters are estimated by a neural
network from the Percoll image directly. On a cohort of 216 subjects, we
achieve a prediction performance that is only slightly below an approach where
the groundtruth laboratory measurements are used. Our proposed method is the
first computational approach for the difficult task of SCD severity prediction.
The two-step approach relies solely on inexpensive and simple blood analysis
tools and can have a significant impact on the patients' survival in
underdeveloped countries where access to medical instruments and doctors is
limited
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Challenges and Solutions in DeepFakes. (arXiv:2109.05397v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05397">
<div class="article-summary-box-inner">
<span><p>Deep learning has been successfully appertained to solve various complex
problems in the area of big data analytics to computer vision. A deep
learning-powered application recently emerged is Deep Fake. It helps to create
fake images and videos that human cannot distinguish them from the real ones
and are recent off-shelf manipulation technique that allows swapping two
identities in a single video. Technology is a controversial technology with
many wide-reaching issues impacting society. So, to counter this emerging
problem, we introduce a dataset of 140k real and fake faces which contain 70k
real faces from the Flickr dataset collected by Nvidia, as well as 70k fake
faces sampled from 1 million fake faces generated by style GAN. We will train
our model in the dataset so that our model can identify real or fake faces.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Team NeuroPoly: Description of the Pipelines for the MICCAI 2021 MS New Lesions Segmentation Challenge. (arXiv:2109.05409v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05409">
<div class="article-summary-box-inner">
<span><p>This paper gives a detailed description of the pipelines used for the 2nd
edition of the MICCAI 2021 Challenge on Multiple Sclerosis Lesion Segmentation.
An overview of the data preprocessing steps applied is provided along with a
brief description of the pipelines used, in terms of the architecture and the
hyperparameters. Our code for this work can be found at:
https://github.com/ivadomed/ms-challenge-2021.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse MLP for Image Recognition: Is Self-Attention Really Necessary?. (arXiv:2109.05422v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05422">
<div class="article-summary-box-inner">
<span><p>Transformers have sprung up in the field of computer vision. In this work, we
explore whether the core self-attention module in Transformer is the key to
achieving excellent performance in image recognition. To this end, we build an
attention-free network called sMLPNet based on the existing MLP-based vision
models. Specifically, we replace the MLP module in the token-mixing step with a
novel sparse MLP (sMLP) module. For 2D image tokens, sMLP applies 1D MLP along
the axial directions and the parameters are shared among rows or columns. By
sparse connection and weight sharing, sMLP module significantly reduces the
number of model parameters and computational complexity, avoiding the common
over-fitting problem that plagues the performance of MLP-like models. When only
trained on the ImageNet-1K dataset, the proposed sMLPNet achieves 81.9% top-1
accuracy with only 24M parameters, which is much better than most CNNs and
vision Transformers under the same model size constraint. When scaling up to
66M parameters, sMLPNet achieves 83.4% top-1 accuracy, which is on par with the
state-of-the-art Swin Transformer. The success of sMLPNet suggests that the
self-attention mechanism is not necessarily a silver bullet in computer vision.
Code will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prioritized Subnet Sampling for Resource-Adaptive Supernet Training. (arXiv:2109.05432v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05432">
<div class="article-summary-box-inner">
<span><p>A resource-adaptive supernet adjusts its subnets for inference to fit the
dynamically available resources. In this paper, we propose Prioritized Subnet
Sampling to train a resource-adaptive supernet, termed PSS-Net. We maintain
multiple subnet pools, each of which stores the information of substantial
subnets with similar resource consumption. Considering a resource constraint,
subnets conditioned on this resource constraint are sampled from a pre-defined
subnet structure space and high-quality ones will be inserted into the
corresponding subnet pool. Then, the sampling will gradually be prone to
sampling subnets from the subnet pools. Moreover, the one with a better
performance metric is assigned with higher priority to train our PSS-Net, if
sampling is from a subnet pool. At the end of training, our PSS-Net retains the
best subnet in each pool to entitle a fast switch of high-quality subnets for
inference when the available resources vary. Experiments on ImageNet using
MobileNetV1/V2 show that our PSS-Net can well outperform state-of-the-art
resource-adaptive supernets. Our project is at
https://github.com/chenbong/PSS-Net.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are Gender-Neutral Queries Really Gender-Neutral? Mitigating Gender Bias in Image Search. (arXiv:2109.05433v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05433">
<div class="article-summary-box-inner">
<span><p>Internet search affects people's cognition of the world, so mitigating biases
in search results and learning fair models is imperative for social good. We
study a unique gender bias in image search in this work: the search images are
often gender-imbalanced for gender-neutral natural language queries. We
diagnose two typical image search models, the specialized model trained on
in-domain datasets and the generalized representation model pre-trained on
massive image and text data across the internet. Both models suffer from severe
gender bias. Therefore, we introduce two novel debiasing approaches: an
in-processing fair sampling method to address the gender imbalance issue for
training models, and a post-processing feature clipping method base on mutual
information to debias multimodal representations of pre-trained models.
Extensive experiments on MS-COCO and Flickr30K benchmarks show that our methods
significantly reduce the gender bias in image search models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cylindrical and Asymmetrical 3D Convolution Networks for LiDAR-based Perception. (arXiv:2109.05441v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05441">
<div class="article-summary-box-inner">
<span><p>State-of-the-art methods for driving-scene LiDAR-based perception (including
point cloud semantic segmentation, panoptic segmentation and 3D detection,
\etc) often project the point clouds to 2D space and then process them via 2D
convolution. Although this cooperation shows the competitiveness in the point
cloud, it inevitably alters and abandons the 3D topology and geometric
relations. A natural remedy is to utilize the 3D voxelization and 3D
convolution network. However, we found that in the outdoor point cloud, the
improvement obtained in this way is quite limited. An important reason is the
property of the outdoor point cloud, namely sparsity and varying density.
Motivated by this investigation, we propose a new framework for the outdoor
LiDAR segmentation, where cylindrical partition and asymmetrical 3D convolution
networks are designed to explore the 3D geometric pattern while maintaining
these inherent properties. The proposed model acts as a backbone and the
learned features from this model can be used for downstream tasks such as point
cloud semantic and panoptic segmentation or 3D detection. In this paper, we
benchmark our model on these three tasks. For semantic segmentation, we
evaluate the proposed model on several large-scale datasets, \ie,
SemanticKITTI, nuScenes and A2D2. Our method achieves the state-of-the-art on
the leaderboard of SemanticKITTI (both single-scan and multi-scan challenge),
and significantly outperforms existing methods on nuScenes and A2D2 dataset.
Furthermore, the proposed 3D framework also shows strong performance and good
generalization on LiDAR panoptic segmentation and LiDAR 3D detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CAN3D: Fast 3D Medical Image Segmentation via Compact Context Aggregation. (arXiv:2109.05443v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05443">
<div class="article-summary-box-inner">
<span><p>Direct automatic segmentation of objects from 3D medical imaging, such as
magnetic resonance (MR) imaging, is challenging as it often involves accurately
identifying a number of individual objects with complex geometries within a
large volume under investigation. To address these challenges, most deep
learning approaches typically enhance their learning capability by
substantially increasing the complexity or the number of trainable parameters
within their models. Consequently, these models generally require long
inference time on standard workstations operating clinical MR systems and are
restricted to high-performance computing hardware due to their large memory
requirement. Further, to fit 3D dataset through these large models using
limited computer memory, trade-off techniques such as patch-wise training are
often used which sacrifice the fine-scale geometric information from input
images which could be clinically significant for diagnostic purposes. To
address these challenges, we present a compact convolutional neural network
with a shallow memory footprint to efficiently reduce the number of model
parameters required for state-of-art performance. This is critical for
practical employment as most clinical environments only have low-end hardware
with limited computing power and memory. The proposed network can maintain data
integrity by directly processing large full-size 3D input volumes with no
patches required and significantly reduces the computational time required for
both training and inference. We also propose a novel loss function with extra
shape constraint to improve the accuracy for imbalanced classes in 3D MR
images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What happens in Face during a facial expression? Using data mining techniques to analyze facial expression motion vectors. (arXiv:2109.05457v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05457">
<div class="article-summary-box-inner">
<span><p>One of the most common problems encountered in human-computer interaction is
automatic facial expression recognition. Although it is easy for human observer
to recognize facial expressions, automatic recognition remains difficult for
machines. One of the methods that machines can recognize facial expression is
analyzing the changes in face during facial expression presentation. In this
paper, optical flow algorithm was used to extract deformation or motion vectors
created in the face because of facial expressions. Then, these extracted motion
vectors are used to be analyzed. Their positions and directions were exploited
for automatic facial expression recognition using different data mining
techniques. It means that by employing motion vector features used as our data,
facial expressions were recognized. Some of the most state-of-the-art
classification algorithms such as C5.0, CRT, QUEST, CHAID, Deep Learning (DL),
SVM and Discriminant algorithms were used to classify the extracted motion
vectors. Using 10-fold cross validation, their performances were calculated. To
compare their performance more precisely, the test was repeated 50 times.
Meanwhile, the deformation of face was also analyzed in this research. For
example, what exactly happened in each part of face when a person showed fear?
Experimental results on Extended Cohen-Kanade (CK+) facial expression dataset
demonstrated that the best methods were DL, SVM and C5.0, with the accuracy of
95.3%, 92.8% and 90.2% respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Re-parameterization Residual Attention Network For Nonhomogeneous Image Dehazing. (arXiv:2109.05479v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05479">
<div class="article-summary-box-inner">
<span><p>This paper proposes an end-to-end Efficient Re-parameterizationResidual
Attention Network(ERRA-Net) to directly restore the nonhomogeneous hazy image.
The contribution of this paper mainly has the following three aspects: 1) A
novel Multi-branch Attention (MA) block. The spatial attention mechanism better
reconstructs high-frequency features, and the channel attention mechanism
treats the features of different channels differently. Multi-branch structure
dramatically improves the representation ability of the model and can be
changed into a single path structure after re-parameterization to speed up the
process of inference. Local Residual Connection allows the low-frequency
information in the nonhomogeneous area to pass through the block without
processing so that the block can focus on detailed features. 2) A lightweight
network structure. We use cascaded MA blocks to extract high-frequency features
step by step, and the Multi-layer attention fusion tail combines the shallow
and deep features of the model to get the residual of the clean image finally.
3)We propose two novel loss functions to help reconstruct the hazy image
ColorAttenuation loss and Laplace Pyramid loss. ERRA-Net has an impressive
speed, processing 1200x1600 HD quality images with an average runtime of 166.11
fps. Extensive evaluations demonstrate that ERSANet performs favorably against
the SOTA approaches on the real-world hazy images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Facial Anatomical Landmark Detection using Regularized Transfer Learning with Application to Fetal Alcohol Syndrome Recognition. (arXiv:2109.05485v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05485">
<div class="article-summary-box-inner">
<span><p>Fetal alcohol syndrome (FAS) caused by prenatal alcohol exposure can result
in a series of cranio-facial anomalies, and behavioral and neurocognitive
problems. Current diagnosis of FAS is typically done by identifying a set of
facial characteristics, which are often obtained by manual examination.
Anatomical landmark detection, which provides rich geometric information, is
important to detect the presence of FAS associated facial anomalies. This
imaging application is characterized by large variations in data appearance and
limited availability of labeled data. Current deep learning-based heatmap
regression methods designed for facial landmark detection in natural images
assume availability of large datasets and are therefore not wellsuited for this
application. To address this restriction, we develop a new regularized transfer
learning approach that exploits the knowledge of a network learned on large
facial recognition datasets. In contrast to standard transfer learning which
focuses on adjusting the pre-trained weights, the proposed learning approach
regularizes the model behavior. It explicitly reuses the rich visual semantics
of a domain-similar source model on the target task data as an additional
supervisory signal for regularizing landmark detection optimization.
Specifically, we develop four regularization constraints for the proposed
transfer learning, including constraining the feature outputs from
classification and intermediate layers, as well as matching activation
attention maps in both spatial and channel levels. Experimental evaluation on a
collected clinical imaging dataset demonstrate that the proposed approach can
effectively improve model generalizability under limited training samples, and
is advantageous to other approaches in the literature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ArtiBoost: Boosting Articulated 3D Hand-Object Pose Estimation via Online Exploration and Synthesis. (arXiv:2109.05488v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05488">
<div class="article-summary-box-inner">
<span><p>Estimating the articulated 3D hand-object pose from a single RGB image is a
highly ambiguous and challenging problem requiring large-scale datasets that
contain diverse hand poses, object poses, and camera viewpoints. Most
real-world datasets lack this diversity. In contrast, synthetic datasets can
easily ensure vast diversity, but learning from them is inefficient and suffers
from heavy training consumption. To address the above issues, we propose
ArtiBoost, a lightweight online data enrichment method that boosts articulated
hand-object pose estimation from the data perspective. ArtiBoost is employed
along with a real-world source dataset. During training, ArtiBoost
alternatively performs data exploration and synthesis. ArtiBoost can cover
various hand-object poses and camera viewpoints based on a Compositional
hand-object Configuration and Viewpoint space (CCV-space) and can adaptively
enrich the current hard-discernable samples by a mining strategy. We apply
ArtiBoost on a simple learning baseline network and demonstrate the performance
boost on several hand-object benchmarks. As an illustrative example, with
ArtiBoost, even a simple baseline network can outperform the previous
start-of-the-art based on Transformer on the HO3D dataset. Our code is
available at https://github.com/MVIG-SJTU/ArtiBoost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LEA-Net: Layer-wise External Attention Network for Efficient Color Anomaly Detection. (arXiv:2109.05493v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05493">
<div class="article-summary-box-inner">
<span><p>The utilization of prior knowledge about anomalies is an essential issue for
anomaly detections. Recently, the visual attention mechanism has become a
promising way to improve the performance of CNNs for some computer vision
tasks. In this paper, we propose a novel model called Layer-wise External
Attention Network (LEA-Net) for efficient image anomaly detection. The core
idea relies on the integration of unsupervised and supervised anomaly detectors
via the visual attention mechanism. Our strategy is as follows: (i) Prior
knowledge about anomalies is represented as the anomaly map generated by
unsupervised learning of normal instances, (ii) The anomaly map is translated
to an attention map by the external network, (iii) The attention map is then
incorporated into intermediate layers of the anomaly detection network.
Notably, this layer-wise external attention can be applied to any CNN model in
an end-to-end training manner. For a pilot study, we validate LEA-Net on color
anomaly detection tasks. Through extensive experiments on PlantVillage, MVTec
AD, and Cloud datasets, we demonstrate that the proposed layer-wise visual
attention mechanism consistently boosts anomaly detection performances of an
existing CNN model, even on imbalanced datasets. Moreover, we show that our
attention mechanism successfully boosts the performance of several CNN models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Complex Constrained Total Variation Image Denoising Algorithm with Application to Phase Retrieval. (arXiv:2109.05496v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05496">
<div class="article-summary-box-inner">
<span><p>This paper considers the constrained total variation (TV) denoising problem
for complex-valued images. We extend the definition of TV seminorms for
real-valued images to dealing with complex-valued ones. In particular, we
introduce two types of complex TV in both isotropic and anisotropic forms. To
solve the constrained denoising problem, we adopt a dual approach and derive an
accelerated gradient projection algorithm. We further generalize the proposed
denoising algorithm as a key building block of the proximal gradient scheme to
solve a vast class of complex constrained optimization problems with TV
regularizers. As an example, we apply the proposed algorithmic framework to
phase retrieval. We combine the complex TV regularizer with the conventional
projection-based method within the constraint complex TV model. Initial results
from both simulated and optical experiments demonstrate the validity of the
constrained TV model in extracting sparsity priors within complex-valued
images, while also utilizing physically tractable constraints that help speed
up convergence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Check Your Other Door! Establishing Backdoor Attacks in the Frequency Domain. (arXiv:2109.05507v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05507">
<div class="article-summary-box-inner">
<span><p>Deep Neural Networks (DNNs) have been utilized in various applications
ranging from image classification and facial recognition to medical imagery
analysis and real-time object detection. As our models become more
sophisticated and complex, the computational cost of training such models
becomes a burden for small companies and individuals; for this reason,
outsourcing the training process has been the go-to option for such users.
Unfortunately, outsourcing the training process comes at the cost of
vulnerability to backdoor attacks. These attacks aim at establishing hidden
backdoors in the DNN such that the model performs well on benign samples but
outputs a particular target label when a trigger is applied to the input.
Current backdoor attacks rely on generating triggers in the image/pixel domain;
however, as we show in this paper, it is not the only domain to exploit and one
should always "check the other doors". In this work, we propose a complete
pipeline for generating a dynamic, efficient, and invisible backdoor attack in
the frequency domain. We show the advantages of utilizing the frequency domain
for establishing undetectable and powerful backdoor attacks through extensive
experiments on various datasets and network architectures. The backdoored
models are shown to break various state-of-the-art defences. We also show two
possible defences that succeed against frequency-based backdoor attacks and
possible ways for the attacker to bypass them. We conclude the work with some
remarks regarding a network's learning capacity and the capability of embedding
a backdoor attack in the model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Robust Monocular Visual Odometry for Flying Robots on Planetary Missions. (arXiv:2109.05509v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05509">
<div class="article-summary-box-inner">
<span><p>In the future, extraterrestrial expeditions will not only be conducted by
rovers but also by flying robots. The technical demonstration drone Ingenuity,
that just landed on Mars, will mark the beginning of a new era of exploration
unhindered by terrain traversability. Robust self-localization is crucial for
that. Cameras that are lightweight, cheap and information-rich sensors are
already used to estimate the ego-motion of vehicles. However, methods proven to
work in man-made environments cannot simply be deployed on other planets. The
highly repetitive textures present in the wastelands of Mars pose a huge
challenge to descriptor matching based approaches.
</p>
<p>In this paper, we present an advanced robust monocular odometry algorithm
that uses efficient optical flow tracking to obtain feature correspondences
between images and a refined keyframe selection criterion. In contrast to most
other approaches, our framework can also handle rotation-only motions that are
particularly challenging for monocular odometry systems. Furthermore, we
present a novel approach to estimate the current risk of scale drift based on a
principal component analysis of the relative translation information matrix.
This way we obtain an implicit measure of uncertainty. We evaluate the validity
of our approach on all sequences of a challenging real-world dataset captured
in a Mars-like environment and show that it outperforms state-of-the-art
approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Constructing Phrase-level Semantic Labels to Form Multi-Grained Supervision for Image-Text Retrieval. (arXiv:2109.05523v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05523">
<div class="article-summary-box-inner">
<span><p>Existing research for image text retrieval mainly relies on sentence-level
supervision to distinguish matched and mismatched sentences for a query image.
However, semantic mismatch between an image and sentences usually happens in
finer grain, i.e., phrase level. In this paper, we explore to introduce
additional phrase-level supervision for the better identification of mismatched
units in the text. In practice, multi-grained semantic labels are automatically
constructed for a query image in both sentence-level and phrase-level. We
construct text scene graphs for the matched sentences and extract entities and
triples as the phrase-level labels. In order to integrate both supervision of
sentence-level and phrase-level, we propose Semantic Structure Aware Multimodal
Transformer (SSAMT) for multi-modal representation learning. Inside the SSAMT,
we utilize different kinds of attention mechanisms to enforce interactions of
multi-grain semantic units in both sides of vision and language. For the
training, we propose multi-scale matching losses from both global and local
perspectives, and penalize mismatched phrases. Experimental results on MS-COCO
and Flickr30K show the effectiveness of our approach compared to some
state-of-the-art models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Decidability-Based Loss Function. (arXiv:2109.05524v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05524">
<div class="article-summary-box-inner">
<span><p>Nowadays, deep learning is the standard approach for a wide range of
problems, including biometrics, such as face recognition and speech
recognition, etc. Biometric problems often use deep learning models to extract
features from images, also known as embeddings. Moreover, the loss function
used during training strongly influences the quality of the generated
embeddings. In this work, a loss function based on the decidability index is
proposed to improve the quality of embeddings for the verification routine. Our
proposal, the D-loss, avoids some Triplet-based loss disadvantages such as the
use of hard samples and tricky parameter tuning, which can lead to slow
convergence. The proposed approach is compared against the Softmax
(cross-entropy), Triplets Soft-Hard, and the Multi Similarity losses in four
different benchmarks: MNIST, Fashion-MNIST, CIFAR10 and CASIA-IrisV4. The
achieved results show the efficacy of the proposal when compared to other
popular metrics in the literature. The D-loss computation, besides being
simple, non-parametric and easy to implement, favors both the inter-class and
intra-class scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Unsupervised Deep-Learning Method for Fingerprint Classification: the CCAE Network and the Hybrid Clustering Strategy. (arXiv:2109.05526v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05526">
<div class="article-summary-box-inner">
<span><p>The fingerprint classification is an important and effective method to
quicken the process and improve the accuracy in the fingerprint matching
process. Conventional supervised methods need a large amount of pre-labeled
data and thus consume immense human resources. In this paper, we propose a new
and efficient unsupervised deep learning method that can extract fingerprint
features and classify fingerprint patterns automatically. In this approach, a
new model named constraint convolutional auto-encoder (CCAE) is used to extract
fingerprint features and a hybrid clustering strategy is applied to obtain the
final clusters. A set of experiments in the NIST-DB4 dataset shows that the
proposed unsupervised method exhibits the efficient performance on fingerprint
classification. For example, the CCAE achieves an accuracy of 97.3% on only
1000 unlabeled fingerprints in the NIST-DB4.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DSSL: Deep Surroundings-person Separation Learning for Text-based Person Retrieval. (arXiv:2109.05534v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05534">
<div class="article-summary-box-inner">
<span><p>Many previous methods on text-based person retrieval tasks are devoted to
learning a latent common space mapping, with the purpose of extracting
modality-invariant features from both visual and textual modality.
Nevertheless, due to the complexity of high-dimensional data, the unconstrained
mapping paradigms are not able to properly catch discriminative clues about the
corresponding person while drop the misaligned information. Intuitively, the
information contained in visual data can be divided into person information
(PI) and surroundings information (SI), which are mutually exclusive from each
other. To this end, we propose a novel Deep Surroundings-person Separation
Learning (DSSL) model in this paper to effectively extract and match person
information, and hence achieve a superior retrieval accuracy. A
surroundings-person separation and fusion mechanism plays the key role to
realize an accurate and effective surroundings-person separation under a
mutually exclusion constraint. In order to adequately utilize multi-modal and
multi-granular information for a higher retrieval accuracy, five diverse
alignment paradigms are adopted. Extensive experiments are carried out to
evaluate the proposed DSSL on CUHK-PEDES, which is currently the only
accessible dataset for text-base person retrieval task. DSSL achieves the
state-of-the-art performance on CUHK-PEDES. To properly evaluate our proposed
DSSL in the real scenarios, a Real Scenarios Text-based Person Reidentification
(RSTPReid) dataset is constructed to benefit future research on text-based
person retrieval, which will be publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BioLCNet: Reward-modulated Locally Connected Spiking Neural Networks. (arXiv:2109.05539v1 [cs.NE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05539">
<div class="article-summary-box-inner">
<span><p>Recent studies have shown that convolutional neural networks (CNNs) are not
the only feasible solution for image classification. Furthermore, weight
sharing and backpropagation used in CNNs do not correspond to the mechanisms
present in the primate visual system. To propose a more biologically plausible
solution, we designed a locally connected spiking neural network (SNN) trained
using spike-timing-dependent plasticity (STDP) and its reward-modulated variant
(R-STDP) learning rules. The use of spiking neurons and local connections along
with reinforcement learning (RL) led us to the nomenclature BioLCNet for our
proposed architecture. Our network consists of a rate-coded input layer
followed by a locally connected hidden layer and a decoding output layer. A
spike population-based voting scheme is adopted for decoding in the output
layer. We used the MNIST dataset to obtain image classification accuracy and to
assess the robustness of our rewarding system to varying target responses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Domain Adaptive Learning via Synthetic Data for Person Re-identification. (arXiv:2109.05542v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05542">
<div class="article-summary-box-inner">
<span><p>Person re-identification (re-ID) has gained more and more attention due to
its widespread applications in intelligent video surveillance. Unfortunately,
the mainstream deep learning methods still need a large quantity of labeled
data to train models, and annotating data is an expensive work in real-world
scenarios. In addition, due to domain gaps between different datasets, the
performance is dramatically decreased when re-ID models pre-trained on
label-rich datasets (source domain) are directly applied to other unlabeled
datasets (target domain). In this paper, we attempt to remedy these problems
from two aspects, namely data and methodology. Firstly, we develop a data
collector to automatically generate synthetic re-ID samples in a computer game,
and construct a data labeler to simultaneously annotate them, which free humans
from heavy data collections and annotations. Based on them, we build two
synthetic person re-ID datasets with different scales, "GSPR" and "mini-GSPR"
datasets. Secondly, we propose a synthesis-based multi-domain collaborative
refinement (SMCR) network, which contains a synthetic pretraining module and
two collaborative-refinement modules to implement sufficient learning for the
valuable knowledge from multiple domains. Extensive experiments show that our
proposed framework obtains significant performance improvements over the
state-of-the-art methods on multiple unsupervised domain adaptation tasks of
person re-ID.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SphereFace Revived: Unifying Hyperspherical Face Recognition. (arXiv:2109.05565v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05565">
<div class="article-summary-box-inner">
<span><p>This paper addresses the deep face recognition problem under an open-set
protocol, where ideal face features are expected to have smaller maximal
intra-class distance than minimal inter-class distance under a suitably chosen
metric space. To this end, hyperspherical face recognition, as a promising line
of research, has attracted increasing attention and gradually become a major
focus in face recognition research. As one of the earliest works in
hyperspherical face recognition, SphereFace explicitly proposed to learn face
embeddings with large inter-class angular margin. However, SphereFace still
suffers from severe training instability which limits its application in
practice. In order to address this problem, we introduce a unified framework to
understand large angular margin in hyperspherical face recognition. Under this
framework, we extend the study of SphereFace and propose an improved variant
with substantially better training stability -- SphereFace-R. Specifically, we
propose two novel ways to implement the multiplicative margin, and study
SphereFace-R under three different feature normalization schemes (no feature
normalization, hard feature normalization and soft feature normalization). We
also propose an implementation strategy -- "characteristic gradient detachment"
-- to stabilize training. Extensive experiments on SphereFace-R show that it is
consistently better than or competitive with state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PQ-Transformer: Jointly Parsing 3D Objects and Layouts from Point Clouds. (arXiv:2109.05566v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05566">
<div class="article-summary-box-inner">
<span><p>3D scene understanding from point clouds plays a vital role for various
robotic applications. Unfortunately, current state-of-the-art methods use
separate neural networks for different tasks like object detection or room
layout estimation. Such a scheme has two limitations: 1) Storing and running
several networks for different tasks are expensive for typical robotic
platforms. 2) The intrinsic structure of separate outputs are ignored and
potentially violated. To this end, we propose the first transformer
architecture that predicts 3D objects and layouts simultaneously, using point
cloud inputs. Unlike existing methods that either estimate layout keypoints or
edges, we directly parameterize room layout as a set of quads. As such, the
proposed architecture is termed as P(oint)Q(uad)-Transformer. Along with the
novel quad representation, we propose a tailored physical constraint loss
function that discourages object-layout interference. The quantitative and
qualitative evaluations on the public benchmark ScanNet show that the proposed
PQ-Transformer succeeds to jointly parse 3D objects and layouts, running at a
quasi-real-time (8.91 FPS) rate without efficiency-oriented optimization.
Moreover, the new physical constraint loss can improve strong baselines, and
the F1-score of the room layout is significantly promoted from 37.9% to 57.9%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MovieCuts: A New Dataset and Benchmark for Cut Type Recognition. (arXiv:2109.05569v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05569">
<div class="article-summary-box-inner">
<span><p>Understanding movies and their structural patterns is a crucial task to
decode the craft of video editing. While previous works have developed tools
for general analysis such as detecting characters or recognizing cinematography
properties at the shot level, less effort has been devoted to understanding the
most basic video edit, the Cut. This paper introduces the cut type recognition
task, which requires modeling of multi-modal information. To ignite research in
the new task, we construct a large-scale dataset called MovieCuts, which
contains more than 170K videoclips labeled among ten cut types. We benchmark a
series of audio-visual approaches, including some that deal with the problem's
multi-modal and multi-label nature. Our best model achieves 45.7% mAP, which
suggests that the task is challenging and that attaining highly accurate cut
type recognition is an open research problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Joint Graph and Image Convolution Network for Automatic Brain Tumor Segmentation. (arXiv:2109.05580v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05580">
<div class="article-summary-box-inner">
<span><p>We present a joint graph convolution-image convolution neural network as our
submission to the Brain Tumor Segmentation (BraTS) 2021 challenge. We model
each brain as a graph composed of distinct image regions, which is initially
segmented by a graph neural network (GNN). Subsequently, the tumorous volume
identified by the GNN is further refined by a simple (voxel) convolutional
neural network (CNN), which produces the final segmentation. This approach
captures both global brain feature interactions via the graphical
representation and local image details through the use of convolutional
filters. We find that the GNN component by itself can effectively identify and
segment the brain tumors. The addition of the CNN further improves the median
performance of the model by 2 percent across all metrics evaluated. On the
validation set, our joint GNN-CNN model achieves mean Dice scores of 0.89,
0.81, 0.73 and mean Hausdorff distances (95th percentile) of 6.8, 12.6, 28.2mm
on the whole tumor, core tumor, and enhancing tumor, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">U-Net Convolutional Network for Recognition of Vessels and Materials in Chemistry Lab. (arXiv:2109.05585v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05585">
<div class="article-summary-box-inner">
<span><p>Convolutional networks have been widely applied for computer vision system.
Encouraged by these results, a U-Net convolutional network was applied to
recognition of vessels and materials in chemistry lab using the recent
Vector-LabPics dataset, which contains 2187 images of materials within mostly
transparent vessels in a chemistry lab and other general settings, labeled with
13 classes. By optimizing hyperparameters including learning rates and learning
rate decays, 87% accuracy in vessel recognition was achieved. In the case of
relatively small training and test sets (relatively rare materials states, the
number of training set samples less than 500 and the number of test set samples
less than 100), a comprehensive improvement over 18% in IoU and 19% in accuracy
for the best model were achieved. Further improvements may be achievable by
incorporating improved convolutional network structure into our models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiresolution Deep Implicit Functions for 3D Shape Representation. (arXiv:2109.05591v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05591">
<div class="article-summary-box-inner">
<span><p>We introduce Multiresolution Deep Implicit Functions (MDIF), a hierarchical
representation that can recover fine geometry detail, while being able to
perform global operations such as shape completion. Our model represents a
complex 3D shape with a hierarchy of latent grids, which can be decoded into
different levels of detail and also achieve better accuracy. For shape
completion, we propose latent grid dropout to simulate partial data in the
latent space and therefore defer the completing functionality to the decoder
side. This along with our multires design significantly improves the shape
completion quality under decoder-only latent optimization. To the best of our
knowledge, MDIF is the first deep implicit function model that can at the same
time (1) represent different levels of detail and allow progressive decoding;
(2) support both encoder-decoder inference and decoder-only latent
optimization, and fulfill multiple applications; (3) perform detailed
decoder-only shape completion. Experiments demonstrate its superior performance
against prior art in various 3D reconstruction tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MSGDD-cGAN: Multi-Scale Gradients Dual Discriminator Conditional Generative Adversarial Network. (arXiv:2109.05614v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05614">
<div class="article-summary-box-inner">
<span><p>Conditional Generative Adversarial Networks (cGANs) have been used in many
image processing tasks. However, they still have serious problems maintaining
the balance between conditioning the output on the input and creating the
output with the desired distribution based on the corresponding ground truth.
The traditional cGANs, similar to most conventional GANs, suffer from vanishing
gradients, which backpropagate from the discriminator to the generator.
Moreover, the traditional cGANs are sensitive to architectural changes due to
previously mentioned gradient problems. Therefore, balancing the architecture
of the cGANs is almost impossible. Recently MSG-GAN has been proposed to
stabilize the performance of the GANs by applying multiple connections between
the generator and discriminator. In this work, we propose a method called
MSGDD-cGAN, which first stabilizes the performance of the cGANs using
multi-connections gradients flow. Secondly, the proposed network architecture
balances the correlation of the output to input and the fitness of the output
on the target distribution. This balance is generated by using the proposed
dual discrimination procedure. We tested our model by segmentation of fetal
ultrasound images. Our model shows a 3.18% increase in the F1 score comparing
to the pix2pix version of cGANs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differential Diagnosis of Frontotemporal Dementia and Alzheimer's Disease using Generative Adversarial Network. (arXiv:2109.05627v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05627">
<div class="article-summary-box-inner">
<span><p>Frontotemporal dementia and Alzheimer's disease are two common forms of
dementia and are easily misdiagnosed as each other due to their similar pattern
of clinical symptoms. Differentiating between the two dementia types is crucial
for determining disease-specific intervention and treatment. Recent development
of Deep-learning-based approaches in the field of medical image computing are
delivering some of the best performance for many binary classification tasks,
although its application in differential diagnosis, such as neuroimage-based
differentiation for multiple types of dementia, has not been explored. In this
study, a novel framework was proposed by using the Generative Adversarial
Network technique to distinguish FTD, AD and normal control subjects, using
volumetric features extracted at coarse-to-fine structural scales from Magnetic
Resonance Imaging scans. Experiments of 10-folds cross-validation on 1,954
images achieved high accuracy. With the proposed framework, we have
demonstrated that the combination of multi-scale structural features and
synthetic data augmentation based on generative adversarial network can improve
the performance of challenging tasks such as differentiating Dementia
sub-types.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Datasets of 3D Garments with Sewing Patterns. (arXiv:2109.05633v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05633">
<div class="article-summary-box-inner">
<span><p>Garments are ubiquitous in both real and many of the virtual worlds. They are
highly deformable objects, exhibit an immense variety of designs and shapes,
and yet, most garments are created from a set of regularly shaped flat pieces.
Exploration of garment structure presents a peculiar case for an object
structure estimation task and might prove useful for downstream tasks of neural
3D garment modeling and reconstruction by providing strong prior on garment
shapes. To facilitate research in these directions, we propose a method for
generating large synthetic datasets of 3D garment designs and their sewing
patterns. Our method consists of a flexible description structure for
specifying parametric sewing pattern templates and the automatic generation
pipeline to produce garment 3D models with little-to-none manual intervention.
To add realism, the pipeline additionally creates corrupted versions of the
final meshes that imitate artifacts of 3D scanning.
</p>
<p>With this pipeline, we created the first large-scale synthetic dataset of 3D
garment models with their sewing patterns. The dataset contains more than 20000
garment design variations produced from 19 different base types. Seven of these
garment types are specifically designed to target evaluation of the
generalization across garment sewing pattern topologies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On The Radon-Nikodym Spectral Approach With Optimal Clustering. (arXiv:1906.00460v17 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1906.00460">
<div class="article-summary-box-inner">
<span><p>Problems of interpolation, classification, and clustering are considered. In
the tenets of Radon--Nikodym approach $\langle f(\mathbf{x})\psi^2 \rangle /
\langle\psi^2\rangle$, where the $\psi(\mathbf{x})$ is a linear function on
input attributes, all the answers are obtained from a generalized eigenproblem
$|f|\psi^{[i]}\rangle = \lambda^{[i]} |\psi^{[i]}\rangle$. The solution to the
interpolation problem is a regular Radon-Nikodym derivative. The solution to
the classification problem requires prior and posterior probabilities that are
obtained using the Lebesgue quadrature[1] technique. Whereas in a Bayesian
approach new observations change only outcome probabilities, in the
Radon-Nikodym approach not only outcome probabilities but also the probability
space $|\psi^{[i]}\rangle$ change with new observations. This is a remarkable
feature of the approach: both the probabilities and the probability space are
constructed from the data. The Lebesgue quadrature technique can be also
applied to the optimal clustering problem. The problem is solved by
constructing a Gaussian quadrature on the Lebesgue measure. A distinguishing
feature of the Radon-Nikodym approach is the knowledge of the invariant group:
all the answers are invariant relatively any non-degenerated linear transform
of input vector $\mathbf{x}$ components. A software product implementing the
algorithms of interpolation, classification, and optimal clustering is
available from the authors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DynaNet: Neural Kalman Dynamical Model for Motion Estimation and Prediction. (arXiv:1908.03918v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1908.03918">
<div class="article-summary-box-inner">
<span><p>Dynamical models estimate and predict the temporal evolution of physical
systems. State Space Models (SSMs) in particular represent the system dynamics
with many desirable properties, such as being able to model uncertainty in both
the model and measurements, and optimal (in the Bayesian sense) recursive
formulations e.g. the Kalman Filter. However, they require significant domain
knowledge to derive the parametric form and considerable hand-tuning to
correctly set all the parameters. Data driven techniques e.g. Recurrent Neural
Networks have emerged as compelling alternatives to SSMs with wide success
across a number of challenging tasks, in part due to their ability to extract
relevant features from rich inputs. They however lack interpretability and
robustness to unseen conditions. In this work, we present DynaNet, a hybrid
deep learning and time-varying state-space model which can be trained
end-to-end. Our neural Kalman dynamical model allows us to exploit the relative
merits of each approach. We demonstrate state-of-the-art estimation and
prediction on a number of physically challenging tasks, including visual
odometry, sensor fusion for visual-inertial navigation and pendulum control. In
addition we show how DynaNet can indicate failures through investigation of
properties such as the rate of innovation (Kalman Gain).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Automatic Threat Detection: A Survey of Advances of Deep Learning within X-ray Security Imaging. (arXiv:2001.01293v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2001.01293">
<div class="article-summary-box-inner">
<span><p>X-ray security screening is widely used to maintain aviation/transport
security, and its significance poses a particular interest in automated
screening systems. This paper aims to review computerised X-ray security
imaging algorithms by taxonomising the field into conventional machine learning
and contemporary deep learning applications. The first part briefly discusses
the classical machine learning approaches utilised within X-ray security
imaging, while the latter part thoroughly investigates the use of modern deep
learning algorithms. The proposed taxonomy sub-categorises the use of deep
learning approaches into supervised, semi-supervised and unsupervised learning,
with a particular focus on object classification, detection, segmentation and
anomaly detection tasks. The paper further explores well-established X-ray
datasets and provides a performance benchmark. Based on the current and future
trends in deep learning, the paper finally presents a discussion and future
directions for X-ray security imagery.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating the Progress of Deep Learning for Visual Relational Concepts. (arXiv:2001.10857v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2001.10857">
<div class="article-summary-box-inner">
<span><p>Convolutional Neural Networks (CNNs) have become the state of the art method
for image classification in the last ten years. Despite the fact that they
achieve superhuman classification accuracy on many popular datasets, they often
perform much worse on more abstract image classification tasks. We will show
that these difficult tasks are linked to relational concepts from cognitive
psychology and that despite progress over the last few years, such relational
reasoning tasks still remain difficult for current neural network
architectures.
</p>
<p>We will review deep learning research that is linked to relational concept
learning, even if it was not originally presented from this angle. Reviewing
the current literature, we will argue that some form of attention will be an
important component of future systems to solve relational tasks.
</p>
<p>In addition, we will point out the shortcomings of currently used datasets,
and we will recommend steps to make future datasets more relevant for testing
systems on relational reasoning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pixel-in-Pixel Net: Towards Efficient Facial Landmark Detection in the Wild. (arXiv:2003.03771v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.03771">
<div class="article-summary-box-inner">
<span><p>Recently, heatmap regression models have become popular due to their superior
performance in locating facial landmarks. However, three major problems still
exist among these models: (1) they are computationally expensive; (2) they
usually lack explicit constraints on global shapes; (3) domain gaps are
commonly present. To address these problems, we propose Pixel-in-Pixel Net
(PIPNet) for facial landmark detection. The proposed model is equipped with a
novel detection head based on heatmap regression, which conducts score and
offset predictions simultaneously on low-resolution feature maps. By doing so,
repeated upsampling layers are no longer necessary, enabling the inference time
to be largely reduced without sacrificing model accuracy. Besides, a simple but
effective neighbor regression module is proposed to enforce local constraints
by fusing predictions from neighboring landmarks, which enhances the robustness
of the new detection head. To further improve the cross-domain generalization
capability of PIPNet, we propose self-training with curriculum. This training
strategy is able to mine more reliable pseudo-labels from unlabeled data across
domains by starting with an easier task, then gradually increasing the
difficulty to provide more precise labels. Extensive experiments demonstrate
the superiority of PIPNet, which obtains state-of-the-art results on three out
of six popular benchmarks under the supervised setting. The results on two
cross-domain test sets are also consistently improved compared to the
baselines. Notably, our lightweight version of PIPNet runs at 35.7 FPS and 200
FPS on CPU and GPU, respectively, while still maintaining a competitive
accuracy to state-of-the-art methods. The code of PIPNet is available at
https://github.com/jhb86253817/PIPNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anomaly Detection in Medical Imaging with Deep Perceptual Autoencoders. (arXiv:2006.13265v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.13265">
<div class="article-summary-box-inner">
<span><p>Anomaly detection is the problem of recognizing abnormal inputs based on the
seen examples of normal data. Despite recent advances of deep learning in
recognizing image anomalies, these methods still prove incapable of handling
complex medical images, such as barely visible abnormalities in chest X-rays
and metastases in lymph nodes. To address this problem, we introduce a new
powerful method of image anomaly detection. It relies on the classical
autoencoder approach with a re-designed training pipeline to handle
high-resolution, complex images and a robust way of computing an image
abnormality score. We revisit the very problem statement of fully unsupervised
anomaly detection, where no abnormal examples at all are provided during the
model setup. We propose to relax this unrealistic assumption by using a very
small number of anomalies of confined variability merely to initiate the search
of hyperparameters of the model. We evaluate our solution on natural image
datasets with a known benchmark, as well as on two medical datasets containing
radiology and digital pathology images. The proposed approach suggests a new
strong baseline for image anomaly detection and outperforms state-of-the-art
approaches in complex medical image analysis tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stabilizing Deep Tomographic Reconstruction. (arXiv:2008.01846v5 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.01846">
<div class="article-summary-box-inner">
<span><p>Tomographic image reconstruction with deep learning is an emerging field, but
a recent landmark study reveals that several deep reconstruction networks are
unstable for computed tomography (CT) and magnetic resonance imaging (MRI).
Specifically, three kinds of instabilities were reported: (1) strong image
artefacts from tiny perturbations, (2) small features missing in a deeply
reconstructed image, and (3) decreased imaging performance with increased input
data. On the other hand, compressed sensing (CS) inspired reconstruction
methods do not suffer from these instabilities because of their built-in kernel
awareness. For deep reconstruction to realize its full potential and become a
mainstream approach for tomographic imaging, it is thus critically important to
meet this challenge by stabilizing deep reconstruction networks. Here we
propose an Analytic Compressed Iterative Deep (ACID) framework to address this
challenge. ACID synergizes a deep reconstruction network trained on big data,
kernel awareness from CS-inspired processing, and iterative refinement to
minimize the data residual relative to real measurement. Our study demonstrates
that the deep reconstruction using ACID is accurate and stable, and sheds light
on the converging mechanism of the ACID iteration under a Bounded Relative
Error Norm (BREN) condition. In particular, the study shows that ACID-based
reconstruction is resilient against adversarial attacks, superior to classic
sparsity-regularized reconstruction alone, and eliminates the three kinds of
instabilities. We anticipate that this integrative data-driven approach will
help promote development and translation of deep tomographic image
reconstruction networks into clinical applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-supervised Semantic Segmentation of Prostate and Organs-at-Risk on 3D Pelvic CT Images. (arXiv:2009.09571v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.09571">
<div class="article-summary-box-inner">
<span><p>Automated segmentation can assist radiotherapy treatment planning by saving
manual contouring efforts and reducing intra-observer and inter-observer
variations. The recent development of deep learning approaches has revoluted
medical data processing, including semantic segmentation, by dramatically
improving performance. However, training effective deep learning models usually
require a large amount of high-quality labeled data, which are often costly to
collect. We developed a novel semi-supervised adversarial deep learning
approach for 3D pelvic CT image semantic segmentation. Unlike supervised deep
learning methods, the new approach can utilize both annotated and un-annotated
data for training. It generates un-annotated synthetic data by a data
augmentation scheme using generative adversarial networks (GANs). We applied
the new approach to segmenting multiple organs in male pelvic CT images, where
CT images without annotations and GAN-synthesized un-annotated images were used
in semi-supervised learning. Experimental results, evaluated by three metrics
(Dice similarity coefficient, average Hausdorff distance, and average surface
Hausdorff distance), showed that the new method achieved either comparable
performance with substantially fewer annotated images or better performance
with the same amount of annotated data, outperforming the existing
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Impact of lung segmentation on the diagnosis and explanation of COVID-19 in chest X-ray images. (arXiv:2009.09780v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.09780">
<div class="article-summary-box-inner">
<span><p>COVID-19 frequently provokes pneumonia, which can be diagnosed using imaging
exams. Chest X-ray (CXR) is often useful because it is cheap, fast, widespread,
and uses less radiation. Here, we demonstrate the impact of lung segmentation
in COVID-19 identification using CXR images and evaluate which contents of the
image influenced the most. Semantic segmentation was performed using a U-Net
CNN architecture, and the classification using three CNN architectures (VGG,
ResNet, and Inception). Explainable Artificial Intelligence techniques were
employed to estimate the impact of segmentation. A three-classes database was
composed: lung opacity (pneumonia), COVID-19, and normal. We assessed the
impact of creating a CXR image database from different sources, and the
COVID-19 generalization from one source to another. The segmentation achieved a
Jaccard distance of 0.034 and a Dice coefficient of 0.982. The classification
using segmented images achieved an F1-Score of 0.88 for the multi-class setup,
and 0.83 for COVID-19 identification. In the cross-dataset scenario, we
obtained an F1-Score of 0.74 and an area under the ROC curve of 0.9 for
COVID-19 identification using segmented images. Experiments support the
conclusion that even after segmentation, there is a strong bias introduced by
underlying factors from different sources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attention Augmented ConvLSTM for Environment Prediction. (arXiv:2010.09662v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.09662">
<div class="article-summary-box-inner">
<span><p>Safe and proactive planning in robotic systems generally requires accurate
predictions of the environment. Prior work on environment prediction applied
video frame prediction techniques to bird's-eye view environment
representations, such as occupancy grids. ConvLSTM-based frameworks used
previously often result in significant blurring and vanishing of moving
objects, thus hindering their applicability for use in safety-critical
applications. In this work, we propose two extensions to the ConvLSTM to
address these issues. We present the Temporal Attention Augmented ConvLSTM
(TAAConvLSTM) and Self-Attention Augmented ConvLSTM (SAAConvLSTM) frameworks
for spatiotemporal occupancy prediction, and demonstrate improved performance
over baseline architectures on the real-world KITTI and Waymo datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anomaly Detection in Video via Self-Supervised and Multi-Task Learning. (arXiv:2011.07491v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.07491">
<div class="article-summary-box-inner">
<span><p>Anomaly detection in video is a challenging computer vision problem. Due to
the lack of anomalous events at training time, anomaly detection requires the
design of learning methods without full supervision. In this paper, we approach
anomalous event detection in video through self-supervised and multi-task
learning at the object level. We first utilize a pre-trained detector to detect
objects. Then, we train a 3D convolutional neural network to produce
discriminative anomaly-specific information by jointly learning multiple proxy
tasks: three self-supervised and one based on knowledge distillation. The
self-supervised tasks are: (i) discrimination of forward/backward moving
objects (arrow of time), (ii) discrimination of objects in
consecutive/intermittent frames (motion irregularity) and (iii) reconstruction
of object-specific appearance information. The knowledge distillation task
takes into account both classification and detection information, generating
large prediction discrepancies between teacher and student models when
anomalies occur. To the best of our knowledge, we are the first to approach
anomalous event detection in video as a multi-task learning problem,
integrating multiple self-supervised and knowledge distillation proxy tasks in
a single architecture. Our lightweight architecture outperforms the
state-of-the-art methods on three benchmarks: Avenue, ShanghaiTech and UCSD
Ped2. Additionally, we perform an ablation study demonstrating the importance
of integrating self-supervised learning and normality-specific distillation in
a multi-task learning setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking deinterlacing for early interlaced videos. (arXiv:2011.13675v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.13675">
<div class="article-summary-box-inner">
<span><p>With the rapid development of image restoration techniques, high-definition
reconstruction of early videos has achieved impressive results. However, there
are few studies about the interlacing artifacts that often appear in early
videos and significantly affect visual perception. Traditional deinterlacing
approaches are mainly focused on early interlacing scanning systems and thus
cannot handle the complex and complicated artifacts in real-world early
interlaced videos. Hence, this paper proposes a specific deinterlacing network
(DIN), which is motivated by the traditional deinterlacing strategy. The
proposed DIN consists of two stages, i.e., a cooperative vertical interpolation
stage for split fields, and a merging stage that is applied to perceive
movements and remove ghost artifacts. Experimental results demonstrate that the
proposed method can effectively remove complex artifacts in early interlaced
videos.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DEF: Deep Estimation of Sharp Geometric Features in 3D Shapes. (arXiv:2011.15081v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.15081">
<div class="article-summary-box-inner">
<span><p>We propose Deep Estimators of Features (DEFs), a learning-based framework for
predicting sharp geometric features in sampled 3D shapes. Differently from
existing data-driven methods, which reduce this problem to feature
classification, we propose to regress a scalar field representing the distance
from point samples to the closest feature line on local patches. Our approach
is the first that scales to massive point clouds by fusing distance-to-feature
estimates obtained on individual patches. We extensively evaluate our approach
against five baselines on newly proposed synthetic and real-world 3D CAD model
benchmarks. Our approach not only outperforms the baselines (with improvements
in Recall and False Positives Rates), but generalizes to real-world scans after
training our model on synthetic data and fine-tuning it on a small dataset of
scanned data. We demonstrate a downstream application, where we reconstruct an
explicit representation of straight and curved sharp feature lines from range
scan data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CPF: Learning a Contact Potential Field to Model the Hand-Object Interaction. (arXiv:2012.00924v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.00924">
<div class="article-summary-box-inner">
<span><p>Modeling the hand-object (HO) interaction not only requires estimation of the
HO pose, but also pays attention to the contact due to their interaction.
Significant progress has been made in estimating hand and object separately
with deep learning methods, simultaneous HO pose estimation and contact
modeling has not yet been fully explored. In this paper, we present an explicit
contact representation namely Contact Potential Field (CPF), and a
learning-fitting hybrid framework namely MIHO to Modeling the Interaction of
Hand and Object. In CPF, we treat each contacting HO vertex pair as a
spring-mass system. Hence the whole system forms a potential field with minimal
elastic energy at the grasp position. Extensive experiments on the two commonly
used benchmarks have demonstrated that our method can achieve state-of-the-art
in several reconstruction metrics, and allow us to produce more physically
plausible HO pose even when the ground-truth exhibits severe interpenetration
or disjointedness. Our code is available at https://github.com/lixiny/CPF.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Guided Image Captioning Performance across Domains. (arXiv:2012.02339v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.02339">
<div class="article-summary-box-inner">
<span><p>Image captioning models generally lack the capability to take into account
user interest, and usually default to global descriptions that try to balance
readability, informativeness, and information overload. On the other hand, VQA
models generally lack the ability to provide long descriptive answers, while
expecting the textual question to be quite precise. We present a method to
control the concepts that an image caption should focus on, using an additional
input called the guiding text that refers to either groundable or ungroundable
concepts in the image. Our model consists of a Transformer-based multimodal
encoder that uses the guiding text together with global and object-level image
features to derive early-fusion representations used to generate the guided
caption. While models trained on Visual Genome data have an in-domain advantage
of fitting well when guided with automatic object labels, we find that guided
captioning models trained on Conceptual Captions generalize better on
out-of-domain images and guiding texts. Our human-evaluation results indicate
that attempting in-the-wild guided image captioning requires access to large,
unrestricted-domain training datasets, and that increased style diversity (even
without increasing the number of unique tokens) is a key factor for improved
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatiotemporal Pattern Mining for Nowcasting Extreme Earthquakes in Southern California. (arXiv:2012.14336v3 [physics.geo-ph] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.14336">
<div class="article-summary-box-inner">
<span><p>Geoscience and seismology have utilized the most advanced technologies and
equipment to monitor seismic events globally from the past few decades. With
the enormous amount of data, modern GPU-powered deep learning presents a
promising approach to analyze data and discover patterns. In recent years,
there are plenty of successful deep learning models for picking seismic waves.
However, forecasting extreme earthquakes, which can cause disasters, is still
an underdeveloped topic in history. Relevant research in spatiotemporal
dynamics mining and forecasting has revealed some successful predictions, a
crucial topic in many scientific research fields. Most studies of them have
many successful applications of using deep neural networks. In Geology and
Earth science studies, earthquake prediction is one of the world's most
challenging problems, about which cutting-edge deep learning technologies may
help discover some valuable patterns. In this project, we propose a deep
learning modeling approach, namely \tseqpre, to mine spatiotemporal patterns
from data to nowcast extreme earthquakes by discovering visual dynamics in
regional coarse-grained spatial grids over time. In this modeling approach, we
use synthetic deep learning neural networks with domain knowledge in geoscience
and seismology to exploit earthquake patterns for prediction using
convolutional long short-term memory neural networks. Our experiments show a
strong correlation between location prediction and magnitude prediction for
earthquakes in Southern California. Ablation studies and visualization validate
the effectiveness of the proposed modeling method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Temporal Dynamics from Cycles in Narrated Video. (arXiv:2101.02337v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.02337">
<div class="article-summary-box-inner">
<span><p>Learning to model how the world changes as time elapses has proven a
challenging problem for the computer vision community. We propose a
self-supervised solution to this problem using temporal cycle consistency
jointly in vision and language, training on narrated video. Our model learns
modality-agnostic functions to predict forward and backward in time, which must
undo each other when composed. This constraint leads to the discovery of
high-level transitions between moments in time, since such transitions are
easily inverted and shared across modalities. We justify the design of our
model with an ablation study on different configurations of the cycle
consistency problem. We then show qualitatively and quantitatively that our
approach yields a meaningful, high-level model of the future and past. We apply
the learned dynamics model without further training to various tasks, such as
predicting future action and temporally ordering sets of images. Project page:
https://dave.ml/mmcc
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TypeNet: Deep Learning Keystroke Biometrics. (arXiv:2101.05570v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.05570">
<div class="article-summary-box-inner">
<span><p>We study the performance of Long Short-Term Memory networks for keystroke
biometric authentication at large scale in free-text scenarios. For this we
explore the performance of Long Short-Term Memory (LSTMs) networks trained with
a moderate number of keystrokes per identity and evaluated under different
scenarios including: i) three learning approaches depending on the loss
function (softmax, contrastive, and triplet loss); ii) different number of
training samples and lengths of keystroke sequences; iii) four databases based
on two device types (physical vs touchscreen keyboard); and iv) comparison with
existing approaches based on both traditional statistical methods and deep
learning architectures. Our approach called TypeNet achieves state-of-the-art
keystroke biometric authentication performance with an Equal Error Rate of 2.2%
and 9.2% for physical and touchscreen keyboards, respectively, significantly
outperforming previous approaches. Our experiments demonstrate a moderate
increase in error with up to 100,000 subjects, demonstrating the potential of
TypeNet to operate at an Internet scale. To the best of our knowledge, the
databases used in this work are the largest existing free-text keystroke
databases available for research with more than 136 million keystrokes from
168,000 subjects in physical keyboards, and 60,000 subjects with more than 63
million keystrokes acquired on mobile touchscreens.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Channelized Axial Attention for Semantic Segmentation -- Considering Channel Relation within Spatial Attention for Semantic Segmentation. (arXiv:2101.07434v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.07434">
<div class="article-summary-box-inner">
<span><p>Spatial and channel attentions, modelling the semantic interdependencies in
spatial and channel dimensions respectively, have recently been widely used for
semantic segmentation. However, computing spatial and channel attentions
separately sometimes causes errors, especially for those difficult cases. In
this paper, we propose Channelized Axial Attention (CAA) to seamlessly
integrate channel attention and spatial attention into a single operation with
negligible computation overhead. Specifically, we break down the dot-product
operation of the spatial attention into two parts and insert channel relation
in between, allowing for independently optimized channel attention on each
spatial location. We further develop grouped vectorization, which allows our
model to run with very little memory consumption without slowing down the
running speed. Comparative experiments conducted on multiple benchmark
datasets, including Cityscapes, PASCAL Context, and COCO-Stuff, demonstrate
that our CAA outperforms many state-of-the-art segmentation models (including
dual attention) on all tested datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep One-Class Classification via Interpolated Gaussian Descriptor. (arXiv:2101.10043v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.10043">
<div class="article-summary-box-inner">
<span><p>One-class classification (OCC) aims to learn an effective data description to
enclose all normal training samples and detect anomalies based on the deviation
from the data description. Current state-of-the-art OCC models learn a compact
normality description by hyper-sphere minimisation, but they often suffer from
overfitting the training data, especially when the training set is small or
contaminated with anomalous samples. To address this issue, we introduce the
interpolated Gaussian descriptor (IGD) method, a novel OCC model that learns a
one-class Gaussian anomaly classifier trained with adversarially interpolated
training samples. The Gaussian anomaly classifier differentiates the training
samples based on their distance to the Gaussian centre and the standard
deviation of these distances, offering the model a discriminability w.r.t. the
given samples during training. The adversarial interpolation is enforced to
consistently learn a smooth Gaussian descriptor, even when the training data is
small or contaminated with anomalous samples. This enables our model to learn
the data description based on the representative normal samples rather than
fringe or anomalous samples, resulting in significantly improved normality
description. In extensive experiments on diverse popular benchmarks, including
MNIST, Fashion MNIST, CIFAR10, MVTec AD and two medical datasets, IGD achieves
better detection accuracy than current state-of-the-art models. IGD also shows
better robustness in problems with small or contaminated training sets. Code is
available at https://github.com/tianyu0207/IGD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Temporal Memory Attention for Video Semantic Segmentation. (arXiv:2102.08643v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.08643">
<div class="article-summary-box-inner">
<span><p>Video semantic segmentation requires to utilize the complex temporal
relations between frames of the video sequence. Previous works usually exploit
accurate optical flow to leverage the temporal relations, which suffer much
from heavy computational cost. In this paper, we propose a Temporal Memory
Attention Network (TMANet) to adaptively integrate the long-range temporal
relations over the video sequence based on the self-attention mechanism without
exhaustive optical flow prediction. Specially, we construct a memory using
several past frames to store the temporal information of the current frame. We
then propose a temporal memory attention module to capture the relation between
the current frame and the memory to enhance the representation of the current
frame. Our method achieves new state-of-the-art performances on two challenging
video semantic segmentation datasets, particularly 80.3% mIoU on Cityscapes and
76.5% mIoU on CamVid with ResNet-50.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MosaicOS: A Simple and Effective Use of Object-Centric Images for Long-Tailed Object Detection. (arXiv:2102.08884v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.08884">
<div class="article-summary-box-inner">
<span><p>Many objects do not appear frequently enough in complex scenes (e.g., certain
handbags in living rooms) for training an accurate object detector, but are
often found frequently by themselves (e.g., in product images). Yet, these
object-centric images are not effectively leveraged for improving object
detection in scene-centric images. In this paper, we propose Mosaic of
Object-centric images as Scene-centric images (MosaicOS), a simple and novel
framework that is surprisingly effective at tackling the challenges of
long-tailed object detection. Keys to our approach are three-fold: (i) pseudo
scene-centric image construction from object-centric images for mitigating
domain differences, (ii) high-quality bounding box imputation using the
object-centric images' class labels, and (iii) a multi-stage training
procedure. On LVIS object detection (and instance segmentation), MosaicOS leads
to a massive 60% (and 23%) relative improvement in average precision for rare
object categories. We also show that our framework can be compatibly used with
other existing approaches to achieve even further gains. Our pre-trained models
are publicly available at https://github.com/czhang0528/MosaicOS/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Neural Network Subspaces. (arXiv:2102.10472v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.10472">
<div class="article-summary-box-inner">
<span><p>Recent observations have advanced our understanding of the neural network
optimization landscape, revealing the existence of (1) paths of high accuracy
containing diverse solutions and (2) wider minima offering improved
performance. Previous methods observing diverse paths require multiple training
runs. In contrast we aim to leverage both property (1) and (2) with a single
method and in a single training run. With a similar computational cost as
training one model, we learn lines, curves, and simplexes of high-accuracy
neural networks. These neural network subspaces contain diverse solutions that
can be ensembled, approaching the ensemble performance of independently trained
networks without the training cost. Moreover, using the subspace midpoint
boosts accuracy, calibration, and robustness to label noise, outperforming
Stochastic Weight Averaging.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OMNet: Learning Overlapping Mask for Partial-to-Partial Point Cloud Registration. (arXiv:2103.00937v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.00937">
<div class="article-summary-box-inner">
<span><p>Point cloud registration is a key task in many computational fields. Previous
correspondence matching based methods require the inputs to have distinctive
geometric structures to fit a 3D rigid transformation according to point-wise
sparse feature matches. However, the accuracy of transformation heavily relies
on the quality of extracted features, which are prone to errors with respect to
partiality and noise. In addition, they can not utilize the geometric knowledge
of all the overlapping regions. On the other hand, previous global feature
based approaches can utilize the entire point cloud for the registration,
however they ignore the negative effect of non-overlapping points when
aggregating global features. In this paper, we present OMNet, a global feature
based iterative network for partial-to-partial point cloud registration. We
learn overlapping masks to reject non-overlapping regions, which converts the
partial-to-partial registration to the registration of the same shape.
Moreover, the previously used data is sampled only once from the CAD models for
each object, resulting in the same point clouds for the source and reference.
We propose a more practical manner of data generation where a CAD model is
sampled twice for the source and reference, avoiding the previously prevalent
over-fitting issue. Experimental results show that our method achieves
state-of-the-art performance compared to traditional and deep learning based
methods. Code is available at https://github.com/megvii-research/OMNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved Techniques for Quantizing Deep Networks with Adaptive Bit-Widths. (arXiv:2103.01435v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.01435">
<div class="article-summary-box-inner">
<span><p>Quantizing deep networks with adaptive bit-widths is a promising technique
for efficient inference across many devices and resource constraints. In
contrast to static methods that repeat the quantization process and train
different models for different constraints, adaptive quantization enables us to
flexibly adjust the bit-widths of a single deep network during inference for
instant adaptation in different scenarios. While existing research shows
encouraging results on common image classification benchmarks, this paper
investigates how to train such adaptive networks more effectively.
Specifically, we present two novel techniques for quantizing deep neural
networks with adaptive bit-widths of weights and activations. First, we propose
a collaborative strategy to choose a high-precision teacher for transferring
knowledge to the low-precision student while jointly optimizing the model with
all bit-widths. Second, to effectively transfer knowledge, we develop a dynamic
block swapping method by randomly replacing the blocks in the lower-precision
student network with the corresponding blocks in the higher-precision teacher
network. Extensive experiments on multiple image classification datasets
including video classification benchmarks for the first time, well demonstrate
the efficacy of our approach over state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MonteFloor: Extending MCTS for Reconstructing Accurate Large-Scale Floor Plans. (arXiv:2103.11161v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.11161">
<div class="article-summary-box-inner">
<span><p>We propose a novel method for reconstructing floor plans from noisy 3D point
clouds. Our main contribution is a principled approach that relies on the Monte
Carlo Tree Search (MCTS) algorithm to maximize a suitable objective function
efficiently despite the complexity of the problem. Like previous work, we first
project the input point cloud to a top view to create a density map and extract
room proposals from it. Our method selects and optimizes the polygonal shapes
of these room proposals jointly to fit the density map and outputs an accurate
vectorized floor map even for large complex scenes. To do this, we adapted
MCTS, an algorithm originally designed to learn to play games, to select the
room proposals by maximizing an objective function combining the fitness with
the density map as predicted by a deep network and regularizing terms on the
room shapes. We also introduce a refinement step to MCTS that adjusts the shape
of the room proposals. For this step, we propose a novel differentiable method
for rendering the polygonal shapes of these proposals. We evaluate our method
on the recent and challenging Structured3D and Floor-SP datasets and show a
significant improvement over the state-of-the-art, without imposing any hard
constraints nor assumptions on the floor plan configurations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">iMAP: Implicit Mapping and Positioning in Real-Time. (arXiv:2103.12352v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.12352">
<div class="article-summary-box-inner">
<span><p>We show for the first time that a multilayer perceptron (MLP) can serve as
the only scene representation in a real-time SLAM system for a handheld RGB-D
camera. Our network is trained in live operation without prior data, building a
dense, scene-specific implicit 3D model of occupancy and colour which is also
immediately used for tracking.
</p>
<p>Achieving real-time SLAM via continual training of a neural network against a
live image stream requires significant innovation. Our iMAP algorithm uses a
keyframe structure and multi-processing computation flow, with dynamic
information-guided pixel sampling for speed, with tracking at 10 Hz and global
map updating at 2 Hz. The advantages of an implicit MLP over standard dense
SLAM techniques include efficient geometry representation with automatic detail
control and smooth, plausible filling-in of unobserved regions such as the back
surfaces of objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic-guided Automatic Natural Image Matting with Trimap Generation Network and Light-weight Non-local Attention. (arXiv:2103.17020v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.17020">
<div class="article-summary-box-inner">
<span><p>Natural image matting aims to precisely separate foreground objects from
background using alpha matte. Fully automatic natural image matting without
external annotation is challenging. Well-performed matting methods usually
require accurate labor-intensive handcrafted trimap as extra input, while the
performance of automatic trimap generation method of dilating foreground
segmentation fluctuates with segmentation quality. Therefore, we argue that how
to handle trade-off of additional information input is a major issue in
automatic matting. This paper presents a semantic-guided automatic natural
image matting pipeline with Trimap Generation Network and light-weight
non-local attention, which does not need trimap and background as input.
Specifically, guided by foreground segmentation, Trimap Generation Network
estimates accurate trimap. Then, with estimated trimap as guidance, our
light-weight Non-local Matting Network with Refinement produces final alpha
matte, whose trimap-guided global aggregation attention block is equipped with
stride downsampling convolution, reducing computation complexity and promoting
performance. Experimental results show that our matting algorithm has
competitive performance with state-of-the-art methods in both trimap-free and
trimap-needed aspects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Noise Estimation for Generative Diffusion Models. (arXiv:2104.02600v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.02600">
<div class="article-summary-box-inner">
<span><p>Generative diffusion models have emerged as leading models in speech and
image generation. However, in order to perform well with a small number of
denoising steps, a costly tuning of the set of noise parameters is needed. In
this work, we present a simple and versatile learning scheme that can
step-by-step adjust those noise parameters, for any given number of steps,
while the previous work needs to retune for each number separately.
Furthermore, without modifying the weights of the diffusion model, we are able
to significantly improve the synthesis results, for a small number of steps.
Our approach comes at a negligible computation cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Jacobian Regularization for Mitigating Universal Adversarial Perturbations. (arXiv:2104.10459v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.10459">
<div class="article-summary-box-inner">
<span><p>Universal Adversarial Perturbations (UAPs) are input perturbations that can
fool a neural network on large sets of data. They are a class of attacks that
represents a significant threat as they facilitate realistic, practical, and
low-cost attacks on neural networks. In this work, we derive upper bounds for
the effectiveness of UAPs based on norms of data-dependent Jacobians. We
empirically verify that Jacobian regularization greatly increases model
robustness to UAPs by up to four times whilst maintaining clean performance.
Our theoretical analysis also allows us to formulate a metric for the strength
of shared adversarial perturbations between pairs of inputs. We apply this
metric to benchmark datasets and show that it is highly correlated with the
actual observed robustness. This suggests that realistic and practical
universal attacks can be reliably mitigated without sacrificing clean accuracy,
which shows promise for the robustness of machine learning systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pyramid Medical Transformer for Medical Image Segmentation. (arXiv:2104.14702v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.14702">
<div class="article-summary-box-inner">
<span><p>Deep neural networks have been a prevailing technique in the field of medical
image processing. However, the most popular convolutional neural networks
(CNNs) based methods for medical image segmentation are imperfect because they
model long-range dependencies by stacking layers or enlarging filters.
Transformers and the self-attention mechanism are recently proposed to
effectively learn long-range dependencies by modeling all pairs of word-to-word
attention regardless of their positions. The idea has also been extended to the
computer vision field by creating and treating image patches as embeddings.
Considering the computation complexity for whole image self-attention, current
transformer-based models settle for a rigid partitioning scheme that
potentially loses informative relations. Besides, current medical transformers
model global context on full resolution images, leading to unnecessary
computation costs. To address these issues, we developed a novel method to
integrate multi-scale attention and CNN feature extraction using a pyramidal
network architecture, namely Pyramid Medical Transformer (PMTrans). The PMTrans
captured multi-range relations by working on multi-resolution images. An
adaptive partitioning scheme was implemented to retain informative relations
and to access different receptive fields efficiently. Experimental results on
three medical image datasets (gland segmentation, MoNuSeg, and HECKTOR
datasets) showed that PMTrans outperformed the latest CNN-based and
transformer-based models for medical image segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Internet of Things (IoT) Based Video Analytics: a use case of Smart Doorbell. (arXiv:2105.06508v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.06508">
<div class="article-summary-box-inner">
<span><p>The vision of the internet of things (IoT) is a reality now. IoT devices are
getting cheaper, smaller. They are becoming more and more computationally and
energy-efficient. The global market of IoT-based video analytics has seen
significant growth in recent years and it is expected to be a growing market
segment. For any IoT-based video analytics application, few key points
required, such as cost-effectiveness, widespread use, flexible design, accurate
scene detection, reusability of the framework. Video-based smart doorbell
system is one such application domain for video analytics where many commercial
offerings are available in the consumer market. However, such existing
offerings are costly, monolithic, and proprietary. Also, there will be a
trade-off between accuracy and portability. To address the foreseen problems,
I'm proposing a distributed framework for video analytics with a use case of a
smart doorbell system. The proposed framework uses AWS cloud services as a base
platform and to meet the price affordability constraint, the system was
implemented on affordable Raspberry Pi. The smart doorbell will be able to
recognize the known/unknown person with at most accuracy. The smart doorbell
system is also having additional detection functionalities such as harmful
weapon detection, noteworthy vehicle detection, animal/pet detection. An iOS
application is specifically developed for this implementation which can receive
the notification from the smart doorbell in real-time. Finally, the paper also
mentions the classical approaches for video analytics, their feasibility in
implementing with this use-case, and comparative analysis in terms of accuracy
and time required to detect an object in the frame is carried out. Results
conclude that AWS cloud-based approach is worthy for this smart doorbell use
case.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">More Than Just Attention: Improving Cross-Modal Attentions with Contrastive Constraints for Image-Text Matching. (arXiv:2105.09597v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.09597">
<div class="article-summary-box-inner">
<span><p>Cross-modal attention mechanisms have been widely applied to the image-text
matching task and have achieved remarkable improvements thanks to its
capability of learning fine-grained relevance across different modalities.
However, the cross-modal attention models of existing methods could be
sub-optimal and inaccurate because there is no direct supervision provided
during the training process. In this work, we propose two novel training
strategies, namely Contrastive Content Re-sourcing (CCR) and Contrastive
Content Swapping (CCS) constraints, to address such limitations. These
constraints supervise the training of cross-modal attention models in a
contrastive learning manner without requiring explicit attention annotations.
They are plug-in training strategies and can be easily integrated into existing
cross-modal attention models. Additionally, we introduce three metrics
including Attention Precision, Recall, and F1-Score to quantitatively measure
the quality of learned attention models. We evaluate the proposed constraints
by incorporating them into four state-of-the-art cross-modal attention-based
image-text matching models. Experimental results on both Flickr30k and MS-COCO
datasets demonstrate that integrating these constraints improves the model
performance in terms of both retrieval performance and attention metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiple Domain Experts Collaborative Learning: Multi-Source Domain Generalization For Person Re-Identification. (arXiv:2105.12355v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.12355">
<div class="article-summary-box-inner">
<span><p>Recent years have witnessed significant progress in person re-identification
(ReID). However, current ReID approaches still suffer from considerable
performance degradation when unseen testing domains exhibit different
characteristics from the source training ones, known as the domain
generalization problem. Given multiple source training domains, previous Domain
Generalizable ReID (DG-ReID) methods usually learn all domains together using a
shared network, which can't learn sufficient knowledge from each domain. In
this paper, we propose a novel Multiple Domain Experts Collaborative Learning
(MECL) framework for better exploiting all training domains, which benefits
from the proposed Domain-Domain Collaborative Learning (DDCL) and
Universal-Domain Collaborative Learning (UDCL). DDCL utilizes domain-specific
experts for fully exploiting each domain, and prevents experts from
over-fitting the corresponding domain using a meta-learning strategy. In UDCL,
a universal expert supervises the learning of domain experts and continuously
gathers knowledge from all domain experts. Note, only the universal expert will
be used for inference. Extensive experiments on DG-ReID benchmarks demonstrate
the effectiveness of DDCL and UDCL, and show that the whole MECL framework
significantly outperforms state-of-the-arts. Experimental results on
DG-classification benchmarks also reveal the great potential of applying MECL
to other DG tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FINet: Dual Branches Feature Interaction for Partial-to-Partial Point Cloud Registration. (arXiv:2106.03479v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03479">
<div class="article-summary-box-inner">
<span><p>Data association is important in the point cloud registration. In this work,
we propose to solve the partial-to-partial registration from a new perspective,
by introducing multi-level feature interactions between the source and the
reference clouds at the feature extraction stage, such that the registration
can be realized without the attentions or explicit mask estimation for the
overlapping detection as adopted previously. Specifically, we present FINet, a
feature interaction-based structure with the capability to enable and
strengthen the information associating between the inputs at multiple stages.
To achieve this, we first split the features into two components, one for
rotation and one for translation, based on the fact that they belong to
different solution spaces, yielding a dual branches structure. Second, we
insert several interaction modules at the feature extractor for the data
association. Third, we propose a transformation sensitivity loss to obtain
rotation-attentive and translation-attentive features. Experiments demonstrate
that our method performs higher precision and robustness compared to the
state-of-the-art traditional and learning-based methods. Code will be available
at https://github.com/HaoXu-Work/FINet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CDN-MEDAL: Two-stage Density and Difference Approximation Framework for Motion Analysis. (arXiv:2106.03776v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03776">
<div class="article-summary-box-inner">
<span><p>Background modeling is a promising research area in video analysis with a
variety of video surveillance applications. Recent years have witnessed the
proliferation of deep neural networks via effective learning-based approaches
in motion analysis. However, these techniques only provide a limited
description of the observed scenes' insufficient properties where a
single-valued mapping is learned to approximate the temporal conditional
averages of the target background. On the other hand, statistical learning in
imagery domains has become one of the most prevalent approaches with high
adaptation to dynamic context transformation, notably Gaussian Mixture Models,
combined with a foreground extraction step. In this work, we propose a novel,
two-stage method of change detection with two convolutional neural networks.
The first architecture is grounded on the unsupervised Gaussian mixtures
statistical learning to describe the scenes' salient features. The second one
implements a light-weight pipeline of foreground detection. Our two-stage
framework contains approximately 3.5K parameters in total but still maintains
rapid convergence to intricate motion patterns. Our experiments on publicly
available datasets show that our proposed networks are not only capable of
generalizing regions of moving objects in unseen cases with promising results
but also are competitive in performance efficiency and effectiveness regarding
foreground segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hyperspectral and Multispectral Classification for Coastal Wetland Using Depthwise Feature Interaction Network. (arXiv:2106.06896v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06896">
<div class="article-summary-box-inner">
<span><p>The monitoring of coastal wetlands is of great importance to the protection
of marine and terrestrial ecosystems. However, due to the complex environment,
severe vegetation mixture, and difficulty of access, it is impossible to
accurately classify coastal wetlands and identify their species with
traditional classifiers. Despite the integration of multisource remote sensing
data for performance enhancement, there are still challenges with acquiring and
exploiting the complementary merits from multisource data. In this paper, the
Deepwise Feature Interaction Network (DFINet) is proposed for wetland
classification. A depthwise cross attention module is designed to extract
self-correlation and cross-correlation from multisource feature pairs. In this
way, meaningful complementary information is emphasized for classification.
DFINet is optimized by coordinating consistency loss, discrimination loss, and
classification loss. Accordingly, DFINet reaches the standard solution-space
under the regularity of loss functions, while the spatial consistency and
feature discrimination are preserved. Comprehensive experimental results on two
hyperspectral and multispectral wetland datasets demonstrate that the proposed
DFINet outperforms other competitive methods in terms of overall accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VSAC: Efficient and Accurate Estimator for H and F. (arXiv:2106.10240v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.10240">
<div class="article-summary-box-inner">
<span><p>We present VSAC, a RANSAC-type robust estimator with a number of novelties.
It benefits from the introduction of the concept of independent inliers that
improves significantly the efficacy of the dominant plane handling and, also,
allows near error-free rejection of incorrect models, without false positives.
The local optimization process and its application is improved so that it is
run on average only once. Further technical improvements include adaptive
sequential hypothesis verification and efficient model estimation via Gaussian
elimination. Experiments on four standard datasets show that VSAC is
significantly faster than all its predecessors and runs on average in 1-2 ms,
on a CPU. It is two orders of magnitude faster and yet as precise as MAGSAC++,
the currently most accurate estimator of two-view geometry. In the repeated
runs on EVD, HPatches, PhotoTourism, and Kusvod2 datasets, it never failed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PERT: A Progressively Region-based Network for Scene Text Removal. (arXiv:2106.13029v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.13029">
<div class="article-summary-box-inner">
<span><p>Scene text removal (STR) contains two processes: text localization and
background reconstruction. Through integrating both processes into a single
network, previous methods provide an implicit erasure guidance by modifying all
pixels in the entire image. However, there exists two problems: 1) the implicit
erasure guidance causes the excessive erasure to non-text areas; 2) the
one-stage erasure lacks the exhaustive removal of text region. In this paper,
we propose a ProgrEssively Region-based scene Text eraser (PERT), introducing
an explicit erasure guidance and performing balanced multi-stage erasure for
accurate and exhaustive text removal. Firstly, we introduce a new region-based
modification strategy (RegionMS) to explicitly guide the erasure process.
Different from previous implicitly guided methods, RegionMS performs targeted
and regional erasure on only text region, and adaptively perceives stroke-level
information to improve the integrity of non-text areas with only bounding box
level annotations. Secondly, PERT performs balanced multi-stage erasure with
several progressive erasing stages. Each erasing stage takes an equal step
toward the text-erased image to ensure the exhaustive erasure of text regions.
Compared with previous methods, PERT outperforms them by a large margin without
the need of adversarial loss, obtaining SOTA results with high speed (71 FPS)
and at least 25% lower parameter complexity. Code is available at
https://github.com/wangyuxin87/PERT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The RSNA-ASNR-MICCAI BraTS 2021 Benchmark on Brain Tumor Segmentation and Radiogenomic Classification. (arXiv:2107.02314v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.02314">
<div class="article-summary-box-inner">
<span><p>The BraTS 2021 challenge celebrates its 10th anniversary and is jointly
organized by the Radiological Society of North America (RSNA), the American
Society of Neuroradiology (ASNR), and the Medical Image Computing and Computer
Assisted Interventions (MICCAI) society. Since its inception, BraTS has been
focusing on being a common benchmarking venue for brain glioma segmentation
algorithms, with well-curated multi-institutional multi-parametric magnetic
resonance imaging (mpMRI) data. Gliomas are the most common primary
malignancies of the central nervous system, with varying degrees of
aggressiveness and prognosis. The RSNA-ASNR-MICCAI BraTS 2021 challenge targets
the evaluation of computational algorithms assessing the same tumor
compartmentalization, as well as the underlying tumor's molecular
characterization, in pre-operative baseline mpMRI data from 2,040 patients.
Specifically, the two tasks that BraTS 2021 focuses on are: a) the segmentation
of the histologically distinct brain tumor sub-regions, and b) the
classification of the tumor's O[6]-methylguanine-DNA methyltransferase (MGMT)
promoter methylation status. The performance evaluation of all participating
algorithms in BraTS 2021 will be conducted through the Sage Bionetworks Synapse
platform (Task 1) and Kaggle (Task 2), concluding in distributing to the top
ranked participants monetary awards of $60,000 collectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Supervised Learning with Multi-Head Co-Training. (arXiv:2107.04795v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.04795">
<div class="article-summary-box-inner">
<span><p>Co-training, extended from self-training, is one of the frameworks for
semi-supervised learning. Without natural split of features, single-view
co-training works at the cost of training extra classifiers, where the
algorithm should be delicately designed to prevent individual classifiers from
collapsing into each other. To remove these obstacles which deter the adoption
of single-view co-training, we present a simple and efficient algorithm
Multi-Head Co-Training. By integrating base learners into a multi-head
structure, the model is in a minimal amount of extra parameters. Every
classification head in the unified model interacts with its peers through a
"Weak and Strong Augmentation" strategy, in which the diversity is naturally
brought by the strong data augmentation. Therefore, the proposed method
facilitates single-view co-training by 1). promoting diversity implicitly and
2). only requiring a small extra computational overhead. The effectiveness of
Multi-Head Co-Training is demonstrated in an empirical study on standard
semi-supervised learning benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Domain Adaptation for Video Semantic Segmentation. (arXiv:2107.11052v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.11052">
<div class="article-summary-box-inner">
<span><p>Unsupervised Domain Adaptation for semantic segmentation has gained immense
popularity since it can transfer knowledge from simulation to real (Sim2Real)
by largely cutting out the laborious per pixel labeling efforts at real. In
this work, we present a new video extension of this task, namely Unsupervised
Domain Adaptation for Video Semantic Segmentation. As it became easy to obtain
large-scale video labels through simulation, we believe attempting to maximize
Sim2Real knowledge transferability is one of the promising directions for
resolving the fundamental data-hungry issue in the video. To tackle this new
problem, we present a novel two-phase adaptation scheme. In the first step, we
exhaustively distill source domain knowledge using supervised loss functions.
Simultaneously, video adversarial training (VAT) is employed to align the
features from source to target utilizing video context. In the second step, we
apply video self-training (VST), focusing only on the target data. To construct
robust pseudo labels, we exploit the temporal information in the video, which
has been rarely explored in the previous image-based self-training approaches.
We set strong baseline scores on 'VIPER to CityscapeVPS' adaptation scenario.
We show that our proposals significantly outperform previous image-based UDA
methods both on image-level (mIoU) and video-level (VPQ) evaluation metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A New Split for Evaluating True Zero-Shot Action Recognition. (arXiv:2107.13029v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.13029">
<div class="article-summary-box-inner">
<span><p>Zero-shot action recognition is the task of classifying action categories
that are not available in the training set. In this setting, the standard
evaluation protocol is to use existing action recognition datasets(e.g. UCF101)
and randomly split the classes into seen and unseen. However, most recent work
builds on representations pre-trained on the Kinetics dataset, where classes
largely overlap with classes in the zero-shot evaluation datasets. As a result,
classes which are supposed to be unseen, are present during supervised
pre-training, invalidating the condition of the zero-shot setting. A similar
concern was previously noted several years ago for image based zero-shot
recognition but has not been considered by the zero-shot action recognition
community. In this paper, we propose a new split for true zero-shot action
recognition with no overlap between unseen test classes and training or
pre-training classes. We benchmark several recent approaches on the proposed
True Zero-Shot(TruZe) Split for UCF101 and HMDB51, with zero-shot and
generalized zero-shot evaluation. In our extensive analysis, we find that our
TruZesplits are significantly harder than comparable random splits as nothing
is leaking from pre-training, i.e. unseen performance is consistently lower,up
to 8.9% for zero-shot action recognition. In an additional evaluation we also
find that similar issues exist in the splits used in few-shot action
recognition, here we see differences of up to 17.1%. We publish oursplits1and
hope that our benchmark analysis will change how the field is evaluating zero-
and few-shot action recognition moving forward.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cycle-Consistent Inverse GAN for Text-to-Image Synthesis. (arXiv:2108.01361v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.01361">
<div class="article-summary-box-inner">
<span><p>This paper investigates an open research task of text-to-image synthesis for
automatically generating or manipulating images from text descriptions.
Prevailing methods mainly use the text as conditions for GAN generation, and
train different models for the text-guided image generation and manipulation
tasks. In this paper, we propose a novel unified framework of Cycle-consistent
Inverse GAN (CI-GAN) for both text-to-image generation and text-guided image
manipulation tasks. Specifically, we first train a GAN model without text
input, aiming to generate images with high diversity and quality. Then we learn
a GAN inversion model to convert the images back to the GAN latent space and
obtain the inverted latent codes for each image, where we introduce the
cycle-consistency training to learn more robust and consistent inverted latent
codes. We further uncover the latent space semantics of the trained GAN model,
by learning a similarity model between text representations and the latent
codes. In the text-guided optimization module, we generate images with the
desired semantic attributes by optimizing the inverted latent codes. Extensive
experiments on the Recipe1M and CUB datasets validate the efficacy of our
proposed framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Alleviating Mode Collapse in GAN via Diversity Penalty Module. (arXiv:2108.02353v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02353">
<div class="article-summary-box-inner">
<span><p>The vanilla GAN (Goodfellow et al. 2014) suffers from mode collapse deeply,
which usually manifests as that the images generated by generators tend to have
a high similarity amongst them, even though their corresponding latent vectors
have been very different. In this paper, we introduce a pluggable diversity
penalty module (DPM) to alleviate mode collapse of GANs. It reduces the
similarity of image pairs in feature space, i.e., if two latent vectors are
different, then we enforce the generator to generate two images with different
features. The normalized Gram matrix is used to measure the similarity. We
compare the proposed method with Unrolled GAN (Metz et al. 2016), BourGAN
(Xiao, Zhong, and Zheng 2018), PacGAN (Lin et al. 2018), VEEGAN (Srivastava et
al. 2017) and ALI (Dumoulin et al. 2016) on 2D synthetic dataset, and results
show that the diversity penalty module can help GAN capture much more modes of
the data distribution. Further, in classification tasks, we apply this method
as image data augmentation on MNIST, Fashion- MNIST and CIFAR-10, and the
classification testing accuracy is improved by 0.24%, 1.34% and 0.52% compared
with WGAN GP (Gulrajani et al. 2017), respectively. In domain translation,
diversity penalty module can help StarGAN (Choi et al. 2018) generate more
accurate attention masks and accelarate the convergence process. Finally, we
quantitatively evaluate the proposed method with IS and FID on CelebA,
CIFAR-10, MNIST and Fashion-MNIST, and the results suggest GAN with diversity
penalty module gets much higher IS and lower FID compared with some SOTA GAN
architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neighborhood Consensus Contrastive Learning for Backward-Compatible Representation. (arXiv:2108.03372v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03372">
<div class="article-summary-box-inner">
<span><p>In object re-identification (ReID), the development of deep learning
techniques often involves model updates and deployment. It is unbearable to
re-embedding and re-index with the system suspended when deploying new models.
Therefore, backward-compatible representation is proposed to enable "new"
features to be compared with "old" features directly, which means that the
database is active when there are both "new" and "old" features in it. Thus we
can scroll-refresh the database or even do nothing on the database to update.
</p>
<p>The existing backward-compatible methods either require a strong overlap
between old and new training data or simply conduct constraints at the instance
level. Thus they are difficult in handling complicated cluster structures and
are limited in eliminating the impact of outliers in old embeddings, resulting
in a risk of damaging the discriminative capability of new features. In this
work, we propose a Neighborhood Consensus Contrastive Learning (NCCL) method.
With no assumptions about the new training data, we estimate the sub-cluster
structures of old embeddings. A new embedding is constrained with multiple old
embeddings in both embedding space and discrimination space at the sub-class
level. The effect of outliers diminished, as the multiple samples serve as
"mean teachers". Besides, we also propose a scheme to filter the old embeddings
with low credibility, further improving the compatibility robustness. Our
method ensures backward compatibility without impairing the accuracy of the new
model. And it can even improve the new model's accuracy in most scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VisEvent: Reliable Object Tracking via Collaboration of Frame and Event Flows. (arXiv:2108.05015v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.05015">
<div class="article-summary-box-inner">
<span><p>Different from visible cameras which record intensity images frame by frame,
the biologically inspired event camera produces a stream of asynchronous and
sparse events with much lower latency. In practice, the visible cameras can
better perceive texture details and slow motion, while event cameras can be
free from motion blurs and have a larger dynamic range which enables them to
work well under fast motion and low illumination. Therefore, the two sensors
can cooperate with each other to achieve more reliable object tracking. In this
work, we propose a large-scale Visible-Event benchmark (termed VisEvent) due to
the lack of a realistic and scaled dataset for this task. Our dataset consists
of 820 video pairs captured under low illumination, high speed, and background
clutter scenarios, and it is divided into a training and a testing subset, each
of which contains 500 and 320 videos, respectively. Based on VisEvent, we
transform the event flows into event images and construct more than 30 baseline
methods by extending current single-modality trackers into dual-modality
versions. More importantly, we further build a simple but effective tracking
algorithm by proposing a cross-modality transformer, to achieve more effective
feature fusion between visible and event data. Extensive experiments on the
proposed VisEvent dataset, FE108, and two simulated datasets (i.e., OTB-DVS and
VOT-DVS), validated the effectiveness of our model. The dataset and source code
have been released at our project page:
\url{https://sites.google.com/view/viseventtrack/}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning From Long-Tailed Data With Noisy Labels. (arXiv:2108.11096v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11096">
<div class="article-summary-box-inner">
<span><p>Class imbalance and noisy labels are the norm rather than the exception in
many large-scale classification datasets. Nevertheless, most works in machine
learning typically assume balanced and clean data. There have been some recent
attempts to tackle, on one side, the problem of learning from noisy labels and,
on the other side, learning from long-tailed data. Each group of methods make
simplifying assumptions about the other. Due to this separation, the proposed
solutions often underperform when both assumptions are violated. In this work,
we present a simple two-stage approach based on recent advances in
self-supervised learning to treat both challenges simultaneously. It consists
of, first, task-agnostic self-supervised pre-training, followed by
task-specific fine-tuning using an appropriate loss. Most significantly, we
find that self-supervised learning approaches are effectively able to cope with
severe class imbalance. In addition, the resulting learned representations are
also remarkably robust to label noise, when fine-tuned with an imbalance- and
noise-resistant loss function. We validate our claims with experiments on
CIFAR-10 and CIFAR-100 augmented with synthetic imbalance and noise, as well as
the large-scale inherently noisy Clothing-1M dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Physical Adversarial Attacks on an Aerial Imagery Object Detector. (arXiv:2108.11765v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11765">
<div class="article-summary-box-inner">
<span><p>Deep neural networks (DNNs) have become essential for processing the vast
amounts of aerial imagery collected using earth-observing satellite platforms.
However, DNNs are vulnerable towards adversarial examples, and it is expected
that this weakness also plagues DNNs for aerial imagery. In this work, we
demonstrate one of the first efforts on physical adversarial attacks on aerial
imagery, whereby adversarial patches were optimised, fabricated and installed
on or near target objects (cars) to significantly reduce the efficacy of an
object detector applied on overhead images. Physical adversarial attacks on
aerial images, particularly those captured from satellite platforms, are
challenged by atmospheric factors (lighting, weather, seasons) and the distance
between the observer and target. To investigate the effects of these
challenges, we devised novel experiments and metrics to evaluate the efficacy
of physical adversarial attacks against object detectors in aerial scenes. Our
results indicate the palpable threat posed by physical adversarial attacks
towards DNNs for processing satellite imagery.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perceptually Optimized Deep High-Dynamic-Range Image Tone Mapping. (arXiv:2109.00180v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00180">
<div class="article-summary-box-inner">
<span><p>We describe a deep high-dynamic-range (HDR) image tone mapping operator that
is computationally efficient and perceptually optimized. We first decompose an
HDR image into a normalized Laplacian pyramid, and use two deep neural networks
(DNNs) to estimate the Laplacian pyramid of the desired tone-mapped image from
the normalized representation. We then end-to-end optimize the entire method
over a database of HDR images by minimizing the normalized Laplacian pyramid
distance (NLPD), a recently proposed perceptual metric. Qualitative and
quantitative experiments demonstrate that our method produces images with
better visual quality, and runs the fastest among existing local tone mapping
algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Study on the Joint Impact of Feature Selection and Data Re-sampling on Imbalance Classification. (arXiv:2109.00201v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00201">
<div class="article-summary-box-inner">
<span><p>In predictive tasks, real-world datasets often present different degrees of
imbalanced (i.e., long-tailed or skewed) distributions. While the majority (the
head) classes have sufficient samples, the minority (the tail) classes can be
under-represented by a rather limited number of samples. Data pre-processing
has been shown to be very effective in dealing with such problems. On one hand,
data re-sampling is a common approach to tackling class imbalance. On the other
hand, dimension reduction, which reduces the feature space, is a conventional
technique for reducing noise and inconsistencies in a dataset. However, the
possible synergy between feature selection and data re-sampling for
high-performance imbalance classification has rarely been investigated before.
To address this issue, we carry out a comprehensive empirical study on the
joint influence of feature selection and re-sampling on two-class imbalance
classification. Specifically, we study the performance of two opposite
pipelines for imbalance classification by applying feature selection before or
after data re-sampling. We conduct a large number of experiments, with a total
of 9225 tests, on 52 publicly available datasets, using 9 feature selection
methods, 6 re-sampling approaches for class imbalance learning, and 3
well-known classification algorithms. Experimental results show that there is
no constant winner between the two pipelines; thus both of them should be
considered to derive the best performing model for imbalance classification. We
find that the performance of an imbalance classification model not only depends
on the classifier adopted and the ratio between the number of majority and
minority samples, but also depends on the ratio between the number of samples
and features. Overall, this study should provide new reference value for
researchers and practitioners in imbalance learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EventPoint: Self-Supervised Local Descriptor Learning for Event Cameras. (arXiv:2109.00210v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00210">
<div class="article-summary-box-inner">
<span><p>We proposes a method of extracting intrest points and descriptors using
self-supervised learning method on frame-based event data, which is called
EventPoint. Different from other feature extraction methods on event data, we
train our model on real event-form driving dataset--DSEC with the
self-supervised learning method we proposed, the training progress fully
consider the characteristics of event data.To verify the effectiveness of our
work,we conducted several complete evaluations: we emulated DART and carried
out feature matching experiments on N-caltech101 dataset, the results shows
that the effect of EventPoint is better than DART; We use Vid2e tool provided
by UZH to convert Oxford robotcar data into event-based format, and combined
with INS information provided to carry out the global pose estimation
experiment which is important in SLAM. As far as we know, this is the first
work to carry out this challenging task.Sufficient experimental data show that
EventPoint can get better results while achieve real time on CPU.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GOHOME: Graph-Oriented Heatmap Output for future Motion Estimation. (arXiv:2109.01827v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01827">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose GOHOME, a method leveraging graph representations
of the High Definition Map and sparse projections to generate a heatmap output
representing the future position probability distribution for a given agent in
a traffic scene. This heatmap output yields an unconstrained 2D grid
representation of agent future possible locations, allowing inherent
multimodality and a measure of the uncertainty of the prediction. Our
graph-oriented model avoids the high computation burden of representing the
surrounding context as squared images and processing it with classical CNNs,
but focuses instead only on the most probable lanes where the agent could end
up in the immediate future. GOHOME reaches 3$rd$ on Argoverse Motion
Forecasting Benchmark on the MissRate$_6$ metric while achieving significant
speed-up and memory burden diminution compared to 1$^{st}$ place method HOME.
We also highlight that heatmap output enables multimodal ensembling and improve
1$^{st}$ place MissRate$_6$ by more than 15$\%$ with our best ensemble.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Fine-Grained Motion Embedding for Landscape Animation. (arXiv:2109.02216v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02216">
<div class="article-summary-box-inner">
<span><p>In this paper we focus on landscape animation, which aims to generate
time-lapse videos from a single landscape image. Motion is crucial for
landscape animation as it determines how objects move in videos. Existing
methods are able to generate appealing videos by learning motion from real
time-lapse videos. However, current methods suffer from inaccurate motion
generation, which leads to unrealistic video results. To tackle this problem,
we propose a model named FGLA to generate high-quality and realistic videos by
learning Fine-Grained motion embedding for Landscape Animation. Our model
consists of two parts: (1) a motion encoder which embeds time-lapse motion in a
fine-grained way. (2) a motion generator which generates realistic motion to
animate input images. To train and evaluate on diverse time-lapse videos, we
build the largest high-resolution Time-lapse video dataset with Diverse scenes,
namely Time-lapse-D, which includes 16,874 video clips with over 10 million
frames. Quantitative and qualitative experimental results demonstrate the
superiority of our method. In particular, our method achieves relative
improvements by 19% on LIPIS and 5.6% on FVD compared with state-of-the-art
methods on our dataset. A user study carried out with 700 human subjects shows
that our approach visually outperforms existing methods by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Voxel Transformer for 3D Object Detection. (arXiv:2109.02497v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02497">
<div class="article-summary-box-inner">
<span><p>We present Voxel Transformer (VoTr), a novel and effective voxel-based
Transformer backbone for 3D object detection from point clouds. Conventional 3D
convolutional backbones in voxel-based 3D detectors cannot efficiently capture
large context information, which is crucial for object recognition and
localization, owing to the limited receptive fields. In this paper, we resolve
the problem by introducing a Transformer-based architecture that enables
long-range relationships between voxels by self-attention. Given the fact that
non-empty voxels are naturally sparse but numerous, directly applying standard
Transformer on voxels is non-trivial. To this end, we propose the sparse voxel
module and the submanifold voxel module, which can operate on the empty and
non-empty voxel positions effectively. To further enlarge the attention range
while maintaining comparable computational overhead to the convolutional
counterparts, we propose two attention mechanisms for multi-head attention in
those two modules: Local Attention and Dilated Attention, and we further
propose Fast Voxel Query to accelerate the querying process in multi-head
attention. VoTr contains a series of sparse and submanifold voxel modules and
can be applied in most voxel-based detectors. Our proposed VoTr shows
consistent improvement over the convolutional baselines while maintaining
computational efficiency on the KITTI dataset and the Waymo Open dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Open Set Detection by Extending CLIP. (arXiv:2109.02748v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02748">
<div class="article-summary-box-inner">
<span><p>In a regular open set detection problem, samples of known classes (also
called closed set classes) are used to train a special classifier. In testing,
the classifier can (1) classify the test samples of known classes to their
respective classes and (2) also detect samples that do not belong to any of the
known classes (we say they belong to some unknown or open set classes). This
paper studies the problem of zero-shot open-set detection, which still performs
the same two tasks in testing but has no training except using the given known
class names. This paper proposes a novel and yet simple method (called ZO-CLIP)
to solve the problem. ZO-CLIP builds on top of the recent advances in zero-shot
classification through multi-modal representation learning. It first extends
the pre-trained multi-modal model CLIP by training a text-based image
description generator on top of CLIP. In testing, it uses the extended model to
generate some candidate unknown class names for each test sample and computes a
confidence score based on both the known class names and candidate unknown
class names for zero-shot open set detection. Experimental results on 5
benchmark datasets for open set detection confirm that ZO-CLIP outperforms the
baselines by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Journalistic Guidelines Aware News Image Captioning. (arXiv:2109.02865v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02865">
<div class="article-summary-box-inner">
<span><p>The task of news article image captioning aims to generate descriptive and
informative captions for news article images. Unlike conventional image
captions that simply describe the content of the image in general terms, news
image captions follow journalistic guidelines and rely heavily on named
entities to describe the image content, often drawing context from the whole
article they are associated with. In this work, we propose a new approach to
this task, motivated by caption guidelines that journalists follow. Our
approach, Journalistic Guidelines Aware News Image Captioning (JoGANIC),
leverages the structure of captions to improve the generation quality and guide
our representation design. Experimental results, including detailed ablation
studies, on two large-scale publicly available datasets show that JoGANIC
substantially outperforms state-of-the-art methods both on caption generation
and named entity related metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perceptual Learned Video Compression with Recurrent Conditional GAN. (arXiv:2109.03082v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03082">
<div class="article-summary-box-inner">
<span><p>This paper proposes a Perceptual Learned Video Compression (PLVC) approach
with recurrent conditional generative adversarial network. In our approach, the
recurrent auto-encoder-based generator learns to fully explore the temporal
correlation for compressing video. More importantly, we propose a recurrent
conditional discriminator, which judges raw and compressed video conditioned on
both spatial and temporal information, including the latent representation,
temporal motion and hidden states in recurrent cells. This way, in the
adversarial training, it pushes the generated video to be not only spatially
photo-realistic but also temporally consistent with groundtruth and coherent
among video frames. The experimental results show that the proposed PLVC model
learns to compress video towards good perceptual quality at low bit-rate, and
outperforms the previous traditional and learned approaches on several
perceptual quality metrics. The user study further validates the outstanding
perceptual performance of PLVC in comparison with the latest learned video
compression approaches and the official HEVC test model (HM 16.20). The codes
will be released at https://github.com/RenYang-home/PLVC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Temporal RoI Align for Video Object Recognition. (arXiv:2109.03495v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03495">
<div class="article-summary-box-inner">
<span><p>Video object detection is challenging in the presence of appearance
deterioration in certain video frames. Therefore, it is a natural choice to
aggregate temporal information from other frames of the same video into the
current frame. However, RoI Align, as one of the most core procedures of video
detectors, still remains extracting features from a single-frame feature map
for proposals, making the extracted RoI features lack temporal information from
videos. In this work, considering the features of the same object instance are
highly similar among frames in a video, a novel Temporal RoI Align operator is
proposed to extract features from other frames feature maps for current frame
proposals by utilizing feature similarity. The proposed Temporal RoI Align
operator can extract temporal information from the entire video for proposals.
We integrate it into single-frame video detectors and other state-of-the-art
video detectors, and conduct quantitative experiments to demonstrate that the
proposed Temporal RoI Align operator can consistently and significantly boost
the performance. Besides, the proposed Temporal RoI Align can also be applied
into video instance segmentation. Codes are available at
https://github.com/open-mmlab/mmtracking
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Recognizing Occluded Faces in the Wild. (arXiv:2109.03672v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03672">
<div class="article-summary-box-inner">
<span><p>Facial appearance variations due to occlusion has been one of the main
challenges for face recognition systems. To facilitate further research in this
area, it is necessary and important to have occluded face datasets collected
from real-world, as synthetically generated occluded faces cannot represent the
nature of the problem. In this paper, we present the Real World Occluded Faces
(ROF) dataset, that contains faces with both upper face occlusion, due to
sunglasses, and lower face occlusion, due to masks. We propose two evaluation
protocols for this dataset. Benchmark experiments on the dataset have shown
that no matter how powerful the deep face representation models are, their
performance degrades significantly when they are tested on real-world occluded
faces. It is observed that the performance drop is far less when the models are
tested on synthetically generated occluded faces. The ROF dataset and the
associated evaluation protocols are publicly available at the following link
https://github.com/ekremerakin/RealWorldOccludedFaces.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Panoptic SegFormer. (arXiv:2109.03814v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03814">
<div class="article-summary-box-inner">
<span><p>We present Panoptic SegFormer, a general framework for end-to-end panoptic
segmentation with Transformers. The proposed method extends Deformable DETR
with a unified mask prediction workflow for both things and stuff, making the
panoptic segmentation pipeline concise and effective. With a ResNet-50
backbone, our method achieves 50.0\% PQ on the COCO test-dev split, surpassing
previous state-of-the-art methods by significant margins without bells and
whistles. Using a more powerful PVTv2-B5 backbone, Panoptic-SegFormer achieves
a new record of 54.1\%PQ and 54.4\% PQ on the COCO val and test-dev splits with
single scale input.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Video-Text Retrieval by Multi-Stream Corpus Alignment and Dual Softmax Loss. (arXiv:2109.04290v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04290">
<div class="article-summary-box-inner">
<span><p>Employing large-scale pre-trained model CLIP to conduct video-text retrieval
task (VTR) has become a new trend, which exceeds previous VTR methods. Though,
due to the heterogeneity of structures and contents between video and text,
previous CLIP-based models are prone to overfitting in the training phase,
resulting in relatively poor retrieval performance. In this paper, we propose a
multi-stream Corpus Alignment network with single gate Mixture-of-Experts
(CAMoE) and a novel Dual Softmax Loss (DSL) to solve the two heterogeneity. The
CAMoE employs Mixture-of-Experts (MoE) to extract multi-perspective video
representations, including action, entity, scene, etc., then align them with
the corresponding part of the text. In this stage, we conduct massive
explorations towards the feature extraction module and feature alignment
module. DSL is proposed to avoid the one-way optimum-match which occurs in
previous contrastive methods. Introducing the intrinsic prior of each pair in a
batch, DSL serves as a reviser to correct the similarity matrix and achieves
the dual optimal match. DSL is easy to implement with only one-line code but
improves significantly. The results show that the proposed CAMoE and DSL are of
strong efficiency, and each of them is capable of achieving State-of-The-Art
(SOTA) individually on various benchmarks such as MSR-VTT, MSVD, and LSMDC.
Further, with both of them, the performance is advanced to a big extend,
surpassing the previous SOTA methods for around 4.6\% R@1 in MSR-VTT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Portrait Video Matting via Context Motion Network. (arXiv:2109.04598v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04598">
<div class="article-summary-box-inner">
<span><p>Automatic portrait video matting is an under-constrained problem. Most
state-of-the-art methods only exploit the semantic information and process each
frame individually. Their performance is compromised due to the lack of
temporal information between the frames. To solve this problem, we propose the
context motion network to leverage semantic information and motion information.
To capture the motion information, we estimate the optical flow and design a
context-motion updating operator to integrate features between frames
recurrently. Our experiments show that our network outperforms state-of-the-art
matting methods significantly on the Video240K SD dataset.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-09-14 23:02:12.402667889 UTC">2021-09-14 23:02:12 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.3</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>