<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-12-20T01:30:00Z">12-20</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Investigation of the Role of Pre-training in Lifelong Learning. (arXiv:2112.09153v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09153">
<div class="article-summary-box-inner">
<span><p>The lifelong learning paradigm in machine learning is an attractive
alternative to the more prominent isolated learning scheme not only due to its
resemblance to biological learning, but also its potential to reduce energy
waste by obviating excessive model re-training. A key challenge to this
paradigm is the phenomenon of catastrophic forgetting. With the increasing
popularity and success of pre-trained models in machine learning, we pose the
question: What role does pre-training play in lifelong learning, specifically
with respect to catastrophic forgetting? We investigate existing methods in the
context of large, pre-trained models and evaluate their performance on a
variety of text and image classification tasks, including a large-scale study
using a novel dataset of 15 diverse NLP tasks. Across all settings, we observe
that generic pre-training implicitly alleviates the effects of catastrophic
forgetting when learning multiple tasks sequentially compared to randomly
initialized models. We then further investigate why pre-training alleviates
forgetting in this setting. We study this phenomenon by analyzing the loss
landscape, finding that pre-trained weights appear to ease forgetting by
leading to wider minima. Based on this insight, we propose jointly optimizing
for current task loss and loss basin sharpness in order to explicitly encourage
wider basins during sequential fine-tuning. We show that this optimization
approach leads to performance comparable to the state-of-the-art in
task-sequential continual learning across multiple settings, without retaining
a memory that scales in size with the number of tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Bounded Context-Free-Grammar via LSTM and the Transformer:Difference and Explanations. (arXiv:2112.09174v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09174">
<div class="article-summary-box-inner">
<span><p>Long Short-Term Memory (LSTM) and Transformers are two popular neural
architectures used for natural language processing tasks. Theoretical results
show that both are Turing-complete and can represent any context-free language
(CFL).In practice, it is often observed that Transformer models have better
representation power than LSTM. But the reason is barely understood. We study
such practical differences between LSTM and Transformer and propose an
explanation based on their latent space decomposition patterns. To achieve this
goal, we introduce an oracle training paradigm, which forces the decomposition
of the latent representation of LSTM and the Transformer and supervises with
the transitions of the Pushdown Automaton (PDA) of the corresponding CFL. With
the forced decomposition, we show that the performance upper bounds of LSTM and
Transformer in learning CFL are close: both of them can simulate a stack and
perform stack operation along with state transitions. However, the absence of
forced decomposition leads to the failure of LSTM models to capture the stack
and stack operations, while having a marginal impact on the Transformer model.
Lastly, we connect the experiment on the prototypical PDA to a real-world
parsing task to re-verify the conclusions
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hyperbolic Disentangled Representation for Fine-Grained Aspect Extraction. (arXiv:2112.09215v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09215">
<div class="article-summary-box-inner">
<span><p>Automatic identification of salient aspects from user reviews is especially
useful for opinion analysis. There has been significant progress in utilizing
weakly supervised approaches, which require only a small set of seed words for
training aspect classifiers. However, there is always room for improvement.
First, no weakly supervised approaches fully utilize latent hierarchies between
words. Second, each seed words representation should have different latent
semantics and be distinct when it represents a different aspect. In this paper,
we propose HDAE, a hyperbolic disentangled aspect extractor in which a
hyperbolic aspect classifier captures words latent hierarchies, and
aspect-disentangled representation models the distinct latent semantics of each
seed word. Compared to previous baselines, HDAE achieves average F1 performance
gains of 18.2% and 24.1% on Amazon product review and restaurant review
datasets, respectively. In addition, the em-bedding visualization experience
demonstrates that HDAE is a more effective approach to leveraging seed words.
An ablation study and a case study further attest to the effectiveness of the
proposed components
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Two-view Graph Neural Networks for Knowledge Graph Completion. (arXiv:2112.09231v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09231">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce a novel GNN-based knowledge graph embedding
model, named WGE, to capture entity-focused graph structure and
relation-focused graph structure. In particular, given the knowledge graph, WGE
builds a single undirected entity-focused graph that views entities as nodes.
In addition, WGE also constructs another single undirected graph from
relation-focused constraints, which views entities and relations as nodes. WGE
then proposes a new architecture of utilizing two vanilla GNNs directly on
these two single graphs to better update vector representations of entities and
relations, followed by a weighted score function to return the triple scores.
Experimental results show that WGE obtains state-of-the-art performances on
three new and challenging benchmark datasets CoDEx for knowledge graph
completion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatically Identifying Semantic Bias in Crowdsourced Natural Language Inference Datasets. (arXiv:2112.09237v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09237">
<div class="article-summary-box-inner">
<span><p>Natural language inference (NLI) is an important task for producing useful
models of human language. Unfortunately large-scale NLI dataset production
relies on crowdworkers who are prone to introduce biases in the sentences they
write. In particular, without quality control they produce hypotheses from
which the relational label can be predicted, without the premise, better than
chance. We introduce a model-driven, unsupervised technique to find "bias
clusters" in a learned embedding space of the hypotheses in NLI datasets, from
which interventions and additional rounds of labeling can be performed to
ameliorate the semantic bias of the hypothesis distribution of a dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Logically at the Factify 2022: Multimodal Fact Verification. (arXiv:2112.09253v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09253">
<div class="article-summary-box-inner">
<span><p>This paper describes our participant system for the multi-modal fact
verification (Factify) challenge at AAAI 2022. Despite the recent advance in
text based verification techniques and large pre-trained multimodal models
cross vision and language, very limited work has been done in applying
multimodal techniques to automate fact checking process, particularly
considering the increasing prevalence of claims and fake news about images and
videos on social media. In our work, the challenge is treated as multimodal
entailment task and framed as multi-class classification. Two baseline
approaches are proposed and explored including an ensemble model (combining two
uni-modal models) and a multi-modal attention network (modeling the interaction
between image and text pair from claim and evidence document). We conduct
several experiments investigating and benchmarking different SoTA pre-trained
transformers and vision models in this work. Our best model is ranked first in
leaderboard which obtains a weighted average F-measure of 0.77 on both
validation and test set. Exploratory analysis of dataset is also carried out on
the Factify data set and uncovers salient patterns and issues (e.g., word
overlapping, visual entailment correlation, source bias) that motivates our
hypothesis. Finally, we highlight challenges of the task and multimodal dataset
for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Link-Intensive Alignment for Incomplete Knowledge Graphs. (arXiv:2112.09266v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09266">
<div class="article-summary-box-inner">
<span><p>Knowledge graph (KG) alignment - the task of recognizing entities referring
to the same thing in different KGs - is recognized as one of the most important
operations in the field of KG construction and completion. However, existing
alignment techniques often assume that the input KGs are complete and
isomorphic, which is not true due to the real-world heterogeneity in the
domain, size, and sparsity. In this work, we address the problem of aligning
incomplete KGs with representation learning. Our KG embedding framework
exploits two feature channels: transitivity-based and proximity-based. The
former captures the consistency constraints between entities via translation
paths, while the latter captures the neighbourhood structure of KGs via
attention guided relation-aware graph neural network. The two feature channels
are jointly learned to exchange important features between the input KGs while
enforcing the output representations of the input KGs in the same embedding
space. Also, we develop a missing links detector that discovers and recovers
the missing links in the input KGs during the training process, which helps
mitigate the incompleteness issue and thus improve the compatibility of the
learned representations. The embeddings then are fused to generate the
alignment result, and the high-confidence matched node pairs are updated to the
pre-aligned supervision data to improve the embeddings gradually. Empirical
results show that our model is up to 15.2\% more accurate than the SOTA and is
robust against different levels of incompleteness. We also demonstrate that the
knowledge exchanging between the KGs helps reveal the unseen facts from
knowledge graphs (a.k.a. knowledge completion), with the result being 3.5\%
higher than the SOTA knowledge graph completion techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Architectures for Biological Inter-Sentence Relation Extraction. (arXiv:2112.09288v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09288">
<div class="article-summary-box-inner">
<span><p>We introduce a family of deep-learning architectures for inter-sentence
relation extraction, i.e., relations where the participants are not necessarily
in the same sentence. We apply these architectures to an important use case in
the biomedical domain: assigning biological context to biochemical events. In
this work, biological context is defined as the type of biological system
within which the biochemical event is observed. The neural architectures encode
and aggregate multiple occurrences of the same candidate context mentions to
determine whether it is the correct context for a particular event mention. We
propose two broad types of architectures: the first type aggregates multiple
instances that correspond to the same candidate context with respect to event
mention before emitting a classification; the second type independently
classifies each instance and uses the results to vote for the final class, akin
to an ensemble approach. Our experiments show that the proposed neural
classifiers are competitive and some achieve better performance than previous
state of the art traditional machine learning methods without the need for
feature engineering. Our analysis shows that the neural methods particularly
improve precision compared to traditional machine learning classifiers and also
demonstrates how the difficulty of inter-sentence relation extraction increases
as the distance between the event and context mentions increase.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Overview of the HASOC Subtrack at FIRE 2021: Hate Speech and Offensive Content Identification in English and Indo-Aryan Languages. (arXiv:2112.09301v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09301">
<div class="article-summary-box-inner">
<span><p>The widespread of offensive content online such as hate speech poses a
growing societal problem. AI tools are necessary for supporting the moderation
process at online platforms. For the evaluation of these identification tools,
continuous experimentation with data sets in different languages are necessary.
The HASOC track (Hate Speech and Offensive Content Identification) is dedicated
to develop benchmark data for this purpose. This paper presents the HASOC
subtrack for English, Hindi, and Marathi. The data set was assembled from
Twitter. This subtrack has two sub-tasks. Task A is a binary classification
problem (Hate and Not Offensive) offered for all three languages. Task B is a
fine-grained classification problem for three classes (HATE) Hate speech,
OFFENSIVE and PROFANITY offered for English and Hindi. Overall, 652 runs were
submitted by 65 teams. The performance of the best classification algorithms
for task A are F1 measures 0.91, 0.78 and 0.83 for Marathi, Hindi and English,
respectively. This overview presents the tasks and the data development as well
as the detailed results. The systems submitted to the competition applied a
variety of technologies. The best performing algorithms were mainly variants of
transformer architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WebGPT: Browser-assisted question-answering with human feedback. (arXiv:2112.09332v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09332">
<div class="article-summary-box-inner">
<span><p>We fine-tune GPT-3 to answer long-form questions using a text-based
web-browsing environment, which allows the model to search and navigate the
web. By setting up the task so that it can be performed by humans, we are able
to train models on the task using imitation learning, and then optimize answer
quality with human feedback. To make human evaluation of factual accuracy
easier, models must collect references while browsing in support of their
answers. We train and evaluate our models on ELI5, a dataset of questions asked
by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior
cloning, and then performing rejection sampling against a reward model trained
to predict human preferences. This model's answers are preferred by humans 56%
of the time to those of our human demonstrators, and 69% of the time to the
highest-voted answer from Reddit.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KGBoost: A Classification-based Knowledge Base Completion Method with Negative Sampling. (arXiv:2112.09340v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09340">
<div class="article-summary-box-inner">
<span><p>Knowledge base completion is formulated as a binary classification problem in
this work, where an XGBoost binary classifier is trained for each relation
using relevant links in knowledge graphs (KGs). The new method, named KGBoost,
adopts a modularized design and attempts to find hard negative samples so as to
train a powerful classifier for missing link prediction. We conduct experiments
on multiple benchmark datasets, and demonstrate that KGBoost outperforms
state-of-the-art methods across most datasets. Furthermore, as compared with
models trained by end-to-end optimization, KGBoost works well under the
low-dimensional setting so as to allow a smaller model size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Expedition: A System for the Unsupervised Learning of a Hierarchy of Concepts. (arXiv:2112.09348v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09348">
<div class="article-summary-box-inner">
<span><p>We present a system for bottom-up cumulative learning of myriad concepts
corresponding to meaningful character strings, and their part-related and
prediction edges. The learning is self-supervised in that the concepts
discovered are used as predictors as well as targets of prediction. We devise
an objective for segmenting with the learned concepts, derived from comparing
to a baseline prediction system, that promotes making and using larger
concepts, which in turn allows for predicting larger spans of text, and we
describe a simple technique to promote exploration, i.e. trying out newly
generated concepts in the segmentation process. We motivate and explain a
layering of the concepts, to help separate the (conditional) distributions
learnt among concepts. The layering of the concepts roughly corresponds to a
part-whole concept hierarchy. With rudimentary segmentation and learning
algorithms, the system is promising in that it acquires many concepts (tens of
thousands in our small-scale experiments), and it learns to segment text well:
when fed with English text with spaces removed, starting at the character
level, much of what is learned respects word or phrase boundaries, and over
time the average number of "bad" splits within segmentations, i.e. splits
inside words, decreases as larger concepts are discovered and the system learns
when to use them during segmentation. We report on promising experiments when
the input text is converted to binary and the system begins with only two
concepts, "0" and "1". The system is transparent, in the sense that it is easy
to tell what the concepts learned correspond to, and which ones are active in a
segmentation, or how the system "sees" its input. We expect this framework to
be extensible and we discuss the current limitations and a number of directions
for enhancing the learning and inference capabilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual Learning for Monolingual End-to-End Automatic Speech Recognition. (arXiv:2112.09427v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09427">
<div class="article-summary-box-inner">
<span><p>Adapting Automatic Speech Recognition (ASR) models to new domains leads to a
deterioration of performance on the original domain(s), a phenomenon called
Catastrophic Forgetting (CF). Even monolingual ASR models cannot be extended to
new accents, dialects, topics, etc. without suffering from CF, making them
unable to be continually enhanced without storing all past data. Fortunately,
Continual Learning (CL) methods, which aim to enable continual adaptation while
overcoming CF, can be used. In this paper, we implement an extensive number of
CL methods for End-to-End ASR and test and compare their ability to extend a
monolingual Hybrid CTC-Transformer model across four new tasks. We find that
the best performing CL method closes the gap between the fine-tuned model
(lower bound) and the model trained jointly on all tasks (upper bound) by more
than 40%, while requiring access to only 0.6% of the original data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multimodal Approach for Automatic Mania Assessment in Bipolar Disorder. (arXiv:2112.09467v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09467">
<div class="article-summary-box-inner">
<span><p>Bipolar disorder is a mental health disorder that causes mood swings that
range from depression to mania. Diagnosis of bipolar disorder is usually done
based on patient interviews, and reports obtained from the caregivers of the
patients. Subsequently, the diagnosis depends on the experience of the expert,
and it is possible to have confusions of the disorder with other mental
disorders. Automated processes in the diagnosis of bipolar disorder can help
providing quantitative indicators, and allow easier observations of the
patients for longer periods. Furthermore, the need for remote treatment and
diagnosis became especially important during the COVID-19 pandemic. In this
thesis, we create a multimodal decision system based on recordings of the
patient in acoustic, linguistic, and visual modalities. The system is trained
on the Bipolar Disorder corpus. Comprehensive analysis of unimodal and
multimodal systems, as well as various fusion techniques are performed. Besides
processing entire patient sessions using unimodal features, a task-level
investigation of the clips is studied. Using acoustic, linguistic, and visual
features in a multimodal fusion system, we achieved a 64.8% unweighted average
recall score, which improves the state-of-the-art performance achieved on this
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Chinese Word Segmentation and Part-of-speech Tagging via Two-stage Span Labeling. (arXiv:2112.09488v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09488">
<div class="article-summary-box-inner">
<span><p>Chinese word segmentation and part-of-speech tagging are necessary tasks in
terms of computational linguistics and application of natural language
processing. Many re-searchers still debate the demand for Chinese word
segmentation and part-of-speech tagging in the deep learning era. Nevertheless,
resolving ambiguities and detecting unknown words are challenging problems in
this field. Previous studies on joint Chinese word segmentation and
part-of-speech tagging mainly follow the character-based tagging model focusing
on modeling n-gram features. Unlike previous works, we propose a neural model
named SpanSegTag for joint Chinese word segmentation and part-of-speech tagging
following the span labeling in which the probability of each n-gram being the
word and the part-of-speech tag is the main problem. We use the biaffine
operation over the left and right boundary representations of consecutive
characters to model the n-grams. Our experiments show that our BERT-based model
SpanSegTag achieved competitive performances on the CTB5, CTB6, and UD, or
significant improvements on CTB7 and CTB9 benchmark datasets compared with the
current state-of-the-art method using BERT or ZEN encoders.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Challenge Dataset of Cognates and False Friend Pairs from Indian Languages. (arXiv:2112.09526v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09526">
<div class="article-summary-box-inner">
<span><p>Cognates are present in multiple variants of the same text across different
languages (e.g., "hund" in German and "hound" in English language mean "dog").
They pose a challenge to various Natural Language Processing (NLP) applications
such as Machine Translation, Cross-lingual Sense Disambiguation, Computational
Phylogenetics, and Information Retrieval. A possible solution to address this
challenge is to identify cognates across language pairs. In this paper, we
describe the creation of two cognate datasets for twelve Indian languages,
namely Sanskrit, Hindi, Assamese, Oriya, Kannada, Gujarati, Tamil, Telugu,
Punjabi, Bengali, Marathi, and Malayalam. We digitize the cognate data from an
Indian language cognate dictionary and utilize linked Indian language Wordnets
to generate cognate sets. Additionally, we use the Wordnet data to create a
False Friends' dataset for eleven language pairs. We also evaluate the efficacy
of our dataset using previously available baseline cognate detection
approaches. We also perform a manual evaluation with the help of lexicographers
and release the curated gold-standard dataset with this paper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topic-Aware Encoding for Extractive Summarization. (arXiv:2112.09572v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09572">
<div class="article-summary-box-inner">
<span><p>Document summarization provides an instrument for faster understanding the
collection of text documents and has several real-life applications. With the
growth of online text data, numerous summarization models have been proposed
recently. The Sequence-to-Sequence (Seq2Seq) based neural summarization model
is the most widely used in the summarization field due to its high performance.
This is because semantic information and structure information in the text is
adequately considered when encoding. However, the existing extractive
summarization models pay little attention to and use the central topic
information to assist the generation of summaries, which leads to models not
ensuring the generated summary under the primary topic. A lengthy document can
span several topics, and a single summary cannot do justice to all the topics.
Therefore, the key to generating a high-quality summary is determining the
central topic and building a summary based on it, especially for a long
document. We propose a topic-aware encoding for document summarization to deal
with this issue. This model effectively combines syntactic-level and
topic-level information to build a comprehensive sentence representation.
Specifically, a neural topic model is added in the neural-based sentence-level
representation learning to adequately consider the central topic information
for capturing the critical content in the original document. The experimental
results on three public datasets show that our model outperforms the
state-of-the-art models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transcribing Natural Languages for The Deaf via Neural Editing Programs. (arXiv:2112.09600v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09600">
<div class="article-summary-box-inner">
<span><p>This work studies the task of glossification, of which the aim is to em
transcribe natural spoken language sentences for the Deaf (hard-of-hearing)
community to ordered sign language glosses. Previous sequence-to-sequence
language models trained with paired sentence-gloss data often fail to capture
the rich connections between the two distinct languages, leading to
unsatisfactory transcriptions. We observe that despite different grammars,
glosses effectively simplify sentences for the ease of deaf communication,
while sharing a large portion of vocabulary with sentences. This has motivated
us to implement glossification by executing a collection of editing actions,
e.g. word addition, deletion, and copying, called editing programs, on their
natural spoken language counterparts. Specifically, we design a new neural
agent that learns to synthesize and execute editing programs, conditioned on
sentence contexts and partial editing results. The agent is trained to imitate
minimal editing programs, while exploring more widely the program space via
policy gradients to optimize sequence-wise transcription quality. Results show
that our approach outperforms previous glossification models by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparsifying Sparse Representations for Passage Retrieval by Top-$k$ Masking. (arXiv:2112.09628v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09628">
<div class="article-summary-box-inner">
<span><p>Sparse lexical representation learning has demonstrated much progress in
improving passage retrieval effectiveness in recent models such as DeepImpact,
uniCOIL, and SPLADE. This paper describes a straightforward yet effective
approach for sparsifying lexical representations for passage retrieval,
building on SPLADE by introducing a top-$k$ masking scheme to control sparsity
and a self-learning method to coax masked representations to mimic unmasked
representations. A basic implementation of our model is competitive with more
sophisticated approaches and achieves a good balance between effectiveness and
efficiency. The simplicity of our methods opens the door for future
explorations in lexical representation learning for passage retrieval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sublinear Time Approximation of Text Similarity Matrices. (arXiv:2112.09631v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09631">
<div class="article-summary-box-inner">
<span><p>We study algorithms for approximating pairwise similarity matrices that arise
in natural language processing. Generally, computing a similarity matrix for
$n$ data points requires $\Omega(n^2)$ similarity computations. This quadratic
scaling is a significant bottleneck, especially when similarities are computed
via expensive functions, e.g., via transformer models. Approximation methods
reduce this quadratic complexity, often by using a small subset of exactly
computed similarities to approximate the remainder of the complete pairwise
similarity matrix.
</p>
<p>Significant work focuses on the efficient approximation of positive
semidefinite (PSD) similarity matrices, which arise e.g., in kernel methods.
However, much less is understood about indefinite (non-PSD) similarity
matrices, which often arise in NLP. Motivated by the observation that many of
these matrices are still somewhat close to PSD, we introduce a generalization
of the popular Nystr\"{o}m method to the indefinite setting. Our algorithm can
be applied to any similarity matrix and runs in sublinear time in the size of
the matrix, producing a rank-$s$ approximation with just $O(ns)$ similarity
computations.
</p>
<p>We show that our method, along with a simple variant of CUR decomposition,
performs very well in approximating a variety of similarity matrices arising in
NLP tasks. We demonstrate high accuracy of the approximated similarity matrices
in the downstream tasks of document classification, sentence similarity, and
cross-document coreference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reasoning Chain Based Adversarial Attack for Multi-hop Question Answering. (arXiv:2112.09658v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09658">
<div class="article-summary-box-inner">
<span><p>Recent years have witnessed impressive advances in challenging multi-hop QA
tasks. However, these QA models may fail when faced with some disturbance in
the input text and their interpretability for conducting multi-hop reasoning
remains uncertain. Previous adversarial attack works usually edit the whole
question sentence, which has limited effect on testing the entity-based
multi-hop inference ability. In this paper, we propose a multi-hop reasoning
chain based adversarial attack method. We formulate the multi-hop reasoning
chains starting from the query entity to the answer entity in the constructed
graph, which allows us to align the question to each reasoning hop and thus
attack any hop. We categorize the questions into different reasoning types and
adversarially modify part of the question corresponding to the selected
reasoning hop to generate the distracting sentence. We test our adversarial
scheme on three QA models on HotpotQA dataset. The results demonstrate
significant performance reduction on both answer and supporting facts
prediction, verifying the effectiveness of our reasoning chain based attack
method for multi-hop reasoning models and the vulnerability of them. Our
adversarial re-training further improves the performance and robustness of
these models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explain, Edit, and Understand: Rethinking User Study Design for Evaluating Model Explanations. (arXiv:2112.09669v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09669">
<div class="article-summary-box-inner">
<span><p>In attempts to "explain" predictions of machine learning models, researchers
have proposed hundreds of techniques for attributing predictions to features
that are deemed important. While these attributions are often claimed to hold
the potential to improve human "understanding" of the models, surprisingly
little work explicitly evaluates progress towards this aspiration. In this
paper, we conduct a crowdsourcing study, where participants interact with
deception detection models that have been trained to distinguish between
genuine and fake hotel reviews. They are challenged both to simulate the model
on fresh reviews, and to edit reviews with the goal of lowering the probability
of the originally predicted class. Successful manipulations would lead to an
adversarial example. During the training (but not the test) phase, input spans
are highlighted to communicate salience. Through our evaluation, we observe
that for a linear bag-of-words model, participants with access to the feature
coefficients during training are able to cause a larger reduction in model
confidence in the testing phase when compared to the no-explanation control.
For the BERT-based classifier, popular local explanations do not improve their
ability to reduce the model confidence over the no-explanation case.
Remarkably, when the explanation for the BERT model is given by the (global)
attributions of a linear model trained to imitate the BERT model, people can
effectively manipulate the model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-Suicidal Self-Injury Online Posts: Implications for Mental Health Professionals. (arXiv:1902.06689v2 [cs.CY] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1902.06689">
<div class="article-summary-box-inner">
<span><p>While non-suicidal self-injury (NSSI) is not a new phenomenon, there is still
a limited yet little is still known about understanding of the behavior, the
intent behind the behavior and what the individuals themselves say about their
behavior. This study collected pro-NSSI public blog posts from Reddit on
pro-NSSI and analyzed the content linguistically using LIWC software, in order
to examine the use of NSSI specific words, linguistic properties and the
psychological linguistic properties. were examined. The results inform current
counseling practices by dispelling myths and providing insight into the inner
world of people who engage in use NSSII to cope. The most frequently appearing
category of For NSSI specific words categories, in the Reddit blogs was the
reasons in which one engagesfor engaging in NSSI was the most frequently used
in the Reddit blogs. The linguistic properties found in the analysis reflected
the predicted results; authors of pro-NSSI posts used demonstrated expected
results of first-person singular pronouns extensively, which indicatesing high
levels of mental health distress and isolation. The psychological linguistic
properties that could be observed of in these public Reddit posts were
dominantly in a negative emotional tone which demonstrates youth and
impulsivity. The linguistic properties found when these posts were analyzed
supports the work of earlier studies that dispelled common myths about NSSI
that were circulating in the mental health community. These findings suggest
that the language of people who engage in NSSI supports research findings in
dispelling common myths about NSSI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantum Language Model with Entanglement Embedding for Question Answering. (arXiv:2008.09943v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.09943">
<div class="article-summary-box-inner">
<span><p>Quantum Language Models (QLMs) in which words are modelled as quantum
superposition of sememes have demonstrated a high level of model transparency
and good post-hoc interpretability. Nevertheless, in the current literature
word sequences are basically modelled as a classical mixture of word states,
which cannot fully exploit the potential of a quantum probabilistic
description. A full quantum model is yet to be developed to explicitly capture
the non-classical correlations within the word sequences. We propose a neural
network model with a novel Entanglement Embedding (EE) module, whose function
is to transform the word sequences into entangled pure states of many-body
quantum systems. Strong quantum entanglement, which is the central concept of
quantum information and an indication of parallelized correlations among the
words, is observed within the word sequences. Numerical experiments show that
the proposed QLM with EE (QLM-EE) achieves superior performance compared with
the classical deep neural network models and other QLMs on Question Answering
(QA) datasets. In addition, the post-hoc interpretability of the model can be
improved by quantizing the degree of entanglement among the words.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">E-BERT: A Phrase and Product Knowledge Enhanced Language Model for E-commerce. (arXiv:2009.02835v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.02835">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models such as BERT have achieved great success in a
broad range of natural language processing tasks. However, BERT cannot well
support E-commerce related tasks due to the lack of two levels of domain
knowledge, i.e., phrase-level and product-level. On one hand, many E-commerce
tasks require an accurate understanding of domain phrases, whereas such
fine-grained phrase-level knowledge is not explicitly modeled by BERT's
training objective. On the other hand, product-level knowledge like product
associations can enhance the language modeling of E-commerce, but they are not
factual knowledge thus using them indiscriminately may introduce noise. To
tackle the problem, we propose a unified pre-training framework, namely,
E-BERT. Specifically, to preserve phrase-level knowledge, we introduce Adaptive
Hybrid Masking, which allows the model to adaptively switch from learning
preliminary word knowledge to learning complex phrases, based on the fitting
progress of two modes. To utilize product-level knowledge, we introduce
Neighbor Product Reconstruction, which trains E-BERT to predict a product's
associated neighbors with a denoising cross attention layer. Our investigation
reveals promising results in four downstream tasks, i.e., review-based question
answering, aspect extraction, aspect sentiment classification, and product
classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HyperText: Endowing FastText with Hyperbolic Geometry. (arXiv:2010.16143v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.16143">
<div class="article-summary-box-inner">
<span><p>Natural language data exhibit tree-like hierarchical structures such as the
hypernym-hyponym relations in WordNet. FastText, as the state-of-the-art text
classifier based on shallow neural network in Euclidean space, may not model
such hierarchies precisely with limited representation capacity. Considering
that hyperbolic space is naturally suitable for modeling tree-like hierarchical
data, we propose a new model named HyperText for efficient text classification
by endowing FastText with hyperbolic geometry. Empirically, we show that
HyperText outperforms FastText on a range of text classification tasks with
much reduced parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning for Text Style Transfer: A Survey. (arXiv:2011.00416v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.00416">
<div class="article-summary-box-inner">
<span><p>Text style transfer is an important task in natural language generation,
which aims to control certain attributes in the generated text, such as
politeness, emotion, humor, and many others. It has a long history in the field
of natural language processing, and recently has re-gained significant
attention thanks to the promising performance brought by deep neural models. In
this paper, we present a systematic survey of the research on neural text style
transfer, spanning over 100 representative articles since the first neural text
style transfer work in 2017. We discuss the task formulation, existing datasets
and subtasks, evaluation, as well as the rich methodologies in the presence of
parallel and non-parallel data. We also provide discussions on a variety of
important topics regarding the future development of this task. Our curated
paper list is at https://github.com/zhijing-jin/Text_Style_Transfer_Survey
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Explanations: How much do explanations from the teacher aid students?. (arXiv:2012.00893v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.00893">
<div class="article-summary-box-inner">
<span><p>While many methods purport to explain predictions by highlighting salient
features, what aims these explanations serve and how they ought to be evaluated
often go unstated. In this work, we introduce a framework to quantify the value
of explanations via the accuracy gains that they confer on a student model
trained to simulate a teacher model. Crucially, the explanations are available
to the student during training, but are not available at test time. Compared to
prior proposals, our approach is less easily gamed, enabling principled,
automatic, model-agnostic evaluation of attributions. Using our framework, we
compare numerous attribution methods for text classification and question
answering, and observe quantitative differences that are consistent (to a
moderate to high degree) across different student model architectures and
learning strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VeeAlign: Multifaceted Context Representation using Dual Attention for Ontology Alignment. (arXiv:2102.04081v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.04081">
<div class="article-summary-box-inner">
<span><p>Ontology Alignment is an important research problem applied to various fields
such as data integration, data transfer, data preparation, etc.
State-of-the-art (SOTA) Ontology Alignment systems typically use naive
domain-dependent approaches with handcrafted rules or domain-specific
architectures, making them unscalable and inefficient. In this work, we propose
VeeAlign, a Deep Learning based model that uses a novel dual-attention
mechanism to compute the contextualized representation of a concept which, in
turn, is used to discover alignments. By doing this, not only is our approach
able to exploit both syntactic and semantic information encoded in ontologies,
it is also, by design, flexible and scalable to different domains with minimal
effort. We evaluate our model on four different datasets from different domains
and languages, and establish its superiority through these results as well as
detailed ablation studies. The code and datasets used are available at
https://github.com/Remorax/VeeAlign.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Three-level Hierarchical Transformer Networks for Long-sequence and Multiple Clinical Documents Classification. (arXiv:2104.08444v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08444">
<div class="article-summary-box-inner">
<span><p>We present a Three-level Hierarchical Transformer Network (3-level-HTN) for
modeling long-term dependencies across clinical notes for the purpose of
patient-level prediction. The network is equipped with three levels of
Transformer-based encoders to learn progressively from words to sentences,
sentences to notes, and finally notes to patients. The first level from word to
sentence directly applies a pre-trained BERT model as a fully trainable
component. While the second and third levels both implement a stack of
transformer-based encoders, before the final patient representation is fed into
a classification layer for clinical predictions. Compared to conventional BERT
models, our model increases the maximum input length from 512 tokens to much
longer sequences that are appropriate for modeling large numbers of clinical
notes. We empirically examine different hyper-parameters to identify an optimal
trade-off given computational resource limits. Our experiment results on the
MIMIC-III dataset for different prediction tasks demonstrate that the proposed
Hierarchical Transformer Network outperforms previous state-of-the-art models,
including but not limited to BigBird.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MathBERT: A Pre-trained Language Model for General NLP Tasks in Mathematics Education. (arXiv:2106.07340v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07340">
<div class="article-summary-box-inner">
<span><p>Since the introduction of the original BERT (i.e., BASE BERT), researchers
have developed various customized BERT models with improved performance for
specific domains and tasks by exploiting the benefits of transfer learning. Due
to the nature of mathematical texts, which often use domain specific vocabulary
along with equations and math symbols, we posit that the development of a new
BERT model for mathematics would be useful for many mathematical downstream
tasks. In this resource paper, we introduce our multi-institutional effort
(i.e., two learning platforms and three academic institutions in the US) toward
this need: MathBERT, a model created by pre-training the BASE BERT model on a
large mathematical corpus ranging from pre-kindergarten (pre-k), to
high-school, to college graduate level mathematical content. In addition, we
select three general NLP tasks that are often used in mathematics education:
prediction of knowledge component, auto-grading open-ended Q&amp;A, and knowledge
tracing, to demonstrate the superiority of MathBERT over BASE BERT. Our
experiments show that MathBERT outperforms prior best methods by 1.2-22% and
BASE BERT by 2-8% on these tasks. In addition, we build a mathematics specific
vocabulary 'mathVocab' to train with MathBERT. We discover that MathBERT
pre-trained with 'mathVocab' outperforms MathBERT trained with the BASE BERT
vocabulary (i.e., 'origVocab'). MathBERT is currently being adopted at the
participated leaning platforms: Stride, Inc, a commercial educational resource
provider, and ASSISTments.org, a free online educational platform. We release
MathBERT for public usage at: https://github.com/tbs17/MathBERT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">N24News: A New Dataset for Multimodal News Classification. (arXiv:2108.13327v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13327">
<div class="article-summary-box-inner">
<span><p>Current news datasets merely focus on text features on the news and rarely
leverage the feature of images, excluding numerous essential features for news
classification. In this paper, we propose a new dataset, N24News, which is
generated from New York Times with 24 categories and contains both text and
image information in each news. We use a multitask multimodal method and the
experimental results show multimodal news classification performs better than
text-only news classification. Depending on the length of the text, the
classification accuracy can be increased by up to 8.11%. Our research reveals
the relationship between the performance of a multimodal classifier and its
sub-classifiers, and also the possible improvements when applying multimodal in
news classification. N24News is shown to have great potential to prompt the
multimodal news studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ethics Sheet for Automatic Emotion Recognition and Sentiment Analysis. (arXiv:2109.08256v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08256">
<div class="article-summary-box-inner">
<span><p>The importance and pervasiveness of emotions in our lives makes affective
computing a tremendously important and vibrant line of work. Systems for
automatic emotion recognition (AER) and sentiment analysis can be facilitators
of enormous progress (e.g., in improving public health and commerce) but also
enablers of great harm (e.g., for suppressing dissidents and manipulating
voters). Thus, it is imperative that the affective computing community actively
engage with the ethical ramifications of their creations. In this paper, I have
synthesized and organized information from AI Ethics and Emotion Recognition
literature to present fifty ethical considerations relevant to AER. Notably,
the sheet fleshes out assumptions hidden in how AER is commonly framed, and in
the choices often made regarding the data, method, and evaluation. Special
attention is paid to the implications of AER on privacy and social groups.
Along the way, key recommendations are made for responsible AER. The objective
of the sheet is to facilitate and encourage more thoughtfulness on why to
automate, how to automate, and how to judge success well before the building of
AER systems. Additionally, the sheet acts as a useful introductory document on
emotion recognition (complementing survey articles).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural News Recommendation with Event Extraction. (arXiv:2111.05068v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.05068">
<div class="article-summary-box-inner">
<span><p>A key challenge of online news recommendation is to help users find articles
they are interested in. Traditional news recommendation methods usually use
single news information, which is insufficient to encode news and user
representation. Recent research uses multiple channel news information, e.g.,
title, category, and body, to enhance news and user representation. However,
these methods only use various attention mechanisms to fuse multi-view
embeddings without considering deep digging higher-level information contained
in the context. These methods encode news content on the word level and jointly
train the attention parameters in the recommendation network, leading to more
corpora being required to train the model. We propose an Event Extraction-based
News Recommendation (EENR) framework to overcome these shortcomings, utilizing
event extraction to abstract higher-level information. EENR also uses a
two-stage strategy to reduce parameters in subsequent parts of the
recommendation network. We train the Event Extraction module by external
corpora in the first stage and apply the trained model to the news
recommendation dataset to predict event-level information, including event
types, roles, and arguments, in the second stage. Then we fuse multiple channel
information, including event information, news title, and category, to encode
news and users. Extensive experiments on a real-world dataset show that our
EENR method can effectively improve the performance of news recommendations.
Finally, we also explore the reasonability of utilizing higher abstract level
information to substitute news body content.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A novel knowledge graph development for industry design: A case study on indirect coal liquefaction process. (arXiv:2111.13854v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13854">
<div class="article-summary-box-inner">
<span><p>Hazard and operability analysis (HAZOP) is a remarkable representative in
industrial safety engineering. However, a great storehouse of industrial safety
knowledge (ISK) in HAZOP reports has not been thoroughly exploited. In order to
reuse and unlock the value of ISK and optimize HAZOP, we have developed a novel
knowledge graph for industrial safety (ISKG) with HAZOP as the carrier through
bridging data science (DS) and engineering design (ED). Specifically, firstly,
considering that the knowledge contained in HAZOP reports of different
processes in industry is not the same, we have creatively developed a general
ISK standardization framework (ISKSF), ISKSF provides a practical scheme for
the standardization of HAZOP reports in various processes and the unified
representation of different types of ISK, which realizes the integration and
circulation of ISK. Secondly, we conceive a novel and reliable information
extraction model (HAINEX) based on deep learning combined with DS. HAINEX can
effectively mine ISK from HAZOP reports, which alleviates the obstacle of ISK
extraction caused by the particularity of HAZOP text. Finally, we build ISK
triples based on ISKSF and HAINEX and store them in the Neo4j graph database.
We take indirect coal liquefaction process as a case study to develop ISKG, and
its oriented applications can optimize HAZOP and mine the potential of ISK,
which is of great significance to improve the security of the system and
enhance prevention awareness for people. ISKG containing ISKSF and HAINEX sets
an example of the interaction between DS and ED for industrial safety, which
can enlighten other researchers committed to DS for ED and extend the
perspectives of industrial safety.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ASCEND: A Spontaneous Chinese-English Dataset for Code-switching in Multi-turn Conversation. (arXiv:2112.06223v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06223">
<div class="article-summary-box-inner">
<span><p>Code-switching is a speech phenomenon when a speaker switches language during
a conversation. Despite the spontaneous nature of code-switching in
conversational spoken language, most existing works collect code-switching data
through read speech instead of spontaneous speech. ASCEND (A Spontaneous
Chinese-English Dataset) introduces a high-quality resource of spontaneous
multi-turn conversational dialogue Chinese-English code-switching corpus
collected in Hong Kong. We report ASCEND's design and procedure of collecting
the speech data, including the annotations in this work. ASCEND includes 23
bilinguals that are fluent in both Chinese and English and consists of 10.62
hours clean speech corpus.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextualized Scene Imagination for Generative Commonsense Reasoning. (arXiv:2112.06318v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06318">
<div class="article-summary-box-inner">
<span><p>Humans use natural language to compose common concepts from their environment
into plausible, day-to-day scene descriptions. However, such generative
commonsense reasoning (GCSR) skills are lacking in state-of-the-art text
generation methods. Descriptive sentences about arbitrary concepts generated by
neural text generation models (e.g., pre-trained text-to-text Transformers) are
often grammatically fluent but may not correspond to human common sense,
largely due to their lack of mechanisms to capture concept relations, to
identify implicit concepts, and to perform generalizable reasoning about unseen
concept compositions. In this paper, we propose an Imagine-and-Verbalize (I&amp;V)
method, which learns to imagine a relational scene knowledge graph (SKG) with
relations between the input concepts, and leverage the SKG as a constraint when
generating a plausible scene description. We collect and harmonize a set of
knowledge resources from different domains and modalities, providing a rich
auxiliary supervision signal for I&amp;V. The experiments demonstrate the
effectiveness of I&amp;V in improving language models on both concept-to-sentence
and concept-to-story generation tasks, while enabling the model to learn well
from fewer task examples and generate SKGs that make common sense to human
annotators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TASSY -- A Text Annotation Survey System. (arXiv:2112.07391v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07391">
<div class="article-summary-box-inner">
<span><p>We present a free and open-source tool for creating web-based surveys that
include text annotation tasks. Existing tools offer either text annotation or
survey functionality but not both. Combining the two input types is
particularly relevant for investigating a reader's perception of a text which
also depends on the reader's background, such as age, gender, and education.
Our tool caters primarily to the needs of researchers in the Library and
Information Sciences, the Social Sciences, and the Humanities who apply Content
Analysis to investigate, e.g., media bias, political communication, or fake
news.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do You Think It's Biased? How To Ask For The Perception Of Media Bias. (arXiv:2112.07392v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07392">
<div class="article-summary-box-inner">
<span><p>Media coverage possesses a substantial effect on the public perception of
events. The way media frames events can significantly alter the beliefs and
perceptions of our society. Nevertheless, nearly all media outlets are known to
report news in a biased way. While such bias can be introduced by altering the
word choice or omitting information, the perception of bias also varies largely
depending on a reader's personal background. Therefore, media bias is a very
complex construct to identify and analyze. Even though media bias has been the
subject of many studies, previous assessment strategies are oversimplified,
lack overlap and empirical evaluation. Thus, this study aims to develop a scale
that can be used as a reliable standard to evaluate article bias. To name an
example: Intending to measure bias in a news article, should we ask, "How
biased is the article?" or should we instead ask, "How did the article treat
the American president?". We conducted a literature search to find 824 relevant
questions about text perception in previous research on the topic. In a
multi-iterative process, we summarized and condensed these questions
semantically to conclude a complete and representative set of possible question
types about bias. The final set consisted of 25 questions with varying
answering formats, 17 questions using semantic differentials, and six ratings
of feelings. We tested each of the questions on 190 articles with overall 663
participants to identify how well the questions measure an article's perceived
bias. Our results show that 21 final items are suitable and reliable for
measuring the perception of media bias. We publish the final set of questions
on <a href="http://bias-question-tree.gipplab.org/.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards A Reliable Ground-Truth For Biased Language Detection. (arXiv:2112.07421v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07421">
<div class="article-summary-box-inner">
<span><p>Reference texts such as encyclopedias and news articles can manifest biased
language when objective reporting is substituted by subjective writing.
Existing methods to detect bias mostly rely on annotated data to train machine
learning models. However, low annotator agreement and comparability is a
substantial drawback in available media bias corpora. To evaluate data
collection options, we collect and compare labels obtained from two popular
crowdsourcing platforms. Our results demonstrate the existing crowdsourcing
approaches' lack of data quality, underlining the need for a trained expert
framework to gather a more reliable dataset. By creating such a framework and
gathering a first dataset, we are able to improve Krippendorff's $\alpha$ =
0.144 (crowdsourcing labels) to $\alpha$ = 0.419 (expert labels). We conclude
that detailed annotator training increases data quality, improving the
performance of existing bias detection systems. We will continue to extend our
dataset in the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reinforced Abstractive Summarization with Adaptive Length Controlling. (arXiv:2112.07534v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07534">
<div class="article-summary-box-inner">
<span><p>Document summarization, as a fundamental task in natural language generation,
aims to generate a short and coherent summary for a given document.
Controllable summarization, especially of the length, is an important issue for
some practical applications, especially how to trade-off the length constraint
and information integrity. In this paper, we propose an \textbf{A}daptive
\textbf{L}ength \textbf{C}ontrolling \textbf{O}ptimization (\textbf{ALCO})
method to leverage two-stage abstractive summarization model via reinforcement
learning. ALCO incorporates length constraint into the stage of sentence
extraction to penalize the overlength extracted sentences. Meanwhile, a
saliency estimation mechanism is designed to preserve the salient information
in the generated sentences. A series of experiments have been conducted on a
wildly-used benchmark dataset \textit{CNN/Daily Mail}. The results have shown
that ALCO performs better than the popular baselines in terms of length
controllability and content preservation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mask-combine Decoding and Classification Approach for Punctuation Prediction with real-time Inference Constraints. (arXiv:2112.08098v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08098">
<div class="article-summary-box-inner">
<span><p>In this work, we unify several existing decoding strategies for punctuation
prediction in one framework and introduce a novel strategy which utilises
multiple predictions at each word across different windows. We show that
significant improvements can be achieved by optimising these strategies after
training a model, only leading to a potential increase in inference time, with
no requirement for retraining. We further use our decoding strategy framework
for the first comparison of tagging and classification approaches for
punctuation prediction in a real-time setting. Our results show that a
classification approach for punctuation prediction can be beneficial when
little or no right-side context is available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLIN-X: pre-trained language models and a study on cross-task transfer for concept extraction in the clinical domain. (arXiv:2112.08754v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08754">
<div class="article-summary-box-inner">
<span><p>The field of natural language processing (NLP) has recently seen a large
change towards using pre-trained language models for solving almost any task.
Despite showing great improvements in benchmark datasets for various tasks,
these models often perform sub-optimal in non-standard domains like the
clinical domain where a large gap between pre-training documents and target
documents is observed. In this paper, we aim at closing this gap with
domain-specific training of the language model and we investigate its effect on
a diverse set of downstream tasks and settings. We introduce the pre-trained
CLIN-X (Clinical XLM-R) language models and show how CLIN-X outperforms other
pre-trained transformer models by a large margin for ten clinical concept
extraction tasks from two languages. In addition, we demonstrate how the
transformer model can be further improved with our proposed task- and
language-agnostic model architecture based on ensembles over random splits and
cross-sentence context. Our studies in low-resource and transfer settings
reveal stable model performance despite a lack of annotated data with
improvements of up to 47 F1 points when only 250 labeled sentences are
available. Our results highlight the importance of specialized language models
as CLIN-X for concept extraction in non-standard domains, but also show that
our task-agnostic model architecture is robust across the tested tasks and
languages so that domain- or task-specific adaptations are not required.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">ASC-Net: Unsupervised Medical Anomaly Segmentation Using an Adversarial-based Selective Cutting Network. (arXiv:2112.09135v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09135">
<div class="article-summary-box-inner">
<span><p>In this paper we consider the problem of unsupervised anomaly segmentation in
medical images, which has attracted increasing attention in recent years due to
the expensive pixel-level annotations from experts and the existence of a large
amount of unannotated normal and abnormal image scans. We introduce a
segmentation network that utilizes adversarial learning to partition an image
into two cuts, with one of them falling into a reference distribution provided
by the user. This Adversarial-based Selective Cutting network (ASC-Net) bridges
the two domains of cluster-based deep segmentation and adversarial-based
anomaly/novelty detection algorithms. Our ASC-Net learns from normal and
abnormal medical scans to segment anomalies in medical scans without any masks
for supervision. We evaluate this unsupervised anomly segmentation model on
three public datasets, i.e., BraTS 2019 for brain tumor segmentation, LiTS for
liver lesion segmentation, and MS-SEG 2015 for brain lesion segmentation, and
also on a private dataset for brain tumor segmentation. Compared to existing
methods, our model demonstrates tremendous performance gains in unsupervised
anomaly segmentation tasks. Although there is still room to further improve
performance compared to supervised learning algorithms, the promising
experimental results and interesting observations shed light on building an
unsupervised learning algorithm for medical anomaly identification using
user-defined knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TAFIM: Targeted Adversarial Attacks against Facial Image Manipulations. (arXiv:2112.09151v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09151">
<div class="article-summary-box-inner">
<span><p>Face image manipulation methods, despite having many beneficial applications
in computer graphics, can also raise concerns by affecting an individual's
privacy or spreading disinformation. In this work, we propose a proactive
defense to prevent face manipulation from happening in the first place. To this
end, we introduce a novel data-driven approach that produces image-specific
perturbations which are embedded in the original images. The key idea is that
these protected images prevent face manipulation by causing the manipulation
model to produce a predefined manipulation target (uniformly colored output
image in our case) instead of the actual manipulation. Compared to traditional
adversarial attacks that optimize noise patterns for each image individually,
our generalized model only needs a single forward pass, thus running orders of
magnitude faster and allowing for easy integration in image processing stacks,
even on resource-constrained devices like smartphones. In addition, we propose
to leverage a differentiable compression approximation, hence making generated
perturbations robust to common image compression. We further show that a
generated perturbation can simultaneously prevent against multiple manipulation
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Investigation of the Role of Pre-training in Lifelong Learning. (arXiv:2112.09153v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09153">
<div class="article-summary-box-inner">
<span><p>The lifelong learning paradigm in machine learning is an attractive
alternative to the more prominent isolated learning scheme not only due to its
resemblance to biological learning, but also its potential to reduce energy
waste by obviating excessive model re-training. A key challenge to this
paradigm is the phenomenon of catastrophic forgetting. With the increasing
popularity and success of pre-trained models in machine learning, we pose the
question: What role does pre-training play in lifelong learning, specifically
with respect to catastrophic forgetting? We investigate existing methods in the
context of large, pre-trained models and evaluate their performance on a
variety of text and image classification tasks, including a large-scale study
using a novel dataset of 15 diverse NLP tasks. Across all settings, we observe
that generic pre-training implicitly alleviates the effects of catastrophic
forgetting when learning multiple tasks sequentially compared to randomly
initialized models. We then further investigate why pre-training alleviates
forgetting in this setting. We study this phenomenon by analyzing the loss
landscape, finding that pre-trained weights appear to ease forgetting by
leading to wider minima. Based on this insight, we propose jointly optimizing
for current task loss and loss basin sharpness in order to explicitly encourage
wider basins during sequential fine-tuning. We show that this optimization
approach leads to performance comparable to the state-of-the-art in
task-sequential continual learning across multiple settings, without retaining
a memory that scales in size with the number of tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ALEBk: Feasibility Study of Attention Level Estimation via Blink Detection applied to e-Learning. (arXiv:2112.09165v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09165">
<div class="article-summary-box-inner">
<span><p>This work presents a feasibility study of remote attention level estimation
based on eye blink frequency. We first propose an eye blink detection system
based on Convolutional Neural Networks (CNNs), very competitive with respect to
related works. Using this detector, we experimentally evaluate the relationship
between the eye blink rate and the attention level of students captured during
online sessions. The experimental framework is carried out using a public
multimodal database for eye blink detection and attention level estimation
called mEBAL, which comprises data from 38 students and multiples acquisition
sensors, in particular, i) an electroencephalogram (EEG) band which provides
the time signals coming from the student's cognitive information, and ii) RGB
and NIR cameras to capture the students face gestures. The results achieved
suggest an inverse correlation between the eye blink frequency and the
attention level. This relation is used in our proposed method called ALEBk for
estimating the attention level as the inverse of the eye blink frequency. Our
results open a new research line to introduce this technology for attention
level estimation on future e-learning platforms, among other applications of
this kind of behavioral biometrics based on face analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Audio-Visual Dataset and Deep Learning Frameworks for Crowded Scene Classification. (arXiv:2112.09172v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09172">
<div class="article-summary-box-inner">
<span><p>This paper presents a task of audio-visual scene classification (SC) where
input videos are classified into one of five real-life crowded scenes: 'Riot',
'Noise-Street', 'Firework-Event', 'Music-Event', and 'Sport-Atmosphere'. To
this end, we firstly collect an audio-visual dataset (videos) of these five
crowded contexts from Youtube (in-the-wild scenes). Then, a wide range of deep
learning frameworks are proposed to deploy either audio or visual input data
independently. Finally, results obtained from high-performed deep learning
frameworks are fused to achieve the best accuracy score. Our experimental
results indicate that audio and visual input factors independently contribute
to the SC task's performance. Significantly, an ensemble of deep learning
frameworks exploring either audio or visual input data can achieve the best
accuracy of 95.7%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Coherence Learning using Keypoint-based Pooling Network for Accurately Assessing Radiographic Knee Osteoarthritis. (arXiv:2112.09177v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09177">
<div class="article-summary-box-inner">
<span><p>Knee osteoarthritis (OA) is a common degenerate joint disorder that affects a
large population of elderly people worldwide. Accurate radiographic assessment
of knee OA severity plays a critical role in chronic patient management.
Current clinically-adopted knee OA grading systems are observer subjective and
suffer from inter-rater disagreements. In this work, we propose a
computer-aided diagnosis approach to provide more accurate and consistent
assessments of both composite and fine-grained OA grades simultaneously. A
novel semi-supervised learning method is presented to exploit the underlying
coherence in the composite and fine-grained OA grades by learning from
unlabeled data. By representing the grade coherence using the log-probability
of a pre-trained Gaussian Mixture Model, we formulate an incoherence loss to
incorporate unlabeled data in training. The proposed method also describes a
keypoint-based pooling network, where deep image features are pooled from the
disease-targeted keypoints (extracted along the knee joint) to provide more
aligned and pathologically informative feature representations, for accurate OA
grade assessments. The proposed method is comprehensively evaluated on the
public Osteoarthritis Initiative (OAI) data, a multi-center ten-year
observational study on 4,796 subjects. Experimental results demonstrate that
our method leads to significant improvements over previous strong whole
image-based deep classification network baselines (like ResNet-50).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Monitoring crop phenology with street-level imagery using computer vision. (arXiv:2112.09190v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09190">
<div class="article-summary-box-inner">
<span><p>Street-level imagery holds a significant potential to scale-up in-situ data
collection. This is enabled by combining the use of cheap high quality cameras
with recent advances in deep learning compute solutions to derive relevant
thematic information. We present a framework to collect and extract crop type
and phenological information from street level imagery using computer vision.
During the 2018 growing season, high definition pictures were captured with
side-looking action cameras in the Flevoland province of the Netherlands. Each
month from March to October, a fixed 200-km route was surveyed collecting one
picture per second resulting in a total of 400,000 geo-tagged pictures. At 220
specific parcel locations detailed on the spot crop phenology observations were
recorded for 17 crop types. Furthermore, the time span included specific
pre-emergence parcel stages, such as differently cultivated bare soil for
spring and summer crops as well as post-harvest cultivation practices, e.g.
green manuring and catch crops. Classification was done using TensorFlow with a
well-known image recognition model, based on transfer learning with
convolutional neural networks (MobileNet). A hypertuning methodology was
developed to obtain the best performing model among 160 models. This best model
was applied on an independent inference set discriminating crop type with a
Macro F1 score of 88.1% and main phenological stage at 86.9% at the parcel
level. Potential and caveats of the approach along with practical
considerations for implementation and improvement are discussed. The proposed
framework speeds up high quality in-situ data collection and suggests avenues
for massive data collection via automated classification using computer vision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mitigating the Bias of Centered Objects in Common Datasets. (arXiv:2112.09195v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09195">
<div class="article-summary-box-inner">
<span><p>Convolutional networks are considered shift invariant, but it was
demonstrated that their response may vary according to the exact location of
the objects. In this paper we will demonstrate that most commonly investigated
datasets have a bias, where objects are over-represented at the center of the
image during training. This bias and the boundary condition of these networks
can have a significant effect on the performance of these architectures and
their accuracy drops significantly as an object approaches the boundary. We
will also demonstrate how this effect can be mitigated with data augmentation
techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic-Based Few-Shot Learning by Interactive Psychometric Testing. (arXiv:2112.09201v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09201">
<div class="article-summary-box-inner">
<span><p>Few-shot classification tasks aim to classify images in query sets based on
only a few labeled examples in support sets. Most studies usually assume that
each image in a task has a single and unique class association. Under these
assumptions, these algorithms may not be able to identify the proper class
assignment when there is no exact matching between support and query classes.
For example, given a few images of lions, bikes, and apples to classify a
tiger. However, in a more general setting, we could consider the higher-level
concept of large carnivores to match the tiger to the lion for semantic
classification. Existing studies rarely considered this situation due to the
incompatibility of label-based supervision with complex conception
relationships. In this work, we advanced the few-shot learning towards this
more challenging scenario, the semantic-based few-shot learning, and proposed a
method to address the paradigm by capturing the inner semantic relationships
using interactive psychometric learning. We evaluate our method on the
CIFAR-100 dataset. The results show the merits of our proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AFDetV2: Rethinking the Necessity of the Second Stage for Object Detection from Point Clouds. (arXiv:2112.09205v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09205">
<div class="article-summary-box-inner">
<span><p>There have been two streams in the 3D detection from point clouds:
single-stage methods and two-stage methods. While the former is more
computationally efficient, the latter usually provides better detection
accuracy. By carefully examining the two-stage approaches, we have found that
if appropriately designed, the first stage can produce accurate box regression.
In this scenario, the second stage mainly rescores the boxes such that the
boxes with better localization get selected. From this observation, we have
devised a single-stage anchor-free network that can fulfill these requirements.
This network, named AFDetV2, extends the previous work by incorporating a
self-calibrated convolution block in the backbone, a keypoint auxiliary
supervision, and an IoU prediction branch in the multi-task head. As a result,
the detection accuracy is drastically boosted in the single-stage. To evaluate
our approach, we have conducted extensive experiments on the Waymo Open Dataset
and the nuScenes Dataset. We have observed that our AFDetV2 achieves the
state-of-the-art results on these two datasets, superior to all the prior arts,
including both the single-stage and the two-stage se3D detectors. AFDetV2 won
the 1st place in the Real-Time 3D Detection of the Waymo Open Dataset Challenge
2021. In addition, a variant of our model AFDetV2-Base was entitled the "Most
Efficient Model" by the Challenge Sponsor, showing a superior computational
efficiency. To demonstrate the generality of this single-stage method, we have
also applied it to the first stage of the two-stage networks. Without
exception, the results show that with the strengthened backbone and the
rescoring approach, the second stage refinement is no longer needed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse Coding with Multi-Layer Decoders using Variance Regularization. (arXiv:2112.09214v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09214">
<div class="article-summary-box-inner">
<span><p>Sparse coding with an $l_1$ penalty and a learned linear dictionary requires
regularization of the dictionary to prevent a collapse in the $l_1$ norms of
the codes. Typically, this regularization entails bounding the Euclidean norms
of the dictionary's elements. In this work, we propose a novel sparse coding
protocol which prevents a collapse in the codes without the need to regularize
the decoder. Our method regularizes the codes directly so that each latent code
component has variance greater than a fixed threshold over a set of sparse
representations for a given set of inputs. Furthermore, we explore ways to
effectively train sparse coding systems with multi-layer decoders since they
can model more complex relationships than linear dictionaries. In our
experiments with MNIST and natural image patches, we show that decoders learned
with our approach have interpretable features both in the linear and
multi-layer case. Moreover, we show that sparse autoencoders with multi-layer
decoders trained using our variance regularization method produce higher
quality reconstructions with sparser representations when compared to
autoencoders with linear dictionaries. Additionally, sparse representations
obtained with our variance regularization approach are useful in the downstream
tasks of denoising and classification in the low-data regime.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Deep-Learning Framework for Improving COVID-19 CT Image Quality and Diagnostic Accuracy. (arXiv:2112.09216v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09216">
<div class="article-summary-box-inner">
<span><p>We present a deep-learning based computing framework for fast-and-accurate CT
(DL-FACT) testing of COVID-19. Our CT-based DL framework was developed to
improve the testing speed and accuracy of COVID-19 (plus its variants) via a
DL-based approach for CT image enhancement and classification. The image
enhancement network is adapted from DDnet, short for DenseNet and Deconvolution
based network. To demonstrate its speed and accuracy, we evaluated DL-FACT
across several sources of COVID-19 CT images. Our results show that DL-FACT can
significantly shorten the turnaround time from days to minutes and improve the
COVID-19 testing accuracy up to 91%. DL-FACT could be used as a software tool
for medical professionals in diagnosing and monitoring COVID-19.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">All You Need is RAW: Defending Against Adversarial Attacks with Camera Image Pipelines. (arXiv:2112.09219v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09219">
<div class="article-summary-box-inner">
<span><p>Existing neural networks for computer vision tasks are vulnerable to
adversarial attacks: adding imperceptible perturbations to the input images can
fool these methods to make a false prediction on an image that was correctly
predicted without the perturbation. Various defense methods have proposed
image-to-image mapping methods, either including these perturbations in the
training process or removing them in a preprocessing denoising step. In doing
so, existing methods often ignore that the natural RGB images in today's
datasets are not captured but, in fact, recovered from RAW color filter array
captures that are subject to various degradations in the capture. In this work,
we exploit this RAW data distribution as an empirical prior for adversarial
defense. Specifically, we proposed a model-agnostic adversarial defensive
method, which maps the input RGB images to Bayer RAW space and back to output
RGB using a learned camera image signal processing (ISP) pipeline to eliminate
potential adversarial patterns. The proposed method acts as an off-the-shelf
preprocessing module and, unlike model-specific adversarial training methods,
does not require adversarial images to train. As a result, the method
generalizes to unseen tasks without additional retraining. Experiments on
large-scale datasets (e.g., ImageNet, COCO) for different vision tasks (e.g.,
classification, semantic segmentation, object detection) validate that the
method significantly outperforms existing methods across task domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sim2Real Docs: Domain Randomization for Documents in Natural Scenes using Ray-traced Rendering. (arXiv:2112.09220v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09220">
<div class="article-summary-box-inner">
<span><p>In the past, computer vision systems for digitized documents could rely on
systematically captured, high-quality scans. Today, transactions involving
digital documents are more likely to start as mobile phone photo uploads taken
by non-professionals. As such, computer vision for document automation must now
account for documents captured in natural scene contexts. An additional
challenge is that task objectives for document processing can be highly
use-case specific, which makes publicly-available datasets limited in their
utility, while manual data labeling is also costly and poorly translates
between use cases.
</p>
<p>To address these issues we created Sim2Real Docs - a framework for
synthesizing datasets and performing domain randomization of documents in
natural scenes. Sim2Real Docs enables programmatic 3D rendering of documents
using Blender, an open source tool for 3D modeling and ray-traced rendering. By
using rendering that simulates physical interactions of light, geometry,
camera, and background, we synthesize datasets of documents in a natural scene
context. Each render is paired with use-case specific ground truth data
specifying latent characteristics of interest, producing unlimited fit-for-task
training data. The role of machine learning models is then to solve the inverse
problem posed by the rendering pipeline. Such models can be further iterated
upon with real-world data by either fine tuning or making adjustments to domain
randomization parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Wanderings of Odysseus in 3D Scenes. (arXiv:2112.09251v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09251">
<div class="article-summary-box-inner">
<span><p>Our goal is to populate digital environments, in which the digital humans
have diverse body shapes, move perpetually, and have plausible body-scene
contact. The core challenge is to generate realistic, controllable, and
infinitely long motions for diverse 3D bodies. To this end, we propose
generative motion primitives via body surface markers, shortened as GAMMA. In
our solution, we decompose the long-term motion into a time sequence of motion
primitives. We exploit body surface markers and conditional variational
autoencoder to model each motion primitive, and generate long-term motion by
implementing the generative model recursively. To control the motion to reach a
goal, we apply a policy network to explore the model latent space, and use a
tree-based search to preserve the motion quality during testing. Experiments
show that our method can produce more realistic and controllable motion than
state-of-the-art data-driven method. With conventional path-finding algorithms,
the generated human bodies can realistically move long distances for a long
period of time in the scene. Code will be released for research purposes at:
\url{https://yz-cnsdqz.github.io/eigenmotion/GAMMA/}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Logically at the Factify 2022: Multimodal Fact Verification. (arXiv:2112.09253v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09253">
<div class="article-summary-box-inner">
<span><p>This paper describes our participant system for the multi-modal fact
verification (Factify) challenge at AAAI 2022. Despite the recent advance in
text based verification techniques and large pre-trained multimodal models
cross vision and language, very limited work has been done in applying
multimodal techniques to automate fact checking process, particularly
considering the increasing prevalence of claims and fake news about images and
videos on social media. In our work, the challenge is treated as multimodal
entailment task and framed as multi-class classification. Two baseline
approaches are proposed and explored including an ensemble model (combining two
uni-modal models) and a multi-modal attention network (modeling the interaction
between image and text pair from claim and evidence document). We conduct
several experiments investigating and benchmarking different SoTA pre-trained
transformers and vision models in this work. Our best model is ranked first in
leaderboard which obtains a weighted average F-measure of 0.77 on both
validation and test set. Exploratory analysis of dataset is also carried out on
the Factify data set and uncovers salient patterns and issues (e.g., word
overlapping, visual entailment correlation, source bias) that motivates our
hypothesis. Finally, we highlight challenges of the task and multimodal dataset
for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Image Denoising Algorithm Using Concepts of Quantum Many-Body Theory. (arXiv:2112.09254v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09254">
<div class="article-summary-box-inner">
<span><p>Sparse representation of real-life images is a very effective approach in
imaging applications, such as denoising. In recent years, with the growth of
computing power, data-driven strategies exploiting the redundancy within
patches extracted from one or several images to increase sparsity have become
more prominent. This paper presents a novel image denoising algorithm
exploiting such an image-dependent basis inspired by the quantum many-body
theory. Based on patch analysis, the similarity measures in a local image
neighborhood are formalized through a term akin to interaction in quantum
mechanics that can efficiently preserve the local structures of real images.
The versatile nature of this adaptive basis extends the scope of its
application to image-independent or image-dependent noise scenarios without any
adjustment. We carry out a rigorous comparison with contemporary methods to
demonstrate the denoising capability of the proposed algorithm regardless of
the image characteristics, noise statistics and intensity. We illustrate the
properties of the hyperparameters and their respective effects on the denoising
performance, together with automated rules of selecting their values close to
the optimal one in experimental setups with ground truth not available.
Finally, we show the ability of our approach to deal with practical images
denoising problems such as medical ultrasound image despeckling applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How to augment your ViTs? Consistency loss and StyleAug, a random style transfer augmentation. (arXiv:2112.09260v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09260">
<div class="article-summary-box-inner">
<span><p>The Vision Transformer (ViT) architecture has recently achieved competitive
performance across a variety of computer vision tasks. One of the motivations
behind ViTs is weaker inductive biases, when compared to convolutional neural
networks (CNNs). However this also makes ViTs more difficult to train. They
require very large training datasets, heavy regularization, and strong data
augmentations. The data augmentation strategies used to train ViTs have largely
been inherited from CNN training, despite the significant differences between
the two architectures. In this work, we empirical evaluated how different data
augmentation strategies performed on CNN (e.g., ResNet) versus ViT
architectures for image classification. We introduced a style transfer data
augmentation, termed StyleAug, which worked best for training ViTs, while
RandAugment and Augmix typically worked best for training CNNs. We also found
that, in addition to a classification loss, using a consistency loss between
multiple augmentations of the same image was especially helpful when training
ViTs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Inpainting Using AutoEncoder and Guided Selection of Predicted Pixels. (arXiv:2112.09262v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09262">
<div class="article-summary-box-inner">
<span><p>Image inpainting is an effective method to enhance distorted digital images.
Different inpainting methods use the information of neighboring pixels to
predict the value of missing pixels. Recently deep neural networks have been
used to learn structural and semantic details of images for inpainting
purposes. In this paper, we propose a network for image inpainting. This
network, similar to U-Net, extracts various features from images, leading to
better results. We improved the final results by replacing the damaged pixels
with the recovered pixels of the output images. Our experimental results show
that this method produces high-quality results compare to the traditional
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">All-photon Polarimetric Time-of-Flight Imaging. (arXiv:2112.09278v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09278">
<div class="article-summary-box-inner">
<span><p>Time-of-flight (ToF) sensors provide an imaging modality fueling diverse
applications, including LiDAR in autonomous driving, robotics, and augmented
reality. Conventional ToF imaging methods estimate the depth by sending pulses
of light into a scene and measuring the ToF of the first-arriving photons
directly reflected from a scene surface without any temporal delay. As such,
all photons following this first response are typically considered as unwanted
noise. In this paper, we depart from the principle of using first-arriving
photons and propose an all-photon ToF imaging method by incorporating the
temporal-polarimetric analysis of first- and late-arriving photons, which
possess rich scene information about its geometry and material. To this end, we
propose a novel temporal-polarimetric reflectance model, an efficient capture
method, and a reconstruction method that exploits the temporal-polarimetric
changes of light reflected by the surface and sub-surface reflection. The
proposed all-photon polarimetric ToF imaging method allows for acquiring depth,
surface normals, and material parameters of a scene by utilizing all photons
captured by the system, whereas conventional ToF imaging only obtains coarse
depth from the first-arriving photons. We validate our method in simulation and
experimentally with a prototype.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PeopleSansPeople: A Synthetic Data Generator for Human-Centric Computer Vision. (arXiv:2112.09290v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09290">
<div class="article-summary-box-inner">
<span><p>In recent years, person detection and human pose estimation have made great
strides, helped by large-scale labeled datasets. However, these datasets had no
guarantees or analysis of human activities, poses, or context diversity.
Additionally, privacy, legal, safety, and ethical concerns may limit the
ability to collect more human data. An emerging alternative to real-world data
that alleviates some of these issues is synthetic data. However, creation of
synthetic data generators is incredibly challenging and prevents researchers
from exploring their usefulness. Therefore, we release a human-centric
synthetic data generator PeopleSansPeople which contains simulation-ready 3D
human assets, a parameterized lighting and camera system, and generates 2D and
3D bounding box, instance and semantic segmentation, and COCO pose labels.
Using PeopleSansPeople, we performed benchmark synthetic data training using a
Detectron2 Keypoint R-CNN variant [1]. We found that pre-training a network
using synthetic data and fine-tuning on target real-world data (few-shot
transfer to limited subsets of COCO-person train [2]) resulted in a keypoint AP
of $60.37 \pm 0.48$ (COCO test-dev2017) outperforming models trained with the
same real data alone (keypoint AP of $55.80$) and pre-trained with ImageNet
(keypoint AP of $57.50$). This freely-available data generator should enable a
wide range of research into the emerging field of simulation to real transfer
learning in the critical area of human-centric computer vision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human-Vehicle Cooperative Visual Perception for Shared Autonomous Driving. (arXiv:2112.09298v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09298">
<div class="article-summary-box-inner">
<span><p>With the development of key technologies like environment perception, the
automation level of autonomous vehicles has been increasing. However, before
reaching highly autonomous driving, manual driving still needs to participate
in the driving process to ensure the safety of human-vehicle shared driving.
The existing human-vehicle cooperative driving focuses on auto engineering and
drivers' behaviors, with few research studies in the field of visual
perception. Due to the bad performance in the complex road traffic conflict
scenarios, cooperative visual perception needs to be studied further. In
addition, the autonomous driving perception system cannot correctly understand
the characteristics of manual driving. Based on the background above, this
paper directly proposes a human-vehicle cooperative visual perception method to
enhance the visual perception ability of shared autonomous driving based on the
transfer learning method and the image fusion algorithm for the complex road
traffic scenarios. Based on transfer learning, the mAP of object detection
reaches 75.52% and lays a solid foundation for visual fusion. And the fusion
experiment further reveals that human-vehicle cooperative visual perception
reflects the riskiest zone and predicts the conflict object's trajectory more
precisely. This study pioneers a cooperative visual perception solution for
shared autonomous driving and experiments in real-world complex traffic
conflict scenarios, which can better support the following planning and
controlling and improve the safety of autonomous vehicles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards End-to-End Image Compression and Analysis with Transformers. (arXiv:2112.09300v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09300">
<div class="article-summary-box-inner">
<span><p>We propose an end-to-end image compression and analysis model with
Transformers, targeting to the cloud-based image classification application.
Instead of placing an existing Transformer-based image classification model
directly after an image codec, we aim to redesign the Vision Transformer (ViT)
model to perform image classification from the compressed features and
facilitate image compression with the long-term information from the
Transformer. Specifically, we first replace the patchify stem (i.e., image
splitting and embedding) of the ViT model with a lightweight image encoder
modelled by a convolutional neural network. The compressed features generated
by the image encoder are injected convolutional inductive bias and are fed to
the Transformer for image classification bypassing image reconstruction.
Meanwhile, we propose a feature aggregation module to fuse the compressed
features with the selected intermediate features of the Transformer, and feed
the aggregated features to a deconvolutional neural network for image
reconstruction. The aggregated features can obtain the long-term information
from the self-attention mechanism of the Transformer and improve the
compression performance. The rate-distortion-accuracy optimization problem is
finally solved by a two-step training strategy. Experimental results
demonstrate the effectiveness of the proposed model in both the image
compression and the classification tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Procedural Kernel Networks. (arXiv:2112.09318v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09318">
<div class="article-summary-box-inner">
<span><p>In the last decade Convolutional Neural Networks (CNNs) have defined the
state of the art for many low level image processing and restoration tasks such
as denoising, demosaicking, upscaling, or inpainting. However, on-device mobile
photography is still dominated by traditional image processing techniques, and
uses mostly simple machine learning techniques or limits the neural network
processing to producing low resolution masks. High computational and memory
requirements of CNNs, limited processing power and thermal constraints of
mobile devices, combined with large output image resolutions (typically 8--12
MPix) prevent their wider application. In this work, we introduce Procedural
Kernel Networks (PKNs), a family of machine learning models which generate
parameters of image filter kernels or other traditional algorithms. A
lightweight CNN processes the input image at a lower resolution, which yields a
significant speedup compared to other kernel-based machine learning methods and
allows for new applications. The architecture is learned end-to-end and is
especially well suited for a wide range of low-level image processing tasks,
where it improves the performance of many traditional algorithms. We also
describe how this framework unifies some previous work applying machine
learning for common image restoration tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cinderella's shoe won't fit Soundarya: An audit of facial processing tools on Indian faces. (arXiv:2112.09326v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09326">
<div class="article-summary-box-inner">
<span><p>The increasing adoption of facial processing systems in India is fraught with
concerns of privacy, transparency, accountability, and missing procedural
safeguards. At the same time, we also know very little about how these
technologies perform on the diverse features, characteristics, and skin tones
of India's 1.34 billion-plus population. In this paper, we test the face
detection and facial analysis functions of four commercial facial processing
tools on a dataset of Indian faces. The tools display varying error rates in
the face detection and gender and age classification functions. The gender
classification error rate for Indian female faces is consistently higher
compared to that of males -- the highest female error rate being 14.68%. In
some cases, this error rate is much higher than that shown by previous studies
for females of other nationalities. Age classification errors are also high.
Despite taking into account an acceptable error margin of plus or minus 10
years from a person's actual age, age prediction failures are in the range of
14.3% to 42.2%. These findings point to the limited accuracy of facial
processing tools, particularly for certain demographic groups, and the need for
more critical thinking before adopting such systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Point2Cyl: Reverse Engineering 3D Objects from Point Clouds to Extrusion Cylinders. (arXiv:2112.09329v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09329">
<div class="article-summary-box-inner">
<span><p>We propose Point2Cyl, a supervised network transforming a raw 3D point cloud
to a set of extrusion cylinders. Reverse engineering from a raw geometry to a
CAD model is an essential task to enable manipulation of the 3D data in shape
editing software and thus expand their usages in many downstream applications.
Particularly, the form of CAD models having a sequence of extrusion cylinders
-- a 2D sketch plus an extrusion axis and range -- and their boolean
combinations is not only widely used in the CAD community/software but also has
great expressivity of shapes, compared to having limited types of primitives
(e.g., planes, spheres, and cylinders). In this work, we introduce a neural
network that solves the extrusion cylinder decomposition problem in a
geometry-grounded way by first learning underlying geometric proxies.
Precisely, our approach first predicts per-point segmentation, base/barrel
labels and normals, then estimates for the underlying extrusion parameters in
differentiable and closed-form formulations. Our experiments show that our
approach demonstrates the best performance on two recent CAD datasets, Fusion
Gallery and DeepCAD, and we further showcase our approach on reverse
engineering and editing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ZeroVL: A Strong Baseline for Aligning Vision-Language Representations with Limited Resources. (arXiv:2112.09331v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09331">
<div class="article-summary-box-inner">
<span><p>Pioneering dual-encoder pre-training works (e.g., CLIP and ALIGN) have
revealed the potential of aligning multi-modal representations with contrastive
learning. However, these works require a tremendous amount of data and
computational resources (e.g., billion-level web data and hundreds of GPUs),
which prevent researchers with limited resources from reproduction and further
exploration. To this end, we explore a stack of simple but effective
heuristics, and provide a comprehensive training guidance, which allows us to
conduct dual-encoder multi-modal representation alignment with limited
resources. We provide a reproducible strong baseline of competitive results,
namely ZeroVL, with only 14M publicly accessible academic datasets and 8 V100
GPUs. Additionally, we collect 100M web data for pre-training, and achieve
comparable or superior results than state-of-the-art methods, further proving
the effectiveness of our method on large-scale data. We hope that this work
will provide useful data points and experience for future research in
multi-modal pre-training. Our code and pre-trained models will be released to
facilitate the research community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Adaptation on Point Clouds via Geometry-Aware Implicits. (arXiv:2112.09343v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09343">
<div class="article-summary-box-inner">
<span><p>As a popular geometric representation, point clouds have attracted much
attention in 3D vision, leading to many applications in autonomous driving and
robotics. One important yet unsolved issue for learning on point cloud is that
point clouds of the same object can have significant geometric variations if
generated using different procedures or captured using different sensors. These
inconsistencies induce domain gaps such that neural networks trained on one
domain may fail to generalize on others. A typical technique to reduce the
domain gap is to perform adversarial training so that point clouds in the
feature space can align. However, adversarial training is easy to fall into
degenerated local minima, resulting in negative adaptation gains. Here we
propose a simple yet effective method for unsupervised domain adaptation on
point clouds by employing a self-supervised task of learning geometry-aware
implicits, which plays two critical roles in one shot. First, the geometric
information in the point clouds is preserved through the implicit
representations for downstream tasks. More importantly, the domain-specific
variations can be effectively learned away in the implicit space. We also
propose an adaptive strategy to compute unsigned distance fields for arbitrary
point clouds due to the lack of shape models in practice. When combined with a
task loss, the proposed outperforms state-of-the-art unsupervised domain
adaptation methods that rely on adversarial domain alignment and more
complicated self-supervised tasks. Our method is evaluated on both PointDA-10
and GraspNet datasets. The code and trained models will be publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified 2D and 3D Pre-training for Medical Image classification and Segmentation. (arXiv:2112.09356v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09356">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning (SSL) opens up huge opportunities for better
utilizing unlabeled data. It is essential for medical image analysis that is
generally known for its lack of annotations. However, when we attempt to use as
many as possible unlabeled medical images in SSL, breaking the dimension
barrier (\ie, making it possible to jointly use both 2D and 3D images) becomes
a must. In this paper, we propose a Universal Self-Supervised Transformer
(USST) framework based on the student-teacher paradigm, aiming to leverage a
huge of unlabeled medical data with multiple dimensions to learn rich
representations. To achieve this, we design a Pyramid Transformer U-Net (PTU)
as the backbone, which is composed of switchable patch embedding (SPE) layers
and Transformer layers. The SPE layer switches to either 2D or 3D patch
embedding depending on the input dimension. After that, the images are
converted to a sequence regardless of their original dimensions. The
Transformer layer then models the long-term dependencies in a
sequence-to-sequence manner, thus enabling USST to learn representations from
both 2D and 3D images. USST has two obvious merits compared to current
dimension-specific SSL: (1) \textbf{more effective} - can learn representations
from more and diverse data; and (2) \textbf{more versatile} - can be
transferred to various downstream tasks. The results show that USST provides
promising results on six 2D/3D medical image classification and segmentation
tasks, outperforming the supervised ImageNet pre-training and advanced SSL
counterparts substantially.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpreting Audiograms with Multi-stage Neural Networks. (arXiv:2112.09357v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09357">
<div class="article-summary-box-inner">
<span><p>Audiograms are a particular type of line charts representing individuals'
hearing level at various frequencies. They are used by audiologists to diagnose
hearing loss, and further select and tune appropriate hearing aids for
customers. There have been several projects such as Autoaudio that aim to
accelerate this process through means of machine learning. But all existing
models at their best can only detect audiograms in images and classify them
into general categories. They are unable to extract hearing level information
from detected audiograms by interpreting the marks, axis, and lines. To address
this issue, we propose a Multi-stage Audiogram Interpretation Network (MAIN)
that directly reads hearing level data from photos of audiograms. We also
established Open Audiogram, an open dataset of audiogram images with
annotations of marks and axes on which we trained and evaluated our proposed
model. Experiments show that our model is feasible and reliable.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Colloquium: Advances in automation of quantum dot devices control. (arXiv:2112.09362v1 [quant-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09362">
<div class="article-summary-box-inner">
<span><p>Arrays of quantum dots (QDs) are a promising candidate system to realize
scalable, coupled qubits systems and serve as a fundamental building block for
quantum computers. In such semiconductor quantum systems, devices now have tens
of individual electrostatic and dynamical voltages that must be carefully set
to localize the system into the single-electron regime and to realize good
qubit operational performance. The mapping of requisite dot locations and
charges to gate voltages presents a challenging classical control problem. With
an increasing number of QD qubits, the relevant parameter space grows
sufficiently to make heuristic control unfeasible. In recent years, there has
been a considerable effort to automate device control that combines
script-based algorithms with machine learning (ML) techniques. In this
Colloquium, we present a comprehensive overview of the recent progress in the
automation of QD device control, with a particular emphasis on silicon- and
GaAs-based QDs formed in two-dimensional electron gases. Combining
physics-based modeling with modern numerical optimization and ML has proven
quite effective in yielding efficient, scalable control. Further integration of
theoretical, computational, and experimental efforts with computer science and
ML holds tremendous potential in advancing semiconductor and other platforms
for quantum computing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SuperStyleNet: Deep Image Synthesis with Superpixel Based Style Encoder. (arXiv:2112.09367v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09367">
<div class="article-summary-box-inner">
<span><p>Existing methods for image synthesis utilized a style encoder based on stacks
of convolutions and pooling layers to generate style codes from input images.
However, the encoded vectors do not necessarily contain local information of
the corresponding images since small-scale objects are tended to "wash away"
through such downscaling procedures. In this paper, we propose deep image
synthesis with superpixel based style encoder, named as SuperStyleNet. First,
we directly extract the style codes from the original image based on
superpixels to consider local objects. Second, we recover spatial relationships
in vectorized style codes based on graphical analysis. Thus, the proposed
network achieves high-quality image synthesis by mapping the style codes into
semantic labels. Experimental results show that the proposed method outperforms
state-of-the-art ones in terms of visual quality and quantitative measurements.
Furthermore, we achieve elaborate spatial style editing by adjusting style
codes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhanced Frame and Event-Based Simulator and Event-Based Video Interpolation Network. (arXiv:2112.09379v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09379">
<div class="article-summary-box-inner">
<span><p>Fast neuromorphic event-based vision sensors (Dynamic Vision Sensor, DVS) can
be combined with slower conventional frame-based sensors to enable
higher-quality inter-frame interpolation than traditional methods relying on
fixed motion approximations using e.g. optical flow. In this work we present a
new, advanced event simulator that can produce realistic scenes recorded by a
camera rig with an arbitrary number of sensors located at fixed offsets. It
includes a new configurable frame-based image sensor model with realistic image
quality reduction effects, and an extended DVS model with more accurate
characteristics. We use our simulator to train a novel reconstruction model
designed for end-to-end reconstruction of high-fps video. Unlike previously
published methods, our method does not require the frame and DVS cameras to
have the same optics, positions, or camera resolutions. It is also not limited
to objects a fixed distance from the sensor. We show that data generated by our
simulator can be used to train our new model, leading to reconstructed images
on public datasets of equivalent or better quality than the state of the art.
We also show our sensor generalizing to data recorded by real sensors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Full Transformer Framework for Robust Point Cloud Registration with Deep Information Interaction. (arXiv:2112.09385v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09385">
<div class="article-summary-box-inner">
<span><p>Recent Transformer-based methods have achieved advanced performance in point
cloud registration by utilizing advantages of the Transformer in
order-invariance and modeling dependency to aggregate information. However,
they still suffer from indistinct feature extraction, sensitivity to noise, and
outliers. The reasons are: (1) the adoption of CNNs fails to model global
relations due to their local receptive fields, resulting in extracted features
susceptible to noise; (2) the shallow-wide architecture of Transformers and
lack of positional encoding lead to indistinct feature extraction due to
inefficient information interaction; (3) the omission of geometrical
compatibility leads to inaccurate classification between inliers and outliers.
To address above limitations, a novel full Transformer network for point cloud
registration is proposed, named the Deep Interaction Transformer (DIT), which
incorporates: (1) a Point Cloud Structure Extractor (PSE) to model global
relations and retrieve structural information with Transformer encoders; (2) a
deep-narrow Point Feature Transformer (PFT) to facilitate deep information
interaction across two point clouds with positional encoding, such that
Transformers can establish comprehensive associations and directly learn
relative position between points; (3) a Geometric Matching-based Correspondence
Confidence Evaluation (GMCCE) method to measure spatial consistency and
estimate inlier confidence by designing the triangulated descriptor. Extensive
experiments on clean, noisy, partially overlapping point cloud registration
demonstrate that our method outperforms state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-attention based anchor proposal for skeleton-based action recognition. (arXiv:2112.09413v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09413">
<div class="article-summary-box-inner">
<span><p>Skeleton sequences are widely used for action recognition task due to its
lightweight and compact characteristics. Recent graph convolutional network
(GCN) approaches have achieved great success for skeleton-based action
recognition since its grateful modeling ability of non-Euclidean data. GCN is
able to utilize the short-range joint dependencies while lack to directly model
the distant joints relations that are vital to distinguishing various actions.
Thus, many GCN approaches try to employ hierarchical mechanism to aggregate
wider-range neighborhood information. We propose a novel self-attention based
skeleton-anchor proposal (SAP) module to comprehensively model the internal
relations of a human body for motion feature learning. The proposed SAP module
aims to explore inherent relationship within human body using a triplet
representation via encoding high order angle information rather than the fixed
pair-wise bone connection used in the existing hierarchical GCN approaches. A
Self-attention based anchor selection method is designed in the proposed SAP
module for extracting the root point of encoding angular information. By
coupling proposed SAP module with popular spatial-temporal graph neural
networks, e.g. MSG3D, it achieves new state-of-the-art accuracy on challenging
benchmark datasets. Further ablation study have shown the effectiveness of our
proposed SAP module, which is able to obviously improve the performance of many
popular skeleton-based action recognition methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Disentangled representations: towards interpretation of sex determination from hip bone. (arXiv:2112.09414v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09414">
<div class="article-summary-box-inner">
<span><p>By highlighting the regions of the input image that contribute the most to
the decision, saliency maps have become a popular method to make neural
networks interpretable. In medical imaging, they are particularly well-suited
to explain neural networks in the context of abnormality localization. However,
from our experiments, they are less suited to classification problems where the
features that allow to distinguish between the different classes are spatially
correlated, scattered and definitely non-trivial. In this paper we thus propose
a new paradigm for better interpretability. To this end we provide the user
with relevant and easily interpretable information so that he can form his own
opinion. We use Disentangled Variational Auto-Encoders which latent
representation is divided into two components: the non-interpretable part and
the disentangled part. The latter accounts for the categorical variables
explicitly representing the different classes of interest. In addition to
providing the class of a given input sample, such a model offers the
possibility to transform the sample from a given class to a sample of another
class, by modifying the value of the categorical variables in the latent
representation. This paves the way to easier interpretation of class
differences. We illustrate the relevance of this approach in the context of
automatic sex determination from hip bones in forensic medicine. The features
encoded by the model, that distinguish the different classes were found to be
consistent with expert knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Review on Visual Privacy Preservation Techniques for Active and Assisted Living. (arXiv:2112.09422v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09422">
<div class="article-summary-box-inner">
<span><p>This paper reviews the state of the art in visual privacy protection
techniques, with particular attention paid to techniques applicable to the
field of active and assisted living (AAL). A novel taxonomy with which
state-of-the-art visual privacy protection methods can be classified is
introduced. Perceptual obfuscation methods, a category in the taxonomy, is
highlighted. These are a category of visual privacy preservation techniques
particularly relevant when considering scenarios that come under video-based
AAL monitoring. Obfuscation against machine learning models is also explored. A
high-level classification scheme of the different levels of privacy by design
is connected to the proposed taxonomy of visual privacy preservation
techniques. Finally, we note open questions that exist in the field and
introduce the reader to some exciting avenues for future research in the area
of visual privacy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SiamTrans: Zero-Shot Multi-Frame Image Restoration with Pre-Trained Siamese Transformers. (arXiv:2112.09426v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09426">
<div class="article-summary-box-inner">
<span><p>We propose a novel zero-shot multi-frame image restoration method for
removing unwanted obstruction elements (such as rains, snow, and moire
patterns) that vary in successive frames. It has three stages: transformer
pre-training, zero-shot restoration, and hard patch refinement. Using the
pre-trained transformers, our model is able to tell the motion difference
between the true image information and the obstructing elements. For zero-shot
image restoration, we design a novel model, termed SiamTrans, which is
constructed by Siamese transformers, encoders, and decoders. Each transformer
has a temporal attention layer and several self-attention layers, to capture
both temporal and spatial information of multiple frames. Only pre-trained
(self-supervised) on the denoising task, SiamTrans is tested on three different
low-level vision tasks (deraining, demoireing, and desnowing). Compared with
related methods, ours achieves the best performances, even outperforming those
with supervised learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamics-aware Adversarial Attack of 3D Sparse Convolution Network. (arXiv:2112.09428v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09428">
<div class="article-summary-box-inner">
<span><p>In this paper, we investigate the dynamics-aware adversarial attack problem
in deep neural networks. Most existing adversarial attack algorithms are
designed under a basic assumption -- the network architecture is fixed
throughout the attack process. However, this assumption does not hold for many
recently proposed networks, e.g. 3D sparse convolution network, which contains
input-dependent execution to improve computational efficiency. It results in a
serious issue of lagged gradient, making the learned attack at the current step
ineffective due to the architecture changes afterward. To address this issue,
we propose a Leaded Gradient Method (LGM) and show the significant effects of
the lagged gradient. More specifically, we re-formulate the gradients to be
aware of the potential dynamic changes of network architectures, so that the
learned attack better "leads" the next step than the dynamics-unaware methods
when network architecture changes dynamically. Extensive experiments on various
datasets show that our LGM achieves impressive performance on semantic
segmentation and classification. Compared with the dynamic-unaware methods, LGM
achieves about 20% lower mIoU averagely on the ScanNet and S3DIS datasets. LGM
also outperforms the recent point cloud attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptively Customizing Activation Functions for Various Layers. (arXiv:2112.09442v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09442">
<div class="article-summary-box-inner">
<span><p>To enhance the nonlinearity of neural networks and increase their mapping
abilities between the inputs and response variables, activation functions play
a crucial role to model more complex relationships and patterns in the data. In
this work, a novel methodology is proposed to adaptively customize activation
functions only by adding very few parameters to the traditional activation
functions such as Sigmoid, Tanh, and ReLU. To verify the effectiveness of the
proposed methodology, some theoretical and experimental analysis on
accelerating the convergence and improving the performance is presented, and a
series of experiments are conducted based on various network models (such as
AlexNet, VGGNet, GoogLeNet, ResNet and DenseNet), and various datasets (such as
CIFAR10, CIFAR100, miniImageNet, PASCAL VOC and COCO) . To further verify the
validity and suitability in various optimization strategies and usage
scenarios, some comparison experiments are also implemented among different
optimization strategies (such as SGD, Momentum, AdaGrad, AdaDelta and ADAM) and
different recognition tasks like classification and detection. The results show
that the proposed methodology is very simple but with significant performance
in convergence speed, precision and generalization, and it can surpass other
popular methods like ReLU and adaptive functions like Swish in almost all
experiments in terms of overall performance.The code is publicly available at
https://github.com/HuHaigen/Adaptively-Customizing-Activation-Functions. The
package includes the proposed three adaptive activation functions for
reproducibility purposes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Efficient Language-supervised Zero-shot Recognition with Optimal Transport Distillation. (arXiv:2112.09445v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09445">
<div class="article-summary-box-inner">
<span><p>Traditional computer vision models are trained to predict a fixed set of
predefined categories. Recently, natural language has been shown to be a
broader and richer source of supervision that provides finer descriptions to
visual concepts than supervised "gold" labels. Previous works, such as CLIP,
use InfoNCE loss to train a model to predict the pairing between images and
text captions. CLIP, however, is data hungry and requires more than 400M
image-text pairs for training. The inefficiency can be partially attributed to
the fact that the image-text pairs are noisy. To address this, we propose OTTER
(Optimal TransporT distillation for Efficient zero-shot Recognition), which
uses online entropic optimal transport to find a soft image-text match as
labels for contrastive learning. Based on pretrained image and text encoders,
models trained with OTTER achieve strong performance with only 3M image text
pairs. Compared with InfoNCE loss, label smoothing, and knowledge distillation,
OTTER consistently outperforms these baselines in zero shot evaluation on
Google Open Images (19,958 classes) and multi-labeled ImageNet 10K (10032
classes) from Tencent ML-Images. Over 42 evaluations on 7 different
dataset/architecture settings x 6 metrics, OTTER outperforms (32) or ties (2)
all baselines in 34 of them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distillation of Human-Object Interaction Contexts for Action Recognition. (arXiv:2112.09448v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09448">
<div class="article-summary-box-inner">
<span><p>Modeling spatial-temporal relations is imperative for recognizing human
actions, especially when a human is interacting with objects, while multiple
objects appear around the human differently over time. Most existing action
recognition models focus on learning overall visual cues of a scene but
disregard informative fine-grained features, which can be captured by learning
human-object relationships and interactions. In this paper, we learn
human-object relationships by exploiting the interaction of their local and
global contexts. We hence propose the Global-Local Interaction Distillation
Network (GLIDN), learning human and object interactions through space and time
via knowledge distillation for fine-grained scene understanding. GLIDN encodes
humans and objects into graph nodes and learns local and global relations via
graph attention network. The local context graphs learn the relation between
humans and objects at a frame level by capturing their co-occurrence at a
specific time step. The global relation graph is constructed based on the
video-level of human and object interactions, identifying their long-term
relations throughout a video sequence. More importantly, we investigate how
knowledge from these graphs can be distilled to their counterparts for
improving human-object interaction (HOI) recognition. We evaluate our model by
conducting comprehensive experiments on two datasets including Charades and
CAD-120 datasets. We have achieved better results than the baselines and
counterpart approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Supervised Semantic Segmentation via Alternative Self-Dual Teaching. (arXiv:2112.09459v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09459">
<div class="article-summary-box-inner">
<span><p>Current weakly supervised semantic segmentation (WSSS) frameworks usually
contain the separated mask-refinement model and the main semantic region mining
model. These approaches would contain redundant feature extraction backbones
and biased learning objectives, making them computational complex yet
sub-optimal to addressing the WSSS task. To solve this problem, this paper
establishes a compact learning framework that embeds the classification and
mask-refinement components into a unified deep model. With the shared feature
extraction backbone, our model is able to facilitate knowledge sharing between
the two components while preserving a low computational complexity. To
encourage high-quality knowledge interaction, we propose a novel alternative
self-dual teaching (ASDT) mechanism. Unlike the conventional distillation
strategy, the knowledge of the two teacher branches in our model is
alternatively distilled to the student branch by a Pulse Width Modulation
(PWM), which generates PW wave-like selection signal to guide the knowledge
distillation process. In this way, the student branch can help prevent the
model from falling into local minimum solutions caused by the imperfect
knowledge provided of either teacher branch. Comprehensive experiments on the
PASCAL VOC 2012 and COCO-Stuff 10K demonstrate the effectiveness of the
proposed alternative self-dual teaching mechanism as well as the new
state-of-the-art performance of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Microfossil Identificationvia Deep Metric Learning. (arXiv:2112.09490v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09490">
<div class="article-summary-box-inner">
<span><p>We apply deep metric learning for the first time to the prob-lem of
classifying planktic foraminifer shells on microscopic images. This species
recognition task is an important information source and scientific pillar for
reconstructing past climates. All foraminifer CNN recognition pipelines in the
literature produce black-box classifiers that lack visualisation options for
human experts and cannot be applied to open set problems. Here, we benchmark
metric learning against these pipelines, produce the first scientific
visualisation of the phenotypic planktic foraminifer morphology space, and
demonstrate that metric learning can be used to cluster species unseen during
training. We show that metric learning out-performs all published CNN-based
state-of-the-art benchmarks in this domain. We evaluate our approach on the
34,640 expert-annotated images of the Endless Forams public library of 35
modern planktic foraminifera species. Our results on this data show leading 92%
accuracy (at 0.84 F1-score) in reproducing expert labels on withheld test data,
and 66.5% accuracy (at 0.70 F1-score) when clustering species never encountered
in training. We conclude that metric learning is highly effective for this
domain and serves as an important tool towards expert-in-the-loop automation of
microfossil identification. Key code, network weights, and data splits are
published with this paper for full reproducibility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Methods for segmenting cracks in 3d images of concrete: A comparison based on semi-synthetic images. (arXiv:2112.09493v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09493">
<div class="article-summary-box-inner">
<span><p>Concrete is the standard construction material for buildings, bridges, and
roads. As safety plays a central role in the design, monitoring, and
maintenance of such constructions, it is important to understand the cracking
behavior of concrete. Computed tomography captures the microstructure of
building materials and allows to study crack initiation and propagation. Manual
segmentation of crack surfaces in large 3d images is not feasible. In this
paper, automatic crack segmentation methods for 3d images are reviewed and
compared. Classical image processing methods (edge detection filters, template
matching, minimal path and region growing algorithms) and learning methods
(convolutional neural networks, random forests) are considered and tested on
semi-synthetic 3d images. Their performance strongly depends on parameter
selection which should be adapted to the grayvalue distribution of the images
and the geometric properties of the concrete. In general, the learning methods
perform best, in particular for thin cracks and low grayvalue contrast.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Launching AI Algorithms for Cellular Pathology into Clinical & Pharmaceutical Orbits. (arXiv:2112.09496v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09496">
<div class="article-summary-box-inner">
<span><p>Computational Pathology (CPath) is an emerging field concerned with the study
of tissue pathology via computational algorithms for the processing and
analysis of digitized high-resolution images of tissue slides. Recent deep
learning based developments in CPath have successfully leveraged sheer volume
of raw pixel data in histology images for predicting target parameters in the
domains of diagnostics, prognostics, treatment sensitivity and patient
stratification -- heralding the promise of a new data-driven AI era for both
histopathology and oncology. With data serving as the fuel and AI as the
engine, CPath algorithms are poised to be ready for takeoff and eventual launch
into clinical and pharmaceutical orbits. In this paper, we discuss CPath
limitations and associated challenges to enable the readers distinguish hope
from hype and provide directions for future research to overcome some of the
major challenges faced by this budding field to enable its launch into the two
orbits.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Symmetry-aware Neural Architecture for Embodied Visual Navigation. (arXiv:2112.09515v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09515">
<div class="article-summary-box-inner">
<span><p>Visual exploration is a task that seeks to visit all the navigable areas of
an environment as quickly as possible. The existing methods employ deep
reinforcement learning (RL) as the standard tool for the task. However, they
tend to be vulnerable to statistical shifts between the training and test data,
resulting in poor generalization over novel environments that are
out-of-distribution (OOD) from the training data. In this paper, we attempt to
improve the generalization ability by utilizing the inductive biases available
for the task. Employing the active neural SLAM (ANS) that learns exploration
policies with the advantage actor-critic (A2C) method as the base framework, we
first point out that the mappings represented by the actor and the critic
should satisfy specific symmetries. We then propose a network design for the
actor and the critic to inherently attain these symmetries. Specifically, we
use $G$-convolution instead of the standard convolution and insert the
semi-global polar pooling (SGPP) layer, which we newly design in this study, in
the last section of the critic network. Experimental results show that our
method increases area coverage by $8.1 m^2$ when trained on the Gibson dataset
and tested on the MP3D dataset, establishing the new state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Rate-Distortion Optimized Learned Hierarchical Bi-Directional Video Compression. (arXiv:2112.09529v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09529">
<div class="article-summary-box-inner">
<span><p>Conventional video compression (VC) methods are based on motion compensated
transform coding, and the steps of motion estimation, mode and quantization
parameter selection, and entropy coding are optimized individually due to the
combinatorial nature of the end-to-end optimization problem. Learned VC allows
end-to-end rate-distortion (R-D) optimized training of nonlinear transform,
motion and entropy model simultaneously. Most works on learned VC consider
end-to-end optimization of a sequential video codec based on R-D loss averaged
over pairs of successive frames. It is well-known in conventional VC that
hierarchical, bi-directional coding outperforms sequential compression because
of its ability to use both past and future reference frames. This paper
proposes a learned hierarchical bi-directional video codec (LHBDC) that
combines the benefits of hierarchical motion-compensated prediction and
end-to-end optimization. Experimental results show that we achieve the best R-D
results that are reported for learned VC schemes to date in both PSNR and
MS-SSIM. Compared to conventional video codecs, the R-D performance of our
end-to-end optimized codec outperforms those of both x265 and SVT-HEVC encoders
("veryslow" preset) in PSNR and MS-SSIM as well as HM 16.23 reference software
in MS-SSIM. We present ablation studies showing performance gains due to
proposed novel tools such as learned masking, flow-field subsampling, and
temporal flow vector prediction. The models and instructions to reproduce our
results can be found in https://github.com/makinyilmaz/LHBDC/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pixel Distillation: A New Knowledge Distillation Scheme for Low-Resolution Image Recognition. (arXiv:2112.09532v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09532">
<div class="article-summary-box-inner">
<span><p>The great success of deep learning is mainly due to the large-scale network
architecture and the high-quality training data. However, it is still
challenging to deploy recent deep models on portable devices with limited
memory and imaging ability. Some existing works have engaged to compress the
model via knowledge distillation. Unfortunately, these methods cannot deal with
images with reduced image quality, such as the low-resolution (LR) images. To
this end, we make a pioneering effort to distill helpful knowledge from a heavy
network model learned from high-resolution (HR) images to a compact network
model that will handle LR images, thus advancing the current knowledge
distillation technique with the novel pixel distillation. To achieve this goal,
we propose a Teacher-Assistant-Student (TAS) framework, which disentangles
knowledge distillation into the model compression stage and the high resolution
representation transfer stage. By equipping a novel Feature Super Resolution
(FSR) module, our approach can learn lightweight network model that can achieve
similar accuracy as the heavy teacher model but with much fewer parameters,
faster inference speed, and lower-resolution inputs. Comprehensive experiments
on three widely-used benchmarks, \ie, CUB-200-2011, PASCAL VOC 2007, and
ImageNetSub, demonstrate the effectiveness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Complex Functional Maps : a Conformal Link Between Tangent Bundles. (arXiv:2112.09546v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09546">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce complex functional maps, which extend the
functional map framework to conformal maps between tangent vector fields on
surfaces. A key property of these maps is their orientation awareness. More
specifically, we demonstrate that unlike regular functional maps that link
functional spaces of two manifolds, our complex functional maps establish a
link between oriented tangent bundles, thus permitting robust and efficient
transfer of tangent vector fields. By first endowing and then exploiting the
tangent bundle of each shape with a complex structure, the resulting operations
become naturally orientationaware, thus favoring orientation and angle
preserving correspondence across shapes, without relying on descriptors or
extra regularization. Finally, and perhaps more importantly, we demonstrate how
these objects enable several practical applications within the functional map
framework. We show that functional maps and their complex counterparts can be
estimated jointly to promote orientation preservation, regularizing pipelines
that previously suffered from orientation-reversing symmetry errors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LTB curves with Lipschitz turn are par-regular. (arXiv:2112.09567v1 [cs.CG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09567">
<div class="article-summary-box-inner">
<span><p>Preserving the topology during a digitization process is a requirement of
first importance. To this end, it is classical in Digital Geometry to assume
the shape borders to be par-regular. Par-regularity was proved to be equivalent
to having positive reach or to belong to the class C 1,1 of curves with
Lipschitz derivative. Recently, we proposed to use a larger class that
encompasses polygons with obtuse angles, the locally turn-bounded curves. The
aim of this technical report is to define the class of par-regular curves
inside the class of locally turn-bounded curves using only the notion of turn,
that is of integral curvature. To be more precise, in a previous article, we
have already proved that par-regular curves are locally turn-bounded.
Incidentally this proof lead us to show that the turn of par-regular curves is
a Lipschitz function of their length. We call the class of curves verifying
this latter property the curves with Lipschitz turn. In this technical report,
we prove the converse assertion : locally turn-bounded curves with Lipschitz
turn are par-regular. The equivalence is stated in Theorem 3.1 and the converse
assertion is proved in Lemma 3.2. In section 1, we recall the definition of
par-regularity and equivalently of sets with positive reach. In section 2, we
present the notions of curves locally turn-bounded and of curves with Lipschitz
turn. Throughout this latter section, some of intermediate steps (Lemmas 2.3
and 2.11) are proved just after the introduction of their related notions. The
last section (section 3) is dedicated to the proof of the equivalence of the
notions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nearest neighbor search with compact codes: A decoder perspective. (arXiv:2112.09568v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09568">
<div class="article-summary-box-inner">
<span><p>Modern approaches for fast retrieval of similar vectors on billion-scaled
datasets rely on compressed-domain approaches such as binary sketches or
product quantization. These methods minimize a certain loss, typically the mean
squared error or other objective functions tailored to the retrieval problem.
In this paper, we re-interpret popular methods such as binary hashing or
product quantizers as auto-encoders, and point out that they implicitly make
suboptimal assumptions on the form of the decoder. We design
backward-compatible decoders that improve the reconstruction of the vectors
from the same codes, which translates to a better performance in nearest
neighbor search. Our method significantly improves over binary hashing methods
or product quantization on popular benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CPPE-5: Medical Personal Protective Equipment Dataset. (arXiv:2112.09569v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09569">
<div class="article-summary-box-inner">
<span><p>We present a new challenging dataset, CPPE - 5 (Medical Personal Protective
Equipment), with the goal to allow the study of subordinate categorization of
medical personal protective equipments, which is not possible with other
popular data sets that focus on broad level categories (such as PASCAL VOC,
ImageNet, Microsoft COCO, OpenImages, etc). To make it easy for models trained
on this dataset to be used in practical scenarios in complex scenes, our
dataset mainly contains images that show complex scenes with several objects in
each scene in their natural context. The image collection for this dataset
focusing on: obtaining as many non-iconic images as possible and making sure
all the images are real-life images unlike other existing datasets in this
area. Our dataset includes 5 object categories (coveralls, face shield, gloves,
mask, and goggles) and each image is annotated with a set of bounding boxes and
positive labels. We present a detailed analysis of the dataset in comparison to
other popular broad category datasets as well as datasets focusing on personal
protective equipments, we also find that at present there exist no such
publicly available datasets. Finally we also analyze performance and compare
model complexities on baseline and state-of-the-art models for bounding box
results. Our code, data, and trained models are available at
https://git.io/cppe5-dataset .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Super-resolution reconstruction of cytoskeleton image based on A-net deep learning network. (arXiv:2112.09574v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09574">
<div class="article-summary-box-inner">
<span><p>To date, live-cell imaging at the nanometer scale remains challenging. Even
though super-resolution microscopy methods have enabled visualization of
subcellular structures below the optical resolution limit, the spatial
resolution is still far from enough for the structural reconstruction of
biomolecules in vivo (i.e. ~24 nm thickness of microtubule fiber). In this
study, we proposed an A-net network and showed that the resolution of
cytoskeleton images captured by a confocal microscope can be significantly
improved by combining the A-net deep learning network with the DWDC algorithm
based on degradation model. Utilizing the DWDC algorithm to construct new
datasets and taking advantage of A-net neural network's features (i.e.,
considerably fewer layers), we successfully removed the noise and flocculent
structures, which originally interfere with the cellular structure in the raw
image, and improved the spatial resolution by 10 times using relatively small
dataset. We, therefore, conclude that the proposed algorithm that combines
A-net neural network with the DWDC method is a suitable and universal approach
for exacting structural details of biomolecules, cells and organs from
low-resolution images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Watermarking Images in Self-Supervised Latent Spaces. (arXiv:2112.09581v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09581">
<div class="article-summary-box-inner">
<span><p>We revisit watermarking techniques based on pre-trained deep networks, in the
light of self-supervised approaches. We present a way to embed both marks and
binary messages into their latent spaces, leveraging data augmentation at
marking time. Our method can operate at any resolution and creates watermarks
robust to a broad range of transformations (rotations, crops, JPEG, contrast,
etc). It significantly outperforms the previous zero-bit methods, and its
performance on multi-bit watermarking is on par with state-of-the-art
encoder-decoder architectures trained end-to-end for watermarking. Our
implementation and models will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Align and Prompt: Video-and-Language Pre-training with Entity Prompts. (arXiv:2112.09583v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09583">
<div class="article-summary-box-inner">
<span><p>Video-and-language pre-training has shown promising improvements on various
downstream tasks. Most previous methods capture cross-modal interactions with a
transformer-based multimodal encoder, not fully addressing the misalignment
between unimodal video and text features. Besides, learning fine-grained
visual-language alignment usually requires off-the-shelf object detectors to
provide object information, which is bottlenecked by the detector's limited
vocabulary and expensive computation cost.
</p>
<p>We propose Align and Prompt: an efficient and effective video-and-language
pre-training framework with better cross-modal alignment. First, we introduce a
video-text contrastive (VTC) loss to align unimodal video-text features at the
instance level, which eases the modeling of cross-modal interactions. Then, we
propose a new visually-grounded pre-training task, prompting entity modeling
(PEM), which aims to learn fine-grained region-entity alignment. To achieve
this, we first introduce an entity prompter module, which is trained with VTC
to produce the similarity between a video crop and text prompts instantiated
with entity names. The PEM task then asks the model to predict the entity
pseudo-labels (i.e~normalized similarity scores) for randomly-selected video
crops. The resulting pre-trained model achieves state-of-the-art performance on
both text-video retrieval and videoQA, outperforming prior work by a
substantial margin. Our code and pre-trained models will be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Global explainability in aligned image modalities. (arXiv:2112.09591v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09591">
<div class="article-summary-box-inner">
<span><p>Deep learning (DL) models are very effective on many computer vision problems
and increasingly used in critical applications. They are also inherently black
box. A number of methods exist to generate image-wise explanations that allow
practitioners to understand and verify model predictions for a given image.
Beyond that, it would be desirable to validate that a DL model
\textit{generally} works in a sensible way, i.e. consistent with domain
knowledge and not relying on undesirable data artefacts. For this purpose, the
model needs to be explained globally. In this work, we focus on image
modalities that are naturally aligned such that each pixel position represents
a similar relative position on the imaged object, as is common in medical
imaging. We propose the pixel-wise aggregation of image-wise explanations as a
simple method to obtain label-wise and overall global explanations. These can
then be used for model validation, knowledge discovery, and as an efficient way
to communicate qualitative conclusions drawn from inspecting image-wise
explanations. We further propose Progressive Erasing Plus Progressive
Restoration (PEPPR) as a method to quantitatively validate that these global
explanations are faithful to how the model makes its predictions. We then apply
these methods to ultra-widefield retinal images, a naturally aligned modality.
We find that the global explanations are consistent with domain knowledge and
faithfully reflect the model's workings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Deep Learning-based 6D Bin Pose Estimation in 3D Scans. (arXiv:2112.09598v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09598">
<div class="article-summary-box-inner">
<span><p>An automated robotic system needs to be as robust as possible and fail-safe
in general while having relatively high precision and repeatability. Although
deep learning-based methods are becoming research standard on how to approach
3D scan and image processing tasks, the industry standard for processing this
data is still analytically-based. Our paper claims that analytical methods are
less robust and harder for testing, updating, and maintaining. This paper
focuses on a specific task of 6D pose estimation of a bin in 3D scans.
Therefore, we present a high-quality dataset composed of synthetic data and
real scans captured by a structured-light scanner with precise annotations.
Additionally, we propose two different methods for 6D bin pose estimation, an
analytical method as the industrial standard and a baseline data-driven method.
Both approaches are cross-evaluated, and our experiments show that augmenting
the training on real scans with synthetic data improves our proposed
data-driven neural model. This position paper is preliminary, as proposed
methods are trained and evaluated on a relatively small initial dataset which
we plan to extend in the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Local contrastive loss with pseudo-label based self-training for semi-supervised medical image segmentation. (arXiv:2112.09645v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09645">
<div class="article-summary-box-inner">
<span><p>Supervised deep learning-based methods yield accurate results for medical
image segmentation. However, they require large labeled datasets for this, and
obtaining them is a laborious task that requires clinical expertise.
Semi/self-supervised learning-based approaches address this limitation by
exploiting unlabeled data along with limited annotated data. Recent
self-supervised learning methods use contrastive loss to learn good global
level representations from unlabeled images and achieve high performance in
classification tasks on popular natural image datasets like ImageNet. In
pixel-level prediction tasks such as segmentation, it is crucial to also learn
good local level representations along with global representations to achieve
better accuracy. However, the impact of the existing local contrastive
loss-based methods remains limited for learning good local representations
because similar and dissimilar local regions are defined based on random
augmentations and spatial proximity; not based on the semantic label of local
regions due to lack of large-scale expert annotations in the
semi/self-supervised setting. In this paper, we propose a local contrastive
loss to learn good pixel level features useful for segmentation by exploiting
semantic label information obtained from pseudo-labels of unlabeled images
alongside limited annotated images. In particular, we define the proposed loss
to encourage similar representations for the pixels that have the same
pseudo-label/ label while being dissimilar to the representation of pixels with
different pseudo-label/label in the dataset. We perform pseudo-label based
self-training and train the network by jointly optimizing the proposed
contrastive loss on both labeled and unlabeled sets and segmentation loss on
only the limited labeled set. We evaluated on three public cardiac and prostate
datasets, and obtain high segmentation performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video-Based Reconstruction of the Trajectories Performed by Skiers. (arXiv:2112.09647v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09647">
<div class="article-summary-box-inner">
<span><p>Trajectories are fundamental in different skiing disciplines. Tools enabling
the analysis of such curves can enhance the training activity and enrich the
broadcasting contents. However, the solutions currently available are based on
geo-localized sensors and surface models. In this short paper, we propose a
video-based approach to reconstruct the sequence of points traversed by an
athlete during its performance. Our prototype is constituted by a pipeline of
deep learning-based algorithms to reconstruct the athlete's motion and to
visualize it according to the camera perspective. This is achieved for
different skiing disciplines in the wild without any camera calibration. We
tested our solution on broadcast and smartphone-captured videos of alpine
skiing and ski jumping professional competitions. The qualitative results
achieved show the potential of our solution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving neural implicit surfaces geometry with patch warping. (arXiv:2112.09648v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09648">
<div class="article-summary-box-inner">
<span><p>Neural implicit surfaces have become an important technique for multi-view 3D
reconstruction but their accuracy remains limited. In this paper, we argue that
this comes from the difficulty to learn and render high frequency textures with
neural networks. We thus propose to add to the standard neural rendering
optimization a direct photo-consistency term across the different views.
Intuitively, we optimize the implicit geometry so that it warps views on each
other in a consistent way. We demonstrate that two elements are key to the
success of such an approach: (i) warping entire patches, using the predicted
occupancy and normals of the 3D points along each ray, and measuring their
similarity with a robust structural similarity (SSIM); (ii) handling visibility
and occlusion in such a way that incorrect warps are not given too much
importance while encouraging a reconstruction as complete as possible. We
evaluate our approach, dubbed NeuralWarp, on the standard DTU and EPFL
benchmarks and show it outperforms state of the art unsupervised implicit
surfaces reconstructions by over 20% on both datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Information-theoretic stochastic contrastive conditional GAN: InfoSCC-GAN. (arXiv:2112.09653v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09653">
<div class="article-summary-box-inner">
<span><p>Conditional generation is a subclass of generative problems where the output
of the generation is conditioned by the attribute information. In this paper,
we present a stochastic contrastive conditional generative adversarial network
(InfoSCC-GAN) with an explorable latent space. The InfoSCC-GAN architecture is
based on an unsupervised contrastive encoder built on the InfoNCE paradigm, an
attribute classifier and an EigenGAN generator. We propose a novel training
method, based on generator regularization using external or internal attributes
every $n$-th iteration, using a pre-trained contrastive encoder and a
pre-trained classifier. The proposed InfoSCC-GAN is derived based on an
information-theoretic formulation of mutual information maximization between
input data and latent space representation as well as latent space and
generated data. Thus, we demonstrate a link between the training objective
functions and the above information-theoretic formulation. The experimental
results show that InfoSCC-GAN outperforms the "vanilla" EigenGAN in the image
generation on AFHQ and CelebA datasets. In addition, we investigate the impact
of discriminator architectures and loss functions by performing ablation
studies. Finally, we demonstrate that thanks to the EigenGAN generator, the
proposed framework enjoys a stochastic generation in contrast to vanilla
deterministic GANs yet with the independent training of encoder, classifier,
and generator in contrast to existing frameworks. Code, experimental results,
and demos are available online at https://github.com/vkinakh/InfoSCC-GAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FastSurferVINN: Building Resolution-Independence into Deep Learning Segmentation Methods -- A Solution for HighRes Brain MRI. (arXiv:2112.09654v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09654">
<div class="article-summary-box-inner">
<span><p>Leading neuroimaging studies have pushed 3T MRI acquisition resolutions below
1.0 mm for improved structure definition and morphometry. Yet, only few,
time-intensive automated image analysis pipelines have been validated for
high-resolution (HiRes) settings. Efficient deep learning approaches, on the
other hand, rarely support more than one fixed resolution (usually 1.0 mm).
Furthermore, the lack of a standard submillimeter resolution as well as limited
availability of diverse HiRes data with sufficient coverage of scanner, age,
diseases, or genetic variance poses additional, unsolved challenges for
training HiRes networks. Incorporating resolution-independence into deep
learning-based segmentation, i.e., the ability to segment images at their
native resolution across a range of different voxel sizes, promises to overcome
these challenges, yet no such approach currently exists. We now fill this gap
by introducing a Voxelsize Independent Neural Network (VINN) for
resolution-independent segmentation tasks and present FastSurferVINN, which (i)
establishes and implements resolution-independence for deep learning as the
first method simultaneously supporting 0.7-1.0 mm whole brain segmentation,
(ii) significantly outperforms state-of-the-art methods across resolutions, and
(iii) mitigates the data imbalance problem present in HiRes datasets. Overall,
internal resolution-independence mutually benefits both HiRes and 1.0 mm MRI
segmentation. With our rigorously validated FastSurferVINN we distribute a
rapid tool for morphometric neuroimage analysis. The VINN architecture,
furthermore, represents an efficient resolution-independent segmentation method
for wider application
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AI-Assisted Verification of Biometric Data Collection. (arXiv:2112.09660v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09660">
<div class="article-summary-box-inner">
<span><p>Recognizing actions from a video feed is a challenging task to automate,
especially so on older hardware. There are two aims for this project: one is to
recognize an action from the front-facing camera on an Android phone, the other
is to support as many phones and Android versions as possible. This limits us
to using models that are small enough to run on mobile phones with and without
GPUs, and only using the camera feed to recognize the action. In this paper we
compare performance of the YOLO architecture across devices (with and without
dedicated GPUs) using models trained on a custom dataset. We also discuss
limitations in recognizing faces and actions from video on limited hardware.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards More Effective PRM-based Crowd Counting via A Multi-resolution Fusion and Attention Network. (arXiv:2112.09664v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09664">
<div class="article-summary-box-inner">
<span><p>The paper focuses on improving the recent plug-and-play patch rescaling
module (PRM) based approaches for crowd counting. In order to make full use of
the PRM potential and obtain more reliable and accurate results for challenging
images with crowd-variation, large perspective, extreme occlusions, and
cluttered background regions, we propose a new PRM based multi-resolution and
multi-task crowd counting network by exploiting the PRM module with more
effectiveness and potency. The proposed model consists of three deep-layered
branches with each branch generating feature maps of different resolutions.
These branches perform a feature-level fusion across each other to build the
vital collective knowledge to be used for the final crowd estimate.
Additionally, early-stage feature maps undergo visual attention to strengthen
the later-stage channels understanding of the foreground regions. The
integration of these deep branches with the PRM module and the early-attended
blocks proves to be more effective than the original PRM based schemes through
extensive numerical and visual evaluations on four benchmark datasets. The
proposed approach yields a significant improvement by a margin of 12.6% in
terms of the RMSE evaluation criterion. It also outperforms state-of-the-art
methods in cross-dataset evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning for Spatiotemporal Modeling of Urbanization. (arXiv:2112.09668v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09668">
<div class="article-summary-box-inner">
<span><p>Urbanization has a strong impact on the health and wellbeing of populations
across the world. Predictive spatial modeling of urbanization therefore can be
a useful tool for effective public health planning. Many spatial urbanization
models have been developed using classic machine learning and numerical
modeling techniques. However, deep learning with its proven capacity to capture
complex spatiotemporal phenomena has not been applied to urbanization modeling.
Here we explore the capacity of deep spatial learning for the predictive
modeling of urbanization. We treat numerical geospatial data as images with
pixels and channels, and enrich the dataset by augmentation, in order to
leverage the high capacity of deep learning. Our resulting model can generate
end-to-end multi-variable urbanization predictions, and outperforms a
state-of-the-art classic machine learning urbanization model in preliminary
comparisons.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neuromorphic Camera Denoising using Graph Neural Network-driven Transformers. (arXiv:2112.09685v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09685">
<div class="article-summary-box-inner">
<span><p>Neuromorphic vision is a bio-inspired technology that has triggered a
paradigm shift in the computer-vision community and is serving as a key-enabler
for a multitude of applications. This technology has offered significant
advantages including reduced power consumption, reduced processing needs, and
communication speed-ups. However, neuromorphic cameras suffer from significant
amounts of measurement noise. This noise deteriorates the performance of
neuromorphic event-based perception and navigation algorithms. In this paper,
we propose a novel noise filtration algorithm to eliminate events which do not
represent real log-intensity variations in the observed scene. We employ a
Graph Neural Network (GNN)-driven transformer algorithm, called
GNN-Transformer, to classify every active event pixel in the raw stream into
real-log intensity variation or noise. Within the GNN, a message-passing
framework, called EventConv, is carried out to reflect the spatiotemporal
correlation among the events, while preserving their asynchronous nature. We
also introduce the Known-object Ground-Truth Labeling (KoGTL) approach for
generating approximate ground truth labels of event streams under various
illumination conditions. KoGTL is used to generate labeled datasets, from
experiments recorded in challenging lighting conditions. These datasets are
used to train and extensively test our proposed algorithm. When tested on
unseen datasets, the proposed algorithm outperforms existing methods by 12% in
terms of filtration accuracy. Additional tests are also conducted on publicly
available datasets to demonstrate the generalization capabilities of the
proposed algorithm in the presence of illumination variations and different
motion dynamics. Compared to existing solutions, qualitative results verified
the superior capability of the proposed algorithm to eliminate noise while
preserving meaningful scene events.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Visual Tracking with Exemplar Transformers. (arXiv:2112.09686v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09686">
<div class="article-summary-box-inner">
<span><p>The design of more complex and powerful neural network models has
significantly advanced the state-of-the-art in visual object tracking. These
advances can be attributed to deeper networks, or to the introduction of new
building blocks, such as transformers. However, in the pursuit of increased
tracking performance, efficient tracking architectures have received
surprisingly little attention. In this paper, we introduce the Exemplar
Transformer, an efficient transformer for real-time visual object tracking.
E.T.Track, our visual tracker that incorporates Exemplar Transformer layers,
runs at 47 fps on a CPU. This is up to 8 times faster than other
transformer-based models, making it the only real-time transformer-based
tracker. When compared to lightweight trackers that can operate in real-time on
standard CPUs, E.T.Track consistently outperforms all other methods on the
LaSOT, OTB-100, NFS, TrackingNet and VOT-ST2020 datasets. The code will soon be
released on https://github.com/visionml/pytracking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Light Field Neural Rendering. (arXiv:2112.09687v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09687">
<div class="article-summary-box-inner">
<span><p>Classical light field rendering for novel view synthesis can accurately
reproduce view-dependent effects such as reflection, refraction, and
translucency, but requires a dense view sampling of the scene. Methods based on
geometric reconstruction need only sparse views, but cannot accurately model
non-Lambertian effects. We introduce a model that combines the strengths and
mitigates the limitations of these two directions. By operating on a
four-dimensional representation of the light field, our model learns to
represent view-dependent effects accurately. By enforcing geometric constraints
during training and inference, the scene geometry is implicitly learned from a
sparse set of views. Concretely, we introduce a two-stage transformer-based
model that first aggregates features along epipolar lines, then aggregates
features along reference views to produce the color of a target ray. Our model
outperforms the state-of-the-art on multiple forward-facing and 360{\deg}
datasets, with larger margins on scenes with severe view-dependent variations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Model Pseudo-Labeling for Semi-Supervised Action Recognition. (arXiv:2112.09690v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09690">
<div class="article-summary-box-inner">
<span><p>Semi-supervised action recognition is a challenging but important task due to
the high cost of data annotation. A common approach to this problem is to
assign unlabeled data with pseudo-labels, which are then used as additional
supervision in training. Typically in recent work, the pseudo-labels are
obtained by training a model on the labeled data, and then using confident
predictions from the model to teach itself. In this work, we propose a more
effective pseudo-labeling scheme, called Cross-Model Pseudo-Labeling (CMPL).
Concretely, we introduce a lightweight auxiliary network in addition to the
primary backbone, and ask them to predict pseudo-labels for each other. We
observe that, due to their different structural biases, these two models tend
to learn complementary representations from the same video clips. Each model
can thus benefit from its counterpart by utilizing cross-model predictions as
supervision. Experiments on different data partition protocols demonstrate the
significant improvement of our framework over existing alternatives. For
example, CMPL achieves $17.6\%$ and $25.1\%$ Top-1 accuracy on Kinetics-400 and
UCF-101 using only the RGB modality and $1\%$ labeled data, outperforming our
baseline model, FixMatch, by $9.0\%$ and $10.3\%$, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Un-Mix: Rethinking Image Mixtures for Unsupervised Visual Representation Learning. (arXiv:2003.05438v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.05438">
<div class="article-summary-box-inner">
<span><p>The recently advanced unsupervised learning approaches use the siamese-like
framework to compare two "views" from the same image for learning
representations. Making the two views distinctive is a core to guarantee that
unsupervised methods can learn meaningful information. However, such frameworks
are sometimes fragile on overfitting if the augmentations used for generating
two views are not strong enough, causing the over-confident issue on the
training data. This drawback hinders the model from learning subtle variance
and fine-grained information. To address this, in this work we aim to involve
the distance concept on label space in the unsupervised learning and let the
model be aware of the soft degree of similarity between positive or negative
pairs through mixing the input data space, to further work collaboratively for
the input and loss spaces. Despite its conceptual simplicity, we show
empirically that with the solution -- Unsupervised image mixtures (Un-Mix), we
can learn subtler, more robust and generalized representations from the
transformed input and corresponding new label space. Extensive experiments are
conducted on CIFAR-10, CIFAR-100, STL-10, Tiny ImageNet and standard ImageNet
with popular unsupervised methods SimCLR, BYOL, MoCo V1&amp;V2, SwAV, etc. Our
proposed image mixture and label assignment strategy can obtain consistent
improvement by 1~3% following exactly the same hyperparameters and training
procedures of the base methods. Code is publicly available at
https://github.com/szq0214/Un-Mix.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DTVNet+: A High-Resolution Scenic Dataset for Dynamic Time-lapse Video Generation. (arXiv:2008.04776v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.04776">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel end-to-end dynamic time-lapse video generation
framework, named DTVNet, to generate diversified time-lapse videos from a
single landscape image conditioned on normalized motion vectors. The proposed
DTVNet consists of two submodules: \emph{Optical Flow Encoder} (OFE) and
\emph{Dynamic Video Generator} (DVG). The OFE maps a sequence of optical flow
maps to a \emph{normalized motion vector} that encodes the motion information
of the generated video. The DVG contains motion and content streams to learn
from the motion vector and the single landscape image. Besides, it contains an
encoder to learn shared content features and a decoder to construct video
frames with corresponding motion. Specifically, the \emph{motion stream}
introduces multiple \emph{adaptive instance normalization} (AdaIN) layers to
integrate multi-level motion information for controlling the object motion. In
the testing stage, videos with the same content but various motion information
can be generated by different \emph{normalized motion vectors} based on only
one input image. Also, we propose a high-resolution scenic time-lapse video
dataset, named Quick-Sky-Time, to evaluate different approaches, which can be
viewed as a new benchmark for high-quality scenic image and video generation
tasks. We further conduct experiments on Sky Time-lapse, Beach, and
Quick-Sky-Time datasets. The results demonstrate the superiority of our
approach over state-of-the-art methods for generating high-quality and various
dynamic videos.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SeasonDepth: Cross-Season Monocular Depth Prediction Dataset and Benchmark under Multiple Environments. (arXiv:2011.04408v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.04408">
<div class="article-summary-box-inner">
<span><p>Different environments pose a great challenge to the outdoor robust visual
perception for long-term autonomous driving and the generalization of
learning-based algorithms on different environmental effects is still an open
problem. Although monocular depth prediction has been well studied recently,
there is few work focusing on the robust learning-based depth prediction across
different environments, e.g. changing illumination and seasons, owing to the
lack of such a multi-environment real-world dataset and benchmark. To this end,
the first cross-season monocular depth prediction dataset and benchmark
SeasonDepth is built based on CMU Visual Localization dataset. To benchmark the
depth estimation performance under different environments, we investigate
representative and recent state-of-the-art open-source supervised,
self-supervised and domain adaptation depth prediction methods from KITTI
benchmark using several newly-formulated metrics. Through extensive
experimental evaluation on the proposed dataset, the influence of multiple
environments on performance and robustness is analyzed qualitatively and
quantitatively, showing that the long-term monocular depth prediction is still
challenging even with fine-tuning. We further give promising avenues that
self-supervised training and stereo geometry constraint help to enhance the
robustness to changing environments. The dataset is available on
https://seasondepth.github.io, and benchmark toolkit is available on
https://github.com/SeasonDepth/SeasonDepth.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MobileSal: Extremely Efficient RGB-D Salient Object Detection. (arXiv:2012.13095v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.13095">
<div class="article-summary-box-inner">
<span><p>The high computational cost of neural networks has prevented recent successes
in RGB-D salient object detection (SOD) from benefiting real-world
applications. Hence, this paper introduces a novel network, MobileSal, which
focuses on efficient RGB-D SOD using mobile networks for deep feature
extraction. However, mobile networks are less powerful in feature
representation than cumbersome networks. To this end, we observe that the depth
information of color images can strengthen the feature representation related
to SOD if leveraged properly. Therefore, we propose an implicit depth
restoration (IDR) technique to strengthen the mobile networks' feature
representation capability for RGB-D SOD. IDR is only adopted in the training
phase and is omitted during testing, so it is computationally free. Besides, we
propose compact pyramid refinement (CPR) for efficient multi-level feature
aggregation to derive salient objects with clear boundaries. With IDR and CPR
incorporated, MobileSal performs favorably against state-of-the-art methods on
six challenging RGB-D SOD datasets with much faster speed (450fps for the input
size of 320 $\times$ 320) and fewer parameters (6.5M). The code is released at
https://mmcheng.net/mobilesal.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">JigsawGAN: Auxiliary Learning for Solving Jigsaw Puzzles with Generative Adversarial Networks. (arXiv:2101.07555v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.07555">
<div class="article-summary-box-inner">
<span><p>The paper proposes a solution based on Generative Adversarial Network (GAN)
for solving jigsaw puzzles. The problem assumes that an image is divided into
equal square pieces, and asks to recover the image according to information
provided by the pieces. Conventional jigsaw puzzle solvers often determine the
relationships based on the boundaries of pieces, which ignore the important
semantic information. In this paper, we propose JigsawGAN, a GAN-based
auxiliary learning method for solving jigsaw puzzles with unpaired images (with
no prior knowledge of the initial images). We design a multi-task pipeline that
includes, (1) a classification branch to classify jigsaw permutations, and (2)
a GAN branch to recover features to images in correct orders. The
classification branch is constrained by the pseudo-labels generated according
to the shuffled pieces. The GAN branch concentrates on the image semantic
information, where the generator produces the natural images to fool the
discriminator, while the discriminator distinguishes whether a given image
belongs to the synthesized or the real target domain. These two branches are
connected by a flow-based warp module that is applied to warp features to
correct the order according to the classification results. The proposed method
can solve jigsaw puzzles more efficiently by utilizing both semantic
information and boundary information simultaneously. Qualitative and
quantitative comparisons against several representative jigsaw puzzle solvers
demonstrate the superiority of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spectral decoupling allows training transferable neural networks in medical imaging. (arXiv:2103.17171v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.17171">
<div class="article-summary-box-inner">
<span><p>Many current neural networks for medical imaging generalise poorly to data
unseen during training. Such behaviour can be caused by networks overfitting
easy-to-learn, or statistically dominant, features while disregarding other
potentially informative features. For example, indistinguishable differences in
the sharpness of the images from two different scanners can degrade the
performance of the network significantly. All neural networks intended for
clinical practice need to be robust to variation in data caused by differences
in imaging equipment, sample preparation and patient populations.
</p>
<p>To address these challenges, we evaluate the utility of spectral decoupling
as an implicit bias mitigation method. Spectral decoupling encourages the
neural network to learn more features by simply regularising the networks'
unnormalised prediction scores with an L2 penalty, thus having no added
computational costs.
</p>
<p>We show that spectral decoupling allows training neural networks on datasets
with strong spurious correlations and increases networks' robustness for data
distribution shifts. To validate our findings, we train networks with and
without spectral decoupling to detect prostate cancer tissue slides and
COVID-19 in chest radiographs. Networks trained with spectral decoupling
achieve up to 9.5 percent point higher performance on external datasets.
</p>
<p>Our results show that spectral decoupling helps with generalisation issues
associated with neural networks, and can be used to complement or replace
computationally expensive explicit bias mitigation methods, such as stain
normalization in histological images. We recommend using spectral decoupling as
an implicit bias mitigation method in any neural network intended for clinical
use.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Defending Against Image Corruptions Through Adversarial Augmentations. (arXiv:2104.01086v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01086">
<div class="article-summary-box-inner">
<span><p>Modern neural networks excel at image classification, yet they remain
vulnerable to common image corruptions such as blur, speckle noise or fog.
Recent methods that focus on this problem, such as AugMix and DeepAugment,
introduce defenses that operate in expectation over a distribution of image
corruptions. In contrast, the literature on $\ell_p$-norm bounded perturbations
focuses on defenses against worst-case corruptions. In this work, we reconcile
both approaches by proposing AdversarialAugment, a technique which optimizes
the parameters of image-to-image models to generate adversarially corrupted
augmented images. We theoretically motivate our method and give sufficient
conditions for the consistency of its idealized version as well as that of
DeepAugment. Our classifiers improve upon the state-of-the-art on common image
corruption benchmarks conducted in expectation on CIFAR-10-C and improve
worst-case performance against $\ell_p$-norm bounded perturbations on both
CIFAR-10 and ImageNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Enabling Meta-Learning from Target Models. (arXiv:2104.03736v5 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.03736">
<div class="article-summary-box-inner">
<span><p>Meta-learning can extract an inductive bias from previous learning experience
and assist the training of new tasks. It is often realized through optimizing a
meta-model with the evaluation loss of task-specific solvers. Most existing
algorithms sample non-overlapping $\mathit{support}$ sets and $\mathit{query}$
sets to train and evaluate the solvers respectively due to simplicity
($\mathcal{S}$/$\mathcal{Q}$ protocol). Different from
$\mathcal{S}$/$\mathcal{Q}$ protocol, we can also evaluate a task-specific
solver by comparing it to a target model $\mathcal{T}$, which is the optimal
model for this task or a model that behaves well enough on this task
($\mathcal{S}$/$\mathcal{T}$ protocol). Although being short of research,
$\mathcal{S}$/$\mathcal{T}$ protocol has unique advantages such as offering
more informative supervision, but it is computationally expensive. This paper
looks into this special evaluation method and takes a step towards putting it
into practice. We find that with a small ratio of tasks armed with target
models, classic meta-learning algorithms can be improved a lot without
consuming many resources. We empirically verify the effectiveness of
$\mathcal{S}$/$\mathcal{T}$ protocol in a typical application of meta-learning,
$\mathit{i.e.}$, few-shot learning. In detail, after constructing target models
by fine-tuning the pre-trained network on those hard tasks, we match the
task-specific solvers and target models via knowledge distillation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta Faster R-CNN: Towards Accurate Few-Shot Object Detection with Attentive Feature Alignment. (arXiv:2104.07719v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07719">
<div class="article-summary-box-inner">
<span><p>Few-shot object detection (FSOD) aims to detect objects using only a few
examples. How to adapt state-of-the-art object detectors to the few-shot domain
remains challenging. Object proposal is a key ingredient in modern object
detectors. However, the quality of proposals generated for few-shot classes
using existing methods is far worse than that of many-shot classes, e.g.,
missing boxes for few-shot classes due to misclassification or inaccurate
spatial locations with respect to true objects. To address the noisy proposal
problem, we propose a novel meta-learning based FSOD model by jointly
optimizing the few-shot proposal generation and fine-grained few-shot proposal
classification. To improve proposal generation for few-shot classes, we propose
to learn a lightweight metric-learning based prototype matching network,
instead of the conventional simple linear object/nonobject classifier, e.g.,
used in RPN. Our non-linear classifier with the feature fusion network could
improve the discriminative prototype matching and the proposal recall for
few-shot classes. To improve the fine-grained few-shot proposal classification,
we propose a novel attentive feature alignment method to address the spatial
misalignment between the noisy proposals and few-shot classes, thus improving
the performance of few-shot object detection. Meanwhile we learn a separate
Faster R-CNN detection head for many-shot base classes and show strong
performance of maintaining base-classes knowledge. Our model achieves
state-of-the-art performance on multiple FSOD benchmarks over most of the shots
and metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attention and Prediction Guided Motion Detection for Low-Contrast Small Moving Targets. (arXiv:2104.13018v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.13018">
<div class="article-summary-box-inner">
<span><p>Small target motion detection within complex natural environments is an
extremely challenging task for autonomous robots. Surprisingly, the visual
systems of insects have evolved to be highly efficient in detecting mates and
tracking prey, even though targets occupy as small as a few degrees of their
visual fields. The excellent sensitivity to small target motion relies on a
class of specialized neurons called small target motion detectors (STMDs).
However, existing STMD-based models are heavily dependent on visual contrast
and perform poorly in complex natural environments where small targets
generally exhibit extremely low contrast against neighbouring backgrounds. In
this paper, we develop an attention and prediction guided visual system to
overcome this limitation. The developed visual system comprises three main
subsystems, namely, an attention module, an STMD-based neural network, and a
prediction module. The attention module searches for potential small targets in
the predicted areas of the input image and enhances their contrast against
complex background. The STMD-based neural network receives the
contrast-enhanced image and discriminates small moving targets from background
false positives. The prediction module foresees future positions of the
detected targets and generates a prediction map for the attention module. The
three subsystems are connected in a recurrent architecture allowing information
to be processed sequentially to activate specific areas for small target
detection. Extensive experiments on synthetic and real-world datasets
demonstrate the effectiveness and superiority of the proposed visual system for
detecting small, low-contrast moving targets against complex natural
environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Momentum Contrastive Voxel-wise Representation Learning for Semi-supervised Volumetric Medical Image Segmentation. (arXiv:2105.07059v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.07059">
<div class="article-summary-box-inner">
<span><p>Automated segmentation in medical image analysis is a challenging task that
requires a large amount of manually labeled data. However, manually annotating
medical data is often laborious, and most existing learning-based approaches
fail to accurately delineate object boundaries without effective geometric
constraints. Contrastive learning, a sub-area of self-supervised learning, has
recently been noted as a promising direction in multiple application fields. In
this work, we present a novel Contrastive Voxel-wise Representation
Distillation (CVRD) method with geometric constraints to learn global-local
visual representations for volumetric medical image segmentation with limited
annotations. Our framework can effectively learn global and local features by
capturing 3D spatial context and rich anatomical information. Specifically, we
introduce a voxel-to-volume contrastive algorithm to learn global information
from 3D images, and propose to perform local voxel-to-voxel distillation to
explicitly make use of local cues in the embedding space. Moreover, we
integrate an elastic interaction-based active contour model as a geometric
regularization term to enable fast and reliable object delineations in an
end-to-end learning manner. Results on the Atrial Segmentation Challenge
dataset demonstrate superiority of our proposed scheme, especially in a setting
with a very limited number of annotated data. The code will be available at
https://github.com/charlesyou999648/CVRD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ACNet: Mask-Aware Attention with Dynamic Context Enhancement for Robust Acne Detection. (arXiv:2105.14891v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14891">
<div class="article-summary-box-inner">
<span><p>Computer-aided diagnosis has recently received attention for its advantage of
low cost and time efficiency. Although deep learning played a major role in the
recent success of acne detection, there are still several challenges such as
color shift by inconsistent illumination, variation in scales, and high density
distribution. To address these problems, we propose an acne detection network
which consists of three components, specifically: Composite Feature Refinement,
Dynamic Context Enhancement, and Mask-Aware Multi-Attention. First, Composite
Feature Refinement integrates semantic information and fine details to enrich
feature representation, which mitigates the adverse impact of imbalanced
illumination. Then, Dynamic Context Enhancement controls different receptive
fields of multi-scale features for context enhancement to handle scale
variation. Finally, Mask-Aware Multi-Attention detects densely arranged and
small acne by suppressing uninformative regions and highlighting probable acne
regions. Experiments are performed on acne image dataset ACNE04 and natural
image dataset PASCAL VOC 2007. We demonstrate how our method achieves the
state-of-the-art result on ACNE04 and competitive performance with previous
state-of-the-art methods on the PASCAL VOC 2007.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analogous to Evolutionary Algorithm: Designing a Unified Sequence Model. (arXiv:2105.15089v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.15089">
<div class="article-summary-box-inner">
<span><p>Inspired by biological evolution, we explain the rationality of Vision
Transformer by analogy with the proven practical Evolutionary Algorithm (EA)
and derive that both of them have consistent mathematical representation.
Analogous to the dynamic local population in EA, we improve the existing
transformer structure and propose a more efficient EAT model, and design
task-related heads to deal with different tasks more flexibly. Moreover, we
introduce the spatial-filling curve into the current vision transformer to
sequence image data into a uniform sequential format. Thus we can design a
unified EAT framework to address multi-modal tasks, separating the network
architecture from the data format adaptation. Our approach achieves
state-of-the-art results on the ImageNet classification task compared with
recent vision transformer works while having smaller parameters and greater
throughput. We further conduct multi-modal tasks to demonstrate the superiority
of the unified EAT, e.g., Text-Based Image Retrieval, and our approach improves
the rank-1 by +3.7 points over the baseline on the CSS dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TSI: Temporal Saliency Integration for Video Action Recognition. (arXiv:2106.01088v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01088">
<div class="article-summary-box-inner">
<span><p>Efficient spatiotemporal modeling is an important yet challenging problem for
video action recognition. Existing state-of-the-art methods exploit neighboring
feature differences to obtain motion clues for short-term temporal modeling
with a simple convolution. However, only one local convolution is incapable of
handling various kinds of actions because of the limited receptive field.
Besides, action-irrelated noises brought by camera movement will also harm the
quality of extracted motion features. In this paper, we propose a Temporal
Saliency Integration (TSI) block, which mainly contains a Salient Motion
Excitation (SME) module and a Cross-perception Temporal Integration (CTI)
module. Specifically, SME aims to highlight the motion-sensitive area through
spatial-level local-global motion modeling, where the saliency alignment and
pyramidal motion modeling are conducted successively between adjacent frames to
capture motion dynamics with fewer noises caused by misaligned background. CTI
is designed to perform multi-perception temporal modeling through a group of
separate 1D convolutions respectively. Meanwhile, temporal interactions across
different perceptions are integrated with the attention mechanism. Through
these two modules, long short-term temporal relationships can be encoded
efficiently by introducing limited additional parameters. Extensive experiments
are conducted on several popular benchmarks (i.e., Something-Something V1 &amp; V2,
Kinetics-400, UCF-101, and HMDB-51), which demonstrate the effectiveness of our
proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RegionViT: Regional-to-Local Attention for Vision Transformers. (arXiv:2106.02689v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02689">
<div class="article-summary-box-inner">
<span><p>Vision transformer (ViT) has recently shown its strong capability in
achieving comparable results to convolutional neural networks (CNNs) on image
classification. However, vanilla ViT simply inherits the same architecture from
the natural language processing directly, which is often not optimized for
vision applications. Motivated by this, in this paper, we propose a new
architecture that adopts the pyramid structure and employ a novel
regional-to-local attention rather than global self-attention in vision
transformers. More specifically, our model first generates regional tokens and
local tokens from an image with different patch sizes, where each regional
token is associated with a set of local tokens based on the spatial location.
The regional-to-local attention includes two steps: first, the regional
self-attention extract global information among all regional tokens and then
the local self-attention exchanges the information among one regional token and
the associated local tokens via self-attention. Therefore, even though local
self-attention confines the scope in a local region but it can still receive
global information. Extensive experiments on four vision tasks, including image
classification, object and keypoint detection, semantics segmentation and
action recognition, show that our approach outperforms or is on par with
state-of-the-art ViT variants including many concurrent works. Our source codes
and models are available at https://github.com/ibm/regionvit.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SUPER-ADAM: Faster and Universal Framework of Adaptive Gradients. (arXiv:2106.08208v7 [math.OC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.08208">
<div class="article-summary-box-inner">
<span><p>Adaptive gradient methods have shown excellent performances for solving many
machine learning problems. Although multiple adaptive gradient methods were
recently studied, they mainly focus on either empirical or theoretical aspects
and also only work for specific problems by using some specific adaptive
learning rates. Thus, it is desired to design a universal framework for
practical algorithms of adaptive gradients with theoretical guarantee to solve
general problems. To fill this gap, we propose a faster and universal framework
of adaptive gradients (i.e., SUPER-ADAM) by introducing a universal adaptive
matrix that includes most existing adaptive gradient forms. Moreover, our
framework can flexibly integrate the momentum and variance reduced techniques.
In particular, our novel framework provides the convergence analysis support
for adaptive gradient methods under the nonconvex setting. In theoretical
analysis, we prove that our SUPER-ADAM algorithm can achieve the best known
gradient (i.e., stochastic first-order oracle (SFO)) complexity of
$\tilde{O}(\epsilon^{-3})$ for finding an $\epsilon$-stationary point of
nonconvex optimization, which matches the lower bound for stochastic smooth
nonconvex optimization. In numerical experiments, we employ various deep
learning tasks to validate that our algorithm consistently outperforms the
existing adaptive algorithms. Code is available at
https://github.com/LIJUNYI95/SuperAdam
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cascaded Diffusion Models for High Fidelity Image Generation. (arXiv:2106.15282v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.15282">
<div class="article-summary-box-inner">
<span><p>We show that cascaded diffusion models are capable of generating high
fidelity images on the class-conditional ImageNet generation benchmark, without
any assistance from auxiliary image classifiers to boost sample quality. A
cascaded diffusion model comprises a pipeline of multiple diffusion models that
generate images of increasing resolution, beginning with a standard diffusion
model at the lowest resolution, followed by one or more super-resolution
diffusion models that successively upsample the image and add higher resolution
details. We find that the sample quality of a cascading pipeline relies
crucially on conditioning augmentation, our proposed method of data
augmentation of the lower resolution conditioning inputs to the
super-resolution models. Our experiments show that conditioning augmentation
prevents compounding error during sampling in a cascaded model, helping us to
train cascading pipelines achieving FID scores of 1.48 at 64x64, 3.52 at
128x128 and 4.88 at 256x256 resolutions, outperforming BigGAN-deep, and
classification accuracy scores of 63.02% (top-1) and 84.06% (top-5) at 256x256,
outperforming VQ-VAE-2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Semantic Segmentation using Psychometric Learning. (arXiv:2107.03212v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.03212">
<div class="article-summary-box-inner">
<span><p>Assigning meaning to parts of image data is the goal of semantic image
segmentation. Machine learning methods, specifically supervised learning is
commonly used in a variety of tasks formulated as semantic segmentation. One of
the major challenges in the supervised learning approaches is expressing and
collecting the rich knowledge that experts have with respect to the meaning
present in the image data. Towards this, typically a fixed set of labels is
specified and experts are tasked with annotating the pixels, patches or
segments in the images with the given labels. In general, however, the set of
classes does not fully capture the rich semantic information present in the
images. For example, in medical imaging such as histology images, the different
parts of cells could be grouped and sub-grouped based on the expertise of the
pathologist.
</p>
<p>To achieve such a precise semantic representation of the concepts in the
image, we need access to the full depth of knowledge of the annotator. In this
work, we develop a novel approach to collect segmentation annotations from
experts based on psychometric testing. Our method consists of the psychometric
testing procedure, active query selection, query enhancement, and a deep metric
learning model to achieve a patch-level image embedding that allows for
semantic segmentation of images. We show the merits of our method with
evaluation on the synthetically generated image, aerial image and histology
image.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Noise-Resistant Deep Metric Learning with Probabilistic Instance Filtering. (arXiv:2108.01431v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.01431">
<div class="article-summary-box-inner">
<span><p>Noisy labels are commonly found in real-world data, which cause performance
degradation of deep neural networks. Cleaning data manually is labour-intensive
and time-consuming. Previous research mostly focuses on enhancing
classification models against noisy labels, while the robustness of deep metric
learning (DML) against noisy labels remains less well-explored. In this paper,
we bridge this important gap by proposing Probabilistic Ranking-based Instance
Selection with Memory (PRISM) approach for DML. PRISM calculates the
probability of a label being clean, and filters out potentially noisy samples.
Specifically, we propose a novel method, namely the von Mises-Fisher
Distribution Similarity (vMF-Sim), to calculate this probability by estimating
a von Mises-Fisher (vMF) distribution for each data class. Compared with the
existing average similarity method (AvgSim), vMF-Sim considers the variance of
each class in addition to the average similarity. With such a design, the
proposed approach can deal with challenging DML situations in which the
majority of the samples are noisy. Extensive experiments on both synthetic and
real-world noisy dataset show that the proposed approach achieves up to 8.37%
higher Precision@1 compared with the best performing state-of-the-art baseline
approaches, within reasonable training time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PnP-3D: A Plug-and-Play for 3D Point Clouds. (arXiv:2108.07378v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07378">
<div class="article-summary-box-inner">
<span><p>With the help of the deep learning paradigm, many point cloud networks have
been invented for visual analysis. However, there is great potential for
development of these networks since the given information of point cloud data
has not been fully exploited. To improve the effectiveness of existing networks
in analyzing point cloud data, we propose a plug-and-play module, PnP-3D,
aiming to refine the fundamental point cloud feature representations by
involving more local context and global bilinear response from explicit 3D
space and implicit feature space. To thoroughly evaluate our approach, we
conduct experiments on three standard point cloud analysis tasks, including
classification, semantic segmentation, and object detection, where we select
three state-of-the-art networks from each task for evaluation. Serving as a
plug-and-play module, PnP-3D can significantly boost the performances of
established networks. In addition to achieving state-of-the-art results on four
widely used point cloud benchmarks, we present comprehensive ablation studies
and visualizations to demonstrate our approach's advantages. The code will be
available at https://github.com/ShiQiu0419/pnp-3d.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Representing Shape Collections with Alignment-Aware Linear Models. (arXiv:2109.01605v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01605">
<div class="article-summary-box-inner">
<span><p>In this paper, we revisit the classical representation of 3D point clouds as
linear shape models. Our key insight is to leverage deep learning to represent
a collection of shapes as affine transformations of low-dimensional linear
shape models. Each linear model is characterized by a shape prototype, a
low-dimensional shape basis and two neural networks. The networks take as input
a point cloud and predict the coordinates of a shape in the linear basis and
the affine transformation which best approximate the input. Both linear models
and neural networks are learned end-to-end using a single reconstruction loss.
The main advantage of our approach is that, in contrast to many recent deep
approaches which learn feature-based complex shape representations, our model
is explicit and every operation occurs in 3D space. As a result, our linear
shape models can be easily visualized and annotated, and failure cases can be
visually understood. While our main goal is to introduce a compact and
interpretable representation of shape collections, we show it leads to state of
the art results for few-shot segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wasserstein Patch Prior for Image Superresolution. (arXiv:2109.12880v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12880">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce a Wasserstein patch prior for superresolution of
two- and three-dimensional images. Here, we assume that we have given
(additionally to the low resolution observation) a reference image which has a
similar patch distribution as the ground truth of the reconstruction. This
assumption is e.g. fulfilled when working with texture images or material data.
Then, the proposed regularizer penalizes the $W_2$-distance of the patch
distribution of the reconstruction to the patch distribution of some reference
image at different scales. We demonstrate the performance of the proposed
regularizer by two- and three-dimensional numerical examples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Fractal Pre-training. (arXiv:2110.03091v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03091">
<div class="article-summary-box-inner">
<span><p>The deep neural networks used in modern computer vision systems require
enormous image datasets to train them. These carefully-curated datasets
typically have a million or more images, across a thousand or more distinct
categories. The process of creating and curating such a dataset is a monumental
undertaking, demanding extensive effort and labelling expense and necessitating
careful navigation of technical and social issues such as label accuracy,
copyright ownership, and content bias.
</p>
<p>What if we had a way to harness the power of large image datasets but with
few or none of the major issues and concerns currently faced? This paper
extends the recent work of Kataoka et. al. (2020), proposing an improved
pre-training dataset based on dynamically-generated fractal images. Challenging
issues with large-scale image datasets become points of elegance for fractal
pre-training: perfect label accuracy at zero cost; no need to store/transmit
large image archives; no privacy/demographic bias/concerns of inappropriate
content, as no humans are pictured; limitless supply and diversity of images;
and the images are free/open-source. Perhaps surprisingly, avoiding these
difficulties imposes only a small penalty in performance. Leveraging a
newly-proposed pre-training task -- multi-instance prediction -- our
experiments demonstrate that fine-tuning a network pre-trained using fractals
attains 92.7-98.1% of the accuracy of an ImageNet pre-trained network.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optical Flow Estimation for Spiking Camera. (arXiv:2110.03916v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03916">
<div class="article-summary-box-inner">
<span><p>As a bio-inspired sensor with high temporal resolution, the spiking camera
has an enormous potential in real applications, especially for motion
estimation in high-speed scenes. However, frame-based and event-based methods
are not well suited to spike streams from the spiking camera due to the
different data modalities. To this end, we present, SCFlow, a tailored deep
learning pipeline to estimate optical flow in high-speed scenes from spike
streams. Importantly, a novel input representation is introduced which can
adaptively remove the motion blur in spike streams according to the prior
motion. Further, for training SCFlow, we synthesize two sets of optical flow
data for the spiking camera, SPIkingly Flying Things and Photo-realistic
High-speed Motion, denoted as SPIFT and PHM respectively, corresponding to
random high-speed and well-designed scenes. Experimental results show that the
SCFlow can predict optical flow from spike streams in different high-speed
scenes. Moreover, SCFlow shows promising generalization on \real spike streams.
All codes and constructed datasets will be released after publication.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parametric Variational Linear Units (PVLUs) in Deep Convolutional Networks. (arXiv:2110.12246v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12246">
<div class="article-summary-box-inner">
<span><p>The Rectified Linear Unit is currently a state-of-the-art activation function
in deep convolutional neural networks. To combat ReLU's dying neuron problem,
we propose the Parametric Variational Linear Unit (PVLU), which adds a
sinusoidal function with trainable coefficients to ReLU. Along with introducing
nonlinearity and non-zero gradients across the entire real domain, PVLU acts as
a mechanism of fine-tuning when implemented in the context of transfer
learning. On a simple, non-transfer sequential CNN, PVLU substitution allowed
for relative error decreases of 16.3% and 11.3% (without and with data
augmentation) on CIFAR-100. PVLU is also tested on transfer learning models.
The VGG-16 and VGG-19 models experience relative error reductions of 9.5% and
10.7% on CIFAR-10, respectively, after the substitution of ReLU with PVLU. When
training on Gaussian-filtered CIFAR-10 images, similar improvements are noted
for the VGG models. Most notably, fine-tuning using PVLU allows for relative
error reductions up to and exceeding 10% for near state-of-the-art residual
neural network architectures on the CIFAR datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CHIP: CHannel Independence-based Pruning for Compact Neural Networks. (arXiv:2110.13981v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.13981">
<div class="article-summary-box-inner">
<span><p>Filter pruning has been widely used for neural network compression because of
its enabled practical acceleration. To date, most of the existing filter
pruning works explore the importance of filters via using intra-channel
information. In this paper, starting from an inter-channel perspective, we
propose to perform efficient filter pruning using Channel Independence, a
metric that measures the correlations among different feature maps. The less
independent feature map is interpreted as containing less useful
information$/$knowledge, and hence its corresponding filter can be pruned
without affecting model capacity. We systematically investigate the
quantification metric, measuring scheme and sensitiveness$/$reliability of
channel independence in the context of filter pruning. Our evaluation results
for different models on various datasets show the superior performance of our
approach. Notably, on CIFAR-10 dataset our solution can bring $0.75\%$ and
$0.94\%$ accuracy increase over baseline ResNet-56 and ResNet-110 models,
respectively, and meanwhile the model size and FLOPs are reduced by $42.8\%$
and $47.4\%$ (for ResNet-56) and $48.3\%$ and $52.1\%$ (for ResNet-110),
respectively. On ImageNet dataset, our approach can achieve $40.8\%$ and
$44.8\%$ storage and computation reductions, respectively, with $0.15\%$
accuracy increase over the baseline ResNet-50 model. The code is available at
https://github.com/Eclipsess/CHIP_NeurIPS2021.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distilling Object Detectors with Feature Richness. (arXiv:2111.00674v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00674">
<div class="article-summary-box-inner">
<span><p>In recent years, large-scale deep models have achieved great success, but the
huge computational complexity and massive storage requirements make it a great
challenge to deploy them in resource-limited devices. As a model compression
and acceleration method, knowledge distillation effectively improves the
performance of small models by transferring the dark knowledge from the teacher
detector. However, most of the existing distillation-based detection methods
mainly imitating features near bounding boxes, which suffer from two
limitations. First, they ignore the beneficial features outside the bounding
boxes. Second, these methods imitate some features which are mistakenly
regarded as the background by the teacher detector. To address the above
issues, we propose a novel Feature-Richness Score (FRS) method to choose
important features that improve generalized detectability during distilling.
The proposed method effectively retrieves the important features outside the
bounding boxes and removes the detrimental features within the bounding boxes.
Extensive experiments show that our methods achieve excellent performance on
both anchor-based and anchor-free detectors. For example, RetinaNet with
ResNet-50 achieves 39.7% in mAP on the COCO2017 dataset, which even surpasses
the ResNet-101 based teacher detector 38.9% by 0.8%. Our implementation is
available at https://github.com/duzhixing/FRS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Role of Pre-Training in High-Resolution Remote Sensing Scene Classification. (arXiv:2111.03690v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.03690">
<div class="article-summary-box-inner">
<span><p>Due to the scarcity of labeled data, using models pre-trained on ImageNet is
a de facto standard in remote sensing scene classification. Although, recently,
several larger high resolution remote sensing (HRRS) datasets have appeared
with a goal of establishing new benchmarks, attempts at training models from
scratch on these datasets are sporadic. In this paper, we show that training
models from scratch on several newer datasets yields comparable results to
fine-tuning the models pre-trained on ImageNet. Furthermore, the
representations learned on HRRS datasets transfer to other HRRS scene
classification tasks better or at least similarly as those learned on ImageNet.
Finally, we show that in many cases the best representations are obtained by
using a second round of pre-training using in-domain data, i.e. domain-adaptive
pre-training. The source code and pre-trained models are available at
\url{https://github.com/risojevicv/RSSC-transfer}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Point Light Fields. (arXiv:2112.01473v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01473">
<div class="article-summary-box-inner">
<span><p>We introduce Neural Point Light Fields that represent scenes implicitly with
a light field living on a sparse point cloud. Combining differentiable volume
rendering with learned implicit density representations has made it possible to
synthesize photo-realistic images for novel views of small scenes. As neural
volumetric rendering methods require dense sampling of the underlying
functional scene representation, at hundreds of samples along a ray cast
through the volume, they are fundamentally limited to small scenes with the
same objects projected to hundreds of training views. Promoting sparse point
clouds to neural implicit light fields allows us to represent large scenes
effectively with only a single implicit sampling operation per ray. These point
light fields are as a function of the ray direction, and local point feature
neighborhood, allowing us to interpolate the light field conditioned training
images without dense object coverage and parallax. We assess the proposed
method for novel view synthesis on large driving scenarios, where we synthesize
realistic unseen views that existing implicit approaches fail to represent. We
validate that Neural Point Light Fields make it possible to predict videos
along unseen trajectories previously only feasible to generate by explicitly
modeling the scene.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Medical Point Transformer: Introducing Convolution to Attention Networks for Medical Point Cloud Analysis. (arXiv:2112.04863v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04863">
<div class="article-summary-box-inner">
<span><p>General point clouds have been increasingly investigated for different tasks,
and recently Transformer-based networks are proposed for point cloud analysis.
However, there are barely related works for medical point clouds, which are
important for disease detection and treatment. In this work, we propose an
attention-based model specifically for medical point clouds, namely 3D medical
point Transformer (3DMedPT), to examine the complex biological structures. By
augmenting contextual information and summarizing local responses at query, our
attention module can capture both local context and global content feature
interactions. However, the insufficient training samples of medical data may
lead to poor feature learning, so we apply position embeddings to learn
accurate local geometry and Multi-Graph Reasoning (MGR) to examine global
knowledge propagation over channel graphs to enrich feature representations.
Experiments conducted on IntrA dataset proves the superiority of 3DMedPT, where
we achieve the best classification and segmentation results. Furthermore, the
promising generalization ability of our method is validated on general 3D point
cloud benchmarks: ModelNet40 and ShapeNetPart. Code is released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CityNeRF: Building NeRF at City Scale. (arXiv:2112.05504v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05504">
<div class="article-summary-box-inner">
<span><p>Neural Radiance Field (NeRF) has achieved outstanding performance in modeling
3D objects and controlled scenes, usually under a single scale. In this work,
we make the first attempt to bring NeRF to city-scale, with views ranging from
satellite-level that captures the overview of a city, to ground-level imagery
showing complex details of an architecture. The wide span of camera distance to
the scene yields multi-scale data with different levels of detail and spatial
coverage, which casts great challenges to vanilla NeRF and biases it towards
compromised results. To address these issues, we introduce CityNeRF, a
progressive learning paradigm that grows the NeRF model and training set
synchronously. Starting from fitting distant views with a shallow base block,
as training progresses, new blocks are appended to accommodate the emerging
details in the increasingly closer views. The strategy effectively activates
high-frequency channels in the positional encoding and unfolds more complex
details as the training proceeds. We demonstrate the superiority of CityNeRF in
modeling diverse city-scale scenes with drastically varying views, and its
support for rendering views in different levels of detail.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tracking and Long-Term Identification Using Non-Visual Markers. (arXiv:2112.06809v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06809">
<div class="article-summary-box-inner">
<span><p>Our objective is to track and identify mice in a cluttered home-cage
environment, as a precursor to automated behaviour recognition for biological
research. This is a very challenging problem due to (i) the lack of
distinguishing visual features for each mouse, and (ii) the close confines of
the scene with constant occlusion, making standard visual tracking approaches
unusable. However, a coarse estimate of each mouse's location is available from
a unique RFID implant, so there is the potential to optimally combine
information from (weak) tracking with coarse information on identity. To
achieve our objective, we make the following key contributions: (a) the
formulation of the identification problem as an assignment problem (solved
using Integer Linear Programming), and (b) a novel probabilistic model of the
affinity between tracklets and RFID data. The latter is a crucial part of the
model, as it provides a principled probabilistic treatment of object detections
given coarse localisation. Our approach achieves 77% accuracy on this
identification problem, and is able to reject spurious detections when the
animals are hidden.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Event-guided Deblurring of Unknown Exposure Time Videos. (arXiv:2112.06988v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06988">
<div class="article-summary-box-inner">
<span><p>Video deblurring is a highly ill-posed problem due to the loss of motion
information in the blur degradation process. Since event cameras can capture
apparent motion with a high temporal resolution, several attempts have explored
the potential of events for guiding video deblurring. These methods generally
assume that the exposure time is the same as the reciprocal of the video frame
rate. However,this is not true in real situations, and the exposure time might
be unknown and dynamically varies depending on the video shooting
environment(e.g., illumination condition). In this paper, we address the
event-guided video deblurring assuming dynamically variable unknown exposure
time of the frame-based camera. To this end, we first derive a new formulation
for event-guided video deblurring by considering the exposure and readout time
in the video frame acquisition process. We then propose a novel end-toend
learning framework for event-guided video deblurring. In particular, we design
a novel Exposure Time-based Event Selection(ETES) module to selectively use
event features by estimating the cross-modal correlation between the features
from blurred frames and the events. Moreover, we propose a feature fusion
module to effectively fuse the selected features from events and blur frames.
We conduct extensive experiments on various datasets and demonstrate that our
method achieves state-of-the-art performance. Our project code and pretrained
models will be available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Body-Aware 3D Shape Generative Models. (arXiv:2112.07022v2 [cs.GR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07022">
<div class="article-summary-box-inner">
<span><p>The shape of many objects in the built environment is dictated by their
relationships to the human body: how will a person interact with this object?
Existing data-driven generative models of 3D shapes produce plausible objects
but do not reason about the relationship of those objects to the human body. In
this paper, we learn body-aware generative models of 3D shapes. Specifically,
we train generative models of chairs, an ubiquitous shape category, which can
be conditioned on a given body shape or sitting pose. The
body-shape-conditioned models produce chairs which will be comfortable for a
person with the given body shape; the pose-conditioned models produce chairs
which accommodate the given sitting pose. To train these models, we define a
"sitting pose matching" metric and a novel "sitting comfort" metric.
Calculating these metrics requires an expensive optimization to sit the body
into the chair, which is too slow to be used as a loss function for training a
generative model. Thus, we train neural networks to efficiently approximate
these metrics. We use our approach to train three body-aware generative shape
models: a structured part-based generator, a point cloud generator, and an
implicit surface generator. In all cases, our approach produces models which
adapt their output chair shapes to input human body specifications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Temporal Transformer Networks with Self-Supervision for Action Recognition. (arXiv:2112.07338v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07338">
<div class="article-summary-box-inner">
<span><p>In recent years, 2D Convolutional Networks-based video action recognition has
encouragingly gained wide popularity; However, constrained by the lack of
long-range non-linear temporal relation modeling and reverse motion information
modeling, the performance of existing models is, therefore, undercut seriously.
To address this urgent problem, we introduce a startling Temporal Transformer
Network with Self-supervision (TTSN). Our high-performance TTSN mainly consists
of a temporal transformer module and a temporal sequence self-supervision
module. Concisely speaking, we utilize the efficient temporal transformer
module to model the non-linear temporal dependencies among non-local frames,
which significantly enhances complex motion feature representations. The
temporal sequence self-supervision module we employ unprecedentedly adopts the
streamlined strategy of "random batch random channel" to reverse the sequence
of video frames, allowing robust extractions of motion information
representation from inversed temporal dimensions and improving the
generalization capability of the model. Extensive experiments on three widely
used datasets (HMDB51, UCF101, and Something-something V1) have conclusively
demonstrated that our proposed TTSN is promising as it successfully achieves
state-of-the-art performance for action recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Homography Decomposition Networks for Planar Object Tracking. (arXiv:2112.07909v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07909">
<div class="article-summary-box-inner">
<span><p>Planar object tracking plays an important role in AI applications, such as
robotics, visual servoing, and visual SLAM. Although the previous planar
trackers work well in most scenarios, it is still a challenging task due to the
rapid motion and large transformation between two consecutive frames. The
essential reason behind this problem is that the condition number of such a
non-linear system changes unstably when the searching range of the homography
parameter space becomes larger. To this end, we propose a novel Homography
Decomposition Networks~(HDN) approach that drastically reduces and stabilizes
the condition number by decomposing the homography transformation into two
groups. Specifically, a similarity transformation estimator is designed to
predict the first group robustly by a deep convolution equivariant network. By
taking advantage of the scale and rotation estimation with high confidence, a
residual transformation is estimated by a simple regression model. Furthermore,
the proposed end-to-end network is trained in a semi-supervised fashion.
Extensive experiments show that our proposed approach outperforms the
state-of-the-art planar tracking methods at a large margin on the challenging
POT, UCSB and POIC datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GRAM: Generative Radiance Manifolds for 3D-Aware Image Generation. (arXiv:2112.08867v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08867">
<div class="article-summary-box-inner">
<span><p>3D-aware image generative modeling aims to generate 3D-consistent images with
explicitly controllable camera poses. Recent works have shown promising results
by training neural radiance field (NeRF) generators on unstructured 2D images,
but still can not generate highly-realistic images with fine details. A
critical reason is that the high memory and computation cost of volumetric
representation learning greatly restricts the number of point samples for
radiance integration during training. Deficient sampling not only limits the
expressive power of the generator to handle fine details but also impedes
effective GAN training due to the noise caused by unstable Monte Carlo
sampling. We propose a novel approach that regulates point sampling and
radiance field learning on 2D manifolds, embodied as a set of learned implicit
surfaces in the 3D volume. For each viewing ray, we calculate ray-surface
intersections and accumulate their radiance generated by the network. By
training and rendering such radiance manifolds, our generator can produce high
quality images with realistic fine details and strong visual 3D consistency.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-12-20 07:46:34.896556155 UTC">2021-12-20 07:46:34 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>