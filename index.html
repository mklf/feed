<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-27T01:30:00Z">09-27</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">CSAGN: Conversational Structure Aware Graph Network for Conversational Semantic Role Labeling. (arXiv:2109.11541v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11541">
<div class="article-summary-box-inner">
<span><p>Conversational semantic role labeling (CSRL) is believed to be a crucial step
towards dialogue understanding. However, it remains a major challenge for
existing CSRL parser to handle conversational structural information. In this
paper, we present a simple and effective architecture for CSRL which aims to
address this problem. Our model is based on a conversational structure-aware
graph network which explicitly encodes the speaker dependent information. We
also propose a multi-task learning method to further improve the model.
Experimental results on benchmark datasets show that our model with our
proposed training objectives significantly outperforms previous baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Document Automation Architectures and Technologies: A Survey. (arXiv:2109.11603v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11603">
<div class="article-summary-box-inner">
<span><p>This paper surveys the current state of the art in document automation (DA).
The objective of DA is to reduce the manual effort during the generation of
documents by automatically integrating input from different sources and
assembling documents conforming to defined templates. There have been reviews
of commercial solutions of DA, particularly in the legal domain, but to date
there has been no comprehensive review of the academic research on DA
architectures and technologies. The current survey of DA reviews the academic
literature and provides a clearer definition and characterization of DA and its
features, identifies state-of-the-art DA architectures and technologies in
academic research, and provides ideas that can lead to new research
opportunities within the DA field in light of recent advances in artificial
intelligence and deep neural networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">iFacetSum: Coreference-based Interactive Faceted Summarization for Multi-Document Exploration. (arXiv:2109.11621v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11621">
<div class="article-summary-box-inner">
<span><p>We introduce iFacetSum, a web application for exploring topical document
sets. iFacetSum integrates interactive summarization together with faceted
search, by providing a novel faceted navigation scheme that yields abstractive
summaries for the user's selections. This approach offers both a comprehensive
overview as well as concise details regarding subtopics of choice. Fine-grained
facets are automatically produced based on cross-document coreference
pipelines, rendering generic concepts, entities and statements surfacing in the
source texts. We analyze the effectiveness of our application through
small-scale user studies, which suggest the usefulness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting the Uniform Information Density Hypothesis. (arXiv:2109.11635v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11635">
<div class="article-summary-box-inner">
<span><p>The uniform information density (UID) hypothesis posits a preference among
language users for utterances structured such that information is distributed
uniformly across a signal. While its implications on language production have
been well explored, the hypothesis potentially makes predictions about language
comprehension and linguistic acceptability as well. Further, it is unclear how
uniformity in a linguistic signal -- or lack thereof -- should be measured, and
over which linguistic unit, e.g., the sentence or language level, this
uniformity should hold. Here we investigate these facets of the UID hypothesis
using reading time and acceptability data. While our reading time results are
generally consistent with previous work, they are also consistent with a weakly
super-linear effect of surprisal, which would be compatible with UID's
predictions. For acceptability judgments, we find clearer evidence that
non-uniformity in information density is predictive of lower acceptability. We
then explore multiple operationalizations of UID, motivated by different
interpretations of the original hypothesis, and analyze the scope over which
the pressure towards uniformity is exerted. The explanatory power of a subset
of the proposed operationalizations suggests that the strongest trend may be a
regression towards a mean surprisal across the language, rather than the
phrase, sentence, or document -- a finding that supports a typical
interpretation of UID, namely that it is the byproduct of language users
maximizing the use of a (hypothetical) communication channel.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simple and Effective Zero-shot Cross-lingual Phoneme Recognition. (arXiv:2109.11680v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11680">
<div class="article-summary-box-inner">
<span><p>Recent progress in self-training, self-supervised pretraining and
unsupervised learning enabled well performing speech recognition systems
without any labeled data. However, in many cases there is labeled data
available for related languages which is not utilized by these methods. This
paper extends previous work on zero-shot cross-lingual transfer learning by
fine-tuning a multilingually pretrained wav2vec 2.0 model to transcribe unseen
languages. This is done by mapping phonemes of the training languages to the
target language using articulatory features. Experiments show that this simple
method significantly outperforms prior work which introduced task-specific
architectures and used only part of a monolingually pretrained model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detect and Perturb: Neutral Rewriting of Biased and Sensitive Text via Gradient-based Decoding. (arXiv:2109.11708v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11708">
<div class="article-summary-box-inner">
<span><p>Written language carries explicit and implicit biases that can distract from
meaningful signals. For example, letters of reference may describe male and
female candidates differently, or their writing style may indirectly reveal
demographic characteristics. At best, such biases distract from the meaningful
content of the text; at worst they can lead to unfair outcomes. We investigate
the challenge of re-generating input sentences to 'neutralize' sensitive
attributes while maintaining the semantic meaning of the original text (e.g. is
the candidate qualified?). We propose a gradient-based rewriting framework,
Detect and Perturb to Neutralize (DEPEN), that first detects sensitive
components and masks them for regeneration, then perturbs the generation model
at decoding time under a neutralizing constraint that pushes the (predicted)
distribution of sensitive attributes towards a uniform distribution. Our
experiments in two different scenarios show that DEPEN can regenerate fluent
alternatives that are neutral in the sensitive attribute while maintaining the
semantics of other attributes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AES Are Both Overstable And Oversensitive: Explaining Why And Proposing Defenses. (arXiv:2109.11728v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11728">
<div class="article-summary-box-inner">
<span><p>Deep-learning based Automatic Essay Scoring (AES) systems are being actively
used by states and language testing agencies alike to evaluate millions of
candidates for life-changing decisions ranging from college applications to
visa approvals. However, little research has been put to understand and
interpret the black-box nature of deep-learning based scoring algorithms.
Previous studies indicate that scoring models can be easily fooled. In this
paper, we explore the reason behind their surprising adversarial brittleness.
We utilize recent advances in interpretability to find the extent to which
features such as coherence, content, vocabulary, and relevance are important
for automated scoring mechanisms. We use this to investigate the
oversensitivity i.e., large change in output score with a little change in
input essay content) and overstability i.e., little change in output scores
with large changes in input essay content) of AES. Our results indicate that
autoscoring models, despite getting trained as "end-to-end" models with rich
contextual embeddings such as BERT, behave like bag-of-words models. A few
words determine the essay score without the requirement of any context making
the model largely overstable. This is in stark contrast to recent probing
studies on pre-trained representation learning models, which show that rich
linguistic features such as parts-of-speech and morphology are encoded by them.
Further, we also find that the models have learnt dataset biases, making them
oversensitive. To deal with these issues, we propose detection-based protection
models that can detect oversensitivity and overstability causing samples with
high accuracies. We find that our proposed models are able to detect unusual
attribution patterns and flag adversarial samples successfully.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DACT-BERT: Differentiable Adaptive Computation Time for an Efficient BERT Inference. (arXiv:2109.11745v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11745">
<div class="article-summary-box-inner">
<span><p>Large-scale pre-trained language models have shown remarkable results in
diverse NLP applications. Unfortunately, these performance gains have been
accompanied by a significant increase in computation time and model size,
stressing the need to develop new or complementary strategies to increase the
efficiency of these models. In this paper we propose DACT-BERT, a
differentiable adaptive computation time strategy for BERT-like models.
DACT-BERT adds an adaptive computational mechanism to BERT's regular processing
pipeline, which controls the number of Transformer blocks that need to be
executed at inference time. By doing this, the model learns to combine the most
appropriate intermediate representations for the task at hand. Our experiments
demonstrate that our approach, when compared to the baselines, excels on a
reduced computational regime and is competitive in other less restrictive ones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lacking the embedding of a word? Look it up into a traditional dictionary. (arXiv:2109.11763v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11763">
<div class="article-summary-box-inner">
<span><p>Word embeddings are powerful dictionaries, which may easily capture language
variations. However, these dictionaries fail to give sense to rare words, which
are surprisingly often covered by traditional dictionaries. In this paper, we
propose to use definitions retrieved in traditional dictionaries to produce
word embeddings for rare words. For this purpose, we introduce two methods:
Definition Neural Network (DefiNNet) and Define BERT (DefBERT). In our
experiments, DefiNNet and DefBERT significantly outperform state-of-the-art as
well as baseline methods devised for producing embeddings of unknown words. In
fact, DefiNNet significantly outperforms FastText, which implements a method
for the same task-based on n-grams, and DefBERT significantly outperforms the
BERT method for OOV words. Then, definitions in traditional dictionaries are
useful to build word embeddings for rare words.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dense Contrastive Visual-Linguistic Pretraining. (arXiv:2109.11778v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11778">
<div class="article-summary-box-inner">
<span><p>Inspired by the success of BERT, several multimodal representation learning
approaches have been proposed that jointly represent image and text. These
approaches achieve superior performance by capturing high-level semantic
information from large-scale multimodal pretraining. In particular, LXMERT and
UNITER adopt visual region feature regression and label classification as
pretext tasks. However, they tend to suffer from the problems of noisy labels
and sparse semantic annotations, based on the visual features having been
pretrained on a crowdsourced dataset with limited and inconsistent semantic
labeling. To overcome these issues, we propose unbiased Dense Contrastive
Visual-Linguistic Pretraining (DCVLP), which replaces the region regression and
classification with cross-modality region contrastive learning that requires no
annotations. Two data augmentation strategies (Mask Perturbation and
Intra-/Inter-Adversarial Perturbation) are developed to improve the quality of
negative samples used in contrastive learning. Overall, DCVLP allows
cross-modality dense region contrastive learning in a self-supervised setting
independent of any object annotations. We compare our method against prior
visual-linguistic pretraining frameworks to validate the superiority of dense
contrastive learning on multimodal representation learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CPT: Colorful Prompt Tuning for Pre-trained Vision-Language Models. (arXiv:2109.11797v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11797">
<div class="article-summary-box-inner">
<span><p>Pre-Trained Vision-Language Models (VL-PTMs) have shown promising
capabilities in grounding natural language in image data, facilitating a broad
variety of cross-modal tasks. However, we note that there exists a significant
gap between the objective forms of model pre-training and fine-tuning,
resulting in a need for quantities of labeled data to stimulate the visual
grounding capability of VL-PTMs for downstream tasks. To address the challenge,
we present Cross-modal Prompt Tuning (CPT, alternatively, Colorful Prompt
Tuning), a novel paradigm for tuning VL-PTMs, which reformulates visual
grounding into a fill-in-the-blank problem with color-based co-referential
markers in image and text, maximally mitigating the gap. In this way, our
prompt tuning approach enables strong few-shot and even zero-shot visual
grounding capabilities of VL-PTMs. Comprehensive experimental results show that
prompt tuned VL-PTMs outperform their fine-tuned counterparts by a large margin
(e.g., 17.3% absolute accuracy improvement, and 73.8% relative standard
deviation reduction on average with one shot in RefCOCO evaluation). All the
data and code will be available to facilitate future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Does Knowledge Graph Embedding Extrapolate to Unseen Data: a Semantic Evidence View. (arXiv:2109.11800v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11800">
<div class="article-summary-box-inner">
<span><p>Knowledge Graph Embedding (KGE) aims to learn representations for entities
and relations. Most KGE models have gained great success, especially on
extrapolation scenarios. Specifically, given an unseen triple (h, r, t), a
trained model can still correctly predict t from (h, r, ?), or h from (?, r,
t), such extrapolation ability is impressive. However, most existing KGE works
focus on the design of delicate triple modeling function, which mainly tell us
how to measure the plausibility of observed triples, but we have limited
understanding of why the methods can extrapolate to unseen data, and what are
the important factors to help KGE extrapolate. Therefore in this work, we
attempt to, from a data relevant view, study KGE extrapolation of two problems:
1. How does KGE extrapolate to unseen data? 2. How to design the KGE model with
better extrapolation ability? For the problem 1, we first discuss the impact
factors for extrapolation and from relation, entity and triple level
respectively, propose three Semantic Evidences (SEs), which can be observed
from training set and provide important semantic information for extrapolation
to unseen data. Then we verify the effectiveness of SEs through extensive
experiments on several typical KGE methods, and demonstrate that SEs serve as
an important role for understanding the extrapolation ability of KGE. For the
problem 2, to make better use of the SE information for more extrapolative
knowledge representation, we propose a novel GNN-based KGE model, called
Semantic Evidence aware Graph Neural Network (SE-GNN). Finally, through
extensive experiments on FB15k-237 and WN18RR datasets, we show that SE-GNN
achieves state-of-the-art performance on Knowledge Graph Completion task and
perform a better extrapolation ability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Diversity-Enhanced and Constraints-Relaxed Augmentation for Low-Resource Classification. (arXiv:2109.11834v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11834">
<div class="article-summary-box-inner">
<span><p>Data augmentation (DA) aims to generate constrained and diversified data to
improve classifiers in Low-Resource Classification (LRC). Previous studies
mostly use a fine-tuned Language Model (LM) to strengthen the constraints but
ignore the fact that the potential of diversity could improve the effectiveness
of generated data. In LRC, strong constraints but weak diversity in DA result
in the poor generalization ability of classifiers. To address this dilemma, we
propose a {D}iversity-{E}nhanced and {C}onstraints-\{R}elaxed {A}ugmentation
(DECRA). Our DECRA has two essential components on top of a transformer-based
backbone model. 1) A k-beta augmentation, an essential component of DECRA, is
proposed to enhance the diversity in generating constrained data. It expands
the changing scope and improves the degree of complexity of the generated data.
2) A masked language model loss, instead of fine-tuning, is used as a
regularization. It relaxes constraints so that the classifier can be trained
with more scattered generated data. The combination of these two components
generates data that can reach or approach category boundaries and hence help
the classifier generalize better. We evaluate our DECRA on three public
benchmark datasets under low-resource settings. Extensive experiments
demonstrate that our DECRA outperforms state-of-the-art approaches by 3.8% in
the overall score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robustness and Sensitivity of BERT Models Predicting Alzheimer's Disease from Text. (arXiv:2109.11888v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11888">
<div class="article-summary-box-inner">
<span><p>Understanding robustness and sensitivity of BERT models predicting
Alzheimer's disease from text is important for both developing better
classification models and for understanding their capabilities and limitations.
In this paper, we analyze how a controlled amount of desired and undesired text
alterations impacts performance of BERT. We show that BERT is robust to natural
linguistic variations in text. On the other hand, we show that BERT is not
sensitive to removing clinically important information from text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Crowd Sourcing for Semantic Similarity. (arXiv:2109.11969v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11969">
<div class="article-summary-box-inner">
<span><p>Estimation of semantic similarity is crucial for a variety of natural
language processing (NLP) tasks. In the absence of a general theory of semantic
information, many papers rely on human annotators as the source of ground truth
for semantic similarity estimation. This paper investigates the ambiguities
inherent in crowd-sourced semantic labeling. It shows that annotators that
treat semantic similarity as a binary category (two sentences are either
similar or not similar and there is no middle ground) play the most important
role in the labeling. The paper offers heuristics to filter out unreliable
annotators and stimulates further discussions on human perception of semantic
similarity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Separating Retention from Extraction in the Evaluation of End-to-end Relation Extraction. (arXiv:2109.12008v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12008">
<div class="article-summary-box-inner">
<span><p>State-of-the-art NLP models can adopt shallow heuristics that limit their
generalization capability (McCoy et al., 2019). Such heuristics include lexical
overlap with the training set in Named-Entity Recognition (Taill\'e et al.,
2020) and Event or Type heuristics in Relation Extraction (Rosenman et al.,
2020). In the more realistic end-to-end RE setting, we can expect yet another
heuristic: the mere retention of training relation triples. In this paper, we
propose several experiments confirming that retention of known facts is a key
factor of performance on standard benchmarks. Furthermore, one experiment
suggests that a pipeline model able to use intermediate type representations is
less prone to over-rely on retention.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Translation of German--Lower Sorbian: Exploring Training and Novel Transfer Methods on a Low-Resource Language. (arXiv:2109.12012v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12012">
<div class="article-summary-box-inner">
<span><p>This paper describes the methods behind the systems submitted by the
University of Groningen for the WMT 2021 Unsupervised Machine Translation task
for German--Lower Sorbian (DE--DSB): a high-resource language to a low-resource
one. Our system uses a transformer encoder-decoder architecture in which we
make three changes to the standard training procedure. First, our training
focuses on two languages at a time, contrasting with a wealth of research on
multilingual systems. Second, we introduce a novel method for initializing the
vocabulary of an unseen language, achieving improvements of 3.2 BLEU for
DE$\rightarrow$DSB and 4.0 BLEU for DSB$\rightarrow$DE. Lastly, we experiment
with the order in which offline and online back-translation are used to train
an unsupervised system, finding that using online back-translation first works
better for DE$\rightarrow$DSB by 2.76 BLEU. Our submissions ranked first (tied
with another team) for DSB$\rightarrow$DE and third for DE$\rightarrow$DSB.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Indirectly Supervised English Sentence Break Prediction Using Paragraph Break Probability Estimates. (arXiv:2109.12023v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12023">
<div class="article-summary-box-inner">
<span><p>This report explores the use of paragraph break probability estimates to help
predict the location of sentence breaks in English natural language text. We
show that a sentence break predictor based almost solely on paragraph break
probability estimates can achieve high accuracy on this task. This sentence
break predictor is trained almost entirely on a large amount of naturally
occurring text without sentence break annotations, with only a small amount of
annotated data needed to tune two hyperparameters. We also show that even
better results can be achieved across in-domain and out-of-domain test data, if
paragraph break probability signals are combined with a support vector machine
classifier trained on a somewhat larger amount of sentence-break-annotated
data. Numerous related issues are addressed along the way.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Post-pretraining Representation Alignment for Cross-Lingual Question Answering. (arXiv:2109.12028v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12028">
<div class="article-summary-box-inner">
<span><p>Human knowledge is collectively encoded in the roughly 6500 languages spoken
around the world, but it is not distributed equally across languages. Hence,
for information-seeking question answering (QA) systems to adequately serve
speakers of all languages, they need to operate cross-lingually. In this work
we investigate the capabilities of multilingually pre-trained language models
on cross-lingual QA. We find that explicitly aligning the representations
across languages with a post-hoc fine-tuning step generally leads to improved
performance. We additionally investigate the effect of data size as well as the
language choice in this fine-tuning step, also releasing a dataset for
evaluating cross-lingual QA systems. Code and dataset are publicly available
here: https://github.com/ffaisal93/aligned_qa
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformers Generalize Linearly. (arXiv:2109.12036v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12036">
<div class="article-summary-box-inner">
<span><p>Natural language exhibits patterns of hierarchically governed dependencies,
in which relations between words are sensitive to syntactic structure rather
than linear ordering. While re-current network models often fail to generalize
in a hierarchically sensitive way (McCoy et al.,2020) when trained on ambiguous
data, the improvement in performance of newer Trans-former language models
(Vaswani et al., 2017)on a range of syntactic benchmarks trained on large data
sets (Goldberg, 2019; Warstadtet al., 2019) opens the question of whether these
models might exhibit hierarchical generalization in the face of impoverished
data.In this paper we examine patterns of structural generalization for
Transformer sequence-to-sequence models and find that not only do Transformers
fail to generalize hierarchically across a wide variety of grammatical mapping
tasks, but they exhibit an even stronger preference for linear generalization
than comparable recurrent networks
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Monolingual and Cross-Lingual Acceptability Judgments with the Italian CoLA corpus. (arXiv:2109.12053v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12053">
<div class="article-summary-box-inner">
<span><p>The development of automated approaches to linguistic acceptability has been
greatly fostered by the availability of the English CoLA corpus, which has also
been included in the widely used GLUE benchmark. However, this kind of research
for languages other than English, as well as the analysis of cross-lingual
approaches, has been hindered by the lack of resources with a comparable size
in other languages. We have therefore developed the ItaCoLA corpus, containing
almost 10,000 sentences with acceptability judgments, which has been created
following the same approach and the same steps as the English one. In this
paper we describe the corpus creation, we detail its content, and we present
the first experiments on this new resource. We compare in-domain and
out-of-domain classification, and perform a specific evaluation of nine
linguistic phenomena. We also present the first cross-lingual experiments,
aimed at assessing whether multilingual transformerbased approaches can benefit
from using sentences in two languages during fine-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AraT5: Text-to-Text Transformers for Arabic Language Understanding and Generation. (arXiv:2109.12068v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12068">
<div class="article-summary-box-inner">
<span><p>Transfer learning with a unified Transformer framework (T5) that converts all
language problems into a text-to-text format has recently been proposed as a
simple, yet effective, transfer learning approach. Although a multilingual
version of the T5 model (mT5) has been introduced, it is not clear how well it
can fare on non-English tasks involving diverse data. To investigate this
question, we apply mT5 on a language with a wide variety of dialects--Arabic.
For evaluation, we use an existing benchmark for Arabic language understanding
and introduce a new benchmark for Arabic language generation (ARGEN). We also
pre-train three powerful Arabic-specific text-to-text Transformer based models
and evaluate them on the two benchmarks. Our new models perform significantly
better than mT5 and exceed MARBERT, the current state-of-the-art Arabic
BERT-based model, on Arabic language understanding. The models also set new
SOTA on the generation benchmark. Our new models and are publicly released at
https://github.com/UBC-NLP/araT5 and ARLGE will be released through the same
repository.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SD-QA: Spoken Dialectal Question Answering for the Real World. (arXiv:2109.12072v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12072">
<div class="article-summary-box-inner">
<span><p>Question answering (QA) systems are now available through numerous commercial
applications for a wide variety of domains, serving millions of users that
interact with them via speech interfaces. However, current benchmarks in QA
research do not account for the errors that speech recognition models might
introduce, nor do they consider the language variations (dialects) of the
users. To address this gap, we augment an existing QA dataset to construct a
multi-dialect, spoken QA benchmark on five languages (Arabic, Bengali, English,
Kiswahili, Korean) with more than 68k audio prompts in 24 dialects from 255
speakers. We provide baseline results showcasing the real-world performance of
QA systems and analyze the effect of language variety and other sensitive
speaker attributes on downstream performance. Last, we study the fairness of
the ASR and QA models with respect to the underlying user populations. The
dataset, model outputs, and code for reproducing all our experiments are
available: https://github.com/ffaisal93/SD-QA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Progressive Adversarial Learning for Bootstrapping: A Case Study on Entity Set Expansion. (arXiv:2109.12082v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12082">
<div class="article-summary-box-inner">
<span><p>Bootstrapping has become the mainstream method for entity set expansion.
Conventional bootstrapping methods mostly define the expansion boundary using
seed-based distance metrics, which heavily depend on the quality of selected
seeds and are hard to be adjusted due to the extremely sparse supervision. In
this paper, we propose BootstrapGAN, a new learning method for bootstrapping
which jointly models the bootstrapping process and the boundary learning
process in a GAN framework. Specifically, the expansion boundaries of different
bootstrapping iterations are learned via different discriminator networks; the
bootstrapping network is the generator to generate new positive entities, and
the discriminator networks identify the expansion boundaries by trying to
distinguish the generated entities from known positive entities. By iteratively
performing the above adversarial learning, the generator and the discriminators
can reinforce each other and be progressively refined along the whole
bootstrapping process. Experiments show that BootstrapGAN achieves the new
state-of-the-art entity set expansion performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text-based NP Enrichment. (arXiv:2109.12085v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12085">
<div class="article-summary-box-inner">
<span><p>Understanding the relations between entities denoted by NPs in text is a
critical part of human-like natural language understanding. However, only a
fraction of such relations is covered by NLP tasks and models nowadays. In this
work, we establish the task of text-based NP enrichment (TNE), that is,
enriching each NP with all the preposition-mediated relations that hold between
this and the other NPs in the text. The relations are represented as triplets,
each denoting two NPs linked via a preposition. Humans recover such relations
seamlessly, while current state-of-the-art models struggle with them due to the
implicit nature of the problem. We build the first large-scale dataset for the
problem, provide the formal framing and scope of annotation, analyze the data,
and report the result of fine-tuned neural language models on the task,
demonstrating the challenge it poses to current technology. We created a
webpage with the data, data-exploration UI, code, models, and demo to foster
further research into this challenging text understanding problem at
yanaiela.github.io/TNE/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SAIS: Supervising and Augmenting Intermediate Steps for Document-Level Relation Extraction. (arXiv:2109.12093v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12093">
<div class="article-summary-box-inner">
<span><p>Stepping from sentence-level to document-level relation extraction, the
research community confronts increasing text length and more complicated entity
interactions. Consequently, it is more challenging to encode the key sources of
information--relevant contexts and entity types. However, existing methods only
implicitly learn to model these critical information sources while being
trained for relation extraction. As a result, they suffer the problems of
ineffective supervision and uninterpretable model predictions. In contrast, we
propose to explicitly teach the model to capture relevant contexts and entity
types by supervising and augmenting intermediate steps (SAIS) for relation
extraction. Based on a broad spectrum of carefully designed tasks, our proposed
SAIS method not only extracts relations of better quality due to more effective
supervision, but also retrieves the corresponding supporting evidence more
accurately so as to enhance interpretability. By assessing model uncertainty,
SAIS further boosts the performance via evidence-based data augmentation and
ensemble inference while reducing the computational cost. Eventually, SAIS
delivers state-of-the-art relation extraction results on three benchmarks
(DocRED, CDR, and GDA) and achieves 5.04% relative gains in F1 score compared
to the runner-up in evidence retrieval on DocRED.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLIPort: What and Where Pathways for Robotic Manipulation. (arXiv:2109.12098v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12098">
<div class="article-summary-box-inner">
<span><p>How can we imbue robots with the ability to manipulate objects precisely but
also to reason about them in terms of abstract concepts? Recent works in
manipulation have shown that end-to-end networks can learn dexterous skills
that require precise spatial reasoning, but these methods often fail to
generalize to new goals or quickly learn transferable concepts across tasks. In
parallel, there has been great progress in learning generalizable semantic
representations for vision and language by training on large-scale internet
data, however these representations lack the spatial understanding necessary
for fine-grained manipulation. To this end, we propose a framework that
combines the best of both worlds: a two-stream architecture with semantic and
spatial pathways for vision-based manipulation. Specifically, we present
CLIPort, a language-conditioned imitation-learning agent that combines the
broad semantic understanding (what) of CLIP [1] with the spatial precision
(where) of Transporter [2]. Our end-to-end framework is capable of solving a
variety of language-specified tabletop tasks from packing unseen objects to
folding cloths, all without any explicit representations of object poses,
instance segmentations, memory, symbolic states, or syntactic structures.
Experiments in simulated and real-world settings show that our approach is data
efficient in few-shot settings and generalizes effectively to seen and unseen
semantic concepts. We even learn one multi-task policy for 10 simulated and 9
real-world tasks that is better or comparable to single-task policies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GERNERMED -- An Open German Medical NER Model. (arXiv:2109.12104v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12104">
<div class="article-summary-box-inner">
<span><p>The current state of adoption of well-structured electronic health records
and integration of digital methods for storing medical patient data in
structured formats can often considered as inferior compared to the use of
traditional, unstructured text based patient data documentation. Data mining in
the field of medical data analysis often needs to rely solely on processing of
unstructured data to retrieve relevant data. In natural language processing
(NLP), statistical models have been shown successful in various tasks like
part-of-speech tagging, relation extraction (RE) and named entity recognition
(NER). In this work, we present GERNERMED, the first open, neural NLP model for
NER tasks dedicated to detect medical entity types in German text data. Here,
we avoid the conflicting goals of protection of sensitive patient data from
training data extraction and the publication of the statistical model weights
by training our model on a custom dataset that was translated from publicly
available datasets in foreign language by a pretrained neural machine
translation model. The sample code and the statistical model is available at:
https://github.com/frankkramer-lab/GERNERMED
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Faithful Target Attribute Prediction in Neural Machine Translation. (arXiv:2109.12105v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12105">
<div class="article-summary-box-inner">
<span><p>The training data used in NMT is rarely controlled with respect to specific
attributes, such as word casing or gender, which can cause errors in
translations. We argue that predicting the target word and attributes
simultaneously is an effective way to ensure that translations are more
faithful to the training data distribution with respect to these attributes.
Experimental results on two tasks, uppercased input translation and gender
prediction, show that this strategy helps mirror the training data distribution
in testing. It also facilitates data augmentation on the task of uppercased
input translation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Source Code Search: A Study of the Past and a Glimpse at the Future. (arXiv:1908.06738v2 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1908.06738">
<div class="article-summary-box-inner">
<span><p>With the recent explosion in the size and complexity of source codebases and
software projects, the need for efficient source code search engines has
increased dramatically. Unfortunately, existing information retrieval-based
methods fail to capture the query semantics and perform well only when the
query contains syntax-based keywords. Consequently, such methods will perform
poorly when given high-level natural language queries. In this paper, we review
existing methods for building code search engines. We also outline the open
research directions and the various obstacles that stand in the way of having a
universal source code search engine.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SOLID: A Large-Scale Semi-Supervised Dataset for Offensive Language Identification. (arXiv:2004.14454v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.14454">
<div class="article-summary-box-inner">
<span><p>The widespread use of offensive content in social media has led to an
abundance of research in detecting language such as hate speech, cyberbullying,
and cyber-aggression. Recent work presented the OLID dataset, which follows a
taxonomy for offensive language identification that provides meaningful
information for understanding the type and the target of offensive messages.
However, it is limited in size and it might be biased towards offensive
language as it was collected using keywords. In this work, we present SOLID, an
expanded dataset, where the tweets were collected in a more principled manner.
SOLID contains over nine million English tweets labeled in a semi-supervised
fashion. We demonstrate that using SOLID along with OLID yields sizable
performance gains on the OLID test set for two different models, especially for
the lower levels of the taxonomy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ValNorm Quantifies Semantics to Reveal Consistent Valence Biases Across Languages and Over Centuries. (arXiv:2006.03950v4 [cs.CY] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.03950">
<div class="article-summary-box-inner">
<span><p>Word embeddings learn implicit biases from linguistic regularities captured
by word co-occurrence statistics. By extending methods that quantify human-like
biases in word embeddings, we introduceValNorm, a novel intrinsic evaluation
task and method to quantify the valence dimension of affect in human-rated word
sets from social psychology. We apply ValNorm on static word embeddings from
seven languages (Chinese, English, German, Polish, Portuguese, Spanish, and
Turkish) and from historical English text spanning 200 years. ValNorm achieves
consistently high accuracy in quantifying the valence of non-discriminatory,
non-social group word sets. Specifically, ValNorm achieves a Pearson
correlation of r=0.88 for human judgment scores of valence for 399 words
collected to establish pleasantness norms in English. In contrast, we measure
gender stereotypes using the same set of word embeddings and find that social
biases vary across languages. Our results indicate that valence associations of
non-discriminatory, non-social group words represent widely-shared
associations, in seven languages and over 200 years.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Calling Out Bluff: Evaluation Toolkit For Robustness Testing Of Automatic Essay Scoring Systems. (arXiv:2007.06796v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.06796">
<div class="article-summary-box-inner">
<span><p>Automatic scoring engines have been used for scoring approximately fifteen
million test-takers in just the last three years. This number is increasing
further due to COVID-19 and the associated automation of education and testing.
Despite such wide usage, the AI-based testing literature of these "intelligent"
models is highly lacking. Most of the papers proposing new models rely only on
quadratic weighted kappa (QWK) based agreement with human raters for showing
model efficacy. However, this effectively ignores the highly multi-feature
nature of essay scoring. Essay scoring depends on features like coherence,
grammar, relevance, sufficiency and, vocabulary. To date, there has been no
study testing Automated Essay Scoring: AES systems holistically on all these
features. With this motivation, we propose a model agnostic adversarial
evaluation scheme and associated metrics for AES systems to test their natural
language understanding capabilities and overall robustness. We evaluate the
current state-of-the-art AES models using the proposed scheme and report the
results on five recent models. These models range from
feature-engineering-based approaches to the latest deep learning algorithms. We
find that AES models are highly overstable. Even heavy modifications(as much as
25%) with content unrelated to the topic of the questions do not decrease the
score produced by the models. On the other hand, irrelevant content, on
average, increases the scores, thus showing that the model evaluation strategy
and rubrics should be reconsidered. We also ask 200 human raters to score both
an original and adversarial response to seeing if humans can detect differences
between the two and whether they agree with the scores assigned by auto scores.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OpenAttack: An Open-source Textual Adversarial Attack Toolkit. (arXiv:2009.09191v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.09191">
<div class="article-summary-box-inner">
<span><p>Textual adversarial attacking has received wide and increasing attention in
recent years. Various attack models have been proposed, which are enormously
distinct and implemented with different programming frameworks and settings.
These facts hinder quick utilization and fair comparison of attack models. In
this paper, we present an open-source textual adversarial attack toolkit named
OpenAttack to solve these issues. Compared with existing other textual
adversarial attack toolkits, OpenAttack has its unique strengths in support for
all attack types, multilinguality, and parallel processing. Currently,
OpenAttack includes 15 typical attack models that cover all attack types. Its
highly inclusive modular design not only supports quick utilization of existing
attack models, but also enables great flexibility and extensibility. OpenAttack
has broad uses including comparing and evaluating attack models, measuring
robustness of a model, assisting in developing new attack models, and
adversarial training. Source code and documentation can be obtained at
https://github.com/thunlp/OpenAttack.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Char2Subword: Extending the Subword Embedding Space Using Robust Character Compositionality. (arXiv:2010.12730v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.12730">
<div class="article-summary-box-inner">
<span><p>Byte-pair encoding (BPE) is a ubiquitous algorithm in the subword
tokenization process of language models as it provides multiple benefits.
However, this process is solely based on pre-training data statistics, making
it hard for the tokenizer to handle infrequent spellings. On the other hand,
though robust to misspellings, pure character-level models often lead to
unreasonably long sequences and make it harder for the model to learn
meaningful words. To alleviate these challenges, we propose a character-based
subword module (char2subword) that learns the subword embedding table in
pre-trained models like BERT. Our char2subword module builds representations
from characters out of the subword vocabulary, and it can be used as a drop-in
replacement of the subword embedding table. The module is robust to
character-level alterations such as misspellings, word inflection, casing, and
punctuation. We integrate it further with BERT through pre-training while
keeping BERT transformer parameters fixed--and thus, providing a practical
method. Finally, we show that incorporating our module to mBERT significantly
improves the performance on the social media linguistic code-switching
evaluation (LinCE) benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Sequence-to-Sequence Pre-training via Sequence Span Rewriting. (arXiv:2101.00416v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00416">
<div class="article-summary-box-inner">
<span><p>In this paper, we generalize text infilling (e.g., masked language models) by
proposing Sequence Span Rewriting (SSR) as a self-supervised
sequence-to-sequence (seq2seq) pre-training objective. SSR provides more
fine-grained learning signals for text representations by supervising the model
to rewrite imperfect spans to ground truth, and it is more consistent than text
infilling with many downstream seq2seq tasks that rewrite a source sentences
into a target sentence. Our experiments with T5 models on various seq2seq tasks
show that SSR can substantially improve seq2seq pre-training. Moreover, we
observe SSR is especially helpful to improve pre-training a small-size seq2seq
model with a powerful imperfect span generator, which indicates a new
perspective of transferring knowledge from a large model to a smaller model for
seq2seq pre-training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Advances and Challenges in Conversational Recommender Systems: A Survey. (arXiv:2101.09459v7 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.09459">
<div class="article-summary-box-inner">
<span><p>Recommender systems exploit interaction history to estimate user preference,
having been heavily used in a wide range of industry applications. However,
static recommendation models are difficult to answer two important questions
well due to inherent shortcomings: (a) What exactly does a user like? (b) Why
does a user like an item? The shortcomings are due to the way that static
models learn user preference, i.e., without explicit instructions and active
feedback from users. The recent rise of conversational recommender systems
(CRSs) changes this situation fundamentally. In a CRS, users and the system can
dynamically communicate through natural language interactions, which provide
unprecedented opportunities to explicitly obtain the exact preference of users.
</p>
<p>Considerable efforts, spread across disparate settings and applications, have
been put into developing CRSs. Existing models, technologies, and evaluation
methods for CRSs are far from mature. In this paper, we provide a systematic
review of the techniques used in current CRSs. We summarize the key challenges
of developing CRSs in five directions: (1) Question-based user preference
elicitation. (2) Multi-turn conversational recommendation strategies. (3)
Dialogue understanding and generation. (4) Exploitation-exploration trade-offs.
(5) Evaluation and user simulation. These research directions involve multiple
research fields like information retrieval (IR), natural language processing
(NLP), and human-computer interaction (HCI). Based on these research
directions, we discuss some future challenges and opportunities. We provide a
road map for researchers from multiple communities to get started in this area.
We hope this survey can help to identify and address challenges in CRSs and
inspire future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Statistically significant detection of semantic shifts using contextual word embeddings. (arXiv:2104.03776v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.03776">
<div class="article-summary-box-inner">
<span><p>Detecting lexical semantic change in smaller data sets, e.g. in historical
linguistics and digital humanities, is challenging due to a lack of statistical
power. This issue is exacerbated by non-contextual embedding models that
produce one embedding per word and, therefore, mask the variability present in
the data. In this article, we propose an approach to estimate semantic shift by
combining contextual word embeddings with permutation-based statistical tests.
We use the false discovery rate procedure to address the large number of
hypothesis tests being conducted simultaneously. We demonstrate the performance
of this approach in simulation where it achieves consistently high precision by
suppressing false positives. We additionally analyze real-world data from
SemEval-2020 Task 1 and the Liverpool FC subreddit corpus. We show that by
taking sample variation into account, we can improve the robustness of
individual semantic shift estimates without degrading overall performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bilingual Alignment Pre-Training for Zero-Shot Cross-Lingual Transfer. (arXiv:2106.01732v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01732">
<div class="article-summary-box-inner">
<span><p>Multilingual pre-trained models have achieved remarkable performance on
cross-lingual transfer learning. Some multilingual models such as mBERT, have
been pre-trained on unlabeled corpora, therefore the embeddings of different
languages in the models may not be aligned very well. In this paper, we aim to
improve the zero-shot cross-lingual transfer performance by proposing a
pre-training task named Word-Exchange Aligning Model (WEAM), which uses the
statistical alignment information as the prior knowledge to guide cross-lingual
word prediction. We evaluate our model on multilingual machine reading
comprehension task MLQA and natural language interface task XNLI. The results
show that WEAM can significantly improve the zero-shot performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learned Token Pruning for Transformers. (arXiv:2107.00910v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.00910">
<div class="article-summary-box-inner">
<span><p>Deploying transformer models in practice is challenging due to their
inference cost, which scales quadratically with input sequence length. To
address this, we present a novel Learned Token Pruning (LTP) method which
adaptively removes unimportant tokens as an input sequence passes through
transformer layers. In particular, LTP prunes tokens with an attention score
below a threshold value which is learned for each layer during training. Our
threshold-based method allows the length of the pruned sequence to vary
adaptively based on the input sequence, and avoids algorithmically expensive
operations such as top-k token selection. We extensively test the performance
of LTP on GLUE tasks and show that our method outperforms the prior
state-of-the-art token pruning methods by up to ~2.5% higher accuracy with the
same amount of FLOPs. In particular, LTP achieves up to 2.1x FLOPs reduction
with less than 1% accuracy drop, which results in up to 1.9x and 2.0x
throughput improvement on Intel Haswell CPUs and NVIDIA V100 GPUs,
respectively. Furthermore, we demonstrate that LTP is more robust than prior
methods to variations on input sentence lengths. Our code has been developed in
PyTorch and has been open-sourced.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Natural Language Understanding Pipeline for Bangla Conversational Agent. (arXiv:2107.05541v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.05541">
<div class="article-summary-box-inner">
<span><p>Chatbots are intelligent software built to be used as a replacement for human
interaction. Existing studies typically do not provide enough support for
low-resource languages like Bangla. Due to the increasing popularity of social
media, we can also see the rise of interactions in Bangla transliteration
(mostly in English) among the native Bangla speakers. In this paper, we propose
a novel approach to build a Bangla chatbot aimed to be used as a business
assistant which can communicate in Bangla and Bangla Transliteration in English
with high confidence consistently. Since annotated data was not available for
this purpose, we had to work on the whole machine learning life cycle (data
preparation, machine learning modeling, and model deployment) using Rasa Open
Source Framework, fastText embeddings, Polyglot embeddings, Flask, and other
systems as building blocks. While working with the skewed annotated dataset, we
try out different setups and pipelines to evaluate which works best and provide
possible reasoning behind the observed results. Finally, we present a pipeline
for intent classification and entity extraction which achieves reasonable
performance (accuracy: 83.02%, precision: 80.82%, recall: 83.02%, F1-score:
80%).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Summary Explorer: Visualizing the State of the Art in Text Summarization. (arXiv:2108.01879v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.01879">
<div class="article-summary-box-inner">
<span><p>This paper introduces Summary Explorer, a new tool to support the manual
inspection of text summarization systems by compiling the outputs of
55~state-of-the-art single document summarization approaches on three benchmark
datasets, and visually exploring them during a qualitative assessment. The
underlying design of the tool considers three well-known summary quality
criteria (coverage, faithfulness, and position bias), encapsulated in a guided
assessment based on tailored visualizations. The tool complements existing
approaches for locally debugging summarization models and improves upon them.
The tool is available at https://tldr.webis.de/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequence Level Contrastive Learning for Text Summarization. (arXiv:2109.03481v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03481">
<div class="article-summary-box-inner">
<span><p>Contrastive learning models have achieved great success in unsupervised
visual representation learning, which maximize the similarities between feature
representations of different views of the same image, while minimize the
similarities between feature representations of views of different images. In
text summarization, the output summary is a shorter form of the input document
and they have similar meanings. In this paper, we propose a contrastive
learning model for supervised abstractive text summarization, where we view a
document, its gold summary and its model generated summaries as different views
of the same mean representation and maximize the similarities between them
during training. We improve over a strong sequence-to-sequence text generation
model (i.e., BART) on three different summarization datasets. Human evaluation
also shows that our model achieves better faithfulness ratings compared to its
counterpart without contrastive objectives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Forget me not: A Gentle Reminder to Mind the Simple Multi-Layer Perceptron Baseline for Text Classification. (arXiv:2109.03777v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03777">
<div class="article-summary-box-inner">
<span><p>Graph neural networks have triggered a resurgence of graph-based text
classification. We show that already a simple MLP baseline achieves comparable
performance on benchmark datasets, questioning the importance of synthetic
graph structures. When considering an inductive scenario, i. e., when adding
new documents to a corpus, a simple MLP even outperforms the recent graph-based
models TextGCN and HeteGCN and is comparable with HyperGAT. We further
fine-tune DistilBERT and find that it outperforms all state-of-the-art models.
We suggest that future studies use at least an MLP baseline to contextualize
the results. We provide recommendations for the design and training of such a
baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NADE: A Benchmark for Robust Adverse Drug Events Extraction in Face of Negations. (arXiv:2109.10080v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10080">
<div class="article-summary-box-inner">
<span><p>Adverse Drug Event (ADE) extraction models can rapidly examine large
collections of social media texts, detecting mentions of drug-related adverse
reactions and trigger medical investigations. However, despite the recent
advances in NLP, it is currently unknown if such models are robust in face of
negation, which is pervasive across language varieties.
</p>
<p>In this paper we evaluate three state-of-the-art systems, showing their
fragility against negation, and then we introduce two possible strategies to
increase the robustness of these models: a pipeline approach, relying on a
specific component for negation detection; an augmentation of an ADE extraction
dataset to artificially create negated samples and further train the models.
</p>
<p>We show that both strategies bring significant increases in performance,
lowering the number of spurious entities predicted by the models. Our dataset
and code will be publicly released to encourage research on the topic.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diarisation using location tracking with agglomerative clustering. (arXiv:2109.10598v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10598">
<div class="article-summary-box-inner">
<span><p>Previous works have shown that spatial location information can be
complementary to speaker embeddings for a speaker diarisation task. However,
the models used often assume that speakers are fairly stationary throughout a
meeting. This paper proposes to relax this assumption, by explicitly modelling
the movements of speakers within an Agglomerative Hierarchical Clustering (AHC)
diarisation framework. Kalman filters, which track the locations of speakers,
are used to compute log-likelihood ratios that contribute to the cluster
affinity computations for the AHC merging and stopping decisions. Experiments
show that the proposed approach is able to yield improvements on a Microsoft
rich meeting transcription task, compared to methods that do not use location
information or that make stationarity assumptions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Alzheimers Dementia Detection using Acoustic & Linguistic features and Pre-Trained BERT. (arXiv:2109.11010v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11010">
<div class="article-summary-box-inner">
<span><p>Alzheimers disease is a fatal progressive brain disorder that worsens with
time. It is high time we have inexpensive and quick clinical diagnostic
techniques for early detection and care. In previous studies, various Machine
Learning techniques and Pre-trained Deep Learning models have been used in
conjunction with the extraction of various acoustic and linguistic features.
Our study focuses on three models for the classification task in the ADReSS
(The Alzheimers Dementia Recognition through Spontaneous Speech) 2021
Challenge. We use the well-balanced dataset provided by the ADReSS Challenge
for training and validating our models. Model 1 uses various acoustic features
from the eGeMAPs feature-set, Model 2 uses various linguistic features that we
generated from auto-generated transcripts and Model 3 uses the auto-generated
transcripts directly to extract features using a Pre-trained BERT and TF-IDF.
These models are described in detail in the models section.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Volctrans GLAT System: Non-autoregressive Translation Meets WMT21. (arXiv:2109.11247v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11247">
<div class="article-summary-box-inner">
<span><p>This paper describes the Volctrans' submission to the WMT21 news translation
shared task for German-&gt;English translation. We build a parallel (i.e.,
non-autoregressive) translation system using the Glancing Transformer, which
enables fast and accurate parallel decoding in contrast to the currently
prevailing autoregressive models. To the best of our knowledge, this is the
first parallel translation system that can be scaled to such a practical
scenario like WMT competition. More importantly, our parallel translation
system achieves the best BLEU score (35.0) on German-&gt;English translation task,
outperforming all strong autoregressive counterparts.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Lifelong 3D Object Recognition and Grasp Synthesis Using Dual Memory Recurrent Self-Organization Networks. (arXiv:2109.11544v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11544">
<div class="article-summary-box-inner">
<span><p>Humans learn to recognize and manipulate new objects in lifelong settings
without forgetting the previously gained knowledge under non-stationary and
sequential conditions. In autonomous systems, the agents also need to mitigate
similar behavior to continually learn the new object categories and adapt to
new environments. In most conventional deep neural networks, this is not
possible due to the problem of catastrophic forgetting, where the newly gained
knowledge overwrites existing representations. Furthermore, most
state-of-the-art models excel either in recognizing the objects or in grasp
prediction, while both tasks use visual input. The combined architecture to
tackle both tasks is very limited. In this paper, we proposed a hybrid model
architecture consists of a dynamically growing dual-memory recurrent neural
network (GDM) and an autoencoder to tackle object recognition and grasping
simultaneously. The autoencoder network is responsible to extract a compact
representation for a given object, which serves as input for the GDM learning,
and is responsible to predict pixel-wise antipodal grasp configurations. The
GDM part is designed to recognize the object in both instances and categories
levels. We address the problem of catastrophic forgetting using the intrinsic
memory replay, where the episodic memory periodically replays the neural
activation trajectories in the absence of external sensory information. To
extensively evaluate the proposed model in a lifelong setting, we generate a
synthetic dataset due to lack of sequential 3D objects dataset. Experiment
results demonstrated that the proposed model can learn both object
representation and grasping simultaneously in continual learning scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SAME: Deformable Image Registration based on Self-supervised Anatomical Embeddings. (arXiv:2109.11572v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11572">
<div class="article-summary-box-inner">
<span><p>In this work, we introduce a fast and accurate method for unsupervised 3D
medical image registration. This work is built on top of a recent algorithm
SAM, which is capable of computing dense anatomical/semantic correspondences
between two images at the pixel level. Our method is named SAME, which breaks
down image registration into three steps: affine transformation, coarse
deformation, and deep deformable registration. Using SAM embeddings, we enhance
these steps by finding more coherent correspondences, and providing features
and a loss function with better semantic guidance. We collect a multi-phase
chest computed tomography dataset with 35 annotated organs for each patient and
conduct inter-subject registration for quantitative evaluation. Results show
that SAME outperforms widely-used traditional registration techniques (Elastix
FFD, ANTs SyN) and learning based VoxelMorph method by at least 4.7% and 2.7%
in Dice scores for two separate tasks of within-contrast-phase and
across-contrast-phase registration, respectively. SAME achieves the comparable
performance to the best traditional registration method, DEEDS (from our
evaluation), while being orders of magnitude faster (from 45 seconds to 1.2
seconds).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly-Supervised Monocular Depth Estimationwith Resolution-Mismatched Data. (arXiv:2109.11573v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11573">
<div class="article-summary-box-inner">
<span><p>Depth estimation from a single image is an active research topic in computer
vision. The most accurate approaches are based on fully supervised learning
models, which rely on a large amount of dense and high-resolution (HR)
ground-truth depth maps. However, in practice, color images are usually
captured with much higher resolution than depth maps, leading to the
resolution-mismatched effect. In this paper, we propose a novel
weakly-supervised framework to train a monocular depth estimation network to
generate HR depth maps with resolution-mismatched supervision, i.e., the inputs
are HR color images and the ground-truth are low-resolution (LR) depth maps.
The proposed weakly supervised framework is composed of a sharing weight
monocular depth estimation network and a depth reconstruction network for
distillation. Specifically, for the monocular depth estimation network the
input color image is first downsampled to obtain its LR version with the same
resolution as the ground-truth depth. Then, both HR and LR color images are fed
into the proposed monocular depth estimation network to obtain the
corresponding estimated depth maps. We introduce three losses to train the
network: 1) reconstruction loss between the estimated LR depth and the
ground-truth LR depth; 2) reconstruction loss between the downsampled estimated
HR depth and the ground-truth LR depth; 3) consistency loss between the
estimated LR depth and the downsampled estimated HR depth. In addition, we
design a depth reconstruction network from depth to depth. Through distillation
loss, features between two networks maintain the structural consistency in
affinity space, and finally improving the estimation network performance.
Experimental results demonstrate that our method achieves superior performance
than unsupervised and semi-supervised learning based schemes, and is
competitive or even better compared to supervised ones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Long Short View Feature Decomposition via Contrastive Video Representation Learning. (arXiv:2109.11593v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11593">
<div class="article-summary-box-inner">
<span><p>Self-supervised video representation methods typically focus on the
representation of temporal attributes in videos. However, the role of
stationary versus non-stationary attributes is less explored: Stationary
features, which remain similar throughout the video, enable the prediction of
video-level action classes. Non-stationary features, which represent temporally
varying attributes, are more beneficial for downstream tasks involving more
fine-grained temporal understanding, such as action segmentation. We argue that
a single representation to capture both types of features is sub-optimal, and
propose to decompose the representation space into stationary and
non-stationary features via contrastive learning from long and short views,
i.e. long video sequences and their shorter sub-sequences. Stationary features
are shared between the short and long views, while non-stationary features
aggregate the short views to match the corresponding long view. To empirically
verify our approach, we demonstrate that our stationary features work
particularly well on an action recognition downstream task, while our
non-stationary features perform better on action segmentation. Furthermore, we
analyse the learned representations and find that stationary features capture
more temporally stable, static attributes, while non-stationary features
encompass more temporally varying ones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPNet: Multi-Shell Kernel Convolution for Point Cloud Semantic Segmentation. (arXiv:2109.11610v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11610">
<div class="article-summary-box-inner">
<span><p>Feature encoding is essential for point cloud analysis. In this paper, we
propose a novel point convolution operator named Shell Point Convolution
(SPConv) for shape encoding and local context learning. Specifically, SPConv
splits 3D neighborhood space into shells, aggregates local features on manually
designed kernel points, and performs convolution on the shells. Moreover,
SPConv incorporates a simple yet effective attention module that enhances local
feature aggregation. Based upon SPConv, a deep neural network named SPNet is
constructed to process large-scale point clouds. Poisson disk sampling and
feature propagation are incorporated in SPNet for better efficiency and
accuracy. We provided details of the shell design and conducted extensive
experiments on challenging large-scale point cloud datasets. Experimental
results show that SPConv is effective in local shape encoding, and our SPNet is
able to achieve top-ranking performances in semantic segmentation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast Point Voxel Convolution Neural Network with Selective Feature Fusion for Point Cloud Semantic Segmentation. (arXiv:2109.11614v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11614">
<div class="article-summary-box-inner">
<span><p>We present a novel lightweight convolutional neural network for point cloud
analysis. In contrast to many current CNNs which increase receptive field by
downsampling point cloud, our method directly operates on the entire point sets
without sampling and achieves good performances efficiently. Our network
consists of point voxel convolution (PVC) layer as building block. Each layer
has two parallel branches, namely the voxel branch and the point branch. For
the voxel branch specifically, we aggregate local features on non-empty voxel
centers to reduce geometric information loss caused by voxelization, then apply
volumetric convolutions to enhance local neighborhood geometry encoding. For
the point branch, we use Multi-Layer Perceptron (MLP) to extract fine-detailed
point-wise features. Outputs from these two branches are adaptively fused via a
feature selection module. Moreover, we supervise the output from every PVC
layer to learn different levels of semantic information. The final prediction
is made by averaging all intermediate predictions. We demonstrate empirically
that our method is able to achieve comparable results while being fast and
memory efficient. We evaluate our method on popular point cloud datasets for
object classification and semantic segmentation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Keypoints-Based Deep Feature Fusion for Cooperative Vehicle Detection of Autonomous Driving. (arXiv:2109.11615v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11615">
<div class="article-summary-box-inner">
<span><p>Sharing collective perception messages (CPM) between vehicles is investigated
to decrease occlusions, so as to improve perception accuracy and safety of
autonomous driving. However, highly accurate data sharing and low communication
overhead is a big challenge for collective perception, especially when
real-time communication is required among connected and automated vehicles. In
this paper, we propose an efficient and effective keypoints-based deep feature
fusion framework, called FPV-RCNN, for collective perception, which is built on
top of the 3D object detector PV-RCNN. We introduce a bounding box proposal
matching module and a keypoints selection strategy to compress the CPM size and
solve the multi-vehicle data fusion problem. Compared to a bird's-eye view
(BEV) keypoints feature fusion, FPV-RCNN achieves improved detection accuracy
by about 14% at a high evaluation criterion (IoU 0.7) on a synthetic dataset
COMAP dedicated to collective perception. Also, its performance is comparable
to two raw data fusion baselines that have no data loss in sharing. Moreover,
our method also significantly decreases the CPM size to less than 0.3KB, which
is about 50 times smaller than the BEV feature map sharing used in previous
works. Even with a further decreased number of CPM feature channels, i.e., from
128 to 32, the detection performance only drops about 1%. The code of our
method is available at https://github.com/YuanYunshuang/FPV_RCNN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Learned Stereo Depth System for Robotic Manipulation in Homes. (arXiv:2109.11644v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11644">
<div class="article-summary-box-inner">
<span><p>We present a passive stereo depth system that produces dense and accurate
point clouds optimized for human environments, including dark, textureless,
thin, reflective and specular surfaces and objects, at 2560x2048 resolution,
with 384 disparities, in 30 ms. The system consists of an algorithm combining
learned stereo matching with engineered filtering, a training and data-mixing
methodology, and a sensor hardware design. Our architecture is 15x faster than
approaches that perform similarly on the Middlebury and Flying Things Stereo
Benchmarks. To effectively supervise the training of this model, we combine
real data labelled using off-the-shelf depth sensors, as well as a number of
different rendered, simulated labeled datasets. We demonstrate the efficacy of
our system by presenting a large number of qualitative results in the form of
depth maps and point-clouds, experiments validating the metric accuracy of our
system and comparisons to other sensors on challenging objects and scenes. We
also show the competitiveness of our algorithm compared to state-of-the-art
learned models using the Middlebury and FlyingThings datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Paint4Poem: A Dataset for Artistic Visualization of Classical Chinese Poems. (arXiv:2109.11682v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11682">
<div class="article-summary-box-inner">
<span><p>In this work we propose a new task: artistic visualization of classical
Chinese poems, where the goal is to generatepaintings of a certain artistic
style for classical Chinese poems. For this purpose, we construct a new dataset
called Paint4Poem. Thefirst part of Paint4Poem consists of 301 high-quality
poem-painting pairs collected manually from an influential modern Chinese
artistFeng Zikai. As its small scale poses challenges for effectively training
poem-to-painting generation models, we introduce the secondpart of Paint4Poem,
which consists of 3,648 caption-painting pairs collected manually from Feng
Zikai's paintings and 89,204 poem-painting pairs collected automatically from
the web. We expect the former to help learning the artist painting style as it
containshis most paintings, and the latter to help learning the semantic
relevance between poems and paintings. Further, we analyze Paint4Poem regarding
poem diversity, painting style, and the semantic relevance between poems and
paintings. We create abenchmark for Paint4Poem: we train two representative
text-to-image generation models: AttnGAN and MirrorGAN, and evaluate
theirperformance regarding painting pictorial quality, painting stylistic
relevance, and semantic relevance between poems and paintings.The results
indicate that the models are able to generate paintings that have good
pictorial quality and mimic Feng Zikai's style, but thereflection of poem
semantics is limited. The dataset also poses many interesting research
directions on this task, including transferlearning, few-shot learning,
text-to-image generation for low-resource data etc. The dataset is publicly
available.(https://github.com/paint4poem/paint4poem)
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feasibility study of urban flood mapping using traffic signs for route optimization. (arXiv:2109.11712v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11712">
<div class="article-summary-box-inner">
<span><p>Water events are the most frequent and costliest climate disasters around the
world. In the U.S., an estimated 127 million people who live in coastal areas
are at risk of substantial home damage from hurricanes or flooding. In flood
emergency management, timely and effective spatial decision-making and
intelligent routing depend on flood depth information at a fine spatiotemporal
scale. In this paper, crowdsourcing is utilized to collect photos of submerged
stop signs, and pair each photo with a pre-flood photo taken at the same
location. Each photo pair is then analyzed using deep neural network and image
processing to estimate the depth of floodwater in the location of the photo.
Generated point-by-point depth data is converted to a flood inundation map and
used by an A* search algorithm to determine an optimal flood-free path
connecting points of interest. Results provide crucial information to rescue
teams and evacuees by enabling effective wayfinding during flooding events.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training Automatic View Planner for Cardiac MR Imaging via Self-Supervision by Spatial Relationship between Views. (arXiv:2109.11715v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11715">
<div class="article-summary-box-inner">
<span><p>View planning for the acquisition of cardiac magnetic resonance imaging (CMR)
requires acquaintance with the cardiac anatomy and remains a challenging task
in clinical practice. Existing approaches to its automation relied either on an
additional volumetric image not typically acquired in clinic routine, or on
laborious manual annotations of cardiac structural landmarks. This work
presents a clinic-compatible and annotation-free system for automatic CMR view
planning. The system mines the spatial relationship -- more specifically,
locates and exploits the intersecting lines -- between the source and target
views, and trains deep networks to regress heatmaps defined by these
intersecting lines. As the spatial relationship is self-contained in properly
stored data, e.g., in the DICOM format, the need for manual annotation is
eliminated. Then, a multi-view planning strategy is proposed to aggregate
information from the predicted heatmaps for all the source views of a target
view, for a globally optimal prescription. The multi-view aggregation mimics
the similar strategy practiced by skilled human prescribers. Experimental
results on 181 clinical CMR exams show that our system achieves superior
accuracy to existing approaches including conventional atlas-based and newer
deep learning based ones, in prescribing four standard CMR views. The mean
angle difference and point-to-plane distance evaluated against the ground truth
planes are 5.98 degrees and 3.48 mm, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A 3D Mesh-based Lifting-and-Projection Network for Human Pose Transfer. (arXiv:2109.11719v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11719">
<div class="article-summary-box-inner">
<span><p>Human pose transfer has typically been modeled as a 2D image-to-image
translation problem. This formulation ignores the human body shape prior in 3D
space and inevitably causes implausible artifacts, especially when facing
occlusion. To address this issue, we propose a lifting-and-projection framework
to perform pose transfer in the 3D mesh space. The core of our framework is a
foreground generation module, that consists of two novel networks: a
lifting-and-projection network (LPNet) and an appearance detail compensating
network (ADCNet). To leverage the human body shape prior, LPNet exploits the
topological information of the body mesh to learn an expressive visual
representation for the target person in the 3D mesh space. To preserve texture
details, ADCNet is further introduced to enhance the feature produced by LPNet
with the source foreground image. Such design of the foreground generation
module enables the model to better handle difficult cases such as those with
occlusions. Experiments on the iPER and Fashion datasets empirically
demonstrate that the proposed lifting-and-projection framework is effective and
outperforms the existing image-to-image-based and mesh-based methods on human
pose transfer task in both self-transfer and cross-transfer settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Holistic Semi-Supervised Approaches for EEG Representation Learning. (arXiv:2109.11732v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11732">
<div class="article-summary-box-inner">
<span><p>Recently, supervised methods, which often require substantial amounts of
class labels, have achieved promising results for EEG representation learning.
However, labeling EEG data is a challenging task. More recently, holistic
semi-supervised learning approaches, which only require few output labels, have
shown promising results in the field of computer vision. These methods,
however, have not yet been adapted for EEG learning. In this paper, we adapt
three state-of-the-art holistic semi-supervised approaches, namely MixMatch,
FixMatch, and AdaMatch, as well as five classical semi-supervised methods for
EEG learning. We perform rigorous experiments with all 8 methods on two public
EEG-based emotion recognition datasets, namely SEED and SEED-IV. The
experiments with different amounts of limited labeled samples show that the
holistic approaches achieve strong results even when only 1 labeled sample is
used per class. Further experiments show that in most cases, AdaMatch is the
most effective method, followed by MixMatch and FixMatch.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unaligned Image-to-Image Translation by Learning to Reweight. (arXiv:2109.11736v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11736">
<div class="article-summary-box-inner">
<span><p>Unsupervised image-to-image translation aims at learning the mapping from the
source to target domain without using paired images for training. An essential
yet restrictive assumption for unsupervised image translation is that the two
domains are aligned, e.g., for the selfie2anime task, the anime (selfie) domain
must contain only anime (selfie) face images that can be translated to some
images in the other domain. Collecting aligned domains can be laborious and
needs lots of attention. In this paper, we consider the task of image
translation between two unaligned domains, which may arise for various possible
reasons. To solve this problem, we propose to select images based on importance
reweighting and develop a method to learn the weights and perform translation
simultaneously and automatically. We compare the proposed method with
state-of-the-art image translation approaches and present qualitative and
quantitative results on different tasks with unaligned domains. Extensive
empirical evidence demonstrates the usefulness of the proposed problem
formulation and the superiority of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-View Video-Based 3D Hand Pose Estimation. (arXiv:2109.11747v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11747">
<div class="article-summary-box-inner">
<span><p>Hand pose estimation (HPE) can be used for a variety of human-computer
interaction applications such as gesture-based control for physical or
virtual/augmented reality devices. Recent works have shown that videos or
multi-view images carry rich information regarding the hand, allowing for the
development of more robust HPE systems. In this paper, we present the
Multi-View Video-Based 3D Hand (MuViHand) dataset, consisting of multi-view
videos of the hand along with ground-truth 3D pose labels. Our dataset includes
more than 402,000 synthetic hand images available in 4,560 videos. The videos
have been simultaneously captured from six different angles with complex
backgrounds and random levels of dynamic lighting. The data has been captured
from 10 distinct animated subjects using 12 cameras in a semi-circle topology
where six tracking cameras only focus on the hand and the other six fixed
cameras capture the entire body. Next, we implement MuViHandNet, a neural
pipeline consisting of image encoders for obtaining visual embeddings of the
hand, recurrent learners to learn both temporal and angular sequential
information, and graph networks with U-Net architectures to estimate the final
3D pose information. We perform extensive experiments and show the challenging
nature of this new dataset as well as the effectiveness of our proposed method.
Ablation studies show the added value of each component in MuViHandNet, as well
as the benefit of having temporal and sequential information in the dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-Grained Image Generation from Bangla Text Description using Attentional Generative Adversarial Network. (arXiv:2109.11749v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11749">
<div class="article-summary-box-inner">
<span><p>Generating fine-grained, realistic images from text has many applications in
the visual and semantic realm. Considering that, we propose Bangla Attentional
Generative Adversarial Network (AttnGAN) that allows intensified, multi-stage
processing for high-resolution Bangla text-to-image generation. Our model can
integrate the most specific details at different sub-regions of the image. We
distinctively concentrate on the relevant words in the natural language
description. This framework has achieved a better inception score on the CUB
dataset. For the first time, a fine-grained image is generated from Bangla text
using attentional GAN. Bangla has achieved 7th position among 100 most spoken
languages. This inspires us to explicitly focus on this language, which will
ensure the inevitable need of many people. Moreover, Bangla has a more complex
syntactic structure and less natural language processing resource that
validates our work more.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantifying point cloud realism through adversarially learned latent representations. (arXiv:2109.11775v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11775">
<div class="article-summary-box-inner">
<span><p>Judging the quality of samples synthesized by generative models can be
tedious and time consuming, especially for complex data structures, such as
point clouds. This paper presents a novel approach to quantify the realism of
local regions in LiDAR point clouds. Relevant features are learned from
real-world and synthetic point clouds by training on a proxy classification
task. Inspired by fair networks, we use an adversarial technique to discourage
the encoding of dataset-specific information. The resulting metric can assign a
quality score to samples without requiring any task specific annotations.
</p>
<p>In a series of experiments, we confirm the soundness of our metric by
applying it in controllable task setups and on unseen data. Additional
experiments show reliable interpolation capabilities of the metric between data
with varying degree of realism. As one important application, we demonstrate
how the local realism score can be used for anomaly detection in point clouds.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dense Contrastive Visual-Linguistic Pretraining. (arXiv:2109.11778v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11778">
<div class="article-summary-box-inner">
<span><p>Inspired by the success of BERT, several multimodal representation learning
approaches have been proposed that jointly represent image and text. These
approaches achieve superior performance by capturing high-level semantic
information from large-scale multimodal pretraining. In particular, LXMERT and
UNITER adopt visual region feature regression and label classification as
pretext tasks. However, they tend to suffer from the problems of noisy labels
and sparse semantic annotations, based on the visual features having been
pretrained on a crowdsourced dataset with limited and inconsistent semantic
labeling. To overcome these issues, we propose unbiased Dense Contrastive
Visual-Linguistic Pretraining (DCVLP), which replaces the region regression and
classification with cross-modality region contrastive learning that requires no
annotations. Two data augmentation strategies (Mask Perturbation and
Intra-/Inter-Adversarial Perturbation) are developed to improve the quality of
negative samples used in contrastive learning. Overall, DCVLP allows
cross-modality dense region contrastive learning in a self-supervised setting
independent of any object annotations. We compare our method against prior
visual-linguistic pretraining frameworks to validate the superiority of dense
contrastive learning on multimodal representation learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CPT: Colorful Prompt Tuning for Pre-trained Vision-Language Models. (arXiv:2109.11797v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11797">
<div class="article-summary-box-inner">
<span><p>Pre-Trained Vision-Language Models (VL-PTMs) have shown promising
capabilities in grounding natural language in image data, facilitating a broad
variety of cross-modal tasks. However, we note that there exists a significant
gap between the objective forms of model pre-training and fine-tuning,
resulting in a need for quantities of labeled data to stimulate the visual
grounding capability of VL-PTMs for downstream tasks. To address the challenge,
we present Cross-modal Prompt Tuning (CPT, alternatively, Colorful Prompt
Tuning), a novel paradigm for tuning VL-PTMs, which reformulates visual
grounding into a fill-in-the-blank problem with color-based co-referential
markers in image and text, maximally mitigating the gap. In this way, our
prompt tuning approach enables strong few-shot and even zero-shot visual
grounding capabilities of VL-PTMs. Comprehensive experimental results show that
prompt tuned VL-PTMs outperform their fine-tuned counterparts by a large margin
(e.g., 17.3% absolute accuracy improvement, and 73.8% relative standard
deviation reduction on average with one shot in RefCOCO evaluation). All the
data and code will be available to facilitate future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Domain Feature Adaptation for Bronchoscopic Depth Estimation. (arXiv:2109.11798v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11798">
<div class="article-summary-box-inner">
<span><p>Depth estimation from monocular images is an important task in localization
and 3D reconstruction pipelines for bronchoscopic navigation. Various
supervised and self-supervised deep learning-based approaches have proven
themselves on this task for natural images. However, the lack of labeled data
and the bronchial tissue's feature-scarce texture make the utilization of these
methods ineffective on bronchoscopic scenes. In this work, we propose an
alternative domain-adaptive approach. Our novel two-step structure first trains
a depth estimation network with labeled synthetic images in a supervised
manner; then adopts an unsupervised adversarial domain feature adaptation
scheme to improve the performance on real images. The results of our
experiments show that the proposed method improves the network's performance on
real images by a considerable margin and can be employed in 3D reconstruction
pipelines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SIM2REALVIZ: Visualizing the Sim2Real Gap in Robot Ego-Pose Estimation. (arXiv:2109.11801v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11801">
<div class="article-summary-box-inner">
<span><p>The Robotics community has started to heavily rely on increasingly realistic
3D simulators for large-scale training of robots on massive amounts of data.
But once robots are deployed in the real world, the simulation gap, as well as
changes in the real world (e.g. lights, objects displacements) lead to errors.
In this paper, we introduce Sim2RealViz, a visual analytics tool to assist
experts in understanding and reducing this gap for robot ego-pose estimation
tasks, i.e. the estimation of a robot's position using trained models.
Sim2RealViz displays details of a given model and the performance of its
instances in both simulation and real-world. Experts can identify environment
differences that impact model predictions at a given location and explore
through direct interactions with the model hypothesis to fix it. We detail the
design of the tool, and case studies related to the exploit of the regression
to the mean bias and how it can be addressed, and how models are perturbed by
the vanish of landmarks such as bikes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Learning Based on Multi-stage Transfer and Class-Balanced Loss for Diabetic Retinopathy Grading. (arXiv:2109.11806v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11806">
<div class="article-summary-box-inner">
<span><p>Diabetic retinopathy (DR) is one of the major blindness-causing diseases
current-ly known. Automatic grading of DR using deep learning methods not only
speeds up the diagnosis of the disease but also reduces the rate of
misdiagnosis. However, problems such as insufficient samples and imbalanced
class distribu-tion in DR datasets have constrained the improvement of grading
performance. In this paper, we introduce the idea of multi-stage transfer into
the grading task of DR. The new transfer learning technique leverages multiple
datasets with differ-ent scales to enable the model to learn more feature
representation information. Meanwhile, to cope with imbalanced DR datasets, we
present a class-balanced loss function that performs well in natural image
classification tasks, and adopt a simple and easy-to-implement training method
for it. The experimental results show that the application of multi-stage
transfer and class-balanced loss function can effectively improve the grading
performance metrics such as accuracy and quadratic weighted kappa. In fact, our
method has outperformed two state-of-the-art methods and achieved the best
result on the DR grading task of IDRiD Sub-Challenge 2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MODNet-V: Improving Portrait Video Matting via Background Restoration. (arXiv:2109.11818v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11818">
<div class="article-summary-box-inner">
<span><p>To address the challenging portrait video matting problem more precisely,
existing works typically apply some matting priors that require additional user
efforts to obtain, such as annotated trimaps or background images. In this
work, we observe that instead of asking the user to explicitly provide a
background image, we may recover it from the input video itself. To this end,
we first propose a novel background restoration module (BRM) to recover the
background image dynamically from the input video. BRM is extremely lightweight
and can be easily integrated into existing matting models. By combining BRM
with a recent image matting model, MODNet, we then present MODNet-V for
portrait video matting. Benefited from the strong background prior provided by
BRM, MODNet-V has only 1/3 of the parameters of MODNet but achieves comparable
or even better performances. Our design allows MODNet-V to be trained in an
end-to-end manner on a single NVIDIA 3090 GPU. Finally, we introduce a new
patch refinement module (PRM) to adapt MODNet-V for high-resolution videos
while keeping MODNet-V lightweight and fast.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GSIP: Green Semantic Segmentation of Large-Scale Indoor Point Clouds. (arXiv:2109.11835v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11835">
<div class="article-summary-box-inner">
<span><p>An efficient solution to semantic segmentation of large-scale indoor scene
point clouds is proposed in this work. It is named GSIP (Green Segmentation of
Indoor Point clouds) and its performance is evaluated on a representative
large-scale benchmark -- the Stanford 3D Indoor Segmentation (S3DIS) dataset.
GSIP has two novel components: 1) a room-style data pre-processing method that
selects a proper subset of points for further processing, and 2) a new feature
extractor which is extended from PointHop. For the former, sampled points of
each room form an input unit. For the latter, the weaknesses of PointHop's
feature extraction when extending it to large-scale point clouds are identified
and fixed with a simpler processing pipeline. As compared with PointNet, which
is a pioneering deep-learning-based solution, GSIP is green since it has
significantly lower computational complexity and a much smaller model size.
Furthermore, experiments show that GSIP outperforms PointNet in segmentation
performance for the S3DIS dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Frequency Pooling: Shift-Equivalent and Anti-Aliasing Downsampling. (arXiv:2109.11839v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11839">
<div class="article-summary-box-inner">
<span><p>Convolution utilizes a shift-equivalent prior of images, thus leading to
great success in image processing tasks. However, commonly used poolings in
convolutional neural networks (CNNs), such as max-pooling, average-pooling, and
strided-convolution, are not shift-equivalent. Thus, the shift-equivalence of
CNNs is destroyed when convolutions and poolings are stacked. Moreover,
anti-aliasing is another essential property of poolings from the perspective of
signal processing. However, recent poolings are neither shift-equivalent nor
anti-aliasing. To address this issue, we propose a new pooling method that is
shift-equivalent and anti-aliasing, named frequency pooling. Frequency pooling
first transforms the features into the frequency domain, and then removes the
frequency components beyond the Nyquist frequency. Finally, it transforms the
features back to the spatial domain. We prove that frequency pooling is
shift-equivalent and anti-aliasing based on the property of Fourier transform
and Nyquist frequency. Experiments on image classification show that frequency
pooling improves accuracy and robustness with respect to the shifts of CNNs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learnable Triangulation for Deep Learning-based 3D Reconstruction of Objects of Arbitrary Topology from Single RGB Images. (arXiv:2109.11844v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11844">
<div class="article-summary-box-inner">
<span><p>We propose a novel deep reinforcement learning-based approach for 3D object
reconstruction from monocular images. Prior works that use mesh representations
are template based. Thus, they are limited to the reconstruction of objects
that have the same topology as the template. Methods that use volumetric grids
as intermediate representations are computationally expensive, which limits
their application in real-time scenarios. In this paper, we propose a novel
end-to-end method that reconstructs 3D objects of arbitrary topology from a
monocular image. It is composed of of (1) a Vertex Generation Network (VGN),
which predicts the initial 3D locations of the object's vertices from an input
RGB image, (2) a differentiable triangulation layer, which learns in a
non-supervised manner, using a novel reinforcement learning algorithm, the best
triangulation of the object's vertices, and finally, (3) a hierarchical mesh
refinement network that uses graph convolutions to refine the initial mesh. Our
key contribution is the learnable triangulation process, which recovers in an
unsupervised manner the topology of the input shape. Our experiments on
ShapeNet and Pix3D benchmarks show that the proposed method outperforms the
state-of-the-art in terms of visual quality, reconstruction accuracy, and
computational time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How to find a good image-text embedding for remote sensing visual question answering?. (arXiv:2109.11848v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11848">
<div class="article-summary-box-inner">
<span><p>Visual question answering (VQA) has recently been introduced to remote
sensing to make information extraction from overhead imagery more accessible to
everyone. VQA considers a question (in natural language, therefore easy to
formulate) about an image and aims at providing an answer through a model based
on computer vision and natural language processing methods. As such, a VQA
model needs to jointly consider visual and textual features, which is
frequently done through a fusion step. In this work, we study three different
fusion methodologies in the context of VQA for remote sensing and analyse the
gains in accuracy with respect to the model complexity. Our findings indicate
that more complex fusion mechanisms yield an improved performance, yet that
seeking a trade-of between model complexity and performance is worthwhile in
practice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training dataset generation for bridge game registration. (arXiv:2109.11861v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11861">
<div class="article-summary-box-inner">
<span><p>This paper presents a method for automatic generation of a training dataset
for a deep convolutional neural network used for playing card detection. The
solution allows to skip the time-consuming processes of manual image collecting
and labelling recognised objects. The YOLOv4 network trained on the generated
dataset achieved an efficiency of 99.8% in the cards detection task. The
proposed method is a part of a project that aims to automate the process of
broadcasting duplicate bridge competitions using a vision system and neural
networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Catadioptric Stereo on a Smartphone. (arXiv:2109.11872v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11872">
<div class="article-summary-box-inner">
<span><p>We present a 3D printed adapter with planar mirrors for stereo reconstruction
using front and back smartphone camera. The adapter presents a practical and
low-cost solution for enabling any smartphone to be used as a stereo camera,
which is currently only possible using high-end phones with expensive 3D
sensors. Using the prototype version of the adapter, we experiment with
parameters like the angles between cameras and mirrors and the distance to each
camera (the stereo baseline). We find the most convenient configuration and
calibrate the stereo pair. Based on the presented preliminary analysis, we
identify possible improvements in the current design. To demonstrate the
working prototype, we reconstruct a 3D human pose using 2D keypoint detections
from the stereo pair and evaluate extracted body lengths. The result shows that
the adapter can be used for anthropometric measurement of several body
segments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Localizing Infinity-shaped fishes: Sketch-guided object localization in the wild. (arXiv:2109.11874v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11874">
<div class="article-summary-box-inner">
<span><p>This work investigates the problem of sketch-guided object localization
(SGOL), where human sketches are used as queries to conduct the object
localization in natural images. In this cross-modal setting, we first
contribute with a tough-to-beat baseline that without any specific SGOL
training is able to outperform the previous works on a fixed set of classes.
The baseline is useful to analyze the performance of SGOL approaches based on
available simple yet powerful methods. We advance prior arts by proposing a
sketch-conditioned DETR (DEtection TRansformer) architecture which avoids a
hard classification and alleviates the domain gap between sketches and images
to localize object instances. Although the main goal of SGOL is focused on
object detection, we explored its natural extension to sketch-guided instance
segmentation. This novel task allows to move towards identifying the objects at
pixel level, which is of key importance in several applications. We
experimentally demonstrate that our model and its variants significantly
advance over previous state-of-the-art results. All training and testing code
of our model will be released to facilitate future
research{{https://github.com/priba/sgol_wild}}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning-based Noise Component Map Estimation for Image Denoising. (arXiv:2109.11877v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11877">
<div class="article-summary-box-inner">
<span><p>A problem of image denoising when images are corrupted by a non-stationary
noise is considered in this paper. Since in practice no a priori information on
noise is available, noise statistics should be pre-estimated for image
denoising. In this paper, deep convolutional neural network (CNN) based method
for estimation of a map of local, patch-wise, standard deviations of noise
(so-called sigma-map) is proposed. It achieves the state-of-the-art performance
in accuracy of estimation of sigma-map for the case of non-stationary noise, as
well as estimation of noise variance for the case of additive white Gaussian
noise. Extensive experiments on image denoising using estimated sigma-maps
demonstrate that our method outperforms recent CNN-based blind image denoising
methods by up to 6 dB in PSNR, as well as other state-of-the-art methods based
on sigma-map estimation by up to 0.5 dB, providing same time better usage
flexibility. Comparison with the ideal case, when denoising is applied using
ground-truth sigma-map, shows that a difference of corresponding PSNR values
for most of noise levels is within 0.1-0.2 dB and does not exceeds 0.6 dB.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tackling Inter-Class Similarity and Intra-Class Variance for Microscopic Image-based Classification. (arXiv:2109.11891v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11891">
<div class="article-summary-box-inner">
<span><p>Automatic classification of aquatic microorganisms is based on the
morphological features extracted from individual images. The current works on
their classification do not consider the inter-class similarity and intra-class
variance that causes misclassification. We are particularly interested in the
case where variance within a class occurs due to discrete visual changes in
microscopic images. In this paper, we propose to account for it by partitioning
the classes with high variance based on the visual features. Our algorithm
automatically decides the optimal number of sub-classes to be created and
consider each of them as a separate class for training. This way, the network
learns finer-grained visual features. Our experiments on two databases of
freshwater benthic diatoms and marine plankton show that our method can
outperform the state-of-the-art approaches for classification of these aquatic
microorganisms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RSDet++: Point-based Modulated Loss for More Accurate Rotated Object Detection. (arXiv:2109.11906v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11906">
<div class="article-summary-box-inner">
<span><p>We classify the discontinuity of loss in both five-param and eight-param
rotated object detection methods as rotation sensitivity error (RSE) which will
result in performance degeneration. We introduce a novel modulated rotation
loss to alleviate the problem and propose a rotation sensitivity detection
network (RSDet) which is consists of an eight-param single-stage rotated object
detector and the modulated rotation loss. Our proposed RSDet has several
advantages: 1) it reformulates the rotated object detection problem as
predicting the corners of objects while most previous methods employ a
five-para-based regression method with different measurement units. 2)
modulated rotation loss achieves consistent improvement on both five-param and
eight-param rotated object detection methods by solving the discontinuity of
loss. To further improve the accuracy of our method on objects smaller than 10
pixels, we introduce a novel RSDet++ which is consists of a point-based
anchor-free rotated object detector and a modulated rotation loss. Extensive
experiments demonstrate the effectiveness of both RSDet and RSDet++, which
achieve competitive results on rotated object detection in the challenging
benchmarks DOTA1.0, DOTA1.5, and DOTA2.0. We hope the proposed method can
provide a new perspective for designing algorithms to solve rotated object
detection and pay more attention to tiny objects. The codes and models are
available at: https://github.com/yangxue0827/RotationDetection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Autonomous Crop-Agnostic Visual Navigation in Arable Fields. (arXiv:2109.11936v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11936">
<div class="article-summary-box-inner">
<span><p>Autonomous navigation of a robot in agricultural fields is essential for
every task from crop monitoring through to weed management and fertilizer
application. Many current approaches rely on accurate GPS, however, such
technology is expensive and also prone to failure~(e.g. through lack of
coverage). As such, navigation through sensors that can interpret their
environment (such as cameras) is important to achieve the goal of autonomy in
agriculture. In this paper, we introduce a purely vision-based navigation
scheme which is able to reliably guide the robot through row-crop fields.
Independent of any global localization or mapping, this approach is able to
accurately follow the crop-rows and switch between the rows, only using
on-board cameras. With the help of a novel crop-row detection and a novel
crop-row switching technique, our navigation scheme can be deployed in a wide
range of fields with different canopy types in various growth stages. We have
extensively tested our approach in five different fields under various
illumination conditions using our agricultural robotic platform (BonnBot-I).
And our evaluations show that we have achieved a navigation accuracy of 3.82cm
over five different crop fields.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Two-Stage Mesh Deep Learning for Automated Tooth Segmentation and Landmark Localization on 3D Intraoral Scans. (arXiv:2109.11941v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11941">
<div class="article-summary-box-inner">
<span><p>Accurately segmenting teeth and identifying the corresponding anatomical
landmarks on dental mesh models are essential in computer-aided orthodontic
treatment. Manually performing these two tasks is time-consuming, tedious, and,
more importantly, highly dependent on orthodontists' experiences due to the
abnormality and large-scale variance of patients' teeth. Some machine
learning-based methods have been designed and applied in the orthodontic field
to automatically segment dental meshes (e.g., intraoral scans). In contrast,
the number of studies on tooth landmark localization is still limited. This
paper proposes a two-stage framework based on mesh deep learning (called
TS-MDL) for joint tooth labeling and landmark identification on raw intraoral
scans. Our TS-MDL first adopts an end-to-end \emph{i}MeshSegNet method (i.e., a
variant of the existing MeshSegNet with both improved accuracy and efficiency)
to label each tooth on the downsampled scan. Guided by the segmentation
outputs, our TS-MDL further selects each tooth's region of interest (ROI) on
the original mesh to construct a light-weight variant of the pioneering
PointNet (i.e., PointNet-Reg) for regressing the corresponding landmark
heatmaps. Our TS-MDL was evaluated on a real-clinical dataset, showing
promising segmentation and localization performance. Specifically,
\emph{i}MeshSegNet in the first stage of TS-MDL reached an averaged Dice
similarity coefficient (DSC) at $0.953\pm0.076$, significantly outperforming
the original MeshSegNet. In the second stage, PointNet-Reg achieved a mean
absolute error (MAE) of $0.623\pm0.718 \, mm$ in distances between the
prediction and ground truth for $44$ landmarks, which is superior compared with
other networks for landmark detection. All these results suggest the potential
usage of our TS-MDL in clinical practices.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Scene Graphs for Audio Source Separation. (arXiv:2109.11955v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11955">
<div class="article-summary-box-inner">
<span><p>State-of-the-art approaches for visually-guided audio source separation
typically assume sources that have characteristic sounds, such as musical
instruments. These approaches often ignore the visual context of these sound
sources or avoid modeling object interactions that may be useful to better
characterize the sources, especially when the same object class may produce
varied sounds from distinct interactions. To address this challenging problem,
we propose Audio Visual Scene Graph Segmenter (AVSGS), a novel deep learning
model that embeds the visual structure of the scene as a graph and segments
this graph into subgraphs, each subgraph being associated with a unique sound
obtained by co-segmenting the audio spectrogram. At its core, AVSGS uses a
recursive neural network that emits mutually-orthogonal sub-graph embeddings of
the visual graph using multi-head attention. These embeddings are used for
conditioning an audio encoder-decoder towards source separation. Our pipeline
is trained end-to-end via a self-supervised task consisting of separating audio
sources using the visual graph from artificially mixed sounds. In this paper,
we also introduce an "in the wild'' video dataset for sound source separation
that contains multiple non-musical sources, which we call Audio Separation in
the Wild (ASIW). This dataset is adapted from the AudioCaps dataset, and
provides a challenging, natural, and daily-life setting for source separation.
Thorough experiments on the proposed ASIW and the standard MUSIC datasets
demonstrate state-of-the-art sound separation performance of our method against
recent prior approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantitative Matching of Forensic Evidence Fragments Utilizing 3D Microscopy Analysis of Fracture Surface Replicas. (arXiv:2109.11972v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11972">
<div class="article-summary-box-inner">
<span><p>Fractured surfaces carry unique details that can provide an accurate
quantitative comparison to support comparative forensic analysis of those
fractured surfaces. In this study, a statistical analysis comparison protocol
was applied to a set of 3D topological images of fractured surface pairs and
their replicas to provide confidence in the quantitative statistical comparison
between fractured items and their replicas. A set of 10 fractured stainless
steel samples was fractured from the same metal rod under controlled conditions
and were cast using a standard forensic casting technique. Six 3D topological
maps with 50% overlap were acquired for each fractured pair. Spectral analysis
was utilized to identify the correlation between topological surface features
at different length scales of the surface topology. We selected two frequency
bands over the critical wavelength (which is greater than two-grain diameters)
for statistical comparison. Our statistical model utilized a matrix-variate-$t$
distribution that accounts for the image-overlap to model the match and
non-match population densities. A decision rule was developed to identify the
probability of matched and unmatched pairs of surfaces. The proposed
methodology correctly classified the fractured steel surfaces and their
replicas with a posterior probability of match exceeding 99.96%. Moreover, the
replication technique shows the potential to accurately replicate fracture
surface topological details with a wavelength greater than 20$\mu$m, which far
exceeds the range for comparison of most metallic alloys of 50-200$\mu$m. The
developed framework establishes the basis of forensic comparison of fractured
articles and their replicas while providing a reliable quantitative statistical
forensic comparison, utilizing fracture mechanics-based analysis of the
fracture surface topology.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From images in the wild to video-informed image classification. (arXiv:2109.12040v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12040">
<div class="article-summary-box-inner">
<span><p>Image classifiers work effectively when applied on structured images, yet
they often fail when applied on images with very high visual complexity. This
paper describes experiments applying state-of-the-art object classifiers toward
a unique set of images in the wild with high visual complexity collected on the
island of Bali. The text describes differences between actual images in the
wild and images from Imagenet, and then discusses a novel approach combining
informational cues particular to video with an ensemble of imperfect
classifiers in order to improve classification results on video sourced images
of plants in the wild.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepStroke: An Efficient Stroke Screening Framework for Emergency Rooms with Multimodal Adversarial Deep Learning. (arXiv:2109.12065v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12065">
<div class="article-summary-box-inner">
<span><p>In an emergency room (ER) setting, the diagnosis of stroke is a common
challenge. Due to excessive execution time and cost, an MRI scan is usually not
available in the ER. Clinical tests are commonly referred to in stroke
screening, but neurologists may not be immediately available. We propose a
novel multimodal deep learning framework, DeepStroke, to achieve computer-aided
stroke presence assessment by recognizing the patterns of facial motion
incoordination and speech inability for patients with suspicion of stroke in an
acute setting. Our proposed DeepStroke takes video data for local facial
paralysis detection and audio data for global speech disorder analysis. It
further leverages a multi-modal lateral fusion to combine the low- and
high-level features and provides mutual regularization for joint training. A
novel adversarial training loss is also introduced to obtain
identity-independent and stroke-discriminative features. Experiments on our
video-audio dataset with actual ER patients show that the proposed approach
outperforms state-of-the-art models and achieves better performance than ER
doctors, attaining a 6.60% higher sensitivity and maintaining 4.62% higher
accuracy when specificity is aligned. Meanwhile, each assessment can be
completed in less than 6 minutes, demonstrating the framework's great potential
for clinical implementation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ZSD-YOLO: Zero-Shot YOLO Detection using Vision-Language KnowledgeDistillation. (arXiv:2109.12066v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12066">
<div class="article-summary-box-inner">
<span><p>Real-world object sampling produces long-tailed distributions requiring
exponentially more images for rare types. Zero-shot detection, which aims to
detect unseen objects, is one direction to address this problem. A dataset such
as COCO is extensively annotated across many images but with a sparse number of
categories and annotating all object classes across a diverse domain is
expensive and challenging. To advance zero-shot detection, we develop a
Vision-Language distillation method that aligns both image and text embeddings
from a zero-shot pre-trained model such as CLIP to a modified semantic
prediction head from a one-stage detector like YOLOv5. With this method, we are
able to train an object detector that achieves state-of-the-art accuracy on the
COCO zero-shot detection splits with fewer model parameters. During inference,
our model can be adapted to detect any number of object classes without
additional training. We also find that the improvements provided by the scaling
of our method are consistent across various YOLOv5 scales. Furthermore, we
develop a self-training method that provides a significant score improvement
without needing extra images nor labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLIPort: What and Where Pathways for Robotic Manipulation. (arXiv:2109.12098v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12098">
<div class="article-summary-box-inner">
<span><p>How can we imbue robots with the ability to manipulate objects precisely but
also to reason about them in terms of abstract concepts? Recent works in
manipulation have shown that end-to-end networks can learn dexterous skills
that require precise spatial reasoning, but these methods often fail to
generalize to new goals or quickly learn transferable concepts across tasks. In
parallel, there has been great progress in learning generalizable semantic
representations for vision and language by training on large-scale internet
data, however these representations lack the spatial understanding necessary
for fine-grained manipulation. To this end, we propose a framework that
combines the best of both worlds: a two-stream architecture with semantic and
spatial pathways for vision-based manipulation. Specifically, we present
CLIPort, a language-conditioned imitation-learning agent that combines the
broad semantic understanding (what) of CLIP [1] with the spatial precision
(where) of Transporter [2]. Our end-to-end framework is capable of solving a
variety of language-specified tabletop tasks from packing unseen objects to
folding cloths, all without any explicit representations of object poses,
instance segmentations, memory, symbolic states, or syntactic structures.
Experiments in simulated and real-world settings show that our approach is data
efficient in few-shot settings and generalizes effectively to seen and unseen
semantic concepts. We even learn one multi-task policy for 10 simulated and 9
real-world tasks that is better or comparable to single-task policies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ImplicitVol: Sensorless 3D Ultrasound Reconstruction with Deep Implicit Representation. (arXiv:2109.12108v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12108">
<div class="article-summary-box-inner">
<span><p>The objective of this work is to achieve sensorless reconstruction of a 3D
volume from a set of 2D freehand ultrasound images with deep implicit
representation. In contrast to the conventional way that represents a 3D volume
as a discrete voxel grid, we do so by parameterizing it as the zero level-set
of a continuous function, i.e. implicitly representing the 3D volume as a
mapping from the spatial coordinates to the corresponding intensity values. Our
proposed model, termed as ImplicitVol, takes a set of 2D scans and their
estimated locations in 3D as input, jointly re?fing the estimated 3D locations
and learning a full reconstruction of the 3D volume. When testing on real 2D
ultrasound images, novel cross-sectional views that are sampled from
ImplicitVol show significantly better visual quality than those sampled from
existing reconstruction approaches, outperforming them by over 30% (NCC and
SSIM), between the output and ground-truth on the 3D volume testing data. The
code will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Goal-Conditioned End-to-End Visuomotor Control for Versatile Skill Primitives. (arXiv:2003.08854v3 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.08854">
<div class="article-summary-box-inner">
<span><p>Visuomotor control (VMC) is an effective means of achieving basic
manipulation tasks such as pushing or pick-and-place from raw images.
Conditioning VMC on desired goal states is a promising way of achieving
versatile skill primitives. However, common conditioning schemes either rely on
task-specific fine tuning - e.g. using one-shot imitation learning (IL) - or on
sampling approaches using a forward model of scene dynamics i.e.
model-predictive control (MPC), leaving deployability and planning horizon
severely limited. In this paper we propose a conditioning scheme which avoids
these pitfalls by learning the controller and its conditioning in an end-to-end
manner. Our model predicts complex action sequences based directly on a dynamic
image representation of the robot motion and the distance to a given target
observation. In contrast to related works, this enables our approach to
efficiently perform complex manipulation tasks from raw image observations
without predefined control primitives or test time demonstrations. We report
significant improvements in task success over representative MPC and IL
baselines. We also demonstrate our model's generalisation capabilities in
challenging, unseen tasks featuring visual noise, cluttered scenes and unseen
object geometries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scan-based Semantic Segmentation of LiDAR Point Clouds: An Experimental Study. (arXiv:2004.11803v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.11803">
<div class="article-summary-box-inner">
<span><p>Autonomous vehicles need to have a semantic understanding of the
three-dimensional world around them in order to reason about their environment.
State of the art methods use deep neural networks to predict semantic classes
for each point in a LiDAR scan. A powerful and efficient way to process LiDAR
measurements is to use two-dimensional, image-like projections. In this work,
we perform a comprehensive experimental study of image-based semantic
segmentation architectures for LiDAR point clouds. We demonstrate various
techniques to boost the performance and to improve runtime as well as memory
constraints.
</p>
<p>First, we examine the effect of network size and suggest that much faster
inference times can be achieved at a very low cost to accuracy. Next, we
introduce an improved point cloud projection technique that does not suffer
from systematic occlusions. We use a cyclic padding mechanism that provides
context at the horizontal field-of-view boundaries. In a third part, we perform
experiments with a soft Dice loss function that directly optimizes for the
intersection-over-union metric. Finally, we propose a new kind of convolution
layer with a reduced amount of weight-sharing along one of the two spatial
dimensions, addressing the large difference in appearance along the vertical
axis of a LiDAR scan. We propose a final set of the above methods with which
the model achieves an increase of 3.2% in mIoU segmentation performance over
the baseline while requiring only 42% of the original inference time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Triplet Loss: Meta Prototypical N-tuple Loss for Person Re-identification. (arXiv:2006.04991v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.04991">
<div class="article-summary-box-inner">
<span><p>Person Re-identification (ReID) aims at matching a person of interest across
images. In convolutional neural network (CNN) based approaches, loss design
plays a vital role in pulling closer features of the same identity and pushing
far apart features of different identities. In recent years, triplet loss
achieves superior performance and is predominant in ReID. However, triplet loss
considers only three instances of two classes in per-query optimization (with
an anchor sample as query) and it is actually equivalent to a two-class
classification. There is a lack of loss design which enables the joint
optimization of multiple instances (of multiple classes) within per-query
optimization for person ReID. In this paper, we introduce a multi-class
classification loss, i.e., N-tuple loss, to jointly consider multiple (N)
instances for per-query optimization. This in fact aligns better with the ReID
test/inference process, which conducts the ranking/comparisons among multiple
instances. Furthermore, for more efficient multi-class classification, we
propose a new meta prototypical N-tuple loss. With the multi-class
classification incorporated, our model achieves the state-of-the-art
performance on the benchmark person ReID datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Kronecker CP Decomposition with Fast Multiplication for Compressing RNNs. (arXiv:2008.09342v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.09342">
<div class="article-summary-box-inner">
<span><p>Recurrent neural networks (RNNs) are powerful in the tasks oriented to
sequential data, such as natural language processing and video recognition.
However, since the modern RNNs, including long-short term memory (LSTM) and
gated recurrent unit (GRU) networks, have complex topologies and expensive
space/computation complexity, compressing them becomes a hot and promising
topic in recent years. Among plenty of compression methods, tensor
decomposition, e.g., tensor train (TT), block term (BT), tensor ring (TR) and
hierarchical Tucker (HT), appears to be the most amazing approach since a very
high compression ratio might be obtained. Nevertheless, none of these tensor
decomposition formats can provide both the space and computation efficiency. In
this paper, we consider to compress RNNs based on a novel Kronecker
CANDECOMP/PARAFAC (KCP) decomposition, which is derived from Kronecker tensor
(KT) decomposition, by proposing two fast algorithms of multiplication between
the input and the tensor-decomposed weight. According to our experiments based
on UCF11, Youtube Celebrities Face and UCF50 datasets, it can be verified that
the proposed KCP-RNNs have comparable performance of accuracy with those in
other tensor-decomposed formats, and even 278,219x compression ratio could be
obtained by the low rank KCP. More importantly, KCP-RNNs are efficient in both
space and computation complexity compared with other tensor-decomposed ones
under similar ranks. Besides, we find KCP has the best potential for parallel
computing to accelerate the calculations in neural networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AFP-SRC: Identification of Antifreeze Proteins Using Sparse Representation Classifier. (arXiv:2009.05277v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.05277">
<div class="article-summary-box-inner">
<span><p>Species living in the extreme cold environment fight against the harsh
conditions using antifreeze proteins (AFPs), that manipulates the freezing
mechanism of water in more than one way. This amazing nature of AFP turns out
to be extremely useful in several industrial and medical applications. The lack
of similarity in their structure and sequence makes their prediction an arduous
task and identifying them experimentally in the wet-lab is time-consuming and
expensive. In this research, we propose a computational framework for the
prediction of AFPs which is essentially based on a sample-specific
classification method using the sparse reconstruction. A linear model and an
over-complete dictionary matrix of known AFPs are used to predict a sparse
class-label vector that provides a sample-association score. Delta-rule is
applied for the reconstruction of two pseudo-samples using lower and upper
parts of the sample-association vector and based on the minimum recovery score,
class labels are assigned. We compare our approach with contemporary methods on
a standard dataset and the proposed method is found to outperform in terms of
Balanced accuracy and Youden's index. The MATLAB implementation of the proposed
method is available at the author's GitHub page
(\{https://github.com/Shujaat123/AFP-SRC}{https://github.com/Shujaat123/AFP-SRC}).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is First Person Vision Challenging for Object Tracking?. (arXiv:2011.12263v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.12263">
<div class="article-summary-box-inner">
<span><p>Understanding human-object interactions is fundamental in First Person Vision
(FPV). Tracking algorithms which follow the objects manipulated by the camera
wearer can provide useful cues to effectively model such interactions. Despite
a few previous attempts to exploit trackers in FPV applications, a methodical
analysis of the performance of state-of-the-art visual trackers in this domain
is still missing. In this short paper, we provide a recap of the first
systematic study of object tracking in FPV. Our work extensively analyses the
performance of recent and baseline FPV trackers with respect to different
aspects. This is achieved through TREK-150, a novel benchmark dataset composed
of 150 densely annotated video sequences. The results suggest that more
research efforts should be devoted to this problem so that tracking could
benefit FPV tasks. The full version of this paper is available at
<a href="/abs/2108.13665">arXiv:2108.13665</a>.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepKoCo: Efficient latent planning with a task-relevant Koopman representation. (arXiv:2011.12690v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.12690">
<div class="article-summary-box-inner">
<span><p>This paper presents DeepKoCo, a novel model-based agent that learns a latent
Koopman representation from images. This representation allows DeepKoCo to plan
efficiently using linear control methods, such as linear model predictive
control. Compared to traditional agents, DeepKoCo learns task-relevant
dynamics, thanks to the use of a tailored lossy autoencoder network that allows
DeepKoCo to learn latent dynamics that reconstruct and predict only observed
costs, rather than all observed dynamics. As our results show, DeepKoCo
achieves similar final performance as traditional model-free methods on complex
control tasks while being considerably more robust to distractor dynamics,
making the proposed agent more amenable for real-life applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Faster Convergence in Deep-Predictive-Coding Networks to Learn Deeper Representations. (arXiv:2101.06848v4 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.06848">
<div class="article-summary-box-inner">
<span><p>Deep-predictive-coding networks (DPCNs) are hierarchical, generative models.
They rely on feed-forward and feed-back connections to modulate latent feature
representations of stimuli in a dynamic and context-sensitive manner. A crucial
element of DPCNs is a forward-backward inference procedure to uncover sparse,
invariant features. However, this inference is a major computational
bottleneck. It severely limits the network depth due to learning stagnation.
Here, we prove why this bottleneck occurs. We then propose a new
forward-inference strategy based on accelerated proximal gradients. This
strategy has faster theoretical convergence guarantees than the one used for
DPCNs. It overcomes learning stagnation. We also demonstrate that it permits
constructing deep and wide predictive-coding networks. Such convolutional
networks implement receptive fields that capture well the entire classes of
objects on which the networks are trained. This improves the feature
representations compared with our lab's previous non-convolutional and
convolutional DPCNs. It yields unsupervised object recognition that surpass
convolutional autoencoders and are on par with convolutional networks trained
in a supervised manner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MISA: Online Defense of Trojaned Models using Misattributions. (arXiv:2103.15918v2 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.15918">
<div class="article-summary-box-inner">
<span><p>Recent studies have shown that neural networks are vulnerable to Trojan
attacks, where a network is trained to respond to specially crafted trigger
patterns in the inputs in specific and potentially malicious ways. This paper
proposes MISA, a new online approach to detect Trojan triggers for neural
networks at inference time. Our approach is based on a novel notion called
misattributions, which captures the anomalous manifestation of a Trojan
activation in the feature space. Given an input image and the corresponding
output prediction, our algorithm first computes the model's attribution on
different features. It then statistically analyzes these attributions to
ascertain the presence of a Trojan trigger. Across a set of benchmarks, we show
that our method can effectively detect Trojan triggers for a wide variety of
trigger patterns, including several recent ones for which there are no known
defenses. Our method achieves 96% AUC for detecting images that include a
Trojan trigger without any assumptions on the trigger pattern.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MRI-based Alzheimer's disease prediction via distilling the knowledge in multi-modal data. (arXiv:2104.03618v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.03618">
<div class="article-summary-box-inner">
<span><p>Mild cognitive impairment (MCI) conversion prediction, i.e., identifying MCI
patients of high risks converting to Alzheimer's disease (AD), is essential for
preventing or slowing the progression of AD. Although previous studies have
shown that the fusion of multi-modal data can effectively improve the
prediction accuracy, their applications are largely restricted by the limited
availability or high cost of multi-modal data. Building an effective prediction
model using only magnetic resonance imaging (MRI) remains a challenging
research topic. In this work, we propose a multi-modal multi-instance
distillation scheme, which aims to distill the knowledge learned from
multi-modal data to an MRI-based network for MCI conversion prediction. In
contrast to existing distillation algorithms, the proposed multi-instance
probabilities demonstrate a superior capability of representing the complicated
atrophy distributions, and can guide the MRI-based network to better explore
the input MRI. To our best knowledge, this is the first study that attempts to
improve an MRI-based prediction model by leveraging extra supervision distilled
from multi-modal information. Experiments demonstrate the advantage of our
framework, suggesting its potentials in the data-limited clinical settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Imaginative Walks: Generative Random Walk Deviation Loss for Improved Unseen Learning Representation. (arXiv:2104.09757v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.09757">
<div class="article-summary-box-inner">
<span><p>We propose a novel loss for generative models, dubbed as GRaWD (Generative
Random Walk Deviation), to improve learning representations of unexplored
visual spaces. Quality learning representation of unseen classes (or styles) is
critical to facilitate novel image generation and better generative
understanding of unseen visual classes, i.e., zero-shot learning (ZSL). By
generating representations of unseen classes based on their semantic
descriptions, e.g., attributes or text, generative ZSL attempts to
differentiate unseen from seen categories. The proposed GRaWD loss is defined
by constructing a dynamic graph that includes the seen class/style centers and
generated samples in the current minibatch. Our loss initiates a random walk
probability from each center through visual generations produced from
hallucinated unseen classes. As a deviation signal, we encourage the random
walk to eventually land after t steps in a feature representation that is
difficult to classify as any of the seen classes. We demonstrate that the
proposed loss can improve unseen class representation quality inductively on
text-based ZSL benchmarks on CUB and NABirds datasets and attribute-based ZSL
benchmarks on AWA2, SUN, and aPY datasets. In addition, we investigate the
ability of the proposed loss to generate meaningful novel visual art on the
WikiArt dataset. The results of experiments and human evaluations demonstrate
that the proposed GRaWD loss can improve StyleGAN1 and StyleGAN2 generation
quality and create novel art that is significantly more preferable. Our code is
made publicly available at https://github.com/Vision-CAIR/GRaWD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection. (arXiv:2104.10956v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.10956">
<div class="article-summary-box-inner">
<span><p>Monocular 3D object detection is an important task for autonomous driving
considering its advantage of low cost. It is much more challenging than
conventional 2D cases due to its inherent ill-posed property, which is mainly
reflected in the lack of depth information. Recent progress on 2D detection
offers opportunities to better solving this problem. However, it is non-trivial
to make a general adapted 2D detector work in this 3D task. In this paper, we
study this problem with a practice built on a fully convolutional single-stage
detector and propose a general framework FCOS3D. Specifically, we first
transform the commonly defined 7-DoF 3D targets to the image domain and
decouple them as 2D and 3D attributes. Then the objects are distributed to
different feature levels with consideration of their 2D scales and assigned
only according to the projected 3D-center for the training procedure.
Furthermore, the center-ness is redefined with a 2D Gaussian distribution based
on the 3D-center to fit the 3D target formulation. All of these make this
framework simple yet effective, getting rid of any 2D detection or 2D-3D
correspondence priors. Our solution achieves 1st place out of all the
vision-only methods in the nuScenes 3D detection challenge of NeurIPS 2020.
Code and models are released at https://github.com/open-mmlab/mmdetection3d.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting Light-Weight Depth Estimation Via Knowledge Distillation. (arXiv:2105.06143v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.06143">
<div class="article-summary-box-inner">
<span><p>The advanced performance of depth estimation is achieved by the employment of
large and complex neural networks. While the performance is still being
continuously improved, we argue that the depth estimation has to be efficient
as well since it is a preliminary requirement for real-world applications.
However, fast depth estimation tends to lower the performance as the trade-off
between the model's capacity and accuracy. In this paper, we aim to achieve
accurate depth estimation with a light-weight network. To this end, we first
introduce a highly compact network that can estimate a depth map in real-time.
We then develop a knowledge distillation paradigm to further improve the
performance. We observe that many scenarios have the same scene scales in
real-world, yielding similar depth histograms, thus they are potentially
valuable and applicable to develop a better learning strategy. Therefore, we
propose to employ auxiliary unlabeled/labeled data to improve knowledge
distillation. Through extensive and rigorous experiments, we show that our
method can achieve comparable performance against state-of-the-of-art methods
with only 1% parameters, and outperforms previous light-weight methods in terms
of inference accuracy, computational efficiency and generalizability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Orthogonal Classifier for Improving the Adversarial Robustness of Neural Networks. (arXiv:2105.09109v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.09109">
<div class="article-summary-box-inner">
<span><p>Neural networks are susceptible to artificially designed adversarial
perturbations. Recent efforts have shown that imposing certain modifications on
classification layer can improve the robustness of the neural networks. In this
paper, we explicitly construct a dense orthogonal weight matrix whose entries
have the same magnitude, thereby leading to a novel robust classifier. The
proposed classifier avoids the undesired structural redundancy issue in
previous work. Applying this classifier in standard training on clean data is
sufficient to ensure the high accuracy and good robustness of the model.
Moreover, when extra adversarial samples are used, better robustness can be
further obtained with the help of a special worst-case loss. Experimental
results show that our method is efficient and competitive to many
state-of-the-art defensive approaches. Our code is available at
\url{https://github.com/MTandHJ/roboc}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SHD360: A Benchmark Dataset for Salient Human Detection in 360{\deg} Videos. (arXiv:2105.11578v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.11578">
<div class="article-summary-box-inner">
<span><p>Salient human detection (SHD) in dynamic 360{\deg} immersive videos is of
great importance for various applications such as robotics, inter-human and
human-object interaction in augmented reality. However, 360{\deg} video SHD has
been seldom discussed in the computer vision community due to a lack of
datasets with large-scale omnidirectional videos and rich annotations. To this
end, we propose SHD360, the first 360{\deg} video SHD dataset which contains
various real-life daily scenes. Our SHD360 provides six-level hierarchical
annotations for 6,268 key frames uniformly sampled from 37,403 omnidirectional
video frames at 4K resolution. Specifically, each collected frame is labeled
with a super-class, a sub-class, associated attributes (e.g., geometrical
distortion), bounding boxes and per-pixel object-/instance-level masks. As a
result, our SHD360 contains totally 16,238 salient human instances with
manually annotated pixel-wise ground truth. Since so far there is no method
proposed for 360{\deg} image/video SHD, we systematically benchmark 11
representative state-of-the-art salient object detection (SOD) approaches on
our SHD360, and explore key issues derived from extensive experimenting
results. We hope our proposed dataset and benchmark could serve as a good
starting point for advancing human-centric researches towards 360{\deg}
panoramic data. Our dataset and benchmark is publicly available at
https://github.com/PanoAsh/SHD360.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-StyleGAN: Towards Image-Based Simulation of Time-Lapse Live-Cell Microscopy. (arXiv:2106.08285v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.08285">
<div class="article-summary-box-inner">
<span><p>Time-lapse fluorescent microscopy (TLFM) combined with predictive
mathematical modelling is a powerful tool to study the inherently dynamic
processes of life on the single-cell level. Such experiments are costly,
complex and labour intensive. A complimentary approach and a step towards in
silico experimentation, is to synthesise the imagery itself. Here, we propose
Multi-StyleGAN as a descriptive approach to simulate time-lapse fluorescence
microscopy imagery of living cells, based on a past experiment. This novel
generative adversarial network synthesises a multi-domain sequence of
consecutive timesteps. We showcase Multi-StyleGAN on imagery of multiple live
yeast cells in microstructured environments and train on a dataset recorded in
our laboratory. The simulation captures underlying biophysical factors and time
dependencies, such as cell morphology, growth, physical interactions, as well
as the intensity of a fluorescent reporter protein. An immediate application is
to generate additional training and validation data for feature extraction
algorithms or to aid and expedite development of advanced experimental
techniques such as online monitoring or control of cells.
</p>
<p>Code and dataset is available at
https://git.rwth-aachen.de/bcs/projects/tp/multi-stylegan.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond the Hausdorff Metric in Digital Topology. (arXiv:2108.03114v2 [cs.CG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03114">
<div class="article-summary-box-inner">
<span><p>Two objects may be close in the Hausdorff metric, yet have very different
geometric and topological properties. We examine other methods of comparing
digital images such that objects close in each of these measures have some
similar geometric or topological property. Such measures may be combined with
the Hausdorff metric to yield a metric in which close images are similar with
respect to multiple properties.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards to Robust and Generalized Medical Image Segmentation Framework. (arXiv:2108.03823v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03823">
<div class="article-summary-box-inner">
<span><p>To mitigate the radiologist's workload, computer-aided diagnosis with the
capability to review and analyze medical images is gradually deployed. Deep
learning-based region of interest segmentation is among the most exciting use
cases. However, this paradigm is restricted in real-world clinical applications
due to poor robustness and generalization. The issue is more sinister with a
lack of training data. In this paper, we address the challenge from the
representation learning point of view. We investigate that the collapsed
representations, as one of the main reasons which caused poor robustness and
generalization, could be avoided through transfer learning. Therefore, we
propose a novel two-stage framework for robust generalized segmentation. In
particular, an unsupervised Tile-wise AutoEncoder (T-AE) pretraining
architecture is coined to learn meaningful representation for improving the
generalization and robustness of the downstream tasks. Furthermore, the learned
knowledge is transferred to the segmentation benchmark. Coupled with an image
reconstruction network, the representation keeps to be decoded, encouraging the
model to capture more semantic features. Experiments of lung segmentation on
multi chest X-ray datasets are conducted. Empirically, the related experimental
results demonstrate the superior generalization capability of the proposed
framework on unseen domains in terms of high performance and robustness to
corruption, especially under the scenario of the limited training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">White blood cell subtype detection and classification. (arXiv:2108.04614v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04614">
<div class="article-summary-box-inner">
<span><p>Machine learning has endless applications in the health care industry. White
blood cell classification is one of the interesting and promising area of
research. The classification of the white blood cells plays an important part
in the medical diagnosis. In practise white blood cell classification is
performed by the haematologist by taking a small smear of blood and careful
examination under the microscope. The current procedures to identify the white
blood cell subtype is more time taking and error-prone. The computer aided
detection and diagnosis of the white blood cells tend to avoid the human error
and reduce the time taken to classify the white blood cells. In the recent
years several deep learning approaches have been developed in the context of
classification of the white blood cells that are able to identify but are
unable to localize the positions of white blood cells in the blood cell image.
Following this, the present research proposes to utilize YOLOv3 object
detection technique to localize and classify the white blood cells with
bounding boxes. With exhaustive experimental analysis, the proposed work is
found to detect the white blood cell with 99.2% accuracy and classify with 90%
accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReGenMorph: Visibly Realistic GAN Generated Face Morphing Attacks by Attack Re-generation. (arXiv:2108.09130v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09130">
<div class="article-summary-box-inner">
<span><p>Face morphing attacks aim at creating face images that are verifiable to be
the face of multiple identities, which can lead to building faulty identity
links in operations like border checks. While creating a morphed face detector
(MFD), training on all possible attack types is essential to achieve good
detection performance. Therefore, investigating new methods of creating
morphing attacks drives the generalizability of MADs. Creating morphing attacks
was performed on the image level, by landmark interpolation, or on the
latent-space level, by manipulating latent vectors in a generative adversarial
network. The earlier results in varying blending artifacts and the latter
results in synthetic-like striping artifacts. This work presents the novel
morphing pipeline, ReGenMorph, to eliminate the LMA blending artifacts by using
a GAN-based generation, as well as, eliminate the manipulation in the latent
space, resulting in visibly realistic morphed images compared to previous
works. The generated ReGenMorph appearance is compared to recent morphing
approaches and evaluated for face recognition vulnerability and attack
detectability, whether as known or unknown attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual-Neighborhood Deep Fusion Network for Point Cloud Classification. (arXiv:2108.09228v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09228">
<div class="article-summary-box-inner">
<span><p>Recently, deep neural networks have made remarkable achievements in 3D point
cloud classification. However, existing classification methods are mainly
implemented on idealized point clouds and suffer heavy degradation of
per-formance on non-idealized scenarios. To handle this prob-lem, a feature
representation learning method, named Dual-Neighborhood Deep Fusion Network
(DNDFN), is proposed to serve as an improved point cloud encoder for the task
of non-idealized point cloud classification. DNDFN utilizes a trainable
neighborhood learning method called TN-Learning to capture the global key
neighborhood. Then, the global neighborhood is fused with the local
neighbor-hood to help the network achieve more powerful reasoning ability.
Besides, an Information Transfer Convolution (IT-Conv) is proposed for DNDFN to
learn the edge infor-mation between point-pairs and benefits the feature
transfer procedure. The transmission of information in IT-Conv is similar to
the propagation of information in the graph which makes DNDFN closer to the
human reasoning mode. Extensive experiments on existing benchmarks especially
non-idealized datasets verify the effectiveness of DNDFN and DNDFN achieves the
state of the arts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Well Googled is Half Done: Multimodal Forecasting of New Fashion Product Sales with Image-based Google Trends. (arXiv:2109.09824v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09824">
<div class="article-summary-box-inner">
<span><p>This paper investigates the effectiveness of systematically probing Google
Trendsagainst textual translations of visual aspects as exogenous knowledge to
predict the sales of brand-new fashion items, where past sales data is not
available, but only an image and few metadata are available. In particular, we
propose GTM-Transformer, standing for Google Trends Multimodal Transformer,
whose encoder works on the representation of the exogenous time series, while
the decoder forecasts the sales using the Google Trends encoding, and the
available visual and metadata information. Our model works in a
non-autoregressive manner, avoiding the compounding effect of the first-step
errors. As a second contribution, we present the VISUELLE dataset, which is the
first publicly available dataset for the task of new fashion product sales
forecasting, containing the sales of 5577 new products sold between 2016-2019,
derived from genuine historical data ofNunalie, an Italian fast-fashion
company. Our dataset is equipped with images of products, metadata, related
sales, and associated Google Trends. We use VISUELLE to compare our approach
against state-of-the-art alternatives and numerous baselines, showing that
GTM-Transformer is the most accurate in terms of both percentage and absolute
error. It is worth noting that the addition of exogenous knowledge boosts the
forecasting accuracy by 1.5% WAPE wise, showing the importance of exploiting
Google Trends. The code and dataset are both available at
https://github.com/HumaticsLAB/GTM-Transformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Live Speech Portraits: Real-Time Photorealistic Talking-Head Animation. (arXiv:2109.10595v2 [cs.GR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10595">
<div class="article-summary-box-inner">
<span><p>To the best of our knowledge, we first present a live system that generates
personalized photorealistic talking-head animation only driven by audio signals
at over 30 fps. Our system contains three stages. The first stage is a deep
neural network that extracts deep audio features along with a manifold
projection to project the features to the target person's speech space. In the
second stage, we learn facial dynamics and motions from the projected audio
features. The predicted motions include head poses and upper body motions,
where the former is generated by an autoregressive probabilistic model which
models the head pose distribution of the target person. Upper body motions are
deduced from head poses. In the final stage, we generate conditional feature
maps from previous predictions and send them with a candidate image set to an
image-to-image translation network to synthesize photorealistic renderings. Our
method generalizes well to wild audio and successfully synthesizes
high-fidelity personalized facial details, e.g., wrinkles, teeth. Our method
also allows explicit control of head poses. Extensive qualitative and
quantitative evaluations, along with user studies, demonstrate the superiority
of our method over state-of-the-art techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HybridSDF: Combining Free Form Shapes and Geometric Primitives for effective Shape Manipulation. (arXiv:2109.10767v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10767">
<div class="article-summary-box-inner">
<span><p>CAD modeling typically involves the use of simple geometric primitives
whereas recent advances in deep-learning based 3D surface modeling have opened
new shape design avenues. Unfortunately, these advances have not yet been
accepted by the CAD community because they cannot be integrated into
engineering workflows. To remedy this, we propose a novel approach to
effectively combining geometric primitives and free-form surfaces represented
by implicit surfaces for accurate modeling that preserves interpretability,
enforces consistency, and enables easy manipulation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mixed-supervised segmentation: Confidence maximization helps knowledge distillation. (arXiv:2109.10902v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10902">
<div class="article-summary-box-inner">
<span><p>Despite achieving promising results in a breadth of medical image
segmentation tasks, deep neural networks require large training datasets with
pixel-wise annotations. Obtaining these curated datasets is a cumbersome
process which limits the application in scenarios where annotated images are
scarce. Mixed supervision is an appealing alternative for mitigating this
obstacle, where only a small fraction of the data contains complete pixel-wise
annotations and other images have a weaker form of supervision. In this work,
we propose a dual-branch architecture, where the upper branch (teacher)
receives strong annotations, while the bottom one (student) is driven by
limited supervision and guided by the upper branch. Combined with a standard
cross-entropy loss over the labeled pixels, our novel formulation integrates
two important terms: (i) a Shannon entropy loss defined over the
less-supervised images, which encourages confident student predictions in the
bottom branch; and (ii) a Kullback-Leibler (KL) divergence term, which
transfers the knowledge of the strongly supervised branch to the
less-supervised branch and guides the entropy (student-confidence) term to
avoid trivial solutions. We show that the synergy between the entropy and KL
divergence yields substantial improvements in performance. We also discuss an
interesting link between Shannon-entropy minimization and standard pseudo-mask
generation, and argue that the former should be preferred over the latter for
leveraging information from unlabeled pixels. Quantitative and qualitative
results on two publicly available datasets demonstrate that our method
significantly outperforms other strategies for semantic segmentation within a
mixed-supervision framework, as well as recent semi-supervised approaches.
Moreover, we show that the branch trained with reduced supervision and guided
by the top branch largely outperforms the latter.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recent Advances of Continual Learning in Computer Vision: An Overview. (arXiv:2109.11369v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11369">
<div class="article-summary-box-inner">
<span><p>In contrast to batch learning where all training data is available at once,
continual learning represents a family of methods that accumulate knowledge and
learn continuously with data available in sequential order. Similar to the
human learning process with the ability of learning, fusing, and accumulating
new knowledge coming at different time steps, continual learning is considered
to have high practical significance. Hence, continual learning has been studied
in various artificial intelligence tasks. In this paper, we present a
comprehensive review of the recent progress of continual learning in computer
vision. In particular, the works are grouped by their representative
techniques, including regularization, knowledge distillation, memory,
generative replay, parameter isolation, and a combination of the above
techniques. For each category of these techniques, both its characteristics and
applications in computer vision are presented. At the end of this overview,
several subareas, where continuous knowledge accumulation is potentially
helpful while continual learning has not been well studied, are discussed.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-09-27 23:02:16.509791175 UTC">2021-09-27 23:02:16 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.3</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>