<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-12-22T01:30:00Z">12-22</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">DB-BERT: a Database Tuning Tool that "Reads the Manual". (arXiv:2112.10925v1 [cs.DB])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10925">
<div class="article-summary-box-inner">
<span><p>DB-BERT is a database tuning tool that exploits information gained via
natural language analysis of manuals and other relevant text documents. It uses
text to identify database system parameters to tune as well as recommended
parameter values. DB-BERT applies large, pre-trained language models
(specifically, the BERT model) for text analysis. During an initial training
phase, it fine-tunes model weights in order to translate natural language hints
into recommended settings. At run time, DB-BERT learns to aggregate, adapt, and
prioritize hints to achieve optimal performance for a specific database system
and benchmark. Both phases are iterative and use reinforcement learning to
guide the selection of tuning settings to evaluate (penalizing settings that
the database system rejects while rewarding settings that improve performance).
In our experiments, we leverage hundreds of text documents about database
tuning as input for DB-BERT. We compare DB-BERT against various baselines,
considering different benchmarks (TPC-C and TPC-H), metrics (throughput and run
time), as well as database systems (Postgres and MySQL). In all cases, DB-BERT
finds the best parameter settings among all compared methods. The code of
DB-BERT is available online at https://itrummer.github.io/dbbert/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Watch Those Words: Video Falsification Detection Using Word-Conditioned Facial Motion. (arXiv:2112.10936v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10936">
<div class="article-summary-box-inner">
<span><p>In today's era of digital misinformation, we are increasingly faced with new
threats posed by video falsification techniques. Such falsifications range from
cheapfakes (e.g., lookalikes or audio dubbing) to deepfakes (e.g.,
sophisticated AI media synthesis methods), which are becoming perceptually
indistinguishable from real videos. To tackle this challenge, we propose a
multi-modal semantic forensic approach to discover clues that go beyond
detecting discrepancies in visual quality, thereby handling both simpler
cheapfakes and visually persuasive deepfakes. In this work, our goal is to
verify that the purported person seen in the video is indeed themselves by
detecting anomalous correspondences between their facial movements and the
words they are saying. We leverage the idea of attribution to learn
person-specific biometric patterns that distinguish a given speaker from
others. We use interpretable Action Units (AUs) to capture a persons' face and
head movement as opposed to deep CNN visual features, and we are the first to
use word-conditioned facial motion analysis. Unlike existing person-specific
approaches, our method is also effective against attacks that focus on lip
manipulation. We further demonstrate our method's effectiveness on a range of
fakes not seen in training including those without video manipulation, that
were not addressed in prior work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Regularizing End-to-End Speech Translation with Triangular Decomposition Agreement. (arXiv:2112.10991v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10991">
<div class="article-summary-box-inner">
<span><p>End-to-end speech-to-text translation~(E2E-ST) is becoming increasingly
popular due to the potential of its less error propagation, lower latency, and
fewer parameters. Given the triplet training corpus $\langle speech,
transcription, translation\rangle$, the conventional high-quality E2E-ST system
leverages the $\langle speech, transcription\rangle$ pair to pre-train the
model and then utilizes the $\langle speech, translation\rangle$ pair to
optimize it further. However, this process only involves two-tuple data at each
stage, and this loose coupling fails to fully exploit the association between
triplet data. In this paper, we attempt to model the joint probability of
transcription and translation based on the speech input to directly leverage
such triplet data. Based on that, we propose a novel regularization method for
model training to improve the agreement of dual-path decomposition within
triplet data, which should be equal in theory. To achieve this goal, we
introduce two Kullback-Leibler divergence regularization terms into the model
training objective to reduce the mismatch between output probabilities of
dual-path. Then the well-trained model can be naturally transformed as the
E2E-ST models by the pre-defined early stop tag. Experiments on the MuST-C
benchmark demonstrate that our proposed approach significantly outperforms
state-of-the-art E2E-ST baselines on all 8 language pairs, while achieving
better performance in the automatic speech recognition task. Our code is
open-sourced at https://github.com/duyichao/E2E-ST-TDA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Cross-Lingual Retrieval with Multilingual Text Encoders. (arXiv:2112.11031v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11031">
<div class="article-summary-box-inner">
<span><p>In this work we present a systematic empirical study focused on the
suitability of the state-of-the-art multilingual encoders for cross-lingual
document and sentence retrieval tasks across a number of diverse language
pairs. We first treat these models as multilingual text encoders and benchmark
their performance in unsupervised ad-hoc sentence- and document-level CLIR. In
contrast to supervised language understanding, our results indicate that for
unsupervised document-level CLIR -- a setup with no relevance judgments for
IR-specific fine-tuning -- pretrained multilingual encoders on average fail to
significantly outperform earlier models based on CLWEs. For sentence-level
retrieval, we do obtain state-of-the-art performance: the peak scores, however,
are met by multilingual encoders that have been further specialized, in a
supervised fashion, for sentence understanding tasks, rather than using their
vanilla 'off-the-shelf' variants. Following these results, we introduce
localized relevance matching for document-level CLIR, where we independently
score a query against document sections. In the second part, we evaluate
multilingual encoders fine-tuned in a supervised fashion (i.e., we learn to
rank) on English relevance data in a series of zero-shot language and domain
transfer CLIR experiments. Our results show that supervised re-ranking rarely
improves the performance of multilingual transformers as unsupervised base
rankers. Finally, only with in-domain contrastive fine-tuning (i.e., same
domain, only language transfer), we manage to improve the ranking quality. We
uncover substantial empirical differences between cross-lingual retrieval
results and results of (zero-shot) cross-lingual transfer for monolingual
retrieval in target languages, which point to "monolingual overfitting" of
retrieval models trained on monolingual data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting Job Titles from Job Descriptions with Multi-label Text Classification. (arXiv:2112.11052v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11052">
<div class="article-summary-box-inner">
<span><p>Finding a suitable job and hunting for eligible candidates are important to
job seeking and human resource agencies. With the vast information about job
descriptions, employees and employers need assistance to automatically detect
job titles based on job description texts. In this paper, we propose the
multi-label classification approach for predicting relevant job titles from job
description texts, and implement the Bi-GRU-LSTM-CNN with different pre-trained
language models to apply for the job titles prediction problem. The BERT with
multilingual pre-trained model obtains the highest result by F1-scores on both
development and test sets, which are 62.20% on the development set, and 47.44%
on the test set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Inference Approach To Question Answering Over Knowledge Graphs. (arXiv:2112.11070v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11070">
<div class="article-summary-box-inner">
<span><p>Knowledge Graphs (KG) act as a great tool for holding distilled information
from large natural language text corpora. The problem of natural language
querying over knowledge graphs is essential for the human consumption of this
information. This problem is typically addressed by converting the natural
language query to a structured query and then firing the structured query on
the KG. Direct answering models over knowledge graphs in literature are very
few. The query conversion models and direct models both require specific
training data pertaining to the domain of the knowledge graph. In this work, we
convert the problem of natural language querying over knowledge graphs to an
inference problem over premise-hypothesis pairs. Using trained deep learning
models for the converted proxy inferencing problem, we provide the solution for
the original natural language querying problem. Our method achieves over 90%
accuracy on MetaQA dataset, beating the existing state-of-the-art. We also
propose a model for inferencing called Hierarchical Recurrent Path
Encoder(HRPE). The inferencing models can be fine-tuned to be used across
domains with less training data. Our approach does not require large
domain-specific training data for querying on new knowledge graphs from
different domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task-oriented Dialogue Systems: performance vs. quality-optima, a review. (arXiv:2112.11176v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11176">
<div class="article-summary-box-inner">
<span><p>Task-oriented dialogue systems (TODS) are continuing to rise in popularity as
various industries find ways to effectively harness their capabilities, saving
both time and money. However, even state-of-the-art TODS are not yet reaching
their full potential. TODS typically have a primary design focus on completing
the task at hand, so the metric of task-resolution should take priority. Other
conversational quality attributes that may point to the success, or otherwise,
of the dialogue, may be ignored. This can cause interactions between human and
dialogue system that leave the user dissatisfied or frustrated. This paper
explores the literature on evaluative frameworks of dialogue systems and the
role of conversational quality attributes in dialogue systems, looking at if,
how, and where they are utilised, and examining their correlation with the
performance of the dialogue system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fake News Detection Tools and Methods -- A Review. (arXiv:2112.11185v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11185">
<div class="article-summary-box-inner">
<span><p>In the past decade, the social networks platforms and micro-blogging sites
such as Facebook, Twitter, Instagram, and Weibo have become an integral part of
our day-to-day activities and is widely used all over the world by billions of
users to share their views and circulate information in the form of messages,
pictures, and videos. These are even used by government agencies to spread
important information through their verified Facebook accounts and official
Twitter handles, as they can reach a huge population within a limited time
window. However, many deceptive activities like propaganda and rumor can
mislead users on a daily basis. In these COVID times, fake news and rumors are
very prevalent and are shared in a huge number which has created chaos in this
tough time. And hence, the need for Fake News Detection in the present scenario
is inevitable. In this paper, we survey the recent literature about different
approaches to detect fake news over the Internet. In particular, we firstly
discuss fake news and the various terms related to it that have been considered
in the literature. Secondly, we highlight the various publicly available
datasets and various online tools that are available and can debunk Fake News
in real-time. Thirdly, we describe fake news detection methods based on two
broader areas i.e., its content and the social context. Finally, we provide a
comparison of various techniques that are used to debunk fake news.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrast and Generation Make BART a Good Dialogue Emotion Recognizer. (arXiv:2112.11202v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11202">
<div class="article-summary-box-inner">
<span><p>In dialogue systems, utterances with similar semantics may have distinctive
emotions under different contexts. Therefore, modeling long-range contextual
emotional relationships with speaker dependency plays a crucial part in
dialogue emotion recognition. Meanwhile, distinguishing the different emotion
categories is non-trivial since they usually have semantically similar
sentiments. To this end, we adopt supervised contrastive learning to make
different emotions mutually exclusive to identify similar emotions better.
Meanwhile, we utilize an auxiliary response generation task to enhance the
model's ability of handling context information, thereby forcing the model to
recognize emotions with similar semantics in diverse contexts. To achieve these
objectives, we use the pre-trained encoder-decoder model BART as our backbone
model since it is very suitable for both understanding and generation tasks.
The experiments on four datasets demonstrate that our proposed model obtains
significantly more favorable results than the state-of-the-art model in
dialogue emotion recognition. The ablation study further demonstrates the
effectiveness of supervised contrastive loss and generative loss.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How are cities pledging net zero? A computational approach to analyzing subnational climate strategies. (arXiv:2112.11207v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11207">
<div class="article-summary-box-inner">
<span><p>Cities have become primary actors on climate change and are increasingly
setting goals aimed at net-zero emissions. The rapid proliferation of
subnational governments "racing to zero" emissions and articulating their own
climate mitigation plans warrants closer examination to understand how these
actors intend to meet these goals. The scattered, incomplete and heterogeneous
nature of city climate policy documents, however, has made their systemic
analysis challenging. We analyze 318 climate action documents from cities that
have pledged net-zero targets or joined a transnational climate initiative with
this goal using machine learning-based natural language processing (NLP)
techniques. We use these approaches to accomplish two primary goals: 1)
determine text patterns that predict "ambitious" net-zero targets, where we
define an ambitious target as one that encompasses a subnational government's
economy-wide emissions; and 2) perform a sectoral analysis to identify patterns
and trade-offs in climate action themes (i.e., land-use, industry, buildings,
etc.). We find that cities that have defined ambitious climate actions tend to
emphasize quantitative metrics and specific high-emitting sectors in their
plans, supported by mentions of governance and citizen participation. Cities
predominantly emphasize energy-related actions in their plans, particularly in
the buildings, transport and heating sectors, but often at the expense of other
sectors, including land-use and climate impacts. The method presented in this
paper provides a replicable, scalable approach to analyzing climate action
plans and a first step towards facilitating cross-city learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An ASP-based Approach to Answering Natural Language Questions for Texts. (arXiv:2112.11241v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11241">
<div class="article-summary-box-inner">
<span><p>An approach based on answer set programming (ASP) is proposed in this paper
for representing knowledge generated from natural language texts. Knowledge in
a text is modeled using a Neo Davidsonian-like formalism, which is then
represented as an answer set program. Relevant commonsense knowledge is
additionally imported from resources such as WordNet and represented in ASP.
The resulting knowledge-base can then be used to perform reasoning with the
help of an ASP system. This approach can facilitate many natural language tasks
such as automated question answering, text summarization, and automated
question generation. ASP-based representation of techniques such as default
reasoning, hierarchical knowledge organization, preferences over defaults,
etc., are used to model commonsense reasoning methods required to accomplish
these tasks. In this paper, we describe the CASPR system that we have developed
to automate the task of answering natural language questions given English
text. CASPR can be regarded as a system that answers questions by
"understanding" the text and has been tested on the SQuAD data set, with
promising results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Supervised Graph Contrastive Pretraining for Text Classification. (arXiv:2112.11389v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11389">
<div class="article-summary-box-inner">
<span><p>Contrastive pretraining techniques for text classification has been largely
studied in an unsupervised setting. However, oftentimes labeled data from
related tasks which share label semantics with current task is available. We
hypothesize that using this labeled data effectively can lead to better
generalization on current task. In this paper, we propose a novel way to
effectively utilize labeled data from related tasks with a graph based
supervised contrastive learning approach. We formulate a token-graph by
extrapolating the supervised information from examples to tokens. Our
formulation results in an embedding space where tokens with high/low
probability of belonging to same class are near/further-away from one another.
We also develop detailed theoretical insights which serve as a motivation for
our method. In our experiments with $13$ datasets, we show our method
outperforms pretraining schemes by $2.5\%$ and also example-level contrastive
learning based formulation by $1.8\%$ on average. In addition, we show
cross-domain effectiveness of our method in a zero-shot setting by $3.91\%$ on
average. Lastly, we also demonstrate our method can be used as a noisy teacher
in a knowledge distillation setting to significantly improve performance of
transformer based models in low labeled data regime by $4.57\%$ on average.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Voice Quality and Pitch Features in Transformer-Based Speech Recognition. (arXiv:2112.11391v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11391">
<div class="article-summary-box-inner">
<span><p>Jitter and shimmer measurements have shown to be carriers of voice quality
and prosodic information which enhance the performance of tasks like speaker
recognition, diarization or automatic speech recognition (ASR). However, such
features have been seldom used in the context of neural-based ASR, where
spectral features often prevail. In this work, we study the effects of
incorporating voice quality and pitch features altogether and separately to a
Transformer-based ASR model, with the intuition that the attention mechanisms
might exploit latent prosodic traits. For doing so, we propose separated
convolutional front-ends for prosodic and spectral features, showing that this
architectural choice yields better results than simple concatenation of such
pitch and voice quality features to mel-spectrogram filterbanks. Furthermore,
we find mean Word Error Rate relative reductions of up to 5.6% with the
LibriSpeech benchmark. Such findings motivate further research on the
application of prosody knowledge for increasing the robustness of
Transformer-based ASR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lyric document embeddings for music tagging. (arXiv:2112.11436v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11436">
<div class="article-summary-box-inner">
<span><p>We present an empirical study on embedding the lyrics of a song into a
fixed-dimensional feature for the purpose of music tagging. Five methods of
computing token-level and four methods of computing document-level
representations are trained on an industrial-scale dataset of tens of millions
of songs. We compare simple averaging of pretrained embeddings to modern
recurrent and attention-based neural architectures. Evaluating on a wide range
of tagging tasks such as genre classification, explicit content identification
and era detection, we find that averaging word embeddings outperform more
complex architectures in many downstream metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mixed Precision Low-bit Quantization of Neural Network Language Models for Speech Recognition. (arXiv:2112.11438v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11438">
<div class="article-summary-box-inner">
<span><p>State-of-the-art language models (LMs) represented by long-short term memory
recurrent neural networks (LSTM-RNNs) and Transformers are becoming
increasingly complex and expensive for practical applications. Low-bit neural
network quantization provides a powerful solution to dramatically reduce their
model size. Current quantization methods are based on uniform precision and
fail to account for the varying performance sensitivity at different parts of
LMs to quantization errors. To this end, novel mixed precision neural network
LM quantization methods are proposed in this paper. The optimal local precision
choices for LSTM-RNN and Transformer based neural LMs are automatically learned
using three techniques. The first two approaches are based on quantization
sensitivity metrics in the form of either the KL-divergence measured between
full precision and quantized LMs, or Hessian trace weighted quantization
perturbation that can be approximated efficiently using matrix free techniques.
The third approach is based on mixed precision neural architecture search. In
order to overcome the difficulty in using gradient descent methods to directly
estimate discrete quantized weights, alternating direction methods of
multipliers (ADMM) are used to efficiently train quantized LMs. Experiments
were conducted on state-of-the-art LF-MMI CNN-TDNN systems featuring speed
perturbation, i-Vector and learning hidden unit contribution (LHUC) based
speaker adaptation on two tasks: Switchboard telephone speech and AMI meeting
transcription. The proposed mixed precision quantization techniques achieved
"lossless" quantization on both tasks, by producing model size compression
ratios of up to approximately 16 times over the full precision LSTM and
Transformer baseline LMs, while incurring no statistically significant word
error rate increase.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Drug-Related Information Extraction from French Clinical Documents: ReLyfe Approach. (arXiv:2112.11439v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11439">
<div class="article-summary-box-inner">
<span><p>Structuring medical data in France remains a challenge mainly because of the
lack of medical data due to privacy concerns and the lack of methods and
approaches on processing the French language. One of these challenges is
structuring drug-related information in French clinical documents. To our
knowledge, over the last decade, there are less than five relevant papers that
study French prescriptions. This paper proposes a new approach for extracting
drug-related information from French clinical scanned documents while
preserving patients' privacy. In addition, we deployed our method in a health
data management platform where it is used to structure drug medical data and
help patients organize their drug schedules. It can be implemented on any web
or mobile platform. This work closes the gap between theoretical and practical
work by creating an application adapted to real production problems. It is a
combination of a rule-based phase and a Deep Learning approach. Finally,
numerical results show the outperformance and relevance of the proposed
methodology.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NLP Techniques for Water Quality Analysis in Social Media Content. (arXiv:2112.11441v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11441">
<div class="article-summary-box-inner">
<span><p>This paper presents our contributions to the MediaEval 2021 task namely
"WaterMM: Water Quality in Social Multimedia". The task aims at analyzing
social media posts relevant to water quality with particular focus on the
aspects like watercolor, smell, taste, and related illnesses. To this aim, a
multimodal dataset containing both textual and visual information along with
meta-data is provided. Considering the quality and quantity of available
content, we mainly focus on textual information by employing three different
models individually and jointly in a late-fusion manner. These models include
(i) Bidirectional Encoder Representations from Transformers (BERT), (ii)
Robustly Optimized BERT Pre-training Approach (XLM-RoBERTa), and a (iii) custom
Long short-term memory (LSTM) model obtaining an overall F1-score of 0.794,
0.717, 0.663 on the official test set, respectively. In the fusion scheme, all
the models are treated equally and no significant improvement is observed in
the performance over the best performing individual model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deliberation of Streaming RNN-Transducer by Non-autoregressive Decoding. (arXiv:2112.11442v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11442">
<div class="article-summary-box-inner">
<span><p>We propose to deliberate the hypothesis alignment of a streaming RNN-T model
with the previously proposed Align-Refine non-autoregressive decoding method
and its improved versions. The method performs a few refinement steps, where
each step shares a transformer decoder that attends to both text features
(extracted from alignments) and audio features, and outputs complete updated
alignments. The transformer decoder is trained with the CTC loss which
facilitates parallel greedy decoding, and performs full-context attention to
capture label dependencies. We improve Align-Refine by introducing cascaded
encoder that captures more audio context before refinement, and alignment
augmentation which enforces learning label dependency. We show that,
conditioned on hypothesis alignments of a streaming RNN-T model, our method
obtains significantly more accurate recognition results than the first-pass
RNN-T, with only small amount of model parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ESAN: Efficient Sentiment Analysis Network of A-Shares Research Reports for Stock Price Prediction. (arXiv:2112.11444v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11444">
<div class="article-summary-box-inner">
<span><p>In this paper, we are going to develop a natural language processing model to
help us to predict stocks in the long term. The whole network includes two
modules. The first module is a natural language processing model which seeks
out reliable factors from input reports. While the other is a time-series
forecasting model which takes the factors as input and aims to predict stocks
earnings yield. To indicate the efficiency of our model to combine the
sentiment analysis module and the time-series forecasting module, we name our
method ESAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controversy Detection: a Text and Graph Neural Network Based Approach. (arXiv:2112.11445v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11445">
<div class="article-summary-box-inner">
<span><p>Controversial content refers to any content that attracts both positive and
negative feedback. Its automatic identification, especially on social media, is
a challenging task as it should be done on a large number of continuously
evolving posts, covering a large variety of topics. Most of the existing
approaches rely on the graph structure of a topic-discussion and/or the content
of messages. This paper proposes a controversy detection approach based on both
graph structure of a discussion and text features. Our proposed approach relies
on Graph Neural Network (gnn) to encode the graph representation (including its
texts) in an embedding vector before performing a graph classification task.
The latter will classify the post as controversial or not. Two controversy
detection strategies are proposed. The first one is based on a hierarchical
graph representation learning. Graph user nodes are embedded hierarchically and
iteratively to compute the whole graph embedding vector. The second one is
based on the attention mechanism, which allows each user node to give more or
less importance to its neighbors when computing node embeddings. We conduct
experiments to evaluate our approach using different real-world datasets.
Conducted experiments show the positive impact of combining textual features
and structural information in terms of performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling Language Models: Methods, Analysis & Insights from Training Gopher. (arXiv:2112.11446v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11446">
<div class="article-summary-box-inner">
<span><p>Language modelling provides a step towards intelligent communication systems
by harnessing large repositories of written human knowledge to better predict
and understand the world. In this paper, we present an analysis of
Transformer-based language model performance across a wide range of model
scales -- from models with tens of millions of parameters up to a 280 billion
parameter model called Gopher. These models are evaluated on 152 diverse tasks,
achieving state-of-the-art performance across the majority. Gains from scale
are largest in areas such as reading comprehension, fact-checking, and the
identification of toxic language, but logical and mathematical reasoning see
less benefit. We provide a holistic analysis of the training dataset and
model's behaviour, covering the intersection of model scale with bias and
toxicity. Finally we discuss the application of language models to AI safety
and the mitigation of downstream harms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lexicon-constrained Copying Network for Chinese Abstractive Summarization. (arXiv:2010.08197v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.08197">
<div class="article-summary-box-inner">
<span><p>Copy mechanism allows sequence-to-sequence models to choose words from the
input and put them directly into the output, which is finding increasing use in
abstractive summarization. However, since there is no explicit delimiter in
Chinese sentences, most existing models for Chinese abstractive summarization
can only perform character copy, resulting in inefficient. To solve this
problem, we propose a lexicon-constrained copying network that models
multi-granularity in both encoder and decoder. On the source side, words and
characters are aggregated into the same input memory using a Transformerbased
encoder. On the target side, the decoder can copy either a character or a
multi-character word at each time step, and the decoding process is guided by a
word-enhanced search algorithm that facilitates the parallel computation and
encourages the model to copy more words. Moreover, we adopt a word selector to
integrate keyword information. Experiments results on a Chinese social media
dataset show that our model can work standalone or with the word selector. Both
forms can outperform previous character-based models and achieve competitive
performances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine Learning Based on Natural Language Processing to Detect Cardiac Failure in Clinical Narratives. (arXiv:2104.03934v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.03934">
<div class="article-summary-box-inner">
<span><p>The purpose of the study presented herein is to develop a machine learning
algorithm based on natural language processing that automatically detects
whether a patient has a cardiac failure or a healthy condition by using
physician notes in Research Data Warehouse at CHU Sainte Justine Hospital.
First, a word representation learning technique was employed by using
bag-of-word (BoW), term frequency inverse document frequency (TFIDF), and
neural word embeddings (word2vec). Each representation technique aims to retain
the words semantic and syntactic analysis in critical care data. It helps to
enrich the mutual information for the word representation and leads to an
advantage for further appropriate analysis steps. Second, a machine learning
classifier was used to detect the patients condition for either cardiac failure
or stable patient through the created word representation vector space from the
previous step. This machine learning approach is based on a supervised binary
classification algorithm, including logistic regression (LR), Gaussian
Naive-Bayes (GaussianNB), and multilayer perceptron neural network (MLPNN).
Technically, it mainly optimizes the empirical loss during training the
classifiers. As a result, an automatic learning algorithm would be accomplished
to draw a high classification performance, including accuracy (acc), precision
(pre), recall (rec), and F1 score (f1). The results show that the combination
of TFIDF and MLPNN always outperformed other combinations with all overall
performance. In the case without any feature selection, the proposed framework
yielded an overall classification performance with acc, pre, rec, and f1 of 84%
and 82%, 85%, and 83%, respectively. Significantly, if the feature selection
was well applied, the overall performance would finally improve up to 4% for
each evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards General Natural Language Understanding with Probabilistic Worldbuilding. (arXiv:2105.02486v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.02486">
<div class="article-summary-box-inner">
<span><p>We introduce the Probabilistic Worldbuilding Model (PWM), a new
fully-symbolic Bayesian model of semantic parsing and reasoning, as a first
step in a research program toward more domain- and task-general NLU and AI.
Humans create internal mental models of their observations which greatly aid in
their ability to understand and reason about a large variety of problems. In
PWM, the meanings of sentences, acquired facts about the world, and
intermediate steps in reasoning are all expressed in a human-readable formal
language, with the design goal of interpretability. PWM is Bayesian, designed
specifically to be able to generalize to new domains and new tasks. We derive
and implement an inference algorithm that reads sentences by parsing and
abducing updates to its latent world model that capture the semantics of those
sentences, and evaluate it on two out-of-domain question-answering datasets:
(1) ProofWriter and (2) a new dataset we call FictionalGeoQA, designed to be
more representative of real language but still simple enough to focus on
evaluating reasoning ability, while being robust against heuristics. Our method
outperforms baselines on both, thereby demonstrating its value as a
proof-of-concept.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-Shot Cross-Lingual Stance Detection with Sentiment-Based Pre-Training. (arXiv:2109.06050v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06050">
<div class="article-summary-box-inner">
<span><p>The goal of stance detection is to determine the viewpoint expressed in a
piece of text towards a target. These viewpoints or contexts are often
expressed in many different languages depending on the user and the platform,
which can be a local news outlet, a social media platform, a news forum, etc.
Most research in stance detection, however, has been limited to working with a
single language and on a few limited targets, with little work on cross-lingual
stance detection. Moreover, non-English sources of labelled data are often
scarce and present additional challenges. Recently, large multilingual language
models have substantially improved the performance on many non-English tasks,
especially such with limited numbers of examples. This highlights the
importance of model pre-training and its ability to learn from few examples. In
this paper, we present the most comprehensive study of cross-lingual stance
detection to date: we experiment with 15 diverse datasets in 12 languages from
6 language families, and with 6 low-resource evaluation settings each. For our
experiments, we build on pattern-exploiting training, proposing the addition of
a novel label encoder to simplify the verbalisation procedure. We further
propose sentiment-based generation of stance data for pre-training, which shows
sizeable improvement of more than 6% F1 absolute in low-shot settings compared
to several strong baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive String Representation Learning using Synthetic Data. (arXiv:2110.04217v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04217">
<div class="article-summary-box-inner">
<span><p>String representation Learning (SRL) is an important task in the field of
Natural Language Processing, but it remains under-explored. The goal of SRL is
to learn dense and low-dimensional vectors (or embeddings) for encoding
character sequences. The learned representation from this task can be used in
many downstream application tasks such as string similarity matching or lexical
normalization. In this paper, we propose a new method for to train a SRL model
by only using synthetic data. Our approach makes use of Contrastive Learning in
order to maximize similarity between related strings while minimizing it for
unrelated strings. We demonstrate the effectiveness of our approach by
evaluating the learned representation on the task of string similarity
matching. Codes, data and pretrained models will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are Words the Quanta of Human Language? Extending the Domain of Quantum Cognition. (arXiv:2110.04913v2 [q-bio.NC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04913">
<div class="article-summary-box-inner">
<span><p>In previous research, we showed that 'texts that tell a story' exhibit a
statistical structure that is not Maxwell-Boltzmann but Bose-Einstein. Our
explanation is that this is due to the presence of 'indistinguishability' in
human language as a result of the same words in different parts of the story
being indistinguishable from one another. In the current article, we set out to
provide an explanation for this Bose-Einstein statistics. We show that it is
the presence of 'meaning' in 'stories' that gives rise to the lack of
independence characteristic of Bose-Einstein, and provides conclusive evidence
that 'words can be considered the quanta of human language', structurally
similar to how 'photons are the quanta of light'. Using several studies on
entanglement from our Brussels research group, we also show that it is also the
presence of 'meaning' in texts that makes the von Neumann entropy of a total
text smaller relative to the entropy of the words composing it. We explain how
the new insights in this article fit in with the research domain called
'quantum cognition', where quantum probability models and quantum vector spaces
are used in human cognition, and are also relevant to the use of quantum
structures in information retrieval and natural language processing, and how
they introduce 'quantization' and 'Bose-Einstein statistics' as relevant
quantum effects there. Inspired by the conceptuality interpretation of quantum
mechanics, and relying on the new insights, we put forward hypotheses about the
nature of physical reality. In doing so, we note how this new type of decrease
in entropy, and its explanation, may be important for the development of
quantum thermodynamics. We likewise note how it can also give rise to an
original explanatory picture of the nature of physical reality on the surface
of planet Earth, in which human culture emerges as a reinforcing continuation
of life.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Much Coffee Was Consumed During EMNLP 2019? Fermi Problems: A New Reasoning Challenge for AI. (arXiv:2110.14207v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14207">
<div class="article-summary-box-inner">
<span><p>Many real-world problems require the combined application of multiple
reasoning abilities employing suitable abstractions, commonsense knowledge, and
creative synthesis of problem-solving strategies. To help advance AI systems
towards such capabilities, we propose a new reasoning challenge, namely Fermi
Problems (FPs), which are questions whose answers can only be approximately
estimated because their precise computation is either impractical or
impossible. For example, "How much would the sea level rise if all ice in the
world melted?" FPs are commonly used in quizzes and interviews to bring out and
evaluate the creative reasoning abilities of humans. To do the same for AI
systems, we present two datasets: 1) A collection of 1k real-world FPs sourced
from quizzes and olympiads; and 2) a bank of 10k synthetic FPs of intermediate
complexity to serve as a sandbox for the harder real-world challenge. In
addition to question answer pairs, the datasets contain detailed solutions in
the form of an executable program and supporting facts, helping in supervision
and evaluation of intermediate steps. We demonstrate that even extensively
fine-tuned large scale language models perform poorly on these datasets, on
average making estimates that are off by two orders of magnitude. Our
contribution is thus the crystallization of several unsolved AI problems into a
single, new challenge that we hope will spur further advances in building
systems that can reason.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Selecting Parallel In-domain Sentences for Neural Machine Translation Using Monolingual Texts. (arXiv:2112.06096v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06096">
<div class="article-summary-box-inner">
<span><p>Continuously-growing data volumes lead to larger generic models. Specific
use-cases are usually left out, since generic models tend to perform poorly in
domain-specific cases. Our work addresses this gap with a method for selecting
in-domain data from generic-domain (parallel text) corpora, for the task of
machine translation. The proposed method ranks sentences in parallel
general-domain data according to their cosine similarity with a monolingual
domain-specific data set. We then select the top K sentences with the highest
similarity score to train a new machine translation system tuned to the
specific in-domain data. Our experimental results show that models trained on
this in-domain data outperform models trained on generic or a mixture of
generic and domain data. That is, our method selects high-quality
domain-specific training instances at low computational cost and data size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Isometric MT: Neural Machine Translation for Automatic Dubbing. (arXiv:2112.08682v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08682">
<div class="article-summary-box-inner">
<span><p>Automatic dubbing (AD) is among the use cases where translations should fit a
given length template in order to achieve synchronicity between source and
target speech. For neural machine translation (MT), generating translations of
length close to the source length (e.g. within +-10% in character count), while
preserving quality is a challenging task. Controlling NMT output length comes
at a cost to translation quality which is usually mitigated with a two step
approach of generation of n-best hypotheses and then re-ranking them based on
length and quality. This work, introduces a self-learning approach that allows
a transformer model to directly learn to generate outputs that closely match
the source length, in short isometric MT. In particular, our approach for
isometric MT does not require to generate multiple hypotheses nor any auxiliary
scoring function. We report results on four language pairs (English - French,
Italian, German, Spanish) with a publicly available benchmark based on TED Talk
data. Both automatic and manual evaluations show that our self-learning
approach to performs on par with more complex isometric MT approaches.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">HarmoFL: Harmonizing Local and Global Drifts in Federated Learning on Heterogeneous Medical Images. (arXiv:2112.10775v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10775">
<div class="article-summary-box-inner">
<span><p>Multiple medical institutions collaboratively training a model using
federated learning (FL) has become a promising solution for maximizing the
potential of data-driven models, yet the non-independent and identically
distributed (non-iid) data in medical images is still an outstanding challenge
in real-world practice. The feature heterogeneity caused by diverse scanners or
protocols introduces a drift in the learning process, in both local (client)
and global (server) optimizations, which harms the convergence as well as model
performance. Many previous works have attempted to address the non-iid issue by
tackling the drift locally or globally, but how to jointly solve the two
essentially coupled drifts is still unclear. In this work, we concentrate on
handling both local and global drifts and introduce a new harmonizing framework
called HarmoFL. First, we propose to mitigate the local update drift by
normalizing amplitudes of images transformed into the frequency domain to mimic
a unified imaging setting, in order to generate a harmonized feature space
across local clients. Second, based on harmonized features, we design a client
weight perturbation guiding each local model to reach a flat optimum, where a
neighborhood area of the local optimal solution has a uniformly low loss.
Without any extra communication cost, the perturbation assists the global model
to optimize towards a converged optimal solution by aggregating several local
flat optima. We have theoretically analyzed the proposed method and empirically
conducted extensive experiments on three medical image classification and
segmentation tasks, showing that HarmoFL outperforms a set of recent
state-of-the-art methods with promising convergence behavior.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lite Vision Transformer with Enhanced Self-Attention. (arXiv:2112.10809v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10809">
<div class="article-summary-box-inner">
<span><p>Despite the impressive representation capacity of vision transformer models,
current light-weight vision transformer models still suffer from inconsistent
and incorrect dense predictions at local regions. We suspect that the power of
their self-attention mechanism is limited in shallower and thinner networks. We
propose Lite Vision Transformer (LVT), a novel light-weight transformer network
with two enhanced self-attention mechanisms to improve the model performances
for mobile deployment. For the low-level features, we introduce Convolutional
Self-Attention (CSA). Unlike previous approaches of merging convolution and
self-attention, CSA introduces local self-attention into the convolution within
a kernel of size 3x3 to enrich low-level features in the first stage of LVT.
For the high-level features, we propose Recursive Atrous Self-Attention (RASA),
which utilizes the multi-scale context when calculating the similarity map and
a recursive mechanism to increase the representation capability with marginal
extra parameter cost. The superiority of LVT is demonstrated on ImageNet
recognition, ADE20K semantic segmentation, and COCO panoptic segmentation. The
code is made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Sketch for All: One-Shot Personalized Sketch Segmentation. (arXiv:2112.10838v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10838">
<div class="article-summary-box-inner">
<span><p>We present the first one-shot personalized sketch segmentation method. We aim
to segment all sketches belonging to the same category provisioned with a
single sketch with a given part annotation while (i) preserving the parts
semantics embedded in the exemplar, and (ii) being robust to input style and
abstraction. We refer to this scenario as personalized. With that, we
importantly enable a much-desired personalization capability for downstream
fine-grained sketch analysis tasks. To train a robust segmentation module, we
deform the exemplar sketch to each of the available sketches of the same
category. Our method generalizes to sketches not observed during training. Our
central contribution is a sketch-specific hierarchical deformation network.
Given a multi-level sketch-strokes encoding obtained via a graph convolutional
network, our method estimates rigid-body transformation from the reference to
the exemplar, on the upper level. Finer deformation from the exemplar to the
globally warped reference sketch is further obtained through stroke-wise
deformations, on the lower level. Both levels of deformation are guided by mean
squared distances between the keypoints learned without supervision, ensuring
that the stroke semantics are preserved. We evaluate our method against the
state-of-the-art segmentation and perceptual grouping baselines re-purposed for
the one-shot setting and against two few-shot 3D shape segmentation methods. We
show that our method outperforms all the alternatives by more than 10% on
average. Ablation studies further demonstrate that our method is robust to
personalization: changes in input part semantics and style differences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Encoding Hierarchical Information in Neural Networks helps in Subpopulation Shift. (arXiv:2112.10844v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10844">
<div class="article-summary-box-inner">
<span><p>Over the past decade, deep neural networks have proven to be adept in image
classification tasks, often surpassing humans in terms of accuracy. However,
standard neural networks often fail to understand the concept of hierarchical
structures and dependencies among different classes for vision related tasks.
Humans on the other hand, seem to learn categories conceptually, progressively
growing from understanding high-level concepts down to granular levels of
categories. One of the issues arising from the inability of neural networks to
encode such dependencies within its learned structure is that of subpopulation
shift -- where models are queried with novel unseen classes taken from a
shifted population of the training set categories. Since the neural network
treats each class as independent from all others, it struggles to categorize
shifting populations that are dependent at higher levels of the hierarchy. In
this work, we study the aforementioned problems through the lens of a novel
conditional supervised training framework. We tackle subpopulation shift by a
structured learning procedure that incorporates hierarchical information
conditionally through labels. Furthermore, we introduce a notion of graphical
distance to model the catastrophic effect of mispredictions. We show that
learning in this structured hierarchical manner results in networks that are
more robust against subpopulation shifts, with an improvement of around ~2% in
terms of accuracy and around 8.5\% in terms of graphical distance over standard
models on subpopulation shift benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Translational Concept Embedding for Generalized Compositional Zero-shot Learning. (arXiv:2112.10871v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10871">
<div class="article-summary-box-inner">
<span><p>Generalized compositional zero-shot learning means to learn composed concepts
of attribute-object pairs in a zero-shot fashion, where a model is trained on a
set of seen concepts and tested on a combined set of seen and unseen concepts.
This task is very challenging because of not only the gap between seen and
unseen concepts but also the contextual dependency between attributes and
objects. This paper introduces a new approach, termed translational concept
embedding, to solve these two difficulties in a unified framework. It models
the effect of applying an attribute to an object as adding a translational
attribute feature to an object prototype. We explicitly take into account of
the contextual dependency between attributes and objects by generating
translational attribute features conditionally dependent on the object
prototypes. Furthermore, we design a ratio variance constraint loss to promote
the model's generalization ability on unseen concepts. It regularizes the
distances between concepts by utilizing knowledge from their pretrained word
embeddings. We evaluate the performance of our model under both the unbiased
and biased concept classification tasks, and show that our model is able to
achieve good balance in predicting unseen and seen concepts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatiotemporal Motion Synchronization for Snowboard Big Air. (arXiv:2112.10909v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10909">
<div class="article-summary-box-inner">
<span><p>During the training for snowboard big air, one of the most popular winter
sports, athletes and coaches extensively shoot and check their jump attempts
using a single camera or smartphone. However, by watching videos sequentially,
it is difficult to compare the precise difference in performance between two
trials. Therefore, side-by-side display or overlay of two videos may be helpful
for training. To accomplish this, the spatial and temporal alignment of
multiple performances must be ensured. In this study, we propose a conventional
but plausible solution using the existing image processing techniques for
snowboard big air training. We conducted interviews with expert snowboarders
who stated that the spatiotemporally aligned videos enabled them to precisely
identify slight differences in body movements. The results suggest that the
proposed method can be used during the training of snowboard big air.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Watch Those Words: Video Falsification Detection Using Word-Conditioned Facial Motion. (arXiv:2112.10936v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10936">
<div class="article-summary-box-inner">
<span><p>In today's era of digital misinformation, we are increasingly faced with new
threats posed by video falsification techniques. Such falsifications range from
cheapfakes (e.g., lookalikes or audio dubbing) to deepfakes (e.g.,
sophisticated AI media synthesis methods), which are becoming perceptually
indistinguishable from real videos. To tackle this challenge, we propose a
multi-modal semantic forensic approach to discover clues that go beyond
detecting discrepancies in visual quality, thereby handling both simpler
cheapfakes and visually persuasive deepfakes. In this work, our goal is to
verify that the purported person seen in the video is indeed themselves by
detecting anomalous correspondences between their facial movements and the
words they are saying. We leverage the idea of attribution to learn
person-specific biometric patterns that distinguish a given speaker from
others. We use interpretable Action Units (AUs) to capture a persons' face and
head movement as opposed to deep CNN visual features, and we are the first to
use word-conditioned facial motion analysis. Unlike existing person-specific
approaches, our method is also effective against attacks that focus on lip
manipulation. We further demonstrate our method's effectiveness on a range of
fakes not seen in training including those without video manipulation, that
were not addressed in prior work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structured Semantic Transfer for Multi-Label Recognition with Partial Labels. (arXiv:2112.10941v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10941">
<div class="article-summary-box-inner">
<span><p>Multi-label image recognition is a fundamental yet practical task because
real-world images inherently possess multiple semantic labels. However, it is
difficult to collect large-scale multi-label annotations due to the complexity
of both the input images and output label spaces. To reduce the annotation
cost, we propose a structured semantic transfer (SST) framework that enables
training multi-label recognition models with partial labels, i.e., merely some
labels are known while other labels are missing (also called unknown labels)
per image. The framework consists of two complementary transfer modules that
explore within-image and cross-image semantic correlations to transfer
knowledge of known labels to generate pseudo labels for unknown labels.
Specifically, an intra-image semantic transfer module learns image-specific
label co-occurrence matrix and maps the known labels to complement unknown
labels based on this matrix. Meanwhile, a cross-image transfer module learns
category-specific feature similarities and helps complement unknown labels with
high similarities. Finally, both known and generated labels are used to train
the multi-label recognition models. Extensive experiments on the Microsoft
COCO, Visual Genome and Pascal VOC datasets show that the proposed SST
framework obtains superior performance over current state-of-the-art
algorithms. Codes are available at
\url{https://github.com/HCPLab-SYSU/SST-MLR-PL
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pixel-Stega: Generative Image Steganography Based on Autoregressive Models. (arXiv:2112.10945v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10945">
<div class="article-summary-box-inner">
<span><p>In this letter, we explored generative image steganography based on
autoregressive models. We proposed Pixel-Stega, which implements pixel-level
information hiding with autoregressive models and arithmetic coding algorithm.
Firstly, one of the autoregressive models, PixelCNN++, is utilized to produce
explicit conditional probability distribution of each pixel. Secondly, secret
messages are encoded to the selection of pixels through steganographic sampling
(stegosampling) based on arithmetic coding. We carried out qualitative and
quantitative assessment on gray-scale and colour image datasets. Experimental
results show that Pixel-Stega is able to embed secret messages adaptively
according to the entropy of the pixels to achieve both high embedding capacity
(up to 4.3 bpp) and nearly perfect imperceptibility (about 50% detection
accuracy).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task-Oriented Image Transmission for Scene Classification in Unmanned Aerial Systems. (arXiv:2112.10948v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10948">
<div class="article-summary-box-inner">
<span><p>The vigorous developments of Internet of Things make it possible to extend
its computing and storage capabilities to computing tasks in the aerial system
with collaboration of cloud and edge, especially for artificial intelligence
(AI) tasks based on deep learning (DL). Collecting a large amount of
image/video data, Unmanned aerial vehicles (UAVs) can only handover intelligent
analysis tasks to the back-end mobile edge computing (MEC) server due to their
limited storage and computing capabilities. How to efficiently transmit the
most correlated information for the AI model is a challenging topic. Inspired
by the task-oriented communication in recent years, we propose a new aerial
image transmission paradigm for the scene classification task. A lightweight
model is developed on the front-end UAV for semantic blocks transmission with
perception of images and channel conditions. In order to achieve the tradeoff
between transmission latency and classification accuracy, deep reinforcement
learning (DRL) is used to explore the semantic blocks which have the best
contribution to the back-end classifier under various channel conditions.
Experimental results show that the proposed method can significantly improve
classification accuracy compared to the fixed transmission strategy and
traditional content perception methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continuous-Time Video Generation via Learning Motion Dynamics with Neural ODE. (arXiv:2112.10960v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10960">
<div class="article-summary-box-inner">
<span><p>In order to perform unconditional video generation, we must learn the
distribution of the real-world videos. In an effort to synthesize high-quality
videos, various studies attempted to learn a mapping function between noise and
videos, including recent efforts to separate motion distribution and appearance
distribution. Previous methods, however, learn motion dynamics in discretized,
fixed-interval timesteps, which is contrary to the continuous nature of motion
of a physical body. In this paper, we propose a novel video generation approach
that learns separate distributions for motion and appearance, the former
modeled by neural ODE to learn natural motion dynamics. Specifically, we employ
a two-stage approach where the first stage converts a noise vector to a
sequence of keypoints in arbitrary frame rates, and the second stage
synthesizes videos based on the given keypoints sequence and the appearance
noise vector. Our model not only quantitatively outperforms recent baselines
for video generation, but also demonstrates versatile functionality such as
dynamic frame rate manipulation and motion transfer between two datasets, thus
opening new doors to diverse video generation applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nonlinear Transform Source-Channel Coding for Semantic Communications. (arXiv:2112.10961v1 [cs.IT])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10961">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a new class of high-efficient deep joint
source-channel coding methods that can closely adapt to the source distribution
under the nonlinear transform, it can be collected under the name nonlinear
transform source-channel coding (NTSCC). In the considered model, the
transmitter first learns a nonlinear analysis transform to map the source data
into latent space, then transmits the latent representation to the receiver via
deep joint source-channel coding. Our model incorporates the nonlinear
transform as a strong prior to effectively extract the source semantic features
and provide side information for source-channel coding. Unlike existing
conventional deep joint source-channel coding methods, the proposed NTSCC
essentially learns both the source latent representation and an entropy model
as the prior on the latent representation. Accordingly, novel adaptive rate
transmission and hyperprior-aided codec refinement mechanisms are developed to
upgrade deep joint source-channel coding. The whole system design is formulated
as an optimization problem whose goal is to minimize the end-to-end
transmission rate-distortion performance under established perceptual quality
metrics. Across simple example sources and test image sources, we find that the
proposed NTSCC transmission method generally outperforms both the analog
transmission using the standard deep joint source-channel coding and the
classical separation-based digital transmission. Notably, the proposed NTSCC
method can potentially support future semantic communications due to its
vigorous content-aware ability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DRPN: Making CNN Dynamically Handle Scale Variation. (arXiv:2112.10963v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10963">
<div class="article-summary-box-inner">
<span><p>Based on our observations of infrared targets, serious scale variation along
within sequence frames has high-frequently occurred. In this paper, we propose
a dynamic re-parameterization network (DRPN) to deal with the scale variation
and balance the detection precision between small targets and large targets in
infrared datasets. DRPN adopts the multiple branches with different sizes of
convolution kernels and the dynamic convolution strategy. Multiple branches
with different sizes of convolution kernels have different sizes of receptive
fields. Dynamic convolution strategy makes DRPN adaptively weight multiple
branches. DRPN can dynamically adjust the receptive field according to the
scale variation of the target. Besides, in order to maintain effective
inference in the test phase, the multi-branch structure is further converted to
a single-branch structure via the re-parameterization technique after training.
Extensive experiments on FLIR, KAIST, and InfraPlane datasets demonstrate the
effectiveness of our proposed DRPN. The experimental results show that
detectors using the proposed DRPN as the basic structure rather than SKNet or
TridentNet obtained the best performances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalizing Interactive Backpropagating Refinement for Dense Prediction. (arXiv:2112.10969v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10969">
<div class="article-summary-box-inner">
<span><p>As deep neural networks become the state-of-the-art approach in the field of
computer vision for dense prediction tasks, many methods have been developed
for automatic estimation of the target outputs given the visual inputs.
Although the estimation accuracy of the proposed automatic methods continues to
improve, interactive refinement is oftentimes necessary for further correction.
Recently, feature backpropagating refinement scheme (\text{\textit{f}-BRS}) has
been proposed for the task of interactive segmentation, which enables efficient
optimization of a small set of auxiliary variables inserted into the pretrained
network to produce object segmentation that better aligns with user inputs.
However, the proposed auxiliary variables only contain channel-wise scale and
bias, limiting the optimization to global refinement only. In this work, in
order to generalize backpropagating refinement for a wide range of dense
prediction tasks, we introduce a set of G-BRS (Generalized Backpropagating
Refinement Scheme) layers that enable both global and localized refinement for
the following tasks: interactive segmentation, semantic segmentation, image
matting and monocular depth estimation. Experiments on SBD, Cityscapes,
Mapillary Vista, Composition-1k and NYU-Depth-V2 show that our method can
successfully generalize and significantly improve performance of existing
pretrained state-of-the-art models with only a few clicks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ACGNet: Action Complement Graph Network for Weakly-supervised Temporal Action Localization. (arXiv:2112.10977v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10977">
<div class="article-summary-box-inner">
<span><p>Weakly-supervised temporal action localization (WTAL) in untrimmed videos has
emerged as a practical but challenging task since only video-level labels are
available. Existing approaches typically leverage off-the-shelf segment-level
features, which suffer from spatial incompleteness and temporal incoherence,
thus limiting their performance. In this paper, we tackle this problem from a
new perspective by enhancing segment-level representations with a simple yet
effective graph convolutional network, namely action complement graph network
(ACGNet). It facilitates the current video segment to perceive spatial-temporal
dependencies from others that potentially convey complementary clues,
implicitly mitigating the negative effects caused by the two issues above. By
this means, the segment-level features are more discriminative and robust to
spatial-temporal variations, contributing to higher localization accuracies.
More importantly, the proposed ACGNet works as a universal module that can be
flexibly plugged into different WTAL frameworks, while maintaining the
end-to-end training fashion. Extensive experiments are conducted on the
THUMOS'14 and ActivityNet1.2 benchmarks, where the state-of-the-art results
clearly demonstrate the superiority of the proposed approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalized Few-Shot Semantic Segmentation: All You Need is Fine-Tuning. (arXiv:2112.10982v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10982">
<div class="article-summary-box-inner">
<span><p>Generalized few-shot semantic segmentation was introduced to move beyond only
evaluating few-shot segmentation models on novel classes to include testing
their ability to remember base classes. While all approaches currently are
based on meta-learning, they perform poorly and saturate in learning after
observing only a few shots. We propose the first fine-tuning solution, and
demonstrate that it addresses the saturation problem while achieving
state-of-art results on two datasets, PASCAL-$5^i$ and COCO-$20^i$. We also
show it outperforms existing methods whether fine-tuning multiple final layers
or only the final layer. Finally, we present a triplet loss regularization that
shows how to redistribute the balance of performance between novel and base
categories so that there is a smaller gap between them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learned ISTA with Error-based Thresholding for Adaptive Sparse Coding. (arXiv:2112.10985v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10985">
<div class="article-summary-box-inner">
<span><p>The learned iterative shrinkage thresholding algorithm (LISTA) introduces
deep unfolding models with learnable thresholds in some shrinkage functions for
sparse coding. Drawing on some theoretical insights, we advocate an error-based
thresholding (EBT) mechanism for LISTA, which leverages a function of the
layer-wise reconstruction error to suggest an appropriate threshold value for
each observation on each layer. We show that the EBT mechanism well
disentangles the learnable parameters in the shrinkage functions from the
reconstruction errors, making them more adaptive to the various observations.
With rigorous theoretical analyses, we show that the proposed EBT can lead to a
faster convergence on the basis of LISTA and its variants, in addition to its
higher adaptivity. Extensive experimental results confirm our theoretical
analyses and verify the effectiveness of our methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mapping industrial poultry operations at scale with deep learning and aerial imagery. (arXiv:2112.10988v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10988">
<div class="article-summary-box-inner">
<span><p>Concentrated Animal Feeding Operations (CAFOs) pose serious risks to air,
water, and public health, but have proven to be challenging to regulate. The
U.S. Government Accountability Office notes that a basic challenge is the lack
of comprehensive location information on CAFOs. We use the USDA's National
Agricultural Imagery Program (NAIP) 1m/pixel aerial imagery to detect poultry
CAFOs across the continental United States. We train convolutional neural
network (CNN) models to identify individual poultry barns and apply the best
performing model to over 42 TB of imagery to create the first national,
open-source dataset of poultry CAFOs. We validate the model predictions against
held-out validation set on poultry CAFO facility locations from 10 hand-labeled
counties in California and demonstrate that this approach has significant
potential to fill gaps in environmental monitoring.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Expansion-Squeeze-Excitation Fusion Network for Elderly Activity Recognition. (arXiv:2112.10992v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10992">
<div class="article-summary-box-inner">
<span><p>This work focuses on the task of elderly activity recognition, which is a
challenging task due to the existence of individual actions and human-object
interactions in elderly activities. Thus, we attempt to effectively aggregate
the discriminative information of actions and interactions from both RGB videos
and skeleton sequences by attentively fusing multi-modal features. Recently,
some nonlinear multi-modal fusion approaches are proposed by utilizing
nonlinear attention mechanism that is extended from Squeeze-and-Excitation
Networks (SENet). Inspired by this, we propose a novel
Expansion-Squeeze-Excitation Fusion Network (ESE-FN) to effectively address the
problem of elderly activity recognition, which learns modal and channel-wise
Expansion-Squeeze-Excitation (ESE) attentions for attentively fusing the
multi-modal features in the modal and channel-wise ways. Furthermore, we design
a new Multi-modal Loss (ML) to keep the consistency between the single-modal
features and the fused multi-modal features by adding the penalty of difference
between the minimum prediction losses on single modalities and the prediction
loss on the fused modality. Finally, we conduct experiments on a largest-scale
elderly activity dataset, i.e., ETRI-Activity3D (including 110,000+ videos, and
50+ categories), to demonstrate that the proposed ESE-FN achieves the best
accuracy compared with the state-of-the-art methods. In addition, more
extensive experimental results show that the proposed ESE-FN is also comparable
to the other methods in terms of normal action recognition task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Point spread function estimation for blind image deblurring problems based on framelet transform. (arXiv:2112.11004v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11004">
<div class="article-summary-box-inner">
<span><p>One of the most important issues in the image processing is the approximation
of the image that has been lost due to the blurring process. These types of
matters are divided into non-blind and blind problems. The second type of
problem is more complex in terms of calculations than the first problems due to
the unknown of original image and point spread function estimation. In the
present paper, an algorithm based on coarse-to-fine iterative by $l_0-\alpha
l_1$ regularization and framelet transform is introduced to approximate the
spread function estimation. Framelet transfer improves the restored kernel due
to the decomposition of the kernel to different frequencies. Also in the
proposed model fraction gradient operator is used instead of ordinary gradient
operator. The proposed method is investigated on different kinds of images such
as text, face, natural. The output of the proposed method reflects the
effectiveness of the proposed algorithm in restoring the images from blind
problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MPViT: Multi-Path Vision Transformer for Dense Prediction. (arXiv:2112.11010v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11010">
<div class="article-summary-box-inner">
<span><p>Dense computer vision tasks such as object detection and segmentation require
effective multi-scale feature representation for detecting or classifying
objects or regions with varying sizes. While Convolutional Neural Networks
(CNNs) have been the dominant architectures for such tasks, recently introduced
Vision Transformers (ViTs) aim to replace them as a backbone. Similar to CNNs,
ViTs build a simple multi-stage structure (i.e., fine-to-coarse) for
multi-scale representation with single-scale patches. In this work, with a
different perspective from existing Transformers, we explore multi-scale patch
embedding and multi-path structure, constructing the Multi-Path Vision
Transformer (MPViT). MPViT embeds features of the same size~(i.e., sequence
length) with patches of different scales simultaneously by using overlapping
convolutional patch embedding. Tokens of different scales are then
independently fed into the Transformer encoders via multiple paths and the
resulting features are aggregated, enabling both fine and coarse feature
representations at the same feature level. Thanks to the diverse, multi-scale
feature representations, our MPViTs scaling from tiny~(5M) to base~(73M)
consistently achieve superior performance over state-of-the-art Vision
Transformers on ImageNet classification, object detection, instance
segmentation, and semantic segmentation. These extensive results demonstrate
that MPViT can serve as a versatile backbone network for various vision tasks.
Code will be made publicly available at \url{https://git.io/MPViT}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">fMRI Neurofeedback Learning Patterns are Predictive of Personal and Clinical Traits. (arXiv:2112.11014v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11014">
<div class="article-summary-box-inner">
<span><p>We obtain a personal signature of a person's learning progress in a
self-neuromodulation task, guided by functional MRI (fMRI). The signature is
based on predicting the activity of the Amygdala in a second neurofeedback
session, given a similar fMRI-derived brain state in the first session. The
prediction is made by a deep neural network, which is trained on the entire
training cohort of patients. This signal, which is indicative of a person's
progress in performing the task of Amygdala modulation, is aggregated across
multiple prototypical brain states and then classified by a linear classifier
to various personal and clinical indications. The predictive power of the
obtained signature is stronger than previous approaches for obtaining a
personal signature from fMRI neurofeedback and provides an indication that a
person's learning pattern may be used as a diagnostic tool. Our code has been
made available, and data would be shared, subject to ethical approvals.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Theoretical View of Linear Backpropagation and Its Convergence. (arXiv:2112.11018v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11018">
<div class="article-summary-box-inner">
<span><p>Backpropagation is widely used for calculating gradients in deep neural
networks (DNNs). Applied often along with stochastic gradient descent (SGD) or
its variants, backpropagation is considered as a de-facto choice in a variety
of machine learning tasks including DNN training and adversarial
attack/defense. Recently, a linear variant of BP named LinBP was introduced for
generating more transferable adversarial examples for black-box adversarial
attacks, by Guo et al. Yet, it has not been theoretically studied and the
convergence analysis of such a method is lacking. This paper serves as a
complement and somewhat an extension to Guo et al.'s paper, by providing
theoretical analyses on LinBP in neural-network-involved learning tasks
including adversarial attack and model training. We demonstrate that, somewhat
surprisingly, LinBP can lead to faster convergence in these tasks in the same
hyper-parameter settings, compared to BP. We confirm our theoretical results
with extensive experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SOIT: Segmenting Objects with Instance-Aware Transformers. (arXiv:2112.11037v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11037">
<div class="article-summary-box-inner">
<span><p>This paper presents an end-to-end instance segmentation framework, termed
SOIT, that Segments Objects with Instance-aware Transformers. Inspired by
DETR~\cite{carion2020end}, our method views instance segmentation as a direct
set prediction problem and effectively removes the need for many hand-crafted
components like RoI cropping, one-to-many label assignment, and non-maximum
suppression (NMS). In SOIT, multiple queries are learned to directly reason a
set of object embeddings of semantic category, bounding-box location, and
pixel-wise mask in parallel under the global image context. The class and
bounding-box can be easily embedded by a fixed-length vector. The pixel-wise
mask, especially, is embedded by a group of parameters to construct a
lightweight instance-aware transformer. Afterward, a full-resolution mask is
produced by the instance-aware transformer without involving any RoI-based
operation. Overall, SOIT introduces a simple single-stage instance segmentation
framework that is both RoI- and NMS-free. Experimental results on the MS COCO
dataset demonstrate that SOIT outperforms state-of-the-art instance
segmentation approaches significantly. Moreover, the joint learning of multiple
tasks in a unified query embedding can also substantially improve the detection
performance. Code is available at \url{https://github.com/yuxiaodongHRI/SOIT}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Geometry-Aware Unsupervised Domain Adaptation. (arXiv:2112.11041v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11041">
<div class="article-summary-box-inner">
<span><p>Unsupervised Domain Adaptation (UDA) aims to transfer the knowledge from the
labeled source domain to the unlabeled target domain in the presence of dataset
shift. Most existing methods cannot address the domain alignment and class
discrimination well, which may distort the intrinsic data structure for
downstream tasks (e.g., classification). To this end, we propose a novel
geometry-aware model to learn the transferability and discriminability
simultaneously via nuclear norm optimization. We introduce the domain coherence
and class orthogonality for UDA from the perspective of subspace geometry. The
domain coherence will ensure the model has a larger capacity for learning
separable representations, and class orthogonality will minimize the
correlation between clusters to alleviate the misalignment. So, they are
consistent and can benefit from each other. Besides, we provide a theoretical
insight into the norm-based learning literature in UDA, which ensures the
interpretability of our model. We show that the norms of domains and clusters
are expected to be larger and smaller to enhance the transferability and
discriminability, respectively. Extensive experimental results on standard UDA
datasets demonstrate the effectiveness of our theory and model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Image Complexity in Macro-Level Neural Network Design for Medical Image Segmentation. (arXiv:2112.11065v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11065">
<div class="article-summary-box-inner">
<span><p>Recent progress in encoder-decoder neural network architecture design has led
to significant performance improvements in a wide range of medical image
segmentation tasks. However, state-of-the-art networks for a given task may be
too computationally demanding to run on affordable hardware, and thus users
often resort to practical workarounds by modifying various macro-level design
aspects. Two common examples are downsampling of the input images and reducing
the network depth to meet computer memory constraints. In this paper we
investigate the effects of these changes on segmentation performance and show
that image complexity can be used as a guideline in choosing what is best for a
given dataset. We consider four statistical measures to quantify image
complexity and evaluate their suitability on ten different public datasets. For
the purpose of our experiments we also propose two new encoder-decoder
architectures representing shallow and deep networks that are more memory
efficient than currently popular networks. Our results suggest that median
frequency is the best complexity measure in deciding about an acceptable input
downsampling factor and network depth. For high-complexity datasets, a shallow
network running on the original images may yield better segmentation results
than a deep network running on downsampled images, whereas the opposite may be
the case for low-complexity images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RC-Net: A Convolutional Neural Network for Retinal Vessel Segmentation. (arXiv:2112.11078v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11078">
<div class="article-summary-box-inner">
<span><p>Over recent years, increasingly complex approaches based on sophisticated
convolutional neural network architectures have been slowly pushing performance
on well-established benchmark datasets. In this paper, we take a step back to
examine the real need for such complexity. We present RC-Net, a fully
convolutional network, where the number of filters per layer is optimized to
reduce feature overlapping and complexity. We also used skip connections to
keep spatial information loss to a minimum by keeping the number of pooling
operations in the network to a minimum. Two publicly available retinal vessel
segmentation datasets were used in our experiments. In our experiments, RC-Net
is quite competitive, outperforming alternatives vessels segmentation methods
with two or even three orders of magnitude less trainable parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RepMLPNet: Hierarchical Vision MLP with Re-parameterized Locality. (arXiv:2112.11081v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11081">
<div class="article-summary-box-inner">
<span><p>Compared to convolutional layers, fully-connected (FC) layers are better at
modeling the long-range dependencies but worse at capturing the local patterns,
hence usually less favored for image recognition. In this paper, we propose a
methodology, Locality Injection, to incorporate local priors into an FC layer
via merging the trained parameters of a parallel conv kernel into the FC
kernel. Locality Injection can be viewed as a novel Structural
Re-parameterization method since it equivalently converts the structures via
transforming the parameters. Based on that, we propose a multi-layer-perceptron
(MLP) block named RepMLP Block, which uses three FC layers to extract features,
and a novel architecture named RepMLPNet. The hierarchical design distinguishes
RepMLPNet from the other concurrently proposed vision MLPs. As it produces
feature maps of different levels, it qualifies as a backbone model for
downstream tasks like semantic segmentation. Our results reveal that 1)
Locality Injection is a general methodology for MLP models; 2) RepMLPNet has
favorable accuracy-efficiency trade-off compared to the other MLPs; 3)
RepMLPNet is the first MLP that seamlessly transfer to Cityscapes semantic
segmentation. The code and models are available at
https://github.com/DingXiaoH/RepMLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can We Use Neural Regularization to Solve Depth Super-Resolution?. (arXiv:2112.11085v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11085">
<div class="article-summary-box-inner">
<span><p>Depth maps captured with commodity sensors often require super-resolution to
be used in applications. In this work we study a super-resolution approach
based on a variational problem statement with Tikhonov regularization where the
regularizer is parametrized with a deep neural network. This approach was
previously applied successfully in photoacoustic tomography. We experimentally
show that its application to depth map super-resolution is difficult, and
provide suggestions about the reasons for that.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EPNet++: Cascade Bi-directional Fusion for Multi-Modal 3D Object Detection. (arXiv:2112.11088v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11088">
<div class="article-summary-box-inner">
<span><p>Recently, fusing the LiDAR point cloud and camera image to improve the
performance and robustness of 3D object detection has received more and more
attention, as these two modalities naturally possess strong complementarity. In
this paper, we propose EPNet++ for multi-modal 3D object detection by
introducing a novel Cascade Bi-directional Fusion~(CB-Fusion) module and a
Multi-Modal Consistency~(MC) loss. More concretely, the proposed CB-Fusion
module boosts the plentiful semantic information of point features with the
image features in a cascade bi-directional interaction fusion manner, leading
to more comprehensive and discriminative feature representations. The MC loss
explicitly guarantees the consistency between predicted scores from two
modalities to obtain more comprehensive and reliable confidence scores. The
experiment results on the KITTI, JRDB and SUN-RGBD datasets demonstrate the
superiority of EPNet++ over the state-of-the-art methods. Besides, we emphasize
a critical but easily overlooked problem, which is to explore the performance
and robustness of a 3D detector in a sparser scene. Extensive experiments
present that EPNet++ outperforms the existing SOTA methods with remarkable
margins in highly sparse point cloud cases, which might be an available
direction to reduce the expensive cost of LiDAR sensors. Code will be released
in the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Registration of Forest Point Clouds by Global Matching of Relative Stem Positions. (arXiv:2112.11121v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11121">
<div class="article-summary-box-inner">
<span><p>Registering point clouds of forest environments is an essential prerequisite
for LiDAR applications in precision forestry. State-of-the-art methods for
forest point cloud registration require the extraction of individual tree
attributes, and they have an efficiency bottleneck when dealing with point
clouds of real-world forests with dense trees. We propose an automatic, robust,
and efficient method for the registration of forest point clouds. Our approach
first locates tree stems from raw point clouds and then matches the stems based
on their relative spatial relationship to determine the registration
transformation. In contrast to existing methods, our algorithm requires no
extra individual tree attributes and has linear complexity to the number of
trees in the environment, allowing it to align point clouds of large forest
environments. Extensive experiments have revealed that our method is superior
to the state-of-the-art methods regarding registration accuracy and robustness,
and it significantly outperforms existing techniques in terms of efficiency.
Besides, we introduce a new benchmark dataset that complements the very few
existing open datasets for the development and evaluation of registration
methods for forest point clouds.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Human Motion Prediction via Stochastic Differential Equations. (arXiv:2112.11124v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11124">
<div class="article-summary-box-inner">
<span><p>Human motion understanding and prediction is an integral aspect in our
pursuit of machine intelligence and human-machine interaction systems. Current
methods typically pursue a kinematics modeling approach, relying heavily upon
prior anatomical knowledge and constraints. However, such an approach is hard
to generalize to different skeletal model representations, and also tends to be
inadequate in accounting for the dynamic range and complexity of motion, thus
hindering predictive accuracy. In this work, we propose a novel approach in
modeling the motion prediction problem based on stochastic differential
equations and path integrals. The motion profile of each skeletal joint is
formulated as a basic stochastic variable and modeled with the Langevin
equation. We develop a strategy of employing GANs to simulate path integrals
that amounts to optimizing over possible future paths. We conduct experiments
in two large benchmark datasets, Human 3.6M and CMU MoCap. It is highlighted
that our approach achieves a 12.48% accuracy improvement over current
state-of-the-art methods in average.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cloud Sphere: A 3D Shape Representation via Progressive Deformation. (arXiv:2112.11133v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11133">
<div class="article-summary-box-inner">
<span><p>In the area of 3D shape analysis, the geometric properties of a shape have
long been studied. Instead of directly extracting representative features using
expert-designed descriptors or end-to-end deep neural networks, this paper is
dedicated to discovering distinctive information from the shape formation
process. Concretely, a spherical point cloud served as the template is
progressively deformed to fit the target shape in a coarse-to-fine manner.
During the shape formation process, several checkpoints are inserted to
facilitate recording and investigating the intermediate stages. For each stage,
the offset field is evaluated as a stage-aware description. The summation of
the offsets throughout the shape formation process can completely define the
target shape in terms of geometry. In this perspective, one can derive the
point-wise shape correspondence from the template inexpensively, which benefits
various graphic applications. In this paper, the Progressive Deformation-based
Auto-Encoder (PDAE) is proposed to learn the stage-aware description through a
coarse-to-fine shape fitting task. Experimental results show that the proposed
PDAE has the ability to reconstruct 3D shapes with high fidelity, and
consistent topology is preserved in the multi-stage deformation process.
Additional applications based on the stage-aware description are performed,
demonstrating its universality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PONet: Robust 3D Human Pose Estimation via Learning Orientations Only. (arXiv:2112.11153v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11153">
<div class="article-summary-box-inner">
<span><p>Conventional 3D human pose estimation relies on first detecting 2D body
keypoints and then solving the 2D to 3D correspondence problem.Despite the
promising results, this learning paradigm is highly dependent on the quality of
the 2D keypoint detector, which is inevitably fragile to occlusions and
out-of-image absences.In this paper,we propose a novel Pose Orientation Net
(PONet) that is able to robustly estimate 3D pose by learning orientations
only, hence bypassing the error-prone keypoint detector in the absence of image
evidence. For images with partially invisible limbs, PONet estimates the 3D
orientation of these limbs by taking advantage of the local image evidence to
recover the 3D pose.Moreover, PONet is competent to infer full 3D poses even
from images with completely invisible limbs, by exploiting the orientation
correlation between visible limbs to complement the estimated poses,further
improving the robustness of 3D pose estimation.We evaluate our method on
multiple datasets, including Human3.6M, MPII, MPI-INF-3DHP, and 3DPW. Our
method achieves results on par with state-of-the-art techniques in ideal
settings, yet significantly eliminates the dependency on keypoint detectors and
the corresponding computation burden. In highly challenging scenarios, such as
truncation and erasing, our method performs very robustly and yields much
superior results as compared to state of the art,demonstrating its potential
for real-world applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalizable Cross-modality Medical Image Segmentation via Style Augmentation and Dual Normalization. (arXiv:2112.11177v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11177">
<div class="article-summary-box-inner">
<span><p>For medical image segmentation, imagine if a model was only trained using MR
images in source domain, how about its performance to directly segment CT
images in target domain? This setting, namely generalizable cross-modality
segmentation, owning its clinical potential, is much more challenging than
other related settings, e.g., domain adaptation. To achieve this goal, we in
this paper propose a novel dual-normalization module by leveraging the
augmented source-similar and source-dissimilar images during our generalizable
segmentation. To be specific, given a single source domain, aiming to simulate
the possible appearance change in unseen target domains, we first utilize a
nonlinear transformation to augment source-similar and source-dissimilar
images. Then, to sufficiently exploit these two types of augmentations, our
proposed dual-normalization based model employs a shared backbone yet
independent batch normalization layer for separate normalization. Afterwards,
we put forward a style-based selection scheme to automatically choose the
appropriate path in the test stage. Extensive experiments on three publicly
available datasets, i.e., BraTS, Cross-Modality Cardiac and Abdominal
Multi-Organ dataset, have demonstrated that our method outperforms other
state-of-the-art domain generalization methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attention-Based Sensor Fusion for Human Activity Recognition Using IMU Signals. (arXiv:2112.11224v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11224">
<div class="article-summary-box-inner">
<span><p>Human Activity Recognition (HAR) using wearable devices such as smart watches
embedded with Inertial Measurement Unit (IMU) sensors has various applications
relevant to our daily life, such as workout tracking and health monitoring. In
this paper, we propose a novel attention-based approach to human activity
recognition using multiple IMU sensors worn at different body locations.
Firstly, a sensor-wise feature extraction module is designed to extract the
most discriminative features from individual sensors with Convolutional Neural
Networks (CNNs). Secondly, an attention-based fusion mechanism is developed to
learn the importance of sensors at different body locations and to generate an
attentive feature representation. Finally, an inter-sensor feature extraction
module is applied to learn the inter-sensor correlations, which are connected
to a classifier to output the predicted classes of activities. The proposed
approach is evaluated using five public datasets and it outperforms
state-of-the-art methods on a wide variety of activity categories.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Robustness with Image Filtering. (arXiv:2112.11235v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11235">
<div class="article-summary-box-inner">
<span><p>Adversarial robustness is one of the most challenging problems in Deep
Learning and Computer Vision research. All the state-of-the-art techniques
require a time-consuming procedure that creates cleverly perturbed images. Due
to its cost, many solutions have been proposed to avoid Adversarial Training.
However, all these attempts proved ineffective as the attacker manages to
exploit spurious correlations among pixels to trigger brittle features
implicitly learned by the model. This paper first introduces a new image
filtering scheme called Image-Graph Extractor (IGE) that extracts the
fundamental nodes of an image and their connections through a graph structure.
By leveraging the IGE representation, we build a new defense method, Filtering
As a Defense, that does not allow the attacker to entangle pixels to create
malicious patterns. Moreover, we show that data augmentation with filtered
images effectively improves the model's robustness to data corruption. We
validate our techniques on CIFAR-10, CIFAR-100, and ImageNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised deep learning techniques for powdery mildew recognition based on multispectral imaging. (arXiv:2112.11242v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11242">
<div class="article-summary-box-inner">
<span><p>Objectives. Sustainable management of plant diseases is an open challenge
which has relevant economic and environmental impact. Optimal strategies rely
on human expertise for field scouting under favourable conditions to assess the
current presence and extent of disease symptoms. This labor-intensive task is
complicated by the large field area to be scouted, combined with the
millimeter-scale size of the early symptoms to be detected. In view of this,
image-based detection of early disease symptoms is an attractive approach to
automate this process, enabling a potential high throughput monitoring at
sustainable costs.
</p>
<p>Methods. Deep learning has been successfully applied in various domains to
obtain an automatic selection of the relevant image features by learning
filters via a training procedure. Deep learning has recently entered also the
domain of plant disease detection: following this idea, in this work we present
a deep learning approach to automatically recognize powdery mildew on cucumber
leaves. We focus on unsupervised deep learning techniques applied to
multispectral imaging data and we propose the use of autoencoder architectures
to investigate two strategies for disease detection: i) clusterization of
features in a compressed space; ii) anomaly detection.
</p>
<p>Results. The two proposed approaches have been assessed by quantitative
indices. The clusterization approach is not fully capable by itself to provide
accurate predictions but it does cater relevant information. Anomaly detection
has instead a significant potential of resolution which could be further
exploited as a prior for supervised architectures with a very limited number of
labeled samples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Projected Sliced Wasserstein Autoencoder-based Hyperspectral Images Anomaly Detection. (arXiv:2112.11243v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11243">
<div class="article-summary-box-inner">
<span><p>Anomaly detection refers to identifying the observation that deviates from
the normal pattern, which has been an active research area in various domains.
Recently, the increasing data scale, complexity, and dimension turns the
traditional representation and statistical-based outlier detection method into
challenging. In this paper, we leverage the generative model in hyperspectral
images anomaly detection. The gist is to model the distribution of the normal
data, while the out-of-distribution sample can be viewed as the outlier. At
first, the variational inference-based anomaly detection methods are
investigated. We theoretically and empirically find that they are unstable due
to the strong notion of distance ($f$-divergence) served as the regularization.
Secondly, this paper introduces sliced Wasserstein distance, which is a weaker
distribution measure compared with f-divergence. However, the number of
randomly slicing poses a difficulty to estimate the true distance. In the end,
we propose a projected sliced Wasserstein (PSW) autoencoder-based anomaly
screening method. In particular, we leverage a computation-friendly
eigen-decomposition method to find the principal component as slicing the
high-dimensional data. Furthermore, our proposed distance can be calculated
with the closed-form, even the prior distribution is not Gaussian.
Comprehensive experiments conducted on various real-world hyperspectral anomaly
detection benchmarks demonstrate the superior performance of our proposed
method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hateful Memes Challenge: An Enhanced Multimodal Framework. (arXiv:2112.11244v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11244">
<div class="article-summary-box-inner">
<span><p>Hateful Meme Challenge proposed by Facebook AI has attracted contestants
around the world. The challenge focuses on detecting hateful speech in
multimodal memes. Various state-of-the-art deep learning models have been
applied to this problem and the performance on challenge's leaderboard has also
been constantly improved. In this paper, we enhance the hateful detection
framework, including utilizing Detectron for feature extraction, exploring
different setups of VisualBERT and UNITER models with different loss functions,
researching the association between the hateful memes and the sensitive text
features, and finally building ensemble method to boost model performance. The
AUROC of our fine-tuned VisualBERT, UNITER, and ensemble method achieves 0.765,
0.790, and 0.803 on the challenge's test set, respectively, which beats the
baseline models. Our code is available at
https://github.com/yatingtian/hateful-meme
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Photo-realistic Images from LiDAR Point Clouds with Generative Adversarial Networks. (arXiv:2112.11245v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11245">
<div class="article-summary-box-inner">
<span><p>We examined the feasibility of generative adversarial networks (GANs) to
generate photo-realistic images from LiDAR point clouds. For this purpose, we
created a dataset of point cloud image pairs and trained the GAN to predict
photorealistic images from LiDAR point clouds containing reflectance and
distance information. Our models learned how to predict realistically looking
images from just point cloud data, even images with black cars. Black cars are
difficult to detect directly from point clouds because of their low level of
reflectivity. This approach might be used in the future to perform visual
object recognition on photorealistic images generated from LiDAR point clouds.
In addition to the conventional LiDAR system, a second system that generates
photorealistic images from LiDAR point clouds would run simultaneously for
visual object recognition in real-time. In this way, we might preserve the
supremacy of LiDAR and benefit from using photo-realistic images for visual
object recognition without the usage of any camera. In addition, this approach
could be used to colorize point clouds without the usage of any camera images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image quality enhancement of embedded holograms in holographic information hiding using deep neural networks. (arXiv:2112.11246v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11246">
<div class="article-summary-box-inner">
<span><p>Holographic information hiding is a technique for embedding holograms or
images into another hologram, used for copyright protection and steganography
of holograms. Using deep neural networks, we offer a way to improve the visual
quality of embedded holograms. The brightness of an embedded hologram is set to
a fraction of that of the host hologram, resulting in a barely damaged
reconstructed image of the host hologram. However, it is difficult to perceive
because the embedded hologram's reconstructed image is darker than the
reconstructed host image. In this study, we use deep neural networks to restore
the darkened image.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PointCaps: Raw Point Cloud Processing using Capsule Networks with Euclidean Distance Routing. (arXiv:2112.11258v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11258">
<div class="article-summary-box-inner">
<span><p>Raw point cloud processing using capsule networks is widely adopted in
classification, reconstruction, and segmentation due to its ability to preserve
spatial agreement of the input data. However, most of the existing capsule
based network approaches are computationally heavy and fail at representing the
entire point cloud as a single capsule. We address these limitations in
existing capsule network based approaches by proposing PointCaps, a novel
convolutional capsule architecture with parameter sharing. Along with
PointCaps, we propose a novel Euclidean distance routing algorithm and a
class-independent latent representation. The latent representation captures
physically interpretable geometric parameters of the point cloud, with dynamic
Euclidean routing, PointCaps well-represents the spatial (point-to-part)
relationships of points. PointCaps has a significantly lower number of
parameters and requires a significantly lower number of FLOPs while achieving
better reconstruction with comparable classification and segmentation accuracy
for raw point clouds compared to state-of-the-art capsule networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">High-Fidelity Point Cloud Completion with Low-Resolution Recovery and Noise-Aware Upsampling. (arXiv:2112.11271v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11271">
<div class="article-summary-box-inner">
<span><p>Completing an unordered partial point cloud is a challenging task. Existing
approaches that rely on decoding a latent feature to recover the complete
shape, often lead to the completed point cloud being over-smoothing, losing
details, and noisy. Instead of decoding a whole shape, we propose to decode and
refine a low-resolution (low-res) point cloud first, and then performs a
patch-wise noise-aware upsampling rather than interpolating the whole sparse
point cloud at once, which tends to lose details. Regarding the possibility of
lacking details of the initially decoded low-res point cloud, we propose an
iterative refinement to recover the geometric details and a symmetrization
process to preserve the trustworthy information from the input partial point
cloud. After obtaining a sparse and complete point cloud, we propose a
patch-wise upsampling strategy. Patch-based upsampling allows to better recover
fine details unlike decoding a whole shape, however, the existing upsampling
methods are not applicable to completion task due to the data discrepancy
(i.e., input sparse data here is not from ground-truth). Therefore, we propose
a patch extraction approach to generate training patch pairs between the sparse
and ground-truth point clouds, and an outlier removal step to suppress the
noisy points from the sparse point cloud. Together with the low-res recovery,
our whole method is able to achieve high-fidelity point cloud completion.
Comprehensive evaluations are provided to demonstrate the effectiveness of the
proposed method and its individual components.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Review of Face Presentation Attack Detection Competitions. (arXiv:2112.11290v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11290">
<div class="article-summary-box-inner">
<span><p>Face presentation attack detection (PAD) has received increasing attention
ever since the vulnerabilities to spoofing have been widely recognized. The
state of the art in unimodal and multi-modal face anti-spoofing has been
assessed in eight international competitions organized in conjunction with
major biometrics and computer vision conferences in 2011, 2013, 2017, 2019,
2020 and 2021, each introducing new challenges to the research community. In
this chapter, we present the design and results of the five latest competitions
from 2019 until 2021. The first two challenges aimed to evaluate the
effectiveness of face PAD in multi-modal setup introducing near-infrared (NIR)
and depth modalities in addition to colour camera data, while the latest three
competitions focused on evaluating domain and attack type generalization
abilities of face PAD algorithms operating on conventional colour images and
videos. We also discuss the lessons learnt from the competitions and future
challenges in the field in general.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Implicit Neural Video Compression. (arXiv:2112.11312v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11312">
<div class="article-summary-box-inner">
<span><p>We propose a method to compress full-resolution video sequences with implicit
neural representations. Each frame is represented as a neural network that maps
coordinate positions to pixel values. We use a separate implicit network to
modulate the coordinate inputs, which enables efficient motion compensation
between frames. Together with a small residual network, this allows us to
efficiently compress P-frames relative to the previous frame. We further lower
the bitrate by storing the network weights with learned integer quantization.
Our method, which we call implicit pixel flow (IPF), offers several
simplifications over established neural video codecs: it does not require the
receiver to have access to a pretrained neural network, does not use expensive
interpolation-based warping operations, and does not require a separate
training dataset. We demonstrate the feasibility of neural implicit compression
on image and video data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">iSegFormer: Interactive Image Segmentation with Transformers. (arXiv:2112.11325v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11325">
<div class="article-summary-box-inner">
<span><p>We propose iSegFormer, a novel transformer-based approach for interactive
image segmentation. iSegFormer is built upon existing segmentation transformers
with user clicks as an additional input, allowing users to interactively and
iteratively refine the segmentation mask.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multispectral image fusion by super pixel statistics. (arXiv:2112.11329v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11329">
<div class="article-summary-box-inner">
<span><p>Multispectral image fusion is a fundamental problem of remote sensing and
image processing. This problem is addressed by both classic and deep learning
approaches. This paper is focused on the classic solutions and introduces a new
novel approach to this family. The proposed method carries out multispectral
image fusion based on the content of the fused images. It relies on analysis
based on the level of information on segmented superpixels in the fused inputs.
Specifically, I address the task of visible color RGB to Near-Infrared (NIR)
fusion. The RGB image captures the color of the scene while the NIR captures
details and sees beyond haze and clouds. Since each channel senses different
information of the scene, their fusion is challenging and interesting. The
proposed method is designed to produce a fusion that contains both advantages
of each spectra. This manuscript experiments show that the proposed method is
visually informative with respect to other classic fusion methods which can be
run fastly on embedded devices with no need for heavy computation resources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PrimSeq: a deep learning-based pipeline to quantitate rehabilitation training. (arXiv:2112.11330v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11330">
<div class="article-summary-box-inner">
<span><p>Stroke rehabilitation seeks to increase neuroplasticity through the repeated
practice of functional motions, but may have minimal impact on recovery because
of insufficient repetitions. The optimal training content and quantity are
currently unknown because no practical tools exist to measure them. Here, we
present PrimSeq, a pipeline to classify and count functional motions trained in
stroke rehabilitation. Our approach integrates wearable sensors to capture
upper-body motion, a deep learning model to predict motion sequences, and an
algorithm to tally motions. The trained model accurately decomposes
rehabilitation activities into component functional motions, outperforming
competitive machine learning methods. PrimSeq furthermore quantifies these
motions at a fraction of the time and labor costs of human experts. We
demonstrate the capabilities of PrimSeq in previously unseen stroke patients
with a range of upper extremity motor impairment. We expect that these advances
will support the rigorous measurement required for quantitative dosing trials
in stroke rehabilitation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning Based 3D Point Cloud Regression for Estimating Forest Biomass. (arXiv:2112.11335v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11335">
<div class="article-summary-box-inner">
<span><p>Knowledge of forest biomass stocks and their development is important for
implementing effective climate change mitigation measures. It is needed for
studying the processes driving af-, re-, and deforestation and is a
prerequisite for carbon-accounting. Remote sensing using airborne LiDAR can be
used to measure vegetation biomass at large scale. We present deep learning
systems for predicting wood volume, above-ground biomass (AGB), and
subsequently carbon directly from 3D LiDAR point cloud data. We devise
different neural network architectures for point cloud regression and evaluate
them on remote sensing data of areas for which AGB estimates have been obtained
from field measurements in a national forest inventory. Our adaptation of
Minkowski convolutional neural networks for regression gave the best results.
The deep neural networks produced significantly more accurate wood volume, AGB,
and carbon estimates compared to state-of-the-art approaches operating on basic
statistics of the point clouds, and we expect this finding to have a strong
impact on LiDAR-based analyses of terrestrial ecosystem dynamics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transferable End-to-end Room Layout Estimation via Implicit Encoding. (arXiv:2112.11340v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11340">
<div class="article-summary-box-inner">
<span><p>We study the problem of estimating room layouts from a single panorama image.
Most former works have two stages: feature extraction and parametric model
fitting. Here we propose an end-to-end method that directly predicts parametric
layouts from an input panorama image. It exploits an implicit encoding
procedure that embeds parametric layouts into a latent space. Then learning a
mapping from images to this latent space makes end-to-end room layout
estimation possible. However end-to-end methods have several notorious
drawbacks despite many intriguing properties. A widely raised criticism is that
they are troubled with dataset bias and do not transfer to unfamiliar domains.
Our study echos this common belief. To this end, we propose to use semantic
boundary prediction maps as an intermediate domain. It brings significant
performance boost on four benchmarks (Structured3D, PanoContext, S3DIS, and
Matterport3D), notably in the zero-shot transfer setting. Code, data, and
models will be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Watch It Move: Unsupervised Discovery of 3D Joints for Re-Posing of Articulated Objects. (arXiv:2112.11347v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11347">
<div class="article-summary-box-inner">
<span><p>Rendering articulated objects while controlling their poses is critical to
applications such as virtual reality or animation for movies. Manipulating the
pose of an object, however, requires the understanding of its underlying
structure, that is, its joints and how they interact with each other.
Unfortunately, assuming the structure to be known, as existing methods do,
precludes the ability to work on new object categories. We propose to learn
both the appearance and the structure of previously unseen articulated objects
by observing them move from multiple views, with no additional supervision,
such as joints annotations, or information about the structure. Our insight is
that adjacent parts that move relative to each other must be connected by a
joint. To leverage this observation, we model the object parts in 3D as
ellipsoids, which allows us to identify joints. We combine this explicit
representation with an implicit one that compensates for the approximation
introduced. We show that our method works for different structures, from
quadrupeds, to single-arm robots, to humans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Object Detection Using Knowledge Graph Embeddings. (arXiv:2112.11366v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11366">
<div class="article-summary-box-inner">
<span><p>Object recognition for the most part has been approached as a one-hot problem
that treats classes to be discrete and unrelated. Each image region has to be
assigned to one member of a set of objects, including a background class,
disregarding any similarities in the object types. In this work, we compare the
error statistics of the class embeddings learned from a one-hot approach with
semantically structured embeddings from natural language processing or
knowledge graphs that are widely applied in open world object detection.
Extensive experimental results on multiple knowledge-embeddings as well as
distance metrics indicate that knowledge-based class representations result in
more semantically grounded misclassifications while performing on par compared
to one-hot methods on the challenging COCO and Cityscapes object detection
benchmarks. We generalize our findings to multiple object detection
architectures by proposing a knowledge-embedded design for keypoint-based and
transformer-based object detection architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shape from Polarization for Complex Scenes in the Wild. (arXiv:2112.11377v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11377">
<div class="article-summary-box-inner">
<span><p>We present a new data-driven approach with physics-based priors to
scene-level normal estimation from a single polarization image. Existing shape
from polarization (SfP) works mainly focus on estimating the normal of a single
object rather than complex scenes in the wild. A key barrier to high-quality
scene-level SfP is the lack of real-world SfP data in complex scenes. Hence, we
contribute the first real-world scene-level SfP dataset with paired input
polarization images and ground-truth normal maps. Then we propose a
learning-based framework with a multi-head self-attention module and viewing
encoding, which is designed to handle increasing polarization ambiguities
caused by complex materials and non-orthographic projection in scene-level SfP.
Our trained model can be generalized to far-field outdoor scenes as the
relationship between polarized light and surface normals is not affected by
distance. Experimental results demonstrate that our approach significantly
outperforms existing SfP models on two datasets. Our dataset and source code
will be publicly available at \url{https://github.com/ChenyangLEI/sfp-wild}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A novel approach for the automated segmentation and volume quantification of cardiac fats on computed tomography. (arXiv:2112.11381v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11381">
<div class="article-summary-box-inner">
<span><p>The deposits of fat on the surroundings of the heart are correlated to
several health risk factors such as atherosclerosis, carotid stiffness,
coronary artery calcification, atrial fibrillation and many others. These
deposits vary unrelated to obesity, which reinforces its direct segmentation
for further quantification. However, manual segmentation of these fats has not
been widely deployed in clinical practice due to the required human workload
and consequential high cost of physicians and technicians. In this work, we
propose a unified method for an autonomous segmentation and quantification of
two types of cardiac fats. The segmented fats are termed epicardial and
mediastinal, and stand apart from each other by the pericardium. Much effort
was devoted to achieve minimal user intervention. The proposed methodology
mainly comprises registration and classification algorithms to perform the
desired segmentation. We compare the performance of several classification
algorithms on this task, including neural networks, probabilistic models and
decision tree algorithms. Experimental results of the proposed methodology have
shown that the mean accuracy regarding both epicardial and mediastinal fats is
98.5% (99.5% if the features are normalized), with a mean true positive rate of
98.0%. In average, the Dice similarity index was equal to 97.6%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sports Video: Fine-Grained Action Detection and Classification of Table Tennis Strokes from Videos for MediaEval 2021. (arXiv:2112.11384v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11384">
<div class="article-summary-box-inner">
<span><p>Sports video analysis is a prevalent research topic due to the variety of
application areas, ranging from multimedia intelligent devices with
user-tailored digests up to analysis of athletes' performance. The Sports Video
task is part of the MediaEval 2021 benchmark. This task tackles fine-grained
action detection and classification from videos. The focus is on recordings of
table tennis games. Running since 2019, the task has offered a classification
challenge from untrimmed video recorded in natural conditions with known
temporal boundaries for each stroke. This year, the dataset is extended and
offers, in addition, a detection challenge from untrimmed videos without
annotations. This work aims at creating tools for sports coaches and players in
order to analyze sports performance. Movement analysis and player profiling may
be built upon such technology to enrich the training experience of athletes and
improve their performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ADJUST: A Dictionary-Based Joint Reconstruction and Unmixing Method for Spectral Tomography. (arXiv:2112.11406v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11406">
<div class="article-summary-box-inner">
<span><p>Advances in multi-spectral detectors are causing a paradigm shift in X-ray
Computed Tomography (CT). Spectral information acquired from these detectors
can be used to extract volumetric material composition maps of the object of
interest. If the materials and their spectral responses are known a priori, the
image reconstruction step is rather straightforward. If they are not known,
however, the maps as well as the responses need to be estimated jointly. A
conventional workflow in spectral CT involves performing volume reconstruction
followed by material decomposition, or vice versa. However, these methods
inherently suffer from the ill-posedness of the joint reconstruction problem.
To resolve this issue, we propose `A Dictionary-based Joint reconstruction and
Unmixing method for Spectral Tomography' (ADJUST). Our formulation relies on
forming a dictionary of spectral signatures of materials common in CT and prior
knowledge of the number of materials present in an object. In particular, we
decompose the spectral volume linearly in terms of spatial material maps, a
spectral dictionary, and the indicator of materials for the dictionary
elements. We propose a memory-efficient accelerated alternating proximal
gradient method to find an approximate solution to the resulting bi-convex
problem. From numerical demonstrations on several synthetic phantoms, we
observe that ADJUST performs exceedingly well when compared to other
state-of-the-art methods. Additionally, we address the robustness of ADJUST
against limited measurement patterns.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StyleSDF: High-Resolution 3D-Consistent Image and Geometry Generation. (arXiv:2112.11427v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11427">
<div class="article-summary-box-inner">
<span><p>We introduce a high resolution, 3D-consistent image and shape generation
technique which we call StyleSDF. Our method is trained on single-view RGB data
only, and stands on the shoulders of StyleGAN2 for image generation, while
solving two main challenges in 3D-aware GANs: 1) high-resolution,
view-consistent generation of the RGB images, and 2) detailed 3D shape. We
achieve this by merging a SDF-based 3D representation with a style-based 2D
generator. Our 3D implicit network renders low-resolution feature maps, from
which the style-based network generates view-consistent, 1024x1024 images.
Notably, our SDF-based 3D modeling defines detailed 3D surfaces, leading to
consistent volume rendering. Our method shows higher quality results compared
to state of the art in terms of visual and geometric quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learned Queries for Efficient Local Attention. (arXiv:2112.11435v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11435">
<div class="article-summary-box-inner">
<span><p>Vision Transformers (ViT) serve as powerful vision models. Unlike
convolutional neural networks, which dominated vision research in previous
years, vision transformers enjoy the ability to capture long-range dependencies
in the data. Nonetheless, an integral part of any transformer architecture, the
self-attention mechanism, suffers from high latency and inefficient memory
utilization, making it less suitable for high-resolution input images. To
alleviate these shortcomings, hierarchical vision models locally employ
self-attention on non-interleaving windows. This relaxation reduces the
complexity to be linear in the input size; however, it limits the cross-window
interaction, hurting the model performance. In this paper, we propose a new
shift-invariant local attention layer, called query and attend (QnA), that
aggregates the input locally in an overlapping manner, much like convolutions.
The key idea behind QnA is to introduce learned queries, which allow fast and
efficient implementation. We verify the effectiveness of our layer by
incorporating it into a hierarchical vision transformer model. We show
improvements in speed and memory complexity while achieving comparable accuracy
with state-of-the-art models. Finally, our layer scales especially well with
window size, requiring up-to x10 less memory while being up-to x5 faster than
existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Modality Distillation via Learning the teacher's modality-level Gram Matrix. (arXiv:2112.11447v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11447">
<div class="article-summary-box-inner">
<span><p>In the context of multi-modality knowledge distillation research, the
existing methods was mainly focus on the problem of only learning teacher final
output. Thus, there are still deep differences between the teacher network and
the student network. It is necessary to force the student network to learn the
modality relationship information of the teacher network. To effectively
exploit transfering knowledge from teachers to students, a novel modality
relation distillation paradigm by modeling the relationship information among
different modality are adopted, that is learning the teacher modality-level
Gram Matrix.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Max-Margin Contrastive Learning. (arXiv:2112.11450v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11450">
<div class="article-summary-box-inner">
<span><p>Standard contrastive learning approaches usually require a large number of
negatives for effective unsupervised learning and often exhibit slow
convergence. We suspect this behavior is due to the suboptimal selection of
negatives used for offering contrast to the positives. We counter this
difficulty by taking inspiration from support vector machines (SVMs) to present
max-margin contrastive learning (MMCL). Our approach selects negatives as the
sparse support vectors obtained via a quadratic optimization problem, and
contrastiveness is enforced by maximizing the decision margin. As SVM
optimization can be computationally demanding, especially in an end-to-end
setting, we present simplifications that alleviate the computational burden. We
validate our approach on standard vision benchmark datasets, demonstrating
better performance in unsupervised representation learning over
state-of-the-art, while having better empirical convergence properties.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GOAL: Generating 4D Whole-Body Motion for Hand-Object Grasping. (arXiv:2112.11454v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11454">
<div class="article-summary-box-inner">
<span><p>Generating digital humans that move realistically has many applications and
is widely studied, but existing methods focus on the major limbs of the body,
ignoring the hands and head. Hands have been separately studied but the focus
has been on generating realistic static grasps of objects. To synthesize
virtual characters that interact with the world, we need to generate full-body
motions and realistic hand grasps simultaneously. Both sub-problems are
challenging on their own and, together, the state-space of poses is
significantly larger, the scales of hand and body motions differ, and the
whole-body posture and the hand grasp must agree, satisfy physical constraints,
and be plausible. Additionally, the head is involved because the avatar must
look at the object to interact with it. For the first time, we address the
problem of generating full-body, hand and head motions of an avatar grasping an
unknown object. As input, our method, called GOAL, takes a 3D object, its
position, and a starting 3D body pose and shape. GOAL outputs a sequence of
whole-body poses using two novel networks. First, GNet generates a goal
whole-body grasp with a realistic body, head, arm, and hand pose, as well as
hand-object contact. Second, MNet generates the motion between the starting and
goal pose. This is challenging, as it requires the avatar to walk towards the
object with foot-ground contact, orient the head towards it, reach out, and
grasp it with a realistic hand pose and hand-object contact. To achieve this
the networks exploit a representation that combines SMPL-X body parameters and
3D vertex offsets. We train and evaluate GOAL, both qualitatively and
quantitatively, on the GRAB dataset. Results show that GOAL generalizes well to
unseen objects, outperforming baselines. GOAL takes a step towards synthesizing
realistic full-body object grasping.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TPPO: A Novel Trajectory Predictor with Pseudo Oracle. (arXiv:2002.01852v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.01852">
<div class="article-summary-box-inner">
<span><p>Forecasting pedestrian trajectories in dynamic scenes remains a critical
problem in various applications, such as autonomous driving and socially aware
robots. Such forecasting is challenging due to human-human and human-object
interactions and future uncertainties caused by human randomness. Generative
model-based methods handle future uncertainties by sampling a latent variable.
However, few studies explored the generation of the latent variable. In this
work, we propose the Trajectory Predictor with Pseudo Oracle (TPPO), which is a
generative model-based trajectory predictor. The first pseudo oracle is
pedestrians' moving directions, and the second one is the latent variable
estimated from ground truth trajectories. A social attention module is used to
aggregate neighbors' interactions based on the correlation between pedestrians'
moving directions and future trajectories. This correlation is inspired by the
fact that pedestrians' future trajectories are often influenced by pedestrians
in front. A latent variable predictor is proposed to estimate latent variable
distributions from observed and ground-truth trajectories. Moreover, the gap
between these two distributions is minimized during training. Therefore, the
latent variable predictor can estimate the latent variable from observed
trajectories to approximate that estimated from ground-truth trajectories. We
compare the performance of TPPO with related methods on several public
datasets. Results demonstrate that TPPO outperforms state-of-the-art methods
with low average and final displacement errors. The ablation study shows that
the prediction performance will not dramatically decrease as sampling times
decline during tests.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial images for the primate brain. (arXiv:2011.05623v2 [q-bio.NC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.05623">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks (CNNs) are vulnerable to adversarial attack,
the phenomenon that adding minuscule noise to an image can fool CNNs into
misclassifying it. Because this noise is nearly imperceptible to human viewers,
biological vision is assumed to be robust to adversarial attack. Despite this
apparent difference in robustness, CNNs are currently the best models of
biological vision, revealing a gap in explaining how the brain responds to
adversarial images. Indeed, sensitivity to adversarial attack has not been
measured for biological vision under normal conditions, nor have attack methods
been specifically designed to affect biological vision. We studied the effects
of adversarial attack on primate vision, measuring both monkey neuronal
responses and human behavior. Adversarial images were created by modifying
images from one category(such as human faces) to look like a target
category(such as monkey faces), while limiting pixel value change. We tested
three attack directions via several attack methods, including directly using
CNN adversarial images and using a CNN-based predictive model to guide monkey
visual neuron responses. We considered a wide range of image change magnitudes
that covered attack success rates up to&gt;90%. We found that adversarial images
designed for CNNs were ineffective in attacking primate vision. Even when
considering the best attack method, primate vision was more robust to
adversarial attack than an ensemble of CNNs, requiring over 100-fold larger
image change to attack successfully. The success of individual attack methods
and images was correlated between monkey neurons and human behavior, but was
less correlated between either and CNN categorization. Consistently, CNN-based
models of neurons, when trained on natural images, did not generalize to
explain neuronal responses to adversarial images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">iToF2dToF: A Robust and Flexible Representation for Data-Driven Time-of-Flight Imaging. (arXiv:2103.07087v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.07087">
<div class="article-summary-box-inner">
<span><p>Indirect Time-of-Flight (iToF) cameras are a promising depth sensing
technology. However, they are prone to errors caused by multi-path interference
(MPI) and low signal-to-noise ratio (SNR). Traditional methods, after
denoising, mitigate MPI by estimating a transient image that encodes depths.
Recently, data-driven methods that jointly denoise and mitigate MPI have become
state-of-the-art without using the intermediate transient representation. In
this paper, we propose to revisit the transient representation. Using
data-driven priors, we interpolate/extrapolate iToF frequencies and use them to
estimate the transient image. Given direct ToF (dToF) sensors capture transient
images, we name our method iToF2dToF. The transient representation is flexible.
It can be integrated with different rule-based depth sensing algorithms that
are robust to low SNR and can deal with ambiguous scenarios that arise in
practice (e.g., specular MPI, optical cross-talk). We demonstrate the benefits
of iToF2dToF over previous methods in real depth sensing scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Trainable Multi-Instance Pose Estimation with Transformers. (arXiv:2103.12115v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.12115">
<div class="article-summary-box-inner">
<span><p>We propose an end-to-end trainable approach for multi-instance pose
estimation, called POET (POse Estimation Transformer). Combining a
convolutional neural network with a transformer encoder-decoder architecture,
we formulate multiinstance pose estimation from images as a direct set
prediction problem. Our model is able to directly regress the pose of all
individuals, utilizing a bipartite matching scheme. POET is trained using a
novel set-based global loss that consists of a keypoint loss, a visibility loss
and a class loss. POET reasons about the relations between multiple detected
individuals and the full image context to directly predict their poses in
parallel. We show that POET achieves high accuracy on the COCO keypoint
detection task while having less parameters and higher inference speed than
other bottom-up and top-down approaches. Moreover, we show successful transfer
learning when applying POET to animal pose estimation. To the best of our
knowledge, this model is the first end-to-end trainable multi-instance pose
estimation method and we hope it will serve as a simple and promising
alternative.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scene Graphs: A Survey of Generations and Applications. (arXiv:2104.01111v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01111">
<div class="article-summary-box-inner">
<span><p>Scene graph is a structured representation of a scene that can clearly
express the objects, attributes, and relationships between objects in the
scene. As computer vision technology continues to develop, people are no longer
satisfied with simply detecting and recognizing objects in images; instead,
people look forward to a higher level of understanding and reasoning about
visual scenes. For example, given an image, we want to not only detect and
recognize objects in the image, but also know the relationship between objects
(visual relationship detection), and generate a text description (image
captioning) based on the image content. Alternatively, we might want the
machine to tell us what the little girl in the image is doing (Visual Question
Answering (VQA)), or even remove the dog from the image and find similar images
(image editing and retrieval), etc. These tasks require a higher level of
understanding and reasoning for image vision tasks. The scene graph is just
such a powerful tool for scene understanding. Therefore, scene graphs have
attracted the attention of a large number of researchers, and related research
is often cross-modal, complex, and rapidly developing. However, no relatively
systematic survey of scene graphs exists at present. To this end, this survey
conducts a comprehensive investigation of the current scene graph research.
More specifically, we first summarized the general definition of the scene
graph, then conducted a comprehensive and systematic discussion on the
generation method of the scene graph (SGG) and the SGG with the aid of prior
knowledge. We then investigated the main applications of scene graphs and
summarized the most commonly used datasets. Finally, we provide some insights
into the future development of scene graphs. We believe this will be a very
helpful foundation for future research on scene graphs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nested Hierarchical Transformer: Towards Accurate, Data-Efficient andInterpretable Visual Understanding. (arXiv:2105.12723v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.12723">
<div class="article-summary-box-inner">
<span><p>Hierarchical structures are popular in recent vision transformers, however,
they require sophisticated designs and massive datasets to work well. In this
paper, we explore the idea of nesting basic local transformers on
non-overlapping image blocks and aggregating them in a hierarchical way. We
find that the block aggregation function plays a critical role in enabling
cross-block non-local information communication. This observation leads us to
design a simplified architecture that requires minor code changes upon the
original vision transformer. The benefits of the proposed judiciously-selected
design are threefold: (1) NesT converges faster and requires much less training
data to achieve good generalization on both ImageNet and small datasets like
CIFAR; (2) when extending our key ideas to image generation, NesT leads to a
strong decoder that is 8$\times$ faster than previous transformer-based
generators; and (3) we show that decoupling the feature learning and
abstraction processes via this nested hierarchy in our design enables
constructing a novel method (named GradCAT) for visually interpreting the
learned model. Source code is available
https://github.com/google-research/nested-transformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-Shot Segmentation via Cycle-Consistent Transformer. (arXiv:2106.02320v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02320">
<div class="article-summary-box-inner">
<span><p>Few-shot segmentation aims to train a segmentation model that can fast adapt
to novel classes with few exemplars. The conventional training paradigm is to
learn to make predictions on query images conditioned on the features from
support images. Previous methods only utilized the semantic-level prototypes of
support images as conditional information. These methods cannot utilize all
pixel-wise support information for the query predictions, which is however
critical for the segmentation task. In this paper, we focus on utilizing
pixel-wise relationships between support and query images to facilitate the
few-shot segmentation task. We design a novel Cycle-Consistent TRansformer
(CyCTR) module to aggregate pixel-wise support features into query ones. CyCTR
performs cross-attention between features from different images, i.e. support
and query images. We observe that there may exist unexpected irrelevant
pixel-level support features. Directly performing cross-attention may aggregate
these features from support to query and bias the query features. Thus, we
propose using a novel cycle-consistent attention mechanism to filter out
possible harmful support features and encourage query features to attend to the
most informative pixels from support images. Experiments on all few-shot
segmentation benchmarks demonstrate that our proposed CyCTR leads to remarkable
improvement compared to previous state-of-the-art methods. Specifically, on
Pascal-$5^i$ and COCO-$20^i$ datasets, we achieve 66.6% and 45.6% mIoU for
5-shot segmentation, outperforming previous state-of-the-art methods by 4.6%
and 7.1% respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Dynamic Spatial-temporal Attention Network for Early Anticipation of Traffic Accidents. (arXiv:2106.10197v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.10197">
<div class="article-summary-box-inner">
<span><p>The rapid advancement of sensor technologies and artificial intelligence are
creating new opportunities for traffic safety enhancement. Dashboard cameras
(dashcams) have been widely deployed on both human driving vehicles and
automated driving vehicles. A computational intelligence model that can
accurately and promptly predict accidents from the dashcam video will enhance
the preparedness for accident prevention. The spatial-temporal interaction of
traffic agents is complex. Visual cues for predicting a future accident are
embedded deeply in dashcam video data. Therefore, the early anticipation of
traffic accidents remains a challenge. Inspired by the attention behavior of
humans in visually perceiving accident risks, this paper proposes a Dynamic
Spatial-Temporal Attention (DSTA) network for the early accident anticipation
from dashcam videos. The DSTA-network learns to select discriminative temporal
segments of a video sequence with a Dynamic Temporal Attention (DTA) module. It
also learns to focus on the informative spatial regions of frames with a
Dynamic Spatial Attention (DSA) module. A Gated Recurrent Unit (GRU) is trained
jointly with the attention modules to predict the probability of a future
accident. The evaluation of the DSTA-network on two benchmark datasets confirms
that it has exceeded the state-of-the-art performance. A thorough ablation
study that assesses the DSTA-network at the component level reveals how the
network achieves such performance. Furthermore, this paper proposes a method to
fuse the prediction scores from two complementary models and verifies its
effectiveness in further boosting the performance of early accident
anticipation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Deep Learning Technique for Video Segmentation. (arXiv:2107.01153v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.01153">
<div class="article-summary-box-inner">
<span><p>Video segmentation, i.e., partitioning video frames into multiple segments or
objects, plays a critical role in a broad range of practical applications,
e.g., visual effect assistance in movie, scene understanding in autonomous
driving, and virtual background creation in video conferencing, to name a few.
Recently, due to the renaissance of connectionism in computer vision, there has
been an influx of numerous deep learning based approaches that have been
dedicated to video segmentation and delivered compelling performance. In this
survey, we comprehensively review two basic lines of research in this area,
i.e., generic object segmentation (of unknown categories) in videos and video
semantic segmentation, by introducing their respective task settings,
background concepts, perceived need, development history, and main challenges.
We also provide a detailed overview of representative literature on both
methods and datasets. Additionally, we present quantitative performance
comparisons of the reviewed methods on benchmark datasets. At last, we point
out a set of unsolved open issues in this field, and suggest possible
opportunities for further research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ANCER: Anisotropic Certification via Sample-wise Volume Maximization. (arXiv:2107.04570v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.04570">
<div class="article-summary-box-inner">
<span><p>Randomized smoothing has recently emerged as an effective tool that enables
certification of deep neural network classifiers at scale. All prior art on
randomized smoothing has focused on isotropic $\ell_p$ certification, which has
the advantage of yielding certificates that can be easily compared among
isotropic methods via $\ell_p$-norm radius. However, isotropic certification
limits the region that can be certified around an input to worst-case
adversaries, i.e., it cannot reason about other "close", potentially large,
constant prediction safe regions. To alleviate this issue, (i) we theoretically
extend the isotropic randomized smoothing $\ell_1$ and $\ell_2$ certificates to
their generalized anisotropic counterparts following a simplified analysis.
Moreover, (ii) we propose evaluation metrics allowing for the comparison of
general certificates - a certificate is superior to another if it certifies a
superset region - with the quantification of each certificate through the
volume of the certified region. We introduce ANCER, a practical framework for
obtaining anisotropic certificates for a given test set sample via volume
maximization. Our empirical results demonstrate that ANCER achieves
state-of-the-art $\ell_1$ and $\ell_2$ certified accuracy on both CIFAR-10 and
ImageNet at multiple radii, while certifying substantially larger regions in
terms of volume, thus highlighting the benefits of moving away from isotropic
analysis. Our code is available in https://github.com/MotasemAlfarra/ANCER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SynthSeg: Domain Randomisation for Segmentation of Brain Scans of any Contrast and Resolution. (arXiv:2107.09559v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.09559">
<div class="article-summary-box-inner">
<span><p>Despite advances in data augmentation and transfer learning, convolutional
neural networks (CNNs) difficultly generalise to unseen domains. When
segmenting brain scans, CNNs are highly sensitive to changes in resolution and
contrast: even within the same MRI modality, performance can decrease across
datasets. Here we introduce SynthSeg, the first segmentation CNN agnostic to
contrast and resolution. SynthSeg is trained with synthetic data sampled from a
generative model conditioned on segmentations. Crucially, we adopt a domain
randomisation strategy where we fully randomise the contrast and resolution of
the synthetic training data. Consequently, SynthSeg can segment real scans of
any target domain without retraining or fine-tuning, which enables, for the
first time, analysis of huge amounts of heterogeneous clinical data. Because
SynthSeg only requires segmentations to be trained (no images), it can learn
from labels obtained by automated methods on subjects of different populations
(e.g., ageing and diseased), thus achieving robustness to a wide range of
morphological variability. We demonstrate SynthSeg on 5,300 scans of six
modalities and ten resolutions, where it exhibits unparalleled generalisation
compared with supervised CNNs, state-of-the-art domain adaptation, and Bayesian
segmentation. Finally, we demonstrate the generalisability of SynthSeg by
applying it to cardiac MRI and CT segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ELSED: Enhanced Line SEgment Drawing. (arXiv:2108.03144v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03144">
<div class="article-summary-box-inner">
<span><p>Detecting local features, such as corners, segments or blobs, is the first
step in the pipeline of many Computer Vision applications. Its speed is crucial
for real-time applications. In this paper we present ELSED, the fastest line
segment detector in the literature. The key for its efficiency is a local
segment growing algorithm that connects gradient-aligned pixels in presence of
small discontinuities. The proposed algorithm not only runs in devices with
very low end hardware, but may also be parametrized to foster the detection of
short or longer segments, depending on the task at hand. We also introduce new
metrics to evaluate the accuracy and repeatability of segment detectors. In our
experiments with different public benchmarks we prove that our method accounts
the highest repeatability and it is the most efficient in the literature. In
the experiments we quantify the accuracy traded for such gain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Significance of Question Encoder Sequence Model in the Out-of-Distribution Performance in Visual Question Answering. (arXiv:2108.12585v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12585">
<div class="article-summary-box-inner">
<span><p>Generalizing beyond the experiences has a significant role in developing
practical AI systems. It has been shown that current Visual Question Answering
(VQA) models are over-dependent on the language-priors (spurious correlations
between question-types and their most frequent answers) from the train set and
pose poor performance on Out-of-Distribution (OOD) test sets. This conduct
limits their generalizability and restricts them from being utilized in
real-world situations. This paper shows that the sequence model architecture
used in the question-encoder has a significant role in the generalizability of
VQA models. To demonstrate this, we performed a detailed analysis of various
existing RNN-based and Transformer-based question-encoders, and along, we
proposed a novel Graph attention network (GAT)-based question-encoder. Our
study found that a better choice of sequence model in the question-encoder
improves the generalizability of VQA models even without using any additional
relatively complex bias-mitigation approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimal Target Shape for LiDAR Pose Estimation. (arXiv:2109.01181v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01181">
<div class="article-summary-box-inner">
<span><p>Targets are essential in problems such as object tracking in cluttered or
textureless environments, camera (and multi-sensor) calibration tasks, and
simultaneous localization and mapping (SLAM). Target shapes for these tasks
typically are symmetric (square, rectangular, or circular) and work well for
structured, dense sensor data such as pixel arrays (i.e., image). However,
symmetric shapes lead to pose ambiguity when using sparse sensor data such as
LiDAR point clouds and suffer from the quantization uncertainty of the LiDAR.
This paper introduces the concept of optimizing target shape to remove pose
ambiguity for LiDAR point clouds. A target is designed to induce large
gradients at edge points under rotation and translation relative to the LiDAR
to ameliorate the quantization uncertainty associated with point cloud
sparseness. Moreover, given a target shape, we present a means that leverages
the target's geometry to estimate the target's vertices while globally
estimating the pose. Both the simulation and the experimental results (verified
by a motion capture system) confirm that by using the optimal shape and the
global solver, we achieve centimeter error in translation and a few degrees in
rotation even when a partially illuminated target is placed 30 meters away. All
the implementations and datasets are available at
https://github.com/UMich-BipedLab/optimal_shape_global_pose_estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Tumor Segmentation through Layer Decomposition. (arXiv:2109.03230v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03230">
<div class="article-summary-box-inner">
<span><p>In this paper, we target self-supervised representation learning for
zero-shot tumor segmentation. We make the following contributions: First, we
advocate a zero-shot setting, where models from pre-training should be directly
applicable for the downstream task, without using any manual annotations.
Second, we take inspiration from "layer-decomposition", and innovate on the
training regime with simulated tumor data. Third, we conduct extensive ablation
studies to analyse the critical components in data simulation, and validate the
necessity of different proxy tasks. We demonstrate that, with sufficient
texture randomization in simulation, model trained on synthetic data can
effortlessly generalise to segment real tumor data. Forth, our approach
achieves superior results for zero-shot tumor segmentation on different
downstream datasets, BraTS2018 for brain tumor segmentation and LiTS2017 for
liver tumor segmentation. While evaluating the model transferability for tumor
segmentation under a low-annotation regime, the proposed approach also
outperforms all existing self-supervised approaches, opening up the usage of
self-supervised learning in practical scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sign-MAML: Efficient Model-Agnostic Meta-Learning by SignSGD. (arXiv:2109.07497v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07497">
<div class="article-summary-box-inner">
<span><p>We propose a new computationally-efficient first-order algorithm for
Model-Agnostic Meta-Learning (MAML). The key enabling technique is to interpret
MAML as a bilevel optimization (BLO) problem and leverage the sign-based
SGD(signSGD) as a lower-level optimizer of BLO. We show that MAML, through the
lens of signSGD-oriented BLO, naturally yields an alternating optimization
scheme that just requires first-order gradients of a learned meta-model. We
term the resulting MAML algorithm Sign-MAML. Compared to the conventional
first-order MAML (FO-MAML) algorithm, Sign-MAML is theoretically-grounded as it
does not impose any assumption on the absence of second-order derivatives
during meta training. In practice, we show that Sign-MAML outperforms FO-MAML
in various few-shot image classification tasks, and compared to MAML, it
achieves a much more graceful tradeoff between classification accuracy and
computation efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting 3D shapes, masks, and properties of materials, liquids, and objects inside transparent containers, using the TransProteus CGI dataset. (arXiv:2109.07577v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07577">
<div class="article-summary-box-inner">
<span><p>We present TransProteus, a dataset, and methods for predicting the 3D
structure, masks, and properties of materials, liquids, and objects inside
transparent vessels from a single image without prior knowledge of the image
source and camera parameters. Manipulating materials in transparent containers
is essential in many fields and depends heavily on vision. This work supplies a
new procedurally generated dataset consisting of 50k images of liquids and
solid objects inside transparent containers. The image annotations include 3D
models, material properties (color/transparency/roughness...), and segmentation
masks for the vessel and its content. The synthetic (CGI) part of the dataset
was procedurally generated using 13k different objects, 500 different
environments (HDRI), and 1450 material textures (PBR) combined with simulated
liquids and procedurally generated vessels. In addition, we supply 104
real-world images of objects inside transparent vessels with depth maps of both
the vessel and its content. We propose a camera agnostic method that predicts
3D models from an image as an XYZ map. This allows the trained net to predict
the 3D model as a map with XYZ coordinates per pixel without prior knowledge of
the image source. To calculate the training loss, we use the distance between
pairs of points inside the 3D model instead of the absolute XYZ coordinates.
This makes the loss function translation invariant. We use this to predict 3D
models of vessels and their content from a single image. Finally, we
demonstrate a net that uses a single image to predict the material properties
of the vessel content and surface.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HUMBI: A Large Multiview Dataset of Human Body Expressions and Benchmark Challenge. (arXiv:2110.00119v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00119">
<div class="article-summary-box-inner">
<span><p>This paper presents a new large multiview dataset called HUMBI for human body
expressions with natural clothing. The goal of HUMBI is to facilitate modeling
view-specific appearance and geometry of five primary body signals including
gaze, face, hand, body, and garment from assorted people. 107 synchronized HD
cameras are used to capture 772 distinctive subjects across gender, ethnicity,
age, and style. With the multiview image streams, we reconstruct high fidelity
body expressions using 3D mesh models, which allows representing view-specific
appearance. We demonstrate that HUMBI is highly effective in learning and
reconstructing a complete human model and is complementary to the existing
datasets of human body expressions with limited views and subjects such as
MPII-Gaze, Multi-PIE, Human3.6M, and Panoptic Studio datasets. Based on HUMBI,
we formulate a new benchmark challenge of a pose-guided appearance rendering
task that aims to substantially extend photorealism in modeling diverse human
expressions in 3D, which is the key enabling factor of authentic social
tele-presence. HUMBI is publicly available at <a href="http://humbi-data.net">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SkullEngine: A Multi-stage CNN Framework for Collaborative CBCT Image Segmentation and Landmark Detection. (arXiv:2110.03828v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03828">
<div class="article-summary-box-inner">
<span><p>We propose a multi-stage coarse-to-fine CNN-based framework, called
SkullEngine, for high-resolution segmentation and large-scale landmark
detection through a collaborative, integrated, and scalable JSD model and three
segmentation and landmark detection refinement models. We evaluated our
framework on a clinical dataset consisting of 170 CBCT/CT images for the task
of segmenting 2 bones (midface and mandible) and detecting 175 clinically
common landmarks on bones, teeth, and soft tissues.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HIERMATCH: Leveraging Label Hierarchies for Improving Semi-Supervised Learning. (arXiv:2111.00164v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00164">
<div class="article-summary-box-inner">
<span><p>Semi-supervised learning approaches have emerged as an active area of
research to combat the challenge of obtaining large amounts of annotated data.
Towards the goal of improving the performance of semi-supervised learning
methods, we propose a novel framework, HIERMATCH, a semi-supervised approach
that leverages hierarchical information to reduce labeling costs and performs
as well as a vanilla semi-supervised learning method. Hierarchical information
is often available as prior knowledge in the form of coarse labels (e.g.,
woodpeckers) for images with fine-grained labels (e.g., downy woodpeckers or
golden-fronted woodpeckers). However, the use of supervision using coarse
category labels to improve semi-supervised techniques has not been explored. In
the absence of fine-grained labels, HIERMATCH exploits the label hierarchy and
uses coarse class labels as a weak supervisory signal. Additionally, HIERMATCH
is a generic-approach to improve any semisupervised learning framework, we
demonstrate this using our results on recent state-of-the-art techniques
MixMatch and FixMatch. We evaluate the efficacy of HIERMATCH on two benchmark
datasets, namely CIFAR-100 and NABirds. HIERMATCH can reduce the usage of
fine-grained labels by 50% on CIFAR-100 with only a marginal drop of 0.59% in
top-1 accuracy as compared to MixMatch. Code:
https://github.com/07Agarg/HIERMATCH
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Denoised Internal Models: a Brain-Inspired Autoencoder against Adversarial Attacks. (arXiv:2111.10844v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10844">
<div class="article-summary-box-inner">
<span><p>Despite its great success, deep learning severely suffers from robustness;
that is, deep neural networks are very vulnerable to adversarial attacks, even
the simplest ones. Inspired by recent advances in brain science, we propose the
Denoised Internal Models (DIM), a novel generative autoencoder-based model to
tackle this challenge. Simulating the pipeline in the human brain for visual
signal processing, DIM adopts a two-stage approach. In the first stage, DIM
uses a denoiser to reduce the noise and the dimensions of inputs, reflecting
the information pre-processing in the thalamus. Inspired from the sparse coding
of memory-related traces in the primary visual cortex, the second stage
produces a set of internal models, one for each category. We evaluate DIM over
42 adversarial attacks, showing that DIM effectively defenses against all the
attacks and outperforms the SOTA on the overall robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">REPLICA: Enhanced Feature Pyramid Network by Local Image Translation and Conjunct Attention for High-Resolution Breast Tumor Detection. (arXiv:2111.11546v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11546">
<div class="article-summary-box-inner">
<span><p>We introduce an improvement to the feature pyramid network of standard object
detection models. We call our method enhanced featuRE Pyramid network by Local
Image translation and Conjunct Attention, or REPLICA. REPLICA improves object
detection performance by simultaneously (1) generating realistic but fake
images with simulated objects to mitigate the data-hungry problem of the
attention mechanism, and (2) advancing the detection model architecture through
a novel modification of attention on image feature patches. Specifically, we
use a convolutional autoencoder as a generator to create new images by
injecting objects into images via local interpolation and reconstruction of
their features extracted in hidden layers. Then due to the larger number of
simulated images, we use a visual transformer to enhance outputs of each ResNet
layer that serve as inputs to a feature pyramid network. We apply our
methodology to the problem of detecting lesions in Digital Breast Tomosynthesis
scans (DBT), a high-resolution medical imaging modality crucial in breast
cancer screening. We demonstrate qualitatively and quantitatively that REPLICA
can improve the accuracy of tumor detection using our enhanced standard object
detection framework via experimental results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">360-DFPE: Leveraging Monocular 360-Layouts for Direct Floor Plan Estimation. (arXiv:2112.06180v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06180">
<div class="article-summary-box-inner">
<span><p>We present 360-DFPE, a sequential floor plan estimation method that directly
takes 360-images as input without relying on active sensors or 3D information.
Our approach leverages a loosely coupled integration between a monocular visual
SLAM solution and a monocular 360-room layout approach, which estimate camera
poses and layout geometries, respectively. Since our task is to sequentially
capture the floor plan using monocular images, the entire scene structure, room
instances, and room shapes are unknown. To tackle these challenges, we first
handle the scale difference between visual odometry and layout geometry via
formulating an entropy minimization process, which enables us to directly align
360-layouts without knowing the entire scene in advance. Second, to
sequentially identify individual rooms, we propose a novel room identification
algorithm that tracks every room along the camera exploration using geometry
information. Lastly, to estimate the final shape of the room, we propose a
shortest path algorithm with an iterative coarse-to-fine strategy, which
improves prior formulations with higher accuracy and faster run-time. Moreover,
we collect a new floor plan dataset with challenging large-scale scenes,
providing both point clouds and sequential 360-image information. Experimental
results show that our monocular solution achieves favorable performance against
the current state-of-the-art algorithms that rely on active sensors and require
the entire scene reconstruction data in advance. Our code and dataset will be
released soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Sea Bubble Stream Characterization Using Wide-Baseline Stereo Photogrammetry. (arXiv:2112.07414v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07414">
<div class="article-summary-box-inner">
<span><p>Reliable quantification of natural and anthropogenic gas release (e.g.\
CO$_2$, methane) from the seafloor into the ocean, and ultimately, the
atmosphere, is a challenging task. While ship-based echo sounders allow
detection of free gas in the water even from a larger distance, exact
quantification requires parameters such as rise speed and bubble size
distribution not obtainable by such sensors. Optical methods are complementary
in the sense that they can provide high temporal and spatial resolution of
single bubbles or bubble streams from close distance. In this contribution we
introduce a complete instrument and evaluation method for optical bubble stream
characterization. The dedicated instrument employs a high-speed deep sea stereo
camera system that can record terabytes of bubble imagery when deployed at a
seep site for later automated analysis. Bubble characteristics can be obtained
for short sequences of few minutes, then relocating the instrument to other
locations, or in autonomous mode of intervals up to several days, in order to
capture variations due to current and pressure changes and across tidal cycles.
Beside reporting the steps to make bubble characterization robust and
autonomous, we carefully evaluate the reachable accuracy and propose a novel
calibration procedure that, due to the lack of point correspondences, uses only
the silhouettes of bubbles. The system has been operated successfully in up to
1000m water depth in the Pacific Ocean to assess methane fluxes. Besides sample
results we also report failure cases and lessons learnt during development.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransZero++: Cross Attribute-Guided Transformer for Zero-Shot Learning. (arXiv:2112.08643v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08643">
<div class="article-summary-box-inner">
<span><p>Zero-shot learning (ZSL) tackles the novel class recognition problem by
transferring semantic knowledge from seen classes to unseen ones. Existing
attention-based models have struggled to learn inferior region features in a
single image by solely using unidirectional attention, which ignore the
transferability and discriminative attribute localization of visual features.
In this paper, we propose a cross attribute-guided Transformer network, termed
TransZero++, to refine visual features and learn accurate attribute
localization for semantic-augmented visual embedding representations in ZSL.
TransZero++ consists of an attribute$\rightarrow$visual Transformer sub-net
(AVT) and a visual$\rightarrow$attribute Transformer sub-net (VAT).
Specifically, AVT first takes a feature augmentation encoder to alleviate the
cross-dataset problem, and improves the transferability of visual features by
reducing the entangled relative geometry relationships among region features.
Then, an attribute$\rightarrow$visual decoder is employed to localize the image
regions most relevant to each attribute in a given image for attribute-based
visual feature representations. Analogously, VAT uses the similar feature
augmentation encoder to refine the visual features, which are further applied
in visual$\rightarrow$attribute decoder to learn visual-based attribute
features. By further introducing semantical collaborative losses, the two
attribute-guided transformers teach each other to learn semantic-augmented
visual embeddings via semantical collaborative learning. Extensive experiments
show that TransZero++ achieves the new state-of-the-art results on three
challenging ZSL benchmarks. The codes are available at:
\url{https://github.com/shiming-chen/TransZero_pp}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learned Half-Quadratic Splitting Network for Magnetic Resonance Image Reconstruction. (arXiv:2112.09760v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09760">
<div class="article-summary-box-inner">
<span><p>Magnetic Resonance (MR) image reconstruction from highly undersampled
$k$-space data is critical in accelerated MR imaging (MRI) techniques. In
recent years, deep learning-based methods have shown great potential in this
task. This paper proposes a learned half-quadratic splitting algorithm for MR
image reconstruction and implements the algorithm in an unrolled deep learning
network architecture. We compare the performance of our proposed method on a
public cardiac MR dataset against DC-CNN and LPDNet, and our method outperforms
other methods in both quantitative results and qualitative results with fewer
model parameters and faster reconstruction speed. Finally, we enlarge our model
to achieve superior reconstruction quality, and the improvement is $1.76$ dB
and $2.74$ dB over LPDNet in peak signal-to-noise ratio on $5\times$ and
$10\times$ acceleration, respectively. Code for our method is publicly
available at https://github.com/hellopipu/HQS-Net.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MoCaNet: Motion Retargeting in-the-wild via Canonicalization Networks. (arXiv:2112.10082v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10082">
<div class="article-summary-box-inner">
<span><p>We present a novel framework that brings the 3D motion retargeting task from
controlled environments to in-the-wild scenarios. In particular, our method is
capable of retargeting body motion from a character in a 2D monocular video to
a 3D character without using any motion capture system or 3D reconstruction
procedure. It is designed to leverage massive online videos for unsupervised
training, needless of 3D annotations or motion-body pairing information. The
proposed method is built upon two novel canonicalization operations, structure
canonicalization and view canonicalization. Trained with the canonicalization
operations and the derived regularizations, our method learns to factorize a
skeleton sequence into three independent semantic subspaces, i.e., motion,
structure, and view angle. The disentangled representation enables motion
retargeting from 2D to 3D with high precision. Our method achieves superior
performance on motion transfer benchmarks with large body variations and
challenging actions. Notably, the canonicalized skeleton sequence could serve
as a disentangled and interpretable representation of human motion that
benefits action analysis and motion retrieval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Deep Learning Based Workflow for Detection of Lung Nodules With Chest Radiograph. (arXiv:2112.10184v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10184">
<div class="article-summary-box-inner">
<span><p>PURPOSE: This study aimed to develop a deep learning-based tool to detect and
localize lung nodules with chest radiographs(CXRs). We expected it to enhance
the efficiency of interpreting CXRs and reduce the possibilities of delayed
diagnosis of lung cancer.
</p>
<p>MATERIALS AND METHODS: We collected CXRs from NCKUH database and VBD, an
open-source medical image dataset, as our training and validation data. A
number of CXRs from the Ministry of Health and Welfare(MOHW) database served as
our test data. We built a segmentation model to identify lung areas from CXRs,
and sliced them into 16 patches. Physicians labeled the CXRs by clicking the
patches. These labeled patches were then used to train and fine-tune a deep
neural network(DNN) model, classifying the patches as positive or negative.
Finally, we test the DNN model with the lung patches of CXRs from MOHW.
</p>
<p>RESULTS: Our segmentation model identified the lung regions well from the
whole CXR. The Intersection over Union(IoU) between the ground truth and the
segmentation result was 0.9228. In addition, our DNN model achieved a
sensitivity of 0.81, specificity of 0.82, and AUROC of 0.869 in 98 of 125
cases. For the other 27 difficult cases, the sensitivity was 0.54, specificity
0.494, and AUROC 0.682. Overall, we obtained a sensitivity of 0.78, specificity
of 0.79, and AUROC 0.837.
</p>
<p>CONCLUSIONS: Our two-step workflow is comparable to state-of-the-art
algorithms in the sensitivity and specificity of localizing lung nodules from
CXRs. Notably, our workflow provides an efficient way for specialists to label
the data, which is valuable for relevant researches because of the relative
rarity of labeled medical image data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Product Re-identification System in Fully Automated Defect Detection. (arXiv:2112.10324v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10324">
<div class="article-summary-box-inner">
<span><p>In this work, we introduce a method and present an improved neural work to
perform product re-identification, which is an essential core function of a
fully automated product defect detection system. Our method is based on feature
distance. It is the combination of feature extraction neural networks, such as
VGG16, AlexNet, with an image search engine - Vearch. The dataset that we used
to develop product re-identification systems is a water-bottle dataset that
consists of 400 images of 18 types of water bottles. This is a small dataset,
which was the biggest challenge of our work. However, the combination of neural
networks with Vearch shows potential to tackle the product re-identification
problems. Especially, our new neural network - AlphaAlexNet that a neural
network was improved based on AlexNet could improve the production
identification accuracy by four percent. This indicates that an ideal
production identification accuracy could be achieved when efficient feature
extraction methods could be introduced and redesigned for image feature
extractions of nearly identical products. In order to solve the biggest
challenges caused by the small size of the dataset and the difficult nature of
identifying productions that have little differences from each other. In our
future work, we propose a new roadmap to tackle nearly-identical production
identifications: to introduce or develop new algorithms that need very few
images to train themselves.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning with Label Noise for Image Retrieval by Selecting Interactions. (arXiv:2112.10453v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10453">
<div class="article-summary-box-inner">
<span><p>Learning with noisy labels is an active research area for image
classification. However, the effect of noisy labels on image retrieval has been
less studied. In this work, we propose a noise-resistant method for image
retrieval named Teacher-based Selection of Interactions, T-SINT, which
identifies noisy interactions, ie. elements in the distance matrix, and selects
correct positive and negative interactions to be considered in the retrieval
loss by using a teacher-based training setup which contributes to the
stability. As a result, it consistently outperforms state-of-the-art methods on
high noise rates across benchmark datasets with synthetic noise and more
realistic noise.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">General Greedy De-bias Learning. (arXiv:2112.10572v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10572">
<div class="article-summary-box-inner">
<span><p>Neural networks often make predictions relying on the spurious correlations
from the datasets rather than the intrinsic properties of the task of interest,
facing sharp degradation on out-of-distribution (OOD) test data. Existing
de-bias learning frameworks try to capture specific dataset bias by bias
annotations, they fail to handle complicated OOD scenarios. Others implicitly
identify the dataset bias by the special design on the low capability biased
model or the loss, but they degrade when the training and testing data are from
the same distribution. In this paper, we propose a General Greedy De-bias
learning framework (GGD), which greedily trains the biased models and the base
model like gradient descent in functional space. It encourages the base model
to focus on examples that are hard to solve with biased models, thus remaining
robust against spurious correlations in the test stage. GGD largely improves
models' OOD generalization ability on various tasks, but sometimes
over-estimates the bias level and degrades on the in-distribution test. We
further re-analyze the ensemble process of GGD and introduce the Curriculum
Regularization into GGD inspired by curriculum learning, which achieves a good
trade-off between in-distribution and out-of-distribution performance.
Extensive experiments on image classification, adversarial question answering,
and visual question answering demonstrate the effectiveness of our method. GGD
can learn a more robust base model under the settings of both task-specific
biased models with prior knowledge and self-ensemble biased model without prior
knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Microfossil Identificationvia Deep Metric Learning. (arXiv:2112.09490v1 [cs.CV] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09490">
<div class="article-summary-box-inner">
<span><p>We apply deep metric learning for the first time to the prob-lem of
classifying planktic foraminifer shells on microscopic images. This species
recognition task is an important information source and scientific pillar for
reconstructing past climates. All foraminifer CNN recognition pipelines in the
literature produce black-box classifiers that lack visualisation options for
human experts and cannot be applied to open set problems. Here, we benchmark
metric learning against these pipelines, produce the first scientific
visualisation of the phenotypic planktic foraminifer morphology space, and
demonstrate that metric learning can be used to cluster species unseen during
training. We show that metric learning out-performs all published CNN-based
state-of-the-art benchmarks in this domain. We evaluate our approach on the
34,640 expert-annotated images of the Endless Forams public library of 35
modern planktic foraminifera species. Our results on this data show leading 92%
accuracy (at 0.84 F1-score) in reproducing expert labels on withheld test data,
and 66.5% accuracy (at 0.70 F1-score) when clustering species never encountered
in training. We conclude that metric learning is highly effective for this
domain and serves as an important tool towards expert-in-the-loop automation of
microfossil identification. Key code, network weights, and data splits are
published with this paper for full reproducibility.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-12-22 23:07:10.144465169 UTC">2021-12-22 23:07:10 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>