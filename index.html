<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-07-18T01:30:00Z">07-18</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">LIP: Lightweight Intelligent Preprocessor for meaningful text-to-speech. (arXiv:2207.07118v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07118">
<div class="article-summary-box-inner">
<span><p>Existing Text-to-Speech (TTS) systems need to read messages from the email
which may have Personal Identifiable Information (PII) to text messages that
can have a streak of emojis and punctuation. 92% of the world's online
population use emoji with more than 10 billion emojis sent everyday. Lack of
preprocessor leads to messages being read as-is including punctuation and
infographics like emoticons. This problem worsens if there is a continuous
sequence of punctuation/emojis that are quite common in real-world
communications like messaging, Social Networking Site (SNS) interactions, etc.
In this work, we aim to introduce a lightweight intelligent preprocessor (LIP)
that can enhance the readability of a message before being passed downstream to
existing TTS systems. We propose multiple sub-modules including: expanding
contraction, censoring swear words, and masking of PII, as part of our
preprocessor to enhance the readability of text. With a memory footprint of
only 3.55 MB and inference time of 4 ms for up to 50-character text, our
solution is suitable for real-time deployment. This work being the first of its
kind, we try to benchmark with an open independent survey, the result of which
shows 76.5% preference towards LIP enabled TTS engine as compared to standard
TTS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emotion Recognition in Conversation using Probabilistic Soft Logic. (arXiv:2207.07238v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07238">
<div class="article-summary-box-inner">
<span><p>Creating agents that can both appropriately respond to conversations and
understand complex human linguistic tendencies and social cues has been a long
standing challenge in the NLP community. A recent pillar of research revolves
around emotion recognition in conversation (ERC); a sub-field of emotion
recognition that focuses on conversations or dialogues that contain two or more
utterances. In this work, we explore an approach to ERC that exploits the use
of neural embeddings along with complex structures in dialogues. We implement
our approach in a framework called Probabilistic Soft Logic (PSL), a
declarative templating language that uses first-order like logical rules, that
when combined with data, define a particular class of graphical model.
Additionally, PSL provides functionality for the incorporation of results from
neural models into PSL models. This allows our model to take advantage of
advanced neural methods, such as sentence embeddings, and logical reasoning
over the structure of a dialogue. We compare our method with state-of-the-art
purely neural ERC systems, and see almost a 20% improvement. With these
results, we provide an extensive qualitative and quantitative analysis over the
DailyDialog conversation dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LineCap: Line Charts for Data Visualization Captioning Models. (arXiv:2207.07243v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07243">
<div class="article-summary-box-inner">
<span><p>Data visualization captions help readers understand the purpose of a
visualization and are crucial for individuals with visual impairments. The
prevalence of poor figure captions and the successful application of deep
learning approaches to image captioning motivate the use of similar techniques
for automated figure captioning. However, research in this field has been
stunted by the lack of suitable datasets. We introduce LineCap, a novel figure
captioning dataset of 3,528 figures, and we provide insights from curating this
dataset and using end-to-end deep learning models for automated figure
captioning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Non-Cooperative Dialogue: Theoretical and Empirical Insights. (arXiv:2207.07255v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07255">
<div class="article-summary-box-inner">
<span><p>Investigating cooperativity of interlocutors is central in studying
pragmatics of dialogue. Models of conversation that only assume cooperative
agents fail to explain the dynamics of strategic conversations. Thus, we
investigate the ability of agents to identify non-cooperative interlocutors
while completing a concurrent visual-dialogue task. Within this novel setting,
we study the optimality of communication strategies for achieving this
multi-task objective. We use the tools of learning theory to develop a
theoretical model for identifying non-cooperative interlocutors and apply this
theory to analyze different communication strategies. We also introduce a
corpus of non-cooperative conversations about images in the GuessWhat?! dataset
proposed by De Vries et al. (2017). We use reinforcement learning to implement
multiple communication strategies in this context and find empirical results
validate our theory.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Flexible Schema-Guided Dialogue Management Framework: From Friendly Peer to Virtual Standardized Cancer Patient. (arXiv:2207.07276v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07276">
<div class="article-summary-box-inner">
<span><p>A schema-guided approach to dialogue management has been shown in recent work
to be effective in creating robust customizable virtual agents capable of
acting as friendly peers or task assistants. However, successful applications
of these methods in open-ended, mixed-initiative domains remain elusive --
particularly within medical domains such as virtual standardized patients,
where such complex interactions are commonplace -- and require more extensive
and flexible dialogue management capabilities than previous systems provide. In
this paper, we describe a general-purpose schema-guided dialogue management
framework used to develop SOPHIE, a virtual standardized cancer patient that
allows a doctor to conveniently practice for interactions with patients. We
conduct a crowdsourced evaluation of conversations between medical students and
SOPHIE. Our agent is judged to produce responses that are natural, emotionally
appropriate, and consistent with her role as a cancer patient. Furthermore, it
significantly outperforms an end-to-end neural model fine-tuned on a human
standardized patient corpus, attesting to the advantages of a schema-guided
approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting Multi-Modal E-commerce Attribute Value Extraction via Unified Learning Scheme and Dynamic Range Minimization. (arXiv:2207.07278v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07278">
<div class="article-summary-box-inner">
<span><p>With the prosperity of e-commerce industry, various modalities, e.g., vision
and language, are utilized to describe product items. It is an enormous
challenge to understand such diversified data, especially via extracting the
attribute-value pairs in text sequences with the aid of helpful image regions.
Although a series of previous works have been dedicated to this task, there
remain seldomly investigated obstacles that hinder further improvements: 1)
Parameters from up-stream single-modal pretraining are inadequately applied,
without proper jointly fine-tuning in a down-stream multi-modal task. 2) To
select descriptive parts of images, a simple late fusion is widely applied,
regardless of priori knowledge that language-related information should be
encoded into a common linguistic embedding space by stronger encoders. 3) Due
to diversity across products, their attribute sets tend to vary greatly, but
current approaches predict with an unnecessary maximal range and lead to more
potential false positives. To address these issues, we propose in this paper a
novel approach to boost multi-modal e-commerce attribute value extraction via
unified learning scheme and dynamic range minimization: 1) Firstly, a unified
scheme is designed to jointly train a multi-modal task with pretrained
single-modal parameters. 2) Secondly, a text-guided information range
minimization method is proposed to adaptively encode descriptive parts of each
modality into an identical space with a powerful pretrained linguistic model.
3) Moreover, a prototype-guided attribute range minimization method is proposed
to first determine the proper attribute set of the current product, and then
select prototypes to guide the prediction of the chosen attributes. Experiments
on the popular multi-modal e-commerce benchmarks show that our approach
achieves superior performance over the other state-of-the-art techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Z-Index at CheckThat! Lab 2022: Check-Worthiness Identification on Tweet Text. (arXiv:2207.07308v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07308">
<div class="article-summary-box-inner">
<span><p>The wide use of social media and digital technologies facilitates sharing
various news and information about events and activities. Despite sharing
positive information misleading and false information is also spreading on
social media. There have been efforts in identifying such misleading
information both manually by human experts and automatic tools. Manual effort
does not scale well due to the high volume of information, containing factual
claims, are appearing online. Therefore, automatically identifying check-worthy
claims can be very useful for human experts. In this study, we describe our
participation in Subtask-1A: Check-worthiness of tweets (English, Dutch and
Spanish) of CheckThat! lab at CLEF 2022. We performed standard preprocessing
steps and applied different models to identify whether a given text is worthy
of fact checking or not. We use the oversampling technique to balance the
dataset and applied SVM and Random Forest (RF) with TF-IDF representations. We
also used BERT multilingual (BERT-m) and XLM-RoBERTa-base pre-trained models
for the experiments. We used BERT-m for the official submissions and our
systems ranked as 3rd, 5th, and 12th in Spanish, Dutch, and English,
respectively. In further experiments, our evaluation shows that transformer
models (BERT-m and XLM-RoBERTa-base) outperform the SVM and RF in Dutch and
English languages where a different scenario is observed for Spanish.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reasoning about Actions over Visual and Linguistic Modalities: A Survey. (arXiv:2207.07568v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07568">
<div class="article-summary-box-inner">
<span><p>'Actions' play a vital role in how humans interact with the world and enable
them to achieve desired goals. As a result, most common sense (CS) knowledge
for humans revolves around actions. While 'Reasoning about Actions &amp; Change'
(RAC) has been widely studied in the Knowledge Representation community, it has
recently piqued the interest of NLP and computer vision researchers. This paper
surveys existing tasks, benchmark datasets, various techniques and models, and
their respective performance concerning advancements in RAC in the vision and
language domain. Towards the end, we summarize our key takeaways, discuss the
present challenges facing this research area, and outline potential directions
for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does Twitter know your political views? POLiTweets dataset and semi-automatic method for political leaning discovery. (arXiv:2207.07586v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07586">
<div class="article-summary-box-inner">
<span><p>Every day, the world is flooded by millions of messages and statements posted
on Twitter or Facebook. Social media platforms try to protect users' personal
data, but there still is a real risk of misuse, including elections
manipulation. Did you know, that only 13 posts addressing important or
controversial topics for society are enough to predict one's political
affiliation with a 0.85 F1-score? To examine this phenomenon, we created a
novel universal method of semi-automated political leaning discovery. It relies
on a heuristical data annotation procedure, which was evaluated to achieve 0.95
agreement with human annotators (counted as an accuracy metric). We also
present POLiTweets - the first publicly open Polish dataset for political
affiliation discovery in a multi-party setup, consisting of over 147k tweets
from almost 10k Polish-writing users annotated heuristically and almost 40k
tweets from 166 users annotated manually as a test set. We used our data to
study the aspects of domain shift in the context of topics and the type of
content writers - ordinary citizens vs. professional politicians.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OASYS: Domain-Agnostic Automated System for Constructing Knowledge Base from Unstructured Text. (arXiv:2207.07597v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07597">
<div class="article-summary-box-inner">
<span><p>In recent years, creating and managing knowledge bases have become crucial to
the retail product and enterprise domains. We present an automatic knowledge
base construction system that mines data from documents. This system can
generate training data during the training process without human intervention.
Therefore, it is domain-agnostic trainable using only the target domain text
corpus and a pre-defined knowledge base. This system is called OASYS and is the
first system built with the Korean language in mind. In addition, we also have
constructed a new human-annotated benchmark dataset of the Korean Wikipedia
corpus paired with a Korean DBpedia to aid system evaluation. The system
performance results on human-annotated benchmark test dataset are meaningful
and show that the generated knowledge base from OASYS trained on only
auto-generated data is useful. We provide both a human-annotated test dataset
and an auto-generated dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differentially Private Fine-tuning of Language Models. (arXiv:2110.06500v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06500">
<div class="article-summary-box-inner">
<span><p>We give simpler, sparser, and faster algorithms for differentially private
fine-tuning of large-scale pre-trained language models, which achieve the
state-of-the-art privacy versus utility tradeoffs on many standard NLP tasks.
We propose a meta-framework for this problem, inspired by the recent success of
highly parameter-efficient methods for fine-tuning. Our experiments show that
differentially private adaptations of these approaches outperform previous
private algorithms in three important dimensions: utility, privacy, and the
computational and memory cost of private training. On many commonly studied
datasets, the utility of private models approaches that of non-private models.
For example, on the MNLI dataset we achieve an accuracy of $87.8\%$ using
RoBERTa-Large and $83.5\%$ using RoBERTa-Base with a privacy budget of
$\epsilon = 6.7$. In comparison, absent privacy constraints, RoBERTa-Large
achieves an accuracy of $90.2\%$. Our findings are similar for natural language
generation tasks. Privately fine-tuning with DART, GPT-2-Small, GPT-2-Medium,
GPT-2-Large, and GPT-2-XL achieve BLEU scores of 38.5, 42.0, 43.1, and 43.8
respectively (privacy budget of $\epsilon = 6.8,\delta=$ 1e-5) whereas the
non-private baseline is $48.1$. All our experiments suggest that larger models
are better suited for private fine-tuning: while they are well known to achieve
superior accuracy non-privately, we find that they also better maintain their
accuracy when privacy is introduced.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Start to Finish: Latency Reduction Strategies for Incremental Speech Synthesis in Simultaneous Speech-to-Speech Translation. (arXiv:2110.08214v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08214">
<div class="article-summary-box-inner">
<span><p>Speech-to-speech translation (S2ST) converts input speech to speech in
another language. A challenge of delivering S2ST in real time is the
accumulated delay between the translation and speech synthesis modules. While
recently incremental text-to-speech (iTTS) models have shown large quality
improvements, they typically require additional future text inputs to reach
optimal performance. In this work, we minimize the initial waiting time of iTTS
by adapting the upstream speech translator to generate high-quality pseudo
lookahead for the speech synthesizer. After mitigating the initial delay, we
demonstrate that the duration of synthesized speech also plays a crucial role
on latency. We formalize this as a latency metric and then present a simple yet
effective duration-scaling approach for latency reduction. Our approaches
consistently reduce latency by 0.2-0.5 second without sacrificing speech
translation quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompting Visual-Language Models for Efficient Video Understanding. (arXiv:2112.04478v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04478">
<div class="article-summary-box-inner">
<span><p>Image-based visual-language (I-VL) pre-training has shown great success for
learning joint visual-textual representations from large-scale web data,
revealing remarkable ability for zero-shot generalisation. This paper presents
a simple but strong baseline to efficiently adapt the pre-trained I-VL model,
and exploit its powerful ability for resource-hungry video understanding tasks,
with minimal training. Specifically, we propose to optimise a few random
vectors, termed as continuous prompt vectors, that convert video-related tasks
into the same format as the pre-training objectives. In addition, to bridge the
gap between static images and videos, temporal information is encoded with
lightweight Transformers stacking on top of frame-wise visual features.
Experimentally, we conduct extensive ablation studies to analyse the critical
components. On 10 public benchmarks of action recognition, action localisation,
and text-video retrieval, across closed-set, few-shot, and zero-shot scenarios,
we achieve competitive or state-of-the-art performance to existing methods,
despite optimising significantly fewer parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Streaming Multi-Talker ASR with Token-Level Serialized Output Training. (arXiv:2202.00842v5 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00842">
<div class="article-summary-box-inner">
<span><p>This paper proposes a token-level serialized output training (t-SOT), a novel
framework for streaming multi-talker automatic speech recognition (ASR). Unlike
existing streaming multi-talker ASR models using multiple output branches, the
t-SOT model has only a single output branch that generates recognition tokens
(e.g., words, subwords) of multiple speakers in chronological order based on
their emission times. A special token that indicates the change of ``virtual''
output channels is introduced to keep track of the overlapping utterances.
Compared to the prior streaming multi-talker ASR models, the t-SOT model has
the advantages of less inference cost and a simpler model architecture.
Moreover, in our experiments with LibriSpeechMix and LibriCSS datasets, the
t-SOT-based transformer transducer model achieves the state-of-the-art word
error rates by a significant margin to the prior results. For non-overlapping
speech, the t-SOT model is on par with a single-talker ASR model in terms of
both accuracy and computational cost, opening the door for deploying one model
for both single- and multi-talker scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Task Sampling for Few-shot Vision-Language Transfer Learning. (arXiv:2203.04904v3 [cs.MM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04904">
<div class="article-summary-box-inner">
<span><p>Despite achieving state-of-the-art zero-shot performance, existing
vision-language models still fall short of few-shot transfer ability on
domain-specific problems. Classical fine-tuning often fails to prevent highly
expressive models from exploiting spurious correlations. Although
model-agnostic meta-learning (MAML) presents as a natural alternative for
few-shot transfer learning, the expensive computation due to implicit
second-order optimization limits its use on large-scale vision-language models
such as CLIP. While much literature has been devoted to exploring alternative
optimization strategies, we identify another essential aspect towards effective
few-shot transfer learning, task sampling, which is previously only be viewed
as part of data pre-processing in MAML. To show the impact of task sampling, we
propose a simple algorithm, Model-Agnostic Multitask Fine-tuning (MAMF), which
differentiates classical fine-tuning only on uniformly sampling multiple tasks.
Despite its simplicity, we show that MAMF consistently outperforms classical
fine-tuning on five few-shot vision-language classification tasks. We further
show that the effectiveness of the bi-level optimization in MAML is highly
sensitive to the zero-shot performance of a task in the context of few-shot
vision-language classification. The goal of this paper is to provide new
insights on what makes few-shot learning work, and encourage more research into
investigating better task sampling strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Theoretically Grounded Benchmark for Evaluating Machine Commonsense. (arXiv:2203.12184v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12184">
<div class="article-summary-box-inner">
<span><p>Programming machines with commonsense reasoning (CSR) abilities is a
longstanding challenge in the Artificial Intelligence community. Current CSR
benchmarks use multiple-choice (and in relatively fewer cases, generative)
question-answering instances to evaluate machine commonsense. Recent progress
in transformer-based language representation models suggest that considerable
progress has been made on existing benchmarks. However, although tens of CSR
benchmarks currently exist, and are growing, it is not evident that the full
suite of commonsense capabilities have been systematically evaluated.
Furthermore, there are doubts about whether language models are 'fitting' to a
benchmark dataset's training partition by picking up on subtle, but normatively
irrelevant (at least for CSR), statistical features to achieve good performance
on the testing partition. To address these challenges, we propose a benchmark
called Theoretically-Grounded Commonsense Reasoning (TG-CSR) that is also based
on discriminative question answering, but with questions designed to evaluate
diverse aspects of commonsense, such as space, time, and world states. TG-CSR
is based on a subset of commonsense categories first proposed as a viable
theory of commonsense by Gordon and Hobbs. The benchmark is also designed to be
few-shot (and in the future, zero-shot), with only a few training and
validation examples provided. This report discusses the structure and
construction of the benchmark. Preliminary results suggest that the benchmark
is challenging even for advanced language representation models designed for
discriminative CSR question answering tasks.
</p>
<p>Benchmark access and leaderboard:
https://codalab.lisn.upsaclay.fr/competitions/3080 Benchmark website:
https://usc-isi-i2.github.io/TGCSR/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Speech Recognition for Speech Assessment of Persian Preschool Children. (arXiv:2203.12886v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12886">
<div class="article-summary-box-inner">
<span><p>Preschool evaluation is crucial because it gives teachers and parents
influential knowledge about a children's growth and development. The
coronavirus pandemic has highlighted the necessity of online assessment for
preschool children. One of the areas that should be tested is the ability to
speak. Because of the differences between children's and adults' voices,
employing Automatic Speech Recognition(ASR) systems is difficult since they are
pre-trained on adults' voices. We constructed an ASR for our cognitive test
system to solve this issue using the Wav2Vec 2.0 model with a new pre-training
objective called Random Frequency Pitch(RFP). In addition, we used our new
dataset to fine-tune our model for Meaningless Words(MW) and Rapid Automatic
Naming(RAN) tests. Our new approach reaches a Word Error Rate(WER) of 6.45 on
the Persian section of the CommonVoice dataset. Furthermore, our novel
methodology produces positive outcomes in zero- and few-shot scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Streaming Speaker-Attributed ASR with Token-Level Speaker Embeddings. (arXiv:2203.16685v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16685">
<div class="article-summary-box-inner">
<span><p>This paper presents a streaming speaker-attributed automatic speech
recognition (SA-ASR) model that can recognize ``who spoke what'' with low
latency even when multiple people are speaking simultaneously. Our model is
based on token-level serialized output training (t-SOT) which was recently
proposed to transcribe multi-talker speech in a streaming fashion. To further
recognize speaker identities, we propose an encoder-decoder based speaker
embedding extractor that can estimate a speaker representation for each
recognized token not only from non-overlapping speech but also from overlapping
speech. The proposed speaker embedding, named t-vector, is extracted
synchronously with the t-SOT ASR model, enabling joint execution of speaker
identification (SID) or speaker diarization (SD) with the multi-talker
transcription with low latency. We evaluate the proposed model for a joint task
of ASR and SID/SD by using LibriSpeechMix and LibriCSS corpora. The proposed
model achieves substantially better accuracy than a prior streaming model and
shows comparable or sometimes even superior results to the state-of-the-art
offline SA-ASR model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perceptual Contrast Stretching on Target Feature for Speech Enhancement. (arXiv:2203.17152v4 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17152">
<div class="article-summary-box-inner">
<span><p>Speech enhancement (SE) performance has improved considerably owing to the
use of deep learning models as a base function. Herein, we propose a perceptual
contrast stretching (PCS) approach to further improve SE performance. The PCS
is derived based on the critical band importance function and is applied to
modify the targets of the SE model. Specifically, the contrast of target
features is stretched based on perceptual importance, thereby improving the
overall SE performance. Compared with post-processing-based implementations,
incorporating PCS into the training phase preserves performance and reduces
online computation. Notably, PCS can be combined with different SE model
architectures and training criteria. Furthermore, PCS does not affect the
causality or convergence of SE model training. Experimental results on the
VoiceBank-DEMAND dataset show that the proposed method can achieve
state-of-the-art performance on both causal (PESQ score = 3.07) and noncausal
(PESQ score = 3.35) SE tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BiSyn-GAT+: Bi-Syntax Aware Graph Attention Network for Aspect-based Sentiment Analysis. (arXiv:2204.03117v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03117">
<div class="article-summary-box-inner">
<span><p>Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis
task that aims to align aspects and corresponding sentiments for
aspect-specific sentiment polarity inference. It is challenging because a
sentence may contain multiple aspects or complicated (e.g., conditional,
coordinating, or adversative) relations. Recently, exploiting dependency syntax
information with graph neural networks has been the most popular trend. Despite
its success, methods that heavily rely on the dependency tree pose challenges
in accurately modeling the alignment of the aspects and their words indicative
of sentiment, since the dependency tree may provide noisy signals of unrelated
associations (e.g., the "conj" relation between "great" and "dreadful" in
Figure 2). In this paper, to alleviate this problem, we propose a Bi-Syntax
aware Graph Attention Network (BiSyn-GAT+). Specifically, BiSyn-GAT+ fully
exploits the syntax information (e.g., phrase segmentation and hierarchical
structure) of the constituent tree of a sentence to model the sentiment-aware
context of every single aspect (called intra-context) and the sentiment
relations across aspects (called inter-context) for learning. Experiments on
four benchmark datasets demonstrate that BiSyn-GAT+ outperforms the
state-of-the-art methods consistently.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Entailment Graph Learning with Textual Entailment and Soft Transitivity. (arXiv:2204.03286v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03286">
<div class="article-summary-box-inner">
<span><p>Typed entailment graphs try to learn the entailment relations between
predicates from text and model them as edges between predicate nodes. The
construction of entailment graphs usually suffers from severe sparsity and
unreliability of distributional similarity. We propose a two-stage method,
Entailment Graph with Textual Entailment and Transitivity (EGT2). EGT2 learns
local entailment relations by recognizing possible textual entailment between
template sentences formed by typed CCG-parsed predicates. Based on the
generated local graph, EGT2 then uses three novel soft transitivity constraints
to consider the logical transitivity in entailment structures. Experiments on
benchmark datasets show that EGT2 can well model the transitivity in entailment
graph to alleviate the sparsity issue, and lead to significant improvement over
current state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Making the Most of Text Semantics to Improve Biomedical Vision--Language Processing. (arXiv:2204.09817v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09817">
<div class="article-summary-box-inner">
<span><p>Multi-modal data abounds in biomedicine, such as radiology images and
reports. Interpreting this data at scale is essential for improving clinical
care and accelerating clinical research. Biomedical text with its complex
semantics poses additional challenges in vision--language modelling compared to
the general domain, and previous work has used insufficiently adapted models
that lack domain-specific language understanding. In this paper, we show that
principled textual semantic modelling can substantially improve contrastive
learning in self-supervised vision--language processing. We release a language
model that achieves state-of-the-art results in radiology natural language
inference through its improved vocabulary and novel language pretraining
objective leveraging semantics and discourse characteristics in radiology
reports. Further, we propose a self-supervised joint vision--language approach
with a focus on better text modelling. It establishes new state of the art
results on a wide range of publicly available benchmarks, in part by leveraging
our new domain-specific language model. We release a new dataset with
locally-aligned phrase grounding annotations by radiologists to facilitate the
study of complex semantic modelling in biomedical vision--language processing.
A broad evaluation, including on this new dataset, shows that our contrastive
learning approach, aided by textual-semantic modelling, outperforms prior
methods in segmentation tasks, despite only using a global-alignment objective.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Calibration of Natural Language Understanding Models with Venn--ABERS Predictors. (arXiv:2205.10586v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10586">
<div class="article-summary-box-inner">
<span><p>Transformers, currently the state-of-the-art in natural language
understanding (NLU) tasks, are prone to generate uncalibrated predictions or
extreme probabilities, making the process of taking different decisions based
on their output relatively difficult. In this paper we propose to build several
inductive Venn--ABERS predictors (IVAP), which are guaranteed to be well
calibrated under minimal assumptions, based on a selection of pre-trained
transformers. We test their performance over a set of diverse NLU tasks and
show that they are capable of producing well-calibrated probabilistic
predictions that are uniformly spread over the [0,1] interval -- all while
retaining the original model's predictive accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt Injection: Parameterization of Fixed Inputs. (arXiv:2206.11349v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.11349">
<div class="article-summary-box-inner">
<span><p>Recent works have shown that attaching prompts to the input is effective at
conditioning Language Models (LM) to perform specific tasks. However, prompts
are always included in the input text during inference, thus incurring
substantial computational and memory overhead. Also, there is currently no
straightforward method of utilizing prompts that are longer than the maximum
input length of the LMs without incurring additional costs during inference. We
propose Prompt Injection (PI), a novel formulation of injecting the prompt into
the parameters of an LM to be an efficient alternative to attaching fixed
prompts to the input. We show that in scenarios with long fixed prompts, PI can
be up to 280 times more efficient in terms of total FLOPs than previous
approaches. We further explore methodologies for PI and show promising results
in persona-dependent conversation, semantic parsing, and zero-shot learning
with task instructions. Through these explorations, we show that PI can be a
promising direction for conditioning language models, especially in scenarios
with long and fixed prompts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequence-aware multimodal page classification of Brazilian legal documents. (arXiv:2207.00748v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00748">
<div class="article-summary-box-inner">
<span><p>The Brazilian Supreme Court receives tens of thousands of cases each
semester. Court employees spend thousands of hours to execute the initial
analysis and classification of those cases -- which takes effort away from
posterior, more complex stages of the case management workflow. In this paper,
we explore multimodal classification of documents from Brazil's Supreme Court.
We train and evaluate our methods on a novel multimodal dataset of 6,510
lawsuits (339,478 pages) with manual annotation assigning each page to one of
six classes. Each lawsuit is an ordered sequence of pages, which are stored
both as an image and as a corresponding text extracted through optical
character recognition. We first train two unimodal classifiers: a ResNet
pre-trained on ImageNet is fine-tuned on the images, and a convolutional
network with filters of multiple kernel sizes is trained from scratch on
document texts. We use them as extractors of visual and textual features, which
are then combined through our proposed Fusion Module. Our Fusion Module can
handle missing textual or visual input by using learned embeddings for missing
data. Moreover, we experiment with bi-directional Long Short-Term Memory
(biLSTM) networks and linear-chain conditional random fields to model the
sequential nature of the pages. The multimodal approaches outperform both
textual and visual classifiers, especially when leveraging the sequential
nature of the pages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HLT-MT: High-resource Language-specific Training for Multilingual Neural Machine Translation. (arXiv:2207.04906v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04906">
<div class="article-summary-box-inner">
<span><p>Multilingual neural machine translation (MNMT) trained in multiple language
pairs has attracted considerable attention due to fewer model parameters and
lower training costs by sharing knowledge among multiple languages.
Nonetheless, multilingual training is plagued by language interference
degeneration in shared parameters because of the negative interference among
different translation directions, especially on high-resource languages. In
this paper, we propose the multilingual translation model with the
high-resource language-specific training (HLT-MT) to alleviate the negative
interference, which adopts the two-stage training with the language-specific
selection mechanism. Specifically, we first train the multilingual model only
with the high-resource pairs and select the language-specific modules at the
top of the decoder to enhance the translation quality of high-resource
directions. Next, the model is further trained on all available corpora to
transfer knowledge from high-resource languages (HRLs) to low-resource
languages (LRLs). Experimental results show that HLT-MT outperforms various
strong baselines on WMT-10 and OPUS-100 benchmarks. Furthermore, the analytic
experiments validate the effectiveness of our method in mitigating the negative
interference in multilingual training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Linear-time calculation of the expected sum of edge lengths in planar linearizations of trees. (arXiv:2207.05564v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05564">
<div class="article-summary-box-inner">
<span><p>Dependency graphs have proven to be a very successful model to represent the
syntactic structure of sentences of human languages. In these graphs, widely
accepted to be trees, vertices are words and arcs connect
syntactically-dependent words. The tendency of these dependencies to be short
has been demonstrated using random baselines for the sum of the lengths of the
edges or its variants. A ubiquitous baseline is the expected sum in projective
orderings (wherein edges do not cross and the root word of the sentence is not
covered by any edge). It was shown that said expected value can be computed in
$O(n)$ time. In this article we focus on planar orderings (where the root word
can be covered) and present two main results. First, we show the relationship
between the expected sum in planar arrangements and the expected sum in
projective arrangements. Second, we also derive a $O(n)$-time algorithm to
calculate the expected value of the sum of edge lengths. These two results stem
from another contribution of the present article, namely a characterization of
planarity that, given a sentence, yields either the number of planar
permutations or an efficient algorithm to generate uniformly random planar
permutations of the words. Our research paves the way for replicating past
research on dependency distance minimization using random planar linearizations
as random baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Long-term Dependencies and Short-term Correlations in Patient Journey Data with Temporal Attention Networks for Health Prediction. (arXiv:2207.06414v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06414">
<div class="article-summary-box-inner">
<span><p>Building models for health prediction based on Electronic Health Records
(EHR) has become an active research area. EHR patient journey data consists of
patient time-ordered clinical events/visits from patients. Most existing
studies focus on modeling long-term dependencies between visits, without
explicitly taking short-term correlations between consecutive visits into
account, where irregular time intervals, incorporated as auxiliary information,
are fed into health prediction models to capture latent progressive patterns of
patient journeys. We present a novel deep neural network with four modules to
take into account the contributions of various variables for health prediction:
i) the Stacked Attention module strengthens the deep semantics in clinical
events within each patient journey and generates visit embeddings, ii) the
Short-Term Temporal Attention module models short-term correlations between
consecutive visit embeddings while capturing the impact of time intervals
within those visit embeddings, iii) the Long-Term Temporal Attention module
models long-term dependencies between visit embeddings while capturing the
impact of time intervals within those visit embeddings, iv) and finally, the
Coupled Attention module adaptively aggregates the outputs of Short-Term
Temporal Attention and Long-Term Temporal Attention modules to make health
predictions. Experimental results on MIMIC-III demonstrate superior predictive
accuracy of our model compared to existing state-of-the-art methods, as well as
the interpretability and robustness of this approach. Furthermore, we found
that modeling short-term correlations contributes to local priors generation,
leading to improved predictive modeling of patient journeys.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Clustering with Contrastive Learning and Multi-scale Graph Convolutional Networks. (arXiv:2207.07173v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07173">
<div class="article-summary-box-inner">
<span><p>Deep clustering has recently attracted significant attention. Despite the
remarkable progress, most of the previous deep clustering works still suffer
from two limitations. First, many of them focus on some distribution-based
clustering loss, lacking the ability to exploit sample-wise (or
augmentation-wise) relationships via contrastive learning. Second, they often
neglect the indirect sample-wise structure information, overlooking the rich
possibilities of multi-scale neighborhood structure learning. In view of this,
this paper presents a new deep clustering approach termed Image clustering with
contrastive learning and multi-scale Graph Convolutional Networks (IcicleGCN),
which bridges the gap between convolutional neural network (CNN) and graph
convolutional network (GCN) as well as the gap between contrastive learning and
multi-scale neighborhood structure learning for the image clustering task. The
proposed IcicleGCN framework consists of four main modules, namely, the
CNN-based backbone, the Instance Similarity Module (ISM), the Joint Cluster
Structure Learning and Instance reconstruction Module (JC-SLIM), and the
Multi-scale GCN module (M-GCN). Specifically, with two random augmentations
performed on each image, the backbone network with two weight-sharing views is
utilized to learn the representations for the augmented samples, which are then
fed to ISM and JC-SLIM for instance-level and cluster-level contrastive
learning, respectively. Further, to enforce multi-scale neighborhood structure
learning, two streams of GCNs and an auto-encoder are simultaneously trained
via (i) the layer-wise interaction with representation fusion and (ii) the
joint self-adaptive learning that ensures their last-layer output distributions
to be consistent. Experiments on multiple image datasets demonstrate the
superior clustering performance of IcicleGCN over the state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Current Trends in Deep Learning for Earth Observation: An Open-source Benchmark Arena for Image Classification. (arXiv:2207.07189v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07189">
<div class="article-summary-box-inner">
<span><p>We present 'AiTLAS: Benchmark Arena' -- an open-source benchmark framework
for evaluating state-of-the-art deep learning approaches for image
classification in Earth Observation (EO). To this end, we present a
comprehensive comparative analysis of more than 400 models derived from nine
different state-of-the-art architectures, and compare them to a variety of
multi-class and multi-label classification tasks from 22 datasets with
different sizes and properties. In addition to models trained entirely on these
datasets, we also benchmark models trained in the context of transfer learning,
leveraging pre-trained model variants, as it is typically performed in
practice. All presented approaches are general and can be easily extended to
many other remote sensing image classification tasks not considered in this
study. To ensure reproducibility and facilitate better usability and further
developments, all of the experimental resources including the trained models,
model configurations and processing details of the datasets (with their
corresponding splits used for training and evaluating the models) are publicly
available on the repository: https://github.com/biasvariancelabs/aitlas-arena.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lipschitz Bound Analysis of Neural Networks. (arXiv:2207.07232v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07232">
<div class="article-summary-box-inner">
<span><p>Lipschitz Bound Estimation is an effective method of regularizing deep neural
networks to make them robust against adversarial attacks. This is useful in a
variety of applications ranging from reinforcement learning to autonomous
systems. In this paper, we highlight the significant gap in obtaining a
non-trivial Lipschitz bound certificate for Convolutional Neural Networks
(CNNs) and empirically support it with extensive graphical analysis. We also
show that unrolling Convolutional layers or Toeplitz matrices can be employed
to convert Convolutional Neural Networks (CNNs) to a Fully Connected Network.
Further, we propose a simple algorithm to show the existing 20x-50x gap in a
particular data distribution between the actual lipschitz constant and the
obtained tight bound. We also ran sets of thorough experiments on various
network architectures and benchmark them on datasets like MNIST and CIFAR-10.
All these proposals are supported by extensive testing, graphs, histograms and
comparative analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Single Model Uncertainty Estimation via Stochastic Data Centering. (arXiv:2207.07235v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07235">
<div class="article-summary-box-inner">
<span><p>We are interested in estimating the uncertainties of deep neural networks,
which play an important role in many scientific and engineering problems. In
this paper, we present a striking new finding that an ensemble of neural
networks with the same weight initialization, trained on datasets that are
shifted by a constant bias gives rise to slightly inconsistent trained models,
where the differences in predictions are a strong indicator of epistemic
uncertainties. Using the neural tangent kernel (NTK), we demonstrate that this
phenomena occurs in part because the NTK is not shift-invariant. Since this is
achieved via a trivial input transformation, we show that it can therefore be
approximated using just a single neural network -- using a technique that we
call $\Delta-$UQ -- that estimates uncertainty around prediction by
marginalizing out the effect of the biases. We show that $\Delta-$UQ's
uncertainty estimates are superior to many of the current methods on a variety
of benchmarks -- outlier rejection, calibration under distribution shift, and
sequential design optimization of black box functions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Classification of Bark Beetle-Induced Forest Tree Mortality using Deep Learning. (arXiv:2207.07241v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07241">
<div class="article-summary-box-inner">
<span><p>Bark beetle outbreaks can dramatically impact forest ecosystems and services
around the world. For the development of effective forest policies and
management plans, the early detection of infested trees is essential. Despite
the visual symptoms of bark beetle infestation, this task remains challenging,
considering overlapping tree crowns and non-homogeneity in crown foliage
discolouration. In this work, a deep learning based method is proposed to
effectively classify different stages of bark beetle attacks at the individual
tree level. The proposed method uses RetinaNet architecture (exploiting a
robust feature extraction backbone pre-trained for tree crown detection) to
train a shallow subnetwork for classifying the different attack stages of
images captured by unmanned aerial vehicles (UAVs). Moreover, various data
augmentation strategies are examined to address the class imbalance problem,
and consequently, the affine transformation is selected to be the most
effective one for this purpose. Experimental evaluations demonstrate the
effectiveness of the proposed method by achieving an average accuracy of
98.95%, considerably outperforming the baseline method by approximately 10%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LineCap: Line Charts for Data Visualization Captioning Models. (arXiv:2207.07243v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07243">
<div class="article-summary-box-inner">
<span><p>Data visualization captions help readers understand the purpose of a
visualization and are crucial for individuals with visual impairments. The
prevalence of poor figure captions and the successful application of deep
learning approaches to image captioning motivate the use of similar techniques
for automated figure captioning. However, research in this field has been
stunted by the lack of suitable datasets. We introduce LineCap, a novel figure
captioning dataset of 3,528 figures, and we provide insights from curating this
dataset and using end-to-end deep learning models for automated figure
captioning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decoupling Recognition from Detection: Single Shot Self-Reliant Scene Text Spotter. (arXiv:2207.07253v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07253">
<div class="article-summary-box-inner">
<span><p>Typical text spotters follow the two-stage spotting strategy: detect the
precise boundary for a text instance first and then perform text recognition
within the located text region. While such strategy has achieved substantial
progress, there are two underlying limitations. 1) The performance of text
recognition depends heavily on the precision of text detection, resulting in
the potential error propagation from detection to recognition. 2) The RoI
cropping which bridges the detection and recognition brings noise from
background and leads to information loss when pooling or interpolating from
feature maps. In this work we propose the single shot Self-Reliant Scene Text
Spotter (SRSTS), which circumvents these limitations by decoupling recognition
from detection. Specifically, we conduct text detection and recognition in
parallel and bridge them by the shared positive anchor point. Consequently, our
method is able to recognize the text instances correctly even though the
precise text boundaries are challenging to detect. Additionally, our method
reduces the annotation cost for text detection substantially. Extensive
experiments on regular-shaped benchmark and arbitrary-shaped benchmark
demonstrate that our SRSTS compares favorably to previous state-of-the-art
spotters in terms of both accuracy and efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ScaleNet: Searching for the Model to Scale. (arXiv:2207.07267v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07267">
<div class="article-summary-box-inner">
<span><p>Recently, community has paid increasing attention on model scaling and
contributed to developing a model family with a wide spectrum of scales.
Current methods either simply resort to a one-shot NAS manner to construct a
non-structural and non-scalable model family or rely on a manual yet fixed
scaling strategy to scale an unnecessarily best base model. In this paper, we
bridge both two components and propose ScaleNet to jointly search base model
and scaling strategy so that the scaled large model can have more promising
performance. Concretely, we design a super-supernet to embody models with
different spectrum of sizes (e.g., FLOPs). Then, the scaling strategy can be
learned interactively with the base model via a Markov chain-based evolution
algorithm and generalized to develop even larger models. To obtain a decent
super-supernet, we design a hierarchical sampling strategy to enhance its
training sufficiency and alleviate the disturbance. Experimental results show
our scaled networks enjoy significant performance superiority on various FLOPs,
but with at least 2.53x reduction on search cost. Codes are available at
https://github.com/luminolx/ScaleNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lightweight Vision Transformer with Cross Feature Attention. (arXiv:2207.07268v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07268">
<div class="article-summary-box-inner">
<span><p>Recent advances in vision transformers (ViTs) have achieved great performance
in visual recognition tasks. Convolutional neural networks (CNNs) exploit
spatial inductive bias to learn visual representations, but these networks are
spatially local. ViTs can learn global representations with their
self-attention mechanism, but they are usually heavy-weight and unsuitable for
mobile devices. In this paper, we propose cross feature attention (XFA) to
bring down computation cost for transformers, and combine efficient mobile CNNs
to form a novel efficient light-weight CNN-ViT hybrid model, XFormer, which can
serve as a general-purpose backbone to learn both global and local
representation. Experimental results show that XFormer outperforms numerous CNN
and ViT-based models across different tasks and datasets. On ImageNet1K
dataset, XFormer achieves top-1 accuracy of 78.5% with 5.5 million parameters,
which is 2.2% and 6.3% more accurate than EfficientNet-B0 (CNN-based) and DeiT
(ViT-based) for similar number of parameters. Our model also performs well when
transferring to object detection and semantic segmentation tasks. On MS COCO
dataset, XFormer exceeds MobileNetV2 by 10.5 AP (22.7 -&gt; 33.2 AP) in YOLOv3
framework with only 6.3M parameters and 3.8G FLOPs. On Cityscapes dataset, with
only a simple all-MLP decoder, XFormer achieves mIoU of 78.5 and FPS of 15.3,
surpassing state-of-the-art lightweight segmentation networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Supervised Video Salient Object Detection via Point Supervision. (arXiv:2207.07269v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07269">
<div class="article-summary-box-inner">
<span><p>Video salient object detection models trained on pixel-wise dense annotation
have achieved excellent performance, yet obtaining pixel-by-pixel annotated
datasets is laborious. Several works attempt to use scribble annotations to
mitigate this problem, but point supervision as a more labor-saving annotation
method (even the most labor-saving method among manual annotation methods for
dense prediction), has not been explored. In this paper, we propose a strong
baseline model based on point supervision. To infer saliency maps with temporal
information, we mine inter-frame complementary information from short-term and
long-term perspectives, respectively. Specifically, we propose a hybrid token
attention module, which mixes optical flow and image information from
orthogonal directions, adaptively highlighting critical optical flow
information (channel dimension) and critical token information (spatial
dimension). To exploit long-term cues, we develop the Long-term Cross-Frame
Attention module (LCFA), which assists the current frame in inferring salient
objects based on multi-frame tokens. Furthermore, we label two point-supervised
datasets, P-DAVIS and P-DAVSOD, by relabeling the DAVIS and the DAVSOD dataset.
Experiments on the six benchmark datasets illustrate our method outperforms the
previous state-of-the-art weakly supervised methods and even is comparable with
some fully supervised approaches. Source code and datasets are available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting Multi-Modal E-commerce Attribute Value Extraction via Unified Learning Scheme and Dynamic Range Minimization. (arXiv:2207.07278v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07278">
<div class="article-summary-box-inner">
<span><p>With the prosperity of e-commerce industry, various modalities, e.g., vision
and language, are utilized to describe product items. It is an enormous
challenge to understand such diversified data, especially via extracting the
attribute-value pairs in text sequences with the aid of helpful image regions.
Although a series of previous works have been dedicated to this task, there
remain seldomly investigated obstacles that hinder further improvements: 1)
Parameters from up-stream single-modal pretraining are inadequately applied,
without proper jointly fine-tuning in a down-stream multi-modal task. 2) To
select descriptive parts of images, a simple late fusion is widely applied,
regardless of priori knowledge that language-related information should be
encoded into a common linguistic embedding space by stronger encoders. 3) Due
to diversity across products, their attribute sets tend to vary greatly, but
current approaches predict with an unnecessary maximal range and lead to more
potential false positives. To address these issues, we propose in this paper a
novel approach to boost multi-modal e-commerce attribute value extraction via
unified learning scheme and dynamic range minimization: 1) Firstly, a unified
scheme is designed to jointly train a multi-modal task with pretrained
single-modal parameters. 2) Secondly, a text-guided information range
minimization method is proposed to adaptively encode descriptive parts of each
modality into an identical space with a powerful pretrained linguistic model.
3) Moreover, a prototype-guided attribute range minimization method is proposed
to first determine the proper attribute set of the current product, and then
select prototypes to guide the prediction of the chosen attributes. Experiments
on the popular multi-modal e-commerce benchmarks show that our approach
achieves superior performance over the other state-of-the-art techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parameterization of Cross-Token Relations with Relative Positional Encoding for Vision MLP. (arXiv:2207.07284v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07284">
<div class="article-summary-box-inner">
<span><p>Vision multi-layer perceptrons (MLPs) have shown promising performance in
computer vision tasks, and become the main competitor of CNNs and vision
Transformers. They use token-mixing layers to capture cross-token interactions,
as opposed to the multi-head self-attention mechanism used by Transformers.
However, the heavily parameterized token-mixing layers naturally lack
mechanisms to capture local information and multi-granular non-local relations,
thus their discriminative power is restrained. To tackle this issue, we propose
a new positional spacial gating unit (PoSGU). It exploits the attention
formulations used in the classical relative positional encoding (RPE), to
efficiently encode the cross-token relations for token mixing. It can
successfully reduce the current quadratic parameter complexity $O(N^2)$ of
vision MLPs to $O(N)$ and $O(1)$. We experiment with two RPE mechanisms, and
further propose a group-wise extension to improve their expressive power with
the accomplishment of multi-granular contexts. These then serve as the key
building blocks of a new type of vision MLP, referred to as PosMLP. We evaluate
the effectiveness of the proposed approach by conducting thorough experiments,
demonstrating an improved or comparable performance with reduced parameter
complexity. For instance, for a model trained on ImageNet1K, we achieve a
performance improvement from 72.14\% to 74.02\% and a learnable parameter
reduction from $19.4M$ to $18.2M$. Code could be found at
\href{https://github.com/Zhicaiwww/PosMLP}{https://github.com/Zhicaiwww/PosMLP}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">X-CLIP: End-to-End Multi-grained Contrastive Learning for Video-Text Retrieval. (arXiv:2207.07285v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07285">
<div class="article-summary-box-inner">
<span><p>Video-text retrieval has been a crucial and fundamental task in multi-modal
research. The development of video-text retrieval has been considerably
promoted by large-scale multi-modal contrastive pre-training, which primarily
focuses on coarse-grained or fine-grained contrast. However, cross-grained
contrast, which is the contrast between coarse-grained representations and
fine-grained representations, has rarely been explored in prior research.
Compared with fine-grained or coarse-grained contrasts, cross-grained contrast
calculate the correlation between coarse-grained features and each fine-grained
feature, and is able to filter out the unnecessary fine-grained features guided
by the coarse-grained feature during similarity calculation, thus improving the
accuracy of retrieval. To this end, this paper presents a novel multi-grained
contrastive model, namely X-CLIP, for video-text retrieval. However, another
challenge lies in the similarity aggregation problem, which aims to aggregate
fine-grained and cross-grained similarity matrices to instance-level
similarity. To address this challenge, we propose the Attention Over Similarity
Matrix (AOSM) module to make the model focus on the contrast between essential
frames and words, thus lowering the impact of unnecessary frames and words on
retrieval results. With multi-grained contrast and the proposed AOSM module,
X-CLIP achieves outstanding performance on five widely-used video-text
retrieval datasets, including MSR-VTT (49.3 R@1), MSVD (50.4 R@1), LSMDC (26.1
R@1), DiDeMo (47.8 R@1) and ActivityNet (46.2 R@1). It outperforms the previous
state-of-theart by +6.3%, +6.6%, +11.1%, +6.7%, +3.8% relative improvements on
these benchmarks, demonstrating the superiority of multi-grained contrast and
AOSM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WaveGAN: Frequency-aware GAN for High-Fidelity Few-shot Image Generation. (arXiv:2207.07288v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07288">
<div class="article-summary-box-inner">
<span><p>Existing few-shot image generation approaches typically employ fusion-based
strategies, either on the image or the feature level, to produce new images.
However, previous approaches struggle to synthesize high-frequency signals with
fine details, deteriorating the synthesis quality. To address this, we propose
WaveGAN, a frequency-aware model for few-shot image generation. Concretely, we
disentangle encoded features into multiple frequency components and perform
low-frequency skip connections to preserve outline and structural information.
Then we alleviate the generator's struggles of synthesizing fine details by
employing high-frequency skip connections, thus providing informative frequency
information to the generator. Moreover, we utilize a frequency L1-loss on the
generated and real images to further impede frequency information loss.
Extensive experiments demonstrate the effectiveness and advancement of our
method on three datasets. Noticeably, we achieve new state-of-the-art with FID
42.17, LPIPS 0.3868, FID 30.35, LPIPS 0.5076, and FID 4.96, LPIPS 0.3822
respectively on Flower, Animal Faces, and VGGFace. GitHub:
https://github.com/kobeshegu/ECCV2022_WaveGAN
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Deep Compressive Sensing with Recurrent-Residual Structural Constraints. (arXiv:2207.07301v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07301">
<div class="article-summary-box-inner">
<span><p>Existing deep compressive sensing (CS) methods either ignore adaptive online
optimization or depend on costly iterative optimizer during reconstruction.
This work explores a novel image CS framework with recurrent-residual
structural constraint, termed as R$^2$CS-NET. The R$^2$CS-NET first
progressively optimizes the acquired samplings through a novel recurrent neural
network. The cascaded residual convolutional network then fully reconstructs
the image from optimized latent representation. As the first deep CS framework
efficiently bridging adaptive online optimization, the R$^2$CS-NET integrates
the robustness of online optimization with the efficiency and nonlinear
capacity of deep learning methods. Signal correlation has been addressed
through the network architecture. The adaptive sensing nature further makes it
an ideal candidate for color image CS via leveraging channel correlation.
Numerical experiments verify the proposed recurrent latent optimization design
not only fulfills the adaptation motivation, but also outperforms classic long
short-term memory (LSTM) architecture in the same scenario. The overall
framework demonstrates hardware implementation feasibility, with leading
robustness and generalization capability among existing deep CS benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Better Dermoscopic Image Feature Representation Learning for Melanoma Classification. (arXiv:2207.07303v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07303">
<div class="article-summary-box-inner">
<span><p>Deep learning-based melanoma classification with dermoscopic images has
recently shown great potential in automatic early-stage melanoma diagnosis.
However, limited by the significant data imbalance and obvious extraneous
artifacts, i.e., the hair and ruler markings, discriminative feature extraction
from dermoscopic images is very challenging. In this study, we seek to resolve
these problems respectively towards better representation learning for lesion
features. Specifically, a GAN-based data augmentation (GDA) strategy is adapted
to generate synthetic melanoma-positive images, in conjunction with the
proposed implicit hair denoising (IHD) strategy. Wherein the hair-related
representations are implicitly disentangled via an auxiliary classifier network
and reversely sent to the melanoma-feature extraction backbone for better
melanoma-specific representation learning. Furthermore, to train the IHD
module, the hair noises are additionally labeled on the ISIC2020 dataset,
making it the first large-scale dermoscopic dataset with annotation of
hair-like artifacts. Extensive experiments demonstrate the superiority of the
proposed framework as well as the effectiveness of each component. The improved
dataset publicly avaliable at https://github.com/kirtsy/DermoscopicDataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Privacy-Preserving Person Re-identification via Person Identify Shift. (arXiv:2207.07311v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07311">
<div class="article-summary-box-inner">
<span><p>Recently privacy concerns of person re-identification (ReID) raise more and
more attention and preserving the privacy of the pedestrian images used by ReID
methods become essential. De-identification (DeID) methods alleviate privacy
issues by removing the identity-related of the ReID data. However, most of the
existing DeID methods tend to remove all personal identity-related information
and compromise the usability of de-identified data on the ReID task. In this
paper, we aim to develop a technique that can achieve a good trade-off between
privacy protection and data usability for person ReID. To achieve this, we
propose a novel de-identification method designed explicitly for person ReID,
named Person Identify Shift (PIS). PIS removes the absolute identity in a
pedestrian image while preserving the identity relationship between image
pairs. By exploiting the interpolation property of variational auto-encoder,
PIS shifts each pedestrian image from the current identity to another with a
new identity, resulting in images still preserving the relative identities.
Experimental results show that our method has a better trade-off between
privacy-preserving and model performance than existing de-identification
methods and can defend against human and model attacks for data privacy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Privacy-Preserving Face Recognition with Learnable Privacy Budgets in Frequency Domain. (arXiv:2207.07316v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07316">
<div class="article-summary-box-inner">
<span><p>Face recognition technology has been used in many fields due to its high
recognition accuracy, including the face unlocking of mobile devices, community
access control systems, and city surveillance. As the current high accuracy is
guaranteed by very deep network structures, facial images often need to be
transmitted to third-party servers with high computational power for inference.
However, facial images visually reveal the user's identity information. In this
process, both untrusted service providers and malicious users can significantly
increase the risk of a personal privacy breach. Current privacy-preserving
approaches to face recognition are often accompanied by many side effects, such
as a significant increase in inference time or a noticeable decrease in
recognition accuracy. This paper proposes a privacy-preserving face recognition
method using differential privacy in the frequency domain. Due to the
utilization of differential privacy, it offers a guarantee of privacy in
theory. Meanwhile, the loss of accuracy is very slight. This method first
converts the original image to the frequency domain and removes the direct
component termed DC. Then a privacy budget allocation method can be learned
based on the loss of the back-end face recognition network within the
differential privacy framework. Finally, it adds the corresponding noise to the
frequency domain features. Our method performs very well with several classical
face recognition test sets according to the extensive experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancement by Your Aesthetic: An Intelligible Unsupervised Personalized Enhancer for Low-Light Images. (arXiv:2207.07317v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07317">
<div class="article-summary-box-inner">
<span><p>Low-light image enhancement is an inherently subjective process whose targets
vary with the user's aesthetic. Motivated by this, several personalized
enhancement methods have been investigated. However, the enhancement process
based on user preferences in these techniques is invisible, i.e., a "black
box". In this work, we propose an intelligible unsupervised personalized
enhancer (iUPEnhancer) for low-light images, which establishes the correlations
between the low-light and the unpaired reference images with regard to three
user-friendly attributions (brightness, chromaticity, and noise). The proposed
iUP-Enhancer is trained with the guidance of these correlations and the
corresponding unsupervised loss functions. Rather than a "black box" process,
our iUP-Enhancer presents an intelligible enhancement process with the above
attributions. Extensive experiments demonstrate that the proposed algorithm
produces competitive qualitative and quantitative results while maintaining
excellent flexibility and scalability. This can be validated by personalization
with single/multiple references, cross-attribution references, or merely
adjusting parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stereo Co-capture System for Recording and Tracking Fish with Frame- and Event Cameras. (arXiv:2207.07332v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07332">
<div class="article-summary-box-inner">
<span><p>This work introduces a co-capture system for multi-animal visual data
acquisition using conventional cameras and event cameras. Event cameras offer
multiple advantages over frame-based cameras, such as a high temporal
resolution and temporal redundancy suppression, which enable us to efficiently
capture the fast and erratic movements of fish. We furthermore present an
event-based multi-animal tracking algorithm, which proves the feasibility of
the approach and sets the baseline for further exploration of combining the
advantages of event cameras and conventional cameras for multi-animal tracking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rain Rate Estimation with SAR using NEXRAD measurements with Convolutional Neural Networks. (arXiv:2207.07333v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07333">
<div class="article-summary-box-inner">
<span><p>Remote sensing of rainfall events is critical for both operational and
scientific needs, including for example weather forecasting, extreme flood
mitigation, water cycle monitoring, etc. Ground-based weather radars, such as
NOAA's Next-Generation Radar (NEXRAD), provide reflectivity and precipitation
measurements of rainfall events. However, the observation range of such radars
is limited to a few hundred kilometers, prompting the exploration of other
remote sensing methods, paricularly over the open ocean, that represents large
areas not covered by land-based radars. For a number of decades, C-band SAR
imagery such a such as Sentinel-1 imagery has been known to exhibit rainfall
signatures over the sea surface. However, the development of SAR-derived
rainfall products remains a challenge. Here we propose a deep learning approach
to extract rainfall information from SAR imagery. We demonstrate that a
convolutional neural network, such as U-Net, trained on a colocated and
preprocessed Sentinel-1/NEXRAD dataset clearly outperforms state-of-the-art
filtering schemes. Our results indicate high performance in segmenting
precipitation regimes, delineated by thresholds at 1, 3, and 10 mm/h. Compared
to current methods that rely on Koch filters to draw binary rainfall maps,
these multi-threshold learning-based models can provide rainfall estimation for
higher wind speeds and thus may be of great interest for data assimilation
weather forecasting or for improving the qualification of SAR-derived wind
field data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Parallax Transformer Network for Stereo Image JPEG Artifacts Removal. (arXiv:2207.07335v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07335">
<div class="article-summary-box-inner">
<span><p>Under stereo settings, the performance of image JPEG artifacts removal can be
further improved by exploiting the additional information provided by a second
view. However, incorporating this information for stereo image JPEG artifacts
removal is a huge challenge, since the existing compression artifacts make
pixel-level view alignment difficult. In this paper, we propose a novel
parallax transformer network (PTNet) to integrate the information from stereo
image pairs for stereo image JPEG artifacts removal. Specifically, a
well-designed symmetric bi-directional parallax transformer module is proposed
to match features with similar textures between different views instead of
pixel-level view alignment. Due to the issues of occlusions and boundaries, a
confidence-based cross-view fusion module is proposed to achieve better feature
fusion for both views, where the cross-view features are weighted with
confidence maps. Especially, we adopt a coarse-to-fine design for the
cross-view interaction, leading to better performance. Comprehensive
experimental results demonstrate that our PTNet can effectively remove
compression artifacts and achieves superior performance than other testing
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DuetFace: Collaborative Privacy-Preserving Face Recognition via Channel Splitting in the Frequency Domain. (arXiv:2207.07340v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07340">
<div class="article-summary-box-inner">
<span><p>With the wide application of face recognition systems, there is rising
concern that original face images could be exposed to malicious intents and
consequently cause personal privacy breaches. This paper presents DuetFace, a
novel privacy-preserving face recognition method that employs collaborative
inference in the frequency domain. Starting from a counterintuitive discovery
that face recognition can achieve surprisingly good performance with only
visually indistinguishable high-frequency channels, this method designs a
credible split of frequency channels by their cruciality for visualization and
operates the server-side model on non-crucial channels. However, the model
degrades in its attention to facial features due to the missing visual
information. To compensate, the method introduces a plug-in interactive block
to allow attention transfer from the client-side by producing a feature mask.
The mask is further refined by deriving and overlaying a facial region of
interest (ROI). Extensive experiments on multiple datasets validate the
effectiveness of the proposed method in protecting face images from undesired
visual inspection, reconstruction, and identification while maintaining high
task availability and performance. Results show that the proposed method
achieves a comparable recognition accuracy and computation cost to the
unprotected ArcFace and outperforms the state-of-the-art privacy-preserving
methods. The source code is available at
https://github.com/Tencent/TFace/tree/master/recognition/tasks/duetface.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feasibility of Inconspicuous GAN-generated Adversarial Patches against Object Detection. (arXiv:2207.07347v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07347">
<div class="article-summary-box-inner">
<span><p>Standard approaches for adversarial patch generation lead to noisy
conspicuous patterns, which are easily recognizable by humans. Recent research
has proposed several approaches to generate naturalistic patches using
generative adversarial networks (GANs), yet only a few of them were evaluated
on the object detection use case. Moreover, the state of the art mostly focuses
on suppressing a single large bounding box in input by overlapping it with the
patch directly. Suppressing objects near the patch is a different, more complex
task. In this work, we have evaluated the existing approaches to generate
inconspicuous patches. We have adapted methods, originally developed for
different computer vision tasks, to the object detection use case with YOLOv3
and the COCO dataset. We have evaluated two approaches to generate naturalistic
patches: by incorporating patch generation into the GAN training process and by
using the pretrained GAN. For both cases, we have assessed a trade-off between
performance and naturalistic patch appearance. Our experiments have shown, that
using a pre-trained GAN helps to gain realistic-looking patches while
preserving the performance similar to conventional adversarial patches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diverse Human Motion Prediction via Gumbel-Softmax Sampling from an Auxiliary Space. (arXiv:2207.07351v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07351">
<div class="article-summary-box-inner">
<span><p>Diverse human motion prediction aims at predicting multiple possible future
pose sequences from a sequence of observed poses. Previous approaches usually
employ deep generative networks to model the conditional distribution of data,
and then randomly sample outcomes from the distribution. While different
results can be obtained, they are usually the most likely ones which are not
diverse enough. Recent work explicitly learns multiple modes of the conditional
distribution via a deterministic network, which however can only cover a fixed
number of modes within a limited range. In this paper, we propose a novel
sampling strategy for sampling very diverse results from an imbalanced
multimodal distribution learned by a deep generative model. Our method works by
generating an auxiliary space and smartly making randomly sampling from the
auxiliary space equivalent to the diverse sampling from the target
distribution. We propose a simple yet effective network architecture that
implements this novel sampling strategy, which incorporates a Gumbel-Softmax
coefficient matrix sampling method and an aggressive diversity promoting hinge
loss function. Extensive experiments demonstrate that our method significantly
improves both the diversity and accuracy of the samplings compared with
previous state-of-the-art sampling approaches. Code and pre-trained models are
available at https://github.com/Droliven/diverse_sampling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Registration based Few-Shot Anomaly Detection. (arXiv:2207.07361v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07361">
<div class="article-summary-box-inner">
<span><p>This paper considers few-shot anomaly detection (FSAD), a practical yet
under-studied setting for anomaly detection (AD), where only a limited number
of normal images are provided for each category at training. So far, existing
FSAD studies follow the one-model-per-category learning paradigm used for
standard AD, and the inter-category commonality has not been explored. Inspired
by how humans detect anomalies, i.e., comparing an image in question to normal
images, we here leverage registration, an image alignment task that is
inherently generalizable across categories, as the proxy task, to train a
category-agnostic anomaly detection model. During testing, the anomalies are
identified by comparing the registered features of the test image and its
corresponding support (normal) images. As far as we know, this is the first
FSAD method that trains a single generalizable model and requires no
re-training or parameter fine-tuning for new categories. Experimental results
have shown that the proposed method outperforms the state-of-the-art FSAD
methods by 3%-8% in AUC on the MVTec and MPDD benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Trainable Joint Bilateral Filters for Enhanced Prediction Stability in Low-dose CT. (arXiv:2207.07368v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07368">
<div class="article-summary-box-inner">
<span><p>Low-dose computed tomography (CT) denoising algorithms aim to enable reduced
patient dose in routine CT acquisitions while maintaining high image quality.
Recently, deep learning~(DL)-based methods were introduced, outperforming
conventional denoising algorithms on this task due to their high model
capacity. However, for the transition of DL-based denoising to clinical
practice, these data-driven approaches must generalize robustly beyond the seen
training data. We, therefore, propose a hybrid denoising approach consisting of
a set of trainable joint bilateral filters (JBFs) combined with a convolutional
DL-based denoising network to predict the guidance image. Our proposed
denoising pipeline combines the high model capacity enabled by DL-based feature
extraction with the reliability of the conventional JBF. The pipeline's ability
to generalize is demonstrated by training on abdomen CT scans without metal
implants and testing on abdomen scans with metal implants as well as on head CT
data. When embedding two well-established DL-based denoisers (RED-CNN/QAE) in
our pipeline, the denoising performance is improved by $10\,\%$/$82\,\%$ (RMSE)
and $3\,\%$/$81\,\%$ (PSNR) in regions containing metal and by $6\,\%$/$78\,\%$
(RMSE) and $2\,\%$/$4\,\%$ (PSNR) on head CT data, compared to the respective
vanilla model. Concluding, the proposed trainable JBFs limit the error bound of
deep neural networks to facilitate the applicability of DL-based denoisers in
low-dose CT pipelines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CKD-TransBTS: Clinical Knowledge-Driven Hybrid Transformer with Modality-Correlated Cross-Attention for Brain Tumor Segmentation. (arXiv:2207.07370v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07370">
<div class="article-summary-box-inner">
<span><p>Brain tumor segmentation (BTS) in magnetic resonance image (MRI) is crucial
for brain tumor diagnosis, cancer management and research purposes. With the
great success of the ten-year BraTS challenges as well as the advances of CNN
and Transformer algorithms, a lot of outstanding BTS models have been proposed
to tackle the difficulties of BTS in different technical aspects. However,
existing studies hardly consider how to fuse the multi-modality images in a
reasonable manner. In this paper, we leverage the clinical knowledge of how
radiologists diagnose brain tumors from multiple MRI modalities and propose a
clinical knowledge-driven brain tumor segmentation model, called CKD-TransBTS.
Instead of directly concatenating all the modalities, we re-organize the input
modalities by separating them into two groups according to the imaging
principle of MRI. A dual-branch hybrid encoder with the proposed
modality-correlated cross-attention block (MCCA) is designed to extract the
multi-modality image features. The proposed model inherits the strengths from
both Transformer and CNN with the local feature representation ability for
precise lesion boundaries and long-range feature extraction for 3D volumetric
images. To bridge the gap between Transformer and CNN features, we propose a
Trans&amp;CNN Feature Calibration block (TCFC) in the decoder. We compare the
proposed model with five CNN-based models and six transformer-based models on
the BraTS 2021 challenge dataset. Extensive experiments demonstrate that the
proposed model achieves state-of-the-art brain tumor segmentation performance
compared with all the competitors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Instances as 1D Kernels. (arXiv:2207.07372v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07372">
<div class="article-summary-box-inner">
<span><p>We introduce a 3D instance representation, termed instance kernels, where
instances are represented by one-dimensional vectors that encode the semantic,
positional, and shape information of 3D instances. We show that instance
kernels enable easy mask inference by simply scanning kernels over the entire
scenes, avoiding the heavy reliance on proposals or heuristic clustering
algorithms in standard 3D instance segmentation pipelines. The idea of instance
kernel is inspired by recent success of dynamic convolutions in 2D/3D instance
segmentation. However, we find it non-trivial to represent 3D instances due to
the disordered and unstructured nature of point cloud data, e.g., poor instance
localization can significantly degrade instance representation. To remedy this,
we construct a novel 3D instance encoding paradigm. First, potential instance
centroids are localized as candidates. Then, a candidate merging scheme is
devised to simultaneously aggregate duplicated candidates and collect context
around the merged centroids to form the instance kernels. Once instance kernels
are available, instance masks can be reconstructed via dynamic convolutions
whose weights are conditioned on instance kernels. The whole pipeline is
instantiated with a dynamic kernel network (DKNet). Results show that DKNet
outperforms the state of the arts on both ScanNetV2 and S3DIS datasets with
better instance localization. Code is available:
https://github.com/W1zheng/DKNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Dual-Masked Auto-Encoder for Robust Motion Capture with Spatial-Temporal Skeletal Token Completion. (arXiv:2207.07381v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07381">
<div class="article-summary-box-inner">
<span><p>Multi-person motion capture can be challenging due to ambiguities caused by
severe occlusion, fast body movement, and complex interactions. Existing
frameworks build on 2D pose estimations and triangulate to 3D coordinates via
reasoning the appearance, trajectory, and geometric consistencies among
multi-camera observations. However, 2D joint detection is usually incomplete
and with wrong identity assignments due to limited observation angle, which
leads to noisy 3D triangulation results. To overcome this issue, we propose to
explore the short-range autoregressive characteristics of skeletal motion using
transformer. First, we propose an adaptive, identity-aware triangulation module
to reconstruct 3D joints and identify the missing joints for each identity. To
generate complete 3D skeletal motion, we then propose a Dual-Masked
Auto-Encoder (D-MAE) which encodes the joint status with both
skeletal-structural and temporal position encoding for trajectory completion.
D-MAE's flexible masking and encoding mechanism enable arbitrary skeleton
definitions to be conveniently deployed under the same framework. In order to
demonstrate the proposed model's capability in dealing with severe data loss
scenarios, we contribute a high-accuracy and challenging motion capture dataset
of multi-person interactions with severe occlusion. Evaluations on both
benchmark and our new dataset demonstrate the efficiency of our proposed model,
as well as its advantage against the other state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LapSeg3D: Weakly Supervised Semantic Segmentation of Point Clouds Representing Laparoscopic Scenes. (arXiv:2207.07418v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07418">
<div class="article-summary-box-inner">
<span><p>The semantic segmentation of surgical scenes is a prerequisite for task
automation in robot assisted interventions. We propose LapSeg3D, a novel
DNN-based approach for the voxel-wise annotation of point clouds representing
surgical scenes. As the manual annotation of training data is highly time
consuming, we introduce a semi-autonomous clustering-based pipeline for the
annotation of the gallbladder, which is used to generate segmented labels for
the DNN. When evaluated against manually annotated data, LapSeg3D achieves an
F1 score of 0.94 for gallbladder segmentation on various datasets of ex-vivo
porcine livers. We show LapSeg3D to generalize accurately across different
gallbladders and datasets recorded with different RGB-D camera systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Object Tracking and Segmentation via Neural Message Passing. (arXiv:2207.07454v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07454">
<div class="article-summary-box-inner">
<span><p>Graphs offer a natural way to formulate Multiple Object Tracking (MOT) and
Multiple Object Tracking and Segmentation (MOTS) within the
tracking-by-detection paradigm. However, they also introduce a major challenge
for learning methods, as defining a model that can operate on such structured
domain is not trivial. In this work, we exploit the classical network flow
formulation of MOT to define a fully differentiable framework based on Message
Passing Networks (MPNs). By operating directly on the graph domain, our method
can reason globally over an entire set of detections and exploit contextual
features. It then jointly predicts both final solutions for the data
association problem and segmentation masks for all objects in the scene while
exploiting synergies between the two tasks. We achieve state-of-the-art results
for both tracking and segmentation in several publicly available datasets. Our
code is available at github.com/ocetintas/MPNTrackSeg.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepSolar tracker: towards unsupervised assessment with open-source data of the accuracy of deep learning-based distributed PV mapping. (arXiv:2207.07466v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07466">
<div class="article-summary-box-inner">
<span><p>Photovoltaic (PV) energy is key to mitigating the current energy crisis.
However, distributed PV generation, which amounts to half of the PV energy
generation, makes it increasingly difficult for transmission system operators
(TSOs) to balance the load and supply and avoid grid congestions. Indeed, in
the absence of measurements, estimating the distributed PV generation is tough.
In recent years, many remote sensing-based approaches have been proposed to map
distributed PV installations. However, to be applicable in industrial settings,
one needs to assess the accuracy of the mapping over the whole deployment area.
We build on existing work to propose an automated PV registry pipeline. This
pipeline automatically generates a dataset recording all distributed PV
installations' location, area, installed capacity, and tilt angle. It only
requires aerial orthoimagery and topological data, both of which are freely
accessible online. In order to assess the accuracy of the registry, we propose
an unsupervised method based on the {\it Registre national d'installation}
(RNI), that centralizes all individual PV systems aggregated at communal level,
enabling practitioners to assess the accuracy of the registry and eventually
remove outliers. We deploy our model on 9 French {\it d\'epartements} covering
more than 50 000 square kilometers, providing the largest mapping of
distributed PV panels with this level of detail to date. We then demonstrate
how practitioners can use our unsupervised accuracy assessment method to assess
the accuracy of the outputs. In particular, we show how it can easily identify
outliers in the detections. Overall, our approach paves the way for a safer
integration of deep learning-based pipelines for remote PV mapping. Code is
available at {\tt https://github.com/gabrielkasmi/dsfrance}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">USegScene: Unsupervised Learning of Depth, Optical Flow and Ego-Motion with Semantic Guidance and Coupled Networks. (arXiv:2207.07469v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07469">
<div class="article-summary-box-inner">
<span><p>In this paper we propose USegScene, a framework for semantically guided
unsupervised learning of depth, optical flow and ego-motion estimation for
stereo camera images using convolutional neural networks. Our framework
leverages semantic information for improved regularization of depth and optical
flow maps, multimodal fusion and occlusion filling considering dynamic rigid
object motions as independent SE(3) transformations. Furthermore, complementary
to pure photo-metric matching, we propose matching of semantic features,
pixel-wise classes and object instance borders between the consecutive images.
In contrast to previous methods, we propose a network architecture that jointly
predicts all outputs using shared encoders and allows passing information
across the task-domains, e.g., the prediction of optical flow can benefit from
the prediction of the depth. Furthermore, we explicitly learn the depth and
optical flow occlusion maps inside the network, which are leveraged in order to
improve the predictions in therespective regions. We present results on the
popular KITTI dataset and show that our approach outperforms other methods by a
large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RESECT-SEG: Open access annotations of intra-operative brain tumor ultrasound images. (arXiv:2207.07494v1 [physics.med-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07494">
<div class="article-summary-box-inner">
<span><p>Purpose: Registration and segmentation of magnetic resonance (MR) and
ultrasound (US) images play an essential role in surgical planning and
resection of brain tumors. However, validating these techniques is challenging
due to the scarcity of publicly accessible sources with high-quality ground
truth information. To this end, we propose a unique annotation dataset of tumor
tissues and resection cavities from the previously published RESECT dataset
(Xiao et al. 2017) to encourage a more rigorous assessments of image processing
techniques. Acquisition and validation methods: The RESECT database consists of
MR and intraoperative US (iUS) images of 23 patients who underwent resection
surgeries. The proposed dataset contains tumor tissues and resection cavity
annotations of the iUS images. The quality of annotations were validated by two
highly experienced neurosurgeons through several assessment criteria. Data
format and availability: Annotations of tumor tissues and resection cavities
are provided in 3D NIFTI formats. Both sets of annotations are accessible
online in the \url{https://osf.io/6y4db}. Discussion and potential
applications: The proposed database includes tumor tissue and resection cavity
annotations from real-world clinical ultrasound brain images to evaluate
segmentation and registration methods. These labels could also be used to train
deep learning approaches. Eventually, this dataset should further improve the
quality of image guidance in neurosurgery.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Augmenting Softmax Information for Selective Classification with Out-of-Distribution Data. (arXiv:2207.07506v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07506">
<div class="article-summary-box-inner">
<span><p>Detecting out-of-distribution (OOD) data is a task that is receiving an
increasing amount of research attention in the domain of deep learning for
computer vision. However, the performance of detection methods is generally
evaluated on the task in isolation, rather than also considering potential
downstream tasks in tandem. In this work, we examine selective classification
in the presence of OOD data (SCOD). That is to say, the motivation for
detecting OOD samples is to reject them so their impact on the quality of
predictions is reduced. We show under this task specification, that existing
post-hoc methods perform quite differently compared to when evaluated only on
OOD detection. This is because it is no longer an issue to conflate
in-distribution (ID) data with OOD data if the ID data is going to be
misclassified. However, the conflation within ID data of correct and incorrect
predictions becomes undesirable. We also propose a novel method for SCOD,
Softmax Information Retaining Combination (SIRC), that augments softmax-based
confidence scores with feature-agnostic information such that their ability to
identify OOD samples is improved without sacrificing separation between correct
and incorrect ID predictions. Experiments on a wide variety of ImageNet-scale
datasets and convolutional neural network architectures show that SIRC is able
to consistently match or outperform the baseline for SCOD, whilst existing OOD
detection methods fail to do so.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Usefulness of Deep Ensemble Diversity for Out-of-Distribution Detection. (arXiv:2207.07517v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07517">
<div class="article-summary-box-inner">
<span><p>The ability to detect Out-of-Distribution (OOD) data is important in
safety-critical applications of deep learning. The aim is to separate
In-Distribution (ID) data drawn from the training distribution from OOD data
using a measure of uncertainty extracted from a deep neural network. Deep
Ensembles are a well-established method of improving the quality of uncertainty
estimates produced by deep neural networks, and have been shown to have
superior OOD detection performance compared to single models. An existing
intuition in the literature is that the diversity of Deep Ensemble predictions
indicates distributional shift, and so measures of diversity such as Mutual
Information (MI) should be used for OOD detection. We show experimentally that
this intuition is not valid on ImageNet-scale OOD detection -- using MI leads
to 30-40% worse %FPR@95 compared to single-model entropy on some OOD datasets.
We suggest an alternative explanation for Deep Ensembles' better OOD detection
performance -- OOD detection is binary classification and we are ensembling
diverse classifiers. As such we show that practically, even better OOD
detection performance can be achieved for Deep Ensembles by averaging
task-specific detection scores such as Energy over the ensemble.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bi-PointFlowNet: Bidirectional Learning for Point Cloud Based Scene Flow Estimation. (arXiv:2207.07522v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07522">
<div class="article-summary-box-inner">
<span><p>Scene flow estimation, which extracts point-wise motion between scenes, is
becoming a crucial task in many computer vision tasks. However, all of the
existing estimation methods utilize only the unidirectional features,
restricting the accuracy and generality. This paper presents a novel scene flow
estimation architecture using bidirectional flow embedding layers. The proposed
bidirectional layer learns features along both forward and backward directions,
enhancing the estimation performance. In addition, hierarchical feature
extraction and warping improve the performance and reduce computational
overhead. Experimental results show that the proposed architecture achieved a
new state-of-the-art record by outperforming other approaches with large margin
in both FlyingThings3D and KITTI benchmarks. Codes are available at
https://github.com/cwc1260/BiFlow.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3DVerifier: Efficient Robustness Verification for 3D Point Cloud Models. (arXiv:2207.07539v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07539">
<div class="article-summary-box-inner">
<span><p>3D point cloud models are widely applied in safety-critical scenes, which
delivers an urgent need to obtain more solid proofs to verify the robustness of
models. Existing verification method for point cloud model is time-expensive
and computationally unattainable on large networks. Additionally, they cannot
handle the complete PointNet model with joint alignment network (JANet) that
contains multiplication layers, which effectively boosts the performance of 3D
models. This motivates us to design a more efficient and general framework to
verify various architectures of point cloud models. The key challenges in
verifying the large-scale complete PointNet models are addressed as dealing
with the cross-non-linearity operations in the multiplication layers and the
high computational complexity of high-dimensional point cloud inputs and added
layers. Thus, we propose an efficient verification framework, 3DVerifier, to
tackle both challenges by adopting a linear relaxation function to bound the
multiplication layer and combining forward and backward propagation to compute
the certified bounds of the outputs of the point cloud models. Our
comprehensive experiments demonstrate that 3DVerifier outperforms existing
verification algorithms for 3D models in terms of both efficiency and accuracy.
Notably, our approach achieves an orders-of-magnitude improvement in
verification efficiency for the large network, and the obtained certified
bounds are also significantly tighter than the state-of-the-art verifiers. We
release our tool 3DVerifier via https://github.com/TrustAI/3DVerifier for use
by the community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CheXplaining in Style: Counterfactual Explanations for Chest X-rays using StyleGAN. (arXiv:2207.07553v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07553">
<div class="article-summary-box-inner">
<span><p>Deep learning models used in medical image analysis are prone to raising
reliability concerns due to their black-box nature. To shed light on these
black-box models, previous works predominantly focus on identifying the
contribution of input features to the diagnosis, i.e., feature attribution. In
this work, we explore counterfactual explanations to identify what patterns the
models rely on for diagnosis. Specifically, we investigate the effect of
changing features within chest X-rays on the classifier's output to understand
its decision mechanism. We leverage a StyleGAN-based approach (StyleEx) to
create counterfactual explanations for chest X-rays by manipulating specific
latent directions in their latent space. In addition, we propose EigenFind to
significantly reduce the computation time of generated explanations. We
clinically evaluate the relevancy of our counterfactual explanations with the
help of radiologists. Our code is publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mobile Keystroke Biometrics Using Transformers. (arXiv:2207.07596v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07596">
<div class="article-summary-box-inner">
<span><p>Behavioural biometrics have proven to be effective against identity theft as
well as be considered user-friendly authentication methods. One of the most
popular traits in the literature is keystroke dynamics due to the large
deployment of computers and mobile devices in our society. This paper focuses
on improving keystroke biometric systems on the free-text scenario. This
scenario is characterised as very challenging due to the uncontrolled text
conditions, the influential of the user's emotional and physical state, and the
in-use application. To overcome these drawbacks, methods based on deep learning
such as Convolutional Neural Networks (CNNs) and Recurrent Neural Networks
(RNNs) have been proposed in the literature, outperforming traditional machine
learning methods. However, these architectures still have aspects that need to
be reviewed and improved. To the best of our knowledge, this is the first study
that proposes keystroke biometric systems based on Transformers. The proposed
Transformer architecture has achieved Equal Error Rate (EER) values of 3.84% in
the popular Aalto mobile keystroke database using only 5 enrolment sessions,
outperforming in large margin other state-of-the-art approaches in the
literature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ST-P3: End-to-end Vision-based Autonomous Driving via Spatial-Temporal Feature Learning. (arXiv:2207.07601v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07601">
<div class="article-summary-box-inner">
<span><p>Many existing autonomous driving paradigms involve a multi-stage discrete
pipeline of tasks. To better predict the control signals and enhance user
safety, an end-to-end approach that benefits from joint spatial-temporal
feature learning is desirable. While there are some pioneering works on
LiDAR-based input or implicit design, in this paper we formulate the problem in
an interpretable vision-based setting. In particular, we propose a
spatial-temporal feature learning scheme towards a set of more representative
features for perception, prediction and planning tasks simultaneously, which is
called ST-P3. Specifically, an egocentric-aligned accumulation technique is
proposed to preserve geometry information in 3D space before the bird's eye
view transformation for perception; a dual pathway modeling is devised to take
past motion variations into account for future prediction; a temporal-based
refinement unit is introduced to compensate for recognizing vision-based
elements for planning. To the best of our knowledge, we are the first to
systematically investigate each part of an interpretable end-to-end
vision-based autonomous driving system. We benchmark our approach against
previous state-of-the-arts on both open-loop nuScenes dataset as well as
closed-loop CARLA simulation. The results show the effectiveness of our method.
Source code, model and protocol details are made publicly available at
https://github.com/OpenPerceptionX/ST-P3.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image and Texture Independent Deep Learning Noise Estimation using Multiple Frames. (arXiv:2207.07604v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07604">
<div class="article-summary-box-inner">
<span><p>In this study, a novel multiple-frame based image and texture independent
convolutional Neural Network (CNN) noise estimator is introduced. The estimator
works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DOLPHINS: Dataset for Collaborative Perception enabled Harmonious and Interconnected Self-driving. (arXiv:2207.07609v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07609">
<div class="article-summary-box-inner">
<span><p>Vehicle-to-Everything (V2X) network has enabled collaborative perception in
autonomous driving, which is a promising solution to the fundamental defect of
stand-alone intelligence including blind zones and long-range perception.
However, the lack of datasets has severely blocked the development of
collaborative perception algorithms. In this work, we release DOLPHINS: Dataset
for cOllaborative Perception enabled Harmonious and INterconnected
Self-driving, as a new simulated large-scale various-scenario multi-view
multi-modality autonomous driving dataset, which provides a ground-breaking
benchmark platform for interconnected autonomous driving. DOLPHINS outperforms
current datasets in six dimensions: temporally-aligned images and point clouds
from both vehicles and Road Side Units (RSUs) enabling both Vehicle-to-Vehicle
(V2V) and Vehicle-to-Infrastructure (V2I) based collaborative perception; 6
typical scenarios with dynamic weather conditions make the most various
interconnected autonomous driving dataset; meticulously selected viewpoints
providing full coverage of the key areas and every object; 42376 frames and
292549 objects, as well as the corresponding 3D annotations, geo-positions, and
calibrations, compose the largest dataset for collaborative perception; Full-HD
images and 64-line LiDARs construct high-resolution data with sufficient
details; well-organized APIs and open-source codes ensure the extensibility of
DOLPHINS. We also construct a benchmark of 2D detection, 3D detection, and
multi-view collaborative perception tasks on DOLPHINS. The experiment results
show that the raw-level fusion scheme through V2X communication can help to
improve the precision as well as to reduce the necessity of expensive LiDAR
equipment on vehicles when RSUs exist, which may accelerate the popularity of
interconnected self-driving vehicles. DOLPHINS is now available on
https://dolphins-dataset.net/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Position Prediction as an Effective Pretraining Strategy. (arXiv:2207.07611v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07611">
<div class="article-summary-box-inner">
<span><p>Transformers have gained increasing popularity in a wide range of
applications, including Natural Language Processing (NLP), Computer Vision and
Speech Recognition, because of their powerful representational capacity.
However, harnessing this representational capacity effectively requires a large
amount of data, strong regularization, or both, to mitigate overfitting.
Recently, the power of the Transformer has been unlocked by self-supervised
pretraining strategies based on masked autoencoders which rely on
reconstructing masked inputs, directly, or contrastively from unmasked content.
This pretraining strategy which has been used in BERT models in NLP, Wav2Vec
models in Speech and, recently, in MAE models in Vision, forces the model to
learn about relationships between the content in different parts of the input
using autoencoding related objectives. In this paper, we propose a novel, but
surprisingly simple alternative to content reconstruction~-- that of predicting
locations from content, without providing positional information for it. Doing
so requires the Transformer to understand the positional relationships between
different parts of the input, from their content alone. This amounts to an
efficient implementation where the pretext task is a classification problem
among all possible positions for each input token. We experiment on both Vision
and Speech benchmarks, where our approach brings improvements over strong
supervised training baselines and is comparable to modern
unsupervised/self-supervised pretraining methods. Our method also enables
Transformers trained without position embeddings to outperform ones trained
with full position information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Non-Anatomical Graph Structure for isolated hand gesture separation in continuous gesture sequences. (arXiv:2207.07619v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07619">
<div class="article-summary-box-inner">
<span><p>Continuous Hand Gesture Recognition (CHGR) has been extensively studied by
researchers in the last few decades. Recently, one model has been presented to
deal with the challenge of the boundary detection of isolated gestures in a
continuous gesture video [17]. To enhance the model performance and also
replace the handcrafted feature extractor in the presented model in [17], we
propose a GCN model and combine it with the stacked Bi-LSTM and Attention
modules to push the temporal information in the video stream. Considering the
breakthroughs of GCN models for skeleton modality, we propose a two-layer GCN
model to empower the 3D hand skeleton features. Finally, the class
probabilities of each isolated gesture are fed to the post-processing module,
borrowed from [17]. Furthermore, we replace the anatomical graph structure with
some non-anatomical graph structures. Due to the lack of a large dataset,
including both the continuous gesture sequences and the corresponding isolated
gestures, three public datasets in Dynamic Hand Gesture Recognition (DHGR),
RKS-PERSIANSIGN, and ASLVID, are used for evaluation. Experimental results show
the superiority of the proposed model in dealing with isolated gesture
boundaries detection in continuous gesture sequences
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MegaPortraits: One-shot Megapixel Neural Head Avatars. (arXiv:2207.07621v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07621">
<div class="article-summary-box-inner">
<span><p>In this work, we advance the neural head avatar technology to the megapixel
resolution while focusing on the particularly challenging task of cross-driving
synthesis, i.e., when the appearance of the driving image is substantially
different from the animated source image. We propose a set of new neural
architectures and training methods that can leverage both medium-resolution
video data and high-resolution image data to achieve the desired levels of
rendered image quality and generalization to novel views and motion. We
demonstrate that suggested architectures and methods produce convincing
high-resolution neural avatars, outperforming the competitors in the
cross-driving scenario. Lastly, we show how a trained high-resolution neural
avatar model can be distilled into a lightweight student model which runs in
real-time and locks the identities of neural avatars to several dozens of
pre-defined source images. Real-time operation and identity lock are essential
for many practical applications head avatar systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Brain MRI study for glioma segmentation using convolutional neural networks and original post-processing techniques with low computational demand. (arXiv:2207.07622v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07622">
<div class="article-summary-box-inner">
<span><p>Gliomas are brain tumors composed of different highly heterogeneous
histological subregions. Image analysis techniques to identify relevant tumor
substructures have high potential for improving patient diagnosis, treatment
and prognosis. However, due to the high heterogeneity of gliomas, the
segmentation task is currently a major challenge in the field of medical image
analysis. In the present work, the database of the Brain Tumor Segmentation
(BraTS) Challenge 2018, composed of multimodal MRI scans of gliomas, was
studied. A segmentation methodology based on the design and application of
convolutional neural networks (CNNs) combined with original post-processing
techniques with low computational demand was proposed. The post-processing
techniques were the main responsible for the results obtained in the
segmentations. The segmented regions were the whole tumor, the tumor core, and
the enhancing tumor core, obtaining averaged Dice coefficients equal to 0.8934,
0.8376, and 0.8113, respectively. These results reached the state of the art in
glioma segmentation determined by the winners of the challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GUSOT: Green and Unsupervised Single Object Tracking for Long Video Sequences. (arXiv:2207.07629v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07629">
<div class="article-summary-box-inner">
<span><p>Supervised and unsupervised deep trackers that rely on deep learning
technologies are popular in recent years. Yet, they demand high computational
complexity and a high memory cost. A green unsupervised single-object tracker,
called GUSOT, that aims at object tracking for long videos under a
resource-constrained environment is proposed in this work. Built upon a
baseline tracker, UHP-SOT++, which works well for short-term tracking, GUSOT
contains two additional new modules: 1) lost object recovery, and 2)
color-saliency-based shape proposal. They help resolve the tracking loss
problem and offer a more flexible object proposal, respectively. Thus, they
enable GUSOT to achieve higher tracking accuracy in the long run. We conduct
experiments on the large-scale dataset LaSOT with long video sequences, and
show that GUSOT offers a lightweight high-performance tracking solution that
finds applications in mobile and edge computing platforms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is a Caption Worth a Thousand Images? A Controlled Study for Representation Learning. (arXiv:2207.07635v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07635">
<div class="article-summary-box-inner">
<span><p>The development of CLIP [Radford et al., 2021] has sparked a debate on
whether language supervision can result in vision models with more transferable
representations than traditional image-only methods. Our work studies this
question through a carefully controlled comparison of two approaches in terms
of their ability to learn representations that generalize to downstream
classification tasks. We find that when the pre-training dataset meets certain
criteria -- it is sufficiently large and contains descriptive captions with low
variability -- image-only methods do not match CLIP's transfer performance,
even when they are trained with more image data. However, contrary to what one
might expect, there are practical settings in which these criteria are not met,
wherein added supervision through captions is actually detrimental. Motivated
by our findings, we devise simple prescriptions to enable CLIP to better
leverage the language information present in existing pre-training datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Open-Vocabulary Video Classification via Pre-Trained Vision and Language Models. (arXiv:2207.07646v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07646">
<div class="article-summary-box-inner">
<span><p>Utilizing vision and language models (VLMs) pre-trained on large-scale
image-text pairs is becoming a promising paradigm for open-vocabulary visual
recognition. In this work, we extend this paradigm by leveraging motion and
audio that naturally exist in video. We present \textbf{MOV}, a simple yet
effective method for \textbf{M}ultimodal \textbf{O}pen-\textbf{V}ocabulary
video classification. In MOV, we directly use the vision encoder from
pre-trained VLMs with minimal modifications to encode video, optical flow and
audio spectrogram. We design a cross-modal fusion mechanism to aggregate
complimentary multimodal information. Experiments on Kinetics-700 and VGGSound
show that introducing flow or audio modality brings large performance gains
over the pre-trained VLM and existing methods. Specifically, MOV greatly
improves the accuracy on base classes, while generalizes better on novel
classes. MOV achieves state-of-the-art results on UCF and HMDB zero-shot video
classification benchmarks, significantly outperforming both traditional
zero-shot methods and recent methods based on VLMs. Code and models will be
released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distributionally Robust Deep Learning using Hardness Weighted Sampling. (arXiv:2001.02658v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2001.02658">
<div class="article-summary-box-inner">
<span><p>Limiting failures of machine learning systems is of paramount importance for
safety-critical applications. In order to improve the robustness of machine
learning systems, Distributionally Robust Optimization (DRO) has been proposed
as a generalization of Empirical Risk Minimization (ERM). However, its use in
deep learning has been severely restricted due to the relative inefficiency of
the optimizers available for DRO in comparison to the wide-spread variants of
Stochastic Gradient Descent (SGD) optimizers for ERM. We propose SGD with
hardness weighted sampling, a principled and efficient optimization method for
DRO in machine learning that is particularly suited in the context of deep
learning. Similar to a hard example mining strategy in practice, the proposed
algorithm is straightforward to implement and computationally as efficient as
SGD-based optimizers used for deep learning, requiring minimal overhead
computation. In contrast to typical ad hoc hard mining approaches, we prove the
convergence of our DRO algorithm for over-parameterized deep learning networks
with ReLU activation and a finite number of layers and parameters. Our
experiments on fetal brain 3D MRI segmentation and brain tumor segmentation in
MRI demonstrate the feasibility and the usefulness of our approach. Using our
hardness weighted sampling for training a state-of-the-art deep learning
pipeline leads to improved robustness to anatomical variabilities in automatic
fetal brain 3D MRI segmentation using deep learning and to improved robustness
to the image protocol variations in brain tumor segmentation. Our code is
available at https://github.com/LucasFidon/HardnessWeightedSampler.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">clDice -- A Novel Topology-Preserving Loss Function for Tubular Structure Segmentation. (arXiv:2003.07311v7 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.07311">
<div class="article-summary-box-inner">
<span><p>Accurate segmentation of tubular, network-like structures, such as vessels,
neurons, or roads, is relevant to many fields of research. For such structures,
the topology is their most important characteristic; particularly preserving
connectedness: in the case of vascular networks, missing a connected vessel
entirely alters the blood-flow dynamics. We introduce a novel similarity
measure termed centerlineDice (short clDice), which is calculated on the
intersection of the segmentation masks and their (morphological) skeleta. We
theoretically prove that clDice guarantees topology preservation up to homotopy
equivalence for binary 2D and 3D segmentation. Extending this, we propose a
computationally efficient, differentiable loss function (soft-clDice) for
training arbitrary neural segmentation networks. We benchmark the soft-clDice
loss on five public datasets, including vessels, roads and neurons (2D and 3D).
Training on soft-clDice leads to segmentation with more accurate connectivity
information, higher graph similarity, and better volumetric scores.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ODFNet: Using orientation distribution functions to characterize 3D point clouds. (arXiv:2012.04708v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.04708">
<div class="article-summary-box-inner">
<span><p>Learning new representations of 3D point clouds is an active research area in
3D vision, as the order-invariant point cloud structure still presents
challenges to the design of neural network architectures. Recent works explored
learning either global or local features or both for point clouds, however none
of the earlier methods focused on capturing contextual shape information by
analysing local orientation distribution of points. In this paper, we leverage
on point orientation distributions around a point in order to obtain an
expressive local neighborhood representation for point clouds. We achieve this
by dividing the spherical neighborhood of a given point into predefined cone
volumes, and statistics inside each volume are used as point features. In this
way, a local patch can be represented by not only the selected point's nearest
neighbors, but also considering a point density distribution defined along
multiple orientations around the point. We are then able to construct an
orientation distribution function (ODF) neural network that involves an
ODFBlock which relies on mlp (multi-layer perceptron) layers. The new ODFNet
model achieves state-of the-art accuracy for object classification on
ModelNet40 and ScanObjectNN datasets, and segmentation on ShapeNet S3DIS
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">JigsawGAN: Auxiliary Learning for Solving Jigsaw Puzzles with Generative Adversarial Networks. (arXiv:2101.07555v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.07555">
<div class="article-summary-box-inner">
<span><p>The paper proposes a solution based on Generative Adversarial Network (GAN)
for solving jigsaw puzzles. The problem assumes that an image is divided into
equal square pieces, and asks to recover the image according to information
provided by the pieces. Conventional jigsaw puzzle solvers often determine the
relationships based on the boundaries of pieces, which ignore the important
semantic information. In this paper, we propose JigsawGAN, a GAN-based
auxiliary learning method for solving jigsaw puzzles with unpaired images (with
no prior knowledge of the initial images). We design a multi-task pipeline that
includes, (1) a classification branch to classify jigsaw permutations, and (2)
a GAN branch to recover features to images in correct orders. The
classification branch is constrained by the pseudo-labels generated according
to the shuffled pieces. The GAN branch concentrates on the image semantic
information, where the generator produces the natural images to fool the
discriminator, while the discriminator distinguishes whether a given image
belongs to the synthesized or the real target domain. These two branches are
connected by a flow-based warp module that is applied to warp features to
correct the order according to the classification results. The proposed method
can solve jigsaw puzzles more efficiently by utilizing both semantic
information and boundary information simultaneously. Qualitative and
quantitative comparisons against several representative jigsaw puzzle solvers
demonstrate the superiority of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UPHDR-GAN: Generative Adversarial Network for High Dynamic Range Imaging with Unpaired Data. (arXiv:2102.01850v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.01850">
<div class="article-summary-box-inner">
<span><p>The paper proposes a method to effectively fuse multi-exposure inputs and
generate high-quality high dynamic range (HDR) images with unpaired datasets.
Deep learning-based HDR image generation methods rely heavily on paired
datasets. The ground truth images play a leading role in generating reasonable
HDR images. Datasets without ground truth are hard to be applied to train deep
neural networks. Recently, Generative Adversarial Networks (GAN) have
demonstrated their potentials of translating images from source domain X to
target domain Y in the absence of paired examples. In this paper, we propose a
GAN-based network for solving such problems while generating enjoyable HDR
results, named UPHDR-GAN. The proposed method relaxes the constraint of the
paired dataset and learns the mapping from the LDR domain to the HDR domain.
Although the pair data are missing, UPHDR-GAN can properly handle the ghosting
artifacts caused by moving objects or misalignments with the help of the
modified GAN loss, the improved discriminator network and the useful
initialization phase. The proposed method preserves the details of important
regions and improves the total image perceptual quality. Qualitative and
quantitative comparisons against the representative methods demonstrate the
superiority of the proposed UPHDR-GAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoMix: Unveiling the Power of Mixup for Stronger Classifiers. (arXiv:2103.13027v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.13027">
<div class="article-summary-box-inner">
<span><p>Data mixing augmentation have proved to be effective for improving the
generalization ability of deep neural networks. While early methods mix samples
by hand-crafted policies (e.g., linear interpolation), recent methods utilize
saliency information to match the mixed samples and labels via complex offline
optimization. However, there arises a trade-off between precise mixing policies
and optimization complexity. To address this challenge, we propose a novel
automatic mixup (AutoMix) framework, where the mixup policy is parameterized
and serves the ultimate classification goal directly. Specifically, AutoMix
reformulates the mixup classification into two sub-tasks (i.e., mixed sample
generation and mixup classification) with corresponding sub-networks and solves
them in a bi-level optimization framework. For the generation, a learnable
lightweight mixup generator, Mix Block, is designed to generate mixed samples
by modeling patch-wise relationships under the direct supervision of the
corresponding mixed labels. To prevent the degradation and instability of
bi-level optimization, we further introduce a momentum pipeline to train
AutoMix in an end-to-end manner. Extensive experiments on nine image benchmarks
prove the superiority of AutoMix compared with state-of-the-arts in various
classification scenarios and downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Sequence Feature Alignment for Domain Adaptive Detection Transformers. (arXiv:2107.12636v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.12636">
<div class="article-summary-box-inner">
<span><p>Detection transformers have recently shown promising object detection results
and attracted increasing attention. However, how to develop effective domain
adaptation techniques to improve its cross-domain performance remains
unexplored and unclear. In this paper, we delve into this topic and empirically
find that direct feature distribution alignment on the CNN backbone only brings
limited improvements, as it does not guarantee domain-invariant sequence
features in the transformer for prediction. To address this issue, we propose a
novel Sequence Feature Alignment (SFA) method that is specially designed for
the adaptation of detection transformers. Technically, SFA consists of a domain
query-based feature alignment (DQFA) module and a token-wise feature alignment
(TDA) module. In DQFA, a novel domain query is used to aggregate and align
global context from the token sequence of both domains. DQFA reduces the domain
discrepancy in global feature representations and object relations when
deploying in the transformer encoder and decoder, respectively. Meanwhile, TDA
aligns token features in the sequence from both domains, which reduces the
domain gaps in local and instance-level feature representations in the
transformer encoder and decoder, respectively. Besides, a novel bipartite
matching consistency loss is proposed to enhance the feature discriminability
for robust object detection. Experiments on three challenging benchmarks show
that SFA outperforms state-of-the-art domain adaptive object detection methods.
Code has been made available at: https://github.com/encounter1997/SFA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReFormer: The Relational Transformer for Image Captioning. (arXiv:2107.14178v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.14178">
<div class="article-summary-box-inner">
<span><p>Image captioning is shown to be able to achieve a better performance by using
scene graphs to represent the relations of objects in the image. The current
captioning encoders generally use a Graph Convolutional Net (GCN) to represent
the relation information and merge it with the object region features via
concatenation or convolution to get the final input for sentence decoding.
However, the GCN-based encoders in the existing methods are less effective for
captioning due to two reasons. First, using the image captioning as the
objective (i.e., Maximum Likelihood Estimation) rather than a relation-centric
loss cannot fully explore the potential of the encoder. Second, using a
pre-trained model instead of the encoder itself to extract the relationships is
not flexible and cannot contribute to the explainability of the model. To
improve the quality of image captioning, we propose a novel architecture
ReFormer -- a RElational transFORMER to generate features with relation
information embedded and to explicitly express the pair-wise relationships
between objects in the image. ReFormer incorporates the objective of scene
graph generation with that of image captioning using one modified Transformer
model. This design allows ReFormer to generate not only better image captions
with the bene-fit of extracting strong relational image features, but also
scene graphs to explicitly describe the pair-wise relation-ships. Experiments
on publicly available datasets show that our model significantly outperforms
state-of-the-art methods on image captioning and scene graph generation
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EKTVQA: Generalized use of External Knowledge to empower Scene Text in Text-VQA. (arXiv:2108.09717v8 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09717">
<div class="article-summary-box-inner">
<span><p>The open-ended question answering task of Text-VQA often requires reading and
reasoning about rarely seen or completely unseen scene-text content of an
image. We address this zero-shot nature of the problem by proposing the
generalized use of external knowledge to augment our understanding of the scene
text. We design a framework to extract, validate, and reason with knowledge
using a standard multimodal transformer for vision language understanding
tasks. Through empirical evidence and qualitative results, we demonstrate how
external knowledge can highlight instance-only cues and thus help deal with
training data bias, improve answer entity type correctness, and detect
multiword named entities. We generate results comparable to the
state-of-the-art on three publicly available datasets, under the constraints of
similar upstream OCR systems and training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Combining Diverse Feature Priors. (arXiv:2110.08220v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08220">
<div class="article-summary-box-inner">
<span><p>To improve model generalization, model designers often restrict the features
that their models use, either implicitly or explicitly. In this work, we
explore the design space of leveraging such feature priors by viewing them as
distinct perspectives on the data. Specifically, we find that models trained
with diverse sets of feature priors have less overlapping failure modes, and
can thus be combined more effectively. Moreover, we demonstrate that jointly
training such models on additional (unlabeled) data allows them to correct each
other's mistakes, which, in turn, leads to better generalization and resilience
to spurious correlations. Code available at
https://github.com/MadryLab/copriors
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Batch Norm Initialization. (arXiv:2110.13989v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.13989">
<div class="article-summary-box-inner">
<span><p>Batch normalization (BN) is comprised of a normalization component followed
by an affine transformation and has become essential for training deep neural
networks. Standard initialization of each BN in a network sets the affine
transformation scale and shift to 1 and 0, respectively. However, after
training we have observed that these parameters do not alter much from their
initialization. Furthermore, we have noticed that the normalization process can
still yield overly large values, which is undesirable for training. We revisit
the BN formulation and present a new initialization method and update approach
for BN to address the aforementioned issues. Experiments are designed to
emphasize and demonstrate the positive influence of proper BN scale
initialization on performance, and use rigorous statistical significance tests
for evaluation. The approach can be used with existing implementations at no
additional computational cost. Source code is available at
https://github.com/osu-cvl/revisiting-bn-init.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deformable ProtoPNet: An Interpretable Image Classifier Using Deformable Prototypes. (arXiv:2111.15000v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.15000">
<div class="article-summary-box-inner">
<span><p>We present a deformable prototypical part network (Deformable ProtoPNet), an
interpretable image classifier that integrates the power of deep learning and
the interpretability of case-based reasoning. This model classifies input
images by comparing them with prototypes learned during training, yielding
explanations in the form of "this looks like that." However, while previous
methods use spatially rigid prototypes, we address this shortcoming by
proposing spatially flexible prototypes. Each prototype is made up of several
prototypical parts that adaptively change their relative spatial positions
depending on the input image. Consequently, a Deformable ProtoPNet can
explicitly capture pose variations and context, improving both model accuracy
and the richness of explanations provided. Compared to other case-based
interpretable models using prototypes, our approach achieves state-of-the-art
accuracy and gives an explanation with greater context. The code is available
at https://github.com/jdonnelly36/Deformable-ProtoPNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompting Visual-Language Models for Efficient Video Understanding. (arXiv:2112.04478v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04478">
<div class="article-summary-box-inner">
<span><p>Image-based visual-language (I-VL) pre-training has shown great success for
learning joint visual-textual representations from large-scale web data,
revealing remarkable ability for zero-shot generalisation. This paper presents
a simple but strong baseline to efficiently adapt the pre-trained I-VL model,
and exploit its powerful ability for resource-hungry video understanding tasks,
with minimal training. Specifically, we propose to optimise a few random
vectors, termed as continuous prompt vectors, that convert video-related tasks
into the same format as the pre-training objectives. In addition, to bridge the
gap between static images and videos, temporal information is encoded with
lightweight Transformers stacking on top of frame-wise visual features.
Experimentally, we conduct extensive ablation studies to analyse the critical
components. On 10 public benchmarks of action recognition, action localisation,
and text-video retrieval, across closed-set, few-shot, and zero-shot scenarios,
we achieve competitive or state-of-the-art performance to existing methods,
despite optimising significantly fewer parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HODOR: High-level Object Descriptors for Object Re-segmentation in Video Learned from Static Images. (arXiv:2112.09131v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09131">
<div class="article-summary-box-inner">
<span><p>Existing state-of-the-art methods for Video Object Segmentation (VOS) learn
low-level pixel-to-pixel correspondences between frames to propagate object
masks across video. This requires a large amount of densely annotated video
data, which is costly to annotate, and largely redundant since frames within a
video are highly correlated. In light of this, we propose HODOR: a novel method
that tackles VOS by effectively leveraging annotated static images for
understanding object appearance and scene context. We encode object instances
and scene information from an image frame into robust high-level descriptors
which can then be used to re-segment those objects in different frames. As a
result, HODOR achieves state-of-the-art performance on the DAVIS and
YouTube-VOS benchmarks compared to existing methods trained without video
annotations. Without any architectural modification, HODOR can also learn from
video context around single annotated video frames by utilizing cyclic
consistency, whereas other methods rely on dense, temporally consistent
annotations. Source code is available at: https://github.com/Ali2500/HODOR
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Out-of-distribution Detection with Boundary Aware Learning. (arXiv:2112.11648v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11648">
<div class="article-summary-box-inner">
<span><p>There is an increasing need to determine whether inputs are
out-of-distribution (\emph{OOD}) for safely deploying machine learning models
in the open world scenario. Typical neural classifiers are based on the closed
world assumption, where the training data and the test data are drawn
\emph{i.i.d.} from the same distribution, and as a result, give over-confident
predictions even faced with \emph{OOD} inputs. For tackling this problem,
previous studies either use real outliers for training or generate synthetic
\emph{OOD} data under strong assumptions, which are either costly or
intractable to generalize. In this paper, we propose boundary aware learning
(\textbf{BAL}), a novel framework that can learn the distribution of \emph{OOD}
features adaptively. The key idea of BAL is to generate \emph{OOD} features
from trivial to hard progressively with a generator, meanwhile, a discriminator
is trained for distinguishing these synthetic \emph{OOD} features and
in-distribution (\emph{ID}) features. Benefiting from the adversarial training
scheme, the discriminator can well separate \emph{ID} and \emph{OOD} features,
allowing more robust \emph{OOD} detection. The proposed BAL achieves
\emph{state-of-the-art} performance on classification benchmarks, reducing up
to 13.9\% FPR95 compared with previous methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recur, Attend or Convolve? On Whether Temporal Modeling Matters for Cross-Domain Robustness in Action Recognition. (arXiv:2112.12175v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.12175">
<div class="article-summary-box-inner">
<span><p>Most action recognition models today are highly parameterized, and evaluated
on datasets with predominantly spatially distinct classes. It has also been
shown that 2D Convolutional Neural Networks (CNNs) tend to be biased toward
texture rather than shape in still image recognition tasks. Taken together,
this raises suspicion that large video models partly learn spurious
correlations rather than to track relevant shapes over time to infer
generalizable semantics from their movement. A natural way to avoid parameter
explosion when learning visual patterns over time is to make use of recurrence.
In this article, we empirically study whether the choice of low-level temporal
modeling has consequences for texture bias and cross-domain robustness. In
order to enable a light-weight and systematic assessment of the ability to
capture temporal structure, not revealed from single frames, we provide the
Temporal Shape (TS) dataset, as well as modified domains of Diving48 allowing
for the investigation of texture bias for video models. We find that across a
variety of model sizes, convolutional-recurrent and attention-based models show
better out-of-domain robustness on TS than 3D CNNs. In domain shift experiments
on Diving48, our experiments indicate that 3D CNNs and attention-based models
exhibit more texture bias than convolutional-recurrent models. Moreover,
qualitative examples suggest that convolutional-recurrent models learn more
correct class attributes from the diving data when compared to the other two
types of models at the same global validation performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Self-Supervised Audio-Visual Speech Recognition. (arXiv:2201.01763v3 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01763">
<div class="article-summary-box-inner">
<span><p>Audio-based automatic speech recognition (ASR) degrades significantly in
noisy environments and is particularly vulnerable to interfering speech, as the
model cannot determine which speaker to transcribe. Audio-visual speech
recognition (AVSR) systems improve robustness by complementing the audio stream
with the visual information that is invariant to noise and helps the model
focus on the desired speaker. However, previous AVSR work focused solely on the
supervised learning setup; hence the progress was hindered by the amount of
labeled data available. In this work, we present a self-supervised AVSR
framework built upon Audio-Visual HuBERT (AV-HuBERT), a state-of-the-art
audio-visual speech representation learning model. On the largest available
AVSR benchmark dataset LRS3, our approach outperforms prior state-of-the-art by
~50% (28.0% vs. 14.1%) using less than 10% of labeled data (433hr vs. 30hr) in
the presence of babble noise, while reducing the WER of an audio-based model by
over 75% (25.8% vs. 5.8%) on average.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are We Ready for Robust and Resilient SLAM? A Framework For Quantitative Characterization of SLAM Datasets. (arXiv:2202.11312v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11312">
<div class="article-summary-box-inner">
<span><p>Reliability of SLAM systems is considered one of the critical requirements in
modern autonomous systems. This directed the efforts to developing many
state-of-the-art systems, creating challenging datasets, and introducing
rigorous metrics to measure SLAM performance. However, the link between
datasets and performance in the robustness/resilience context has rarely been
explored. In order to fill this void, characterization of the operating
conditions of SLAM systems is essential in order to provide an environment for
quantitative measurement of robustness and resilience. In this paper, we argue
that for proper evaluation of SLAM performance, the characterization of SLAM
datasets serves as a critical first step. The study starts by reviewing
previous efforts for quantitative characterization of SLAM datasets. Then, the
problem of perturbation characterization is discussed and the linkage to SLAM
robustness/resilience is established. After that, we propose a novel, generic
and extendable framework for quantitative analysis and comparison of SLAM
datasets. Additionally, a description of different characterization parameters
is provided. Finally, we demonstrate the application of our framework by
presenting the characterization results of three SLAM datasets: KITTI,
EuroC-MAV, and TUM-VI highlighting the level of insights achieved by the
proposed framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PanoFlow: Learning 360{\deg} Optical Flow for Surrounding Temporal Understanding. (arXiv:2202.13388v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13388">
<div class="article-summary-box-inner">
<span><p>Optical flow estimation is a basic task in self-driving and robotics systems,
which enables to temporally interpret traffic scenes. Autonomous vehicles
clearly benefit from the ultra-wide Field of View (FoV) offered by 360{\deg}
panoramic sensors. However, due to the unique imaging process of panoramic
cameras, models designed for pinhole images do not directly generalize
satisfactorily to 360{\deg} panoramic images. In this paper, we put forward a
novel network framework--PanoFlow, to learn optical flow for panoramic images.
To overcome the distortions introduced by equirectangular projection in
panoramic transformation, we design a Flow Distortion Augmentation (FDA)
method, which contains radial flow distortion (FDA-R) or equirectangular flow
distortion (FDA-E). We further look into the definition and properties of
cyclic optical flow for panoramic videos, and hereby propose a Cyclic Flow
Estimation (CFE) method by leveraging the cyclicity of spherical images to
infer 360{\deg} optical flow and converting large displacement to relatively
small displacement. PanoFlow is applicable to any existing flow estimation
method and benefits from the progress of narrow-FoV flow estimation. In
addition, we create and release a synthetic panoramic dataset Flow360 based on
CARLA to facilitate training and quantitative analysis. PanoFlow achieves
state-of-the-art performance on the public OmniFlowNet and the established
Flow360 benchmarks. Our proposed approach reduces the End-Point-Error (EPE) on
Flow360 by 27.3%. On OmniFlowNet, PanoFlow achieves an EPE of 3.17 pixels, a
55.5% error reduction from the best published result. We also qualitatively
validate our method via a collection vehicle and a public real-world OmniPhotos
dataset, indicating strong potential and robustness for real-world navigation
applications. Code and dataset are publicly available at
https://github.com/MasterHow/PanoFlow.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Highly Accurate Dichotomous Image Segmentation. (arXiv:2203.03041v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03041">
<div class="article-summary-box-inner">
<span><p>We present a systematic study on a new task called dichotomous image
segmentation (DIS) , which aims to segment highly accurate objects from natural
images. To this end, we collected the first large-scale DIS dataset, called
DIS5K, which contains 5,470 high-resolution (e.g., 2K, 4K or larger) images
covering camouflaged, salient, or meticulous objects in various backgrounds.
DIS is annotated with extremely fine-grained labels. Besides, we introduce a
simple intermediate supervision baseline (IS-Net) using both feature-level and
mask-level guidance for DIS model training. IS-Net outperforms various
cutting-edge baselines on the proposed DIS5K, making it a general self-learned
supervision network that can facilitate future research in DIS. Further, we
design a new metric called human correction efforts (HCE) which approximates
the number of mouse clicking operations required to correct the false positives
and false negatives. HCE is utilized to measure the gap between models and
real-world applications and thus can complement existing metrics. Finally, we
conduct the largest-scale benchmark, evaluating 16 representative segmentation
models, providing a more insightful discussion regarding object complexities,
and showing several potential applications (e.g., background removal, art
design, 3D reconstruction). Hoping these efforts can open up promising
directions for both academic and industries. Project page:
https://xuebinqin.github.io/dis/index.html.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Task Sampling for Few-shot Vision-Language Transfer Learning. (arXiv:2203.04904v3 [cs.MM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04904">
<div class="article-summary-box-inner">
<span><p>Despite achieving state-of-the-art zero-shot performance, existing
vision-language models still fall short of few-shot transfer ability on
domain-specific problems. Classical fine-tuning often fails to prevent highly
expressive models from exploiting spurious correlations. Although
model-agnostic meta-learning (MAML) presents as a natural alternative for
few-shot transfer learning, the expensive computation due to implicit
second-order optimization limits its use on large-scale vision-language models
such as CLIP. While much literature has been devoted to exploring alternative
optimization strategies, we identify another essential aspect towards effective
few-shot transfer learning, task sampling, which is previously only be viewed
as part of data pre-processing in MAML. To show the impact of task sampling, we
propose a simple algorithm, Model-Agnostic Multitask Fine-tuning (MAMF), which
differentiates classical fine-tuning only on uniformly sampling multiple tasks.
Despite its simplicity, we show that MAMF consistently outperforms classical
fine-tuning on five few-shot vision-language classification tasks. We further
show that the effectiveness of the bi-level optimization in MAML is highly
sensitive to the zero-shot performance of a task in the context of few-shot
vision-language classification. The goal of this paper is to provide new
insights on what makes few-shot learning work, and encourage more research into
investigating better task sampling strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CVFNet: Real-time 3D Object Detection by Learning Cross View Features. (arXiv:2203.06585v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06585">
<div class="article-summary-box-inner">
<span><p>In recent years 3D object detection from LiDAR point clouds has made great
progress thanks to the development of deep learning technologies. Although
voxel or point based methods are popular in 3D object detection, they usually
involve time-consuming operations such as 3D convolutions on voxels or ball
query among points, making the resulting network inappropriate for time
critical applications. On the other hand, 2D view-based methods feature high
computing efficiency while usually obtaining inferior performance than the
voxel or point based methods. In this work, we present a real-time view-based
single stage 3D object detector, namely CVFNet to fulfill this task. To
strengthen the cross-view feature learning under the condition of demanding
efficiency, our framework extracts the features of different views and fuses
them in an efficient progressive way. We first propose a novel Point-Range
feature fusion module that deeply integrates point and range view features in
multiple stages. Then, a special Slice Pillar is designed to well maintain the
3D geometry when transforming the obtained deep point-view features into bird's
eye view. To better balance the ratio of samples, a sparse pillar detection
head is presented to focus the detection on the nonempty grids. We conduct
experiments on the popular KITTI and NuScenes benchmark, and state-of-the-art
performances are achieved in terms of both accuracy and speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">STDAN: Deformable Attention Network for Space-Time Video Super-Resolution. (arXiv:2203.06841v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06841">
<div class="article-summary-box-inner">
<span><p>The target of space-time video super-resolution (STVSR) is to increase the
spatial-temporal resolution of low-resolution (LR) and low frame rate (LFR)
videos. Recent approaches based on deep learning have made significant
improvements, but most of them only use two adjacent frames, that is,
short-term features, to synthesize the missing frame embedding, which cannot
fully explore the information flow of consecutive input LR frames. In addition,
existing STVSR models hardly exploit the temporal contexts explicitly to assist
high-resolution (HR) frame reconstruction. To address these issues, in this
paper, we propose a deformable attention network called STDAN for STVSR. First,
we devise a long-short term feature interpolation (LSTFI) module, which is
capable of excavating abundant content from more neighboring input frames for
the interpolation process through a bidirectional RNN structure. Second, we put
forward a spatial-temporal deformable feature aggregation (STDFA) module, in
which spatial and temporal contexts in dynamic video frames are adaptively
captured and aggregated to enhance SR reconstruction. Experimental results on
several datasets demonstrate that our approach outperforms state-of-the-art
STVSR methods. The code is available at
https://github.com/littlewhitesea/STDAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">deepNIR: Datasets for generating synthetic NIR images and improved fruit detection system using deep learning techniques. (arXiv:2203.09091v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09091">
<div class="article-summary-box-inner">
<span><p>This paper presents datasets utilised for synthetic near-infrared (NIR) image
generation and bounding-box level fruit detection systems. It is undeniable
that high-calibre machine learning frameworks such as Tensorflow or Pytorch,
and large-scale ImageNet or COCO datasets with the aid of accelerated GPU
hardware have pushed the limit of machine learning techniques for more than
decades. Among these breakthroughs, a high-quality dataset is one of the
essential building blocks that can lead to success in model generalisation and
the deployment of data-driven deep neural networks. In particular, synthetic
data generation tasks often require more training samples than other supervised
approaches. Therefore, in this paper, we share the NIR+RGB datasets that are
re-processed from two public datasets (i.e., nirscene and SEN12MS) and our
novel NIR+RGB sweet pepper(capsicum) dataset. We quantitatively and
qualitatively demonstrate that these NIR+RGB datasets are sufficient to be used
for synthetic NIR image generation. We achieved Frechet Inception Distance
(FID) of 11.36, 26.53, and 40.15 for nirscene1, SEN12MS, and sweet pepper
datasets respectively. In addition, we release manual annotations of 11 fruit
bounding boxes that can be exported as various formats using cloud service.
Four newly added fruits [blueberry, cherry, kiwi, and wheat] compound 11 novel
bounding box datasets on top of our previous work presented in the deepFruits
project [apple, avocado, capsicum, mango, orange, rockmelon, strawberry]. The
total number of bounding box instances of the dataset is 162k and it is ready
to use from cloud service. For the evaluation of the dataset, Yolov5 single
stage detector is exploited and reported impressive
mean-average-precision,mAP[0.5:0.95] results of[min:0.49, max:0.812]. We hope
these datasets are useful and serve as a baseline for the future studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Implicit Neural Representations for Variable Length Human Motion Generation. (arXiv:2203.13694v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13694">
<div class="article-summary-box-inner">
<span><p>We propose an action-conditional human motion generation method using
variational implicit neural representations (INR). The variational formalism
enables action-conditional distributions of INRs, from which one can easily
sample representations to generate novel human motion sequences. Our method
offers variable-length sequence generation by construction because a part of
INR is optimized for a whole sequence of arbitrary length with temporal
embeddings. In contrast, previous works reported difficulties with modeling
variable-length sequences. We confirm that our method with a Transformer
decoder outperforms all relevant methods on HumanAct12, NTU-RGBD, and UESTC
datasets in terms of realism and diversity of generated motions. Surprisingly,
even our method with an MLP decoder consistently outperforms the
state-of-the-art Transformer-based auto-encoder. In particular, we show that
variable-length motions generated by our method are better than fixed-length
motions generated by the state-of-the-art method in terms of realism and
diversity. Code at https://github.com/PACerv/ImplicitMotion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DFNet: Enhance Absolute Pose Regression with Direct Feature Matching. (arXiv:2204.00559v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00559">
<div class="article-summary-box-inner">
<span><p>We introduce a camera relocalization pipeline that combines absolute pose
regression (APR) and direct feature matching. By incorporating
exposure-adaptive novel view synthesis, our method successfully addresses
photometric distortions in outdoor environments that existing photometric-based
methods fail to handle. With domain-invariant feature matching, our solution
improves pose regression accuracy using semi-supervised learning on unlabeled
data. In particular, the pipeline consists of two components: Novel View
Synthesizer and DFNet. The former synthesizes novel views compensating for
changes in exposure and the latter regresses camera poses and extracts robust
features that close the domain gap between real images and synthetic ones.
Furthermore, we introduce an online synthetic data generation scheme. We show
that these approaches effectively enhance camera pose estimation both in indoor
and outdoor scenes. Hence, our method achieves a state-of-the-art accuracy by
outperforming existing single-image APR methods by as much as 56%, comparable
to 3D structure-based methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Token Fusion for Vision Transformers. (arXiv:2204.08721v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08721">
<div class="article-summary-box-inner">
<span><p>Many adaptations of transformers have emerged to address the single-modal
vision tasks, where self-attention modules are stacked to handle input sources
like images. Intuitively, feeding multiple modalities of data to vision
transformers could improve the performance, yet the inner-modal attentive
weights may also be diluted, which could thus undermine the final performance.
In this paper, we propose a multimodal token fusion method (TokenFusion),
tailored for transformer-based vision tasks. To effectively fuse multiple
modalities, TokenFusion dynamically detects uninformative tokens and
substitutes these tokens with projected and aggregated inter-modal features.
Residual positional alignment is also adopted to enable explicit utilization of
the inter-modal alignments after fusion. The design of TokenFusion allows the
transformer to learn correlations among multimodal features, while the
single-modal transformer architecture remains largely intact. Extensive
experiments are conducted on a variety of homogeneous and heterogeneous
modalities and demonstrate that TokenFusion surpasses state-of-the-art methods
in three typical vision tasks: multimodal image-to-image translation, RGB-depth
semantic segmentation, and 3D object detection with point cloud and images. Our
code is available at https://github.com/yikaiw/TokenFusion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Making the Most of Text Semantics to Improve Biomedical Vision--Language Processing. (arXiv:2204.09817v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09817">
<div class="article-summary-box-inner">
<span><p>Multi-modal data abounds in biomedicine, such as radiology images and
reports. Interpreting this data at scale is essential for improving clinical
care and accelerating clinical research. Biomedical text with its complex
semantics poses additional challenges in vision--language modelling compared to
the general domain, and previous work has used insufficiently adapted models
that lack domain-specific language understanding. In this paper, we show that
principled textual semantic modelling can substantially improve contrastive
learning in self-supervised vision--language processing. We release a language
model that achieves state-of-the-art results in radiology natural language
inference through its improved vocabulary and novel language pretraining
objective leveraging semantics and discourse characteristics in radiology
reports. Further, we propose a self-supervised joint vision--language approach
with a focus on better text modelling. It establishes new state of the art
results on a wide range of publicly available benchmarks, in part by leveraging
our new domain-specific language model. We release a new dataset with
locally-aligned phrase grounding annotations by radiologists to facilitate the
study of complex semantic modelling in biomedical vision--language processing.
A broad evaluation, including on this new dataset, shows that our contrastive
learning approach, aided by textual-semantic modelling, outperforms prior
methods in segmentation tasks, despite only using a global-alignment objective.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attentive Fine-Grained Structured Sparsity for Image Restoration. (arXiv:2204.12266v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12266">
<div class="article-summary-box-inner">
<span><p>Image restoration tasks have witnessed great performance improvement in
recent years by developing large deep models. Despite the outstanding
performance, the heavy computation demanded by the deep models has restricted
the application of image restoration. To lift the restriction, it is required
to reduce the size of the networks while maintaining accuracy. Recently, N:M
structured pruning has appeared as one of the effective and practical pruning
approaches for making the model efficient with the accuracy constraint.
However, it fails to account for different computational complexities and
performance requirements for different layers of an image restoration network.
To further optimize the trade-off between the efficiency and the restoration
accuracy, we propose a novel pruning method that determines the pruning ratio
for N:M structured sparsity at each layer. Extensive experimental results on
super-resolution and deblurring tasks demonstrate the efficacy of our method
which outperforms previous pruning methods significantly. PyTorch
implementation for the proposed methods will be publicly available at
https://github.com/JungHunOh/SLS_CVPR2022.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised learning in non-small cell lung cancer discovers novel morphological clusters linked to patient outcome and molecular phenotypes. (arXiv:2205.01931v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01931">
<div class="article-summary-box-inner">
<span><p>Histopathological images provide the definitive source of cancer diagnosis,
containing information used by pathologists to identify and subclassify
malignant disease, and to guide therapeutic choices. These images contain vast
amounts of information, much of which is currently unavailable to human
interpretation. Supervised deep learning approaches have been powerful for
classification tasks, but they are inherently limited by the cost and quality
of annotations. Therefore, we developed Histomorphological Phenotype Learning,
an unsupervised methodology, which requires no annotations and operates via the
self-discovery of discriminatory image features in small image tiles. Tiles are
grouped into morphologically similar clusters which appear to represent
recurrent modes of tumor growth emerging under natural selection. These
clusters have distinct features which can be identified using orthogonal
methods. Applied to lung cancer tissues, we show that they align closely with
patient outcomes, with histopathologically recognised tumor types and growth
patterns, and with transcriptomic measures of immunophenotype.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distilling Inter-Class Distance for Semantic Segmentation. (arXiv:2205.03650v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03650">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation is widely adopted in semantic segmentation to reduce
the computation cost.The previous knowledge distillation methods for semantic
segmentation focus on pixel-wise feature alignment and intra-class feature
variation distillation, neglecting to transfer the knowledge of the inter-class
distance in the feature space, which is important for semantic segmentation. To
address this issue, we propose an Inter-class Distance Distillation (IDD)
method to transfer the inter-class distance in the feature space from the
teacher network to the student network. Furthermore, semantic segmentation is a
position-dependent task,thus we exploit a position information distillation
module to help the student network encode more position information. Extensive
experiments on three popular datasets: Cityscapes, Pascal VOC and ADE20K show
that our method is helpful to improve the accuracy of semantic segmentation
models and achieves the state-of-the-art performance. E.g. it boosts the
benchmark model("PSPNet+ResNet18") by 7.50% in accuracy on the Cityscapes
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Lip-Based Audio-Visual Speaker Embeddings with AV-HuBERT. (arXiv:2205.07180v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07180">
<div class="article-summary-box-inner">
<span><p>This paper investigates self-supervised pre-training for audio-visual speaker
representation learning where a visual stream showing the speaker's mouth area
is used alongside speech as inputs. Our study focuses on the Audio-Visual
Hidden Unit BERT (AV-HuBERT) approach, a recently developed general-purpose
audio-visual speech pre-training framework. We conducted extensive experiments
probing the effectiveness of pre-training and visual modality. Experimental
results suggest that AV-HuBERT generalizes decently to speaker related
downstream tasks, improving label efficiency by roughly ten fold for both
audio-only and audio-visual speaker verification. We also show that
incorporating visual information, even just the lip area, greatly improves the
performance and noise robustness, reducing EER by 38% in the clean condition
and 75% in noisy conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">blob loss: instance imbalance aware loss functions for semantic segmentation. (arXiv:2205.08209v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08209">
<div class="article-summary-box-inner">
<span><p>Deep convolutional neural networks have proven to be remarkably effective in
semantic segmentation tasks. Most popular loss functions were introduced
targeting improved volumetric scores, such as the Sorensen Dice coefficient. By
design, DSC can tackle class imbalance; however, it does not recognize instance
imbalance within a class. As a result, a large foreground instance can dominate
minor instances and still produce a satisfactory Sorensen Dice coefficient.
Nevertheless, missing out on instances will lead to poor detection performance.
This represents a critical issue in applications such as disease progression
monitoring. For example, it is imperative to locate and surveil small-scale
lesions in the follow-up of multiple sclerosis patients. We propose a novel
family of loss functions, nicknamed blob loss, primarily aimed at maximizing
instance-level detection metrics, such as F1 score and sensitivity. Blob loss
is designed for semantic segmentation problems in which the instances are the
connected components within a class. We extensively evaluate a DSC-based blob
loss in five complex 3D semantic segmentation tasks featuring pronounced
instance heterogeneity in terms of texture and morphology. Compared to soft
Dice loss, we achieve 5 percent improvement for MS lesions, 3 percent
improvement for liver tumor, and an average 2 percent improvement for
Microscopy segmentation tasks considering F1 score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HoVer-Trans: Anatomy-aware HoVer-Transformer for ROI-free Breast Cancer Diagnosis in Ultrasound Images. (arXiv:2205.08390v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08390">
<div class="article-summary-box-inner">
<span><p>Ultrasonography is an important routine examination for breast cancer
diagnosis, due to its non-invasive, radiation-free and low-cost properties.
However, the diagnostic accuracy of breast cancer is still limited due to its
inherent limitations. It would be a tremendous success if we can precisely
diagnose breast cancer by breast ultrasound images (BUS). Many learning-based
computer-aided diagnostic methods have been proposed to achieve breast cancer
diagnosis/lesion classification. However, most of them require a pre-define ROI
and then classify the lesion inside the ROI. Conventional classification
backbones, such as VGG16 and ResNet50, can achieve promising classification
results with no ROI requirement. But these models lack interpretability, thus
restricting their use in clinical practice. In this study, we propose a novel
ROI-free model for breast cancer diagnosis in ultrasound images with
interpretable feature representations. We leverage the anatomical prior
knowledge that malignant and benign tumors have different spatial relationships
between different tissue layers, and propose a HoVer-Transformer to formulate
this prior knowledge. The proposed HoVer-Trans block extracts the inter- and
intra-layer spatial information horizontally and vertically. We conduct and
release an open dataset GDPH&amp;SYSUCC for breast cancer diagnosis in BUS. The
proposed model is evaluated in three datasets by comparing with four CNN-based
models and two vision transformer models via five-fold cross validation. It
achieves state-of-the-art classification performance with the best model
interpretability. In the meanwhile, our proposed model outperforms two senior
sonographers on the breast cancer diagnosis when only one BUS image is given.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SALAD: Source-free Active Label-Agnostic Domain Adaptation for Classification, Segmentation and Detection. (arXiv:2205.12840v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12840">
<div class="article-summary-box-inner">
<span><p>We present a novel method, SALAD, for the challenging vision task of adapting
a pre-trained "source" domain network to a "target" domain, with a small budget
for annotation in the "target" domain and a shift in the label space. Further,
the task assumes that the source data is not available for adaptation, due to
privacy concerns or otherwise. We postulate that such systems need to jointly
optimize the dual task of (i) selecting fixed number of samples from the target
domain for annotation and (ii) transfer of knowledge from the pre-trained
network to the target domain. To do this, SALAD consists of a novel Guided
Attention Transfer Network (GATN) and an active learning function, HAL. The
GATN enables feature distillation from pre-trained network to the target
network, complemented with the target samples mined by HAL using
transfer-ability and uncertainty criteria. SALAD has three key benefits: (i) it
is task-agnostic, and can be applied across various visual tasks such as
classification, segmentation and detection; (ii) it can handle shifts in output
label space from the pre-trained source network to the target domain; (iii) it
does not require access to source data for adaptation. We conduct extensive
experiments across 3 visual tasks, viz. digits classification (MNIST, SVHN,
VISDA), synthetic (GTA5) to real (CityScapes) image segmentation, and document
layout detection (PubLayNet to DSSE). We show that our source-free approach,
SALAD, results in an improvement of 0.5%-31.3%(across datasets and tasks) over
prior adaptation methods that assume access to large amounts of annotated
source data for adaptation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ATDN vSLAM: An all-through Deep Learning-Based Solution for Visual Simultaneous Localization and Mapping. (arXiv:2206.05963v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05963">
<div class="article-summary-box-inner">
<span><p>In this paper, a novel solution is introduced for visual Simultaneous
Localization and Mapping (vSLAM) that is built up of Deep Learning components.
The proposed architecture is a highly modular framework in which each component
offers state of the art results in their respective fields of vision-based deep
learning solutions. The paper shows that with the synergic integration of these
individual building blocks, a functioning and efficient all-through deep neural
(ATDN) vSLAM system can be created. The Embedding Distance Loss function is
introduced and using it the ATDN architecture is trained. The resulting system
managed to achieve 4.4% translation and 0.0176 deg/m rotational error on a
subset of the KITTI dataset. The proposed architecture can be used for
efficient and low-latency autonomous driving (AD) aiding database creation as
well as a basis for autonomous vehicle (AV) control.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HyperRes: Efficient Hypernetwork-Based Continuous Image Restoration. (arXiv:2206.05970v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05970">
<div class="article-summary-box-inner">
<span><p>Continuous image restoration attempts to provide a model that can restore
images with unseen degradation levels during training at inference time.
Existing methods are limited in terms of either the accuracy of the
restoration, the range of degradation levels they can support, or the size of
the model they require. We introduce a novel approach that achieves the optimal
accuracy of multiple dedicated models for a wide range of degradation levels
with the same number of parameters as a single base model. We present a
hypernetwork that can efficiently generate an image restoration network to best
adapt to the required level of degradation. Experiments on popular datasets
show that our approach outperforms the state-of-the-art for a variety of image
restoration tasks, including denoising, DeJPEG, and super-resolution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Embodied Scene-aware Human Pose Estimation. (arXiv:2206.09106v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.09106">
<div class="article-summary-box-inner">
<span><p>We propose embodied scene-aware human pose estimation where we estimate 3D
poses based on a simulated agent's proprioception and scene awareness, along
with external third-person observations. Unlike prior methods that often resort
to multistage optimization, non-causal inference, and complex contact modeling
to estimate human pose and human scene interactions, our method is one stage,
causal, and recovers global 3D human poses in a simulated environment. Since 2D
third-person observations are coupled with the camera pose, we propose to
disentangle the camera pose and use a multi-step projection gradient defined in
the global coordinate frame as the movement cue for our embodied agent.
Leveraging a physics simulation and prescanned scenes (e.g., 3D mesh), we
simulate our agent in everyday environments (libraries, offices, bedrooms,
etc.) and equip our agent with environmental sensors to intelligently navigate
and interact with scene geometries. Our method also relies only on 2D keypoints
and can be trained on synthetic datasets derived from popular human motion
databases. To evaluate, we use the popular H36M and PROX datasets and, for the
first time, achieve a success rate of 96.7% on the challenging PROX dataset
without ever using PROX motion sequences for training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Demystifying the Adversarial Robustness of Random Transformation Defenses. (arXiv:2207.03574v2 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03574">
<div class="article-summary-box-inner">
<span><p>Neural networks' lack of robustness against attacks raises concerns in
security-sensitive settings such as autonomous vehicles. While many
countermeasures may look promising, only a few withstand rigorous evaluation.
Defenses using random transformations (RT) have shown impressive results,
particularly BaRT (Raff et al., 2019) on ImageNet. However, this type of
defense has not been rigorously evaluated, leaving its robustness properties
poorly understood. Their stochastic properties make evaluation more challenging
and render many proposed attacks on deterministic models inapplicable. First,
we show that the BPDA attack (Athalye et al., 2018a) used in BaRT's evaluation
is ineffective and likely overestimates its robustness. We then attempt to
construct the strongest possible RT defense through the informed selection of
transformations and Bayesian optimization for tuning their parameters.
Furthermore, we create the strongest possible attack to evaluate our RT
defense. Our new attack vastly outperforms the baseline, reducing the accuracy
by 83% compared to the 19% reduction by the commonly used EoT attack
($4.3\times$ improvement). Our result indicates that the RT defense on the
Imagenette dataset (a ten-class subset of ImageNet) is not robust against
adversarial examples. Extending the study further, we use our new attack to
adversarially train RT defense (called AdvRT), resulting in a large robustness
gain. Code is available at
https://github.com/wagner-group/demystify-random-transform.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Human Vision Inspired Action Recognition using Adaptive Spatiotemporal Sampling. (arXiv:2207.05249v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05249">
<div class="article-summary-box-inner">
<span><p>Adaptive sampling that exploits the spatiotemporal redundancy in videos is
critical for always-on action recognition on wearable devices with limited
computing and battery resources. The commonly used fixed sampling strategy is
not context-aware and may under-sample the visual content, and thus adversely
impacts both computation efficiency and accuracy. Inspired by the concepts of
foveal vision and pre-attentive processing from the human visual perception
mechanism, we introduce a novel adaptive spatiotemporal sampling scheme for
efficient action recognition. Our system pre-scans the global scene context at
low-resolution and decides to skip or request high-resolution features at
salient regions for further processing. We validate the system on EPIC-KITCHENS
and UCF-101 datasets for action recognition, and show that our proposed
approach can greatly speed up inference with a tolerable loss of accuracy
compared with those from state-of-the-art baselines. Source code is available
in https://github.com/knmac/adaptive_spatiotemporal.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controllable Shadow Generation Using Pixel Height Maps. (arXiv:2207.05385v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05385">
<div class="article-summary-box-inner">
<span><p>Shadows are essential for realistic image compositing. Physics-based shadow
rendering methods require 3D geometries, which are not always available. Deep
learning-based shadow synthesis methods learn a mapping from the light
information to an object's shadow without explicitly modeling the shadow
geometry. Still, they lack control and are prone to visual artifacts. We
introduce pixel heigh, a novel geometry representation that encodes the
correlations between objects, ground, and camera pose. The pixel height can be
calculated from 3D geometries, manually annotated on 2D images, and can also be
predicted from a single-view RGB image by a supervised approach. It can be used
to calculate hard shadows in a 2D image based on the projective geometry,
providing precise control of the shadows' direction and shape. Furthermore, we
propose a data-driven soft shadow generator to apply softness to a hard shadow
based on a softness input parameter. Qualitative and quantitative evaluations
demonstrate that the proposed pixel height significantly improves the quality
of the shadow generation while allowing for controllability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tracking Objects as Pixel-wise Distributions. (arXiv:2207.05518v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05518">
<div class="article-summary-box-inner">
<span><p>Multi-object tracking (MOT) requires detecting and associating objects
through frames. Unlike tracking via detected bounding boxes or tracking objects
as points, we propose tracking objects as pixel-wise distributions. We
instantiate this idea on a transformer-based architecture, P3AFormer, with
pixel-wise propagation, prediction, and association. P3AFormer propagates
pixel-wise features guided by flow information to pass messages between frames.
Furthermore, P3AFormer adopts a meta-architecture to produce multi-scale object
feature maps. During inference, a pixel-wise association procedure is proposed
to recover object connections through frames based on the pixel-wise
prediction. P3AFormer yields 81.2\% in terms of MOTA on the MOT17 benchmark --
the first among all transformer networks to reach 80\% MOTA in literature.
P3AFormer also outperforms state-of-the-arts on the MOT20 and KITTI benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Low-Resolution Distillation for Cost-Efficient End-to-End Text Spotting. (arXiv:2207.06694v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06694">
<div class="article-summary-box-inner">
<span><p>End-to-end text spotting has attached great attention recently due to its
benefits on global optimization and high maintainability for real applications.
However, the input scale has always been a tough trade-off since recognizing a
small text instance usually requires enlarging the whole image, which brings
high computational costs. In this paper, to address this problem, we propose a
novel cost-efficient Dynamic Low-resolution Distillation (DLD) text spotting
framework, which aims to infer images in different small but recognizable
resolutions and achieve a better balance between accuracy and efficiency.
Concretely, we adopt a resolution selector to dynamically decide the input
resolutions for different images, which is constraint by both inference
accuracy and computational cost. Another sequential knowledge distillation
strategy is conducted on the text recognition branch, making the low-res input
obtains comparable performance to a high-res image. The proposed method can be
optimized end-to-end and adopted in any current text spotting framework to
improve the practicability. Extensive experiments on several text spotting
benchmarks show that the proposed method vastly improves the usability of
low-res models. The code is available at
https://github.com/hikopensource/DAVAR-Lab-OCR/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">iColoriT: Towards Propagating Local Hint to the Right Region in Interactive Colorization by Leveraging Vision Transformer. (arXiv:2207.06831v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06831">
<div class="article-summary-box-inner">
<span><p>Point-interactive image colorization aims to colorize grayscale images when a
user provides the colors for specific locations. It is essential for
point-interactive colorization methods to appropriately propagate user-provided
colors (i.e., user hints) in the entire image to obtain a reasonably colorized
image with minimal user effort. However, existing approaches often produce
partially colorized results due to the inefficient design of stacking
convolutional layers to propagate hints to distant relevant regions. To address
this problem, we present iColoriT, a novel point-interactive colorization
Vision Transformer capable of propagating user hints to relevant regions,
leveraging the global receptive field of Transformers. The self-attention
mechanism of Transformers enables iColoriT to selectively colorize relevant
regions with only a few local hints. Our approach colorizes images in real-time
by utilizing pixel shuffling, an efficient upsampling technique that replaces
the decoder architecture. Also, in order to mitigate the artifacts caused by
pixel shuffling with large upsampling ratios, we present the local stabilizing
layer. Extensive quantitative and qualitative results demonstrate that our
approach highly outperforms existing methods for point-interactive
colorization, producing accurately colorized images with a user's minimal
effort.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Benchmarking Omni-Vision Representation through the Lens of Visual Realms. (arXiv:2207.07106v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07106">
<div class="article-summary-box-inner">
<span><p>Though impressive performance has been achieved in specific visual realms
(e.g. faces, dogs, and places), an omni-vision representation generalizing to
many natural visual domains is highly desirable. But, existing benchmarks are
biased and inefficient to evaluate the omni-vision representation -- these
benchmarks either only include several specific realms, or cover most realms at
the expense of subsuming numerous datasets that have extensive realm
overlapping. In this paper, we propose Omni-Realm Benchmark (OmniBenchmark). It
includes 21 realm-wise datasets with 7,372 concepts and 1,074,346 images.
Without semantic overlapping, these datasets cover most visual realms
comprehensively and meanwhile efficiently. In addition, we propose a new
supervised contrastive learning framework, namely Relational Contrastive
learning (ReCo), for a better omni-vision representation. Beyond pulling two
instances from the same concept closer -- the typical supervised contrastive
learning framework -- ReCo also pulls two instances from the same semantic
realm closer, encoding the semantic relation between concepts, and facilitating
omni-vision representation learning. We benchmark ReCo and other advances in
omni-vision representation studies that are different in architectures (from
CNNs to transformers) and in learning paradigms (from supervised learning to
self-supervised learning) on OmniBenchmark. We illustrate the superior of ReCo
to other supervised contrastive learning methods and reveal multiple practical
observations to facilitate future research.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-07-18 23:10:35.576728730 UTC">2022-07-18 23:10:35 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>