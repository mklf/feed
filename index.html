<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-06-17T01:30:00Z">06-17</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">How Adults Understand What Young Children Say. (arXiv:2206.07807v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07807">
<div class="article-summary-box-inner">
<span><p>Children's early speech often bears little resemblance to adult speech in
form or content, and yet caregivers often find meaning in young children's
utterances. Precisely how caregivers are able to do this remains poorly
understood. We propose that successful early communication (an essential
building block of language development) relies not just on children's growing
linguistic knowledge, but also on adults' sophisticated inferences. These
inferences, we further propose, are optimized for fine-grained details of how
children speak. We evaluate these ideas using a set of candidate computational
models of spoken word recognition based on deep learning and Bayesian
inference, which instantiate competing hypotheses regarding the information
sources used by adults to understand children. We find that the best-performing
models (evaluated on datasets of adult interpretations of child speech) are
those that have strong prior expectations about what children are likely to
want to communicate, rather than the actual phonetic contents of what children
say. We further find that adults' behavior is best characterized as well-tuned
to specific children: the more closely a word recognition model is tuned to the
particulars of an individual child's actual linguistic behavior, the better it
predicts adults' inferences about what the child has said. These results offer
a comprehensive investigation into the role of caregivers as child-directed
listeners, with broader consequences for theories of language acquisition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Alexa Teacher Model: Pretraining and Distilling Multi-Billion-Parameter Encoders for Natural Language Understanding Systems. (arXiv:2206.07808v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07808">
<div class="article-summary-box-inner">
<span><p>We present results from a large-scale experiment on pretraining encoders with
non-embedding parameter counts ranging from 700M to 9.3B, their subsequent
distillation into smaller models ranging from 17M-170M parameters, and their
application to the Natural Language Understanding (NLU) component of a virtual
assistant system. Though we train using 70% spoken-form data, our teacher
models perform comparably to XLM-R and mT5 when evaluated on the written-form
Cross-lingual Natural Language Inference (XNLI) corpus. We perform a second
stage of pretraining on our teacher models using in-domain data from our
system, improving error rates by 3.86% relative for intent classification and
7.01% relative for slot filling. We find that even a 170M-parameter model
distilled from our Stage 2 teacher model has 2.88% better intent classification
and 7.69% better slot filling error rates when compared to the 2.3B-parameter
teacher trained only on public data (Stage 1), emphasizing the importance of
in-domain data for pretraining. When evaluated offline using labeled NLU data,
our 17M-parameter Stage 2 distilled model outperforms both XLM-R Base (85M
params) and DistillBERT (42M params) by 4.23% to 6.14%, respectively. Finally,
we present results from a full virtual assistant experimentation platform,
where we find that models trained using our pretraining and distillation
pipeline outperform models distilled from 85M-parameter teachers by 3.74%-4.91%
on an automatic measurement of full-system user dissatisfaction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Personal Entity, Concept, and Named Entity Linking in Conversations. (arXiv:2206.07836v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07836">
<div class="article-summary-box-inner">
<span><p>Building conversational agents that can have natural and knowledge-grounded
interactions with humans requires understanding user utterances. Entity Linking
(EL) is an effective and widely used method for understanding natural language
text and connecting it to external knowledge. It is, however, shown that
existing EL methods developed for annotating documents are suboptimal for
conversations, where personal entities (e.g., "my cars") and concepts are
essential for understanding user utterances. In this paper, we introduce a
collection and a tool for entity linking in conversations. We collect EL
annotations for 1327 conversational utterances, consisting of links to named
entities, concepts, and personal entities. The dataset is used for training our
toolkit for conversational entity linking, CREL. Unlike existing EL methods,
CREL is developed to identify both named entities and concepts. It also
utilizes coreference resolution techniques to identify personal entities and
references to the explicit entity mentions in the conversations. We compare
CREL with state-of-the-art techniques and show that it outperforms all existing
baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TOKEN is a MASK: Few-shot Named Entity Recognition with Pre-trained Language Models. (arXiv:2206.07841v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07841">
<div class="article-summary-box-inner">
<span><p>Transferring knowledge from one domain to another is of practical importance
for many tasks in natural language processing, especially when the amount of
available data in the target domain is limited. In this work, we propose a
novel few-shot approach to domain adaptation in the context of Named Entity
Recognition (NER). We propose a two-step approach consisting of a variable base
module and a template module that leverages the knowledge captured in
pre-trained language models with the help of simple descriptive patterns. Our
approach is simple yet versatile and can be applied in few-shot and zero-shot
settings. Evaluating our lightweight approach across a number of different
datasets shows that it can boost the performance of state-of-the-art baselines
by 2-5% F1-score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text normalization for endangered languages: the case of Ligurian. (arXiv:2206.07861v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07861">
<div class="article-summary-box-inner">
<span><p>Text normalization is a crucial technology for low-resource languages which
lack rigid spelling conventions. Low-resource text normalization has so far
relied upon hand-crafted rules, which are perceived to be more data efficient
than neural methods.
</p>
<p>In this paper we examine the case of text normalization for Ligurian, an
endangered Romance language. We collect 4,394 Ligurian sentences paired with
their normalized versions, as well as the first monolingual corpus for
Ligurian. We show that, in spite of the small amounts of data available, a
compact transformer-based model can be trained to achieve very low error rates
by the use of backtranslation and appropriate tokenization. Our datasets are
released to the public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accelerating Inference and Language Model Fusion of Recurrent Neural Network Transducers via End-to-End 4-bit Quantization. (arXiv:2206.07882v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07882">
<div class="article-summary-box-inner">
<span><p>We report on aggressive quantization strategies that greatly accelerate
inference of Recurrent Neural Network Transducers (RNN-T). We use a 4 bit
integer representation for both weights and activations and apply Quantization
Aware Training (QAT) to retrain the full model (acoustic encoder and language
model) and achieve near-iso-accuracy. We show that customized quantization
schemes that are tailored to the local properties of the network are essential
to achieve good performance while limiting the computational overhead of QAT.
</p>
<p>Density ratio Language Model fusion has shown remarkable accuracy gains on
RNN-T workloads but it severely increases the computational cost of inference.
We show that our quantization strategies enable using large beam widths for
hypothesis search while achieving streaming-compatible runtimes and a full
model compression ratio of 7.6$\times$ compared to the full precision model.
</p>
<p>Via hardware simulations, we estimate a 3.4$\times$ acceleration from FP16 to
INT4 for the end-to-end quantized RNN-T inclusive of LM fusion, resulting in a
Real Time Factor (RTF) of 0.06. On the NIST Hub5 2000, Hub5 2001, and RT-03
test sets, we retain most of the gains associated with LM fusion, improving the
average WER by $&gt;$1.5%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Dialogue State Tracking. (arXiv:2206.07898v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07898">
<div class="article-summary-box-inner">
<span><p>Designed for tracking user goals in dialogues, a dialogue state tracker is an
essential component in a dialogue system. However, the research of dialogue
state tracking has largely been limited to unimodality, in which slots and slot
values are limited by knowledge domains (e.g. restaurant domain with slots of
restaurant name and price range) and are defined by specific database schema.
In this paper, we propose to extend the definition of dialogue state tracking
to multimodality. Specifically, we introduce a novel dialogue state tracking
task to track the information of visual objects that are mentioned in
video-grounded dialogues. Each new dialogue utterance may introduce a new video
segment, new visual objects, or new object attributes, and a state tracker is
required to update these information slots accordingly. We created a new
synthetic benchmark and designed a novel baseline, Video-Dialogue Transformer
Network (VDTN), for this task. VDTN combines both object-level features and
segment-level features and learns contextual dependencies between videos and
dialogues to generate multimodal dialogue states. We optimized VDTN for a state
generation task as well as a self-supervised video understanding task which
recovers video segment or object representations. Finally, we trained VDTN to
use the decoded states in a response prediction task. Together with
comprehensive ablation and qualitative analysis, we discovered interesting
insights towards building more capable multimodal dialogue systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PInKS: Preconditioned Commonsense Inference with Minimal Supervision. (arXiv:2206.07920v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07920">
<div class="article-summary-box-inner">
<span><p>Reasoning with preconditions such as "glass can be used for drinking water
unless the glass is shattered" remains an open problem for language models. The
main challenge lies in the scarcity of preconditions data and the model's lack
of support for such reasoning. We present PInKS, Preconditioned Commonsense
Inference with WeaK Supervision, an improved model for reasoning with
preconditions through minimum supervision. We show, both empirically and
theoretically, that PInKS improves the results on benchmarks focused on
reasoning with the preconditions of commonsense knowledge (up to 40% Macro-F1
scores). We further investigate PInKS through PAC-Bayesian informativeness
analysis, precision measures, and ablation study.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Prosody Annotation with Pre-Trained Text-Speech Model. (arXiv:2206.07956v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07956">
<div class="article-summary-box-inner">
<span><p>Prosodic boundary plays an important role in text-to-speech synthesis (TTS)
in terms of naturalness and readability. However, the acquisition of prosodic
boundary labels relies on manual annotation, which is costly and
time-consuming. In this paper, we propose to automatically extract prosodic
boundary labels from text-audio data via a neural text-speech model with
pre-trained audio encoders. This model is pre-trained on text and speech data
separately and jointly fine-tuned on TTS data in a triplet format: {speech,
text, prosody}. The experimental results on both automatic evaluation and human
evaluation demonstrate that: 1) the proposed text-speech prosody annotation
framework significantly outperforms text-only baselines; 2) the quality of
automatic prosodic boundary annotations is comparable to human annotations; 3)
TTS systems trained with model-annotated boundaries are slightly better than
systems that use manual ones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Better Understanding with Uniformity and Explicit Regularization of Embeddings in Embedding-based Neural Topic Models. (arXiv:2206.07960v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07960">
<div class="article-summary-box-inner">
<span><p>Embedding-based neural topic models could explicitly represent words and
topics by embedding them to a homogeneous feature space, which shows higher
interpretability. However, there are no explicit constraints for the training
of embeddings, leading to a larger optimization space. Also, a clear
description of the changes in embeddings and the impact on model performance is
still lacking. In this paper, we propose an embedding regularized neural topic
model, which applies the specially designed training constraints on word
embedding and topic embedding to reduce the optimization space of parameters.
To reveal the changes and roles of embeddings, we introduce \textbf{uniformity}
into the embedding-based neural topic model as the evaluation metric of
embedding space. On this basis, we describe how embeddings tend to change
during training via the changes in the uniformity of embeddings. Furthermore,
we demonstrate the impact of changes in embeddings in embedding-based neural
topic models through ablation studies. The results of experiments on two
mainstream datasets indicate that our model significantly outperforms baseline
models in terms of the harmony between topic quality and document modeling.
This work is the first attempt to exploit uniformity to explore changes in
embeddings of embedding-based neural topic models and their impact on model
performance to the best of our knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Global Semantic Similarities in Knowledge Graphs by Relational Prototype Entities. (arXiv:2206.08021v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08021">
<div class="article-summary-box-inner">
<span><p>Knowledge graph (KG) embedding aims at learning the latent representations
for entities and relations of a KG in continuous vector spaces. An empirical
observation is that the head (tail) entities connected by the same relation
often share similar semantic attributes -- specifically, they often belong to
the same category -- no matter how far away they are from each other in the KG;
that is, they share global semantic similarities. However, many existing
methods derive KG embeddings based on the local information, which fail to
effectively capture such global semantic similarities among entities. To
address this challenge, we propose a novel approach, which introduces a set of
virtual nodes called \textit{\textbf{relational prototype entities}} to
represent the prototypes of the head and tail entities connected by the same
relations. By enforcing the entities' embeddings close to their associated
prototypes' embeddings, our approach can effectively encourage the global
semantic similarities of entities -- that can be far away in the KG --
connected by the same relation. Experiments on the entity alignment and KG
completion tasks demonstrate that our approach significantly outperforms recent
state-of-the-arts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DIALOG-22 RuATD Generated Text Detection. (arXiv:2206.08029v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08029">
<div class="article-summary-box-inner">
<span><p>Text Generation Models (TGMs) succeed in creating text that matches human
language style reasonably well. Detectors that can distinguish between
TGM-generated text and human-written ones play an important role in preventing
abuse of TGM.
</p>
<p>In this paper, we describe our pipeline for the two DIALOG-22 RuATD tasks:
detecting generated text (binary task) and classification of which model was
used to generate text (multiclass task). We achieved 1st place on the binary
classification task with an accuracy score of 0.82995 on the private test set
and 4th place on the multiclass classification task with an accuracy score of
0.62856 on the private test set. We proposed an ensemble method of different
pre-trained models based on the attention mechanism.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Acoustic Modeling for End-to-End Empathetic Dialogue Speech Synthesis Using Linguistic and Prosodic Contexts of Dialogue History. (arXiv:2206.08039v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08039">
<div class="article-summary-box-inner">
<span><p>We propose an end-to-end empathetic dialogue speech synthesis (DSS) model
that considers both the linguistic and prosodic contexts of dialogue history.
Empathy is the active attempt by humans to get inside the interlocutor in
dialogue, and empathetic DSS is a technology to implement this act in spoken
dialogue systems. Our model is conditioned by the history of linguistic and
prosody features for predicting appropriate dialogue context. As such, it can
be regarded as an extension of the conventional linguistic-feature-based
dialogue history modeling. To train the empathetic DSS model effectively, we
investigate 1) a self-supervised learning model pretrained with large speech
corpora, 2) a style-guided training using a prosody embedding of the current
utterance to be predicted by the dialogue context embedding, 3) a cross-modal
attention to combine text and speech modalities, and 4) a sentence-wise
embedding to achieve fine-grained prosody modeling rather than utterance-wise
modeling. The evaluation results demonstrate that 1) simply considering
prosodic contexts of the dialogue history does not improve the quality of
speech in empathetic DSS and 2) introducing style-guided training and
sentence-wise embedding modeling achieves higher speech quality than that by
the conventional method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Open-Domain QA System for e-Governance. (arXiv:2206.08046v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08046">
<div class="article-summary-box-inner">
<span><p>The paper presents an open-domain Question Answering system for Romanian,
answering COVID-19 related questions. The QA system pipeline involves automatic
question processing, automatic query generation, web searching for the top 10
most relevant documents and answer extraction using a fine-tuned BERT model for
Extractive QA, trained on a COVID-19 data set that we have manually created.
The paper will present the QA system and its integration with the Romanian
language technologies portal RELATE, the COVID-19 data set and different
evaluations of the QA performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">JU_NLP at HinglishEval: Quality Evaluation of the Low-Resource Code-Mixed Hinglish Text. (arXiv:2206.08053v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08053">
<div class="article-summary-box-inner">
<span><p>In this paper we describe a system submitted to the INLG 2022 Generation
Challenge (GenChal) on Quality Evaluation of the Low-Resource Synthetically
Generated Code-Mixed Hinglish Text. We implement a Bi-LSTM-based neural network
model to predict the Average rating score and Disagreement score of the
synthetic Hinglish dataset. In our models, we used word embeddings for English
and Hindi data, and one hot encodings for Hinglish data. We achieved a F1 score
of 0.11, and mean squared error of 6.0 in the average rating score prediction
task. In the task of Disagreement score prediction, we achieve a F1 score of
0.18, and mean squared error of 5.0.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nonwords Pronunciation Classification in Language Development Tests for Preschool Children. (arXiv:2206.08058v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08058">
<div class="article-summary-box-inner">
<span><p>This work aims to automatically evaluate whether the language development of
children is age-appropriate. Validated speech and language tests are used for
this purpose to test the auditory memory. In this work, the task is to
determine whether spoken nonwords have been uttered correctly. We compare
different approaches that are motivated to model specific language structures:
Low-level features (FFT), speaker embeddings (ECAPA-TDNN), grapheme-motivated
embeddings (wav2vec 2.0), and phonetic embeddings in form of senones (ASR
acoustic model). Each of the approaches provides input for VGG-like 5-layer CNN
classifiers. We also examine the adaptation per nonword. The evaluation of the
proposed systems was performed using recordings from different kindergartens of
spoken nonwords. ECAPA-TDNN and low-level FFT features do not explicitly model
phonetic information; wav2vec2.0 is trained on grapheme labels, our ASR
acoustic model features contain (sub-)phonetic information. We found that the
more granular the phonetic modeling is, the higher are the achieved recognition
rates. The best system trained on ASR acoustic model features with VTLN
achieved an accuracy of 89.4% and an area under the ROC (Receiver Operating
Characteristic) curve (AUC) of 0.923. This corresponds to an improvement in
accuracy of 20.2% and AUC of 0.309 relative compared to the FFT-baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Robust Ranker for Text Retrieval. (arXiv:2206.08063v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08063">
<div class="article-summary-box-inner">
<span><p>A ranker plays an indispensable role in the de facto 'retrieval &amp; rerank'
pipeline, but its training still lags behind -- learning from moderate
negatives or/and serving as an auxiliary module for a retriever. In this work,
we first identify two major barriers to a robust ranker, i.e., inherent label
noises caused by a well-trained retriever and non-ideal negatives sampled for a
high-capable ranker. Thereby, we propose multiple retrievers as negative
generators improve the ranker's robustness, where i) involving extensive
out-of-distribution label noises renders the ranker against each noise
distribution, and ii) diverse hard negatives from a joint distribution are
relatively close to the ranker's negative distribution, leading to more
challenging thus effective training. To evaluate our robust ranker (dubbed
R$^2$anker), we conduct experiments in various settings on the popular passage
retrieval benchmark, including BM25-reranking, full-ranking, retriever
distillation, etc. The empirical results verify the new state-of-the-art
effectiveness of our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransDrift: Modeling Word-Embedding Drift using Transformer. (arXiv:2206.08081v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08081">
<div class="article-summary-box-inner">
<span><p>In modern NLP applications, word embeddings are a crucial backbone that can
be readily shared across a number of tasks. However as the text distributions
change and word semantics evolve over time, the downstream applications using
the embeddings can suffer if the word representations do not conform to the
data drift. Thus, maintaining word embeddings to be consistent with the
underlying data distribution is a key problem. In this work, we tackle this
problem and propose TransDrift, a transformer-based prediction model for word
embeddings. Leveraging the flexibility of transformer, our model accurately
learns the dynamics of the embedding drift and predicts the future embedding.
In experiments, we compare with existing methods and show that our model makes
significantly more accurate predictions of the word embedding than the
baselines. Crucially, by applying the predicted embeddings as a backbone for
downstream classification tasks, we show that our embeddings lead to superior
performance compared to the previous methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Generated In-Context Learning: Leveraging Auto-regressive Language Models as a Demonstration Generator. (arXiv:2206.08082v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08082">
<div class="article-summary-box-inner">
<span><p>Large-scale pre-trained language models (PLMs) are well-known for being
capable of solving a task simply by conditioning a few input-label pairs dubbed
demonstrations on a prompt without being explicitly tuned for the desired
downstream task. Such a process (i.e., in-context learning), however, naturally
leads to high reliance on the demonstrations which are usually selected from
external datasets. In this paper, we propose self-generated in-context learning
(SG-ICL), which generates demonstrations for in-context learning from PLM
itself to minimize the reliance on the external demonstration. We conduct
experiments on four different text classification tasks and show SG-ICL
significantly outperforms zero-shot learning and is generally worth
approximately 0.6 gold training samples. Moreover, our generated demonstrations
show more consistent performance with low variance compared to randomly
selected demonstrations from the training dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Video Question Answering via Frozen Bidirectional Language Models. (arXiv:2206.08155v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08155">
<div class="article-summary-box-inner">
<span><p>Video question answering (VideoQA) is a complex task that requires diverse
multi-modal data for training. Manual annotation of question and answers for
videos, however, is tedious and prohibits scalability. To tackle this problem,
recent methods consider zero-shot settings with no manual annotation of visual
question-answer. In particular, a promising approach adapts frozen
autoregressive language models pretrained on Web-scale text-only data to
multi-modal inputs. In contrast, we here build on frozen bidirectional language
models (BiLM) and show that such an approach provides a stronger and cheaper
alternative for zero-shot VideoQA. In particular, (i) we combine visual inputs
with the frozen BiLM using light trainable modules, (ii) we train such modules
using Web-scraped multi-modal data, and finally (iii) we perform zero-shot
VideoQA inference through masked language modeling, where the masked text is
the answer to a given question. Our proposed approach, FrozenBiLM, outperforms
the state of the art in zero-shot VideoQA by a significant margin on a variety
of datasets, including LSMDC-FiB, iVQA, MSRVTT-QA, MSVD-QA, ActivityNet-QA,
TGIF-FrameQA, How2QA and TVQA. It also demonstrates competitive performance in
the few-shot and fully-supervised setting. Our code and models will be made
publicly available at https://antoyang.github.io/frozenbilm.html.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">All the World's a (Hyper)Graph: A Data Drama. (arXiv:2206.08225v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08225">
<div class="article-summary-box-inner">
<span><p>We introduce Hyperbard, a dataset of diverse relational data representations
derived from Shakespeare's plays. Our representations range from simple graphs
capturing character co-occurrence in single scenes to hypergraphs encoding
complex communication settings and character contributions as hyperedges with
edge-specific node weights. By making multiple intuitive representations
readily available for experimentation, we facilitate rigorous representation
robustness checks in graph learning, graph mining, and network analysis,
highlighting the advantages and drawbacks of specific representations.
Leveraging the data released in Hyperbard, we demonstrate that many solutions
to popular graph mining problems are highly dependent on the representation
choice, thus calling current graph curation practices into question. As an
homage to our data source, and asserting that science can also be art, we
present all our points in the form of a play.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning Architecture for Automatic Essay Scoring. (arXiv:2206.08232v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08232">
<div class="article-summary-box-inner">
<span><p>Automatic evaluation of essay (AES) and also called automatic essay scoring
has become a severe problem due to the rise of online learning and evaluation
platforms such as Coursera, Udemy, Khan academy, and so on. Researchers have
recently proposed many techniques for automatic evaluation. However, many of
these techniques use hand-crafted features and thus are limited from the
feature representation point of view. Deep learning has emerged as a new
paradigm in machine learning which can exploit the vast data and identify the
features useful for essay evaluation. To this end, we propose a novel
architecture based on recurrent networks (RNN) and convolution neural network
(CNN). In the proposed architecture, the multichannel convolutional layer
learns and captures the contextual features of the word n-gram from the word
embedding vectors and the essential semantic concepts to form the feature
vector at essay level using max-pooling operation. A variant of RNN called
Bi-gated recurrent unit (BGRU) is used to access both previous and subsequent
contextual representations. The experiment was carried out on eight data sets
available on Kaggle for the task of AES. The experimental results show that our
proposed system achieves significantly higher grading accuracy than other deep
learning-based AES systems and also other state-of-the-art AES systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">'John ate 5 apples' != 'John ate some apples': Self-Supervised Paraphrase Quality Detection for Algebraic Word Problems. (arXiv:2206.08263v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08263">
<div class="article-summary-box-inner">
<span><p>This paper introduces the novel task of scoring paraphrases for Algebraic
Word Problems (AWP) and presents a self-supervised method for doing so. In the
current online pedagogical setting, paraphrasing these problems is helpful for
academicians to generate multiple syntactically diverse questions for
assessments. It also helps induce variation to ensure that the student has
understood the problem instead of just memorizing it or using unfair means to
solve it. The current state-of-the-art paraphrase generation models often
cannot effectively paraphrase word problems, losing a critical piece of
information (such as numbers or units) which renders the question unsolvable.
There is a need for paraphrase scoring methods in the context of AWP to enable
the training of good paraphrasers. Thus, we propose ParaQD, a self-supervised
paraphrase quality detection method using novel data augmentations that can
learn latent representations to separate a high-quality paraphrase of an
algebraic question from a poor one by a wide margin. Through extensive
experimentation, we demonstrate that our method outperforms existing
state-of-the-art self-supervised methods by up to 32% while also demonstrating
impressive zero-shot performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards the Generation of Musical Explanations with GPT-3. (arXiv:2206.08264v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08264">
<div class="article-summary-box-inner">
<span><p>Open AI's language model, GPT-3, has shown great potential for many NLP
tasks, with applications in many different domains. In this work we carry out a
first study on GPT-3's capability to communicate musical decisions through
textual explanations when prompted with a textual representation of a piece of
music. Enabling a dialogue in human-AI music partnerships is an important step
towards more engaging and creative human-AI interactions. Our results show that
GPT-3 lacks the necessary intelligence to really understand musical decisions.
A major barrier to reach a better performance is the lack of data that includes
explanations of the creative process carried out by artists for musical pieces.
We believe such a resource would aid the understanding and collaboration with
AI music systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ANGLEr: A Next-Generation Natural Language Exploratory Framework. (arXiv:2206.08266v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08266">
<div class="article-summary-box-inner">
<span><p>Natural language processing is used for solving a wide variety of problems.
Some scholars and interest groups working with language resources are not well
versed in programming, so there is a need for a good graphical framework that
allows users to quickly design and test natural language processing pipelines
without the need for programming. The existing frameworks do not satisfy all
the requirements for such a tool. We, therefore, propose a new framework that
provides a simple way for its users to build language processing pipelines. It
also allows a simple programming language agnostic way for adding new modules,
which will help the adoption by natural language processing developers and
researchers. The main parts of the proposed framework consist of (a) a
pluggable Docker-based architecture, (b) a general data model, and (c) APIs
description along with the graphical user interface. The proposed design is
being used for implementation of a new natural language processing framework,
called ANGLEr.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ratatouille: A tool for Novel Recipe Generation. (arXiv:2206.08267v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08267">
<div class="article-summary-box-inner">
<span><p>Due to availability of a large amount of cooking recipes online, there is a
growing interest in using this as data to create novel recipes. Novel Recipe
Generation is a problem in the field of Natural Language Processing in which
our main interest is to generate realistic, novel cooking recipes. To come up
with such novel recipes, we trained various Deep Learning models such as LSTMs
and GPT-2 with a large amount of recipe data. We present Ratatouille
(https://cosylab.iiitd.edu.in/ratatouille2/), a web based application to
generate novel recipes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Balancing Cost and Quality: An Exploration of Human-in-the-loop Frameworks for Automated Short Answer Scoring. (arXiv:2206.08288v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08288">
<div class="article-summary-box-inner">
<span><p>Short answer scoring (SAS) is the task of grading short text written by a
learner. In recent years, deep-learning-based approaches have substantially
improved the performance of SAS models, but how to guarantee high-quality
predictions still remains a critical issue when applying such models to the
education field. Towards guaranteeing high-quality predictions, we present the
first study of exploring the use of human-in-the-loop framework for minimizing
the grading cost while guaranteeing the grading quality by allowing a SAS model
to share the grading task with a human grader. Specifically, by introducing a
confidence estimation method for indicating the reliability of the model
predictions, one can guarantee the scoring quality by utilizing only
predictions with high reliability for the scoring results and casting
predictions with low reliability to human graders. In our experiments, we
investigate the feasibility of the proposed framework using multiple confidence
estimation methods and multiple SAS datasets. We find that our
human-in-the-loop framework allows automatic scoring models and human graders
to achieve the target scoring quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Paraformer: Fast and Accurate Parallel Transformer for Non-autoregressive End-to-End Speech Recognition. (arXiv:2206.08317v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08317">
<div class="article-summary-box-inner">
<span><p>Transformers have recently dominated the ASR field. Although able to yield
good performance, they involve an autoregressive (AR) decoder to generate
tokens one by one, which is computationally inefficient. To speed up inference,
non-autoregressive (NAR) methods, e.g. single-step NAR, were designed, to
enable parallel generation. However, due to an independence assumption within
the output tokens, performance of single-step NAR is inferior to that of AR
models, especially with a large-scale corpus. There are two challenges to
improving single-step NAR: Firstly to accurately predict the number of output
tokens and extract hidden variables; secondly, to enhance modeling of
interdependence between output tokens. To tackle both challenges, we propose a
fast and accurate parallel transformer, termed Paraformer. This utilizes a
continuous integrate-and-fire based predictor to predict the number of tokens
and generate hidden variables. A glancing language model (GLM) sampler then
generates semantic embeddings to enhance the NAR decoder's ability to model
context interdependence. Finally, we design a strategy to generate negative
samples for minimum word error rate training to further improve performance.
Experiments using the public AISHELL-1, AISHELL-2 benchmark, and an
industrial-level 20,000 hour task demonstrate that the proposed Paraformer can
attain comparable performance to the state-of-the-art AR transformer, with more
than 10x speedup.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Characteristics of Harmful Text: Towards Rigorous Benchmarking of Language Models. (arXiv:2206.08325v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08325">
<div class="article-summary-box-inner">
<span><p>Large language models produce human-like text that drive a growing number of
applications. However, recent literature and, increasingly, real world
observations, have demonstrated that these models can generate language that is
toxic, biased, untruthful or otherwise harmful. Though work to evaluate
language model harms is under way, translating foresight about which harms may
arise into rigorous benchmarks is not straightforward. To facilitate this
translation, we outline six ways of characterizing harmful text which merit
explicit consideration when designing new benchmarks. We then use these
characteristics as a lens to identify trends and gaps in existing benchmarks.
Finally, we apply them in a case study of the Perspective API, a toxicity
classifier that is widely used in harm benchmarks. Our characteristics provide
one piece of the bridge that translates between foresight and effective
evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Know your audience: specializing grounded language models with the game of Dixit. (arXiv:2206.08349v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08349">
<div class="article-summary-box-inner">
<span><p>Effective communication requires adapting to the idiosyncratic common ground
shared with each communicative partner. We study a particularly challenging
instantiation of this problem: the popular game Dixit. We formulate a round of
Dixit as a multi-agent image reference game where a (trained) speaker model is
rewarded for describing a target image such that one (pretrained) listener
model can correctly identify it from a pool of distractors, but another
listener cannot. To adapt to this setting, the speaker must exploit differences
in the common ground it shares with the different listeners. We show that
finetuning an attention-based adapter between a CLIP vision encoder and a large
language model in this contrastive, multi-agent setting gives rise to
context-dependent natural language specialization from rewards only, without
direct supervision. In a series of controlled experiments, we show that the
speaker can adapt according to the idiosyncratic strengths and weaknesses of
various pairs of different listeners. Furthermore, we show zero-shot transfer
of the speaker's specialization to unseen real-world data. Our experiments
offer a step towards adaptive communication in complex multi-partner settings
and highlight the interesting research challenges posed by games like Dixit. We
hope that our work will inspire creative new approaches to adapting pretrained
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Survey on Publicly Available Sinhala Natural Language Processing Tools and Research. (arXiv:1906.02358v13 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1906.02358">
<div class="article-summary-box-inner">
<span><p>Sinhala is the native language of the Sinhalese people who make up the
largest ethnic group of Sri Lanka. The language belongs to the globe-spanning
language tree, Indo-European. However, due to poverty in both linguistic and
economic capital, Sinhala, in the perspective of Natural Language Processing
tools and research, remains a resource-poor language which has neither the
economic drive its cousin English has nor the sheer push of the law of numbers
a language such as Chinese has. A number of research groups from Sri Lanka have
noticed this dearth and the resultant dire need for proper tools and research
for Sinhala natural language processing. However, due to various reasons, these
attempts seem to lack coordination and awareness of each other. The objective
of this paper is to fill that gap of a comprehensive literature survey of the
publicly available Sinhala natural language tools and research so that the
researchers working in this field can better utilize contributions of their
peers. As such, we shall be uploading this paper to arXiv and perpetually
update it periodically to reflect the advances made in the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Broader terms curriculum mapping: Using natural language processing and visual-supported communication to create representative program planning experiences. (arXiv:2102.04811v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.04811">
<div class="article-summary-box-inner">
<span><p>Accreditation bodies call for curriculum development processes open to all
stakeholders, reflecting viewpoints of students, industry, university faculty
and society. However, communication difficulties between faculty and
non-faculty groups leave unexplored an immense collaboration potential. Using
classification of learning objectives, natural language processing, and data
visualization, this paper presents a method to deliver program plan
representations that are universal, self-explanatory, and empowering. A simple
example shows how the method contributes to representative program planning
experiences and a case study is used to confirm the method's accuracy and
utility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Benchmarking Differential Privacy and Federated Learning for BERT Models. (arXiv:2106.13973v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.13973">
<div class="article-summary-box-inner">
<span><p>Natural Language Processing (NLP) techniques can be applied to help with the
diagnosis of medical conditions such as depression, using a collection of a
person's utterances. Depression is a serious medical illness that can have
adverse effects on how one feels, thinks, and acts, which can lead to emotional
and physical problems. Due to the sensitive nature of such data, privacy
measures need to be taken for handling and training models with such data. In
this work, we study the effects that the application of Differential Privacy
(DP) has, in both a centralized and a Federated Learning (FL) setup, on
training contextualized language models (BERT, ALBERT, RoBERTa and DistilBERT).
We offer insights on how to privately train NLP models and what architectures
and setups provide more desirable privacy utility trade-offs. We envisage this
work to be used in future healthcare and mental health studies to keep medical
history private. Therefore, we provide an open-source implementation of this
work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LightSeq2: Accelerated Training for Transformer-based Models on GPUs. (arXiv:2110.05722v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05722">
<div class="article-summary-box-inner">
<span><p>Transformer-based neural models are used in many AI applications. Training
these models is expensive, as it takes huge GPU resources and long duration. It
is challenging because typical data like sentences have variable lengths, and
Transformer's computation patterns are more complex than convolutional neural
networks. Existing systems either only focus on model inference or optimization
for only BERT-like encoder models. In this paper, we present LightSeq2, a
system to accelerate training for a general family of Transformer models on
GPUs. We propose a series of GPU optimization techniques tailored to the
specific computation flow and memory access patterns of Transformer models.
LightSeq2 supports many model architectures, including BERT (encoder-only), GPT
(decoder-only), Transformer (encoder-decoder), and vision Transformer. Our
experiments for a variety of models and benchmarks show that LightSeq2 is
consistently faster (1.4-3.5x) than previous systems on different GPUs. In
particular, it gains 308% training speedup compared with existing systems on a
large public machine translation benchmark (WMT14 English-German).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Interpretation of Neural Text Classification. (arXiv:2202.09792v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09792">
<div class="article-summary-box-inner">
<span><p>Recent years have witnessed increasing interests in developing interpretable
models in Natural Language Processing (NLP). Most existing models aim at
identifying input features such as words or phrases important for model
predictions. Neural models developed in NLP however often compose word
semantics in a hierarchical manner and text classification requires
hierarchical modelling to aggregate local information in order to deal with
topic and label shifts more effectively. As such, interpretation by words or
phrases only cannot faithfully explain model decisions in text classification.
This paper proposes a novel Hierarchical INTerpretable neural text classifier,
called Hint, which can automatically generate explanations of model predictions
in the form of label-associated topics in a hierarchical manner. Model
interpretation is no longer at the word level, but built on topics as the basic
semantic unit. Experimental results on both review datasets and news datasets
show that our proposed approach achieves text classification results on par
with existing state-of-the-art text classifiers, and generates interpretations
more faithful to model predictions and better understood by humans than other
interpretable neural text classifiers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KSoF: The Kassel State of Fluency Dataset -- A Therapy Centered Dataset of Stuttering. (arXiv:2203.05383v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05383">
<div class="article-summary-box-inner">
<span><p>Stuttering is a complex speech disorder that negatively affects an
individual's ability to communicate effectively. Persons who stutter (PWS)
often suffer considerably under the condition and seek help through therapy.
Fluency shaping is a therapy approach where PWSs learn to modify their speech
to help them to overcome their stutter. Mastering such speech techniques takes
time and practice, even after therapy. Shortly after therapy, success is
evaluated highly, but relapse rates are high. To be able to monitor speech
behavior over a long time, the ability to detect stuttering events and
modifications in speech could help PWSs and speech pathologists to track the
level of fluency. Monitoring could create the ability to intervene early by
detecting lapses in fluency. To the best of our knowledge, no public dataset is
available that contains speech from people who underwent stuttering therapy
that changed the style of speaking. This work introduces the Kassel State of
Fluency (KSoF), a therapy-based dataset containing over 5500 clips of PWSs. The
clips were labeled with six stuttering-related event types: blocks,
prolongations, sound repetitions, word repetitions, interjections, and -
specific to therapy - speech modifications. The audio was recorded during
therapy sessions at the Institut der Kasseler Stottertherapie. The data will be
made available for research purposes upon request.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Graph-Enabled Text-Based Automatic Personality Prediction. (arXiv:2203.09103v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09103">
<div class="article-summary-box-inner">
<span><p>How people think, feel, and behave, primarily is a representation of their
personality characteristics. By being conscious of personality characteristics
of individuals whom we are dealing with or decided to deal with, one can
competently ameliorate the relationship, regardless of its type. With the rise
of Internet-based communication infrastructures (social networks, forums,
etc.), a considerable amount of human communications take place there. The most
prominent tool in such communications, is the language in written and spoken
form that adroitly encodes all those essential personality characteristics of
individuals. Text-based Automatic Personality Prediction (APP) is the automated
forecasting of the personality of individuals based on the generated/exchanged
text contents. This paper presents a novel knowledge graph-enabled approach to
text-based APP that relies on the Big Five personality traits. To this end,
given a text a knowledge graph which is a set of interlinked descriptions of
concepts, was built through matching the input text's concepts with DBpedia
knowledge base entries. Then, due to achieving more powerful representation the
graph was enriched with the DBpedia ontology, NRC Emotion Intensity Lexicon,
and MRC psycholinguistic database information. Afterwards, the knowledge graph
which is now a knowledgeable alternative for the input text was embedded to
yield an embedding matrix. Finally, to perform personality predictions the
resulting embedding matrix was fed to four suggested deep learning models
independently, which are based on convolutional neural network (CNN), simple
recurrent neural network (RNN), long short term memory (LSTM) and bidirectional
long short term memory (BiLSTM). The results indicated a considerable
improvements in prediction accuracies in all of the suggested classifiers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">STUDIES: Corpus of Japanese Empathetic Dialogue Speech Towards Friendly Voice Agent. (arXiv:2203.14757v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14757">
<div class="article-summary-box-inner">
<span><p>We present STUDIES, a new speech corpus for developing a voice agent that can
speak in a friendly manner. Humans naturally control their speech prosody to
empathize with each other. By incorporating this "empathetic dialogue" behavior
into a spoken dialogue system, we can develop a voice agent that can respond to
a user more naturally. We designed the STUDIES corpus to include a speaker who
speaks with empathy for the interlocutor's emotion explicitly. We describe our
methodology to construct an empathetic dialogue speech corpus and report the
analysis results of the STUDIES corpus. We conducted a text-to-speech
experiment to initially investigate how we can develop more natural voice agent
that can tune its speaking style corresponding to the interlocutor's emotion.
The results show that the use of interlocutor's emotion label and
conversational context embedding can produce speech with the same degree of
naturalness as that synthesized by using the agent's emotion label. Our project
page of the STUDIES corpus is <a href="http://sython.org/Corpus/STUDIES.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Dysfluencies in Stuttering Therapy Using wav2vec 2.0. (arXiv:2204.03417v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03417">
<div class="article-summary-box-inner">
<span><p>Stuttering is a varied speech disorder that harms an individual's
communication ability. Persons who stutter (PWS) often use speech therapy to
cope with their condition. Improving speech recognition systems for people with
such non-typical speech or tracking the effectiveness of speech therapy would
require systems that can detect dysfluencies while at the same time being able
to detect speech techniques acquired in therapy. This paper shows that
fine-tuning wav2vec 2.0 [1] for the classification of stuttering on a sizeable
English corpus containing stuttered speech, in conjunction with multi-task
learning, boosts the effectiveness of the general-purpose wav2vec 2.0 features
for detecting stuttering in speech; both within and across languages. We
evaluate our method on FluencyBank , [2] and the German therapy-centric Kassel
State of Fluency (KSoF) [3] dataset by training Support Vector Machine
classifiers using features extracted from the finetuned models for six
different stuttering-related event types: blocks, prolongations, sound
repetitions, word repetitions, interjections, and - specific to therapy -
speech modifications. Using embeddings from the fine-tuned models leads to
relative classification performance gains up to 27% w.r.t. F1-score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Handling sign language transcription system with the computer-friendly numerical multilabels. (arXiv:2204.06924v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06924">
<div class="article-summary-box-inner">
<span><p>This paper presents our recent developments in the automatic processing of
sign language corpora using the Hamburg Sign Language Annotation System
(HamNoSys). We designed an automated tool to convert HamNoSys annotations into
numerical labels for defined initial features of body and hand positions. Our
proposed numerical multilabels greatly simplify annotations' structure without
significant loss of gloss meaning. These numerical multilabels can potentially
be used to feed the machine learning models, which would accelerate the
development of vision-based sign language recognition. In addition, this tool
can assist experts in the annotation process and help identify semantic errors.
The code and sample annotations are publicly available at
\url{https://github.com/hearai/parse-hamnosys}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DDXPlus: A New Dataset For Automatic Medical Diagnosis. (arXiv:2205.09148v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.09148">
<div class="article-summary-box-inner">
<span><p>There has been a rapidly growing interest in Automatic Symptom Detection
(ASD) and Automatic Diagnosis (AD) systems in the machine learning research
literature, aiming to assist doctors in telemedicine services. These systems
are designed to interact with patients, collect evidence about their symptoms
and relevant antecedents, and possibly make predictions about the underlying
diseases. Doctors would review the interactions, including the evidence and the
predictions, collect if necessary additional information from patients, before
deciding on next steps. Despite recent progress in this area, an important
piece of doctors' interactions with patients is missing in the design of these
systems, namely the differential diagnosis. Its absence is largely due to the
lack of datasets that include such information for models to train on. In this
work, we present a large-scale synthetic dataset of roughly 1.3 million
patients that includes a differential diagnosis, along with the ground truth
pathology, symptoms and antecedents, for each patient. Unlike existing datasets
which only contain binary symptoms and antecedents, this dataset also contains
categorical and multi-choice symptoms and antecedents useful for efficient data
collection. Moreover, some symptoms are organized in a hierarchy, making it
possible to design systems able to interact with patients in a logical way. As
a proof-of-concept, we extend two existing AD and ASD systems to incorporate
the differential diagnosis, and provide empirical evidence that using
differentials as training signals is essential for the efficiency of such
systems. The dataset is available at
\href{https://figshare.com/articles/dataset/DDXPlus_Dataset/20043374}{https://figshare.com/articles/dataset/DDXPlus\_Dataset/20043374}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Over-Generation Cannot Be Rewarded: Length-Adaptive Average Lagging for Simultaneous Speech Translation. (arXiv:2206.05807v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05807">
<div class="article-summary-box-inner">
<span><p>Simultaneous speech translation (SimulST) systems aim at generating their
output with the lowest possible latency, which is normally computed in terms of
Average Lagging (AL). In this paper we highlight that, despite its widespread
adoption, AL provides underestimated scores for systems that generate longer
predictions compared to the corresponding references. We also show that this
problem has practical relevance, as recent SimulST systems have indeed a
tendency to over-generate. As a solution, we propose LAAL (Length-Adaptive
Average Lagging), a modified version of the metric that takes into account the
over-generation phenomenon and allows for unbiased evaluation of both
under-/over-generating systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Maximum Linear Arrangement Problem for trees under projectivity and planarity. (arXiv:2206.06924v2 [cs.DS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06924">
<div class="article-summary-box-inner">
<span><p>The Maximum Linear Arrangement problem (MaxLA) consists of finding a mapping
$\pi$ from the $n$ vertices of a graph $G$ to distinct consecutive integers
that maximizes $D_{\pi}(G)=\sum_{uv\in E(G)}|\pi(u) - \pi(v)|$. In this
setting, vertices are considered to lie on a horizontal line and edges are
drawn as semicircles above the line. There exist variants of MaxLA in which the
arrangements are constrained. In the planar variant edge crossings are
forbidden. In the projective variant for rooted trees arrangements are planar
and the root cannot be covered by any edge. Here we present $O(n)$-time and
$O(n)$-space algorithms that solve Planar and Projective MaxLA for trees. We
also prove several properties of maximum projective and planar arrangements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Emotion is Not One-hot Encoding: Learning with Grayscale Label for Emotion Recognition in Conversation. (arXiv:2206.07359v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07359">
<div class="article-summary-box-inner">
<span><p>In emotion recognition in conversation (ERC), the emotion of the current
utterance is predicted by considering the previous context, which can be
utilized in many natural language processing tasks. Although multiple emotions
can coexist in a given sentence, most previous approaches take the perspective
of a classification task to predict only a given label. However, it is
expensive and difficult to label the emotion of a sentence with confidence or
multi-label. In this paper, we automatically construct a grayscale label
considering the correlation between emotions and use it for learning. That is,
instead of using a given label as a one-hot encoding, we construct a grayscale
label by measuring scores for different emotions. We introduce several methods
for constructing grayscale labels and confirm that each method improves the
emotion recognition performance. Our method is simple, effective, and
universally applicable to previous systems. The experiments show a significant
improvement in the performance of baselines.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Diversity with Adversarially Learned Transformations for Domain Generalization. (arXiv:2206.07736v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07736">
<div class="article-summary-box-inner">
<span><p>To be successful in single source domain generalization, maximizing diversity
of synthesized domains has emerged as one of the most effective strategies.
Many of the recent successes have come from methods that pre-specify the types
of diversity that a model is exposed to during training, so that it can
ultimately generalize well to new domains. However, na\"ive diversity based
augmentations do not work effectively for domain generalization either because
they cannot model large domain shift, or because the span of transforms that
are pre-specified do not cover the types of shift commonly occurring in domain
generalization. To address this issue, we present a novel framework that uses
adversarially learned transformations (ALT) using a neural network to model
plausible, yet hard image transformations that fool the classifier. This
network is randomly initialized for each batch and trained for a fixed number
of steps to maximize classification error. Further, we enforce consistency
between the classifier's predictions on the clean and transformed images. With
extensive empirical analysis, we find that this new form of adversarial
transformations achieve both objectives of diversity and hardness
simultaneously, outperforming all existing techniques on competitive benchmarks
for single source domain generalization. We also show that ALT can naturally
work with existing diversity modules to produce highly distinct, and large
transformations of the source domain leading to state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Edge Inference with Fully Differentiable Quantized Mixed Precision Neural Networks. (arXiv:2206.07741v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07741">
<div class="article-summary-box-inner">
<span><p>The large computing and memory cost of deep neural networks (DNNs) often
precludes their use in resource-constrained devices. Quantizing the parameters
and operations to lower bit-precision offers substantial memory and energy
savings for neural network inference, facilitating the use of DNNs on edge
computing platforms. Recent efforts at quantizing DNNs have employed a range of
techniques encompassing progressive quantization, step-size adaptation, and
gradient scaling. This paper proposes a new quantization approach for mixed
precision convolutional neural networks (CNNs) targeting edge-computing. Our
method establishes a new pareto frontier in model accuracy and memory footprint
demonstrating a range of quantized models, delivering best-in-class accuracy
below 4.3 MB of weights (wgts.) and activations (acts.). Our main contributions
are: (i) hardware-aware heterogeneous differentiable quantization with
tensor-sliced learned precision, (ii) targeted gradient modification for wgts.
and acts. to mitigate quantization errors, and (iii) a multi-phase learning
schedule to address instability in learning arising from updates to the learned
quantizer and model parameters. We demonstrate the effectiveness of our
techniques on the ImageNet dataset across a range of models including
EfficientNet-Lite0 (e.g., 4.14MB of wgts. and acts. at 67.66% accuracy) and
MobileNetV2 (e.g., 3.51MB wgts. and acts. at 65.39% accuracy).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reconstructing Training Data from Trained Neural Networks. (arXiv:2206.07758v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07758">
<div class="article-summary-box-inner">
<span><p>Understanding to what extent neural networks memorize training data is an
intriguing question with practical and theoretical implications. In this paper
we show that in some cases a significant fraction of the training data can in
fact be reconstructed from the parameters of a trained neural network
classifier. We propose a novel reconstruction scheme that stems from recent
theoretical results about the implicit bias in training neural networks with
gradient-based methods. To the best of our knowledge, our results are the first
to show that reconstructing a large portion of the actual training samples from
a trained neural network classifier is generally possible. This has negative
implications on privacy, as it can be used as an attack for revealing sensitive
training data. We demonstrate our method for binary MLP classifiers on a few
standard computer vision datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SAVi++: Towards End-to-End Object-Centric Learning from Real-World Videos. (arXiv:2206.07764v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07764">
<div class="article-summary-box-inner">
<span><p>The visual world can be parsimoniously characterized in terms of distinct
entities with sparse interactions. Discovering this compositional structure in
dynamic visual scenes has proven challenging for end-to-end computer vision
approaches unless explicit instance-level supervision is provided. Slot-based
models leveraging motion cues have recently shown great promise in learning to
represent, segment, and track objects without direct supervision, but they
still fail to scale to complex real-world multi-object videos. In an effort to
bridge this gap, we take inspiration from human development and hypothesize
that information about scene geometry in the form of depth signals can
facilitate object-centric learning. We introduce SAVi++, an object-centric
video model which is trained to predict depth signals from a slot-based video
representation. By further leveraging best practices for model scaling, we are
able to train SAVi++ to segment complex dynamic scenes recorded with moving
cameras, containing both static and moving objects of diverse appearance on
naturalistic backgrounds, without the need for segmentation supervision.
Finally, we demonstrate that by using sparse depth signals obtained from LiDAR,
SAVi++ is able to learn emergent object segmentation and tracking from videos
in the real-world Waymo Open dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discrete Contrastive Diffusion for Cross-Modal and Conditional Generation. (arXiv:2206.07771v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07771">
<div class="article-summary-box-inner">
<span><p>Diffusion probabilistic models (DPMs) have become a popular approach to
conditional generation, due to their promising results and support for
cross-modal synthesis. A key desideratum in conditional synthesis is to achieve
high correspondence between the conditioning input and generated output. Most
existing methods learn such relationships implicitly, by incorporating the
prior into the variational lower bound. In this work, we take a different route
-- we enhance input-output connections by maximizing their mutual information
using contrastive learning. To this end, we introduce a Conditional Discrete
Contrastive Diffusion (CDCD) loss and design two contrastive diffusion
mechanisms to effectively incorporate it into the denoising process. We
formulate CDCD by connecting it with the conventional variational objectives.
We demonstrate the efficacy of our approach in evaluations with three diverse,
multimodal conditional synthesis tasks: dance-to-music generation,
text-to-image synthesis, and class-conditioned image synthesis. On each, we
achieve state-of-the-art or higher synthesis quality and improve the
input-output correspondence. Furthermore, the proposed approach improves the
convergence of diffusion models, reducing the number of required diffusion
steps by more than 35% on two benchmarks, significantly increasing the
inference speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Calibrated Model Uncertainty in Deep Learning. (arXiv:2206.07795v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07795">
<div class="article-summary-box-inner">
<span><p>Estimated uncertainty by approximate posteriors in Bayesian neural networks
are prone to miscalibration, which leads to overconfident predictions in
critical tasks that have a clear asymmetric cost or significant losses. Here,
we extend the approximate inference for the loss-calibrated Bayesian framework
to dropweights based Bayesian neural networks by maximising expected utility
over a model posterior to calibrate uncertainty in deep learning. Furthermore,
we show that decisions informed by loss-calibrated uncertainty can improve
diagnostic performance to a greater extent than straightforward alternatives.
We propose Maximum Uncertainty Calibration Error (MUCE) as a metric to measure
calibrated confidence, in addition to its prediction especially for high-risk
applications, where the goal is to minimise the worst-case deviation between
error and estimated uncertainty. In experiments, we show the correlation
between error in prediction and estimated uncertainty by interpreting
Wasserstein distance as the accuracy of prediction. We evaluated the
effectiveness of our approach to detecting Covid-19 from X-Ray images.
Experimental results show that our method reduces miscalibration considerably,
without impacting the models accuracy and improves reliability of
computer-based diagnostics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What makes domain generalization hard?. (arXiv:2206.07802v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07802">
<div class="article-summary-box-inner">
<span><p>While several methodologies have been proposed for the daunting task of
domain generalization, understanding what makes this task challenging has
received little attention. Here we present SemanticDG (Semantic Domain
Generalization): a benchmark with 15 photo-realistic domains with the same
geometry, scene layout and camera parameters as the popular 3D ScanNet dataset,
but with controlled domain shifts in lighting, materials, and viewpoints. Using
this benchmark, we investigate the impact of each of these semantic shifts on
generalization independently. Visual recognition models easily generalize to
novel lighting, but struggle with distribution shifts in materials and
viewpoints. Inspired by human vision, we hypothesize that scene context can
serve as a bridge to help models generalize across material and viewpoint
domain shifts and propose a context-aware vision transformer along with a
contrastive loss over material and viewpoint changes to address these domain
shifts. Our approach (dubbed as CDCNet) outperforms existing domain
generalization methods by over an 18% margin. As a critical benchmark, we also
conduct psychophysics experiments and find that humans generalize equally well
across lighting, materials and viewpoints. The benchmark and computational
model introduced here help understand the challenges associated with
generalization across domains and provide initial steps towards extrapolation
to semantic distribution shifts. We include all data and source code in the
supplement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Disentangling visual and written concepts in CLIP. (arXiv:2206.07835v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07835">
<div class="article-summary-box-inner">
<span><p>The CLIP network measures the similarity between natural text and images; in
this work, we investigate the entanglement of the representation of word images
and natural images in its image encoder. First, we find that the image encoder
has an ability to match word images with natural images of scenes described by
those words. This is consistent with previous research that suggests that the
meaning and the spelling of a word might be entangled deep within the network.
On the other hand, we also find that CLIP has a strong ability to match
nonsense words, suggesting that processing of letters is separated from
processing of their meaning. To explicitly determine whether the spelling
capability of CLIP is separable, we devise a procedure for identifying
representation subspaces that selectively isolate or eliminate spelling
capabilities. We benchmark our methods against a range of retrieval tasks, and
we also test them by measuring the appearance of text in CLIP-guided generated
images. We find that our methods are able to cleanly separate spelling
capabilities of CLIP from the visual processing of natural images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Action Spotting using Dense Detection Anchors Revisited: Submission to the SoccerNet Challenge 2022. (arXiv:2206.07846v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07846">
<div class="article-summary-box-inner">
<span><p>This technical report describes our submission to the Action Spotting
SoccerNet Challenge 2022. The challenge is part of the CVPR 2022 ActivityNet
Workshop. Our submission is based on a method that we proposed recently, which
focuses on increasing temporal precision via a densely sampled set of detection
anchors. Due to its emphasis on temporal precision, this approach is able to
produce competitive results on the tight average-mAP metric, which uses small
temporal evaluation tolerances. This recently proposed metric is the evaluation
criterion used for the challenge. In order to further improve results, here we
introduce small changes in the pre- and post-processing steps, and also combine
different input feature types via late fusion. This report describes the
resulting overall approach, focusing on the modifications introduced. We also
describe the training procedures used, and present our results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved surface reconstruction using high-frequency details. (arXiv:2206.07850v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07850">
<div class="article-summary-box-inner">
<span><p>Neural rendering can be used to reconstruct implicit representations of
shapes without 3D supervision. However, current neural surface reconstruction
methods have difficulty learning high-frequency details of shapes, so that the
reconstructed shapes are often oversmoothed. We propose a novel method to
improve the quality of surface reconstruction in neural rendering. We follow
recent work to model surfaces as signed distance fields. First, we offer a
derivation to analyze the relationship between the signed distance function,
the volume density, the transparency function, and the weighting function used
in the volume rendering equation. Second, we observe that attempting to jointly
encode high-frequency and low frequency components in a single signed distance
function leads to unstable optimization. We propose to decompose the signed
distance function in a base function and a displacement function together with
a coarse-to-fine strategy to gradually increase the high-frequency details.
Finally, we propose to use an adaptive strategy that enables the optimization
to focus on improving certain regions near the surface where the signed
distance fields have artifacts. Our qualitative and quantitative results show
that our method can reconstruct high-frequency surface details and obtain
better surface reconstruction quality than the current state of the art. Code
will be released at https://github.com/yiqun-wang/HFS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PeQuENet: Perceptual Quality Enhancement of Compressed Video with Adaptation- and Attention-based Network. (arXiv:2206.07893v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07893">
<div class="article-summary-box-inner">
<span><p>In this paper we propose a generative adversarial network (GAN) framework to
enhance the perceptual quality of compressed videos. Our framework includes
attention and adaptation to different quantization parameters (QPs) in a single
model. The attention module exploits global receptive fields that can capture
and align long-range correlations between consecutive frames, which can be
beneficial for enhancing perceptual quality of videos. The frame to be enhanced
is fed into the deep network together with its neighboring frames, and in the
first stage features at different depths are extracted. Then extracted features
are fed into attention blocks to explore global temporal correlations, followed
by a series of upsampling and convolution layers. Finally, the resulting
features are processed by the QP-conditional adaptation module which leverages
the corresponding QP information. In this way, a single model can be used to
enhance adaptively to various QPs without requiring multiple models specific
for every QP value, while having similar performance. Experimental results
demonstrate the superior performance of the proposed PeQuENet compared with the
state-of-the-art compressed video quality enhancement algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual Contrastive Attributed Graph Clustering Network. (arXiv:2206.07897v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07897">
<div class="article-summary-box-inner">
<span><p>Attributed graph clustering is one of the most important tasks in graph
analysis field, the goal of which is to group nodes with similar
representations into the same cluster without manual guidance. Recent studies
based on graph contrastive learning have achieved impressive results in
processing graph-structured data. However, existing graph contrastive learning
based methods 1) do not directly address the clustering task, since the
representation learning and clustering process are separated; 2) depend too
much on graph data augmentation, which greatly limits the capability of
contrastive learning; 3) ignore the contrastive message for subspace
clustering. To accommodate the aforementioned issues, we propose a generic
framework called Dual Contrastive Attributed Graph Clustering Network (DCAGC).
In DCAGC, by leveraging Neighborhood Contrast Module, the similarity of the
neighbor nodes will be maximized and the quality of the node representation
will be improved. Meanwhile, the Contrastive Self-Expression Module is built by
minimizing the node representation before and after the reconstruction of the
self-expression layer to obtain a discriminative self-expression matrix for
spectral clustering. All the modules of DCAGC are trained and optimized in a
unified framework, so the learned node representation contains
clustering-oriented messages. Extensive experimental results on four attributed
graph datasets show the superiority of DCAGC compared with 16 state-of-the-art
clustering methods. The code of this paper is available at
https://github.com/wangtong627/Dual-Contrastive-Attributed-Graph-Clustering-Network.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Dialogue State Tracking. (arXiv:2206.07898v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07898">
<div class="article-summary-box-inner">
<span><p>Designed for tracking user goals in dialogues, a dialogue state tracker is an
essential component in a dialogue system. However, the research of dialogue
state tracking has largely been limited to unimodality, in which slots and slot
values are limited by knowledge domains (e.g. restaurant domain with slots of
restaurant name and price range) and are defined by specific database schema.
In this paper, we propose to extend the definition of dialogue state tracking
to multimodality. Specifically, we introduce a novel dialogue state tracking
task to track the information of visual objects that are mentioned in
video-grounded dialogues. Each new dialogue utterance may introduce a new video
segment, new visual objects, or new object attributes, and a state tracker is
required to update these information slots accordingly. We created a new
synthetic benchmark and designed a novel baseline, Video-Dialogue Transformer
Network (VDTN), for this task. VDTN combines both object-level features and
segment-level features and learns contextual dependencies between videos and
dialogues to generate multimodal dialogue states. We optimized VDTN for a state
generation task as well as a self-supervised video understanding task which
recovers video segment or object representations. Finally, we trained VDTN to
use the decoded states in a response prediction task. Together with
comprehensive ablation and qualitative analysis, we discovered interesting
insights towards building more capable multimodal dialogue systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lifelong Wandering: A realistic few-shot online continual learning setting. (arXiv:2206.07932v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07932">
<div class="article-summary-box-inner">
<span><p>Online few-shot learning describes a setting where models are trained and
evaluated on a stream of data while learning emerging classes. While prior work
in this setting has achieved very promising performance on instance
classification when learning from data-streams composed of a single indoor
environment, we propose to extend this setting to consider object
classification on a series of several indoor environments, which is likely to
occur in applications such as robotics. Importantly, our setting, which we
refer to as online few-shot continual learning, injects the well-studied issue
of catastrophic forgetting into the few-shot online learning paradigm. In this
work, we benchmark several existing methods and adapted baselines within our
setting, and show there exists a trade-off between catastrophic forgetting and
online performance. Our findings motivate the need for future work in this
setting, which can achieve better online performance without catastrophic
forgetting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Technical Report for Argoverse2 Challenge 2022 -- Motion Forecasting Task. (arXiv:2206.07934v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07934">
<div class="article-summary-box-inner">
<span><p>We propose a motion forecasting model called BANet, which means
Boundary-Aware Network, and it is a variant of LaneGCN. We believe that it is
not enough to use only the lane centerline as input to obtain the embedding
features of the vector map nodes. The lane centerline can only provide the
topology of the lanes, and other elements of the vector map also contain rich
information. For example, the lane boundary can provide traffic rule constraint
information such as whether it is possible to change lanes which is very
important. Therefore, we achieved better performance by encoding more vector
map elements in the motion forecasting model.We report our results on the 2022
Argoverse2 Motion Forecasting challenge and rank 2nd on the test leaderboard.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analysis and Extensions of Adversarial Training for Video Classification. (arXiv:2206.07953v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07953">
<div class="article-summary-box-inner">
<span><p>Adversarial training (AT) is a simple yet effective defense against
adversarial attacks to image classification systems, which is based on
augmenting the training set with attacks that maximize the loss. However, the
effectiveness of AT as a defense for video classification has not been
thoroughly studied. Our first contribution is to show that generating optimal
attacks for video requires carefully tuning the attack parameters, especially
the step size. Notably, we show that the optimal step size varies linearly with
the attack budget. Our second contribution is to show that using a smaller
(sub-optimal) attack budget at training time leads to a more robust performance
at test time. Based on these findings, we propose three defenses against
attacks with variable attack budgets. The first one, Adaptive AT, is a
technique where the attack budget is drawn from a distribution that is adapted
as training iterations proceed. The second, Curriculum AT, is a technique where
the attack budget is increased as training iterations proceed. The third,
Generative AT, further couples AT with a denoising generative adversarial
network to boost robust performance. Experiments on the UCF101 dataset
demonstrate that the proposed methods improve adversarial robustness against
multiple attack types.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simple Baseline for BEV Perception Without LiDAR. (arXiv:2206.07959v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07959">
<div class="article-summary-box-inner">
<span><p>Building 3D perception systems for autonomous vehicles that do not rely on
LiDAR is a critical research problem because of the high expense of LiDAR
systems compared to cameras and other sensors. Current methods use multi-view
RGB data collected from cameras around the vehicle and neurally "lift" features
from the perspective images to the 2D ground plane, yielding a "bird's eye
view" (BEV) feature representation of the 3D space around the vehicle. Recent
research focuses on the way the features are lifted from images to the BEV
plane. We instead propose a simple baseline model, where the "lifting" step
simply averages features from all projected image locations, and find that it
outperforms the current state-of-the-art in BEV vehicle segmentation. Our
ablations show that batch size, data augmentation, and input resolution play a
large part in performance. Additionally, we reconsider the utility of radar
input, which has previously been either ignored or found non-helpful by recent
works. With a simple RGB-radar fusion module, we obtain a sizable boost in
performance, approaching the accuracy of a LiDAR-enabled system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DreamNet: A Deep Riemannian Network based on SPD Manifold Learning for Visual Classification. (arXiv:2206.07967v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07967">
<div class="article-summary-box-inner">
<span><p>Image set-based visual classification methods have achieved remarkable
performance, via characterising the image set in terms of a non-singular
covariance matrix on a symmetric positive definite (SPD) manifold. To adapt to
complicated visual scenarios better, several Riemannian networks (RiemNets) for
SPD matrix nonlinear processing have recently been studied. However, it is
pertinent to ask, whether greater accuracy gains can be achieved by simply
increasing the depth of RiemNets. The answer appears to be negative, as deeper
RiemNets tend to lose generalization ability. To explore a possible solution to
this issue, we propose a new architecture for SPD matrix learning.
Specifically, to enrich the deep representations, we adopt SPDNet [1] as the
backbone, with a stacked Riemannian autoencoder (SRAE) built on the tail. The
associated reconstruction error term can make the embedding functions of both
SRAE and of each RAE an approximate identity mapping, which helps to prevent
the degradation of statistical information. We then insert several
residual-like blocks with shortcut connections to augment the representational
capacity of SRAE, and to simplify the training of a deeper network. The
experimental evidence demonstrates that our DreamNet can achieve improved
accuracy with increased depth of the network.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-scale Cooperative Multimodal Transformers for Multimodal Sentiment Analysis in Videos. (arXiv:2206.07981v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07981">
<div class="article-summary-box-inner">
<span><p>Multimodal sentiment analysis in videos is a key task in many real-world
applications, which usually requires integrating multimodal streams including
visual, verbal and acoustic behaviors. To improve the robustness of multimodal
fusion, some of the existing methods let different modalities communicate with
each other and modal the crossmodal interaction via transformers. However,
these methods only use the single-scale representations during the interaction
but forget to exploit multi-scale representations that contain different levels
of semantic information. As a result, the representations learned by
transformers could be biased especially for unaligned multimodal data. In this
paper, we propose a multi-scale cooperative multimodal transformer (MCMulT)
architecture for multimodal sentiment analysis. On the whole, the "multi-scale"
mechanism is capable of exploiting the different levels of semantic information
of each modality which are used for fine-grained crossmodal interactions.
Meanwhile, each modality learns its feature hierarchies via integrating the
crossmodal interactions from multiple level features of its source modality. In
this way, each pair of modalities progressively builds feature hierarchies
respectively in a cooperative manner. The empirical results illustrate that our
MCMulT model not only outperforms existing approaches on unaligned multimodal
sequences but also has strong performance on aligned multimodal sequences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Captioning based on Feature Refinement and Reflective Decoding. (arXiv:2206.07986v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07986">
<div class="article-summary-box-inner">
<span><p>Automatically generating a description of an image in natural language is
called image captioning. It is an active research topic that lies at the
intersection of two major fields in artificial intelligence, computer vision,
and natural language processing. Image captioning is one of the significant
challenges in image understanding since it requires not only recognizing
salient objects in the image but also their attributes and the way they
interact. The system must then generate a syntactically and semantically
correct caption that describes the image content in natural language. With the
significant progress in deep learning models and their ability to effectively
encode large sets of images and generate correct sentences, several
neural-based captioning approaches have been proposed recently, each trying to
achieve better accuracy and caption quality. This paper introduces an
encoder-decoder-based image captioning system in which the encoder extracts
spatial and global features for each region in the image using the Faster R-CNN
with ResNet-101 as a backbone. This stage is followed by a refining model,
which uses an attention-on-attention mechanism to extract the visual features
of the target image objects, then determine their interactions. The decoder
consists of an attention-based recurrent module and a reflective attention
module, which collaboratively apply attention to the visual and textual
features to enhance the decoder's ability to model long-term sequential
dependencies. Extensive experiments performed on two benchmark datasets, MSCOCO
and Flickr30K, show the effectiveness the proposed approach and the high
quality of the generated captions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Patch-level Representation Learning for Self-supervised Vision Transformers. (arXiv:2206.07990v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07990">
<div class="article-summary-box-inner">
<span><p>Recent self-supervised learning (SSL) methods have shown impressive results
in learning visual representations from unlabeled images. This paper aims to
improve their performance further by utilizing the architectural advantages of
the underlying neural network, as the current state-of-the-art visual pretext
tasks for SSL do not enjoy the benefit, i.e., they are architecture-agnostic.
In particular, we focus on Vision Transformers (ViTs), which have gained much
attention recently as a better architectural choice, often outperforming
convolutional networks for various visual tasks. The unique characteristic of
ViT is that it takes a sequence of disjoint patches from an image and processes
patch-level representations internally. Inspired by this, we design a simple
yet effective visual pretext task, coined SelfPatch, for learning better
patch-level representations. To be specific, we enforce invariance against each
patch and its neighbors, i.e., each patch treats similar neighboring patches as
positive samples. Consequently, training ViTs with SelfPatch learns more
semantically meaningful relations among patches (without using human-annotated
labels), which can be beneficial, in particular, to downstream tasks of a dense
prediction type. Despite its simplicity, we demonstrate that it can
significantly improve the performance of existing SSL methods for various
visual tasks, including object detection and semantic segmentation.
Specifically, SelfPatch significantly improves the recent self-supervised ViT,
DINO, by achieving +1.3 AP on COCO object detection, +1.2 AP on COCO instance
segmentation, and +2.9 mIoU on ADE20K semantic segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Class-Affinity Loss Correction for Robust Medical Image Segmentation with Noisy Labels. (arXiv:2206.07994v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07994">
<div class="article-summary-box-inner">
<span><p>Noisy labels collected with limited annotation cost prevent medical image
segmentation algorithms from learning precise semantic correlations. Previous
segmentation arts of learning with noisy labels merely perform a pixel-wise
manner to preserve semantics, such as pixel-wise label correction, but neglect
the pair-wise manner. In fact, we observe that the pair-wise manner capturing
affinity relations between pixels can greatly reduce the label noise rate.
Motivated by this observation, we present a novel perspective for noisy
mitigation by incorporating both pixel-wise and pair-wise manners, where
supervisions are derived from noisy class and affinity labels, respectively.
Unifying the pixel-wise and pair-wise manners, we propose a robust Joint
Class-Affinity Segmentation (JCAS) framework to combat label noise issues in
medical image segmentation. Considering the affinity in pair-wise manner
incorporates contextual dependencies, a differentiated affinity reasoning (DAR)
module is devised to rectify the pixel-wise segmentation prediction by
reasoning about intra-class and inter-class affinity relations. To further
enhance the noise resistance, a class-affinity loss correction (CALC) strategy
is designed to correct supervision signals via the modeled noise label
distributions in class and affinity labels. Meanwhile, CALC strategy interacts
the pixel-wise and pair-wise manners through the theoretically derived
consistency regularization. Extensive experiments under both synthetic and
real-world noisy labels corroborate the efficacy of the proposed JCAS framework
with a minimum gap towards the upper bound performance. The source code is
available at \url{https://github.com/CityU-AIM-Group/JCAS}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Balancing Discriminability and Transferability for Source-Free Domain Adaptation. (arXiv:2206.08009v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08009">
<div class="article-summary-box-inner">
<span><p>Conventional domain adaptation (DA) techniques aim to improve domain
transferability by learning domain-invariant representations; while
concurrently preserving the task-discriminability knowledge gathered from the
labeled source data. However, the requirement of simultaneous access to labeled
source and unlabeled target renders them unsuitable for the challenging
source-free DA setting. The trivial solution of realizing an effective original
to generic domain mapping improves transferability but degrades task
discriminability. Upon analyzing the hurdles from both theoretical and
empirical standpoints, we derive novel insights to show that a mixup between
original and corresponding translated generic samples enhances the
discriminability-transferability trade-off while duly respecting the
privacy-oriented source-free setting. A simple but effective realization of the
proposed insights on top of the existing source-free DA approaches yields
state-of-the-art performance with faster convergence. Beyond single-source, we
also outperform multi-source prior-arts across both classification and semantic
segmentation benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MoDi: Unconditional Motion Synthesis from Diverse Data. (arXiv:2206.08010v1 [cs.GR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08010">
<div class="article-summary-box-inner">
<span><p>The emergence of neural networks has revolutionized the field of motion
synthesis. Yet, learning to unconditionally synthesize motions from a given
distribution remains a challenging task, especially when the motions are highly
diverse. We present MoDi, an unconditional generative model that synthesizes
diverse motions. Our model is trained in a completely unsupervised setting from
a diverse, unstructured and unlabeled motion dataset and yields a well-behaved,
highly semantic latent space. The design of our model follows the prolific
architecture of StyleGAN and adapts two of its key technical components into
the motion domain: a set of style-codes injected into each level of the
generator hierarchy and a mapping function that learns and forms a disentangled
latent space. We show that despite the lack of any structure in the dataset,
the latent space can be semantically clustered, and facilitates semantic
editing and motion interpolation. In addition, we propose a technique to invert
unseen motions into the latent space, and demonstrate latent-based motion
editing operations that otherwise cannot be achieved by naive manipulation of
explicit motion representations. Our qualitative and quantitative experiments
show that our framework achieves state-of-the-art synthesis quality that can
follow the distribution of highly diverse motion datasets. Code and trained
models will be released at https://sigal-raab.github.io/MoDi.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Backbones-Review: Feature Extraction Networks for Deep Learning and Deep Reinforcement Learning Approaches. (arXiv:2206.08016v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08016">
<div class="article-summary-box-inner">
<span><p>To understand the real world using various types of data, Artificial
Intelligence (AI) is the most used technique nowadays. While finding the
pattern within the analyzed data represents the main task. This is performed by
extracting representative features step, which is proceeded using the
statistical algorithms or using some specific filters. However, the selection
of useful features from large-scale data represented a crucial challenge. Now,
with the development of convolution neural networks (CNNs), the feature
extraction operation has become more automatic and easier. CNNs allow to work
on large-scale size of data, as well as cover different scenarios for a
specific task. For computer vision tasks, convolutional networks are used to
extract features also for the other parts of a deep learning model. The
selection of a suitable network for feature extraction or the other parts of a
DL model is not random work. So, the implementation of such a model can be
related to the target task as well as the computational complexity of it. Many
networks have been proposed and become the famous networks used for any DL
models in any AI task. These networks are exploited for feature extraction or
at the beginning of any DL model which is named backbones. A backbone is a
known network trained in many other tasks before and demonstrates its
effectiveness. In this paper, an overview of the existing backbones, e.g. VGGs,
ResNets, DenseNet, etc, is given with a detailed description. Also, a couple of
computer vision tasks are discussed by providing a review of each task
regarding the backbones used. In addition, a comparison in terms of performance
is also provided, based on the backbone used for each task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-View Imputation and Cross-Attention Network Based on Incomplete Longitudinal and Multi-Modal Data for Alzheimer's Disease Prediction. (arXiv:2206.08019v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08019">
<div class="article-summary-box-inner">
<span><p>Longitudinal variations and complementary information inherent in
longitudinal and multi-modal data play an important role in Alzheimer's disease
(AD) prediction, particularly in identifying subjects with mild cognitive
impairment who are about to have AD. However, longitudinal and multi-modal data
may have missing data, which hinders the effective application of these data.
Additionally, previous longitudinal studies require existing longitudinal data
to achieve prediction, but AD prediction is expected to be conducted at
patients' baseline visit (BL) in clinical practice. Thus, we proposed a
multi-view imputation and cross-attention network (MCNet) to integrate data
imputation and AD prediction in a unified framework and achieve accurate AD
prediction. First, a multi-view imputation method combined with adversarial
learning, which can handle a wide range of missing data situations and reduce
imputation errors, was presented. Second, two cross-attention blocks were
introduced to exploit the potential associations in longitudinal and
multi-modal data. Finally, a multi-task learning model was built for data
imputation, longitudinal classification, and AD prediction tasks. When the
model was properly trained, the disease progression information learned from
longitudinal data can be leveraged by BL data to improve AD prediction. The
proposed method was tested on two independent testing sets and single-model
data at BL to verify its effectiveness and flexibility on AD prediction.
Results showed that MCNet outperformed several state-of-the-art methods.
Moreover, the interpretability of MCNet was presented. Thus, our MCNet is a
tool with a great application potential in longitudinal and multi-modal data
analysis for AD prediction. Codes are available at
https://github.com/Meiyan88/MCNET.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AMOS: A Large-Scale Abdominal Multi-Organ Benchmark for Versatile Medical Image Segmentation. (arXiv:2206.08023v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08023">
<div class="article-summary-box-inner">
<span><p>Despite the considerable progress in automatic abdominal multi-organ
segmentation from CT/MRI scans in recent years, a comprehensive evaluation of
the models' capabilities is hampered by the lack of a large-scale benchmark
from diverse clinical scenarios. Constraint by the high cost of collecting and
labeling 3D medical data, most of the deep learning models to date are driven
by datasets with a limited number of organs of interest or samples, which still
limits the power of modern deep models and makes it difficult to provide a
fully comprehensive and fair estimate of various methods. To mitigate the
limitations, we present AMOS, a large-scale, diverse, clinical dataset for
abdominal organ segmentation. AMOS provides 500 CT and 100 MRI scans collected
from multi-center, multi-vendor, multi-modality, multi-phase, multi-disease
patients, each with voxel-level annotations of 15 abdominal organs, providing
challenging examples and test-bed for studying robust segmentation algorithms
under diverse targets and scenarios. We further benchmark several
state-of-the-art medical segmentation models to evaluate the status of the
existing methods on this new challenging dataset. We have made our datasets,
benchmark servers, and baselines publicly available, and hope to inspire future
research. Information can be found at https://amos22.grand-challenge.org.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepFormableTag: End-to-end Generation and Recognition of Deformable Fiducial Markers. (arXiv:2206.08026v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08026">
<div class="article-summary-box-inner">
<span><p>Fiducial markers have been broadly used to identify objects or embed messages
that can be detected by a camera. Primarily, existing detection methods assume
that markers are printed on ideally planar surfaces. Markers often fail to be
recognized due to various imaging artifacts of optical/perspective distortion
and motion blur. To overcome these limitations, we propose a novel deformable
fiducial marker system that consists of three main parts: First, a fiducial
marker generator creates a set of free-form color patterns to encode
significantly large-scale information in unique visual codes. Second, a
differentiable image simulator creates a training dataset of photorealistic
scene images with the deformed markers, being rendered during optimization in a
differentiable manner. The rendered images include realistic shading with
specular reflection, optical distortion, defocus and motion blur, color
alteration, imaging noise, and shape deformation of markers. Lastly, a trained
marker detector seeks the regions of interest and recognizes multiple marker
patterns simultaneously via inverse deformation transformation. The deformable
marker creator and detector networks are jointly optimized via the
differentiable photorealistic renderer in an end-to-end manner, allowing us to
robustly recognize a wide range of deformable markers with high accuracy. Our
deformable marker system is capable of decoding 36-bit messages successfully at
~29 fps with severe shape deformation. Results validate that our system
significantly outperforms the traditional and data-driven marker methods. Our
learning-based marker system opens up new interesting applications of fiducial
markers, including cost-effective motion capture of the human body, active 3D
scanning using our fiducial markers' array as structured light patterns, and
robust augmented reality rendering of virtual objects on dynamic surfaces.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Effect of Lay People in Gesture-Based Locomotion in Virtual Reality. (arXiv:2206.08076v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08076">
<div class="article-summary-box-inner">
<span><p>Locomotion in Virtual Reality (VR) is an important part of VR applications.
Many scientists are enriching the community with different variations that
enable locomotion in VR. Some of the most promising methods are gesture-based
and do not require additional handheld hardware. Recent work focused mostly on
user preference and performance of the different locomotion techniques. This
ignores the learning effect that users go through while new methods are being
explored. In this work, it is investigated whether and how quickly users can
adapt to a hand gesture-based locomotion system in VR. Four different
locomotion techniques are implemented and tested by participants. The goal of
this paper is twofold: First, it aims to encourage researchers to consider the
learning effect in their studies. Second, this study aims to provide insight
into the learning effect of users in gesture-based systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Scene Representation for Locomotion on Structured Terrain. (arXiv:2206.08077v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08077">
<div class="article-summary-box-inner">
<span><p>We propose a learning-based method to reconstruct the local terrain for
locomotion with a mobile robot traversing urban environments. Using a stream of
depth measurements from the onboard cameras and the robot's trajectory, the
algorithm estimates the topography in the robot's vicinity. The raw
measurements from these cameras are noisy and only provide partial and occluded
observations that in many cases do not show the terrain the robot stands on.
Therefore, we propose a 3D reconstruction model that faithfully reconstructs
the scene, despite the noisy measurements and large amounts of missing data
coming from the blind spots of the camera arrangement. The model consists of a
4D fully convolutional network on point clouds that learns the geometric priors
to complete the scene from the context and an auto-regressive feedback to
leverage spatio-temporal consistency and use evidence from the past. The
network can be solely trained with synthetic data, and due to extensive
augmentation, it is robust in the real world, as shown in the validation on a
quadrupedal robot, ANYmal, traversing challenging settings. We run the pipeline
on the robot's onboard low-power computer using an efficient sparse tensor
implementation and show that the proposed method outperforms classical map
representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">U-PET: MRI-based Dementia Detection with Joint Generation of Synthetic FDG-PET Images. (arXiv:2206.08078v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08078">
<div class="article-summary-box-inner">
<span><p>Alzheimer's disease (AD) is the most common cause of dementia. An early
detection is crucial for slowing down the disease and mitigating risks related
to the progression. While the combination of MRI and FDG-PET is the best
image-based tool for diagnosis, FDG-PET is not always available. The reliable
detection of Alzheimer's disease with only MRI could be beneficial, especially
in regions where FDG-PET might not be affordable for all patients. To this end,
we propose a multi-task method based on U-Net that takes T1-weighted MR images
as an input to generate synthetic FDG-PET images and classifies the dementia
progression of the patient into cognitive normal (CN), cognitive impairment
(MCI), and AD. The attention gates used in both task heads can visualize the
most relevant parts of the brain, guiding the examiner and adding
interpretability. Results show the successful generation of synthetic FDG-PET
images and a performance increase in disease classification over the naive
single-task baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CARLANE: A Lane Detection Benchmark for Unsupervised Domain Adaptation from Simulation to multiple Real-World Domains. (arXiv:2206.08083v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08083">
<div class="article-summary-box-inner">
<span><p>Unsupervised Domain Adaptation demonstrates great potential to mitigate
domain shifts by transferring models from labeled source domains to unlabeled
target domains. While Unsupervised Domain Adaptation has been applied to a wide
variety of complex vision tasks, only few works focus on lane detection for
autonomous driving. This can be attributed to the lack of publicly available
datasets. To facilitate research in these directions, we propose CARLANE, a
3-way sim-to-real domain adaptation benchmark for 2D lane detection. CARLANE
encompasses the single-target datasets MoLane and TuLane and the multi-target
dataset MuLane. These datasets are built from three different domains, which
cover diverse scenes and contain a total of 163K unique images, 118K of which
are annotated. In addition we evaluate and report systematic baselines,
including our own method, which builds upon Prototypical Cross-domain
Self-supervised Learning. We find that false positive and false negative rates
of the evaluated domain adaptation methods are high compared to those of fully
supervised baselines. This affirms the need for benchmarks such as CARLANE to
further strengthen research in Unsupervised Domain Adaptation for lane
detection. CARLANE, all evaluated models and the corresponding implementations
are publicly available at https://carlanebenchmark.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Improved Normed-Deformable Convolution for Crowd Counting. (arXiv:2206.08084v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08084">
<div class="article-summary-box-inner">
<span><p>In recent years, crowd counting has become an important issue in computer
vision. In most methods, the density maps are generated by convolving with a
Gaussian kernel from the ground-truth dot maps which are marked around the
center of human heads. Due to the fixed geometric structures in CNNs and
indistinct head-scale information, the head features are obtained incompletely.
Deformable convolution is proposed to exploit the scale-adaptive capabilities
for CNN features in the heads. By learning the coordinate offsets of the
sampling points, it is tractable to improve the ability to adjust the receptive
field. However, the heads are not uniformly covered by the sampling points in
the deformable convolution, resulting in loss of head information. To handle
the non-uniformed sampling, an improved Normed-Deformable Convolution
(\textit{i.e.,}NDConv) implemented by Normed-Deformable loss
(\textit{i.e.,}NDloss) is proposed in this paper. The offsets of the sampling
points which are constrained by NDloss tend to be more even. Then, the features
in the heads are obtained more completely, leading to better performance.
Especially, the proposed NDConv is a light-weight module which shares similar
computation burden with Deformable Convolution. In the extensive experiments,
our method outperforms state-of-the-art methods on ShanghaiTech A, ShanghaiTech
B, UCF\_QNRF, and UCF\_CC\_50 dataset, achieving 61.4, 7.8, 91.2, and 167.2
MAE, respectively. The code is available at
https://github.com/bingshuangzhuzi/NDConv
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simple Baseline for Adversarial Domain Adaptation-based Unsupervised Flood Forecasting. (arXiv:2206.08105v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08105">
<div class="article-summary-box-inner">
<span><p>Flood disasters cause enormous social and economic losses. However, both
traditional physical models and learning-based flood forecasting models require
massive historical flood data to train the model parameters. When come to some
new site that does not have sufficient historical data, the model performance
will drop dramatically due to overfitting. This technical report presents a
Flood Domain Adaptation Network (FloodDAN), a baseline of applying Unsupervised
Domain Adaptation (UDA) to the flood forecasting problem. Specifically,
training of FloodDAN includes two stages: in the first stage, we train a
rainfall encoder and a prediction head to learn general transferable
hydrological knowledge on large-scale source domain data; in the second stage,
we transfer the knowledge in the pretrained encoder into the rainfall encoder
of target domain through adversarial domain alignment. During inference, we
utilize the target domain rainfall encoder trained in the second stage and the
prediction head trained in the first stage to get flood forecasting
predictions. Experimental results on Tunxi and Changhua flood dataset show that
FloodDAN can perform flood forecasting effectively with zero target domain
supervision. The performance of the FloodDAN is on par with supervised models
that uses 450-500 hours of supervision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Channel Importance Matters in Few-Shot Image Classification. (arXiv:2206.08126v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08126">
<div class="article-summary-box-inner">
<span><p>Few-Shot Learning (FSL) requires vision models to quickly adapt to brand-new
classification tasks with a shift in task distribution. Understanding the
difficulties posed by this task distribution shift is central to FSL. In this
paper, we show that a simple channel-wise feature transformation may be the key
to unraveling this secret from a channel perspective. When facing novel
few-shot tasks in the test-time datasets, this transformation can greatly
improve the generalization ability of learned image representations, while
being agnostic to the choice of training algorithms and datasets. Through an
in-depth analysis of this transformation, we find that the difficulty of
representation transfer in FSL stems from the severe channel bias problem of
image representations: channels may have different importance in different
tasks, while convolutional neural networks are likely to be insensitive, or
respond incorrectly to such a shift. This points out a core problem of the
generalization ability of modern vision systems and needs further attention in
the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Trajectory-guided Control Prediction for End-to-end Autonomous Driving: A Simple yet Strong Baseline. (arXiv:2206.08129v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08129">
<div class="article-summary-box-inner">
<span><p>Current end-to-end autonomous driving methods either run a controller based
on a planned trajectory or perform control prediction directly, which have
spanned two separately studied lines of research. Seeing their potential mutual
benefits to each other, this paper takes the initiative to explore the
combination of these two well-developed worlds. Specifically, our integrated
approach has two branches for trajectory planning and direct control,
respectively. The trajectory branch predicts the future trajectory, while the
control branch involves a novel multi-step prediction scheme such that the
relationship between current actions and future states can be reasoned. The two
branches are connected so that the control branch receives corresponding
guidance from the trajectory branch at each time step. The outputs from two
branches are then fused to achieve complementary advantages. Our results are
evaluated in the closed-loop urban driving setting with challenging scenarios
using the CARLA simulator. Even with a monocular camera input, the proposed
approach ranks $first$ on the official CARLA Leaderboard, outperforming other
complex candidates with multiple sensors or fusion mechanisms by a large
margin. The source code and data will be made publicly available at
https://github.com/OpenPerceptionX/TCP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lessons learned from the NeurIPS 2021 MetaDL challenge: Backbone fine-tuning without episodic meta-learning dominates for few-shot learning image classification. (arXiv:2206.08138v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08138">
<div class="article-summary-box-inner">
<span><p>Although deep neural networks are capable of achieving performance superior
to humans on various tasks, they are notorious for requiring large amounts of
data and computing resources, restricting their success to domains where such
resources are available. Metalearning methods can address this problem by
transferring knowledge from related tasks, thus reducing the amount of data and
computing resources needed to learn new tasks. We organize the MetaDL
competition series, which provide opportunities for research groups all over
the world to create and experimentally assess new meta-(deep)learning solutions
for real problems. In this paper, authored collaboratively between the
competition organizers and the top-ranked participants, we describe the design
of the competition, the datasets, the best experimental results, as well as the
top-ranked methods in the NeurIPS 2021 challenge, which attracted 15 active
teams who made it to the final phase (by outperforming the baseline), making
over 100 code submissions during the feedback phase. The solutions of the top
participants have been open-sourced. The lessons learned include that learning
good representations is essential for effective transfer learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Adaptive Label Augmentation for Semi-supervised Few-shot Classification. (arXiv:2206.08150v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08150">
<div class="article-summary-box-inner">
<span><p>Few-shot classification aims to learn a model that can generalize well to new
tasks when only a few labeled samples are available. To make use of unlabeled
data that are more abundantly available in real applications, Ren et al.
\shortcite{ren2018meta} propose a semi-supervised few-shot classification
method that assigns an appropriate label to each unlabeled sample by a manually
defined metric. However, the manually defined metric fails to capture the
intrinsic property in data. In this paper, we propose a
\textbf{S}elf-\textbf{A}daptive \textbf{L}abel \textbf{A}ugmentation approach,
called \textbf{SALA}, for semi-supervised few-shot classification. A major
novelty of SALA is the task-adaptive metric, which can learn the metric
adaptively for different tasks in an end-to-end fashion. Another appealing
feature of SALA is a progressive neighbor selection strategy, which selects
unlabeled data with high confidence progressively through the training phase.
Experiments demonstrate that SALA outperforms several state-of-the-art methods
for semi-supervised few-shot classification on benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Video Question Answering via Frozen Bidirectional Language Models. (arXiv:2206.08155v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08155">
<div class="article-summary-box-inner">
<span><p>Video question answering (VideoQA) is a complex task that requires diverse
multi-modal data for training. Manual annotation of question and answers for
videos, however, is tedious and prohibits scalability. To tackle this problem,
recent methods consider zero-shot settings with no manual annotation of visual
question-answer. In particular, a promising approach adapts frozen
autoregressive language models pretrained on Web-scale text-only data to
multi-modal inputs. In contrast, we here build on frozen bidirectional language
models (BiLM) and show that such an approach provides a stronger and cheaper
alternative for zero-shot VideoQA. In particular, (i) we combine visual inputs
with the frozen BiLM using light trainable modules, (ii) we train such modules
using Web-scraped multi-modal data, and finally (iii) we perform zero-shot
VideoQA inference through masked language modeling, where the masked text is
the answer to a given question. Our proposed approach, FrozenBiLM, outperforms
the state of the art in zero-shot VideoQA by a significant margin on a variety
of datasets, including LSMDC-FiB, iVQA, MSRVTT-QA, MSVD-QA, ActivityNet-QA,
TGIF-FrameQA, How2QA and TVQA. It also demonstrates competitive performance in
the few-shot and fully-supervised setting. Our code and models will be made
publicly available at https://antoyang.github.io/frozenbilm.html.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Volumetric Supervised Contrastive Learning for Seismic Semantic Segmentation. (arXiv:2206.08158v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08158">
<div class="article-summary-box-inner">
<span><p>In seismic interpretation, pixel-level labels of various rock structures can
be time-consuming and expensive to obtain. As a result, there oftentimes exists
a non-trivial quantity of unlabeled data that is left unused simply because
traditional deep learning methods rely on access to fully labeled volumes. To
rectify this problem, contrastive learning approaches have been proposed that
use a self-supervised methodology in order to learn useful representations from
unlabeled data. However, traditional contrastive learning approaches are based
on assumptions from the domain of natural images that do not make use of
seismic context. In order to incorporate this context within contrastive
learning, we propose a novel positive pair selection strategy based on the
position of slices within a seismic volume. We show that the learnt
representations from our method out-perform a state of the art contrastive
learning methodology in a semantic segmentation task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">K-Radar: 4D Radar Object Detection Dataset and Benchmark for Autonomous Driving in Various Weather Conditions. (arXiv:2206.08171v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08171">
<div class="article-summary-box-inner">
<span><p>Unlike RGB cameras that use visible light bands (384$\sim$769 THz) and Lidar
that use infrared bands (361$\sim$331 THz), Radars use relatively longer
wavelength radio bands (77$\sim$81 GHz), resulting in robust measurements in
adverse weathers. Unfortunately, existing Radar datasets only contain a
relatively small number of samples compared to the existing camera and Lidar
datasets. This may hinder the development of sophisticated data-driven deep
learning techniques for Radar-based perception. Moreover, most of the existing
Radar datasets only provide 3D Radar tensor (3DRT) data that contain power
measurements along the Doppler, range, and azimuth dimensions. As there is no
elevation information, it is challenging to estimate the 3D bounding box of an
object from 3DRT. In this work, we introduce KAIST-Radar (K-Radar), a novel
large-scale object detection dataset and benchmark that contains 35K frames of
4D Radar tensor (4DRT) data with power measurements along the Doppler, range,
azimuth, and elevation dimensions, together with carefully annotated 3D
bounding box labels of objects on the roads. K-Radar includes challenging
driving conditions such as adverse weathers (fog, rain, and snow) on various
road structures (urban, suburban roads, alleyways, and highways). In addition
to the 4DRT, we provide auxiliary measurements from carefully calibrated
high-resolution Lidars, surround stereo cameras, and RTK-GPS. We also provide
4DRT-based object detection baseline neural networks (baseline NNs) and show
that the height information is crucial for 3D object detection. And by
comparing the baseline NN with a similarly-structured Lidar-based neural
network, we demonstrate that 4D Radar is a more robust sensor for adverse
weather conditions. All codes are available at
https://github.com/kaist-avelab/k-radar.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RefCrowd: Grounding the Target in Crowd with Referring Expressions. (arXiv:2206.08172v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08172">
<div class="article-summary-box-inner">
<span><p>Crowd understanding has aroused the widespread interest in vision domain due
to its important practical significance. Unfortunately, there is no effort to
explore crowd understanding in multi-modal domain that bridges natural language
and computer vision. Referring expression comprehension (REF) is such a
representative multi-modal task. Current REF studies focus more on grounding
the target object from multiple distinctive categories in general scenarios. It
is difficult to applied to complex real-world crowd understanding. To fill this
gap, we propose a new challenging dataset, called RefCrowd, which towards
looking for the target person in crowd with referring expressions. It not only
requires to sufficiently mine the natural language information, but also
requires to carefully focus on subtle differences between the target and a
crowd of persons with similar appearance, so as to realize the fine-grained
mapping from language to vision. Furthermore, we propose a Fine-grained
Multi-modal Attribute Contrastive Network (FMAC) to deal with REF in crowd
understanding. It first decomposes the intricate visual and language features
into attribute-aware multi-modal features, and then captures discriminative but
robustness fine-grained attribute features to effectively distinguish these
subtle differences between similar persons. The proposed method outperforms
existing state-of-the-art (SoTA) methods on our RefCrowd dataset and existing
REF datasets. In addition, we implement an end-to-end REF toolbox for the
deeper research in multi-modal domain. Our dataset and code can be available
at: \url{https://qiuheqian.github.io/datasets/refcrowd/}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Level 2 Autonomous Driving on a Single Device: Diving into the Devils of Openpilot. (arXiv:2206.08176v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08176">
<div class="article-summary-box-inner">
<span><p>Equipped with a wide span of sensors, predominant autonomous driving
solutions are becoming more modular-oriented for safe system design. Though
these sensors have laid a solid foundation, most massive-production solutions
up to date still fall into L2 phase. Among these, Comma.ai comes to our sight,
claiming one $999 aftermarket device mounted with a single camera and board
inside owns the ability to handle L2 scenarios. Together with open-sourced
software of the entire system released by Comma.ai, the project is named
Openpilot. Is it possible? If so, how is it made possible? With curiosity in
mind, we deep-dive into Openpilot and conclude that its key to success is the
end-to-end system design instead of a conventional modular framework. The model
is briefed as Supercombo, and it can predict the ego vehicle's future
trajectory and other road semantics on the fly from monocular input.
Unfortunately, the training process and massive amount of data to make all
these work are not publicly available. To achieve an intensive investigation,
we try to reimplement the training details and test the pipeline on public
benchmarks. The refactored network proposed in this work is referred to as
OP-Deepdive. For a fair comparison of our version to the original Supercombo,
we introduce a dual-model deployment scheme to test the driving performance in
the real world. Experimental results on nuScenes, Comma2k19, CARLA, and
in-house realistic scenarios verify that a low-cost device can indeed achieve
most L2 functionalities and be on par with the original Supercombo model. In
this report, we would like to share our latest findings, shed some light on the
new perspective of end-to-end autonomous driving from an industrial
product-level side, and potentially inspire the community to continue improving
the performance. Our code, benchmarks are at
https://github.com/OpenPerceptionX/Openpilot-Deepdive.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nucleus Segmentation and Analysis in Breast Cancer with the MIScnn Framework. (arXiv:2206.08182v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08182">
<div class="article-summary-box-inner">
<span><p>The NuCLS dataset contains over 220.000 annotations of cell nuclei in breast
cancers. We show how to use these data to create a multi-rater model with the
MIScnn Framework to automate the analysis of cell nuclei. For the model
creation, we use the widespread U-Net approach embedded in a pipeline. This
pipeline provides besides the high performance convolution neural network,
several preprocessor techniques and a extended data exploration. The final
model is tested in the evaluation phase using a wide variety of metrics with a
subsequent visualization. Finally, the results are compared and interpreted
with the results of the NuCLS study. As an outlook, indications are given which
are important for the future development of models in the context of cell
nuclei.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Asymptotic Soft Cluster Pruning for Deep Neural Networks. (arXiv:2206.08186v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08186">
<div class="article-summary-box-inner">
<span><p>Filter pruning method introduces structural sparsity by removing selected
filters and is thus particularly effective for reducing complexity. Previous
works empirically prune networks from the point of view that filter with
smaller norm contributes less to the final results. However, such criteria has
been proven sensitive to the distribution of filters, and the accuracy may hard
to recover since the capacity gap is fixed once pruned. In this paper, we
propose a novel filter pruning method called Asymptotic Soft Cluster Pruning
(ASCP), to identify the redundancy of network based on the similarity of
filters. Each filter from over-parameterized network is first distinguished by
clustering, and then reconstructed to manually introduce redundancy into it.
Several guidelines of clustering are proposed to better preserve feature
extraction ability. After reconstruction, filters are allowed to be updated to
eliminate the effect caused by mistakenly selected. Besides, various decaying
strategies of the pruning rate are adopted to stabilize the pruning process and
improve the final performance as well. By gradually generating more identical
filters within each cluster, ASCP can remove them through channel addition
operation with almost no accuracy drop. Extensive experiments on CIFAR-10 and
ImageNet datasets show that our method can achieve competitive results compared
with many state-of-the-art algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Online Segmentation of LiDAR Sequences: Dataset and Algorithm. (arXiv:2206.08194v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08194">
<div class="article-summary-box-inner">
<span><p>Roof-mounted spinning LiDAR sensors are widely used by autonomous vehicles,
driving the need for real-time processing of 3D point sequences. However, most
LiDAR semantic segmentation datasets and algorithms split these acquisitions
into $360^\circ$ frames, leading to acquisition latency that is incompatible
with realistic real-time applications and evaluations. We address this issue
with two key contributions. First, we introduce HelixNet, a $10$ billion point
dataset with fine-grained labels, timestamps, and sensor rotation information
that allows an accurate assessment of real-time readiness of segmentation
algorithms. Second, we propose Helix4D, a compact and efficient spatio-temporal
transformer architecture specifically designed for rotating LiDAR point
sequences. Helix4D operates on acquisition slices that correspond to a fraction
of a full rotation of the sensor, significantly reducing the total latency. We
present an extensive benchmark of the performance and real-time readiness of
several state-of-the-art models on HelixNet and SemanticKITTI. Helix4D reaches
accuracy on par with the best segmentation algorithms with a reduction of more
than $5\times$ in terms of latency and $50\times$ in model size. Code and data
are available at: https://romainloiseau.fr/helixnet
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Selective Multi-Scale Learning for Object Detection. (arXiv:2206.08206v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08206">
<div class="article-summary-box-inner">
<span><p>Pyramidal networks are standard methods for multi-scale object detection.
Current researches on feature pyramid networks usually adopt layer connections
to collect features from certain levels of the feature hierarchy, and do not
consider the significant differences among them. We propose a better
architecture of feature pyramid networks, named selective multi-scale learning
(SMSL), to address this issue. SMSL is efficient and general, which can be
integrated in both single-stage and two-stage detectors to boost detection
performance, with nearly no extra inference cost. RetinaNet combined with SMSL
obtains 1.8\% improvement in AP (from 39.1\% to 40.9\%) on COCO dataset. When
integrated with SMSL, two-stage detectors can get around 1.0\% improvement in
AP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Closer Look at Smoothness in Domain Adversarial Training. (arXiv:2206.08213v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08213">
<div class="article-summary-box-inner">
<span><p>Domain adversarial training has been ubiquitous for achieving invariant
representations and is used widely for various domain adaptation tasks. In
recent times, methods converging to smooth optima have shown improved
generalization for supervised learning tasks like classification. In this work,
we analyze the effect of smoothness enhancing formulations on domain
adversarial training, the objective of which is a combination of task loss (eg.
classification, regression, etc.) and adversarial terms. We find that
converging to a smooth minima with respect to (w.r.t.) task loss stabilizes the
adversarial training leading to better performance on target domain. In
contrast to task loss, our analysis shows that converging to smooth minima
w.r.t. adversarial loss leads to sub-optimal generalization on the target
domain. Based on the analysis, we introduce the Smooth Domain Adversarial
Training (SDAT) procedure, which effectively enhances the performance of
existing domain adversarial methods for both classification and object
detection tasks. Our analysis also provides insight into the extensive usage of
SGD over Adam in the community for domain adversarial training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HaGRID - HAnd Gesture Recognition Image Dataset. (arXiv:2206.08219v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08219">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce an enormous dataset HaGRID (HAnd Gesture
Recognition Image Dataset) for hand gesture recognition (HGR) systems. This
dataset contains 552,992 samples divided into 18 classes of gestures. The
annotations consist of bounding boxes of hands with gesture labels and markups
of leading hands. The proposed dataset allows for building HGR systems, which
can be used in video conferencing services, home automation systems, the
automotive sector, services for people with speech and hearing impairments,
etc. We are especially focused on interaction with devices to manage them. That
is why all 18 chosen gestures are functional, familiar to the majority of
people, and may be an incentive to take some action. In addition, we used
crowdsourcing platforms to collect the dataset and took into account various
parameters to ensure data diversity. We describe the challenges of using
existing HGR datasets for our task and provide a detailed overview of them.
Furthermore, the baselines for the hand detection and gesture classification
tasks are proposed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adapting Self-Supervised Vision Transformers by Probing Attention-Conditioned Masking Consistency. (arXiv:2206.08222v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08222">
<div class="article-summary-box-inner">
<span><p>Visual domain adaptation (DA) seeks to transfer trained models to unseen,
unlabeled domains across distribution shift, but approaches typically focus on
adapting convolutional neural network architectures initialized with supervised
ImageNet representations. In this work, we shift focus to adapting modern
architectures for object recognition -- the increasingly popular Vision
Transformer (ViT) -- and modern pretraining based on self-supervised learning
(SSL). Inspired by the design of recent SSL approaches based on learning from
partial image inputs generated via masking or cropping -- either by learning to
predict the missing pixels, or learning representational invariances to such
augmentations -- we propose PACMAC, a simple two-stage adaptation algorithm for
self-supervised ViTs. PACMAC first performs in-domain SSL on pooled source and
target data to learn task-discriminative features, and then probes the model's
predictive consistency across a set of partial target inputs generated via a
novel attention-conditioned masking strategy, to identify reliable candidates
for self-training. Our simple approach leads to consistent performance gains
over competing methods that use ViTs and self-supervised initializations on
standard object recognition benchmarks. Code available at
https://github.com/virajprabhu/PACMAC
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi scale Feature Extraction and Fusion for Online Knowledge Distillation. (arXiv:2206.08224v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08224">
<div class="article-summary-box-inner">
<span><p>Online knowledge distillation conducts knowledge transfer among all student
models to alleviate the reliance on pre-trained models. However, existing
online methods rely heavily on the prediction distributions and neglect the
further exploration of the representational knowledge. In this paper, we
propose a novel Multi-scale Feature Extraction and Fusion method (MFEF) for
online knowledge distillation, which comprises three key components:
Multi-scale Feature Extraction, Dual-attention and Feature Fusion, towards
generating more informative feature maps for distillation. The multiscale
feature extraction exploiting divide-and-concatenate in channel dimension is
proposed to improve the multi-scale representation ability of feature maps. To
obtain more accurate information, we design a dual-attention to strengthen the
important channel and spatial regions adaptively. Moreover, we aggregate and
fuse the former processed feature maps via feature fusion to assist the
training of student models. Extensive experiments on CIF AR-10, CIF AR-100, and
CINIC-10 show that MFEF transfers more beneficial representational knowledge
for distillation and outperforms alternative methods among various network
architectures
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Delving into the Scale Variance Problem in Object Detection. (arXiv:2206.08227v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08227">
<div class="article-summary-box-inner">
<span><p>Object detection has made substantial progress in the last decade, due to the
capability of convolution in extracting local context of objects. However, the
scales of objects are diverse and current convolution can only process
single-scale input. The capability of traditional convolution with a fixed
receptive field in dealing with such a scale variance problem, is thus limited.
Multi-scale feature representation has been proven to be an effective way to
mitigate the scale variance problem. Recent researches mainly adopt partial
connection with certain scales, or aggregate features from all scales and focus
on the global information across the scales. However, the information across
spatial and depth dimensions is ignored. Inspired by this, we propose the
multi-scale convolution (MSConv) to handle this problem. Taking into
consideration scale, spatial and depth information at the same time, MSConv is
able to process multi-scale input more comprehensively. MSConv is effective and
computationally efficient, with only a small increase of computational cost.
For most of the single-stage object detectors, replacing the traditional
convolutions with MSConvs in the detection head can bring more than 2.5\%
improvement in AP (on COCO 2017 dataset), with only 3\% increase of FLOPs.
MSConv is also flexible and effective for two-stage object detectors. When
extended to the mainstream two-stage object detectors, MSConv can bring up to
3.0\% improvement in AP. Our best model under single-scale testing achieves
48.9\% AP on COCO 2017 \textit{test-dev} split, which surpasses many
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open-Set Recognition with Gradient-Based Representations. (arXiv:2206.08229v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08229">
<div class="article-summary-box-inner">
<span><p>Neural networks for image classification tasks assume that any given image
during inference belongs to one of the training classes. This closed-set
assumption is challenged in real-world applications where models may encounter
inputs of unknown classes. Open-set recognition aims to solve this problem by
rejecting unknown classes while classifying known classes correctly. In this
paper, we propose to utilize gradient-based representations obtained from a
known classifier to train an unknown detector with instances of known classes
only. Gradients correspond to the amount of model updates required to properly
represent a given sample, which we exploit to understand the model's capability
to characterize inputs with its learned features. Our approach can be utilized
with any classifier trained in a supervised manner on known classes without the
need to model the distribution of unknown samples explicitly. We show that our
gradient-based approach outperforms state-of-the-art methods by up to 11.6% in
open-set classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simple and Efficient Architectures for Semantic Segmentation. (arXiv:2206.08236v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08236">
<div class="article-summary-box-inner">
<span><p>Though the state-of-the architectures for semantic segmentation, such as
HRNet, demonstrate impressive accuracy, the complexity arising from their
salient design choices hinders a range of model acceleration tools, and further
they make use of operations that are inefficient on current hardware. This
paper demonstrates that a simple encoder-decoder architecture with a
ResNet-like backbone and a small multi-scale head, performs on-par or better
than complex semantic segmentation architectures such as HRNet, FANet and
DDRNets. Naively applying deep backbones designed for Image Classification to
the task of Semantic Segmentation leads to sub-par results, owing to a much
smaller effective receptive field of these backbones. Implicit among the
various design choices put forth in works like HRNet, DDRNet, and FANet are
networks with a large effective receptive field. It is natural to ask if a
simple encoder-decoder architecture would compare favorably if comprised of
backbones that have a larger effective receptive field, though without the use
of inefficient operations like dilated convolutions. We show that with minor
and inexpensive modifications to ResNets, enlarging the receptive field, very
simple and competitive baselines can be created for Semantic Segmentation. We
present a family of such simple architectures for desktop as well as mobile
targets, which match or exceed the performance of complex models on the
Cityscapes dataset. We hope that our work provides simple yet effective
baselines for practitioners to develop efficient semantic segmentation models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Catastrophic overfitting is a bug but also a feature. (arXiv:2206.08242v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08242">
<div class="article-summary-box-inner">
<span><p>Despite clear computational advantages in building robust neural networks,
adversarial training (AT) using single-step methods is unstable as it suffers
from catastrophic overfitting (CO): Networks gain non-trivial robustness during
the first stages of adversarial training, but suddenly reach a breaking point
where they quickly lose all robustness in just a few iterations. Although some
works have succeeded at preventing CO, the different mechanisms that lead to
this remarkable failure mode are still poorly understood. In this work,
however, we find that the interplay between the structure of the data and the
dynamics of AT plays a fundamental role in CO. Specifically, through active
interventions on typical datasets of natural images, we establish a causal link
between the structure of the data and the onset of CO in single-step AT
methods. This new perspective provides important insights into the mechanisms
that lead to CO and paves the way towards a better understanding of the general
dynamics of robust model construction. The code to reproduce the experiments of
this paper can be found at https://github.com/gortizji/co_features .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gradient-Based Adversarial and Out-of-Distribution Detection. (arXiv:2206.08255v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08255">
<div class="article-summary-box-inner">
<span><p>We propose to utilize gradients for detecting adversarial and
out-of-distribution samples. We introduce confounding labels -- labels that
differ from normal labels seen during training -- in gradient generation to
probe the effective expressivity of neural networks. Gradients depict the
amount of change required for a model to properly represent given inputs,
providing insight into the representational power of the model established by
network architectural properties as well as training data. By introducing a
label of different design, we remove the dependency on ground truth labels for
gradient generation during inference. We show that our gradient-based approach
allows for capturing the anomaly in inputs based on the effective expressivity
of the models with no hyperparameter tuning or additional processing, and
outperforms state-of-the-art methods for adversarial and out-of-distribution
detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Longitudinal detection of new MS lesions using Deep Learning. (arXiv:2206.08272v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08272">
<div class="article-summary-box-inner">
<span><p>The detection of new multiple sclerosis (MS) lesions is an important marker
of the evolution of the disease. The applicability of learning-based methods
could automate this task efficiently. However, the lack of annotated
longitudinal data with new-appearing lesions is a limiting factor for the
training of robust and generalizing models. In this work, we describe a
deep-learning-based pipeline addressing the challenging task of detecting and
segmenting new MS lesions. First, we propose to use transfer-learning from a
model trained on a segmentation task using single time-points. Therefore, we
exploit knowledge from an easier task and for which more annotated datasets are
available. Second, we propose a data synthesis strategy to generate realistic
longitudinal time-points with new lesions using single time-point scans. In
this way, we pretrain our detection model on large synthetic annotated
datasets. Finally, we use a data-augmentation technique designed to simulate
data diversity in MRI. By doing that, we increase the size of the available
small annotated longitudinal datasets. Our ablation study showed that each
contribution lead to an enhancement of the segmentation accuracy. Using the
proposed pipeline, we obtained the best score for the segmentation and the
detection of new MS lesions in the MSSEG2 MICCAI challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rank the triplets: A ranking-based multiple instance learning framework for detecting HPV infection in head and neck cancers using routine H&E images. (arXiv:2206.08275v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08275">
<div class="article-summary-box-inner">
<span><p>The aetiology of head and neck squamous cell carcinoma (HNSCC) involves
multiple carcinogens such as alcohol, tobacco and infection with human
papillomavirus (HPV). As the HPV infection influences the prognosis, treatment
and survival of patients with HNSCC, it is important to determine the HPV
status of these tumours. In this paper, we propose a novel triplet-ranking loss
function and a multiple instance learning pipeline for HPV status prediction.
This achieves a new state-of-the-art performance in HPV detection using only
the routine H&amp;E stained WSIs on two HNSCC cohorts. Furthermore, a comprehensive
tumour microenvironment profiling was performed, which characterised the unique
patterns between HPV+/- HNSCC from genomic, immunology and cellular
perspectives. Positive correlations of the proposed score with different
subtypes of T cells (e.g. T cells follicular helper, CD8+ T cells), and
negative correlations with macrophages and connective cells (e.g. fibroblast)
were identified, which is in line with clinical findings. Unique gene
expression profiles were also identified with respect to HPV infection status,
and is in line with existing findings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Capsule Endoscopy Classification using Focal Modulation Guided Convolutional Neural Network. (arXiv:2206.08298v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08298">
<div class="article-summary-box-inner">
<span><p>Video capsule endoscopy is a hot topic in computer vision and medicine. Deep
learning can have a positive impact on the future of video capsule endoscopy
technology. It can improve the anomaly detection rate, reduce physicians' time
for screening, and aid in real-world clinical analysis. CADx classification
system for video capsule endoscopy has shown a great promise for further
improvement. For example, detection of cancerous polyp and bleeding can lead to
swift medical response and improve the survival rate of the patients. To this
end, an automated CADx system must have high throughput and decent accuracy. In
this paper, we propose FocalConvNet, a focal modulation network integrated with
lightweight convolutional layers for the classification of small bowel
anatomical landmarks and luminal findings. FocalConvNet leverages focal
modulation to attain global context and allows global-local spatial
interactions throughout the forward pass. Moreover, the convolutional block
with its intrinsic inductive/learning bias and capacity to extract hierarchical
features allows our FocalConvNet to achieve favourable results with high
throughput. We compare our FocalConvNet with other SOTA on Kvasir-Capsule, a
large-scale VCE dataset with 44,228 frames with 13 classes of different
anomalies. Our proposed method achieves the weighted F1-score, recall and MCC}
of 0.6734, 0.6373 and 0.2974, respectively outperforming other SOTA
methodologies. Furthermore, we report the highest throughput of 148.02
images/second rate to establish the potential of FocalConvNet in a real-time
clinical environment. The code of the proposed FocalConvNet is available at
https://github.com/NoviceMAn-prog/FocalConvNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Patch Attacks and Defences in Vision-Based Tasks: A Survey. (arXiv:2206.08304v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08304">
<div class="article-summary-box-inner">
<span><p>Adversarial attacks in deep learning models, especially for safety-critical
systems, are gaining more and more attention in recent years, due to the lack
of trust in the security and robustness of AI models. Yet the more primitive
adversarial attacks might be physically infeasible or require some resources
that are hard to access like the training data, which motivated the emergence
of patch attacks. In this survey, we provide a comprehensive overview to cover
existing techniques of adversarial patch attacks, aiming to help interested
researchers quickly catch up with the progress in this field. We also discuss
existing techniques for developing detection and defences against adversarial
patches, aiming to help the community better understand this field and its
applications in the real world.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deepfake histological images for enhancing digital pathology. (arXiv:2206.08308v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08308">
<div class="article-summary-box-inner">
<span><p>An optical microscopic examination of thinly cut stained tissue on glass
slides prepared from a FFPE tissue blocks is the gold standard for tissue
diagnostics. In addition, the diagnostic abilities and expertise of any
pathologist is dependent on their direct experience with common as well as
rarer variant morphologies. Recently, deep learning approaches have been used
to successfully show a high level of accuracy for such tasks. However,
obtaining expert-level annotated images is an expensive and time-consuming task
and artificially synthesized histological images can prove greatly beneficial.
Here, we present an approach to not only generate histological images that
reproduce the diagnostic morphologic features of common disease but also
provide a user ability to generate new and rare morphologies. Our approach
involves developing a generative adversarial network model that synthesizes
pathology images constrained by class labels. We investigated the ability of
this framework in synthesizing realistic prostate and colon tissue images and
assessed the utility of these images in augmenting diagnostic ability of
machine learning methods as well as their usability by a panel of experienced
anatomic pathologists. Synthetic data generated by our framework performed
similar to real data in training a deep learning model for diagnosis.
Pathologists were not able to distinguish between real and synthetic images and
showed a similar level of inter-observer agreement for prostate cancer grading.
We extended the approach to significantly more complex images from colon
biopsies and showed that the complex microenvironment in such tissues can also
be reproduced. Finally, we present the ability for a user to generate deepfake
histological images via a simple markup of sematic labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SoundSpaces 2.0: A Simulation Platform for Visual-Acoustic Learning. (arXiv:2206.08312v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08312">
<div class="article-summary-box-inner">
<span><p>We introduce SoundSpaces 2.0, a platform for on-the-fly geometry-based audio
rendering for 3D environments. Given a 3D mesh of a real-world environment,
SoundSpaces can generate highly realistic acoustics for arbitrary sounds
captured from arbitrary microphone locations. Together with existing 3D visual
assets, it supports an array of audio-visual research tasks, such as
audio-visual navigation, mapping, source localization and separation, and
acoustic matching. Compared to existing resources, SoundSpaces 2.0 has the
advantages of allowing continuous spatial sampling, generalization to novel
environments, and configurable microphone and material properties. To our best
knowledge, this is the first geometry-based acoustic simulation that offers
high fidelity and realism while also being fast enough to use for embodied
learning. We showcase the simulator's properties and benchmark its performance
against real-world audio measurements. In addition, through two downstream
tasks covering embodied navigation and far-field automatic speech recognition,
highlighting sim2real performance for the latter. SoundSpaces 2.0 is publicly
available to facilitate wider research for perceptual systems that can both see
and hear.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting the Adversarial Transferability of Surrogate Model with Dark Knowledge. (arXiv:2206.08316v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08316">
<div class="article-summary-box-inner">
<span><p>Deep neural networks (DNNs) for image classification are known to be
vulnerable to adversarial examples. And, the adversarial examples have
transferability, which means an adversarial example for a DNN model can fool
another black-box model with a non-trivial probability. This gave birth of the
transfer-based adversarial attack where the adversarial examples generated by a
pretrained or known model (called surrogate model) are used to conduct
black-box attack. There are some work on how to generate the adversarial
examples from a given surrogate model to achieve better transferability.
However, training a special surrogate model to generate adversarial examples
with better transferability is relatively under-explored. In this paper, we
propose a method of training a surrogate model with abundant dark knowledge to
boost the adversarial transferability of the adversarial examples generated by
the surrogate model. This trained surrogate model is named dark surrogate model
(DSM), and the proposed method to train DSM consists of two key components: a
teacher model extracting dark knowledge and providing soft labels, and the
mixing augmentation skill which enhances the dark knowledge of training data.
Extensive experiments have been conducted to show that the proposed method can
substantially improve the adversarial transferability of surrogate model across
different architectures of surrogate model and optimizers for generating
adversarial examples. We also show that the proposed method can be applied to
other scenarios of transfer-based attack that contain dark knowledge, like face
verification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">iBoot: Image-bootstrapped Self-Supervised Video Representation Learning. (arXiv:2206.08339v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08339">
<div class="article-summary-box-inner">
<span><p>Learning visual representations through self-supervision is an extremely
challenging task as the network needs to sieve relevant patterns from spurious
distractors without the active guidance provided by supervision. This is
achieved through heavy data augmentation, large-scale datasets and prohibitive
amounts of compute. Video self-supervised learning (SSL) suffers from added
challenges: video datasets are typically not as large as image datasets,
compute is an order of magnitude larger, and the amount of spurious patterns
the optimizer has to sieve through is multiplied several fold. Thus, directly
learning self-supervised representations from video data might result in
sub-optimal performance. To address this, we propose to utilize a strong
image-based model, pre-trained with self- or language supervision, in a video
representation learning framework, enabling the model to learn strong spatial
and temporal information without relying on the video labeled data. To this
end, we modify the typical video-based SSL design and objective to encourage
the video encoder to \textit{subsume} the semantic content of an image-based
model trained on a general domain. The proposed algorithm is shown to learn
much more efficiently (i.e. in less epochs and with a smaller batch) and
results in a new state-of-the-art performance on standard downstream tasks
among single-modality SSL methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Realistic One-shot Mesh-based Head Avatars. (arXiv:2206.08343v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08343">
<div class="article-summary-box-inner">
<span><p>We present a system for realistic one-shot mesh-based human head avatars
creation, ROME for short. Using a single photograph, our model estimates a
person-specific head mesh and the associated neural texture, which encodes both
local photometric and geometric details. The resulting avatars are rigged and
can be rendered using a neural network, which is trained alongside the mesh and
texture estimators on a dataset of in-the-wild videos. In the experiments, we
observe that our system performs competitively both in terms of head geometry
recovery and the quality of renders, especially for the cross-person
reenactment. See results https://samsunglabs.github.io/rome/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-World Single Image Super-Resolution Under Rainy Condition. (arXiv:2206.08345v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08345">
<div class="article-summary-box-inner">
<span><p>Image super-resolution is an important research area in computer vision that
has a wide variety of applications including surveillance, medical imaging etc.
Real-world signal image super-resolution has become very popular now-a-days due
to its real-time application. There are still a lot of scopes to improve
real-world single image super-resolution specially during challenging weather
scenarios. In this paper, we have proposed a new algorithm to perform
real-world single image super-resolution during rainy condition. Our proposed
method can mitigate the influence of rainy conditions during image
super-resolution. Our experiment results show that our proposed algorithm can
perform image super-resolution decreasing the negative effects of the rain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Supervised vs. Unsupervised: Representative Benchmarking and Analysis of Image Representation Learning. (arXiv:2206.08347v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08347">
<div class="article-summary-box-inner">
<span><p>By leveraging contrastive learning, clustering, and other pretext tasks,
unsupervised methods for learning image representations have reached impressive
results on standard benchmarks. The result has been a crowded field - many
methods with substantially different implementations yield results that seem
nearly identical on popular benchmarks, such as linear evaluation on ImageNet.
However, a single result does not tell the whole story. In this paper, we
compare methods using performance-based benchmarks such as linear evaluation,
nearest neighbor classification, and clustering for several different datasets,
demonstrating the lack of a clear front-runner within the current
state-of-the-art. In contrast to prior work that performs only supervised vs.
unsupervised comparison, we compare several different unsupervised methods
against each other. To enrich this comparison, we analyze embeddings with
measurements such as uniformity, tolerance, and centered kernel alignment
(CKA), and propose two new metrics of our own: nearest neighbor graph
similarity and linear prediction overlap. We reveal through our analysis that
in isolation, single popular methods should not be treated as though they
represent the field as a whole, and that future work ought to consider how to
leverage the complimentary nature of these methods. We also leverage CKA to
provide a framework to robustly quantify augmentation invariance, and provide a
reminder that certain types of invariance will be undesirable for downstream
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FWD: Real-time Novel View Synthesis with Forward Warping and Depth. (arXiv:2206.08355v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08355">
<div class="article-summary-box-inner">
<span><p>Novel view synthesis (NVS) is a challenging task requiring systems to
generate photorealistic images of scenes from new viewpoints, where both
quality and speed are important for applications. Previous image-based
rendering (IBR) methods are fast, but have poor quality when input views are
sparse. Recent Neural Radiance Fields (NeRF) and generalizable variants give
impressive results but are not real-time. In our paper, we propose a
generalizable NVS method with sparse inputs, called FWD, which gives
high-quality synthesis in real-time. With explicit depth and differentiable
rendering, it achieves competitive results to the SOTA methods with 130-1000x
speedup and better perceptual quality. If available, we can seamlessly
integrate sensor depth during either training or inference to improve image
quality while retaining real-time speed. With the growing prevalence of depths
sensors, we hope that methods making use of depth will become increasingly
useful.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OmniMAE: Single Model Masked Pretraining on Images and Videos. (arXiv:2206.08356v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08356">
<div class="article-summary-box-inner">
<span><p>Transformer-based architectures have become competitive across a variety of
visual domains, most notably images and videos. While prior work has studied
these modalities in isolation, having a common architecture suggests that one
can train a single unified model for multiple visual modalities. Prior attempts
at unified modeling typically use architectures tailored for vision tasks, or
obtain worse performance compared to single modality models. In this work, we
show that masked autoencoding can be used to train a simple Vision Transformer
on images and videos, without requiring any labeled data. This single model
learns visual representations that are comparable to or better than
single-modality representations on both image and video benchmarks, while using
a much simpler architecture. In particular, our single pretrained model can be
finetuned to achieve 86.5% on ImageNet and 75.3% on the challenging Something
Something-v2 video benchmark. Furthermore, this model can be learned by
dropping 90% of the image and 95% of the video patches, enabling extremely fast
training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatially-Adaptive Multilayer Selection for GAN Inversion and Editing. (arXiv:2206.08357v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08357">
<div class="article-summary-box-inner">
<span><p>Existing GAN inversion and editing methods work well for aligned objects with
a clean background, such as portraits and animal faces, but often struggle for
more difficult categories with complex scene layouts and object occlusions,
such as cars, animals, and outdoor images. We propose a new method to invert
and edit such complex images in the latent space of GANs, such as StyleGAN2.
Our key idea is to explore inversion with a collection of layers, spatially
adapting the inversion process to the difficulty of the image. We learn to
predict the "invertibility" of different image segments and project each
segment into a latent layer. Easier regions can be inverted into an earlier
layer in the generator's latent space, while more challenging regions can be
inverted into a later feature space. Experiments show that our method obtains
better inversion results compared to the recent approaches on complex
categories, while maintaining downstream editability. Please refer to our
project page at https://www.cs.cmu.edu/~SAMInversion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MixGen: A New Multi-Modal Data Augmentation. (arXiv:2206.08358v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08358">
<div class="article-summary-box-inner">
<span><p>Data augmentation is a necessity to enhance data efficiency in deep learning.
For vision-language pre-training, data is only augmented either for images or
for text in previous works. In this paper, we present MixGen: a joint data
augmentation for vision-language representation learning to further improve
data efficiency. It generates new image-text pairs with semantic relationships
preserved by interpolating images and concatenating text. It's simple, and can
be plug-and-played into existing pipelines. We evaluate MixGen on four
architectures, including CLIP, ViLT, ALBEF and TCL, across five downstream
vision-language tasks to show its versatility and effectiveness. For example,
adding MixGen in ALBEF pre-training leads to absolute performance improvements
on downstream tasks: image-text retrieval (+6.2% on COCO fine-tuned and +5.3%
on Flicker30K zero-shot), visual grounding (+0.9% on RefCOCO+), visual
reasoning (+0.9% on NLVR$^{2}$), visual question answering (+0.3% on VQA2.0),
and visual entailment (+0.4% on SNLI-VE).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controllable 3D Face Synthesis with Conditional Generative Occupancy Fields. (arXiv:2206.08361v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08361">
<div class="article-summary-box-inner">
<span><p>Capitalizing on the recent advances in image generation models, existing
controllable face image synthesis methods are able to generate high-fidelity
images with some levels of controllability, e.g., controlling the shapes,
expressions, textures, and poses of the generated face images. However, these
methods focus on 2D image generative models, which are prone to producing
inconsistent face images under large expression and pose changes. In this
paper, we propose a new NeRF-based conditional 3D face synthesis framework,
which enables 3D controllability over the generated face images by imposing
explicit 3D conditions from 3D face priors. At its core is a conditional
Generative Occupancy Field (cGOF) that effectively enforces the shape of the
generated face to commit to a given 3D Morphable Model (3DMM) mesh. To achieve
accurate control over fine-grained 3D face shapes of the synthesized image, we
additionally incorporate a 3D landmark loss as well as a volume warping loss
into our synthesis algorithm. Experiments validate the effectiveness of the
proposed method, which is able to generate high-fidelity face images and shows
more precise 3D controllability than state-of-the-art 2D-based controllable
face synthesis methods. Find code and demo at
https://keqiangsun.github.io/projects/cgof.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified Fourier-based Kernel and Nonlinearity Design for Equivariant Networks on Homogeneous Spaces. (arXiv:2206.08362v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08362">
<div class="article-summary-box-inner">
<span><p>We introduce a unified framework for group equivariant networks on
homogeneous spaces derived from a Fourier perspective. We address the case of
feature fields being tensor valued before and after a convolutional layer. We
present a unified derivation of kernels via the Fourier domain by taking
advantage of the sparsity of Fourier coefficients of the lifted feature fields.
The sparsity emerges when the stabilizer subgroup of the homogeneous space is a
compact Lie group. We further introduce an activation method via an elementwise
nonlinearity on the regular representation after lifting and projecting back to
the field through an equivariant convolution. We show that other methods
treating features as the Fourier coefficients in the stabilizer subgroup are
special cases of our activation. Experiments on $SO(3)$ and $SE(3)$ show
state-of-the-art performance in spherical vector field regression, point cloud
classification, and molecular completion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Virtual Correspondence: Humans as a Cue for Extreme-View Geometry. (arXiv:2206.08365v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08365">
<div class="article-summary-box-inner">
<span><p>Recovering the spatial layout of the cameras and the geometry of the scene
from extreme-view images is a longstanding challenge in computer vision.
Prevailing 3D reconstruction algorithms often adopt the image matching paradigm
and presume that a portion of the scene is co-visible across images, yielding
poor performance when there is little overlap among inputs. In contrast, humans
can associate visible parts in one image to the corresponding invisible
components in another image via prior knowledge of the shapes. Inspired by this
fact, we present a novel concept called virtual correspondences (VCs). VCs are
a pair of pixels from two images whose camera rays intersect in 3D. Similar to
classic correspondences, VCs conform with epipolar geometry; unlike classic
correspondences, VCs do not need to be co-visible across views. Therefore VCs
can be established and exploited even if images do not overlap. We introduce a
method to find virtual correspondences based on humans in the scene. We
showcase how VCs can be seamlessly integrated with classic bundle adjustment to
recover camera poses across extreme views. Experiments show that our method
significantly outperforms state-of-the-art camera pose estimation methods in
challenging scenarios and is comparable in the traditional densely captured
setup. Our approach also unleashes the potential of multiple downstream tasks
such as scene reconstruction from multi-view stereo and novel view synthesis in
extreme-view scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SHIFT: A Synthetic Driving Dataset for Continuous Multi-Task Domain Adaptation. (arXiv:2206.08367v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08367">
<div class="article-summary-box-inner">
<span><p>Adapting to a continuously evolving environment is a safety-critical
challenge inevitably faced by all autonomous driving systems. Existing image
and video driving datasets, however, fall short of capturing the mutable nature
of the real world. In this paper, we introduce the largest multi-task synthetic
dataset for autonomous driving, SHIFT. It presents discrete and continuous
shifts in cloudiness, rain and fog intensity, time of day, and vehicle and
pedestrian density. Featuring a comprehensive sensor suite and annotations for
several mainstream perception tasks, SHIFT allows investigating the degradation
of a perception system performance at increasing levels of domain shift,
fostering the development of continuous adaptation strategies to mitigate this
problem and assess model robustness and generality. Our dataset and benchmark
toolkit are publicly available at www.vis.xyz/shift.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unbiased 4D: Monocular 4D Reconstruction with a Neural Deformation Model. (arXiv:2206.08368v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08368">
<div class="article-summary-box-inner">
<span><p>Capturing general deforming scenes is crucial for many computer graphics and
vision applications, and it is especially challenging when only a monocular RGB
video of the scene is available. Competing methods assume dense point tracks,
3D templates, large-scale training datasets, or only capture small-scale
deformations. In contrast to those, our method, Ub4D, makes none of these
assumptions while outperforming the previous state of the art in challenging
scenarios. Our technique includes two new, in the context of non-rigid 3D
reconstruction, components, i.e., 1) A coordinate-based and implicit neural
representation for non-rigid scenes, which enables an unbiased reconstruction
of dynamic scenes, and 2) A novel dynamic scene flow loss, which enables the
reconstruction of larger deformations. Results on our new dataset, which will
be made publicly available, demonstrate the clear improvement over the state of
the art in terms of surface reconstruction accuracy and robustness to large
deformations. Visit the project page https://4dqv.mpi-inf.mpg.de/Ub4D/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Robust and Reproducible Active Learning Using Neural Networks. (arXiv:2002.09564v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.09564">
<div class="article-summary-box-inner">
<span><p>Active learning (AL) is a promising ML paradigm that has the potential to
parse through large unlabeled data and help reduce annotation cost in domains
where labeling data can be prohibitive. Recently proposed neural network based
AL methods use different heuristics to accomplish this goal. In this study, we
demonstrate that under identical experimental settings, different types of AL
algorithms (uncertainty based, diversity based, and committee based) produce an
inconsistent gain over random sampling baseline. Through a variety of
experiments, controlling for sources of stochasticity, we show that variance in
performance metrics achieved by AL algorithms can lead to results that are not
consistent with the previously reported results. We also found that under
strong regularization, AL methods show marginal or no advantage over the random
sampling baseline under a variety of experimental conditions. Finally, we
conclude with a set of recommendations on how to assess the results using a new
AL algorithm to ensure results are reproducible and robust under changes in
experimental conditions. We share our codes to facilitate AL evaluations. We
believe our findings and recommendations will help advance reproducible
research in AL using neural networks. We open source our code at
https://github.com/PrateekMunjal/TorchAL
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Face Anti-Spoofing by Learning Polarization Cues in a Real-World Scenario. (arXiv:2003.08024v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.08024">
<div class="article-summary-box-inner">
<span><p>Face anti-spoofing is the key to preventing security breaches in biometric
recognition applications. Existing software-based and hardware-based face
liveness detection methods are effective in constrained environments or
designated datasets only. Deep learning method using RGB and infrared images
demands a large amount of training data for new attacks. In this paper, we
present a face anti-spoofing method in a real-world scenario by automatic
learning the physical characteristics in polarization images of a real face
compared to a deceptive attack. A computational framework is developed to
extract and classify the unique face features using convolutional neural
networks and SVM together. Our real-time polarized face anti-spoofing (PAAS)
detection method uses a on-chip integrated polarization imaging sensor with
optimized processing algorithms. Extensive experiments demonstrate the
advantages of the PAAS technique to counter diverse face spoofing attacks
(print, replay, mask) in uncontrolled indoor and outdoor conditions by learning
polarized face images of 33 people. A four-directional polarized face image
dataset is released to inspire future applications within biometric
anti-spoofing field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pointly-Supervised Instance Segmentation. (arXiv:2104.06404v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06404">
<div class="article-summary-box-inner">
<span><p>We propose an embarrassingly simple point annotation scheme to collect weak
supervision for instance segmentation. In addition to bounding boxes, we
collect binary labels for a set of points uniformly sampled inside each
bounding box. We show that the existing instance segmentation models developed
for full mask supervision can be seamlessly trained with point-based
supervision collected via our scheme. Remarkably, Mask R-CNN trained on COCO,
PASCAL VOC, Cityscapes, and LVIS with only 10 annotated random points per
object achieves 94%--98% of its fully-supervised performance, setting a strong
baseline for weakly-supervised instance segmentation. The new point annotation
scheme is approximately 5 times faster than annotating full object masks,
making high-quality instance segmentation more accessible in practice.
</p>
<p>Inspired by the point-based annotation form, we propose a modification to
PointRend instance segmentation module. For each object, the new architecture,
called Implicit PointRend, generates parameters for a function that makes the
final point-level mask prediction. Implicit PointRend is more straightforward
and uses a single point-level mask loss. Our experiments show that the new
module is more suitable for the point-based supervision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DEEMD: Drug Efficacy Estimation against SARS-CoV-2 based on cell Morphology with Deep multiple instance learning. (arXiv:2105.05758v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.05758">
<div class="article-summary-box-inner">
<span><p>Drug repurposing can accelerate the identification of effective compounds for
clinical use against SARS-CoV-2, with the advantage of pre-existing clinical
safety data and an established supply chain. RNA viruses such as SARS-CoV-2
manipulate cellular pathways and induce reorganization of subcellular
structures to support their life cycle. These morphological changes can be
quantified using bioimaging techniques. In this work, we developed DEEMD: a
computational pipeline using deep neural network models within a multiple
instance learning framework, to identify putative treatments effective against
SARS-CoV-2 based on morphological analysis of the publicly available RxRx19a
dataset. This dataset consists of fluorescence microscopy images of SARS-CoV-2
non-infected cells and infected cells, with and without drug treatment. DEEMD
first extracts discriminative morphological features to generate cell
morphological profiles from the non-infected and infected cells. These
morphological profiles are then used in a statistical model to estimate the
applied treatment efficacy on infected cells based on similarities to
non-infected cells. DEEMD is capable of localizing infected cells via weak
supervision without any expensive pixel-level annotations. DEEMD identifies
known SARS-CoV-2 inhibitors, such as Remdesivir and Aloxistatin, supporting the
validity of our approach. DEEMD can be explored for use on other emerging
viruses and datasets to rapidly identify candidate antiviral treatments in the
future}. Our implementation is available online at
https://www.github.com/Sadegh-Saberian/DEEMD
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OpenCoS: Contrastive Semi-supervised Learning for Handling Open-set Unlabeled Data. (arXiv:2107.08943v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.08943">
<div class="article-summary-box-inner">
<span><p>Semi-supervised learning (SSL) is one of the most promising paradigms to
circumvent the expensive labeling cost for building a high-performance model.
Most existing SSL methods conventionally assume both labeled and unlabeled data
are drawn from the same (class) distribution. However, unlabeled data may
include out-of-class samples in practice; those that cannot have one-hot
encoded labels from a closed-set of classes in label data, i.e. unlabeled data
is an open-set. In this paper, we introduce OpenCoS, a method for handling this
realistic semi-supervised learning scenario based upon a recent framework of
self-supervised visual representation learning. Specifically, we first observe
that the out-of-class samples in the open-set unlabeled dataset can be
identified effectively via self-supervised contrastive learning. Then, OpenCoS
utilizes this information to overcome the failure modes in the existing
state-of-the-art semi-supervised methods, by utilizing one-hot pseudo-labels
and soft-labels for the identified in- and out-of-class unlabeled data,
respectively. Our extensive experimental results show the effectiveness of
OpenCoS, fixing up the state-of-the-art semi-supervised methods to be suitable
for diverse scenarios involving open-set unlabeled data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Out-of-Domain Generalization from a Single Source: An Uncertainty Quantification Approach. (arXiv:2108.02888v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02888">
<div class="article-summary-box-inner">
<span><p>We are concerned with a worst-case scenario in model generalization, in the
sense that a model aims to perform well on many unseen domains while there is
only one single domain available for training. We propose Meta-Learning based
Adversarial Domain Augmentation to solve this Out-of-Domain generalization
problem. The key idea is to leverage adversarial training to create
"fictitious" yet "challenging" populations, from which a model can learn to
generalize with theoretical guarantees. To facilitate fast and desirable domain
augmentation, we cast the model training in a meta-learning scheme and use a
Wasserstein Auto-Encoder to relax the widely used worst-case constraint. We
further improve our method by integrating uncertainty quantification for
efficient domain generalization. Extensive experiments on multiple benchmark
datasets indicate its superior performance in tackling single domain
generalization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Skin Deep Unlearning: Artefact and Instrument Debiasing in the Context of Melanoma Classification. (arXiv:2109.09818v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09818">
<div class="article-summary-box-inner">
<span><p>Convolutional Neural Networks have demonstrated dermatologist-level
performance in the classification of melanoma from skin lesion images, but
prediction irregularities due to biases seen within the training data are an
issue that should be addressed before widespread deployment is possible. In
this work, we robustly remove bias and spurious variation from an automated
melanoma classification pipeline using two leading bias unlearning techniques.
We show that the biases introduced by surgical markings and rulers presented in
previous studies can be reasonably mitigated using these bias removal methods.
We also demonstrate the generalisation benefits of unlearning spurious
variation relating to the imaging instrument used to capture lesion images. Our
experimental results provide evidence that the effects of each of the
aforementioned biases are notably reduced, with different debiasing techniques
excelling at different tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mutual Consistency Learning for Semi-supervised Medical Image Segmentation. (arXiv:2109.09960v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09960">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel mutual consistency network (MC-Net+) to
effectively exploit the unlabeled data for semi-supervised medical image
segmentation. The MC-Net+ model is motivated by the observation that deep
models trained with limited annotations are prone to output highly uncertain
and easily mis-classified predictions in the ambiguous regions (e.g., adhesive
edges or thin branches) for medical image segmentation. Leveraging these
challenging samples can make the semi-supervised segmentation model training
more effective. Therefore, our proposed MC-Net+ model consists of two new
designs. First, the model contains one shared encoder and multiple slightly
different decoders (i.e., using different up-sampling strategies). The
statistical discrepancy of multiple decoders' outputs is computed to denote the
model's uncertainty, which indicates the unlabeled hard regions. Second, we
apply a novel mutual consistency constraint between one decoder's probability
output and other decoders' soft pseudo labels. In this way, we minimize the
discrepancy of multiple outputs (i.e., the model uncertainty) during training
and force the model to generate invariant results in such challenging regions,
aiming at regularizing the model training. We compared the segmentation results
of our MC-Net+ model with five state-of-the-art semi-supervised approaches on
three public medical datasets. Extension experiments with two standard
semi-supervised settings demonstrate the superior performance of our model over
other methods, which sets a new state of the art for semi-supervised medical
image segmentation. Our code is released publicly at
https://github.com/ycwu1997/MC-Net.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CausalAF: Causal Autoregressive Flow for Safety-Critical Driving Scenario Generation. (arXiv:2110.13939v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.13939">
<div class="article-summary-box-inner">
<span><p>Generating safety-critical scenarios, which are crucial yet difficult to
collect, provides an effective way to evaluate the robustness of autonomous
driving systems. However, the diversity of scenarios and efficiency of
generation methods are heavily restricted by the rareness and structure of
safety-critical scenarios. Therefore, existing generative models that only
estimate distributions from observational data are not satisfying to solve this
problem. In this paper, we integrate causality as a prior into the scenario
generation and propose a flow-based generative framework, Causal Autoregressive
Flow (CausalAF). CausalAF encourages the generative model to uncover and follow
the causal relationship among generated objects via novel causal masking
operations instead of searching the sample only from observational data. By
learning the cause-and-effect mechanism of how the generated scenario causes
risk situations rather than just learning correlations from data, CausalAF
significantly improves learning efficiency. Extensive experiments on three
heterogeneous traffic scenarios illustrate that CausalAF requires much fewer
optimization resources to effectively generate safety-critical scenarios. We
also show that using generated scenarios as additional training samples
empirically improves the robustness of autonomous driving algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Solving Inverse Problems in Medical Imaging with Score-Based Generative Models. (arXiv:2111.08005v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.08005">
<div class="article-summary-box-inner">
<span><p>Reconstructing medical images from partial measurements is an important
inverse problem in Computed Tomography (CT) and Magnetic Resonance Imaging
(MRI). Existing solutions based on machine learning typically train a model to
directly map measurements to medical images, leveraging a training dataset of
paired images and measurements. These measurements are typically synthesized
from images using a fixed physical model of the measurement process, which
hinders the generalization capability of models to unknown measurement
processes. To address this issue, we propose a fully unsupervised technique for
inverse problem solving, leveraging the recently introduced score-based
generative models. Specifically, we first train a score-based generative model
on medical images to capture their prior distribution. Given measurements and a
physical model of the measurement process at test time, we introduce a sampling
method to reconstruct an image consistent with both the prior and the observed
measurements. Our method does not assume a fixed measurement process during
training, and can thus be flexibly adapted to different measurement processes
at test time. Empirically, we observe comparable or better performance to
supervised learning techniques in several medical imaging tasks in CT and MRI,
while demonstrating significantly better generalization to unknown measurement
processes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SeCGAN: Parallel Conditional Generative Adversarial Networks for Face Editing via Semantic Consistency. (arXiv:2111.09298v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.09298">
<div class="article-summary-box-inner">
<span><p>Semantically guided conditional Generative Adversarial Networks (cGANs) have
become a popular approach for face editing in recent years. However, most
existing methods introduce semantic masks as direct conditional inputs to the
generator and often require the target masks to perform the corresponding
translation in the RGB space. We propose SeCGAN, a novel label-guided cGAN for
editing face images utilising semantic information without the need to specify
target semantic masks. During training, SeCGAN has two branches of generators
and discriminators operating in parallel, with one trained to translate RGB
images and the other for semantic masks. To bridge the two branches in a
mutually beneficial manner, we introduce a semantic consistency loss which
constrains both branches to have consistent semantic outputs. Whilst both
branches are required during training, the RGB branch is our primary network
and the semantic branch is not needed for inference. Our results on CelebA and
CelebA-HQ demonstrate that our approach is able to generate facial images with
more accurate attributes, outperforming competitive baselines in terms of
Target Attribute Recognition Rate whilst maintaining quality metrics such as
self-supervised Fr\'{e}chet Inception Distance and Inception Score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Masked-attention Mask Transformer for Universal Image Segmentation. (arXiv:2112.01527v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01527">
<div class="article-summary-box-inner">
<span><p>Image segmentation is about grouping pixels with different semantics, e.g.,
category or instance membership, where each choice of semantics defines a task.
While only the semantics of each task differ, current research focuses on
designing specialized architectures for each task. We present Masked-attention
Mask Transformer (Mask2Former), a new architecture capable of addressing any
image segmentation task (panoptic, instance or semantic). Its key components
include masked attention, which extracts localized features by constraining
cross-attention within predicted mask regions. In addition to reducing the
research effort by at least three times, it outperforms the best specialized
architectures by a significant margin on four popular datasets. Most notably,
Mask2Former sets a new state-of-the-art for panoptic segmentation (57.8 PQ on
COCO), instance segmentation (50.1 AP on COCO) and semantic segmentation (57.7
mIoU on ADE20K).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wild ToFu: Improving Range and Quality of Indirect Time-of-Flight Depth with RGB Fusion in Challenging Environments. (arXiv:2112.03750v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03750">
<div class="article-summary-box-inner">
<span><p>Indirect Time-of-Flight (I-ToF) imaging is a widespread way of depth
estimation for mobile devices due to its small size and affordable price.
Previous works have mainly focused on quality improvement for I-ToF imaging
especially curing the effect of Multi Path Interference (MPI). These
investigations are typically done in specifically constrained scenarios at
close distance, indoors and under little ambient light. Surprisingly little
work has investigated I-ToF quality improvement in real-life scenarios where
strong ambient light and far distances pose difficulties due to an extreme
amount of induced shot noise and signal sparsity, caused by the attenuation
with limited sensor power and light scattering. In this work, we propose a new
learning based end-to-end depth prediction network which takes noisy raw I-ToF
signals as well as an RGB image and fuses their latent representation based on
a multi step approach involving both implicit and explicit alignment to predict
a high quality long range depth map aligned to the RGB viewpoint. We test our
approach on challenging real-world scenes and show more than 40% RMSE
improvement on the final depth map compared to the baseline approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BEVDet: High-performance Multi-camera 3D Object Detection in Bird-Eye-View. (arXiv:2112.11790v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11790">
<div class="article-summary-box-inner">
<span><p>Autonomous driving perceives its surroundings for decision making, which is
one of the most complex scenarios in visual perception. The success of paradigm
innovation in solving the 2D object detection task inspires us to seek an
elegant, feasible, and scalable paradigm for fundamentally pushing the
performance boundary in this area. To this end, we contribute the BEVDet
paradigm in this paper. BEVDet performs 3D object detection in Bird-Eye-View
(BEV), where most target values are defined and route planning can be handily
performed. We merely reuse existing modules to build its framework but
substantially develop its performance by constructing an exclusive data
augmentation strategy and upgrading the Non-Maximum Suppression strategy. In
the experiment, BEVDet offers an excellent trade-off between accuracy and
time-efficiency. As a fast version, BEVDet-Tiny scores 31.2% mAP and 39.2% NDS
on the nuScenes val set. It is comparable with FCOS3D, but requires just 11%
computational budget of 215.3 GFLOPs and runs 9.2 times faster at 15.6 FPS.
Another high-precision version dubbed BEVDet-Base scores 39.3% mAP and 47.2%
NDS, significantly exceeding all published results. With a comparable inference
speed, it surpasses FCOS3D by a large margin of +9.8% mAP and +10.0% NDS. The
source code is publicly available for further research at
https://github.com/HuangJunJie2017/BEVDet .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning from Synthetic InSAR with Vision Transformers: The case of volcanic unrest detection. (arXiv:2201.03016v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03016">
<div class="article-summary-box-inner">
<span><p>The detection of early signs of volcanic unrest preceding an eruption, in the
form of ground deformation in Interferometric Synthetic Aperture Radar (InSAR)
data is critical for assessing volcanic hazard. In this work we treat this as a
binary classification problem of InSAR images, and propose a novel deep
learning methodology that exploits a rich source of synthetically generated
interferograms to train quality classifiers that perform equally well in real
interferograms. The imbalanced nature of the problem, with orders of magnitude
fewer positive samples, coupled with the lack of a curated database with
labeled InSAR data, sets a challenging task for conventional deep learning
architectures. We propose a new framework for domain adaptation, in which we
learn class prototypes from synthetic data with vision transformers. We report
detection accuracy that amounts to the highest reported accuracy on a large
test set for volcanic unrest detection. Moreover, we built upon this knowledge
by learning a new, non-linear, projection between the learnt representations
and prototype space, using pseudo labels produced by our model from an
unlabeled real InSAR dataset. This leads to the new state of the art with 97.1%
accuracy on our test set. We demonstrate the robustness of our approach by
training a simple ResNet-18 Convolutional Neural Network on the unlabeled real
InSAR dataset with pseudo-labels generated from our top transformer-prototype
model. Our methodology provides a significant improvement in performance
without the need of manually labeling any sample, opening the road for further
exploitation of synthetic InSAR data in various remote sensing applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can We Find Neurons that Cause Unrealistic Images in Deep Generative Networks?. (arXiv:2201.06346v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06346">
<div class="article-summary-box-inner">
<span><p>Even though Generative Adversarial Networks (GANs) have shown a remarkable
ability to generate high-quality images, GANs do not always guarantee the
generation of photorealistic images. Occasionally, they generate images that
have defective or unnatural objects, which are referred to as 'artifacts'.
Research to investigate why these artifacts emerge and how they can be detected
and removed has yet to be sufficiently carried out. To analyze this, we first
hypothesize that rarely activated neurons and frequently activated neurons have
different purposes and responsibilities for the progress of generating images.
In this study, by analyzing the statistics and the roles for those neurons, we
empirically show that rarely activated neurons are related to the failure
results of making diverse objects and inducing artifacts. In addition, we
suggest a correction method, called 'Sequential Ablation', to repair the
defective part of the generated images without high computational cost and
manual efforts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dataset Condensation with Contrastive Signals. (arXiv:2202.02916v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02916">
<div class="article-summary-box-inner">
<span><p>Recent studies have demonstrated that gradient matching-based dataset
synthesis, or dataset condensation (DC), methods can achieve state-of-the-art
performance when applied to data-efficient learning tasks. However, in this
study, we prove that the existing DC methods can perform worse than the random
selection method when task-irrelevant information forms a significant part of
the training dataset. We attribute this to the lack of participation of the
contrastive signals between the classes resulting from the class-wise gradient
matching strategy. To address this problem, we propose Dataset Condensation
with Contrastive signals (DCC) by modifying the loss function to enable the DC
methods to effectively capture the differences between classes. In addition, we
analyze the new loss function in terms of training dynamics by tracking the
kernel velocity. Furthermore, we introduce a bi-level warm-up strategy to
stabilize the optimization. Our experimental results indicate that while the
existing methods are ineffective for fine-grained image classification tasks,
the proposed method can successfully generate informative synthetic datasets
for the same tasks. Moreover, we demonstrate that the proposed method
outperforms the baselines even on benchmark datasets such as SVHN, CIFAR-10,
and CIFAR-100. Finally, we demonstrate the high applicability of the proposed
method by applying it to continual learning tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">General Cyclical Training of Neural Networks. (arXiv:2202.08835v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08835">
<div class="article-summary-box-inner">
<span><p>This paper describes the principle of "General Cyclical Training" in machine
learning, where training starts and ends with "easy training" and the "hard
training" happens during the middle epochs. We propose several manifestations
for training neural networks, including algorithmic examples (via
hyper-parameters and loss functions), data-based examples, and model-based
examples. Specifically, we introduce several novel techniques: cyclical weight
decay, cyclical batch size, cyclical focal loss, cyclical softmax temperature,
cyclical data augmentation, cyclical gradient clipping, and cyclical
semi-supervised learning. In addition, we demonstrate that cyclical weight
decay, cyclical softmax temperature, and cyclical gradient clipping (as three
examples of this principle) are beneficial in the test accuracy performance of
a trained model. Furthermore, we discuss model-based examples (such as
pretraining and knowledge distillation) from the perspective of general
cyclical training and recommend some changes to the typical training
methodology. In summary, this paper defines the general cyclical training
concept and discusses several specific ways in which this concept can be
applied to training neural networks. In the spirit of reproducibility, the code
used in our experiments is available at \url{https://github.com/lnsmith54/CFL}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cyclical Focal Loss. (arXiv:2202.08978v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08978">
<div class="article-summary-box-inner">
<span><p>The cross-entropy softmax loss is the primary loss function used to train
deep neural networks. On the other hand, the focal loss function has been
demonstrated to provide improved performance when there is an imbalance in the
number of training samples in each class, such as in long-tailed datasets. In
this paper, we introduce a novel cyclical focal loss and demonstrate that it is
a more universal loss function than cross-entropy softmax loss or focal loss.
We describe the intuition behind the cyclical focal loss and our experiments
provide evidence that cyclical focal loss provides superior performance for
balanced, imbalanced, or long-tailed datasets. We provide numerous experimental
results for CIFAR-10/CIFAR-100, ImageNet, balanced and imbalanced 4,000
training sample versions of CIFAR-10/CIFAR-100, and ImageNet-LT and Places-LT
from the Open Long-Tailed Recognition (OLTR) challenge. Implementing the
cyclical focal loss function requires only a few lines of code and does not
increase training time. In the spirit of reproducibility, our code is available
at \url{https://github.com/lnsmith54/CFL}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Smoothness and Class-Separation for Semi-supervised Medical Image Segmentation. (arXiv:2203.01324v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01324">
<div class="article-summary-box-inner">
<span><p>Semi-supervised segmentation remains challenging in medical imaging since the
amount of annotated medical data is often limited and there are many blurred
pixels near the adhesive edges or low-contrast regions. To address the issues,
we advocate to firstly constrain the consistency of samples with and without
strong perturbations to apply sufficient smoothness regularization and further
encourage the class-level separation to exploit the unlabeled ambiguous pixels
for the model training. Particularly, in this paper, we propose the SS-Net for
semi-supervised medical image segmentation tasks, via exploring the pixel-level
Smoothness and inter-class Separation at the same time. The pixel-level
smoothness forces the model to generate invariant results under adversarial
perturbations. Meanwhile, the inter-class separation constrains individual
class features should approach their corresponding high-quality prototypes, in
order to make each class distribution compact and separate different classes.
We evaluated our SS-Net against five recent methods on the public LA and ACDC
datasets. The experimental results under two semi-supervised settings
demonstrate the superiority of our proposed SS-Net, achieving new
state-of-the-art (SOTA) performance on both datasets. The code is available at
https://github.com/ycwu1997/SS-Net.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HDNet: High-resolution Dual-domain Learning for Spectral Compressive Imaging. (arXiv:2203.02149v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02149">
<div class="article-summary-box-inner">
<span><p>The rapid development of deep learning provides a better solution for the
end-to-end reconstruction of hyperspectral image (HSI). However, existing
learning-based methods have two major defects. Firstly, networks with
self-attention usually sacrifice internal resolution to balance model
performance against complexity, losing fine-grained high-resolution (HR)
features. Secondly, even if the optimization focusing on spatial-spectral
domain learning (SDL) converges to the ideal solution, there is still a
significant visual difference between the reconstructed HSI and the truth.
Therefore, we propose a high-resolution dual-domain learning network (HDNet)
for HSI reconstruction. On the one hand, the proposed HR spatial-spectral
attention module with its efficient feature fusion provides continuous and fine
pixel-level features. On the other hand, frequency domain learning (FDL) is
introduced for HSI reconstruction to narrow the frequency domain discrepancy.
Dynamic FDL supervision forces the model to reconstruct fine-grained
frequencies and compensate for excessive smoothing and distortion caused by
pixel-level losses. The HR pixel-level attention and frequency-level refinement
in our HDNet mutually promote HSI perceptual quality. Extensive quantitative
and qualitative evaluation experiments show that our method achieves SOTA
performance on simulated and real HSI datasets. Code and models will be
released at https://github.com/caiyuanhao1998/MST
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Large-scale Comprehensive Dataset and Copy-overlap Aware Evaluation Protocol for Segment-level Video Copy Detection. (arXiv:2203.02654v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02654">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce VCSL (Video Copy Segment Localization), a new
comprehensive segment-level annotated video copy dataset. Compared with
existing copy detection datasets restricted by either video-level annotation or
small-scale, VCSL not only has two orders of magnitude more segment-level
labelled data, with 160k realistic video copy pairs containing more than 280k
localized copied segment pairs, but also covers a variety of video categories
and a wide range of video duration. All the copied segments inside each
collected video pair are manually extracted and accompanied by precisely
annotated starting and ending timestamps. Alongside the dataset, we also
propose a novel evaluation protocol that better measures the prediction
accuracy of copy overlapping segments between a video pair and shows improved
adaptability in different scenarios. By benchmarking several baseline and
state-of-the-art segment-level video copy detection methods with the proposed
dataset and evaluation metric, we provide a comprehensive analysis that
uncovers the strengths and weaknesses of current approaches, hoping to open up
promising directions for future works. The VCSL dataset, metric and benchmark
codes are all publicly available at https://github.com/alipay/VCSL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Coarse-to-Fine Sparse Transformer for Hyperspectral Image Reconstruction. (arXiv:2203.04845v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04845">
<div class="article-summary-box-inner">
<span><p>Many algorithms have been developed to solve the inverse problem of coded
aperture snapshot spectral imaging (CASSI), i.e., recovering the 3D
hyperspectral images (HSIs) from a 2D compressive measurement. In recent years,
learning-based methods have demonstrated promising performance and dominated
the mainstream research direction. However, existing CNN-based methods show
limitations in capturing long-range dependencies and non-local self-similarity.
Previous Transformer-based methods densely sample tokens, some of which are
uninformative, and calculate the multi-head self-attention (MSA) between some
tokens that are unrelated in content. This does not fit the spatially sparse
nature of HSI signals and limits the model scalability. In this paper, we
propose a novel Transformer-based method, coarse-to-fine sparse Transformer
(CST), firstly embedding HSI sparsity into deep learning for HSI
reconstruction. In particular, CST uses our proposed spectra-aware screening
mechanism (SASM) for coarse patch selecting. Then the selected patches are fed
into our customized spectra-aggregation hashing multi-head self-attention
(SAH-MSA) for fine pixel clustering and self-similarity capturing.
Comprehensive experiments show that our CST significantly outperforms
state-of-the-art methods while requiring cheaper computational costs. The code
and models will be released at https://github.com/caiyuanhao1998/MST
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Enhanced Belief Propagation for Data Association in Multiobject Tracking. (arXiv:2203.09948v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09948">
<div class="article-summary-box-inner">
<span><p>Situation-aware technologies enabled by multiobject tracking (MOT) methods
will create new services and applications in fields such as autonomous
navigation and applied ocean sciences. Belief propagation (BP) is a
state-of-the-art method for Bayesian MOT but fully relies on a statistical
model and preprocessed sensor measurements. In this paper, we establish a
hybrid method for model-based and data-driven MOT. The proposed neural enhanced
belief propagation (NEBP) approach complements BP by information learned from
raw sensor data with the goal to improve data association and to reject false
alarm measurements. We evaluate the performance of our NEBP approach for MOT on
the nuScenes autonomous driving dataset and demonstrate that it can outperform
state-of-the-art reference methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluation of April Tag and WhyCode Fiducial Systems for Autonomous Precision Drone Landing with a Gimbal-Mounted Camera. (arXiv:2203.10180v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10180">
<div class="article-summary-box-inner">
<span><p>Fiducial markers provide a computationally cheap way for drones to determine
their location with respect to a landing pad and execute precision landings.
However, most existing work in this field uses a fixed, downward facing camera
that does not leverage the common gimbal-mounted camera setup found on many
drones. Such rigid systems cannot easily track detected markers, and may lose
sight of the markers in non-ideal conditions (e.g. wind gusts). This paper
evaluates April Tag and WhyCode fiducial systems for drone landing with a
gimbal-mounted, monocular camera, with the advantage that the drone system can
track the marker over time. However, since the orientation of the camera
changes, we must know the orientation of the marker, which is unreliable in
monocular fiducial systems. Additionally, the system must be fast. We propose 2
methods for mitigating the orientation ambiguity of WhyCode, and 1 method for
increasing the runtime detection rate of April Tag. We evaluate our 3 systems
against 2 default systems in terms of marker orientation ambiguity, and
detection rate. We test rates of marker detection in a ROS framework on a
Raspberry Pi 4, and we rank the systems in terms of their performance. Our
first WhyCode variant significantly reduces orientation ambiguity with an
insignificant reduction in detection rate. Our second WhyCode variant does not
show significantly different orientation ambiguity from the default WhyCode
system, but does provide additional functionality in terms of multi-marker
WhyCode bundle arrangements. Our April Tag variant does not show performance
improvements on a Raspberry Pi 4.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-end Document Recognition and Understanding with Dessurt. (arXiv:2203.16618v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16618">
<div class="article-summary-box-inner">
<span><p>We introduce Dessurt, a relatively simple document understanding transformer
capable of being fine-tuned on a greater variety of document tasks than prior
methods. It receives a document image and task string as input and generates
arbitrary text autoregressively as output. Because Dessurt is an end-to-end
architecture that performs text recognition in addition to the document
understanding, it does not require an external recognition model as prior
methods do. Dessurt is a more flexible model than prior methods and is able to
handle a variety of document domains and tasks. We show that this model is
effective at 9 different dataset-task combinations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BEVDet4D: Exploit Temporal Cues in Multi-camera 3D Object Detection. (arXiv:2203.17054v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17054">
<div class="article-summary-box-inner">
<span><p>Single frame data contains finite information which limits the performance of
the existing vision-based multi-camera 3D object detection paradigms. For
fundamentally pushing the performance boundary in this area, a novel paradigm
dubbed BEVDet4D is proposed to lift the scalable BEVDet paradigm from the
spatial-only 3D space to the spatial-temporal 4D space. We upgrade the naive
BEVDet framework with a few modifications just for fusing the feature from the
previous frame with the corresponding one in the current frame. In this way,
with negligible additional computing budget, we enable BEVDet4D to access the
temporal cues by querying and comparing the two candidate features. Beyond
this, we simplify the task of velocity prediction by removing the factors of
ego-motion and time in the learning target. As a result, BEVDet4D with robust
generalization performance reduces the velocity error by up to -62.9%. This
makes the vision-based methods, for the first time, become comparable with
those relied on LiDAR or radar in this aspect. On challenge benchmark nuScenes,
we report a new record of 54.5% NDS with the high-performance configuration
dubbed BEVDet4D-Base, which surpasses the previous leading method BEVDet-Base
by +7.3% NDS. The source code is publicly available for further research at
https://github.com/HuangJunJie2017/BEVDet .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Faculty Distillation with Optimal Transport. (arXiv:2204.11526v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11526">
<div class="article-summary-box-inner">
<span><p>The outpouring of various pre-trained models empowers knowledge
distillation~(KD) by providing abundant teacher resources. Meanwhile, exploring
the massive model repository to select a suitable teacher and further
extracting its knowledge become daunting challenges. Standard KD fails to
surmount two obstacles when training a student with the availability of
plentiful pre-trained teachers, i.e., the "faculty". First, we need to seek out
the most contributive teacher in the faculty efficiently rather than
enumerating all of them for a student. Second, since the teacher may be
pre-trained on different tasks w.r.t. the student, we must distill the
knowledge from a more general label space. This paper studies this ``faculty
distillation'' where a student performs teacher assessment and generalized
knowledge reuse. We take advantage of optimal transport to construct a unifying
objective for both problems, which bridges the semantic gap and measures the
relatedness between a pair of models. This objective can select the most
relevant teacher, and we minimize the same objective over student parameters to
transfer the knowledge from the selected teacher subsequently. Experiments in
various settings demonstrate the succinctness and versatility of our proposed
method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unlocking High-Accuracy Differentially Private Image Classification through Scale. (arXiv:2204.13650v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13650">
<div class="article-summary-box-inner">
<span><p>Differential Privacy (DP) provides a formal privacy guarantee preventing
adversaries with access to a machine learning model from extracting information
about individual training points. Differentially Private Stochastic Gradient
Descent (DP-SGD), the most popular DP training method for deep learning,
realizes this protection by injecting noise during training. However previous
works have found that DP-SGD often leads to a significant degradation in
performance on standard image classification benchmarks. Furthermore, some
authors have postulated that DP-SGD inherently performs poorly on large models,
since the norm of the noise required to preserve privacy is proportional to the
model dimension. In contrast, we demonstrate that DP-SGD on over-parameterized
models can perform significantly better than previously thought. Combining
careful hyper-parameter tuning with simple techniques to ensure signal
propagation and improve the convergence rate, we obtain a new SOTA without
extra data on CIFAR-10 of 81.4% under (8, 10^{-5})-DP using a 40-layer
Wide-ResNet, improving over the previous SOTA of 71.7%. When fine-tuning a
pre-trained NFNet-F3, we achieve a remarkable 83.8% top-1 accuracy on ImageNet
under (0.5, 8*10^{-7})-DP. Additionally, we also achieve 86.7% top-1 accuracy
under (8, 8 \cdot 10^{-7})-DP, which is just 4.3% below the current non-private
SOTA for this task. We believe our results are a significant step towards
closing the accuracy gap between private and non-private image classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Representation Learning for Compressed Video Action Recognition via Attentive Cross-modal Interaction with Motion Enhancement. (arXiv:2205.03569v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03569">
<div class="article-summary-box-inner">
<span><p>Compressed video action recognition has recently drawn growing attention,
since it remarkably reduces the storage and computational cost via replacing
raw videos by sparsely sampled RGB frames and compressed motion cues (e.g.,
motion vectors and residuals). However, this task severely suffers from the
coarse and noisy dynamics and the insufficient fusion of the heterogeneous RGB
and motion modalities. To address the two issues above, this paper proposes a
novel framework, namely Attentive Cross-modal Interaction Network with Motion
Enhancement (MEACI-Net). It follows the two-stream architecture, i.e. one for
the RGB modality and the other for the motion modality. Particularly, the
motion stream employs a multi-scale block embedded with a denoising module to
enhance representation learning. The interaction between the two streams is
then strengthened by introducing the Selective Motion Complement (SMC) and
Cross-Modality Augment (CMA) modules, where SMC complements the RGB modality
with spatio-temporally attentive local motion features and CMA further combines
the two modalities with selective feature augmentation. Extensive experiments
on the UCF-101, HMDB-51 and Kinetics-400 benchmarks demonstrate the
effectiveness and efficiency of MEACI-Net.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-level Consistency Learning for Semi-supervised Domain Adaptation. (arXiv:2205.04066v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04066">
<div class="article-summary-box-inner">
<span><p>Semi-supervised domain adaptation (SSDA) aims to apply knowledge learned from
a fully labeled source domain to a scarcely labeled target domain. In this
paper, we propose a Multi-level Consistency Learning (MCL) framework for SSDA.
Specifically, our MCL regularizes the consistency of different views of target
domain samples at three levels: (i) at inter-domain level, we robustly and
accurately align the source and target domains using a prototype-based optimal
transport method that utilizes the pros and cons of different views of target
samples; (ii) at intra-domain level, we facilitate the learning of both
discriminative and compact target feature representations by proposing a novel
class-wise contrastive clustering loss; (iii) at sample level, we follow
standard practice and improve the prediction accuracy by conducting a
consistency-based self-training. Empirically, we verified the effectiveness of
our MCL framework on three popular SSDA benchmarks, i.e., VisDA2017, DomainNet,
and Office-Home datasets, and the experimental results demonstrate that our MCL
framework achieves the state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Convolutional Dictionary Network for CT Metal Artifact Reduction. (arXiv:2205.07471v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07471">
<div class="article-summary-box-inner">
<span><p>Inspired by the great success of deep neural networks, learning-based methods
have gained promising performances for metal artifact reduction (MAR) in
computed tomography (CT) images. However, most of the existing approaches put
less emphasis on modelling and embedding the intrinsic prior knowledge
underlying this specific MAR task into their network designs. Against this
issue, we propose an adaptive convolutional dictionary network (ACDNet), which
leverages both model-based and learning-based methods. Specifically, we explore
the prior structures of metal artifacts, e.g., non-local repetitive streaking
patterns, and encode them as an explicit weighted convolutional dictionary
model. Then, a simple-yet-effective algorithm is carefully designed to solve
the model. By unfolding every iterative substep of the proposed algorithm into
a network module, we explicitly embed the prior structure into a deep network,
\emph{i.e.,} a clear interpretability for the MAR task. Furthermore, our ACDNet
can automatically learn the prior for artifact-free CT images via training data
and adaptively adjust the representation kernels for each input CT image based
on its content. Hence, our method inherits the clear interpretability of
model-based methods and maintains the powerful representation ability of
learning-based methods. Comprehensive experiments executed on synthetic and
clinical datasets show the superiority of our ACDNet in terms of effectiveness
and model generalization. {\color{blue}{{\textit{Code is available at
{\url{https://github.com/hongwang01/ACDNet}}.}}}}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Degradation-Aware Unfolding Half-Shuffle Transformer for Spectral Compressive Imaging. (arXiv:2205.10102v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10102">
<div class="article-summary-box-inner">
<span><p>In coded aperture snapshot spectral compressive imaging (CASSI) systems,
hyperspectral image (HSI) reconstruction methods are employed to recover the
spatial-spectral signal from a compressed measurement. Among these algorithms,
deep unfolding methods demonstrate promising performance but suffer from two
issues. Firstly, they do not estimate the degradation patterns and
ill-posedness degree from the highly related CASSI to guide the iterative
learning. Secondly, they are mainly CNN-based, showing limitations in capturing
long-range dependencies. In this paper, we propose a principled
Degradation-Aware Unfolding Framework (DAUF) that estimates parameters from the
compressed image and physical mask, and then uses these parameters to control
each iteration. Moreover, we customize a novel Half-Shuffle Transformer (HST)
that simultaneously captures local contents and non-local dependencies. By
plugging HST into DAUF, we establish the first Transformer-based deep unfolding
method, Degradation-Aware Unfolding Half-Shuffle Transformer (DAUHST), for HSI
reconstruction. Experiments show that DAUHST significantly surpasses
state-of-the-art methods while requiring cheaper computational and memory
costs. Code and models will be released at
https://github.com/caiyuanhao1998/MST
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Flow-Aligned Sequence-to-Sequence Learning for Video Restoration. (arXiv:2205.10195v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10195">
<div class="article-summary-box-inner">
<span><p>How to properly model the inter-frame relation within the video sequence is
an important but unsolved challenge for video restoration (VR). In this work,
we propose an unsupervised flow-aligned sequence-to-sequence model (S2SVR) to
address this problem. On the one hand, the sequence-to-sequence model, which
has proven capable of sequence modeling in the field of natural language
processing, is explored for the first time in VR. Optimized serialization
modeling shows potential in capturing long-range dependencies among frames. On
the other hand, we equip the sequence-to-sequence model with an unsupervised
optical flow estimator to maximize its potential. The flow estimator is trained
with our proposed unsupervised distillation loss, which can alleviate the data
discrepancy and inaccurate degraded optical flow issues of previous flow-based
methods. With reliable optical flow, we can establish accurate correspondence
among multiple frames, narrowing the domain difference between 1D language and
2D misaligned frames and improving the potential of the sequence-to-sequence
model. S2SVR shows superior performance in multiple VR tasks, including video
deblurring, video super-resolution, and compressed video quality enhancement.
Code and models are publicly available at
https://github.com/linjing7/VR-Baseline
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird's-Eye View Representation. (arXiv:2205.13542v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.13542">
<div class="article-summary-box-inner">
<span><p>Multi-sensor fusion is essential for an accurate and reliable autonomous
driving system. Recent approaches are based on point-level fusion: augmenting
the LiDAR point cloud with camera features. However, the camera-to-LiDAR
projection throws away the semantic density of camera features, hindering the
effectiveness of such methods, especially for semantic-oriented tasks (such as
3D scene segmentation). In this paper, we break this deeply-rooted convention
with BEVFusion, an efficient and generic multi-task multi-sensor fusion
framework. It unifies multi-modal features in the shared bird's-eye view (BEV)
representation space, which nicely preserves both geometric and semantic
information. To achieve this, we diagnose and lift key efficiency bottlenecks
in the view transformation with optimized BEV pooling, reducing latency by more
than 40x. BEVFusion is fundamentally task-agnostic and seamlessly supports
different 3D perception tasks with almost no architectural changes. It
establishes the new state of the art on nuScenes, achieving 1.3% higher mAP and
NDS on 3D object detection and 13.6% higher mIoU on BEV map segmentation, with
1.9x lower computation cost. Code to reproduce our results is available at
https://github.com/mit-han-lab/bevfusion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Virtual embeddings and self-consistency for self-supervised learning. (arXiv:2206.06023v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06023">
<div class="article-summary-box-inner">
<span><p>Self-supervised Learning (SSL) has recently gained much attention due to the
high cost and data limitation in the training of supervised learning models.
The current paradigm in the SSL is to utilize data augmentation at the input
space to create different views of the same images and train a model to
maximize the representations between similar images and minimize them for
different ones. While this approach achieves state-of-the-art (SOTA) results in
various downstream tasks, it still lakes the opportunity to investigate the
latent space augmentation. This paper proposes TriMix, a novel concept for SSL
that generates virtual embeddings through linear interpolation of the data,
thus providing the model with novel representations. Our strategy focuses on
training the model to extract the original embeddings from virtual ones, hence,
better representation learning. Additionally, we propose a self-consistency
term that improves the consistency between the virtual and actual embeddings.
We validate TriMix on eight benchmark datasets consisting of natural and
medical images with an improvement of 2.71% and 0.41% better than the
second-best models for both data types. Further, our approach outperformed the
current methods in semi-supervised learning, particularly in low data regimes.
Besides, our pre-trained models showed better transfer to other datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Decoder-free Object Detection with Transformers. (arXiv:2206.06829v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06829">
<div class="article-summary-box-inner">
<span><p>Vision transformers (ViTs) are changing the landscape of object detection
approaches. A natural usage of ViTs in detection is to replace the CNN-based
backbone with a transformer-based backbone, which is straightforward and
effective, with the price of bringing considerable computation burden for
inference. More subtle usage is the DETR family, which eliminates the need for
many hand-designed components in object detection but introduces a decoder
demanding an extra-long time to converge. As a result, transformer-based object
detection can not prevail in large-scale applications. To overcome these
issues, we propose a novel decoder-free fully transformer-based (DFFT) object
detector, achieving high efficiency in both training and inference stages, for
the first time. We simplify objection detection into an encoder-only
single-level anchor-based dense prediction problem by centering around two
entry points: 1) Eliminate the training-inefficient decoder and leverage two
strong encoders to preserve the accuracy of single-level feature map
prediction; 2) Explore low-level semantic features for the detection task with
limited computational resources. In particular, we design a novel lightweight
detection-oriented transformer backbone that efficiently captures low-level
features with rich semantics based on a well-conceived ablation study.
Extensive experiments on the MS COCO benchmark demonstrate that DFFT_SMALL
outperforms DETR by 2.5% AP with 28% computation cost reduction and more than
$10$x fewer training epochs. Compared with the cutting-edge anchor-based
detector RetinaNet, DFFT_SMALL obtains over 5.5% AP gain while cutting down 70%
computation cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">S$^2$-FPN: Scale-ware Strip Attention Guided Feature Pyramid Network for Real-time Semantic Segmentation. (arXiv:2206.07298v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07298">
<div class="article-summary-box-inner">
<span><p>Modern high-performance semantic segmentation methods employ a heavy backbone
and dilated convolution to extract the relevant feature. Although extracting
features with both contextual and semantic information is critical for the
segmentation tasks, it brings a memory footprint and high computation cost for
real-time applications. This paper presents a new model to achieve a trade-off
between accuracy/speed for real-time road scene semantic segmentation.
Specifically, we proposed a lightweight model named Scale-aware Strip Attention
Guided Feature Pyramid Network (S$^2$-FPN). Our network consists of three main
modules: Attention Pyramid Fusion (APF) module, Scale-aware Strip Attention
Module (SSAM), and Global Feature Upsample (GFU) module. APF adopts an
attention mechanisms to learn discriminative multi-scale features and help
close the semantic gap between different levels. APF uses the scale-aware
attention to encode global context with vertical stripping operation and models
the long-range dependencies, which helps relate pixels with similar semantic
label. In addition, APF employs channel-wise reweighting block (CRB) to
emphasize the channel features. Finally, the decoder of S$^2$-FPN then adopts
GFU, which is used to fuse features from APF and the encoder. Extensive
experiments have been conducted on two challenging semantic segmentation
benchmarks, which demonstrate that our approach achieves better accuracy/speed
trade-off with different model settings. The proposed models have achieved a
results of 76.2\%mIoU/87.3FPS, 77.4\%mIoU/67FPS, and 77.8\%mIoU/30.5FPS on
Cityscapes dataset, and 69.6\%mIoU,71.0\% mIoU, and 74.2\% mIoU on Camvid
dataset. The code for this work will be made available at
\url{https://github.com/mohamedac29/S2-FPN
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Seeking Common Ground While Reserving Differences: Multiple Anatomy Collaborative Framework for Undersampled MRI Reconstruction. (arXiv:2206.07364v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07364">
<div class="article-summary-box-inner">
<span><p>Recently, deep neural networks have greatly advanced undersampled Magnetic
Resonance Image (MRI) reconstruction, wherein most studies follow the
one-anatomy-one-network fashion, i.e., each expert network is trained and
evaluated for a specific anatomy. Apart from inefficiency in training multiple
independent models, such convention ignores the shared de-aliasing knowledge
across various anatomies which can benefit each other. To explore the shared
knowledge, one naive way is to combine all the data from various anatomies to
train an all-round network. Unfortunately, despite the existence of the shared
de-aliasing knowledge, we reveal that the exclusive knowledge across different
anatomies can deteriorate specific reconstruction targets, yielding overall
performance degradation. Observing this, in this study, we present a novel deep
MRI reconstruction framework with both anatomy-shared and anatomy-specific
parameterized learners, aiming to "seek common ground while reserving
differences" across different anatomies.Particularly, the primary
anatomy-shared learners are exposed to different anatomies to model flourishing
shared knowledge, while the efficient anatomy-specific learners are trained
with their target anatomy for exclusive knowledge. Four different
implementations of anatomy-specific learners are presented and explored on the
top of our framework in two MRI reconstruction networks. Comprehensive
experiments on brain, knee and cardiac MRI datasets demonstrate that three of
these learners are able to enhance reconstruction performance via multiple
anatomy collaborative learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Detection Methods for Die Attachment and Wire Bonding Defects in Integrated Circuit Manufacturing. (arXiv:2206.07481v2 [eess.SP] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07481">
<div class="article-summary-box-inner">
<span><p>Defect detection plays a vital role in the manufacturing process of
integrated circuits (ICs). Die attachment and wire bonding are two steps of the
manufacturing process that determine the power and signal transmission quality
and dependability in an IC. This paper presents a survey or literature review
of the methods used for detecting these defects based on different sensing
modalities used including optical, radiological, acoustical, and infrared
thermography. A discussion of the detection methods used is provided in this
survey. Both conventional and deep learning approaches for detecting die
attachment and wire bonding defects are considered along with challenges and
future research directions.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-06-18 23:07:20.436106200 UTC">2022-06-18 23:07:20 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>