<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-06-09T01:30:00Z">06-09</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">How to Dissect a Muppet: The Structure of Transformer Embedding Spaces. (arXiv:2206.03529v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03529">
<div class="article-summary-box-inner">
<span><p>Pretrained embeddings based on the Transformer architecture have taken the
NLP community by storm. We show that they can mathematically be reframed as a
sum of vector factors and showcase how to use this reframing to study the
impact of each component. We provide evidence that multi-head attentions and
feed-forwards are not equally useful in all downstream applications, as well as
a quantitative overview of the effects of finetuning on the overall embedding
space. This approach allows us to draw connections to a wide range of previous
studies, from vector space anisotropy to attention weights.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Guidelines and a Corpus for Extracting Biographical Events. (arXiv:2206.03547v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03547">
<div class="article-summary-box-inner">
<span><p>Despite biographies are widely spread within the Semantic Web, resources and
approaches to automatically extract biographical events are limited. Such
limitation reduces the amount of structured, machine-readable biographical
information, especially about people belonging to underrepresented groups. Our
work challenges this limitation by providing a set of guidelines for the
semantic annotation of life events. The guidelines are designed to be
interoperable with existing ISO-standards for semantic annotation: ISO-TimeML
(ISO-24617-1), and SemAF (ISO-24617-4). Guidelines were tested through an
annotation task of Wikipedia biographies of underrepresented writers, namely
authors born in non-Western countries, migrants, or belonging to ethnic
minorities. 1,000 sentences were annotated by 4 annotators with an average
Inter-Annotator Agreement of 0.825. The resulting corpus was mapped on
OntoNotes. Such mapping allowed to to expand our corpus, showing that already
existing resources may be exploited for the biographical event extraction task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">1Cademy at Semeval-2022 Task 1: Investigating the Effectiveness of Multilingual, Multitask, and Language-Agnostic Tricks for the Reverse Dictionary Task. (arXiv:2206.03702v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03702">
<div class="article-summary-box-inner">
<span><p>This paper describes our system for the SemEval2022 task of matching
dictionary glosses to word embeddings. We focus on the Reverse Dictionary Track
of the competition, which maps multilingual glosses to reconstructed vector
representations. More specifically, models convert the input of sentences to
three types of embeddings: SGNS, Char, and Electra. We propose several
experiments for applying neural network cells, general multilingual and
multitask structures, and language-agnostic tricks to the task. We also provide
comparisons over different types of word embeddings and ablation studies to
suggest helpful strategies. Our initial transformer-based model achieves
relatively low performance. However, trials on different retokenization
methodologies indicate improved performance. Our proposed Elmobased monolingual
model achieves the highest outcome, and its multitask, and multilingual
varieties show competitive results as well.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modularized Transfer Learning with Multiple Knowledge Graphs for Zero-shot Commonsense Reasoning. (arXiv:2206.03715v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03715">
<div class="article-summary-box-inner">
<span><p>Commonsense reasoning systems should be able to generalize to diverse
reasoning cases. However, most state-of-the-art approaches depend on expensive
data annotations and overfit to a specific benchmark without learning how to
perform general semantic reasoning. To overcome these drawbacks, zero-shot QA
systems have shown promise as a robust learning scheme by transforming a
commonsense knowledge graph (KG) into synthetic QA-form samples for model
training. Considering the increasing type of different commonsense KGs, this
paper aims to extend the zero-shot transfer learning scenario into
multiple-source settings, where different KGs can be utilized synergetically.
Towards this goal, we propose to mitigate the loss of knowledge from the
interference among the different knowledge sources, by developing a modular
variant of the knowledge aggregation as a new zero-shot commonsense reasoning
framework. Results on five commonsense reasoning benchmarks demonstrate the
efficacy of our framework, improving the performance with multiple KGs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Set Interdependence Transformer: Set-to-Sequence Neural Networks for Permutation Learning and Structure Prediction. (arXiv:2206.03720v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03720">
<div class="article-summary-box-inner">
<span><p>The task of learning to map an input set onto a permuted sequence of its
elements is challenging for neural networks. Set-to-sequence problems occur in
natural language processing, computer vision and structure prediction, where
interactions between elements of large sets define the optimal output. Models
must exhibit relational reasoning, handle varying cardinalities and manage
combinatorial complexity. Previous attention-based methods require $n$ layers
of their set transformations to explicitly represent $n$-th order relations.
Our aim is to enhance their ability to efficiently model higher-order
interactions through an additional interdependence component. We propose a
novel neural set encoding method called the Set Interdependence Transformer,
capable of relating the set's permutation invariant representation to its
elements within sets of any cardinality. We combine it with a permutation
learning module into a complete, 3-part set-to-sequence model and demonstrate
its state-of-the-art performance on a number of tasks. These range from
combinatorial optimization problems, through permutation learning challenges on
both synthetic and established NLP datasets for sentence ordering, to a novel
domain of product catalog structure prediction. Additionally, the network's
ability to generalize to unseen sequence lengths is investigated and a
comparative empirical analysis of the existing methods' ability to learn
higher-order interactions is provided.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Realistic Zero-Shot Cross-Lingual Transfer in Legal Topic Classification. (arXiv:2206.03785v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03785">
<div class="article-summary-box-inner">
<span><p>We consider zero-shot cross-lingual transfer in legal topic classification
using the recent MultiEURLEX dataset. Since the original dataset contains
parallel documents, which is unrealistic for zero-shot cross-lingual transfer,
we develop a new version of the dataset without parallel documents. We use it
to show that translation-based methods vastly outperform cross-lingual
fine-tuning of multilingually pre-trained models, the best previous zero-shot
transfer method for MultiEURLEX. We also develop a bilingual teacher-student
zero-shot transfer approach, which exploits additional unlabeled documents of
the target language and performs better than a model fine-tuned directly on
labeled target language documents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Open corpus of the Veps and Karelian languages: overview and applications. (arXiv:2206.03870v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03870">
<div class="article-summary-box-inner">
<span><p>A growing priority in the study of Baltic-Finnic languages of the Republic of
Karelia has been the methods and tools of corpus linguistics. Since 2016,
linguists, mathematicians, and programmers at the Karelian Research Centre have
been working with the Open Corpus of the Veps and Karelian Languages (VepKar),
which is an extension of the Veps Corpus created in 2009. The VepKar corpus
comprises texts in Karelian and Veps, multifunctional dictionaries linked to
them, and software with an advanced system of search using various criteria of
the texts (language, genre, etc.) and numerous linguistic categories (lexical
and grammatical search in texts was implemented thanks to the generator of word
forms that we created earlier). A corpus of 3000 texts was compiled, texts were
uploaded and marked up, the system for classifying texts into languages,
dialects, types and genres was introduced, and the word-form generator was
created. Future plans include developing a speech module for working with audio
recordings and a syntactic tagging module using morphological analysis outputs.
Owing to continuous functional advancements in the corpus manager and ongoing
VepKar enrichment with new material and text markup, users can handle a wide
range of scientific and applied tasks. In creating the universal national
VepKar corpus, its developers and managers strive to preserve and exhibit as
fully as possible the state of the Veps and Karelian languages in the 19th-21st
centuries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Counseling Summarization using Mental Health Knowledge Guided Utterance Filtering. (arXiv:2206.03886v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03886">
<div class="article-summary-box-inner">
<span><p>The psychotherapy intervention technique is a multifaceted conversation
between a therapist and a patient. Unlike general clinical discussions,
psychotherapy's core components (viz. symptoms) are hard to distinguish, thus
becoming a complex problem to summarize later. A structured counseling
conversation may contain discussions about symptoms, history of mental health
issues, or the discovery of the patient's behavior. It may also contain
discussion filler words irrelevant to a clinical summary. We refer to these
elements of structured psychotherapy as counseling components. In this paper,
the aim is mental health counseling summarization to build upon domain
knowledge and to help clinicians quickly glean meaning. We create a new dataset
after annotating 12.9K utterances of counseling components and reference
summaries for each dialogue. Further, we propose ConSum, a novel
counseling-component guided summarization model. ConSum undergoes three
independent modules. First, to assess the presence of depressive symptoms, it
filters utterances utilizing the Patient Health Questionnaire (PHQ-9), while
the second and third modules aim to classify counseling components. At last, we
propose a problem-specific Mental Health Information Capture (MHIC) evaluation
metric for counseling summaries. Our comparative study shows that we improve on
performance and generate cohesive, semantic, and coherent summaries. We
comprehensively analyze the generated summaries to investigate the capturing of
psychotherapy elements. Human and clinical evaluations on the summary show that
ConSum generates quality summary. Further, mental health experts validate the
clinical acceptability of the ConSum. Lastly, we discuss the uniqueness in
mental health counseling summarization in the real world and show evidences of
its deployment on an online application with the support of mpathic.ai
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Prompting Toward Controllable Response Generation. (arXiv:2206.03931v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03931">
<div class="article-summary-box-inner">
<span><p>Much literature has shown that prompt-based learning is an efficient method
to make use of the large pre-trained language model. Recent works also exhibit
the possibility of steering a chatbot's output by plugging in an appropriate
prompt. Gradient-based methods are often used to perturb the prompts. However,
some language models are not even available to the public. In this work, we
first explored the combination of prompting and reinforcement learning (RL) to
steer models' generation without accessing any of the models' parameters.
Second, to reduce the training effort and enhance the generalizability to the
unseen task, we apply multi-task learning to make the model learn to generalize
to new tasks better. The experiment results show that our proposed method can
successfully control several state-of-the-art (SOTA) dialogue models without
accessing their parameters. Furthermore, the model demonstrates the strong
ability to quickly adapt to an unseen task in fewer steps than the baseline
model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TURJUMAN: A Public Toolkit for Neural Arabic Machine Translation. (arXiv:2206.03933v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03933">
<div class="article-summary-box-inner">
<span><p>We present TURJUMAN, a neural toolkit for translating from 20 languages into
Modern Standard Arabic (MSA). TURJUMAN exploits the recently-introduced
text-to-text Transformer AraT5 model, endowing it with a powerful ability to
decode into Arabic. The toolkit offers the possibility of employing a number of
diverse decoding methods, making it suited for acquiring paraphrases for the
MSA translations as an added value. To train TURJUMAN, we sample from publicly
available parallel data employing a simple semantic similarity method to ensure
data quality. This allows us to prepare and release AraOPUS-20, a new machine
translation benchmark. We publicly release our translation toolkit (TURJUMAN)
as well as our benchmark dataset (AraOPUS-20).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Challenges in Applying Explainability Methods to Improve the Fairness of NLP Models. (arXiv:2206.03945v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03945">
<div class="article-summary-box-inner">
<span><p>Motivations for methods in explainable artificial intelligence (XAI) often
include detecting, quantifying and mitigating bias, and contributing to making
machine learning models fairer. However, exactly how an XAI method can help in
combating biases is often left unspecified. In this paper, we briefly review
trends in explainability and fairness in NLP research, identify the current
practices in which explainability methods are applied to detect and mitigate
bias, and investigate the barriers preventing XAI methods from being used more
widely in tackling fairness issues.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Proactively Reducing the Hate Intensity of Online Posts via Hate Speech Normalization. (arXiv:2206.04007v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04007">
<div class="article-summary-box-inner">
<span><p>Curbing online hate speech has become the need of the hour; however, a
blanket ban on such activities is infeasible for several geopolitical and
cultural reasons. To reduce the severity of the problem, in this paper, we
introduce a novel task, hate speech normalization, that aims to weaken the
intensity of hatred exhibited by an online post. The intention of hate speech
normalization is not to support hate but instead to provide the users with a
stepping stone towards non-hate while giving online platforms more time to
monitor any improvement in the user's behavior.
</p>
<p>To this end, we manually curated a parallel corpus - hate texts and their
normalized counterparts (a normalized text is less hateful and more benign). We
introduce NACL, a simple yet efficient hate speech normalization model that
operates in three stages - first, it measures the hate intensity of the
original sample; second, it identifies the hate span(s) within it; and finally,
it reduces hate intensity by paraphrasing the hate spans. We perform extensive
experiments to measure the efficacy of NACL via three-way evaluation
(intrinsic, extrinsic, and human-study). We observe that NACL outperforms six
baselines - NACL yields a score of 0.1365 RMSE for the intensity prediction,
0.622 F1-score in the span identification, and 82.27 BLEU and 80.05 perplexity
for the normalized text generation. We further show the generalizability of
NACL across other platforms (Reddit, Facebook, Gab). An interactive prototype
of NACL was put together for the user study. Further, the tool is being
deployed in a real-world setting at Wipro AI as a part of its mission to tackle
harmful content on online platforms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Resolving the Human Subjects Status of Machine Learning's Crowdworkers. (arXiv:2206.04039v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04039">
<div class="article-summary-box-inner">
<span><p>In recent years, machine learning (ML) has come to rely more heavily on
crowdworkers, both for building bigger datasets and for addressing research
questions requiring human interaction or judgment. Owing to the diverse tasks
performed by crowdworkers, and the myriad ways the resulting datasets are used,
it can be difficult to determine when these individuals are best thought of as
workers, versus as human subjects. These difficulties are compounded by
conflicting policies, with some institutions and researchers treating all ML
crowdwork as human subjects research, and other institutions holding that ML
crowdworkers rarely constitute human subjects. Additionally, few ML papers
involving crowdwork mention IRB oversight, raising the prospect that many might
not be in compliance with ethical and regulatory requirements. In this paper,
we focus on research in natural language processing to investigate the
appropriate designation of crowdsourcing studies and the unique challenges that
ML research poses for research oversight. Crucially, under the U.S. Common
Rule, these judgments hinge on determinations of "aboutness", both whom (or
what) the collected data is about and whom (or what) the analysis is about. We
highlight two challenges posed by ML: (1) the same set of workers can serve
multiple roles and provide many sorts of information; and (2) compared to the
life sciences and social sciences, ML research tends to embrace a dynamic
workflow, where research questions are seldom stated ex ante and data sharing
opens the door for future studies to ask questions about different targets from
the original study. In particular, our analysis exposes a potential loophole in
the Common Rule, where researchers can elude research ethics oversight by
splitting data collection and analysis into distinct studies. We offer several
policy recommendations to address these concerns.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">STable: Table Generation Framework for Encoder-Decoder Models. (arXiv:2206.04045v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04045">
<div class="article-summary-box-inner">
<span><p>The output structure of database-like tables, consisting of values structured
in horizontal rows and vertical columns identifiable by name, can cover a wide
range of NLP tasks. Following this constatation, we propose a framework for
text-to-table neural models applicable to problems such as extraction of line
items, joint entity and relation extraction, or knowledge base population. The
permutation-based decoder of our proposal is a generalized sequential method
that comprehends information from all cells in the table. The training
maximizes the expected log-likelihood for a table's content across all random
permutations of the factorization order. During the content inference, we
exploit the model's ability to generate cells in any order by searching over
possible orderings to maximize the model's confidence and avoid substantial
error accumulation, which other sequential models are prone to. Experiments
demonstrate a high practical value of the framework, which establishes
state-of-the-art results on several challenging datasets, outperforming
previous solutions by up to 15%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Personality Prediction; an Enhanced Method Using Ensemble Modeling. (arXiv:2007.04571v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.04571">
<div class="article-summary-box-inner">
<span><p>Human personality is significantly represented by those words which he/she
uses in his/her speech or writing. As a consequence of spreading the
information infrastructures (specifically the Internet and social media), human
communications have reformed notably from face to face communication.
Generally, Automatic Personality Prediction (or Perception) (APP) is the
automated forecasting of the personality on different types of human
generated/exchanged contents (like text, speech, image, video, etc.). The major
objective of this study is to enhance the accuracy of APP from the text. To
this end, we suggest five new APP methods including term frequency
vector-based, ontology-based, enriched ontology-based, latent semantic analysis
(LSA)-based, and deep learning-based (BiLSTM) methods. These methods as the
base ones, contribute to each other to enhance the APP accuracy through
ensemble modeling (stacking) based on a hierarchical attention network (HAN) as
the meta-model. The results show that ensemble modeling enhances the accuracy
of APP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Word2Box: Capturing Set-Theoretic Semantics of Words using Box Embeddings. (arXiv:2106.14361v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.14361">
<div class="article-summary-box-inner">
<span><p>Learning representations of words in a continuous space is perhaps the most
fundamental task in NLP, however words interact in ways much richer than vector
dot product similarity can provide. Many relationships between words can be
expressed set-theoretically, for example, adjective-noun compounds (eg. "red
cars"$\subseteq$"cars") and homographs (eg. "tongue"$\cap$"body" should be
similar to "mouth", while "tongue"$\cap$"language" should be similar to
"dialect") have natural set-theoretic interpretations. Box embeddings are a
novel region-based representation which provide the capability to perform these
set-theoretic operations. In this work, we provide a fuzzy-set interpretation
of box embeddings, and learn box representations of words using a set-theoretic
training objective. We demonstrate improved performance on various word
similarity tasks, particularly on less common words, and perform a quantitative
and qualitative analysis exploring the additional unique expressivity provided
by Word2Box.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Models In a Spelling Bee: Language Models Implicitly Learn the Character Composition of Tokens. (arXiv:2108.11193v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11193">
<div class="article-summary-box-inner">
<span><p>Standard pretrained language models operate on sequences of subword tokens
without direct access to the characters that compose each token's string
representation. We probe the embedding layer of pretrained language models and
show that models learn the internal character composition of whole word and
subword tokens to a surprising extent, without ever seeing the characters
coupled with the tokens. Our results show that the embedding layer of RoBERTa
holds enough information to accurately spell up to a third of the vocabulary
and reach high average character ngram overlap on all token types. We further
test whether enriching subword models with additional character information can
improve language modeling, and observe that this method has a near-identical
learning curve as training without spelling-based enrichment. Overall, our
results suggest that language modeling objectives incentivize the model to
implicitly learn some notion of spelling, and that explicitly teaching the
model how to spell does not appear to enhance its performance on such tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rare Tokens Degenerate All Tokens: Improving Neural Text Generation via Adaptive Gradient Gating for Rare Token Embeddings. (arXiv:2109.03127v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03127">
<div class="article-summary-box-inner">
<span><p>Recent studies have determined that the learned token embeddings of
large-scale neural language models are degenerated to be anisotropic with a
narrow-cone shape. This phenomenon, called the representation degeneration
problem, facilitates an increase in the overall similarity between token
embeddings that negatively affect the performance of the models. Although the
existing methods that address the degeneration problem based on observations of
the phenomenon triggered by the problem improves the performance of the text
generation, the training dynamics of token embeddings behind the degeneration
problem are still not explored. In this study, we analyze the training dynamics
of the token embeddings focusing on rare token embedding. We demonstrate that
the specific part of the gradient for rare token embeddings is the key cause of
the degeneration problem for all tokens during training stage. Based on the
analysis, we propose a novel method called, adaptive gradient gating (AGG). AGG
addresses the degeneration problem by gating the specific part of the gradient
for rare token embeddings. Experimental results from language modeling, word
similarity, and machine translation tasks quantitatively and qualitatively
verify the effectiveness of AGG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Think Before You Speak: Explicitly Generating Implicit Commonsense Knowledge for Response Generation. (arXiv:2110.08501v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08501">
<div class="article-summary-box-inner">
<span><p>Implicit knowledge, such as common sense, is key to fluid human
conversations. Current neural response generation (RG) models are trained to
generate responses directly, omitting unstated implicit knowledge. In this
paper, we present Think-Before-Speaking (TBS), a generative approach to first
externalize implicit commonsense knowledge (think) and use this knowledge to
generate responses (speak). We expect that externalizing implicit knowledge
allows more efficient learning, produces more informative responses, and
enables more explainable models. We analyze different choices to collect
knowledge-aligned dialogues, represent implicit knowledge, and transition
between knowledge and dialogues. Empirical results show TBS models outperform
end-to-end and knowledge-augmented RG baselines on most automatic metrics and
generate more informative, specific, and commonsense-following responses, as
evaluated by human annotators. TBS also generates knowledge that makes sense
and is relevant to the dialogue around 85\% of the time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-language Information Retrieval. (arXiv:2111.05988v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.05988">
<div class="article-summary-box-inner">
<span><p>Two key assumptions shape the usual view of ranked retrieval: (1) that the
searcher can choose words for their query that might appear in the documents
that they wish to see, and (2) that ranking retrieved documents will suffice
because the searcher will be able to recognize those which they wished to find.
When the documents to be searched are in a language not known by the searcher,
neither assumption is true. In such cases, Cross-Language Information Retrieval
(CLIR) is needed. This chapter reviews the state of the art for CLIR and
outlines some open research questions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph Enhanced Contrastive Learning for Radiology Findings Summarization. (arXiv:2204.00203v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00203">
<div class="article-summary-box-inner">
<span><p>The impression section of a radiology report summarizes the most prominent
observation from the findings section and is the most important section for
radiologists to communicate to physicians. Summarizing findings is
time-consuming and can be prone to error for inexperienced radiologists, and
thus automatic impression generation has attracted substantial attention. With
the encoder-decoder framework, most previous studies explore incorporating
extra knowledge (e.g., static pre-defined clinical ontologies or extra
background information). Yet, they encode such knowledge by a separate encoder
to treat it as an extra input to their models, which is limited in leveraging
their relations with the original findings. To address the limitation, we
propose a unified framework for exploiting both extra knowledge and the
original findings in an integrated way so that the critical information (i.e.,
key words and their relations) can be extracted in an appropriate way to
facilitate impression generation. In detail, for each input findings, it is
encoded by a text encoder, and a graph is constructed through its entities and
dependency tree. Then, a graph encoder (e.g., graph neural networks (GNNs)) is
adopted to model relation information in the constructed graph. Finally, to
emphasize the key words in the findings, contrastive learning is introduced to
map positive samples (constructed by masking non-key words) closer and push
apart negative ones (constructed by masking key words). The experimental
results on OpenI and MIMIC-CXR confirm the effectiveness of our proposed
method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CREER: A Large-Scale Corpus for Relation Extraction and Entity Recognition. (arXiv:2204.12710v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12710">
<div class="article-summary-box-inner">
<span><p>We describe the design and use of the CREER dataset, a large corpus annotated
with rich English grammar and semantic attributes. The CREER dataset uses the
Stanford CoreNLP Annotator to capture rich language structures from Wikipedia
plain text. This dataset follows widely used linguistic and semantic
annotations so that it can be used for not only most natural language
processing tasks but also scaling the dataset. This large supervised dataset
can serve as the basis for improving the performance of NLP tasks in the
future. We publicize the dataset through the link:
https://140.116.82.111/share.cgi?ssid=000 dOJ4
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generic and Trend-aware Curriculum Learning for Relation Extraction in Graph Neural Networks. (arXiv:2205.08625v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.08625">
<div class="article-summary-box-inner">
<span><p>We present a generic and trend-aware curriculum learning approach for graph
neural networks. It extends existing approaches by incorporating sample-level
loss trends to better discriminate easier from harder samples and schedule them
for training. The model effectively integrates textual and structural
information for relation extraction in text graphs. Experimental results show
that the model provides robust estimations of sample difficulty and shows
sizable improvement over the state-of-the-art approaches across several
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Disagreement in Automatic Data Labelling for Semi-Supervised Learning in Clinical Natural Language Processing. (arXiv:2205.14761v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14761">
<div class="article-summary-box-inner">
<span><p>Computational models providing accurate estimates of their uncertainty are
crucial for risk management associated with decision making in healthcare
contexts. This is especially true since many state-of-the-art systems are
trained using the data which has been labelled automatically (self-supervised
mode) and tend to overfit. In this work, we investigate the quality of
uncertainty estimates from a range of current state-of-the-art predictive
models applied to the problem of observation detection in radiology reports.
This problem remains understudied for Natural Language Processing in the
healthcare domain. We demonstrate that Gaussian Processes (GPs) provide
superior performance in quantifying the risks of 3 uncertainty labels based on
the negative log predictive probability (NLPP) evaluation metric and mean
maximum predicted confidence levels (MMPCL), whilst retaining strong predictive
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompting ELECTRA: Few-Shot Learning with Discriminative Pre-Trained Models. (arXiv:2205.15223v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15223">
<div class="article-summary-box-inner">
<span><p>Pre-trained masked language models successfully perform few-shot learning by
formulating downstream tasks as text infilling. However, as a strong
alternative in full-shot settings, discriminative pre-trained models like
ELECTRA do not fit into the paradigm. In this work, we adapt prompt-based
few-shot learning to ELECTRA and show that it outperforms masked language
models in a wide range of tasks. ELECTRA is pre-trained to distinguish if a
token is generated or original. We naturally extend that to prompt-based
few-shot learning by training to score the originality of the target options
without introducing new parameters. Our method can be easily adapted to tasks
involving multi-token predictions without extra computation overhead. Analysis
shows that ELECTRA learns distributions that align better with downstream
tasks.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">EiX-GNN : Concept-level eigencentrality explainer for graph neural networks. (arXiv:2206.03491v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03491">
<div class="article-summary-box-inner">
<span><p>Explaining is a human knowledge transfer process regarding a phenomenon
between an explainer and an explainee. Each word used to explain this
phenomenon must be carefully selected by the explainer in accordance with the
current explainee phenomenon-related knowledge level and the phenomenon itself
in order to have a high understanding from the explainee of the phenomenon.
Nowadays, deep models, especially graph neural networks, have a major place in
daily life even in critical applications. In such context, those models need to
have a human high interpretability also referred as being explainable, in order
to improve usage trustability of them in sensitive cases. Explaining is also a
human dependent task and methods that explain deep model behavior must include
these social-related concerns for providing profitable and quality
explanations. Current explaining methods often occlude such social aspect for
providing their explanations and only focus on the signal aspect of the
question. In this contribution we propose a reliable social-aware explaining
method suited for graph neural network that includes this social feature as a
modular concept generator and by both leveraging signal and graph domain aspect
thanks to an eigencentrality concept ordering approach. Besides our method
takes into account the human-dependent aspect underlying any explanation
process, we also reach high score regarding state-of-the-art objective metrics
assessing explanation methods for graph neural networks models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Penny for Your (visual) Thoughts: Self-Supervised Reconstruction of Natural Movies from Brain Activity. (arXiv:2206.03544v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03544">
<div class="article-summary-box-inner">
<span><p>Reconstructing natural videos from fMRI brain recordings is very challenging,
for two main reasons: (i) As fMRI data acquisition is diffcult, we only have a
limited amount of supervised samples, which is not enough to cover the huge
space of natural videos; and (ii) The temporal resolution of fMRI recordings is
much lower than the frame rate of natural videos. In this paper, we propose a
selfsupervised approach for natural movie reconstruction. By employing cycle
consistency over Encoding-Decoding natural videos, we can: (i) exploit the full
framerate of the training videos, and not be limited only to clips that
correspond to fMRI recordings; (ii) exploit massive amounts of external natural
videos which the subjects never saw inside the fMRI machine. These enable
increasing the applicable training data by several orders of magnitude,
introducing natural video priors to the decoding network, as well as temporal
coherence. Our approach signifcantly outperforms competing methods, since those
train only on the limited supervised data. We further introduce a new and
simple temporal prior of natural videos, which when folded into our fMRI
decoder further allows us to reconstruct videos at a higher framerate (HFR) of
up to x8 of the original fMRI sample rate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contributor-Aware Defenses Against Adversarial Backdoor Attacks. (arXiv:2206.03583v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03583">
<div class="article-summary-box-inner">
<span><p>Deep neural networks for image classification are well-known to be vulnerable
to adversarial attacks. One such attack that has garnered recent attention is
the adversarial backdoor attack, which has demonstrated the capability to
perform targeted misclassification of specific examples. In particular,
backdoor attacks attempt to force a model to learn spurious relations between
backdoor trigger patterns and false labels. In response to this threat,
numerous defensive measures have been proposed; however, defenses against
backdoor attacks focus on backdoor pattern detection, which may be unreliable
against novel or unexpected types of backdoor pattern designs. We introduce a
novel re-contextualization of the adversarial setting, where the presence of an
adversary implicitly admits the existence of multiple database contributors.
Then, under the mild assumption of contributor awareness, it becomes possible
to exploit this knowledge to defend against backdoor attacks by destroying the
false label associations. We propose a contributor-aware universal defensive
framework for learning in the presence of multiple, potentially adversarial
data sources that utilizes semi-supervised ensembles and learning from crowds
to filter the false labels produced by adversarial triggers. Importantly, this
defensive strategy is agnostic to backdoor pattern design, as it functions
without needing -- or even attempting -- to perform either adversary
identification or backdoor pattern detection during either training or
inference. Our empirical studies demonstrate the robustness of the proposed
framework against adversarial backdoor attacks from multiple simultaneous
adversaries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">White-box Membership Attack Against Machine Learning Based Retinopathy Classification. (arXiv:2206.03584v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03584">
<div class="article-summary-box-inner">
<span><p>The advances in machine learning (ML) have greatly improved AI-based
diagnosis aid systems in medical imaging. However, being based on collecting
medical data specific to individuals induces several security issues,
especially in terms of privacy. Even though the owner of the images like a
hospital put in place strict privacy protection provisions at the level of its
information system, the model trained over his images still holds disclosure
potential. The trained model may be accessible to an attacker as: 1) White-box:
accessing to the model architecture and parameters; 2) Black box: where he can
only query the model with his own inputs through an appropriate interface.
Existing attack methods include: feature estimation attacks (FEA), membership
inference attack (MIA), model memorization attack (MMA) and identification
attacks (IA). In this work we focus on MIA against a model that has been
trained to detect diabetic retinopathy from retinal images. Diabetic
retinopathy is a condition that can cause vision loss and blindness in the
people who have diabetes. MIA is the process of determining whether a data
sample comes from the training data set of a trained ML model or not. From a
privacy perspective in our use case where a diabetic retinopathy classification
model is given to partners that have at their disposal images along with
patients' identifiers, inferring the membership status of a data sample can
help to state if a patient has contributed or not to the training of the model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ObPose: Leveraging Canonical Pose for Object-Centric Scene Inference in 3D. (arXiv:2206.03591v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03591">
<div class="article-summary-box-inner">
<span><p>We present ObPose, an unsupervised object-centric generative model that
learns to segment 3D objects from RGB-D video in an unsupervised manner.
Inspired by prior art in 2D representation learning, ObPose considers a
factorised latent space, separately encoding object-wise location (where) and
appearance (what) information. In particular, ObPose leverages an object's
canonical pose, defined via a minimum volume principle, as a novel inductive
bias for learning the where component. To achieve this, we propose an
efficient, voxelised approximation approach to recover the object shape
directly from a neural radiance field (NeRF). As a consequence, ObPose models
scenes as compositions of NeRFs representing individual objects. When evaluated
on the YCB dataset for unsupervised scene segmentation, ObPose outperforms the
current state-of-the-art in 3D scene inference (ObSuRF) by a significant margin
in terms of segmentation quality for both video inputs as well as for
multi-view static scenes. In addition, the design choices made in the ObPose
encoder are validated with relevant ablations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Network Compression via Effective Filter Analysis and Hierarchical Pruning. (arXiv:2206.03596v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03596">
<div class="article-summary-box-inner">
<span><p>Network compression is crucial to making the deep networks to be more
efficient, faster, and generalizable to low-end hardware. Current network
compression methods have two open problems: first, there lacks a theoretical
framework to estimate the maximum compression rate; second, some layers may get
over-prunned, resulting in significant network performance drop. To solve these
two problems, this study propose a gradient-matrix singularity analysis-based
method to estimate the maximum network redundancy. Guided by that maximum rate,
a novel and efficient hierarchical network pruning algorithm is developed to
maximally condense the neuronal network structure without sacrificing network
performance. Substantial experiments are performed to demonstrate the efficacy
of the new method for pruning several advanced convolutional neural network
(CNN) architectures. Compared to existing pruning methods, the proposed pruning
algorithm achieved state-of-the-art performance. At the same or similar
compression ratio, the new method provided the highest network prediction
accuracy as compared to other methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Ring to Bring Them All: Towards Open-Set Recognition under Domain Shift. (arXiv:2206.03600v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03600">
<div class="article-summary-box-inner">
<span><p>In this paper, we investigate $\textit{open-set recognition}$ with domain
shift, where the final goal is to achieve $\textit{Source-free Universal Domain
Adaptation}$ (SF-UNDA), which addresses the situation where there exist both
domain and category shifts between source and target domains. Under the SF-UNDA
setting, the model cannot access source data anymore during target adaptation,
which aims to address data privacy concerns. We propose a novel training scheme
to learn a ($n$+1)-way classifier to predict the $n$ source classes and the
unknown class, where samples of only known source categories are available for
training. Furthermore, for target adaptation, we simply adopt a weighted
entropy minimization to adapt the source pretrained model to the unlabeled
target domain without source data. In experiments, we show: $\textbf{1)}$ After
source training, the resulting source model can get excellent performance for
$\textit{open-set single domain generalization}$ and also $\textit{open-set
recognition}$ tasks; $\textbf{2)}$ After target adaptation, our method
surpasses current UNDA approaches which demand source data during adaptation on
several benchmarks. The versatility to several different tasks strongly proves
the efficacy and generalization ability of our method. $\textbf{3)}$ When
augmented with a closed-set domain adaptation approach during target
adaptation, our source-free method further outperforms the current
state-of-the-art UNDA method by 2.5%, 7.2% and 13% on Office-31, Office-Home
and VisDA respectively. Code will be available in
https://github.com/Albert0147/OneRing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A new method incorporating deep learning with shape priors for left ventricular segmentation in myocardial perfusion SPECT images. (arXiv:2206.03603v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03603">
<div class="article-summary-box-inner">
<span><p>Background: The assessment of left ventricular (LV) function by myocardial
perfusion SPECT (MPS) relies on accurate myocardial segmentation. The purpose
of this paper is to develop and validate a new method incorporating deep
learning with shape priors to accurately extract the LV myocardium for
automatic measurement of LV functional parameters. Methods: A segmentation
architecture that integrates a three-dimensional (3D) V-Net with a shape
deformation module was developed. Using the shape priors generated by a dynamic
programming (DP) algorithm, the model output was then constrained and guided
during the model training for quick convergence and improved performance. A
stratified 5-fold cross-validation was used to train and validate our models.
Results: Results of our proposed method agree well with those from the ground
truth. Our proposed model achieved a Dice similarity coefficient (DSC) of
0.9573(0.0244), 0.9821(0.0137), and 0.9903(0.0041), a Hausdorff distances (HD)
of 6.7529(2.7334) mm, 7.2507(3.1952) mm, and 7.6121(3.0134) mm in extracting
the endocardium, myocardium, and epicardium, respectively. Conclusion: Our
proposed method achieved a high accuracy in extracting LV myocardial contours
and assessing LV function.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predictive Modeling of Charge Levels for Battery Electric Vehicles using CNN EfficientNet and IGTD Algorithm. (arXiv:2206.03612v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03612">
<div class="article-summary-box-inner">
<span><p>Convolutional Neural Networks (CNN) have been a good solution for
understanding a vast image dataset. As the increased number of battery-equipped
electric vehicles is flourishing globally, there has been much research on
understanding which charge levels electric vehicle drivers would choose to
charge their vehicles to get to their destination without any prevention. We
implemented deep learning approaches to analyze the tabular datasets to
understand their state of charge and which charge levels they would choose. In
addition, we implemented the Image Generator for Tabular Dataset algorithm to
utilize tabular datasets as image datasets to train convolutional neural
networks. Also, we integrated other CNN architecture such as EfficientNet to
prove that CNN is a great learner for reading information from images that were
converted from the tabular dataset, and able to predict charge levels for
battery-equipped electric vehicles. We also evaluated several optimization
methods to enhance the learning rate of the models and examined further
analysis on improving the model architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Delving into the Pre-training Paradigm of Monocular 3D Object Detection. (arXiv:2206.03657v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03657">
<div class="article-summary-box-inner">
<span><p>The labels of monocular 3D object detection (M3OD) are expensive to obtain.
Meanwhile, there usually exists numerous unlabeled data in practical
applications, and pre-training is an efficient way of exploiting the knowledge
in unlabeled data. However, the pre-training paradigm for M3OD is hardly
studied. We aim to bridge this gap in this work. To this end, we first draw two
observations: (1) The guideline of devising pre-training tasks is imitating the
representation of the target task. (2) Combining depth estimation and 2D object
detection is a promising M3OD pre-training baseline. Afterwards, following the
guideline, we propose several strategies to further improve this baseline,
which mainly include target guided semi-dense depth estimation, keypoint-aware
2D object detection, and class-level loss adjustment. Combining all the
developed techniques, the obtained pre-training framework produces pre-trained
backbones that improve M3OD performance significantly on both the KITTI-3D and
nuScenes benchmarks. For example, by applying a DLA34 backbone to a naive
center-based M3OD detector, the moderate ${\rm AP}_{3D}70$ score of Car on the
KITTI-3D testing set is boosted by 18.71\% and the NDS score on the nuScenes
validation set is improved by 40.41\% relatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Hyper-Initializer for All Network Architectures in Medical Image Analysis. (arXiv:2206.03661v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03661">
<div class="article-summary-box-inner">
<span><p>Pre-training is essential to deep learning model performance, especially in
medical image analysis tasks where limited training data are available.
However, existing pre-training methods are inflexible as the pre-trained
weights of one model cannot be reused by other network architectures. In this
paper, we propose an architecture-irrelevant hyper-initializer, which can
initialize any given network architecture well after being pre-trained for only
once. The proposed initializer is a hypernetwork which takes a downstream
architecture as input graphs and outputs the initialization parameters of the
respective architecture. We show the effectiveness and efficiency of the
hyper-initializer through extensive experimental results on multiple medical
imaging modalities, especially in data-limited fields. Moreover, we prove that
the proposed algorithm can be reused as a favorable plug-and-play initializer
for any downstream architecture and task (both classification and segmentation)
of the same modality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Depth Estimation Matters Most: Improving Per-Object Depth Estimation for Monocular 3D Detection and Tracking. (arXiv:2206.03666v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03666">
<div class="article-summary-box-inner">
<span><p>Monocular image-based 3D perception has become an active research area in
recent years owing to its applications in autonomous driving. Approaches to
monocular 3D perception including detection and tracking, however, often yield
inferior performance when compared to LiDAR-based techniques. Through
systematic analysis, we identified that per-object depth estimation accuracy is
a major factor bounding the performance. Motivated by this observation, we
propose a multi-level fusion method that combines different representations
(RGB and pseudo-LiDAR) and temporal information across multiple frames for
objects (tracklets) to enhance per-object depth estimation. Our proposed fusion
method achieves the state-of-the-art performance of per-object depth estimation
on the Waymo Open Dataset, the KITTI detection dataset, and the KITTI MOT
dataset. We further demonstrate that by simply replacing estimated depth with
fusion-enhanced depth, we can achieve significant improvements in monocular 3D
perception tasks, including detection and tracking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COVIDx CXR-3: A Large-Scale, Open-Source Benchmark Dataset of Chest X-ray Images for Computer-Aided COVID-19 Diagnostics. (arXiv:2206.03671v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03671">
<div class="article-summary-box-inner">
<span><p>After more than two years since the beginning of the COVID-19 pandemic, the
pressure of this crisis continues to devastate globally. The use of chest X-ray
(CXR) imaging as a complementary screening strategy to RT-PCR testing is not
only prevailing but has greatly increased due to its routine clinical use for
respiratory complaints. Thus far, many visual perception models have been
proposed for COVID-19 screening based on CXR imaging. Nevertheless, the
accuracy and the generalization capacity of these models are very much
dependent on the diversity and the size of the dataset they were trained on.
Motivated by this, we introduce COVIDx CXR-3, a large-scale benchmark dataset
of CXR images for supporting COVID-19 computer vision research. COVIDx CXR-3 is
composed of 30,386 CXR images from a multinational cohort of 17,026 patients
from at least 51 countries, making it, to the best of our knowledge, the most
extensive, most diverse COVID-19 CXR dataset in open access form. Here, we
provide comprehensive details on the various aspects of the proposed dataset
including patient demographics, imaging views, and infection types. The hope is
that COVIDx CXR-3 can assist scientists in advancing computer vision research
against the COVID-19 pandemic.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Learning of 3D Scene Flow from Monocular Camera. (arXiv:2206.03673v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03673">
<div class="article-summary-box-inner">
<span><p>Scene flow represents the motion of points in the 3D space, which is the
counterpart of the optical flow that represents the motion of pixels in the 2D
image. However, it is difficult to obtain the ground truth of scene flow in the
real scenes, and recent studies are based on synthetic data for training.
Therefore, how to train a scene flow network with unsupervised methods based on
real-world data shows crucial significance. A novel unsupervised learning
method for scene flow is proposed in this paper, which utilizes the images of
two consecutive frames taken by monocular camera without the ground truth of
scene flow for training. Our method realizes the goal that training scene flow
network with real-world data, which bridges the gap between training data and
test data and broadens the scope of available data for training. Unsupervised
learning of scene flow in this paper mainly consists of two parts: (i) depth
estimation and camera pose estimation, and (ii) scene flow estimation based on
four different loss functions. Depth estimation and camera pose estimation
obtain the depth maps and camera pose between two consecutive frames, which
provide further information for the next scene flow estimation. After that, we
used depth consistency loss, dynamic-static consistency loss, Chamfer loss, and
Laplacian regularization loss to carry out unsupervised training of the scene
flow network. To our knowledge, this is the first paper that realizes the
unsupervised learning of 3D scene flow from monocular camera. The experiment
results on KITTI show that our method for unsupervised learning of scene flow
meets great performance compared to traditional methods Iterative Closest Point
(ICP) and Fast Global Registration (FGR). The source code is available at:
https://github.com/IRMVLab/3DUnMonoFlow.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UHD Image Deblurring via Multi-scale Cubic-Mixer. (arXiv:2206.03678v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03678">
<div class="article-summary-box-inner">
<span><p>Currently, transformer-based algorithms are making a splash in the domain of
image deblurring. Their achievement depends on the self-attention mechanism
with CNN stem to model long range dependencies between tokens. Unfortunately,
this ear-pleasing pipeline introduces high computational complexity and makes
it difficult to run an ultra-high-definition image on a single GPU in real
time. To trade-off accuracy and efficiency, the input degraded image is
computed cyclically over three dimensional ($C$, $W$, and $H$) signals without
a self-attention mechanism. We term this deep network as Multi-scale
Cubic-Mixer, which is acted on both the real and imaginary components after
fast Fourier transform to estimate the Fourier coefficients and thus obtain a
deblurred image. Furthermore, we combine the multi-scale cubic-mixer with a
slicing strategy to generate high-quality results at a much lower computational
cost. Experimental results demonstrate that the proposed algorithm performs
favorably against the state-of-the-art deblurring approaches on the several
benchmarks and a new ultra-high-definition dataset in terms of accuracy and
speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DebiasBench: Benchmark for Fair Comparison of Debiasing in Image Classification. (arXiv:2206.03680v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03680">
<div class="article-summary-box-inner">
<span><p>Image classifiers often rely overly on peripheral attributes that have a
strong correlation with the target class (i.e., dataset bias) when making
predictions. Recently, a myriad of studies focus on mitigating such dataset
bias, the task of which is referred to as debiasing. However, these debiasing
methods often have inconsistent experimental settings (e.g., datasets and
neural network architectures). Additionally, most of the previous studies in
debiasing do not specify how they select their model parameters which involve
early stopping and hyper-parameter tuning. The goal of this paper is to
standardize the inconsistent experimental settings and propose a consistent
model parameter selection criterion for debiasing. Based on such unified
experimental settings and model parameter selection criterion, we build a
benchmark named DebiasBench which includes five datasets and seven debiasing
methods. We carefully conduct extensive experiments in various aspects and show
that different state-of-the-art methods work best in different datasets,
respectively. Even, the vanilla method, the method with no debiasing module,
also shows competitive results in datasets with low bias severity. We publicly
release the implementation of existing debiasing methods in DebiasBench to
encourage future researchers in debiasing to conduct fair comparisons and
further push the state-of-the-art performances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unified Model for Multi-class Anomaly Detection. (arXiv:2206.03687v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03687">
<div class="article-summary-box-inner">
<span><p>Despite the rapid advance of unsupervised anomaly detection, existing methods
require to train separate models for different objects. In this work, we
present UniAD that accomplishes anomaly detection for multiple classes with a
unified framework. Under such a challenging setting, popular reconstruction
networks may fall into an "identical shortcut", where both normal and anomalous
samples can be well recovered, and hence fail to spot outliers. To tackle this
obstacle, we make three improvements. First, we revisit the formulations of
fully-connected layer, convolutional layer, as well as attention layer, and
confirm the important role of query embedding (i.e., within attention layer) in
preventing the network from learning the shortcut. We therefore come up with a
layer-wise query decoder to help model the multi-class distribution. Second, we
employ a neighbor masked attention module to further avoid the information leak
from the input feature to the reconstructed output feature. Third, we propose a
feature jittering strategy that urges the model to recover the correct message
even with noisy inputs. We evaluate our algorithm on MVTec-AD and CIFAR-10
datasets, where we surpass the state-of-the-art alternatives by a sufficiently
large margin. For example, when learning a unified model for 15 categories in
MVTec-AD, we surpass the second competitor on the tasks of both anomaly
detection (from 88.1% to 96.5%) and anomaly localization (from 89.5% to 96.8%).
Code will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Deep Ensemble Method for Real-world Image Denoising. (arXiv:2206.03691v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03691">
<div class="article-summary-box-inner">
<span><p>Recently, deep learning-based image denoising methods have achieved promising
performance on test data with the same distribution as training set, where
various denoising models based on synthetic or collected real-world training
data have been learned. However, when handling real-world noisy images, the
denoising performance is still limited. In this paper, we propose a simple yet
effective Bayesian deep ensemble (BDE) method for real-world image denoising,
where several representative deep denoisers pre-trained with various training
data settings can be fused to improve robustness. The foundation of BDE is that
real-world image noises are highly signal-dependent, and heterogeneous noises
in a real-world noisy image can be separately handled by different denoisers.
In particular, we take well-trained CBDNet, NBNet, HINet, Uformer and GMSNet
into denoiser pool, and a U-Net is adopted to predict pixel-wise weighting maps
to fuse these denoisers. Instead of solely learning pixel-wise weighting maps,
Bayesian deep learning strategy is introduced to predict weighting uncertainty
as well as weighting map, by which prediction variance can be modeled for
improving robustness on real-world noisy images. Extensive experiments have
shown that real-world noises can be better removed by fusing existing denoisers
instead of training a big denoiser with expensive cost. On DND dataset, our BDE
achieves +0.28~dB PSNR gain over the state-of-the-art denoising method.
Moreover, we note that our BDE denoiser based on different Gaussian noise
levels outperforms state-of-the-art CBDNet when applying to real-world noisy
images. Furthermore, our BDE can be extended to other image restoration tasks,
and achieves +0.30dB, +0.18dB and +0.12dB PSNR gains on benchmark datasets for
image deblurring, image deraining and single image super-resolution,
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Blind Face Restoration: Benchmark Datasets and a Baseline Model. (arXiv:2206.03697v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03697">
<div class="article-summary-box-inner">
<span><p>Blind Face Restoration (BFR) aims to construct a high-quality (HQ) face image
from its corresponding low-quality (LQ) input. Recently, many BFR methods have
been proposed and they have achieved remarkable success. However, these methods
are trained or evaluated on privately synthesized datasets, which makes it
infeasible for the subsequent approaches to fairly compare with them. To
address this problem, we first synthesize two blind face restoration benchmark
datasets called EDFace-Celeb-1M (BFR128) and EDFace-Celeb-150K (BFR512).
State-of-the-art methods are benchmarked on them under five settings including
blur, noise, low resolution, JPEG compression artifacts, and the combination of
them (full degradation). To make the comparison more comprehensive, five
widely-used quantitative metrics and two task-driven metrics including Average
Face Landmark Distance (AFLD) and Average Face ID Cosine Similarity (AFICS) are
applied. Furthermore, we develop an effective baseline model called Swin
Transformer U-Net (STUNet). The STUNet with U-net architecture applies an
attention mechanism and a shifted windowing scheme to capture long-range pixel
interactions and focus more on significant features while still being trained
efficiently. Experimental results show that the proposed baseline method
performs favourably against the SOTA methods on various BFR tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What do we learn? Debunking the Myth of Unsupervised Outlier Detection. (arXiv:2206.03698v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03698">
<div class="article-summary-box-inner">
<span><p>Even though auto-encoders (AEs) have the desirable property of learning
compact representations without labels and have been widely applied to
out-of-distribution (OoD) detection, they are generally still poorly understood
and are used incorrectly in detecting outliers where the normal and abnormal
distributions are strongly overlapping. In general, the learned manifold is
assumed to contain key information that is only important for describing
samples within the training distribution, and that the reconstruction of
outliers leads to high residual errors. However, recent work suggests that AEs
are likely to be even better at reconstructing some types of OoD samples. In
this work, we challenge this assumption and investigate what auto-encoders
actually learn when they are posed to solve two different tasks. First, we
propose two metrics based on the Fr\'echet inception distance (FID) and
confidence scores of a trained classifier to assess whether AEs can learn the
training distribution and reliably recognize samples from other domains.
Second, we investigate whether AEs are able to synthesize normal images from
samples with abnormal regions, on a more challenging lung pathology detection
task. We have found that state-of-the-art (SOTA) AEs are either unable to
constrain the latent manifold and allow reconstruction of abnormal patterns, or
they are failing to accurately restore the inputs from their latent
distribution, resulting in blurred or misaligned reconstructions. We propose
novel deformable auto-encoders (MorphAEus) to learn perceptually aware global
image priors and locally adapt their morphometry based on estimated dense
deformation fields. We demonstrate superior performance over unsupervised
methods in detecting OoD and pathology.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hypernetwork-based Personalized Federated Learning for Multi-Institutional CT Imaging. (arXiv:2206.03709v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03709">
<div class="article-summary-box-inner">
<span><p>Computed tomography (CT) is of great importance in clinical practice due to
its powerful ability to provide patients' anatomical information without any
invasive inspection, but its potential radiation risk is raising people's
concerns. Deep learning-based methods are considered promising in CT
reconstruction, but these network models are usually trained with the measured
data obtained from specific scanning protocol and need to centralizedly collect
large amounts of data, which will lead to serious data domain shift, and
privacy concerns. To relieve these problems, in this paper, we propose a
hypernetwork-based federated learning method for personalized CT imaging,
dubbed as HyperFed. The basic assumption of HyperFed is that the optimization
problem for each institution can be divided into two parts: the local data
adaption problem and the global CT imaging problem, which are implemented by an
institution-specific hypernetwork and a global-sharing imaging network,
respectively. The purpose of global-sharing imaging network is to learn stable
and effective common features from different institutions. The
institution-specific hypernetwork is carefully designed to obtain
hyperparameters to condition the global-sharing imaging network for
personalized local CT reconstruction. Experiments show that HyperFed achieves
competitive performance in CT reconstruction compared with several other
state-of-the-art methods. It is believed as a promising direction to improve CT
imaging quality and achieve personalized demands of different institutions or
scanners without privacy data sharing. The codes will be released at
https://github.com/Zi-YuanYang/HyperFed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wavelet Regularization Benefits Adversarial Training. (arXiv:2206.03727v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03727">
<div class="article-summary-box-inner">
<span><p>Adversarial training methods are state-of-the-art (SOTA) empirical defense
methods against adversarial examples. Many regularization methods have been
proven to be effective with the combination of adversarial training.
Nevertheless, such regularization methods are implemented in the time domain.
Since adversarial vulnerability can be regarded as a high-frequency phenomenon,
it is essential to regulate the adversarially-trained neural network models in
the frequency domain. Faced with these challenges, we make a theoretical
analysis on the regularization property of wavelets which can enhance
adversarial training. We propose a wavelet regularization method based on the
Haar wavelet decomposition which is named Wavelet Average Pooling. This wavelet
regularization module is integrated into the wide residual neural network so
that a new WideWaveletResNet model is formed. On the datasets of CIFAR-10 and
CIFAR-100, our proposed Adversarial Wavelet Training method realizes
considerable robustness under different types of attacks. It verifies the
assumption that our wavelet regularization method can enhance adversarial
robustness especially in the deep wide neural networks. The visualization
experiments of the Frequency Principle (F-Principle) and interpretability are
implemented to show the effectiveness of our method. A detailed comparison
based on different wavelet base functions is presented. The code is available
at the repository:
\url{https://github.com/momo1986/AdversarialWaveletTraining}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Disentangled Ontology Embedding for Zero-shot Learning. (arXiv:2206.03739v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03739">
<div class="article-summary-box-inner">
<span><p>Knowledge Graph (KG) and its variant of ontology have been widely used for
knowledge representation, and have shown to be quite effective in augmenting
Zero-shot Learning (ZSL). However, existing ZSL methods that utilize KGs all
neglect the intrinsic complexity of inter-class relationships represented in
KGs. One typical feature is that a class is often related to other classes in
different semantic aspects. In this paper, we focus on ontologies for
augmenting ZSL, and propose to learn disentangled ontology embeddings guided by
ontology properties to capture and utilize more fine-grained class
relationships in different aspects. We also contribute a new ZSL framework
named DOZSL, which contains two new ZSL solutions based on generative models
and graph propagation models, respectively, for effectively utilizing the
disentangled ontology embeddings. Extensive evaluations have been conducted on
five benchmarks across zero-shot image classification (ZS-IMGC) and zero-shot
KG completion (ZS-KGC). DOZSL often achieves better performance than the
state-of-the-art, and its components have been verified by ablation studies and
case studies. Our codes and datasets are available at
https://github.com/zjukg/DOZSL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Loss Matters in Weakly Supervised Multi-Label Classification. (arXiv:2206.03740v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03740">
<div class="article-summary-box-inner">
<span><p>Weakly supervised multi-label classification (WSML) task, which is to learn a
multi-label classification using partially observed labels per image, is
becoming increasingly important due to its huge annotation cost. In this work,
we first regard unobserved labels as negative labels, casting the WSML task
into noisy multi-label classification. From this point of view, we empirically
observe that memorization effect, which was first discovered in a noisy
multi-class setting, also occurs in a multi-label setting. That is, the model
first learns the representation of clean labels, and then starts memorizing
noisy labels. Based on this finding, we propose novel methods for WSML which
reject or correct the large loss samples to prevent model from memorizing the
noisy label. Without heavy and complex components, our proposed methods
outperform previous state-of-the-art WSML methods on several partial label
settings including Pascal VOC 2012, MS COCO, NUSWIDE, CUB, and OpenImages V3
datasets. Various analysis also show that our methodology actually works well,
validating that treating large loss properly matters in a weakly supervised
multi-label classification. Our code is available at
https://github.com/snucml/LargeLossMatters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Task Agnostic Temporal Consistency Correction. (arXiv:2206.03753v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03753">
<div class="article-summary-box-inner">
<span><p>Due to the scarcity of video processing methodologies, image processing
operations are naively extended to the video domain by processing each frame
independently. This disregard for the temporal connection in video processing
often leads to severe temporal inconsistencies. State-of-the-art techniques
that address these inconsistencies rely on the availability of unprocessed
videos to siphon consistent video dynamics to restore the temporal consistency
of frame-wise processed videos. We propose a novel general framework for this
task that learns to infer consistent motion dynamics from inconsistent videos
to mitigate the temporal flicker while preserving the perceptual quality for
both the temporally neighboring and relatively distant frames. The proposed
framework produces state-of-the-art results on two large-scale datasets, DAVIS
and videvo.net, processed by numerous image processing tasks in a feed-forward
manner. The code and the trained models will be released upon acceptance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PixSelect: Less but Reliable Pixels for Accurate and Efficient Localization. (arXiv:2206.03775v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03775">
<div class="article-summary-box-inner">
<span><p>Accurate camera pose estimation is a fundamental requirement for numerous
applications, such as autonomous driving, mobile robotics, and augmented
reality. In this work, we address the problem of estimating the global 6 DoF
camera pose from a single RGB image in a given environment. Previous works
consider every part of the image valuable for localization. However, many image
regions such as the sky, occlusions, and repetitive non-distinguishable
patterns cannot be utilized for localization. In addition to adding unnecessary
computation efforts, extracting and matching features from such regions produce
many wrong matches which in turn degrades the localization accuracy and
efficiency. Our work addresses this particular issue and shows by exploiting an
interesting concept of sparse 3D models that we can exploit discriminatory
environment parts and avoid useless image regions for the sake of a single
image localization. Interestingly, through avoiding selecting keypoints from
non-reliable image regions such as trees, bushes, cars, pedestrians, and
occlusions, our work acts naturally as an outlier filter. This makes our system
highly efficient in that minimal set of correspondences is needed and highly
accurate as the number of outliers is low. Our work exceeds state-ofthe-art
methods on outdoor Cambridge Landmarks dataset. With only relying on single
image at inference, it outweighs in terms of accuracy methods that exploit pose
priors and/or reference 3D models while being much faster. By choosing as
little as 100 correspondences, it surpasses similar methods that localize from
thousands of correspondences, while being more efficient. In particular, it
achieves, compared to these methods, an improvement of localization by 33% on
OldHospital scene. Furthermore, It outstands direct pose regressors even those
that learn from sequence of images
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Digital Terrain Models from Point Clouds: ALS2DTM Dataset and Rasterization-based GAN. (arXiv:2206.03778v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03778">
<div class="article-summary-box-inner">
<span><p>Despite the popularity of deep neural networks in various domains, the
extraction of digital terrain models (DTMs) from airborne laser scanning (ALS)
point clouds is still challenging. This might be due to the lack of dedicated
large-scale annotated dataset and the data-structure discrepancy between point
clouds and DTMs. To promote data-driven DTM extraction, this paper collects
from open sources a large-scale dataset of ALS point clouds and corresponding
DTMs with various urban, forested, and mountainous scenes. A baseline method is
proposed as the first attempt to train a Deep neural network to extract digital
Terrain models directly from ALS point clouds via Rasterization techniques,
coined DeepTerRa. Extensive studies with well-established methods are performed
to benchmark the dataset and analyze the challenges in learning to extract DTM
from point clouds. The experimental results show the interest of the agnostic
data-driven approach, with sub-metric error level compared to methods designed
for DTM extraction. The data and source code is provided at
https://lhoangan.github.io/deepterra/ for reproducibility and further similar
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language-Bridged Spatial-Temporal Interaction for Referring Video Object Segmentation. (arXiv:2206.03789v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03789">
<div class="article-summary-box-inner">
<span><p>Referring video object segmentation aims to predict foreground labels for
objects referred by natural language expressions in videos. Previous methods
either depend on 3D ConvNets or incorporate additional 2D ConvNets as encoders
to extract mixed spatial-temporal features. However, these methods suffer from
spatial misalignment or false distractors due to delayed and implicit
spatial-temporal interaction occurring in the decoding phase. To tackle these
limitations, we propose a Language-Bridged Duplex Transfer (LBDT) module which
utilizes language as an intermediary bridge to accomplish explicit and adaptive
spatial-temporal interaction earlier in the encoding phase. Concretely,
cross-modal attention is performed among the temporal encoder, referring words
and the spatial encoder to aggregate and transfer language-relevant motion and
appearance information. In addition, we also propose a Bilateral Channel
Activation (BCA) module in the decoding phase for further denoising and
highlighting the spatial-temporal consistent features via channel-wise
activation. Extensive experiments show our method achieves new state-of-the-art
performances on four popular benchmarks with 6.8% and 6.9% absolute AP gains on
A2D Sentences and J-HMDB Sentences respectively, while consuming around 7x less
computational overhead.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dyna-DM: Dynamic Object-aware Self-supervised Monocular Depth Maps. (arXiv:2206.03799v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03799">
<div class="article-summary-box-inner">
<span><p>Self-supervised monocular depth estimation has been a subject of intense
study in recent years, because of its applications in robotics and autonomous
driving. Much of the recent work focuses on improving depth estimation by
increasing architecture complexity. This paper shows that state-of-the-art
performance can also be achieved by improving the learning process rather than
increasing model complexity. More specifically, we propose (i) only using
invariant pose loss for the first few epochs during training, (ii) disregarding
small potentially dynamic objects when training, and (iii) employing an
appearance-based approach to separately estimate object pose for truly dynamic
objects. We demonstrate that these simplifications reduce GPU memory usage by
29% and result in qualitatively and quantitatively improved depth maps
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual Windows Are Significant: Learning from Mediastinal Window and Focusing on Lung Window. (arXiv:2206.03803v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03803">
<div class="article-summary-box-inner">
<span><p>Since the pandemic of COVID-19, several deep learning methods were proposed
to analyze the chest Computed Tomography (CT) for diagnosis. In the current
situation, the disease course classification is significant for medical
personnel to decide the treatment. Most previous deep-learning-based methods
extract features observed from the lung window. However, it has been proved
that some appearances related to diagnosis can be observed better from the
mediastinal window rather than the lung window, e.g., the pulmonary
consolidation happens more in severe symptoms. In this paper, we propose a
novel Dual Window RCNN Network (DWRNet), which mainly learns the distinctive
features from the successive mediastinal window. Regarding the features
extracted from the lung window, we introduce the Lung Window Attention Block
(LWA Block) to pay additional attention to them for enhancing the
mediastinal-window features. Moreover, instead of picking up specific slices
from the whole CT slices, we use a Recurrent CNN and analyze successive slices
as videos. Experimental results show that the fused and representative features
improve the predictions of disease course by reaching the accuracy of 90.57%,
against the baseline with an accuracy of 84.86%. Ablation studies demonstrate
that combined dual window features are more efficient than lung-window features
alone, while paying attention to lung-window features can improve the model's
stability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SUPER-IVIM-DC: Intra-voxel incoherent motion based Fetal lung maturity assessment from limited DWI data using supervised learning coupled with data-consistency. (arXiv:2206.03820v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03820">
<div class="article-summary-box-inner">
<span><p>Intra-voxel incoherent motion (IVIM) analysis of fetal lungs
Diffusion-Weighted MRI (DWI) data shows potential in providing quantitative
imaging bio-markers that reflect, indirectly, diffusion and pseudo-diffusion
for non-invasive fetal lung maturation assessment. However, long acquisition
times, due to the large number of different "b-value" images required for IVIM
analysis, precluded clinical feasibility. We introduce SUPER-IVIM-DC a
deep-neural-networks (DNN) approach which couples supervised loss with a
data-consistency term to enable IVIM analysis of DWI data acquired with a
limited number of b-values. We demonstrated the added-value of SUPER-IVIM-DC
over both classical and recent DNN approaches for IVIM analysis through
numerical simulations, healthy volunteer study, and IVIM analysis of fetal lung
maturation from fetal DWI data. % add results Our numerical simulations and
healthy volunteer study show that SUPER-IVIM-DC estimates of the IVIM model
parameters from limited DWI data had lower normalized root mean-squared error
compared to previous DNN-based approaches. Further, SUPER-IVIM-DC estimates of
the pseudo-diffusion fraction parameter from limited DWI data of fetal lungs
correlate better with gestational age compared to both to classical and
DNN-based approaches (0.242 vs. -0.079 and 0.239). SUPER-IVIM-DC has the
potential to reduce the long acquisition times associated with IVIM analysis of
DWI data and to provide clinically feasible bio-markers for non-invasive fetal
lung maturity assessment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Understanding Why Mask-Reconstruction Pretraining Helps in Downstream Tasks. (arXiv:2206.03826v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03826">
<div class="article-summary-box-inner">
<span><p>For unsupervised pretraining, mask-reconstruction pretraining (MRP)
approaches randomly mask input patches and then reconstruct pixels or semantic
features of these masked patches via an auto-encoder. Then for a downstream
task, supervised fine-tuning the pretrained encoder remarkably surpasses the
conventional supervised learning (SL) trained from scratch. However, it is
still unclear 1) how MRP performs semantic learning in the pretraining phase
and 2) why it helps in downstream tasks. To solve these problems, we
theoretically show that on an auto-encoder of a two/one-layered convolution
encoder/decoder, MRP can capture all discriminative semantics in the
pretraining dataset, and accordingly show its provable improvement over SL on
the classification downstream task. Specifically, we assume that pretraining
dataset contains multi-view samples of ratio $1-\mu$ and single-view samples of
ratio $\mu$, where multi/single-view samples has multiple/single discriminative
semantics. Then for pretraining, we prove that 1) the convolution kernels of
the MRP encoder captures all discriminative semantics in the pretraining data;
and 2) a convolution kernel captures at most one semantic. Accordingly, in the
downstream supervised fine-tuning, most semantics would be captured and
different semantics would not be fused together. This helps the downstream
fine-tuned network to easily establish the relation between kernels and
semantic class labels. In this way, the fine-tuned encoder in MRP provably
achieves zero test error with high probability for both multi-view and
single-view test data. In contrast, as proved by~[3], conventional SL can only
obtain a test accuracy between around $0.5\mu$ for single-view test data. These
results together explain the benefits of MRP in downstream tasks. Experimental
results testify to multi-view data assumptions and our theoretical
implications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Myocardial Motion Tracking via Latent Space Exploration with Biomechanics-informed Prior. (arXiv:2206.03830v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03830">
<div class="article-summary-box-inner">
<span><p>Myocardial motion and deformation are rich descriptors that characterize
cardiac function. Image registration, as the most commonly used technique for
myocardial motion tracking, is an ill-posed inverse problem which often
requires prior assumptions on the solution space. In contrast to most existing
approaches which impose explicit generic regularization such as smoothness, in
this work we propose a novel method that can implicitly learn an
application-specific biomechanics-informed prior and embed it into a neural
network-parameterized transformation model. Particularly, the proposed method
leverages a variational autoencoder-based generative model to learn a manifold
for biomechanically plausible deformations. The motion tracking then can be
performed via traversing the learnt manifold to search for the optimal
transformations while considering the sequence information. The proposed method
is validated on three public cardiac cine MRI datasets with comprehensive
evaluations. The results demonstrate that the proposed method can outperform
other approaches, yielding higher motion tracking accuracy with reasonable
volume preservation and better generalizability to varying data distributions.
It also enables better estimates of myocardial strains, which indicates the
potential of the method in characterizing spatiotemporal signatures for
understanding cardiovascular diseases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rotation-Equivariant Conditional Spherical Neural Fields for Learning a Natural Illumination Prior. (arXiv:2206.03858v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03858">
<div class="article-summary-box-inner">
<span><p>Inverse rendering is an ill-posed problem. Previous work has sought to
resolve this by focussing on priors for object or scene shape or appearance. In
this work, we instead focus on a prior for natural illuminations. Current
methods rely on spherical harmonic lighting or other generic representations
and, at best, a simplistic prior on the parameters. We propose a conditional
neural field representation based on a variational auto-decoder with a SIREN
network and, extending Vector Neurons, build equivariance directly into the
network. Using this we develop a rotation-equivariant, high dynamic range (HDR)
neural illumination model that is compact and able to express complex,
high-frequency features of natural environment maps. Training our model on a
curated dataset of 1.6K HDR environment maps of natural scenes, we compare it
against traditional representations, demonstrate its applicability for an
inverse rendering task and show environment map completion from partial
observations. A PyTorch implementation, our dataset and trained models can be
found at jadgardner.github.io/RENI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Orthonormal Convolutions for the Rotation Based Iterative Gaussianization. (arXiv:2206.03860v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03860">
<div class="article-summary-box-inner">
<span><p>In this paper we elaborate an extension of rotation-based iterative
Gaussianization, RBIG, which makes image Gaussianization possible. Although
RBIG has been successfully applied to many tasks, it is limited to medium
dimensionality data (on the order of a thousand dimensions). In images its
application has been restricted to small image patches or isolated pixels,
because rotation in RBIG is based on principal or independent component
analysis and these transformations are difficult to learn and scale. Here we
present the \emph{Convolutional RBIG}: an extension that alleviates this issue
by imposing that the rotation in RBIG is a convolution. We propose to learn
convolutional rotations (i.e. orthonormal convolutions) by optimising for the
reconstruction loss between the input and an approximate inverse of the
transformation using the transposed convolution operation. Additionally, we
suggest different regularizers in learning these orthonormal convolutions. For
example, imposing sparsity in the activations leads to a transformation that
extends convolutional independent component analysis to multilayer
architectures. We also highlight how statistical properties of the data, such
as multivariate mutual information, can be obtained from \emph{Convolutional
RBIG}. We illustrate the behavior of the transform with a simple example of
texture synthesis, and analyze its properties by visualizing the stimuli that
maximize the response in certain feature and layer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perceptual Quality Assessment for Fine-Grained Compressed Images. (arXiv:2206.03862v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03862">
<div class="article-summary-box-inner">
<span><p>Recent years have witnessed the rapid development of image storage and
transmission systems, in which image compression plays an important role.
Generally speaking, image compression algorithms are developed to ensure good
visual quality at limited bit rates. However, due to the different compression
optimization methods, the compressed images may have different levels of
quality, which needs to be evaluated quantificationally. Nowadays, the
mainstream full-reference (FR) metrics are effective to predict the quality of
compressed images at coarse-grained levels (the bit rates differences of
compressed images are obvious), however, they may perform poorly for
fine-grained compressed images whose bit rates differences are quite subtle.
Therefore, to better improve the Quality of Experience (QoE) and provide useful
guidance for compression algorithms, we propose a full-reference image quality
assessment (FR-IQA) method for compressed images of fine-grained levels.
Specifically, the reference images and compressed images are first converted to
$YCbCr$ color space. The gradient features are extracted from regions that are
sensitive to compression artifacts. Then we employ the Log-Gabor transformation
to further analyze the texture difference. Finally, the obtained features are
fused into a quality score. The proposed method is validated on the
fine-grained compression image quality assessment (FGIQA) database, which is
especially constructed for assessing the quality of compressed images with
close bit rates. The experimental results show that our metric outperforms
mainstream FR-IQA metrics on the FGIQA database. We also test our method on
other commonly used compression IQA databases and the results show that our
method obtains competitive performance on the coarse-grained compression IQA
databases as well.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Progressive GANomaly: Anomaly detection with progressively growing GANs. (arXiv:2206.03876v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03876">
<div class="article-summary-box-inner">
<span><p>In medical imaging, obtaining large amounts of labeled data is often a
hurdle, because annotations and pathologies are scarce. Anomaly detection is a
method that is capable of detecting unseen abnormal data while only being
trained on normal (unannotated) data. Several algorithms based on generative
adversarial networks (GANs) exist to perform this task, yet certain limitations
are in place because of the instability of GANs. This paper proposes a new
method by combining an existing method, GANomaly, with progressively growing
GANs. The latter is known to be more stable, considering its ability to
generate high-resolution images. The method is tested using Fashion MNIST,
Medical Out-of-Distribution Analysis Challenge (MOOD), and in-house brain MRI;
using patches of sizes 16x16 and 32x32. Progressive GANomaly outperforms a
one-class SVM or regular GANomaly on Fashion MNIST. Artificial anomalies are
created in MOOD images with varying intensities and diameters. Progressive
GANomaly detected the most anomalies with varying intensity and size.
Additionally, the intermittent reconstructions are proven to be better from
progressive GANomaly. On the in-house brain MRI dataset, regular GANomaly
outperformed the other methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ConFUDA: Contrastive Fewshot Unsupervised Domain Adaptation for Medical Image Segmentation. (arXiv:2206.03888v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03888">
<div class="article-summary-box-inner">
<span><p>Unsupervised domain adaptation (UDA) aims to transfer knowledge learned from
a labeled source domain to an unlabeled target domain. Contrastive learning
(CL) in the context of UDA can help to better separate classes in feature
space. However, in image segmentation, the large memory footprint due to the
computation of the pixel-wise contrastive loss makes it prohibitive to use.
Furthermore, labeled target data is not easily available in medical imaging,
and obtaining new samples is not economical. As a result, in this work, we
tackle a more challenging UDA task when there are only a few (fewshot) or a
single (oneshot) image available from the target domain. We apply a style
transfer module to mitigate the scarcity of target samples. Then, to align the
source and target features and tackle the memory issue of the traditional
contrastive loss, we propose the centroid-based contrastive learning (CCL) and
a centroid norm regularizer (CNR) to optimize the contrastive pairs in both
direction and magnitude. In addition, we propose multi-partition centroid
contrastive learning (MPCCL) to further reduce the variance in the target
features. Fewshot evaluation on MS-CMRSeg dataset demonstrates that ConFUDA
improves the segmentation performance by 0.34 of the Dice score on the target
domain compared with the baseline, and 0.31 Dice score improvement in a more
rigorous oneshot setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PrivHAR: Recognizing Human Actions From Privacy-preserving Lens. (arXiv:2206.03891v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03891">
<div class="article-summary-box-inner">
<span><p>The accelerated use of digital cameras prompts an increasing concern about
privacy and security, particularly in applications such as action recognition.
In this paper, we propose an optimizing framework to provide robust visual
privacy protection along the human action recognition pipeline. Our framework
parameterizes the camera lens to successfully degrade the quality of the videos
to inhibit privacy attributes and protect against adversarial attacks while
maintaining relevant features for activity recognition. We validate our
approach with extensive simulations and hardware experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Deformable Image Registration with Absent Correspondences in Pre-operative and Post-Recurrence Brain Tumor MRI Scans. (arXiv:2206.03900v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03900">
<div class="article-summary-box-inner">
<span><p>Registration of pre-operative and post-recurrence brain images is often
needed to evaluate the effectiveness of brain gliomas treatment. While recent
deep learning-based deformable registration methods have achieved remarkable
success with healthy brain images, most of them would be unable to accurately
align images with pathologies due to the absent correspondences in the
reference image. In this paper, we propose a deep learning-based deformable
registration method that jointly estimates regions with absent correspondence
and bidirectional deformation fields. A forward-backward consistency constraint
is used to aid in the localization of the resection and recurrence region from
voxels with absence correspondences in the two images. Results on 3D clinical
data from the BraTS-Reg challenge demonstrate our method can improve image
alignment compared to traditional and deep learning-based registration
approaches with or without cost function masking strategy. The source code is
available at https://github.com/cwmok/DIRAC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Direct Triangulation with Spherical Projection for Omnidirectional Cameras. (arXiv:2206.03928v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03928">
<div class="article-summary-box-inner">
<span><p>In this paper, it is proposed to solve the problem of triangulation for
calibrated omnidirectional cameras through the optimisation of ray-pairs on the
projective sphere. The proposed solution boils down to finding the roots of a
quadratic function, and as such is closed form, completely non-iterative and
computationally inexpensive when compared to previous methods. In addition,
even thought the motivation is clearly to solve the triangulation problem for
omnidirectional cameras, it is demonstrated that the proposed methods can be
applied to non-omnidirectional, narrow field-of-view cameras.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual-Distribution Discrepancy for Anomaly Detection in Chest X-Rays. (arXiv:2206.03935v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03935">
<div class="article-summary-box-inner">
<span><p>Chest X-ray (CXR) is the most typical radiological exam for diagnosis of
various diseases. Due to the expensive and time-consuming annotations,
detecting anomalies in CXRs in an unsupervised fashion is very promising.
However, almost all of the existing methods consider anomaly detection as a
One-Class Classification (OCC) problem. They model the distribution of only
known normal images during training and identify the samples not conforming to
normal profile as anomalies in the testing phase. A large number of unlabeled
images containing anomalies are thus ignored in the training phase, although
they are easy to obtain in clinical practice. In this paper, we propose a novel
strategy, Dual-distribution Discrepancy for Anomaly Detection (DDAD), utilizing
both known normal images and unlabeled images. The proposed method consists of
two modules, denoted as A and B. During training, module A takes both known
normal and unlabeled images as inputs, capturing anomalous features from
unlabeled images in some way, while module B models the distribution of only
known normal images. Subsequently, the inter-discrepancy between modules A and
B, and intra-discrepancy inside module B are designed as anomaly scores to
indicate anomalies. Experiments on three CXR datasets demonstrate that the
proposed DDAD achieves consistent, significant gains and outperforms
state-of-the-art methods. Code is available at
https://github.com/caiyu6666/DDAD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Depth-Adapted CNNs for RGB-D Semantic Segmentation. (arXiv:2206.03939v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03939">
<div class="article-summary-box-inner">
<span><p>Recent RGB-D semantic segmentation has motivated research interest thanks to
the accessibility of complementary modalities from the input side. Existing
works often adopt a two-stream architecture that processes photometric and
geometric information in parallel, with few methods explicitly leveraging the
contribution of depth cues to adjust the sampling position on RGB images. In
this paper, we propose a novel framework to incorporate the depth information
in the RGB convolutional neural network (CNN), termed Z-ACN (Depth-Adapted
CNN). Specifically, our Z-ACN generates a 2D depth-adapted offset which is
fully constrained by low-level features to guide the feature extraction on RGB
images. With the generated offset, we introduce two intuitive and effective
operations to replace basic CNN operators: depth-adapted convolution and
depth-adapted average pooling. Extensive experiments on both indoor and outdoor
semantic segmentation tasks demonstrate the effectiveness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Environment Perception for Automated Driving: A Unified Learning Pipeline for Visual-Infrared Object Detection. (arXiv:2206.03943v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03943">
<div class="article-summary-box-inner">
<span><p>The RGB complementary metal-oxidesemiconductor (CMOS) sensor works within the
visible light spectrum. Therefore it is very sensitive to environmental light
conditions. On the contrary, a long-wave infrared (LWIR) sensor operating in
8-14 micro meter spectral band, functions independent of visible light.
</p>
<p>In this paper, we exploit both visual and thermal perception units for robust
object detection purposes. After delicate synchronization and (cross-) labeling
of the FLIR [1] dataset, this multi-modal perception data passes through a
convolutional neural network (CNN) to detect three critical objects on the
road, namely pedestrians, bicycles, and cars. After evaluation of RGB and
infrared (thermal and infrared are often used interchangeably) sensors
separately, various network structures are compared to fuse the data at the
feature level effectively. Our RGB-thermal (RGBT) fusion network, which takes
advantage of a novel entropy-block attention module (EBAM), outperforms the
state-of-the-art network [2] by 10% with 82.9% mAP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Out-of-Distribution Detection with Class Ratio Estimation. (arXiv:2206.03955v1 [stat.ML])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03955">
<div class="article-summary-box-inner">
<span><p>Density-based Out-of-distribution (OOD) detection has recently been shown
unreliable for the task of detecting OOD images. Various density ratio based
approaches achieve good empirical performance, however methods typically lack a
principled probabilistic modelling explanation. In this work, we propose to
unify density ratio based methods under a novel framework that builds
energy-based models and employs differing base distributions. Under our
framework, the density ratio can be viewed as the unnormalized density of an
implicit semantic distribution. Further, we propose to directly estimate the
density ratio of a data sample through class ratio estimation. We report
competitive results on OOD image problems in comparison with recent work that
alternatively requires training of deep generative models for the task. Our
approach enables a simple and yet effective path towards solving the OOD
detection problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Narrowing the Coordinate-frame Gap in Behavior Prediction Models: Distillation for Efficient and Accurate Scene-centric Motion Forecasting. (arXiv:2206.03970v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03970">
<div class="article-summary-box-inner">
<span><p>Behavior prediction models have proliferated in recent years, especially in
the popular real-world robotics application of autonomous driving, where
representing the distribution over possible futures of moving agents is
essential for safe and comfortable motion planning. In these models, the choice
of coordinate frames to represent inputs and outputs has crucial trade offs
which broadly fall into one of two categories. Agent-centric models transform
inputs and perform inference in agent-centric coordinates. These models are
intrinsically invariant to translation and rotation between scene elements, are
best-performing on public leaderboards, but scale quadratically with the number
of agents and scene elements. Scene-centric models use a fixed coordinate
system to process all agents. This gives them the advantage of sharing
representations among all agents, offering efficient amortized inference
computation which scales linearly with the number of agents. However, these
models have to learn invariance to translation and rotation between scene
elements, and typically underperform agent-centric models. In this work, we
develop knowledge distillation techniques between probabilistic motion
forecasting models, and apply these techniques to close the gap in performance
between agent-centric and scene-centric models. This improves scene-centric
model performance by 13.2% on the public Argoverse benchmark, 7.8% on Waymo
Open Dataset and up to 9.4% on a large In-House dataset. These improved
scene-centric models rank highly in public leaderboards and are up to 15 times
more efficient than their agent-centric teacher counterparts in busy scenes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Patch-based Object-centric Transformers for Efficient Video Generation. (arXiv:2206.04003v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04003">
<div class="article-summary-box-inner">
<span><p>In this work, we present Patch-based Object-centric Video Transformer (POVT),
a novel region-based video generation architecture that leverages
object-centric information to efficiently model temporal dynamics in videos. We
build upon prior work in video prediction via an autoregressive transformer
over the discrete latent space of compressed videos, with an added modification
to model object-centric information via bounding boxes. Due to better
compressibility of object-centric representations, we can improve training
efficiency by allowing the model to only access object information for longer
horizon temporal information. When evaluated on various difficult
object-centric datasets, our method achieves better or equal performance to
other video generation models, while remaining computationally more efficient
and scalable. In addition, we show that our method is able to perform
object-centric controllability through bounding box manipulation, which may aid
downstream tasks such as video editing, or visual planning. Samples are
available at
https://sites.google.com/view/povt-public}{https://sites.google.com/view/povt-public
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-Shot Audio-Visual Learning of Environment Acoustics. (arXiv:2206.04006v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04006">
<div class="article-summary-box-inner">
<span><p>Room impulse response (RIR) functions capture how the surrounding physical
environment transforms the sounds heard by a listener, with implications for
various applications in AR, VR, and robotics. Whereas traditional methods to
estimate RIRs assume dense geometry and/or sound measurements throughout the
environment, we explore how to infer RIRs based on a sparse set of images and
echoes observed in the space. Towards that goal, we introduce a
transformer-based method that uses self-attention to build a rich acoustic
context, then predicts RIRs of arbitrary query source-receiver locations
through cross-attention. Additionally, we design a novel training objective
that improves the match in the acoustic signature between the RIR predictions
and the targets. In experiments using a state-of-the-art audio-visual simulator
for 3D environments, we demonstrate that our method successfully generates
arbitrary RIRs, outperforming state-of-the-art methods and--in a major
departure from traditional methods--generalizing to novel environments in a
few-shot manner. Project: <a href="http://vision.cs.utexas.edu/projects/fs_rir.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SYNERgy between SYNaptic consolidation and Experience Replay for general continual learning. (arXiv:2206.04016v1 [cs.NE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04016">
<div class="article-summary-box-inner">
<span><p>Continual learning (CL) in the brain is facilitated by a complex set of
mechanisms. This includes the interplay of multiple memory systems for
consolidating information as posited by the complementary learning systems
(CLS) theory and synaptic consolidation for protecting the acquired knowledge
from erasure. Thus, we propose a general CL method that creates a synergy
between SYNaptic consolidation and dual memory Experience Replay (SYNERgy). Our
method maintains a semantic memory that accumulates and consolidates
information across the tasks and interacts with the episodic memory for
effective replay. It further employs synaptic consolidation by tracking the
importance of parameters during the training trajectory and anchoring them to
the consolidated parameters in the semantic memory. To the best of our
knowledge, our study is the first to employ dual memory experience replay in
conjunction with synaptic consolidation that is suitable for general CL whereby
the network does not utilize task boundaries or task labels during training or
inference. Our evaluation on various challenging CL scenarios and
characteristics analyses demonstrate the efficacy of incorporating both
synaptic consolidation and CLS theory in enabling effective CL in DNNs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CO^3: Cooperative Unsupervised 3D Representation Learning for Autonomous Driving. (arXiv:2206.04028v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04028">
<div class="article-summary-box-inner">
<span><p>Unsupervised contrastive learning for indoor-scene point clouds has achieved
great successes. However, unsupervised learning point clouds in outdoor scenes
remains challenging because previous methods need to reconstruct the whole
scene and capture partial views for the contrastive objective. This is
infeasible in outdoor scenes with moving objects, obstacles, and sensors. In
this paper, we propose CO^3, namely Cooperative Contrastive Learning and
Contextual Shape Prediction, to learn 3D representation for outdoor-scene point
clouds in an unsupervised manner. CO^3 has several merits compared to existing
methods. (1) It utilizes LiDAR point clouds from vehicle-side and
infrastructure-side to build views that differ enough but meanwhile maintain
common semantic information for contrastive learning, which are more
appropriate than views built by previous methods. (2) Alongside the contrastive
objective, shape context prediction is proposed as pre-training goal and brings
more task-relevant information for unsupervised 3D point cloud representation
learning, which are beneficial when transferring the learned representation to
downstream detection tasks. (3) As compared to previous methods, representation
learned by CO^3 is able to be transferred to different outdoor scene dataset
collected by different type of LiDAR sensors. (4) CO^3 improves current
state-of-the-art methods on both Once and KITTI datasets by up to 2.58 mAP.
Codes and models will be released. We believe CO^3 will facilitate
understanding LiDAR point clouds in outdoor scene.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accelerating Score-based Generative Models for High-Resolution Image Synthesis. (arXiv:2206.04029v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04029">
<div class="article-summary-box-inner">
<span><p>Score-based generative models (SGMs) have recently emerged as a promising
class of generative models. The key idea is to produce high-quality images by
recurrently adding Gaussian noises and gradients to a Gaussian sample until
converging to the target distribution, a.k.a. the diffusion sampling. To ensure
stability of convergence in sampling and generation quality, however, this
sequential sampling process has to take a small step size and many sampling
iterations (e.g., 2000). Several acceleration methods have been proposed with
focus on low-resolution generation. In this work, we consider the acceleration
of high-resolution generation with SGMs, a more challenging yet more important
problem. We prove theoretically that this slow convergence drawback is
primarily due to the ignorance of the target distribution. Further, we
introduce a novel Target Distribution Aware Sampling (TDAS) method by
leveraging the structural priors in space and frequency domains. Extensive
experiments on CIFAR-10, CelebA, LSUN, and FFHQ datasets validate that TDAS can
consistently accelerate state-of-the-art SGMs, particularly on more challenging
high resolution (1024x1024) image generation tasks by up to 18.4x, whilst
largely maintaining the synthesis quality. With fewer sampling iterations, TDAS
can still generate good quality images. In contrast, the existing methods
degrade drastically or even fails completely
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Improved One millisecond Mobile Backbone. (arXiv:2206.04040v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04040">
<div class="article-summary-box-inner">
<span><p>Efficient neural network backbones for mobile devices are often optimized for
metrics such as FLOPs or parameter count. However, these metrics may not
correlate well with latency of the network when deployed on a mobile device.
Therefore, we perform extensive analysis of different metrics by deploying
several mobile-friendly networks on a mobile device. We identify and analyze
architectural and optimization bottlenecks in recent efficient neural networks
and provide ways to mitigate these bottlenecks. To this end, we design an
efficient backbone MobileOne, with variants achieving an inference time under 1
ms on an iPhone12 with 75.9% top-1 accuracy on ImageNet. We show that MobileOne
achieves state-of-the-art performance within the efficient architectures while
being many times faster on mobile. Our best model obtains similar performance
on ImageNet as MobileFormer while being 38x faster. Our model obtains 2.3%
better top-1 accuracy on ImageNet than EfficientNet at similar latency.
Furthermore, we show that our model generalizes to multiple tasks - image
classification, object detection, and semantic segmentation with significant
improvements in latency and accuracy as compared to existing efficient
architectures when deployed on a mobile device.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Ego 3D Representation as Ray Tracing. (arXiv:2206.04042v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04042">
<div class="article-summary-box-inner">
<span><p>A self-driving perception model aims to extract 3D semantic representations
from multiple cameras collectively into the bird's-eye-view (BEV) coordinate
frame of the ego car in order to ground downstream planner. Existing perception
methods often rely on error-prone depth estimation of the whole scene or
learning sparse virtual 3D representations without the target geometry
structure, both of which remain limited in performance and/or capability. In
this paper, we present a novel end-to-end architecture for ego 3D
representation learning from an arbitrary number of unconstrained camera views.
Inspired by the ray tracing principle, we design a polarized grid of "imaginary
eyes" as the learnable ego 3D representation and formulate the learning process
with the adaptive attention mechanism in conjunction with the 3D-to-2D
projection. Critically, this formulation allows extracting rich 3D
representation from 2D images without any depth supervision, and with the
built-in geometry structure consistent w.r.t. BEV. Despite its simplicity and
versatility, extensive experiments on standard BEV visual tasks (e.g.,
camera-based 3D object detection and BEV segmentation) show that our model
outperforms all state-of-the-art alternatives significantly, with an extra
advantage in computational efficiency from multi-task learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse Fusion Mixture-of-Experts are Domain Generalizable Learners. (arXiv:2206.04046v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04046">
<div class="article-summary-box-inner">
<span><p>Domain generalization (DG) aims at learning generalizable models under
distribution shifts to avoid redundantly overfitting massive training data.
Previous works with complex loss design and gradient constraint have not yet
led to empirical success on large-scale benchmarks. In this work, we reveal the
mixture-of-experts (MoE) model's generalizability on DG by leveraging to
distributively handle multiple aspects of the predictive features across
domains. To this end, we propose Sparse Fusion Mixture-of-Experts (SF-MoE),
which incorporates sparsity and fusion mechanisms into the MoE framework to
keep the model both sparse and predictive. SF-MoE has two dedicated modules: 1)
sparse block and 2) fusion block, which disentangle and aggregate the diverse
learned signals of an object, respectively. Extensive experiments demonstrate
that SF-MoE is a domain-generalizable learner on large-scale benchmarks. It
outperforms state-of-the-art counterparts by more than 2% across 5 large-scale
DG datasets (e.g., DomainNet), with the same or even lower computational costs.
We further reveal the internal mechanism of SF-MoE from distributed
representation perspective (e.g., visual attributes). We hope this framework
could facilitate future research to push generalizable object recognition to
the real world. Code and models are released at
https://github.com/Luodian/SF-MoE-DG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Efficient Target Detection and Recognition Method in Aerial Remote-sensing Images Based on Multiangle Regions-of-Interest. (arXiv:1907.09320v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1907.09320">
<div class="article-summary-box-inner">
<span><p>Recently, deep learning technology have been extensively used in the field of
image recognition. However, its main application is the recognition and
detection of ordinary pictures and common scenes. It is challenging to
effectively and expediently analyze remote-sensing images obtained by the image
acquisition systems on unmanned aerial vehicles (UAVs), which includes the
identification of the target and calculation of its position. Aerial remote
sensing images have different shooting angles and methods compared with
ordinary pictures or images, which makes remote-sensing images play an
irreplaceable role in some areas. In this study, a new target detection and
recognition method in remote-sensing images is proposed based on deep
convolution neural network (CNN) for the provision of multilevel information of
images in combination with a region proposal network used to generate
multiangle regions-of-interest. The proposed method generated results that were
much more accurate and precise than those obtained with traditional ways. This
demonstrated that the model proposed herein displays tremendous applicability
potential in remote-sensing image recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Blacklight: Defending Black-Box Adversarial Attacks on Deep Neural Networks. (arXiv:2006.14042v2 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.14042">
<div class="article-summary-box-inner">
<span><p>Deep learning systems are known to be vulnerable to adversarial examples. In
particular, query-based black-box attacks do not require knowledge of the deep
learning model, but can compute adversarial examples over the network by
submitting queries and inspecting returns. Recent work largely improves the
efficiency of those attacks, demonstrating their practicality on today's
ML-as-a-service platforms.
</p>
<p>We propose Blacklight, a new defense against query-based black-box
adversarial attacks. The fundamental insight driving our design is that, to
compute adversarial examples, these attacks perform iterative optimization over
the network, producing image queries highly similar in the input space.
Blacklight detects query-based black-box attacks by detecting highly similar
queries, using an efficient similarity engine operating on probabilistic
content fingerprints. We evaluate Blacklight against eight state-of-the-art
attacks, across a variety of models and image classification tasks. Blacklight
identifies them all, often after only a handful of queries. By rejecting all
detected queries, Blacklight prevents any attack to complete, even when
attackers persist to submit queries after account ban or query rejection.
Blacklight is also robust against several powerful countermeasures, including
an optimal black-box attack that approximates white-box attacks in efficiency.
Finally, we illustrate how Blacklight generalizes to other domains like text
classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Surface Topography Characterization Using a Simple Optical Device and Artificial Neural Networks. (arXiv:2103.08482v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.08482">
<div class="article-summary-box-inner">
<span><p>State-of-the-art methods for quantifying wear in cylinder liners of large
internal combustion engines for stationary power generation require disassembly
and cutting of the examined liner. This is followed by laboratory-based
high-resolution microscopic surface depth measurement that quantitatively
evaluates wear based on bearing load curves (also known as Abbott-Firestone
curves). Such reference methods are destructive, time-consuming and costly. The
goal of the research presented here is to develop nondestructive yet reliable
methods for quantifying the surface topography. A novel machine learning
framework is proposed that allows prediction of the bearing load curves
representing the depth profiles from reflection RGB images of the liner
surface. These images can be collected with a simple handheld microscope. A
joint deep learning approach involving two neural network modules optimizes the
prediction quality of surface roughness parameters as well. The network stack
is trained using a custom-built database containing 422 perfectly aligned depth
profile and reflection image pairs of liner surfaces of large gas engines. The
observed success of the method suggests its great potential for on-site wear
assessment of engines during service.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Escaping the Big Data Paradigm with Compact Transformers. (arXiv:2104.05704v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05704">
<div class="article-summary-box-inner">
<span><p>With the rise of Transformers as the standard for language processing, and
their advancements in computer vision, there has been a corresponding growth in
parameter size and amounts of training data. Many have come to believe that
because of this, transformers are not suitable for small sets of data. This
trend leads to concerns such as: limited availability of data in certain
scientific domains and the exclusion of those with limited resource from
research in the field. In this paper, we aim to present an approach for
small-scale learning by introducing Compact Transformers. We show for the first
time that with the right size, convolutional tokenization, transformers can
avoid overfitting and outperform state-of-the-art CNNs on small datasets. Our
models are flexible in terms of model size, and can have as little as 0.28M
parameters while achieving competitive results. Our best model can reach 98%
accuracy when training from scratch on CIFAR-10 with only 3.7M parameters,
which is a significant improvement in data-efficiency over previous Transformer
based models being over 10x smaller than other transformers and is 15% the size
of ResNet50 while achieving similar performance. CCT also outperforms many
modern CNN based approaches, and even some recent NAS-based approaches.
Additionally, we obtain a new SOTA result on Flowers-102 with 99.76% top-1
accuracy, and improve upon the existing baseline on ImageNet (82.71% accuracy
with 29% as many parameters as ViT), as well as NLP tasks. Our simple and
compact design for transformers makes them more feasible to study for those
with limited computing resources and/or dealing with small datasets, while
extending existing research efforts in data efficient transformers. Our code
and pre-trained models are publicly available at
https://github.com/SHI-Labs/Compact-Transformers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Convolutions for Spatial Interaction Modeling. (arXiv:2104.07182v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07182">
<div class="article-summary-box-inner">
<span><p>In many different fields interactions between objects play a critical role in
determining their behavior. Graph neural networks (GNNs) have emerged as a
powerful tool for modeling interactions, although often at the cost of adding
considerable complexity and latency. In this paper, we consider the problem of
spatial interaction modeling in the context of predicting the motion of actors
around autonomous vehicles, and investigate alternatives to GNNs. We revisit 2D
convolutions and show that they can demonstrate comparable performance to graph
networks in modeling spatial interactions with lower latency, thus providing an
effective and efficient alternative in time-critical systems. Moreover, we
propose a novel interaction loss to further improve the interaction modeling of
the considered methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Random and Adversarial Bit Error Robustness: Energy-Efficient and Secure DNN Accelerators. (arXiv:2104.08323v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08323">
<div class="article-summary-box-inner">
<span><p>Deep neural network (DNN) accelerators received considerable attention in
recent years due to the potential to save energy compared to mainstream
hardware. Low-voltage operation of DNN accelerators allows to further reduce
energy consumption, however, causes bit-level failures in the memory storing
the quantized weights. Furthermore, DNN accelerators are vulnerable to
adversarial attacks on voltage controllers or individual bits. In this paper,
we show that a combination of robust fixed-point quantization, weight clipping,
as well as random bit error training (RandBET) or adversarial bit error
training (AdvBET) improves robustness against random or adversarial bit errors
in quantized DNN weights significantly. This leads not only to high energy
savings for low-voltage operation as well as low-precision quantization, but
also improves security of DNN accelerators. In contrast to related work, our
approach generalizes across operating voltages and accelerators and does not
require hardware changes. Moreover, we present a novel adversarial bit error
attack and are able to obtain robustness against both targeted and untargeted
bit-level attacks. Without losing more than 0.8%/2% in test accuracy, we can
reduce energy consumption on CIFAR10 by 20%/30% for 8/4-bit quantization.
Allowing up to 320 adversarial bit errors, we reduce test error from above 90%
(chance level) to 26.22%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GasHis-Transformer: A Multi-scale Visual Transformer Approach for Gastric Histopathological Image Detection. (arXiv:2104.14528v7 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.14528">
<div class="article-summary-box-inner">
<span><p>In this paper, a multi-scale visual transformer model, referred as
GasHis-Transformer, is proposed for Gastric Histopathological Image Detection
(GHID), which enables the automatic global detection of gastric cancer images.
GasHis-Transformer model consists of two key modules designed to extract global
and local information using a position-encoded transformer model and a
convolutional neural network with local convolution, respectively. A publicly
available hematoxylin and eosin (H&amp;E) stained gastric histopathological image
dataset is used in the experiment. Furthermore, a Dropconnect based lightweight
network is proposed to reduce the model size and training time of
GasHis-Transformer for clinical applications with improved confidence.
Moreover, a series of contrast and extended experiments verify the robustness,
extensibility and stability of GasHis-Transformer. In conclusion,
GasHis-Transformer demonstrates high global detection performance and shows its
significant potential in GHID task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Online Learning System for Wireless Charging Alignment using Surround-view Fisheye Cameras. (arXiv:2105.12763v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.12763">
<div class="article-summary-box-inner">
<span><p>Electric Vehicles are increasingly common, with inductive chargepads being
considered a convenient and efficient means of charging electric vehicles.
However, drivers are typically poor at aligning the vehicle to the necessary
accuracy for efficient inductive charging, making the automated alignment of
the two charging plates desirable. In parallel to the electrification of the
vehicular fleet, automated parking systems that make use of surround-view
camera systems are becoming increasingly popular. In this work, we propose a
system based on the surround-view camera architecture to detect, localize, and
automatically align the vehicle with the inductive chargepad. The visual design
of the chargepads is not standardized and not necessarily known beforehand.
Therefore, a system that relies on offline training will fail in some
situations. Thus, we propose a self-supervised online learning method that
leverages the driver's actions when manually aligning the vehicle with the
chargepad and combine it with weak supervision from semantic segmentation and
depth to learn a classifier to auto-annotate the chargepad in the video for
further training. In this way, when faced with a previously unseen chargepad,
the driver needs only manually align the vehicle a single time. As the
chargepad is flat on the ground, it is not easy to detect it from a distance.
Thus, we propose using a Visual SLAM pipeline to learn landmarks relative to
the chargepad to enable alignment from a greater range. We demonstrate the
working system on an automated vehicle as illustrated in the video at
https://youtu.be/_cLCmkW4UYo. To encourage further research, we will share a
chargepad dataset used in this work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cosmic-CoNN: A Cosmic Ray Detection Deep-Learning Framework, Dataset, and Toolkit. (arXiv:2106.14922v2 [astro-ph.IM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.14922">
<div class="article-summary-box-inner">
<span><p>Rejecting cosmic rays (CRs) is essential for the scientific interpretation of
CCD-captured data, but detecting CRs in single-exposure images has remained
challenging. Conventional CR detectors require experimental parameter tuning
for different instruments, and recent deep learning methods only produce
instrument-specific models that suffer from performance loss on telescopes not
included in the training data. In this work, we present Cosmic-CoNN, a generic
CR detector deployed for 24 telescopes at the Las Cumbres Observatory (LCO). We
first leverage thousands of images from LCO's global telescope network to build
a large, diverse ground-based CR dataset for rich coverage of instruments and
CR features. We then optimize a neural network and propose a novel
Median-Weighted loss function for CR detection to train a generic model that
achieves a 99.91% true-positive detection rate on LCO imaging data and
maintains over 96.40% on unseen data from Gemini GMOS-N/S, with a
false-positive rate of 0.01%. We also build a suite of tools including an
interactive CR mask visualization and editing interface, console commands, and
Python APIs to make automatic, robust CR detection widely accessible by the
community of astronomers. Our dataset, open-source codebase, and trained models
are available at https://github.com/cy-xu/cosmic-conn.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PU-Flow: a Point Cloud Upsampling Network with Normalizing Flows. (arXiv:2107.05893v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.05893">
<div class="article-summary-box-inner">
<span><p>Point cloud upsampling aims to generate dense point clouds from given sparse
ones, which is a challenging task due to the irregular and unordered nature of
point sets. To address this issue, we present a novel deep learning-based
model, called PU-Flow, which incorporates normalizing flows and weight
prediction techniques to produce dense points uniformly distributed on the
underlying surface. Specifically, we exploit the invertible characteristics of
normalizing flows to transform points between Euclidean and latent spaces and
formulate the upsampling process as ensemble of neighbouring points in a latent
space, where the ensemble weights are adaptively learned from local geometric
context. Extensive experiments show that our method is competitive and, in most
test cases, it outperforms state-of-the-art methods in terms of reconstruction
quality, proximity-to-surface accuracy, and computation efficiency. The source
code will be publicly available at https://github.com/unknownue/pu-flow.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attribution of Predictive Uncertainties in Classification Models. (arXiv:2107.08756v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.08756">
<div class="article-summary-box-inner">
<span><p>Predictive uncertainties in classification tasks are often a consequence of
model inadequacy or insufficient training data. In popular applications, such
as image processing, we are often required to scrutinise these uncertainties by
meaningfully attributing them to input features. This helps to improve
interpretability assessments. However, there exist few effective frameworks for
this purpose. Vanilla forms of popular methods for the provision of saliency
masks, such as SHAP or integrated gradients, adapt poorly to target measures of
uncertainty. Thus, state-of-the-art tools instead proceed by creating
counterfactual or adversarial feature vectors, and assign attributions by
direct comparison to original images. In this paper, we present a novel
framework that combines path integrals, counterfactual explanations and
generative models, in order to procure attributions that contain few observable
artefacts or noise. We evidence that this outperforms existing alternatives
through quantitative evaluations with popular benchmarking methods and data
sets of varying complexity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Label Cleaning Multiple Instance Learning: Refining Coarse Annotations on Single Whole-Slide Images. (arXiv:2109.10778v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10778">
<div class="article-summary-box-inner">
<span><p>Annotating cancerous regions in whole-slide images (WSIs) of pathology
samples plays a critical role in clinical diagnosis, biomedical research, and
machine learning algorithms development. However, generating exhaustive and
accurate annotations is labor-intensive, challenging, and costly. Drawing only
coarse and approximate annotations is a much easier task, less costly, and it
alleviates pathologists' workload. In this paper, we study the problem of
refining these approximate annotations in digital pathology to obtain more
accurate ones. Some previous works have explored obtaining machine learning
models from these inaccurate annotations, but few of them tackle the refinement
problem where the mislabeled regions should be explicitly identified and
corrected, and all of them require a -- often very large -- number of training
samples. We present a method, named Label Cleaning Multiple Instance Learning
(LC-MIL), to refine coarse annotations on a single WSI without the need of
external training data. Patches cropped from a WSI with inaccurate labels are
processed jointly within a multiple instance learning framework, mitigating
their impact on the predictive model and refining the segmentation. Our
experiments on a heterogeneous WSI set with breast cancer lymph node
metastasis, liver cancer, and colorectal cancer samples show that LC-MIL
significantly refines the coarse annotations, outperforming state-of-the-art
alternatives, even while learning from a single slide. Moreover, we demonstrate
how real annotations drawn by pathologists can be efficiently refined and
improved by the proposed approach. All these results demonstrate that LC-MIL is
a promising, light-weight tool to provide fine-grained annotations from
coarsely annotated pathology sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EDFace-Celeb-1M: Benchmarking Face Hallucination with a Million-scale Dataset. (arXiv:2110.05031v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05031">
<div class="article-summary-box-inner">
<span><p>Recent deep face hallucination methods show stunning performance in
super-resolving severely degraded facial images, even surpassing human ability.
However, these algorithms are mainly evaluated on non-public synthetic
datasets. It is thus unclear how these algorithms perform on public face
hallucination datasets. Meanwhile, most of the existing datasets do not well
consider the distribution of races, which makes face hallucination methods
trained on these datasets biased toward some specific races. To address the
above two problems, in this paper, we build a public Ethnically Diverse Face
dataset, EDFace-Celeb-1M, and design a benchmark task for face hallucination.
Our dataset includes 1.7 million photos that cover different countries, with
balanced race composition. To the best of our knowledge, it is the largest and
publicly available face hallucination dataset in the wild. Associated with this
dataset, this paper also contributes various evaluation protocols and provides
comprehensive analysis to benchmark the existing state-of-the-art methods. The
benchmark evaluations demonstrate the performance and limitations of
state-of-the-art algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Practical Galaxy Morphology Tools from Deep Supervised Representation Learning. (arXiv:2110.12735v2 [astro-ph.GA] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12735">
<div class="article-summary-box-inner">
<span><p>Astronomers have typically set out to solve supervised machine learning
problems by creating their own representations from scratch. We show that deep
learning models trained to answer every Galaxy Zoo DECaLS question learn
meaningful semantic representations of galaxies that are useful for new tasks
on which the models were never trained. We exploit these representations to
outperform several recent approaches at practical tasks crucial for
investigating large galaxy samples. The first task is identifying galaxies of
similar morphology to a query galaxy. Given a single galaxy assigned a free
text tag by humans (e.g. "#diffuse"), we can find galaxies matching that tag
for most tags. The second task is identifying the most interesting anomalies to
a particular researcher. Our approach is 100% accurate at identifying the most
interesting 100 anomalies (as judged by Galaxy Zoo 2 volunteers). The third
task is adapting a model to solve a new task using only a small number of
newly-labelled galaxies. Models fine-tuned from our representation are better
able to identify ring galaxies than models fine-tuned from terrestrial images
(ImageNet) or trained from scratch. We solve each task with very few new
labels; either one (for the similarity search) or several hundred (for anomaly
detection or fine-tuning). This challenges the longstanding view that deep
supervised methods require new large labelled datasets for practical use in
astronomy. To help the community benefit from our pretrained models, we release
our fine-tuning code Zoobot. Zoobot is accessible to researchers with no prior
experience in deep learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Pruned Structure and Weights Simultaneously from Scratch: an Attention based Approach. (arXiv:2111.02399v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02399">
<div class="article-summary-box-inner">
<span><p>As a deep learning model typically contains millions of trainable weights,
there has been a growing demand for a more efficient network structure with
reduced storage space and improved run-time efficiency. Pruning is one of the
most popular network compression techniques. In this paper, we propose a novel
unstructured pruning pipeline, Attention-based Simultaneous sparse structure
and Weight Learning (ASWL). Unlike traditional channel-wise or weight-wise
attention mechanism, ASWL proposed an efficient algorithm to calculate the
pruning ratio through layer-wise attention for each layer, and both weights for
the dense network and the sparse network are tracked so that the pruned
structure is simultaneously learned from randomly initialized weights. Our
experiments on MNIST, Cifar10, and ImageNet show that ASWL achieves superior
pruning results in terms of accuracy, pruning ratio and operating efficiency
when compared with state-of-the-art network pruning methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HEAT: Holistic Edge Attention Transformer for Structured Reconstruction. (arXiv:2111.15143v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.15143">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel attention-based neural network for structured
reconstruction, which takes a 2D raster image as an input and reconstructs a
planar graph depicting an underlying geometric structure. The approach detects
corners and classifies edge candidates between corners in an end-to-end manner.
Our contribution is a holistic edge classification architecture, which 1)
initializes the feature of an edge candidate by a trigonometric positional
encoding of its end-points; 2) fuses image feature to each edge candidate by
deformable attention; 3) employs two weight-sharing Transformer decoders to
learn holistic structural patterns over the graph edge candidates; and 4) is
trained with a masked learning strategy. The corner detector is a variant of
the edge classification architecture, adapted to operate on pixels as corner
candidates. We conduct experiments on two structured reconstruction tasks:
outdoor building architecture and indoor floorplan planar graph reconstruction.
Extensive qualitative and quantitative evaluations demonstrate the superiority
of our approach over the state of the art. Code and pre-trained models are
available at https://heat-structured-reconstruction.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting Axillary Lymph Node Metastasis in Early Breast Cancer Using Deep Learning on Primary Tumor Biopsy Slides. (arXiv:2112.02222v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02222">
<div class="article-summary-box-inner">
<span><p>Objectives: To develop and validate a deep learning (DL)-based primary tumor
biopsy signature for predicting axillary lymph node (ALN) metastasis
preoperatively in early breast cancer (EBC) patients with clinically negative
ALN.
</p>
<p>Methods: A total of 1,058 EBC patients with pathologically confirmed ALN
status were enrolled from May 2010 to August 2020. A DL core-needle biopsy
(DL-CNB) model was built on the attention-based multiple instance-learning
(AMIL) framework to predict ALN status utilizing the DL features, which were
extracted from the cancer areas of digitized whole-slide images (WSIs) of
breast CNB specimens annotated by two pathologists. Accuracy, sensitivity,
specificity, receiver operating characteristic (ROC) curves, and areas under
the ROC curve (AUCs) were analyzed to evaluate our model.
</p>
<p>Results: The best-performing DL-CNB model with VGG16_BN as the feature
extractor achieved an AUC of 0.816 (95% confidence interval (CI): 0.758, 0.865)
in predicting positive ALN metastasis in the independent test cohort.
Furthermore, our model incorporating the clinical data, which was called
DL-CNB+C, yielded the best accuracy of 0.831 (95%CI: 0.775, 0.878), especially
for patients younger than 50 years (AUC: 0.918, 95%CI: 0.825, 0.971). The
interpretation of DL-CNB model showed that the top signatures most predictive
of ALN metastasis were characterized by the nucleus features including density
($p$ = 0.015), circumference ($p$ = 0.009), circularity ($p$ = 0.010), and
orientation ($p$ = 0.012).
</p>
<p>Conclusion: Our study provides a novel DL-based biomarker on primary tumor
CNB slides to predict the metastatic status of ALN preoperatively for patients
with EBC. The codes and dataset are available at
https://github.com/bupt-ai-cz/BALNMP
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Hierarchical Refinement and Augmentation for Unsupervised Learning of Depth and Pose from Monocular Video. (arXiv:2112.03045v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03045">
<div class="article-summary-box-inner">
<span><p>Depth and ego-motion estimations are essential for the localization and
navigation of autonomous robots and autonomous driving. Recent studies make it
possible to learn the per-pixel depth and ego-motion from the unlabeled
monocular video. A novel unsupervised training framework is proposed with 3D
hierarchical refinement and augmentation using explicit 3D geometry. In this
framework, the depth and pose estimations are hierarchically and mutually
coupled to refine the estimated pose layer by layer. The intermediate view
image is proposed and synthesized by warping the pixels in an image with the
estimated depth and coarse pose. Then, the residual pose transformation can be
estimated from the new view image and the image of the adjacent frame to refine
the coarse pose. The iterative refinement is implemented in a differentiable
manner in this paper, making the whole framework optimized uniformly.
Meanwhile, a new image augmentation method is proposed for the pose estimation
by synthesizing a new view image, which creatively augments the pose in 3D
space but gets a new augmented 2D image. The experiments on KITTI demonstrate
that our depth estimation achieves state-of-the-art performance and even
surpasses recent approaches that utilize other auxiliary tasks. Our visual
odometry outperforms all recent unsupervised monocular learning-based methods
and achieves competitive performance to the geometry-based method, ORB-SLAM2
with back-end optimization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Smoothness and effective regularizations in learned embeddings for shape matching. (arXiv:2112.07289v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07289">
<div class="article-summary-box-inner">
<span><p>Many innovative applications require establishing correspondences among 3D
geometric objects. However, the countless possible deformations of smooth
surfaces make shape matching a challenging task. Finding an embedding to
represent the different shapes in high-dimensional space where the matching is
easier to solve is a well-trodden path that has given many outstanding
solutions. Recently, a new trend has shown advantages in learning such
representations. This novel idea motivated us to investigate which properties
differentiate these data-driven embeddings and which ones promote
state-of-the-art results. In this study, we analyze, for the first time,
properties that arise in data-driven learned embedding and their relation to
the shape-matching task. Our discoveries highlight the close link between
matching and smoothness, which naturally emerge from training. Also, we
demonstrate the relation between the orthogonality of the embedding and the
bijectivity of the correspondence. Our experiments show exciting results,
overcoming well-established alternatives and shedding a different light on
relevant contexts and properties for learned embeddings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">General Greedy De-bias Learning. (arXiv:2112.10572v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10572">
<div class="article-summary-box-inner">
<span><p>Neural networks often make predictions relying on the spurious correlations
from the datasets rather than the intrinsic properties of the task of interest,
facing sharp degradation on out-of-distribution (OOD) test data. Existing
de-bias learning frameworks try to capture specific dataset bias by annotations
but they fail to handle complicated OOD scenarios. Others implicitly identify
the dataset bias by special design low capability biased models or losses, but
they degrade when the training and testing data are from the same distribution.
In this paper, we propose a General Greedy De-bias learning framework (GGD),
which greedily trains the biased models and the base model. The base model is
encouraged to focus on examples that are hard to solve with biased models, thus
remaining robust against spurious correlations in the test stage. GGD largely
improves models' OOD generalization ability on various tasks, but sometimes
over-estimates the bias level and degrades on the in-distribution test. We
further re-analyze the ensemble process of GGD and introduce the Curriculum
Regularization inspired by curriculum learning, which achieves a good trade-off
between in-distribution and out-of-distribution performance. Extensive
experiments on image classification, adversarial question answering, and visual
question answering demonstrate the effectiveness of our method. GGD can learn a
more robust base model under the settings of both task-specific biased models
with prior knowledge and self-ensemble biased model without prior knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Roadside Lidar Vehicle Detection and Tracking Using Range And Intensity Background Subtraction. (arXiv:2201.04756v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04756">
<div class="article-summary-box-inner">
<span><p>In this paper, we developed the solution of roadside LiDAR object detection
using a combination of two unsupervised learning algorithms. The 3D point
clouds are firstly converted into spherical coordinates and filled into the
elevation-azimuth matrix using a hash function. After that, the raw LiDAR data
were rearranged into new data structures to store the information of range,
azimuth, and intensity. Then, the Dynamic Mode Decomposition method is applied
to decompose the LiDAR data into low-rank backgrounds and sparse foregrounds
based on intensity channel pattern recognition. The Coarse Fine Triangle
Algorithm (CFTA) automatically finds the dividing value to separate the moving
targets from static background according to range information. After intensity
and range background subtraction, the foreground moving objects will be
detected using a density-based detector and encoded into the state-space model
for tracking. The output of the proposed solution includes vehicle trajectories
that can enable many mobility and safety applications. The method was validated
at both path and point levels and outperformed the state-of-the-art. In
contrast to the previous methods that process directly on the scattered and
discrete point clouds, the dynamic classification method can establish the less
sophisticated linear relationship of the 3D measurement data, which captures
the spatial-temporal structure that we often desire.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Density Estimation from Schlieren Images through Machine Learning. (arXiv:2201.05233v2 [physics.flu-dyn] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05233">
<div class="article-summary-box-inner">
<span><p>This study proposes a radically alternate approach for extracting
quantitative information from schlieren images. The method uses a scaled,
derivative enhanced Gaussian process model to obtain true density estimates
from two corresponding schlieren images with the knife-edge at horizontal and
vertical orientations. We illustrate our approach on schlieren images taken
from a wind tunnel sting model, a supersonic aircraft in flight, and a
high-order numerical shock tube simulation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Single-shot Depth Estimation using Perceptual Reconstruction. (arXiv:2201.12170v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12170">
<div class="article-summary-box-inner">
<span><p>Real-time estimation of actual object depth is an essential module for
various autonomous system tasks such as 3D reconstruction, scene understanding
and condition assessment. During the last decade of machine learning, extensive
deployment of deep learning methods to computer vision tasks has yielded
approaches that succeed in achieving realistic depth synthesis out of a simple
RGB modality. Most of these models are based on paired RGB-depth data and/or
the availability of video sequences and stereo images. The lack of sequences,
stereo data and RGB-depth pairs makes depth estimation a fully unsupervised
single-image transfer problem that has barely been explored so far. This study
builds on recent advances in the field of generative neural networks in order
to establish fully unsupervised single-shot depth estimation. Two generators
for RGB-to-depth and depth-to-RGB transfer are implemented and simultaneously
optimized using the Wasserstein-1 distance, a novel perceptual reconstruction
term and hand-crafted image filters. We comprehensively evaluate the models
using industrial surface depth data as well as the Texas 3D Face Recognition
Database, the CelebAMask-HQ database of human portraits and the SURREAL dataset
that records body depth. For each evaluation dataset the proposed method shows
a significant increase in depth accuracy compared to state-of-the-art
single-image transfer methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probabilistically Robust Learning: Balancing Average- and Worst-case Performance. (arXiv:2202.01136v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01136">
<div class="article-summary-box-inner">
<span><p>Many of the successes of machine learning are based on minimizing an averaged
loss function. However, it is well-known that this paradigm suffers from
robustness issues that hinder its applicability in safety-critical domains.
These issues are often addressed by training against worst-case perturbations
of data, a technique known as adversarial training. Although empirically
effective, adversarial training can be overly conservative, leading to
unfavorable trade-offs between nominal performance and robustness. To this end,
in this paper we propose a framework called probabilistic robustness that
bridges the gap between the accurate, yet brittle average case and the robust,
yet conservative worst case by enforcing robustness to most rather than to all
perturbations. From a theoretical point of view, this framework overcomes the
trade-offs between the performance and the sample-complexity of worst-case and
average-case learning. From a practical point of view, we propose a novel
algorithm based on risk-aware optimization that effectively balances average-
and worst-case performance at a considerably lower computational cost relative
to adversarial training. Our results on MNIST, CIFAR-10, and SVHN illustrate
the advantages of this framework on the spectrum from average- to worst-case
robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dataset Condensation with Contrastive Signals. (arXiv:2202.02916v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02916">
<div class="article-summary-box-inner">
<span><p>Recent studies have demonstrated that gradient matching-based dataset
synthesis, or dataset condensation (DC), methods can achieve state-of-the-art
performance when applied to data-efficient learning tasks. However, in this
study, we prove that the existing DC methods can perform worse than the random
selection method when task-irrelevant information forms a significant part of
the training dataset. We attribute this to the lack of participation of the
contrastive signals between the classes resulting from the class-wise gradient
matching strategy. To address this problem, we propose Dataset Condensation
with Contrastive signals (DCC) by modifying the loss function to enable the DC
methods to effectively capture the differences between classes. In addition, we
analyze the new loss function in terms of training dynamics by tracking the
kernel velocity. Furthermore, we introduce a bi-level warm-up strategy to
stabilize the optimization. Our experimental results indicate that while the
existing methods are ineffective for fine-grained image classification tasks,
the proposed method can successfully generate informative synthetic datasets
for the same tasks. Moreover, we demonstrate that the proposed method
outperforms the baselines even on benchmark datasets such as SVHN, CIFAR-10,
and CIFAR-100. Finally, we demonstrate the high applicability of the proposed
method by applying it to continual learning tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MOST-Net: A Memory Oriented Style Transfer Network for Face Sketch Synthesis. (arXiv:2202.03596v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03596">
<div class="article-summary-box-inner">
<span><p>Face sketch synthesis has been widely used in multi-media entertainment and
law enforcement. Despite the recent developments in deep neural networks,
accurate and realistic face sketch synthesis is still a challenging task due to
the diversity and complexity of human faces. Current image-to-image
translation-based face sketch synthesis frequently encounters over-fitting
problems when it comes to small-scale datasets. To tackle this problem, we
present an end-to-end Memory Oriented Style Transfer Network (MOST-Net) for
face sketch synthesis which can produce high-fidelity sketches with limited
data. Specifically, an external self-supervised dynamic memory module is
introduced to capture the domain alignment knowledge in the long term. In this
way, our proposed model could obtain the domain-transfer ability by
establishing the durable relationship between faces and corresponding sketches
on the feature level. Furthermore, we design a novel Memory Refinement Loss (MR
Loss) for feature alignment in the memory module, which enhances the accuracy
of memory slots in an unsupervised manner. Extensive experiments on the CUFS
and the CUFSF datasets show that our MOST-Net achieves state-of-the-art
performance, especially in terms of the Structural Similarity Index(SSIM).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Do Vision Transformers Work?. (arXiv:2202.06709v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06709">
<div class="article-summary-box-inner">
<span><p>The success of multi-head self-attentions (MSAs) for computer vision is now
indisputable. However, little is known about how MSAs work. We present
fundamental explanations to help better understand the nature of MSAs. In
particular, we demonstrate the following properties of MSAs and Vision
Transformers (ViTs): (1) MSAs improve not only accuracy but also generalization
by flattening the loss landscapes. Such improvement is primarily attributable
to their data specificity, not long-range dependency. On the other hand, ViTs
suffer from non-convex losses. Large datasets and loss landscape smoothing
methods alleviate this problem; (2) MSAs and Convs exhibit opposite behaviors.
For example, MSAs are low-pass filters, but Convs are high-pass filters.
Therefore, MSAs and Convs are complementary; (3) Multi-stage neural networks
behave like a series connection of small individual models. In addition, MSAs
at the end of a stage play a key role in prediction. Based on these insights,
we propose AlterNet, a model in which Conv blocks at the end of a stage are
replaced with MSA blocks. AlterNet outperforms CNNs not only in large data
regimes but also in small data regimes. The code is available at
https://github.com/xxxnell/how-do-vits-work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What's in the Black Box? The False Negative Mechanisms Inside Object Detectors. (arXiv:2203.07662v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07662">
<div class="article-summary-box-inner">
<span><p>In object detection, false negatives arise when a detector fails to detect a
target object. To understand why object detectors produce false negatives, we
identify five 'false negative mechanisms', where each mechanism describes how a
specific component inside the detector architecture failed. Focusing on
two-stage and one-stage anchor-box object detector architectures, we introduce
a framework for quantifying these false negative mechanisms. Using this
framework, we investigate why Faster R-CNN and RetinaNet fail to detect objects
in benchmark vision datasets and robotics datasets. We show that a detector's
false negative mechanisms differ significantly between computer vision
benchmark datasets and robotics deployment scenarios. This has implications for
the translation of object detectors developed for benchmark datasets to
robotics applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">iPLAN: Interactive and Procedural Layout Planning. (arXiv:2203.14412v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14412">
<div class="article-summary-box-inner">
<span><p>Layout design is ubiquitous in many applications, e.g. architecture/urban
planning, etc, which involves a lengthy iterative design process. Recently,
deep learning has been leveraged to automatically generate layouts via image
generation, showing a huge potential to free designers from laborious routines.
While automatic generation can greatly boost productivity, designer input is
undoubtedly crucial. An ideal AI-aided design tool should automate repetitive
routines, and meanwhile accept human guidance and provide smart/proactive
suggestions. However, the capability of involving humans into the loop has been
largely ignored in existing methods which are mostly end-to-end approaches. To
this end, we propose a new human-in-the-loop generative model, iPLAN, which is
capable of automatically generating layouts, but also interacting with
designers throughout the whole procedure, enabling humans and AI to co-evolve a
sketchy idea gradually into the final design. iPLAN is evaluated on diverse
datasets and compared with existing methods. The results show that iPLAN has
high fidelity in producing similar layouts to those from human designers, great
flexibility in accepting designer inputs and providing design suggestions
accordingly, and strong generalizability when facing unseen design tasks and
limited training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified Transformer Tracker for Object Tracking. (arXiv:2203.15175v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15175">
<div class="article-summary-box-inner">
<span><p>As an important area in computer vision, object tracking has formed two
separate communities that respectively study Single Object Tracking (SOT) and
Multiple Object Tracking (MOT). However, current methods in one tracking
scenario are not easily adapted to the other due to the divergent training
datasets and tracking objects of both tasks. Although UniTrack
\cite{wang2021different} demonstrates that a shared appearance model with
multiple heads can be used to tackle individual tracking tasks, it fails to
exploit the large-scale tracking datasets for training and performs poorly on
single object tracking. In this work, we present the Unified Transformer
Tracker (UTT) to address tracking problems in different scenarios with one
paradigm. A track transformer is developed in our UTT to track the target in
both SOT and MOT. The correlation between the target and tracking frame
features is exploited to localize the target. We demonstrate that both SOT and
MOT tasks can be solved within this framework. The model can be simultaneously
end-to-end trained by alternatively optimizing the SOT and MOT objectives on
the datasets of individual tasks. Extensive experiments are conducted on
several benchmarks with a unified model trained on SOT and MOT datasets. Code
will be available at https://github.com/Flowerfan/Trackron.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Residual Mixture of Experts. (arXiv:2204.09636v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.09636">
<div class="article-summary-box-inner">
<span><p>Mixture of Experts (MoE) is able to scale up vision transformers effectively.
However, it requires prohibiting computation resources to train a large MoE
transformer. In this paper, we propose Residual Mixture of Experts (RMoE), an
efficient training pipeline for MoE vision transformers on downstream tasks,
such as segmentation and detection. RMoE achieves comparable results with the
upper-bound MoE training, while only introducing minor additional training cost
than the lower-bound non-MoE training pipelines. The efficiency is supported by
our key observation: the weights of an MoE transformer can be factored into an
input-independent core and an input-dependent residual. Compared with the
weight core, the weight residual can be efficiently trained with much less
computation resource, e.g., finetuning on the downstream data. We show that,
compared with the current MoE training pipeline, we get comparable results
while saving over 30% training cost. When compared with state-of-the-art non-
MoE transformers, such as Swin-T / CvT-13 / Swin-L, we get +1.1 / 0.9 / 1.0
mIoU gain on ADE20K segmentation and +1.4 / 1.6 / 0.6 AP gain on MS-COCO object
detection task with less than 3% additional training cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Iterative Labeling Method for Annotating Fisheries Imagery. (arXiv:2204.12934v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12934">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a methodology for fisheries-related data that
allows us to converge on a labeled image dataset by iterating over the dataset
with multiple training and production loops that can exploit crowdsourcing
interfaces. We present our algorithm and its results on two separate sets of
image data collected using the Seabed autonomous underwater vehicle. The first
dataset comprises of 2,026 completely unlabeled images, while the second
consists of 21,968 images that were point annotated by experts. Our results
indicate that training with a small subset and iterating on that to build a
larger set of labeled data allows us to converge to a fully annotated dataset
with a small number of iterations. Even in the case of a dataset labeled by
experts, a single iteration of the methodology improves the labels by
discovering additional complicated examples of labels associated with fish that
overlap, are very small, or obscured by the contrast limitations associated
with underwater imagery.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Visual Grounding with Visual-Linguistic Verification and Iterative Reasoning. (arXiv:2205.00272v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00272">
<div class="article-summary-box-inner">
<span><p>Visual grounding is a task to locate the target indicated by a natural
language expression. Existing methods extend the generic object detection
framework to this problem. They base the visual grounding on the features from
pre-generated proposals or anchors, and fuse these features with the text
embeddings to locate the target mentioned by the text. However, modeling the
visual features from these predefined locations may fail to fully exploit the
visual context and attribute information in the text query, which limits their
performance. In this paper, we propose a transformer-based framework for
accurate visual grounding by establishing text-conditioned discriminative
features and performing multi-stage cross-modal reasoning. Specifically, we
develop a visual-linguistic verification module to focus the visual features on
regions relevant to the textual descriptions while suppressing the unrelated
areas. A language-guided feature encoder is also devised to aggregate the
visual contexts of the target object to improve the object's distinctiveness.
To retrieve the target from the encoded visual features, we further propose a
multi-stage cross-modal decoder to iteratively speculate on the correlations
between the image and text for accurate target localization. Extensive
experiments on five widely used datasets validate the efficacy of our proposed
components and demonstrate state-of-the-art performance. Our code is public at
https://github.com/yangli18/VLTVG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond a Pre-Trained Object Detector: Cross-Modal Textual and Visual Context for Image Captioning. (arXiv:2205.04363v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04363">
<div class="article-summary-box-inner">
<span><p>Significant progress has been made on visual captioning, largely relying on
pre-trained features and later fixed object detectors that serve as rich inputs
to auto-regressive models. A key limitation of such methods, however, is that
the output of the model is conditioned only on the object detector's outputs.
The assumption that such outputs can represent all necessary information is
unrealistic, especially when the detector is transferred across datasets. In
this work, we reason about the graphical model induced by this assumption, and
propose to add an auxiliary input to represent missing information such as
object relationships. We specifically propose to mine attributes and
relationships from the Visual Genome dataset and condition the captioning model
on them. Crucially, we propose (and show to be important) the use of a
multi-modal pre-trained model (CLIP) to retrieve such contextual descriptions.
Further, object detector models are frozen and do not have sufficient richness
to allow the captioning model to properly ground them. As a result, we propose
to condition both the detector and description outputs on the image, and show
qualitatively and quantitatively that this can improve grounding. We validate
our method on image captioning, perform thorough analyses of each component and
importance of the pre-trained multi-modal model, and demonstrate significant
improvements over the current state of the art, specifically +7.5% in CIDEr and
+1.3% in BLEU-4 metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-Time Video Deblurring via Lightweight Motion Compensation. (arXiv:2205.12634v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12634">
<div class="article-summary-box-inner">
<span><p>While motion compensation greatly improves video deblurring quality,
separately performing motion compensation and video deblurring demands huge
computational overhead. This paper proposes a real-time video deblurring
framework consisting of a lightweight multi-task unit that supports both video
deblurring and motion compensation in an efficient way. The multi-task unit is
specifically designed to handle large portions of the two tasks using a single
shared network, and consists of a multi-task detail network and simple networks
for deblurring and motion compensation. The multi-task unit minimizes the cost
of incorporating motion compensation into video deblurring and enables
real-time deblurring. Moreover, by stacking multiple multi-task units, our
framework provides flexible control between the cost and deblurring quality. We
experimentally validate the state-of-the-art deblurring quality of our
approach, which runs at a much faster speed compared to previous methods, and
show practical real-time performance (30.99dB@30fps measured in the DVD
dataset).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scalable Interpretability via Polynomials. (arXiv:2205.14108v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14108">
<div class="article-summary-box-inner">
<span><p>Generalized Additive Models (GAMs) have quickly become the leading choice for
fully-interpretable machine learning. However, unlike uninterpretable methods
such as DNNs, they lack expressive power and easy scalability, and are hence
not a feasible alternative for real-world tasks. We present a new class of GAMs
that use tensor rank decompositions of polynomials to learn powerful, {\em
fully-interpretable} models. Our approach, titled Scalable Polynomial Additive
Models (SPAM) is effortlessly scalable and models {\em all} higher-order
feature interactions without a combinatorial parameter explosion. SPAM
outperforms all current interpretable approaches, and matches DNN/XGBoost
performance on a series of real-world benchmarks with up to hundreds of
thousands of features. We demonstrate by human subject evaluations that SPAMs
are demonstrably more interpretable in practice, and are hence an effortless
replacement for DNNs for creating interpretable and high-performance systems
suitable for large-scale machine learning. Source code is available at
https://github.com/facebookresearch/nbm-spam.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Basis Models for Interpretability. (arXiv:2205.14120v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14120">
<div class="article-summary-box-inner">
<span><p>Due to the widespread use of complex machine learning models in real-world
applications, it is becoming critical to explain model predictions. However,
these models are typically black-box deep neural networks, explained post-hoc
via methods with known faithfulness limitations. Generalized Additive Models
(GAMs) are an inherently interpretable class of models that address this
limitation by learning a non-linear shape function for each feature separately,
followed by a linear model on top. However, these models are typically
difficult to train, require numerous parameters, and are difficult to scale.
</p>
<p>We propose an entirely new subfamily of GAMs that utilizes basis
decomposition of shape functions. A small number of basis functions are shared
among all features, and are learned jointly for a given task, thus making our
model scale much better to large-scale data with high-dimensional features,
especially when features are sparse. We propose an architecture denoted as the
Neural Basis Model (NBM) which uses a single neural network to learn these
bases. On a variety of tabular and image datasets, we demonstrate that for
interpretable machine learning, NBMs are the state-of-the-art in accuracy,
model size, and, throughput and can easily model all higher-order feature
interactions.
</p>
<p>Source code is available at https://github.com/facebookresearch/nbm-spam.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Point RCNN: An Angle-Free Framework for Rotated Object Detection. (arXiv:2205.14328v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14328">
<div class="article-summary-box-inner">
<span><p>Rotated object detection in aerial images is still challenging due to
arbitrary orientations, large scale and aspect ratio variations, and extreme
density of objects. Existing state-of-the-art rotated object detection methods
mainly rely on angle-based detectors. However, angle regression can easily
suffer from the long-standing boundary problem. To tackle this problem, we
propose a purely angle-free framework for rotated object detection, called
Point RCNN, which mainly consists of PointRPN and PointReg. In particular,
PointRPN generates accurate rotated RoIs (RRoIs) by converting the learned
representative points with a coarse-to-fine manner, which is motivated by
RepPoints. Based on the learned RRoIs, PointReg performs corner points
refinement for more accurate detection. In addition, aerial images are often
severely unbalanced in categories, and existing methods almost ignore this
issue. In this paper, we also experimentally verify that re-sampling the images
of the rare categories will stabilize training and further improve the
detection performance. Experiments demonstrate that our Point RCNN achieves the
new state-of-the-art detection performance on commonly used aerial datasets,
including DOTA-v1.0, DOTA-v1.5, and HRSC2016.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differentiable Point-Based Radiance Fields for Efficient View Synthesis. (arXiv:2205.14330v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14330">
<div class="article-summary-box-inner">
<span><p>We propose a differentiable rendering algorithm for efficient novel view
synthesis. By departing from volume-based representations in favor of a learned
point representation, we improve on existing methods more than an order of
magnitude in memory and runtime, both in training and inference. The method
begins with a uniformly-sampled random point cloud and learns per-point
position and view-dependent appearance, using a differentiable splat-based
renderer to evolve the model to match a set of input images. Our method is up
to 300x faster than NeRF in both training and inference, with only a marginal
sacrifice in quality, while using less than 10~MB of memory for a static scene.
For dynamic scenes, our method trains two orders of magnitude faster than
STNeRF and renders at near interactive rate, while maintaining high image
quality and temporal coherence even without imposing any temporal-coherency
regularizers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CAINNFlow: Convolutional block Attention modules and Invertible Neural Networks Flow for anomaly detection and localization tasks. (arXiv:2206.01992v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01992">
<div class="article-summary-box-inner">
<span><p>Detection of object anomalies is crucial in industrial processes, but
unsupervised anomaly detection and localization is particularly important due
to the difficulty of obtaining a large number of defective samples and the
unpredictable types of anomalies in real life. Among the existing unsupervised
anomaly detection and localization methods, the NF-based scheme has achieved
better results. However, the two subnets (complex functions) $s_{i}(u_{i})$ and
$t_{i}(u_{i})$ in NF are usually multilayer perceptrons, which need to squeeze
the input visual features from 2D flattening to 1D, destroying the spatial
location relationship in the feature map and losing the spatial structure
information. In order to retain and effectively extract spatial structure
information, we design in this study a complex function model with alternating
CBAM embedded in a stacked $3\times3$ full convolution, which is able to retain
and effectively extract spatial structure information in the normalized flow
model. Extensive experimental results on the MVTec AD dataset show that
CAINNFlow achieves advanced levels of accuracy and inference efficiency based
on CNN and Transformer backbone networks as feature extractors, and CAINNFlow
achieves a pixel-level AUC of $98.64\%$ for anomaly detection in MVTec AD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Individual Grevy's Zebra Identification via Deep 3D Fitting and Metric Learning. (arXiv:2206.02261v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02261">
<div class="article-summary-box-inner">
<span><p>This paper combines deep learning techniques for species detection, 3D model
fitting, and metric learning in one pipeline to perform individual animal
identification from photographs by exploiting unique coat patterns. This is the
first work to attempt this and, compared to traditional 2D bounding box or
segmentation based CNN identification pipelines, the approach provides
effective and explicit view-point normalisation and allows for a straight
forward visualisation of the learned biometric population space. Note that due
to the use of metric learning the pipeline is also readily applicable to open
set and zero shot re-identification scenarios. We apply the proposed approach
to individual Grevy's zebra (Equus grevyi) identification and show in a small
study on the SMALST dataset that the use of 3D model fitting can indeed benefit
performance. In particular, back-projected textures from 3D fitted models
improve identification accuracy from 48.0% to 56.8% compared to 2D bounding box
approaches for the dataset. Whilst the study is far too small accurately to
estimate the full performance potential achievable in larger-scale real-world
application settings and in comparisons against polished tools, our work lays
the conceptual and practical foundations for a next step in animal biometrics
towards deep metric learning driven, fully 3D-aware animal identification in
open population settings. We publish network weights and relevant facilitating
source code with this paper for full reproducibility and as inspiration for
further research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Just Vision: A Review on Self-Supervised Representation Learning on Multimodal and Temporal Data. (arXiv:2206.02353v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.02353">
<div class="article-summary-box-inner">
<span><p>Recently, Self-Supervised Representation Learning (SSRL) has attracted much
attention in the field of computer vision, speech, natural language processing
(NLP), and recently, with other types of modalities, including time series from
sensors. The popularity of self-supervised learning is driven by the fact that
traditional models typically require a huge amount of well-annotated data for
training. Acquiring annotated data can be a difficult and costly process.
Self-supervised methods have been introduced to improve the efficiency of
training data through discriminative pre-training of models using supervisory
signals that have been freely obtained from the raw data. Unlike existing
reviews of SSRL that have pre-dominately focused upon methods in the fields of
CV or NLP for a single modality, we aim to provide the first comprehensive
review of multimodal self-supervised learning methods for temporal data. To
this end, we 1) provide a comprehensive categorization of existing SSRL
methods, 2) introduce a generic pipeline by defining the key components of a
SSRL framework, 3) compare existing models in terms of their objective
function, network architecture and potential applications, and 4) review
existing multimodal techniques in each category and various modalities.
Finally, we present existing weaknesses and future opportunities. We believe
our work develops a perspective on the requirements of SSRL in domains that
utilise multimodal and/or temporal data
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning Techniques for Visual Counting. (arXiv:2206.03033v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03033">
<div class="article-summary-box-inner">
<span><p>In this dissertation, we investigated and enhanced Deep Learning (DL)
techniques for counting objects, like pedestrians, cells or vehicles, in still
images or video frames. In particular, we tackled the challenge related to the
lack of data needed for training current DL-based solutions. Given that the
budget for labeling is limited, data scarcity still represents an open problem
that prevents the scalability of existing solutions based on the supervised
learning of neural networks and that is responsible for a significant drop in
performance at inference time when new scenarios are presented to these
algorithms. We introduced solutions addressing this issue from several
complementary sides, collecting datasets gathered from virtual environments
automatically labeled, proposing Domain Adaptation strategies aiming at
mitigating the domain gap existing between the training and test data
distributions, and presenting a counting strategy in a weakly labeled data
scenario, i.e., in the presence of non-negligible disagreement between multiple
annotators. Moreover, we tackled the non-trivial engineering challenges coming
out of the adoption of Convolutional Neural Network-based techniques in
environments with limited power resources, introducing solutions for counting
vehicles and pedestrians directly onboard embedded vision systems, i.e.,
devices equipped with constrained computational capabilities that can capture
images and elaborate them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast and Robust Non-Rigid Registration Using Accelerated Majorization-Minimization. (arXiv:2206.03410v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03410">
<div class="article-summary-box-inner">
<span><p>Non-rigid registration, which deforms a source shape in a non-rigid way to
align with a target shape, is a classical problem in computer vision. Such
problems can be challenging because of imperfect data (noise, outliers and
partial overlap) and high degrees of freedom. Existing methods typically adopt
the $\ell_{p}$ type robust norm to measure the alignment error and regularize
the smoothness of deformation, and use a proximal algorithm to solve the
resulting non-smooth optimization problem. However, the slow convergence of
such algorithms limits their wide applications. In this paper, we propose a
formulation for robust non-rigid registration based on a globally smooth robust
norm for alignment and regularization, which can effectively handle outliers
and partial overlaps. The problem is solved using the majorization-minimization
algorithm, which reduces each iteration to a convex quadratic problem with a
closed-form solution. We further apply Anderson acceleration to speed up the
convergence of the solver, enabling the solver to run efficiently on devices
with limited compute capability. Extensive experiments demonstrate the
effectiveness of our method for non-rigid alignment between two shapes with
outliers and partial overlaps, with quantitative evaluation showing that it
outperforms state-of-the-art methods in terms of registration accuracy and
computational speed. The source code is available at
https://github.com/yaoyx689/AMM_NRR.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-06-09 23:08:07.955506831 UTC">2022-06-09 23:08:07 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>