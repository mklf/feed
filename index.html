<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-05-12T01:30:00Z">05-12</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Problems with Cosine as a Measure of Embedding Similarity for High Frequency Words. (arXiv:2205.05092v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05092">
<div class="article-summary-box-inner">
<span><p>Cosine similarity of contextual embeddings is used in many NLP tasks (e.g.,
QA, IR, MT) and metrics (e.g., BERTScore). Here, we uncover systematic ways in
which word similarities estimated by cosine over BERT embeddings are
understated and trace this effect to training data frequency. We find that
relative to human judgements, cosine similarity underestimates the similarity
of frequent words with other instances of the same word or other words across
contexts, even after controlling for polysemy and other factors. We conjecture
that this underestimation of similarity for high frequency words is due to
differences in the representational geometry of high and low frequency words
and provide a formal argument for the two-dimensional case.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Richer Countries and Richer Representations. (arXiv:2205.05093v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05093">
<div class="article-summary-box-inner">
<span><p>We examine whether some countries are more richly represented in embedding
space than others. We find that countries whose names occur with low frequency
in training corpora are more likely to be tokenized into subwords, are less
semantically distinct in embedding space, and are less likely to be correctly
predicted: e.g., Ghana (the correct answer and in-vocabulary) is not predicted
for, "The country producing the most cocoa is [MASK].". Although these
performance discrepancies and representational harms are due to frequency, we
find that frequency is highly correlated with a country's GDP; thus
perpetuating historic power and wealth inequalities. We analyze the
effectiveness of mitigation strategies; recommend that researchers report
training word frequencies; and recommend future work for the community to
define and design representational guarantees.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extracting Latent Steering Vectors from Pretrained Language Models. (arXiv:2205.05124v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05124">
<div class="article-summary-box-inner">
<span><p>Prior work on controllable text generation has focused on learning how to
control language models through trainable decoding, smart-prompt design, or
fine-tuning based on a desired objective. We hypothesize that the information
needed to steer the model to generate a target sentence is already encoded
within the model. Accordingly, we explore a different approach altogether:
extracting latent vectors directly from pretrained language model decoders
without fine-tuning. Experiments show that there exist steering vectors, which,
when added to the hidden states of the language model, generate a target
sentence nearly perfectly (&gt; 99 BLEU) for English sentences from a variety of
domains. We show that vector arithmetic can be used for unsupervised sentiment
transfer on the Yelp sentiment benchmark, with performance comparable to models
tailored to this task. We find that distances between steering vectors reflect
sentence similarity when evaluated on a textual similarity benchmark (STS-B),
outperforming pooled hidden states of models. Finally, we present an analysis
of the intrinsic properties of the steering vectors. Taken together, our
results suggest that frozen LMs can be effectively controlled through their
latent steering space.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human Language Modeling. (arXiv:2205.05128v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05128">
<div class="article-summary-box-inner">
<span><p>Natural language is generated by people, yet traditional language modeling
views words or documents as if generated independently. Here, we propose human
language modeling (HuLM), a hierarchical extension to the language modeling
problem whereby a human-level exists to connect sequences of documents (e.g.
social media messages) and capture the notion that human language is moderated
by changing human states. We introduce, HaRT, a large-scale transformer model
for the HuLM task, pre-trained on approximately 100,000 social media users, and
demonstrate its effectiveness in terms of both language modeling (perplexity)
for social media and fine-tuning for 4 downstream tasks spanning document- and
user-levels: stance detection, sentiment classification, age estimation, and
personality assessment. Results on all tasks meet or surpass the current
state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unifying Language Learning Paradigms. (arXiv:2205.05131v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05131">
<div class="article-summary-box-inner">
<span><p>Existing pre-trained models are generally geared towards a particular class
of problems. To date, there seems to be still no consensus on what the right
architecture and pre-training setup should be. This paper presents a unified
framework for pre-training models that are universally effective across
datasets and setups. We begin by disentangling architectural archetypes with
pre-training objectives -- two concepts that are commonly conflated. Next, we
present a generalized and unified perspective for self-supervision in NLP and
show how different pre-training objectives can be cast as one another and how
interpolating between different objectives can be effective. We then propose
Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse
pre-training paradigms together. We furthermore introduce a notion of mode
switching, wherein downstream fine-tuning is associated with specific
pre-training schemes. We conduct extensive ablative experiments to compare
multiple pre-training objectives and find that our method pushes the
Pareto-frontier by outperforming T5 and/or GPT-like models across multiple
diverse setups. Finally, by scaling our model up to 20B parameters, we achieve
SOTA performance on 50 well-established supervised NLP tasks ranging from
language generation (with automated and human evaluation), language
understanding, text classification, question answering, commonsense reasoning,
long text reasoning, structured knowledge grounding and information retrieval.
Our model also achieve strong results at in-context learning, outperforming
175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on
one-shot summarization. We release Flax-based T5X model checkpoints for the 20B
model at
\url{https://github.com/google-research/google-research/tree/master/ul2}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sibylvariant Transformations for Robust Text Classification. (arXiv:2205.05137v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05137">
<div class="article-summary-box-inner">
<span><p>The vast majority of text transformation techniques in NLP are inherently
limited in their ability to expand input space coverage due to an implicit
constraint to preserve the original class label. In this work, we propose the
notion of sibylvariance (SIB) to describe the broader set of transforms that
relax the label-preserving constraint, knowably vary the expected class, and
lead to significantly more diverse input distributions. We offer a unified
framework to organize all data transformations, including two types of SIB: (1)
Transmutations convert one discrete kind into another, (2) Mixture Mutations
blend two or more classes together. To explore the role of sibylvariance within
NLP, we implemented 41 text transformations, including several novel techniques
like Concept2Sentence and SentMix. Sibylvariance also enables a unique form of
adaptive training that generates new input mixtures for the most confused class
pairs, challenging the learner to differentiate with greater nuance. Our
experiments on six benchmark datasets strongly support the efficacy of
sibylvariance for generalization performance, defect detection, and adversarial
robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reducing Activation Recomputation in Large Transformer Models. (arXiv:2205.05198v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05198">
<div class="article-summary-box-inner">
<span><p>Training large transformer models is one of the most important computational
challenges of modern AI. In this paper, we show how to significantly accelerate
training of large transformer models by reducing activation recomputation.
Activation recomputation is commonly used to work around memory capacity
constraints. Rather than storing activations for backpropagation, they are
traditionally recomputed, which saves memory but adds redundant compute. In
this work, we show most of this redundant compute is unnecessary because we can
reduce memory consumption sufficiently without it. We present two novel yet
very simple techniques: sequence parallelism and selective activation
recomputation. In conjunction with tensor parallelism, these techniques almost
eliminate the need to recompute activations. We evaluate our approach on
language models up to one trillion parameters in scale and show that our method
reduces activation memory by 5x, while reducing execution time overhead from
activation recomputation by over 90%. For example, when training a 530B
parameter GPT-3 style model on 2240 NVIDIA A100 GPUs, we achieve a Model Flops
Utilization of 54.2%, which is 29% faster than the 42.1% we achieve using
recomputation. Our implementation will be available in both Megatron-LM and
NeMo-Megatron.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Separator-Transducer-Segmenter: Streaming Recognition and Segmentation of Multi-party Speech. (arXiv:2205.05199v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05199">
<div class="article-summary-box-inner">
<span><p>Streaming recognition and segmentation of multi-party conversations with
overlapping speech is crucial for the next generation of voice assistant
applications. In this work we address its challenges discovered in the previous
work on multi-turn recurrent neural network transducer (MT-RNN-T) with a novel
approach, separator-transducer-segmenter (STS), that enables tighter
integration of speech separation, recognition and segmentation in a single
model. First, we propose a new segmentation modeling strategy through
start-of-turn and end-of-turn tokens that improves segmentation without
recognition accuracy degradation. Second, we further improve both speech
recognition and segmentation accuracy through an emission regularization
method, FastEmit, and multi-task training with speech activity information as
an additional training signal. Third, we experiment with end-of-turn emission
latency penalty to improve end-point detection for each speaker turn. Finally,
we establish a novel framework for segmentation analysis of multi-party
conversations through emission latency metrics. With our best model, we report
4.6% abs. turn counting accuracy improvement and 17% rel. word error rate (WER)
improvement on LibriCSS dataset compared to the previously published work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Improved Zero-shot Voice Conversion with Conditional DSVAE. (arXiv:2205.05227v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05227">
<div class="article-summary-box-inner">
<span><p>Disentangling content and speaking style information is essential for
zero-shot non-parallel voice conversion (VC). Our previous study investigated a
novel framework with disentangled sequential variational autoencoder (DSVAE) as
the backbone for information decomposition. We have demonstrated that
simultaneous disentangling content embedding and speaker embedding from one
utterance is feasible for zero-shot VC. In this study, we continue the
direction by raising one concern about the prior distribution of content branch
in the DSVAE baseline. We find the random initialized prior distribution will
force the content embedding to reduce the phonetic-structure information during
the learning process, which is not a desired property. Here, we seek to achieve
a better content embedding with more phonetic information preserved. We propose
conditional DSVAE, a new model that enables content bias as a condition to the
prior modeling and reshapes the content embedding sampled from the posterior
distribution. In our experiment on the VCTK dataset, we demonstrate that
content embeddings derived from the conditional DSVAE overcome the randomness
and achieve a much better phoneme classification accuracy, a stabilized
vocalization and a better zero-shot VC performance compared with the
competitive DSVAE baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relational Triple Extraction: One Step is Enough. (arXiv:2205.05270v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05270">
<div class="article-summary-box-inner">
<span><p>Extracting relational triples from unstructured text is an essential task in
natural language processing and knowledge graph construction. Existing
approaches usually contain two fundamental steps: (1) finding the boundary
positions of head and tail entities; (2) concatenating specific tokens to form
triples. However, nearly all previous methods suffer from the problem of error
accumulation, i.e., the boundary recognition error of each entity in step (1)
will be accumulated into the final combined triples. To solve the problem, in
this paper, we introduce a fresh perspective to revisit the triple extraction
task, and propose a simple but effective model, named DirectRel. Specifically,
the proposed model first generates candidate entities through enumerating token
sequences in a sentence, and then transforms the triple extraction task into a
linking problem on a "head $\rightarrow$ tail" bipartite graph. By doing so,
all triples can be directly extracted in only one step. Extensive experimental
results on two widely used datasets demonstrate that the proposed model
performs better than the state-of-the-art baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">User Guide for KOTE: Korean Online Comments Emotions Dataset. (arXiv:2205.05300v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05300">
<div class="article-summary-box-inner">
<span><p>Sentiment analysis that classifies data into positive or negative has been
dominantly used to recognize emotional aspects of texts, despite the deficit of
thorough examination of emotional meanings. Recently, corpora labeled with more
than just valence are built to exceed this limit. However, most Korean emotion
corpora are small in the number of instances and cover a limited range of
emotions. We introduce KOTE dataset. KOTE contains 50k (250k cases) Korean
online comments, each of which is manually labeled for 43 emotion labels or one
special label (NO EMOTION) by crowdsourcing (Ps = 3,048). The emotion taxonomy
of the 43 emotions is systematically established by cluster analysis of Korean
emotion concepts expressed on word embedding space. After explaining how KOTE
is developed, we also discuss the results of finetuning and analysis for social
discrimination in the corpus.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Unified Prompt Tuning for Few-shot Text Classification. (arXiv:2205.05313v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05313">
<div class="article-summary-box-inner">
<span><p>Prompt-based fine-tuning has boosted the performance of Pre-trained Language
Models (PLMs) on few-shot text classification by employing task-specific
prompts. Yet, PLMs are unfamiliar with prompt-style expressions during
pre-training, which limits the few-shot learning performance on downstream
tasks. It would be desirable if the models can acquire some prompting knowledge
before adaptation to specific NLP tasks. We present the Unified Prompt Tuning
(UPT) framework, leading to better few-shot text classification for BERT-style
models by explicitly capturing prompting semantics from non-target NLP
datasets. In UPT, a novel paradigm Prompt-Options-Verbalizer is proposed for
joint prompt learning across different NLP tasks, forcing PLMs to capture
task-invariant prompting knowledge. We further design a self-supervised task
named Knowledge-enhanced Selective Masked Language Modeling to improve the
PLM's generalization abilities for accurate adaptation to previously unseen
tasks. After multi-task learning across multiple tasks, the PLM can be better
prompt-tuned towards any dissimilar target tasks in low-resourced settings.
Experiments over a variety of NLP tasks show that UPT consistently outperforms
state-of-the-arts for prompt-based fine-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Efficient Summation Algorithm for the Accuracy, Convergence and Reproducibility of Parallel Numerical Methods. (arXiv:2205.05339v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05339">
<div class="article-summary-box-inner">
<span><p>Nowadays, parallel computing is ubiquitous in several application fields,
both in engineering and science. The computations rely on the floating-point
arithmetic specified by the IEEE754 Standard. In this context, an elementary
brick of computation, used everywhere, is the sum of a sequence of numbers.
This sum is subject to many numerical errors in floating-point arithmetic. To
alleviate this issue, we have introduced a new parallel algorithm for summing a
sequence of floating-point numbers. This algorithm which scales up easily with
the number of processors, adds numbers of the same exponents first. In this
article, our main contribution is an extensive analysis of its efficiency with
respect to several properties: accuracy, convergence and reproducibility. In
order to show the usefulness of our algorithm, we have chosen a set of
representative numerical methods which are Simpson, Jacobi, LU factorization
and the Iterated power method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pre-trained Language Models as Re-Annotators. (arXiv:2205.05368v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05368">
<div class="article-summary-box-inner">
<span><p>Annotation noise is widespread in datasets, but manually revising a flawed
corpus is time-consuming and error-prone. Hence, given the prior knowledge in
Pre-trained Language Models and the expected uniformity across all annotations,
we attempt to reduce annotation noise in the corpus through two tasks
automatically: (1) Annotation Inconsistency Detection that indicates the
credibility of annotations, and (2) Annotation Error Correction that rectifies
the abnormal annotations.
</p>
<p>We investigate how to acquire semantic sensitive annotation representations
from Pre-trained Language Models, expecting to embed the examples with
identical annotations to the mutually adjacent positions even without
fine-tuning. We proposed a novel credibility score to reveal the likelihood of
annotation inconsistencies based on the neighbouring consistency. Then, we
fine-tune the Pre-trained Language Models based classifier with
cross-validation for annotation correction. The annotation corrector is further
elaborated with two approaches: (1) soft labelling by Kernel Density Estimation
and (2) a novel distant-peer contrastive loss.
</p>
<p>We study the re-annotation in relation extraction and create a new manually
revised dataset, Re-DocRED, for evaluating document-level re-annotation. The
proposed credibility scores show promising agreement with human revisions,
achieving a Binary F1 of 93.4 and 72.5 in detecting inconsistencies on TACRED
and DocRED respectively. Moreover, the neighbour-aware classifiers based on
distant-peer contrastive learning and uncertain labels achieve Macro F1 up to
66.2 and 57.8 in correcting annotations on TACRED and DocRED respectively.
These improvements are not merely theoretical: Rather, automatically denoised
training sets demonstrate up to 3.6% performance improvement for
state-of-the-art relation extraction models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoKE: An automatic knowledge embedding framework for scientific machine learning. (arXiv:2205.05390v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05390">
<div class="article-summary-box-inner">
<span><p>Imposing physical constraints on neural networks as a method of knowledge
embedding has achieved great progress in solving physical problems described by
governing equations. However, for many engineering problems, governing
equations often have complex forms, including complex partial derivatives or
stochastic physical fields, which results in significant inconveniences from
the perspective of implementation. In this paper, a scientific machine learning
framework, called AutoKE, is proposed, and a reservoir flow problem is taken as
an instance to demonstrate that this framework can effectively automate the
process of embedding physical knowledge. In AutoKE, an emulator comprised of
deep neural networks (DNNs) is built for predicting the physical variables of
interest. An arbitrarily complex equation can be parsed and automatically
converted into a computational graph through the equation parser module, and
the fitness of the emulator to the governing equation is evaluated via
automatic differentiation. Furthermore, the fixed weights in the loss function
are substituted with adaptive weights by incorporating the Lagrangian dual
method. Neural architecture search (NAS) is also introduced into the AutoKE to
select an optimal network architecture of the emulator according to the
specific problem. Finally, we apply transfer learning to enhance the
scalability of the emulator. In experiments, the framework is verified by a
series of physical problems in which it can automatically embed physical
knowledge into an emulator without heavy hand-coding. The results demonstrate
that the emulator can not only make accurate predictions, but also be applied
to similar problems with high efficiency via transfer learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Query-Based Keyphrase Extraction from Long Documents. (arXiv:2205.05391v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05391">
<div class="article-summary-box-inner">
<span><p>Transformer-based architectures in natural language processing force input
size limits that can be problematic when long documents need to be processed.
This paper overcomes this issue for keyphrase extraction by chunking the long
documents while keeping a global context as a query defining the topic for
which relevant keyphrases should be extracted. The developed system employs a
pre-trained BERT model and adapts it to estimate the probability that a given
text span forms a keyphrase. We experimented using various context sizes on two
popular datasets, Inspec and SemEval, and a large novel dataset. The presented
results show that a shorter context with a query overcomes a longer one without
the query on long documents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ALIGNMEET: A Comprehensive Tool for Meeting Annotation, Alignment, and Evaluation. (arXiv:2205.05433v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05433">
<div class="article-summary-box-inner">
<span><p>Summarization is a challenging problem, and even more challenging is to
manually create, correct, and evaluate the summaries. The severity of the
problem grows when the inputs are multi-party dialogues in a meeting setup. To
facilitate the research in this area, we present ALIGNMEET, a comprehensive
tool for meeting annotation, alignment, and evaluation. The tool aims to
provide an efficient and clear interface for fast annotation while mitigating
the risk of introducing errors. Moreover, we add an evaluation mode that
enables a comprehensive quality evaluation of meeting minutes. To the best of
our knowledge, there is no such tool available. We release the tool as open
source. It is also directly installable from PyPI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Building for Tomorrow: Assessing the Temporal Persistence of Text Classifiers. (arXiv:2205.05435v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05435">
<div class="article-summary-box-inner">
<span><p>Performance of text classification models can drop over time when new data to
be classified is more distant in time from the data used for training, due to
naturally occurring changes in the data, such as vocabulary change. A solution
to this is to continually label new data to retrain the model, which is,
however, often unaffordable to be performed regularly due to its associated
cost. This raises important research questions on the design of text
classification models that are intended to persist over time: do all embedding
models and classification algorithms exhibit similar performance drops over
time and is the performance drop more prominent in some tasks or datasets than
others? With the aim of answering these research questions, we perform
longitudinal classification experiments on three datasets spanning between 6
and 19 years. Findings from these experiments inform the design of text
classification models with the aim of preserving performance over time,
discussing the extent to which one can rely on classification models trained
from temporally distant training data, as well as how the characteristics of
the dataset impact this.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Making Pre-trained Language Models Good Long-tailed Learners. (arXiv:2205.05461v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05461">
<div class="article-summary-box-inner">
<span><p>Prompt-tuning has shown appealing performance in few-shot classification by
virtue of its capability in effectively exploiting pre-trained knowledge. This
motivates us to check the hypothesis that prompt-tuning is also a promising
choice for long-tailed classification, since the tail classes are intuitively
few-shot ones. To achieve this aim, we conduct empirical studies to examine the
hypothesis. The results demonstrate that prompt-tuning exactly makes
pre-trained language models at least good long-tailed learners. For intuitions
on why prompt-tuning can achieve good performance in long-tailed
classification, we carry out an in-depth analysis by progressively bridging the
gap between prompt-tuning and commonly used fine-tuning. The summary is that
the classifier structure and parameterization form the key to making good
long-tailed learners, in comparison with the less important input structure.
Finally, we verify the applicability of our finding to few-shot classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Utilizing coarse-grained data in low-data settings for event extraction. (arXiv:2205.05468v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05468">
<div class="article-summary-box-inner">
<span><p>Annotating text data for event information extraction systems is hard,
expensive, and error-prone. We investigate the feasibility of integrating
coarse-grained data (document or sentence labels), which is far more feasible
to obtain, instead of annotating more documents. We utilize a multi-task model
with two auxiliary tasks, document and sentence binary classification, in
addition to the main task of token classification. We perform a series of
experiments with varying data regimes for the aforementioned integration.
Results show that while introducing extra coarse-grained data offers greater
improvement and robustness, a gain is still possible with only the addition of
negative documents that have no information on any event.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Clinical Prompt Learning with Frozen Language Models. (arXiv:2205.05535v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05535">
<div class="article-summary-box-inner">
<span><p>Prompt learning is a new paradigm in the Natural Language Processing (NLP)
field which has shown impressive performance on a number of natural language
tasks with common benchmarking text datasets in full, few-shot, and zero-shot
train-evaluation setups. Recently, it has even been observed that large but
frozen pre-trained language models (PLMs) with prompt learning outperform
smaller but fine-tuned models. However, as with many recent NLP trends, the
performance of even the largest PLMs such as GPT-3 do not perform well on
specialized domains (e.g. medical text), and the common practice to achieve
State of the Art (SoTA) results still consists of pre-training and fine-tuning
the PLMs on downstream tasks. The reliance on fine-tuning large PLMs is
problematic in clinical settings where data is often held in non-GPU
environments, and more resource efficient methods of training specialized
domain models is crucial. We investigated the viability of prompt learning on
clinically meaningful decision tasks and directly compared with more
traditional fine-tuning methods. Results are partially in line with the prompt
learning literature, with prompt learning able to match or improve on
traditional fine-tuning with substantially fewer trainable parameters and
requiring less training data. We argue that prompt learning therefore provides
lower computational resource costs applicable to clinical settings, that can
serve as an alternative to fine-tuning ever increasing in size PLMs.
Complementary code to reproduce experiments presented in this work can be found
at: https://github.com/NtaylorOX/Public_Clinical_Prompt.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KETOD: Knowledge-Enriched Task-Oriented Dialogue. (arXiv:2205.05589v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05589">
<div class="article-summary-box-inner">
<span><p>Existing studies in dialogue system research mostly treat task-oriented
dialogue and chit-chat as separate domains. Towards building a human-like
assistant that can converse naturally and seamlessly with users, it is
important to build a dialogue system that conducts both types of conversations
effectively. In this work, we investigate how task-oriented dialogue and
knowledge-grounded chit-chat can be effectively integrated into a single model.
To this end, we create a new dataset, KETOD (Knowledge-Enriched Task-Oriented
Dialogue), where we naturally enrich task-oriented dialogues with chit-chat
based on relevant entity knowledge. We also propose two new models,
SimpleToDPlus and Combiner, for the proposed task. Experimental results on both
automatic and human evaluations show that the proposed methods can
significantly improve the performance in knowledge-enriched response generation
while maintaining a competitive task-oriented dialog performance. We believe
our new dataset will be a valuable resource for future studies. Our dataset and
code are publicly available at \url{https://github.com/facebookresearch/ketod}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A neural prosody encoder for end-ro-end dialogue act classification. (arXiv:2205.05590v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05590">
<div class="article-summary-box-inner">
<span><p>Dialogue act classification (DAC) is a critical task for spoken language
understanding in dialogue systems. Prosodic features such as energy and pitch
have been shown to be useful for DAC. Despite their importance, little research
has explored neural approaches to integrate prosodic features into end-to-end
(E2E) DAC models which infer dialogue acts directly from audio signals. In this
work, we propose an E2E neural architecture that takes into account the need
for characterizing prosodic phenomena co-occurring at different levels inside
an utterance. A novel part of this architecture is a learnable gating mechanism
that assesses the importance of prosodic features and selectively retains core
information necessary for E2E DAC. Our proposed model improves DAC accuracy by
1.07% absolute across three publicly available benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying Moments of Change from Longitudinal User Text. (arXiv:2205.05593v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05593">
<div class="article-summary-box-inner">
<span><p>Identifying changes in individuals' behaviour and mood, as observed via
content shared on online platforms, is increasingly gaining importance. Most
research to-date on this topic focuses on either: (a) identifying individuals
at risk or with a certain mental health condition given a batch of posts or (b)
providing equivalent labels at the post level. A disadvantage of such work is
the lack of a strong temporal component and the inability to make longitudinal
assessments following an individual's trajectory and allowing timely
interventions. Here we define a new task, that of identifying moments of change
in individuals on the basis of their shared content online. The changes we
consider are sudden shifts in mood (switches) or gradual mood progression
(escalations). We have created detailed guidelines for capturing moments of
change and a corpus of 500 manually annotated user timelines (18.7K posts). We
have developed a variety of baseline models drawing inspiration from related
tasks and show that the best performance is obtained through context aware
sequential modelling. We also introduce new metrics for capturing rare events
in temporal windows.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning. (arXiv:2205.05638v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05638">
<div class="article-summary-box-inner">
<span><p>Few-shot in-context learning (ICL) enables pre-trained language models to
perform a previously-unseen task without any gradient-based training by feeding
a small number of training examples as part of the input. ICL incurs
substantial computational, memory, and storage costs because it involves
processing all of the training examples every time a prediction is made.
Parameter-efficient fine-tuning (e.g. adapter modules, prompt tuning, sparse
update methods, etc.) offers an alternative paradigm where a small set of
parameters are trained to enable a model to perform the new task. In this
paper, we rigorously compare few-shot ICL and parameter-efficient fine-tuning
and demonstrate that the latter offers better accuracy as well as dramatically
lower computational costs. Along the way, we introduce a new
parameter-efficient fine-tuning method called (IA)$^3$ that scales activations
by learned vectors, attaining stronger performance while only introducing a
relatively tiny amount of new parameters. We also propose a simple recipe based
on the T0 model called T-Few that can be applied to new tasks without
task-specific tuning or modifications. We validate the effectiveness of T-Few
on completely unseen tasks by applying it to the RAFT benchmark, attaining
super-human performance for the first time and outperforming the
state-of-the-art by 6% absolute. All of the code used in our experiments is
publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Aggregating Pairwise Semantic Differences for Few-Shot Claim Veracity Classification. (arXiv:2205.05646v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05646">
<div class="article-summary-box-inner">
<span><p>As part of an automated fact-checking pipeline, the claim veracity
classification task consists in determining if a claim is supported by an
associated piece of evidence. The complexity of gathering labelled
claim-evidence pairs leads to a scarcity of datasets, particularly when dealing
with new domains. In this paper, we introduce SEED, a novel vector-based method
to few-shot claim veracity classification that aggregates pairwise semantic
differences for claim-evidence pairs. We build on the hypothesis that we can
simulate class representative vectors that capture average semantic differences
for claim-evidence pairs in a class, which can then be used for classification
of new instances. We compare the performance of our method with competitive
baselines including fine-tuned BERT/RoBERTa models, as well as the
state-of-the-art few-shot veracity classification method that leverages
language model perplexity. Experiments conducted on the FEVER and SCIFACT
datasets show consistent improvements over competitive baselines in few-shot
settings. Our code is available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ontology-Based and Weakly Supervised Rare Disease Phenotyping from Clinical Notes. (arXiv:2205.05656v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05656">
<div class="article-summary-box-inner">
<span><p>Computational text phenotyping is the practice of identifying patients with
certain disorders and traits from clinical notes. Rare diseases are challenging
to be identified due to few cases available for machine learning and the need
for data annotation from domain experts. We propose a method using ontologies
and weak supervision, with recent pre-trained contextual representations from
Bi-directional Transformers (e.g. BERT). The ontology-based framework includes
two steps: (i) Text-to-UMLS, extracting phenotypes by contextually linking
mentions to concepts in Unified Medical Language System (UMLS), with a Named
Entity Recognition and Linking (NER+L) tool, SemEHR, and weak supervision with
customised rules and contextual mention representation; (ii) UMLS-to-ORDO,
matching UMLS concepts to rare diseases in Orphanet Rare Disease Ontology
(ORDO). The weakly supervised approach is proposed to learn a phenotype
confirmation model to improve Text-to-UMLS linking, without annotated data from
domain experts. We evaluated the approach on three clinical datasets of
discharge summaries and radiology reports from two institutions in the US and
the UK. Our best weakly supervised method achieved 81.4% precision and 91.4%
recall on extracting rare disease UMLS phenotypes from MIMIC-III discharge
summaries. The overall pipeline processing clinical notes can surface rare
disease cases, mostly uncaptured in structured data (manually assigned ICD
codes). Results on radiology reports from MIMIC-III and NHS Tayside were
consistent with the discharge summaries. We discuss the usefulness of the weak
supervision approach and propose directions for future studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying concept libraries from language about object structure. (arXiv:2205.05666v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05666">
<div class="article-summary-box-inner">
<span><p>Our understanding of the visual world goes beyond naming objects,
encompassing our ability to parse objects into meaningful parts, attributes,
and relations. In this work, we leverage natural language descriptions for a
diverse set of 2K procedurally generated objects to identify the parts people
use and the principles leading these parts to be favored over others. We
formalize our problem as search over a space of program libraries that contain
different part concepts, using tools from machine translation to evaluate how
well programs expressed in each library align to human language. By combining
naturalistic language at scale with structured program representations, we
discover a fundamental information-theoretic tradeoff governing the part
concepts people name: people favor a lexicon that allows concise descriptions
of each object, while also minimizing the size of the lexicon itself.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Augmenting Poetry Composition with Verse by Verse. (arXiv:2103.17205v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.17205">
<div class="article-summary-box-inner">
<span><p>We describe Verse by Verse, our experiment in augmenting the creative process
of writing poetry with an AI. We have created a group of AI poets, styled after
various American classic poets, that are able to offer as suggestions generated
lines of verse while a user is composing a poem. In this paper, we describe the
underlying system to offer these suggestions. This includes a generative model,
which is tasked with generating a large corpus of lines of verse offline and
which are then stored in an index, and a dual-encoder model that is tasked with
recommending the next possible set of verses from our index given the previous
line of verse.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">XLM-T: Multilingual Language Models in Twitter for Sentiment Analysis and Beyond. (arXiv:2104.12250v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12250">
<div class="article-summary-box-inner">
<span><p>Language models are ubiquitous in current NLP, and their multilingual
capacity has recently attracted considerable attention. However, current
analyses have almost exclusively focused on (multilingual variants of) standard
benchmarks, and have relied on clean pre-training and task-specific corpora as
multilingual signals. In this paper, we introduce XLM-T, a model to train and
evaluate multilingual language models in Twitter. In this paper we provide: (1)
a new strong multilingual baseline consisting of an XLM-R (Conneau et al. 2020)
model pre-trained on millions of tweets in over thirty languages, alongside
starter code to subsequently fine-tune on a target task; and (2) a set of
unified sentiment analysis Twitter datasets in eight different languages and a
XLM-T model fine-tuned on them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Re-evaluating Word Mover's Distance. (arXiv:2105.14403v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14403">
<div class="article-summary-box-inner">
<span><p>The word mover's distance (WMD) is a fundamental technique for measuring the
similarity of two documents. As the crux of WMD, it can take advantage of the
underlying geometry of the word space by employing an optimal transport
formulation. The original study on WMD reported that WMD outperforms classical
baselines such as bag-of-words (BOW) and TF-IDF by significant margins in
various datasets. In this paper, we point out that the evaluation in the
original study could be misleading. We re-evaluate the performances of WMD and
the classical baselines and find that the classical baselines are competitive
with WMD if we employ an appropriate preprocessing, i.e., L1 normalization. In
addition, We introduce an analogy between WMD and L1-normalized BOW and find
that not only the performance of WMD but also the distance values resemble
those of BOW in high dimensional spaces.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EENLP: Cross-lingual Eastern European NLP Index. (arXiv:2108.02605v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02605">
<div class="article-summary-box-inner">
<span><p>Motivated by the sparsity of NLP resources for Eastern European languages, we
present a broad index of existing Eastern European language resources (90+
datasets and 45+ models) published as a github repository open for updates from
the community. Furthermore, to support the evaluation of commonsense reasoning
tasks, we provide hand-crafted cross-lingual datasets for five different
semantic tasks (namely news categorization, paraphrase detection, Natural
Language Inference (NLI) task, tweet sentiment detection, and news sentiment
detection) for some of the Eastern European languages. We perform several
experiments with the existing multilingual models on these datasets to define
the performance baselines and compare them to the existing results for other
languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topic Model Supervised by Understanding Map. (arXiv:2110.06043v11 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06043">
<div class="article-summary-box-inner">
<span><p>Inspired by the notion of Center of Mass in physics, an extension called
Semantic Center of Mass (SCOM) is proposed, and used to discover the abstract
"topic" of a document. The notion is under a framework model called
Understanding Map Supervised Topic Model (UM-S-TM). The devising aim of UM-S-TM
is to let both the document content and a semantic network -- specifically,
Understanding Map -- play a role, in interpreting the meaning of a document.
Based on different justifications, three possible methods are devised to
discover the SCOM of a document. Some experiments on artificial documents and
Understanding Maps are conducted to test their outcomes. In addition, its
ability of vectorization of documents and capturing sequential information are
tested. We also compared UM-S-TM with probabilistic topic models like Latent
Dirichlet Allocation (LDA) and probabilistic Latent Semantic Analysis (pLSA).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Linguistic Frameworks Go Toe-to-Toe at Neuro-Symbolic Language Modeling. (arXiv:2112.07874v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07874">
<div class="article-summary-box-inner">
<span><p>We examine the extent to which, in principle, linguistic graph
representations can complement and improve neural language modeling. With an
ensemble setup consisting of a pretrained Transformer and ground-truth graphs
from one of 7 different formalisms, we find that, overall, semantic
constituency structures are most useful to language modeling performance --
outpacing syntactic constituency structures as well as syntactic and semantic
dependency structures. Further, effects vary greatly depending on
part-of-speech class. In sum, our findings point to promising tendencies in
neuro-symbolic language modeling and invite future research quantifying the
design choices made by different formalisms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QuALITY: Question Answering with Long Input Texts, Yes!. (arXiv:2112.08608v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08608">
<div class="article-summary-box-inner">
<span><p>To enable building and testing models on long-document comprehension, we
introduce QuALITY, a multiple-choice QA dataset with context passages in
English that have an average length of about 5,000 tokens, much longer than
typical current models can process. Unlike in prior work with passages, our
questions are written and validated by contributors who have read the entire
passage, rather than relying on summaries or excerpts. In addition, only half
of the questions are answerable by annotators working under tight time
constraints, indicating that skimming and simple search are not enough to
consistently perform well. Our baseline models perform poorly on this task
(55.4%) and significantly lag behind human performance (93.5%).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Models in the Loop: Aiding Crowdworkers with Generative Annotation Assistants. (arXiv:2112.09062v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09062">
<div class="article-summary-box-inner">
<span><p>In Dynamic Adversarial Data Collection (DADC), human annotators are tasked
with finding examples that models struggle to predict correctly. Models trained
on DADC-collected training data have been shown to be more robust in
adversarial and out-of-domain settings, and are considerably harder for humans
to fool. However, DADC is more time-consuming than traditional data collection
and thus more costly per annotated example. In this work, we examine whether we
can maintain the advantages of DADC, without incurring the additional cost. To
that end, we introduce Generative Annotation Assistants (GAAs),
generator-in-the-loop models that provide real-time suggestions that annotators
can either approve, modify, or reject entirely. We collect training datasets in
twenty experimental settings and perform a detailed analysis of this approach
for the task of extractive question answering (QA) for both standard and
adversarial data collection. We demonstrate that GAAs provide significant
efficiency benefits with over a 30% annotation speed-up, while leading to over
a 5x improvement in model fooling rates. In addition, we find that using
GAA-assisted training data leads to higher downstream model performance on a
variety of question answering tasks over adversarial data collection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multimodal German Dataset for Automatic Lip Reading Systems and Transfer Learning. (arXiv:2202.13403v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13403">
<div class="article-summary-box-inner">
<span><p>Large datasets as required for deep learning of lip reading do not exist in
many languages. In this paper we present the dataset GLips (German Lips)
consisting of 250,000 publicly available videos of the faces of speakers of the
Hessian Parliament, which was processed for word-level lip reading using an
automatic pipeline. The format is similar to that of the English language LRW
(Lip Reading in the Wild) dataset, with each video encoding one word of
interest in a context of 1.16 seconds duration, which yields compatibility for
studying transfer learning between both datasets. By training a deep neural
network, we investigate whether lip reading has language-independent features,
so that datasets of different languages can be used to improve lip reading
models. We demonstrate learning from scratch and show that transfer learning
from LRW to GLips and vice versa improves learning speed and performance, in
particular for the validation set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Linking Theories and Methods in Cognitive Sciences via Joint Embedding of the Scientific Literature: The Example of Cognitive Control. (arXiv:2203.11016v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11016">
<div class="article-summary-box-inner">
<span><p>Traditionally, theory and practice of Cognitive Control are linked via
literature reviews by human domain experts. This approach, however, is
inadequate to track the ever-growing literature. It may also be biased, and
yield redundancies and confusion.
</p>
<p>Here we present an alternative approach. We performed automated text analyses
on a large body of scientific texts to create a joint representation of tasks
and constructs. More specifically, 385,705 scientific abstracts were first
mapped into an embedding space using a transformers-based language model.
Document embeddings were then used to identify a task-construct graph embedding
that grounds constructs on tasks and supports nuanced meaning of the constructs
by taking advantage of constrained random walks in the graph.
</p>
<p>This joint task-construct graph embedding, can be queried to generate task
batteries targeting specific constructs, may reveal knowledge gaps in the
literature, and inspire new tasks and novel hypotheses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CUNI-KIT System for Simultaneous Speech Translation Task at IWSLT 2022. (arXiv:2204.06028v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06028">
<div class="article-summary-box-inner">
<span><p>In this paper, we describe our submission to the Simultaneous Speech
Translation at IWSLT 2022. We explore strategies to utilize an offline model in
a simultaneous setting without the need to modify the original model. In our
experiments, we show that our onlinization algorithm is almost on par with the
offline setting while being $3\times$ faster than offline in terms of latency
on the test set. We also show that the onlinized offline model outperforms the
best IWSLT2021 simultaneous system in medium and high latency regimes and is
almost on par in the low latency regime. We make our system publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fairly Accurate: Learning Optimal Accuracy vs. Fairness Tradeoffs for Hate Speech Detection. (arXiv:2204.07661v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07661">
<div class="article-summary-box-inner">
<span><p>Recent work has emphasized the importance of balancing competing objectives
in model training (e.g., accuracy vs. fairness, or competing measures of
fairness). Such trade-offs reflect a broader class of multi-objective
optimization (MOO) problems in which optimization methods seek Pareto optimal
trade-offs between competing goals. In this work, we first introduce a
differentiable measure that enables direct optimization of group fairness
(specifically, balancing accuracy across groups) in model training. Next, we
demonstrate two model-agnostic MOO frameworks for learning Pareto optimal
parameterizations over different groups of neural classification models. We
evaluate our methods on the specific task of hate speech detection, in which
prior work has shown lack of group fairness across speakers of different
English dialects. Empirical results across convolutional, sequential, and
transformer-based neural architectures show superior empirical accuracy vs.
fairness trade-offs over prior work. More significantly, our measure enables
the Pareto machinery to ensure that each architecture achieves the best
possible trade-off between fairness and accuracy w.r.t. the dataset, given
user-prescribed error tolerance bounds.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dialogue Meaning Representation for Task-Oriented Dialogue Systems. (arXiv:2204.10989v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10989">
<div class="article-summary-box-inner">
<span><p>Dialogue meaning representation formulates natural language utterance
semantics in their conversational context in an explicit and machine-readable
form. Previous work typically follows the intent-slot framework, which is easy
for annotation yet limited on scalability for complex linguistic expressions. A
line of works alleviates the representation issue by introducing hierarchical
structures but challenging to express complex compositional semantics, such as
negation and coreference. We propose Dialogue Meaning Representation (DMR), a
flexible and easily extendable representation for task-oriented dialogue. Our
representation contains a set of nodes and edges with inheritance hierarchy to
represent rich semantics for compositional semantics and task-specific
concepts. We annotated DMR-FastFood, a multi-turn dialogue dataset with more
than 70k utterances, with DMR. We propose two evaluation tasks to evaluate
different dialogue models, and further propose a novel coreference resolution
model GNNCoref for the graph-based coreference resolution task. Experiments
show that DMR can be parsed well with pretrained Seq2Seq model, and GNNCoref
outperforms the baseline models by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Original or Translated? A Causal Analysis of the Impact of Translationese on Machine Translation Performance. (arXiv:2205.02293v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02293">
<div class="article-summary-box-inner">
<span><p>Human-translated text displays distinct features from naturally written text
in the same language. This phenomena, known as translationese, has been argued
to confound the machine translation (MT) evaluation. Yet, we find that existing
work on translationese neglects some important factors and the conclusions are
mostly correlational but not causal. In this work, we collect CausalMT, a
dataset where the MT training data are also labeled with the human translation
directions. We inspect two critical factors, the train-test direction match
(whether the human translation directions in the training and test sets are
aligned), and data-model direction match (whether the model learns in the same
direction as the human translation direction in the dataset). We show that
these two factors have a large causal effect on the MT performance, in addition
to the test-model direction mismatch highlighted by existing work on the impact
of translationese. In light of our findings, we provide a set of suggestions
for MT training and evaluation. Our code and data are at
https://github.com/EdisonNi-hku/CausalMT
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conversational Analysis of Daily Dialog Data using Polite Emotional Dialogue Acts. (arXiv:2205.02921v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02921">
<div class="article-summary-box-inner">
<span><p>Many socio-linguistic cues are used in conversational analysis, such as
emotion, sentiment, and dialogue acts. One of the fundamental cues is
politeness, which linguistically possesses properties such as social manners
useful in conversational analysis. This article presents findings of polite
emotional dialogue act associations, where we can correlate the relationships
between the socio-linguistic cues. We confirm our hypothesis that the
utterances with the emotion classes Anger and Disgust are more likely to be
impolite. At the same time, Happiness and Sadness are more likely to be polite.
A less expectable phenomenon occurs with dialogue acts Inform and Commissive
which contain more polite utterances than Question and Directive. Finally, we
conclude on the future work of these findings to extend the learning of social
behaviours using politeness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards a Progression-Aware Autonomous Dialogue Agent. (arXiv:2205.03692v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03692">
<div class="article-summary-box-inner">
<span><p>Recent advances in large-scale language modeling and generation have enabled
the creation of dialogue agents that exhibit human-like responses in a wide
range of conversational scenarios spanning a diverse set of tasks, from general
chit-chat to focused goal-oriented discourse. While these agents excel at
generating high-quality responses that are relevant to prior context, they
suffer from a lack of awareness of the overall direction in which the
conversation is headed, and the likelihood of task success inherent therein.
Thus, we propose a framework in which dialogue agents can evaluate the
progression of a conversation toward or away from desired outcomes, and use
this signal to inform planning for subsequent responses. Our framework is
composed of three key elements: (1) the notion of a "global" dialogue state
(GDS) space, (2) a task-specific progression function (PF) computed in terms of
a conversation's trajectory through this space, and (3) a planning mechanism
based on dialogue rollouts by which an agent may use progression signals to
select its next response.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context-Aware Abbreviation Expansion Using Large Language Models. (arXiv:2205.03767v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03767">
<div class="article-summary-box-inner">
<span><p>Motivated by the need for accelerating text entry in augmentative and
alternative communication (AAC) for people with severe motor impairments, we
propose a paradigm in which phrases are abbreviated aggressively as primarily
word-initial letters. Our approach is to expand the abbreviations into
full-phrase options by leveraging conversation context with the power of
pretrained large language models (LLMs). Through zero-shot, few-shot, and
fine-tuning experiments on four public conversation datasets, we show that for
replies to the initial turn of a dialog, an LLM with 64B parameters is able to
exactly expand over 70% of phrases with abbreviation length up to 10, leading
to an effective keystroke saving rate of up to about 77% on these exact
expansions. Including a small amount of context in the form of a single
conversation turn more than doubles abbreviation expansion accuracies compared
to having no context, an effect that is more pronounced for longer phrases.
Additionally, the robustness of models against typo noise can be enhanced
through fine-tuning on noisy data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Answer Visual Questions from Web Videos. (arXiv:2205.05019v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05019">
<div class="article-summary-box-inner">
<span><p>Recent methods for visual question answering rely on large-scale annotated
datasets. Manual annotation of questions and answers for videos, however, is
tedious, expensive and prevents scalability. In this work, we propose to avoid
manual annotation and generate a large-scale training dataset for video
question answering making use of automatic cross-modal supervision. We leverage
a question generation transformer trained on text data and use it to generate
question-answer pairs from transcribed video narrations. Given narrated videos,
we then automatically generate the HowToVQA69M dataset with 69M
video-question-answer triplets. To handle the open vocabulary of diverse
answers in this dataset, we propose a training procedure based on a contrastive
loss between a video-question multi-modal transformer and an answer
transformer. We introduce the zero-shot VideoQA task and the VideoQA feature
probe evaluation setting and show excellent results, in particular for rare
answers. Furthermore, our method achieves competitive results on MSRVTT-QA,
ActivityNet-QA, MSVD-QA and How2QA datasets. We also show that our VideoQA
dataset generation approach generalizes to another source of web video and text
data. We use our method to generate the WebVidVQA3M dataset from the WebVid
dataset, i.e., videos with alt-text annotations, and show its benefits for
training VideoQA models. Finally, for a detailed evaluation we introduce iVQA,
a new VideoQA dataset with reduced language bias and high-quality manual
annotations. Code, datasets and trained models are available at
https://antoyang.github.io/just-ask.html
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">An Efficient Calculation of Quaternion Correlation of Signals and Color Images. (arXiv:2205.05113v1 [math.AC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05113">
<div class="article-summary-box-inner">
<span><p>Over the past century, a correlation has been an essential mathematical
technique utilized in engineering sciences, including practically every
signal/image processing field. This paper describes an effective method of
calculating the correlation function of signals and color images in quaternion
algebra. We propose using the quaternions with a commutative multiplication
operation and defining the corresponding correlation function in this
arithmetic. The correlation between quaternion signals and images can be
calculated by multiplying two quaternion DFTs of signals and images. The
complexity of the correlation of color images is three times higher than in
complex algebra.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep fusion of gray level co-occurrence matrices for lung nodule classification. (arXiv:2205.05123v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05123">
<div class="article-summary-box-inner">
<span><p>Lung cancer is a severe menace to human health, due to which millions of
people die because of late diagnoses of cancer; thus, it is vital to detect the
disease as early as possible. The Computerized chest analysis Tomography of
scan is assumed to be one of the efficient solutions for detecting and
classifying lung nodules. The necessity of high accuracy of analyzing C.T. scan
images of the lung is considered as one of the crucial challenges in detecting
and classifying lung cancer. A new long-short-term-memory (LSTM) based deep
fusion structure, is introduced, where, the texture features computed from lung
nodules through new volumetric grey-level-co-occurrence-matrices (GLCM)
computations are applied to classify the nodules into: benign, malignant and
ambiguous. An improved Otsu segmentation method combined with the water strider
optimization algorithm (WSA) is proposed to detect the lung nodules. Otsu-WSA
thresholding can overcome the restrictions present in previous thresholding
methods. Extended experiments are run to assess this fusion structure by
considering 2D-GLCM computations based 2D-slices fusion, and an approximation
of this 3D-GLCM with volumetric 2.5D-GLCM computations-based LSTM fusion
structure. The proposed methods are trained and assessed through the LIDC-IDRI
dataset, where 94.4%, 91.6%, and 95.8% Accuracy, sensitivity, and specificity
are obtained, respectively for 2D-GLCM fusion and 97.33%, 96%, and 98%,
accuracy, sensitivity, and specificity, respectively, for 2.5D-GLCM fusion. The
yield of the same are 98.7%, 98%, and 99%, for the 3D-GLCM fusion. The obtained
results and analysis indicate that the WSA-Otsu method requires less execution
time and yields a more accurate thresholding process. It is found that 3D-GLCM
based LSTM outperforms its counterparts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-Shot Image Classification Benchmarks are Too Far From Reality: Build Back Better with Semantic Task Sampling. (arXiv:2205.05155v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05155">
<div class="article-summary-box-inner">
<span><p>Every day, a new method is published to tackle Few-Shot Image Classification,
showing better and better performances on academic benchmarks. Nevertheless, we
observe that these current benchmarks do not accurately represent the real
industrial use cases that we encountered. In this work, through both
qualitative and quantitative studies, we expose that the widely used benchmark
tieredImageNet is strongly biased towards tasks composed of very semantically
dissimilar classes e.g. bathtub, cabbage, pizza, schipperke, and cardoon. This
makes tieredImageNet (and similar benchmarks) irrelevant to evaluate the
ability of a model to solve real-life use cases usually involving more
fine-grained classification. We mitigate this bias using semantic information
about the classes of tieredImageNet and generate an improved, balanced
benchmark. Going further, we also introduce a new benchmark for Few-Shot Image
Classification using the Danish Fungi 2020 dataset. This benchmark proposes a
wide variety of evaluation tasks with various fine-graininess. Moreover, this
benchmark includes many-way tasks (e.g. composed of 100 classes), which is a
challenging setting yet very common in industrial applications. Our experiments
bring out the correlation between the difficulty of a task and the semantic
similarity between its classes, as well as a heavy performance drop of
state-of-the-art methods on many-way few-shot classification, raising questions
about the scaling abilities of these methods. We hope that our work will
encourage the community to further question the quality of standard evaluation
processes and their relevance to real-life applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robustness of Humans and Machines on Object Recognition with Extreme Image Transformations. (arXiv:2205.05167v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05167">
<div class="article-summary-box-inner">
<span><p>Recent neural network architectures have claimed to explain data from the
human visual cortex. Their demonstrated performance is however still limited by
the dependence on exploiting low-level features for solving visual tasks. This
strategy limits their performance in case of out-of-distribution/adversarial
data. Humans, meanwhile learn abstract concepts and are mostly unaffected by
even extreme image distortions. Humans and networks employ strikingly different
strategies to solve visual tasks. To probe this, we introduce a novel set of
image transforms and evaluate humans and networks on an object recognition
task. We found performance for a few common networks quickly decreases while
humans are able to recognize objects with a high accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Scale Space Radon Transform, Properties and Image Reconstruction. (arXiv:2205.05188v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05188">
<div class="article-summary-box-inner">
<span><p>Aware of the importance of the good behavior in the scale space that a
mathematical transform must have, we depict, in this paper, the basic
properties and the inverse transform of the Scale Space Radon Transform (SSRT).
To reconstruct the image from SSRT sinogram, the Filtered backprojection (FBP)
technique is used in two different ways: (1) Deconvolve SSRT to obtain the
estimated Radon transform (RT) and then, reconstruct image using classical FBP
or (2) Adapt FBP technique to SSRT so that the Radon projections spectrum used
in classical FBP is replaced by SSRT and Wiener filtering, expressed in the
frequency domain. Comparison of image reconstruction techniques using SSRT and
RT are performed on Shepp-Logan head phantom image. Using the Mean Absolute
Error (MAE) as image reconstruction quality measure, the preliminary results
present an outstanding performance for SSRT-based image reconstruction
techniques compared to the RT-based one. Furthermore, the method (2)
outperforms the method (1) in terms of computation time and adaptability for
high level of noise when fairly large Gaussian kernel is used.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Student Collaboration Improves Self-Supervised Learning: Dual-Loss Adaptive Masked Autoencoder for Brain Cell Image Analysis. (arXiv:2205.05194v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05194">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning leverages the underlying data structure as the
source of the supervisory signal without the need for human annotation effort.
This approach offers a practical solution to learning with a large amount of
biomedical data and limited annotation. Unlike other studies exploiting data
via multi-view (e.g., augmented images), this study presents a self-supervised
Dual-Loss Adaptive Masked Autoencoder (DAMA) algorithm established from the
viewpoint of the information theory. Specifically, our objective function
maximizes the mutual information by minimizing the conditional entropy in
pixel-level reconstruction and feature-level regression. We further introduce
an adaptive mask sampling strategy to maximize mutual information. We conduct
extensive experiments on brain cell images to validate the proposed method.
DAMA significantly outperforms both state-of-the-art self-supervised and
supervised methods on brain cells data and demonstrates competitive result on
ImageNet-1k. Code: https://github.com/hula-ai/DAMA
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Best of Both Worlds: Multi-task Audio-Visual Automatic Speech Recognition and Active Speaker Detection. (arXiv:2205.05206v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05206">
<div class="article-summary-box-inner">
<span><p>Under noisy conditions, automatic speech recognition (ASR) can greatly
benefit from the addition of visual signals coming from a video of the
speaker's face. However, when multiple candidate speakers are visible this
traditionally requires solving a separate problem, namely active speaker
detection (ASD), which entails selecting at each moment in time which of the
visible faces corresponds to the audio. Recent work has shown that we can solve
both problems simultaneously by employing an attention mechanism over the
competing video tracks of the speakers' faces, at the cost of sacrificing some
accuracy on active speaker detection. This work closes this gap in active
speaker detection accuracy by presenting a single model that can be jointly
trained with a multi-task loss. By combining the two tasks during training we
reduce the ASD classification accuracy by approximately 25%, while
simultaneously improving the ASR performance when compared to the multi-person
baseline trained exclusively for ASR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DcnnGrasp: Towards Accurate Grasp Pattern Recognition with Adaptive Regularizer Learning. (arXiv:2205.05218v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05218">
<div class="article-summary-box-inner">
<span><p>The task of grasp pattern recognition aims to derive the applicable grasp
types of an object according to the visual information. Current
state-of-the-art methods ignore category information of objects which is
crucial for grasp pattern recognition. This paper presents a novel dual-branch
convolutional neural network (DcnnGrasp) to achieve joint learning of object
category classification and grasp pattern recognition. DcnnGrasp takes object
category classification as an auxiliary task to improve the effectiveness of
grasp pattern recognition. Meanwhile, a new loss function called joint
cross-entropy with an adaptive regularizer is derived through maximizing a
posterior, which significantly improves the model performance. Besides, based
on the new loss function, a training strategy is proposed to maximize the
collaborative learning of the two tasks. The experiment was performed on five
household objects datasets including the RGB-D Object dataset, Hit-GPRec
dataset, Amsterdam library of object images (ALOI), Columbia University Image
Library (COIL-100), and MeganePro dataset 1. The experimental results
demonstrated that the proposed method can achieve competitive performance on
grasp pattern recognition with several state-of-the-art methods. Specifically,
our method even outperformed the second-best one by nearly 15% in terms of
global accuracy for the case of testing a novel object on the RGB-D Object
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Salient Object Detection via Bounding-box Supervision. (arXiv:2205.05245v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05245">
<div class="article-summary-box-inner">
<span><p>The success of fully supervised saliency detection models depends on a large
number of pixel-wise labeling. In this paper, we work on bounding-box based
weakly-supervised saliency detection to relieve the labeling effort. Given the
bounding box annotation, we observe that pixels inside the bounding box may
contain extensive labeling noise. However, as a large amount of background is
excluded, the foreground bounding box region contains a less complex
background, making it possible to perform handcrafted features-based saliency
detection with only the cropped foreground region. As the conventional
handcrafted features are not representative enough, leading to noisy saliency
maps, we further introduce structure-aware self-supervised loss to regularize
the structure of the prediction. Further, we claim that pixels outside the
bounding box should be background, thus partial cross-entropy loss function can
be used to accurately localize the accurate background region. Experimental
results on six benchmark RGB saliency datasets illustrate the effectiveness of
our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Secure Federated Learning for Neuroimaging. (arXiv:2205.05249v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05249">
<div class="article-summary-box-inner">
<span><p>The amount of biomedical data continues to grow rapidly. However, the ability
to collect data from multiple sites for joint analysis remains challenging due
to security, privacy, and regulatory concerns. We present a Secure Federated
Learning architecture, MetisFL, which enables distributed training of neural
networks over multiple data sources without sharing data. Each site trains the
neural network over its private data for some time, then shares the neural
network parameters (i.e., weights, gradients) with a Federation Controller,
which in turn aggregates the local models, sends the resulting community model
back to each site, and the process repeats. Our architecture provides strong
security and privacy. First, sample data never leaves a site. Second, neural
parameters are encrypted before transmission and the community model is
computed under fully-homomorphic encryption. Finally, we use
information-theoretic methods to limit information leakage from the neural
model to prevent a curious site from performing membership attacks. We
demonstrate this architecture in neuroimaging. Specifically, we investigate
training neural models to classify Alzheimer's disease, and estimate Brain Age,
from magnetic resonance imaging datasets distributed across multiple sites,
including heterogeneous environments where sites have different amounts of
data, statistical distributions, and computational capabilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatial-Temporal Space Hand-in-Hand: Spatial-Temporal Video Super-Resolution via Cycle-Projected Mutual Learning. (arXiv:2205.05264v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05264">
<div class="article-summary-box-inner">
<span><p>Spatial-Temporal Video Super-Resolution (ST-VSR) aims to generate
super-resolved videos with higher resolution(HR) and higher frame rate (HFR).
Quite intuitively, pioneering two-stage based methods complete ST-VSR by
directly combining two sub-tasks: Spatial Video Super-Resolution (S-VSR) and
Temporal Video Super-Resolution(T-VSR) but ignore the reciprocal relations
among them. Specifically, 1) T-VSR to S-VSR: temporal correlations help
accurate spatial detail representation with more clues; 2) S-VSR to T-VSR:
abundant spatial information contributes to the refinement of temporal
prediction. To this end, we propose a one-stage based Cycle-projected Mutual
learning network (CycMu-Net) for ST-VSR, which makes full use of
spatial-temporal correlations via the mutual learning between S-VSR and T-VSR.
Specifically, we propose to exploit the mutual information among them via
iterative up-and-down projections, where the spatial and temporal features are
fully fused and distilled, helping the high-quality video reconstruction.
Besides extensive experiments on benchmark datasets, we also compare our
proposed CycMu-Net with S-VSR and T-VSR tasks, demonstrating that our method
significantly outperforms state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AggPose: Deep Aggregation Vision Transformer for Infant Pose Estimation. (arXiv:2205.05277v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05277">
<div class="article-summary-box-inner">
<span><p>Movement and pose assessment of newborns lets experienced pediatricians
predict neurodevelopmental disorders, allowing early intervention for related
diseases. However, most of the newest AI approaches for human pose estimation
methods focus on adults, lacking publicly benchmark for infant pose estimation.
In this paper, we fill this gap by proposing infant pose dataset and Deep
Aggregation Vision Transformer for human pose estimation, which introduces a
fast trained full transformer framework without using convolution operations to
extract features in the early stages. It generalizes Transformer + MLP to
high-resolution deep layer aggregation within feature maps, thus enabling
information fusion between different vision levels. We pre-train AggPose on
COCO pose dataset and apply it on our newly released large-scale infant pose
estimation dataset. The results show that AggPose could effectively learn the
multi-scale features among different resolutions and significantly improve the
performance of infant pose estimation. We show that AggPose outperforms hybrid
model HRFormer and TokenPose in the infant pose estimation dataset. Moreover,
our AggPose outperforms HRFormer by 0.7% AP on COCO val pose estimation on
average. Our code is available at github.com/SZAR-LAB/AggPose.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReFine: Re-randomization before Fine-tuning for Cross-domain Few-shot Learning. (arXiv:2205.05282v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05282">
<div class="article-summary-box-inner">
<span><p>Cross-domain few-shot learning (CD-FSL), where there are few target samples
under extreme differences between source and target domains, has recently
attracted huge attention. For CD-FSL, recent studies generally have developed
transfer learning based approaches that pre-train a neural network on popular
labeled source domain datasets and then transfer it to target domain data.
Although the labeled datasets may provide suitable initial parameters for the
target data, the domain difference between the source and target might hinder
the fine-tuning on the target domain. This paper proposes a simple yet powerful
method that re-randomizes the parameters fitted on the source domain before
adapting to the target data. The re-randomization resets source-specific
parameters of the source pre-trained model and thus facilitates fine-tuning on
the target domain, improving few-shot performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Invisible-to-Visible: Privacy-Aware Human Segmentation using Airborne Ultrasound via Collaborative Learning Probabilistic U-Net. (arXiv:2205.05293v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05293">
<div class="article-summary-box-inner">
<span><p>Color images are easy to understand visually and can acquire a great deal of
information, such as color and texture. They are highly and widely used in
tasks such as segmentation. On the other hand, in indoor person segmentation,
it is necessary to collect person data considering privacy. We propose a new
task for human segmentation from invisible information, especially airborne
ultrasound. We first convert ultrasound waves to reflected ultrasound
directional images (ultrasound images) to perform segmentation from invisible
information. Although ultrasound images can roughly identify a person's
location, the detailed shape is ambiguous. To address this problem, we propose
a collaborative learning probabilistic U-Net that uses ultrasound and
segmentation images simultaneously during training, closing the probabilistic
distributions between ultrasound and segmentation images by comparing the
parameters of the latent spaces. In inference, only ultrasound images can be
used to obtain segmentation results. As a result of performance verification,
the proposed method could estimate human segmentations more accurately than
conventional probabilistic U-Net and other variational autoencoder models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Arbitrary Shape Text Detection via Boundary Transformer. (arXiv:2205.05320v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05320">
<div class="article-summary-box-inner">
<span><p>Arbitrary shape text detection is a challenging task due to its complexity
and variety, e.g, various scales, random rotations, and curve shapes. In this
paper, we propose an arbitrary shape text detector with a boundary transformer,
which can accurately and directly locate text boundaries without any
post-processing. Our method mainly consists of a boundary proposal module and
an iteratively optimized boundary transformer module. The boundary proposal
module consisting of multi-layer dilated convolutions will compute important
prior information (including classification map, distance field, and direction
field) for generating coarse boundary proposals meanwhile guiding the
optimization of boundary transformer. The boundary transformer module adopts an
encoder-decoder structure, in which the encoder is constructed by multi-layer
transformer blocks with residual connection while the decoder is a simple
multi-layer perceptron network (MLP). Under the guidance of prior information,
the boundary transformer module will gradually refine the coarse boundary
proposals via boundary deformation in an iterative manner. Furthermore, we
propose a novel boundary energy loss (BEL) which introduces an energy
minimization constraint and an energy monotonically decreasing constraint for
every boundary optimization step. Extensive experiments on publicly available
and challenging datasets demonstrate the state-of-the-art performance and
promising efficiency of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Depth Completion: A Survey. (arXiv:2205.05335v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05335">
<div class="article-summary-box-inner">
<span><p>Depth completion aims at predicting dense pixel-wise depth from a sparse map
captured from a depth sensor. It plays an essential role in various
applications such as autonomous driving, 3D reconstruction, augmented reality,
and robot navigation. Recent successes on the task have been demonstrated and
dominated by deep learning based solutions. In this article, for the first
time, we provide a comprehensive literature review that helps readers better
grasp the research trends and clearly understand the current advances. We
investigate the related studies from the design aspects of network
architectures, loss functions, benchmark datasets, and learning strategies with
a proposal of a novel taxonomy that categorizes existing methods. Besides, we
present a quantitative comparison of model performance on two widely used
benchmark datasets, including an indoor and an outdoor dataset. Finally, we
discuss the challenges of prior works and provide readers with some insights
for future research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoLC: Search Lightweight and Top-Performing Architecture for Remote Sensing Image Land-Cover Classification. (arXiv:2205.05369v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05369">
<div class="article-summary-box-inner">
<span><p>Land-cover classification has long been a hot and difficult challenge in
remote sensing community. With massive High-resolution Remote Sensing (HRS)
images available, manually and automatically designed Convolutional Neural
Networks (CNNs) have already shown their great latent capacity on HRS
land-cover classification in recent years. Especially, the former can achieve
better performance while the latter is able to generate lightweight
architecture. Unfortunately, they both have shortcomings. On the one hand,
because manual CNNs are almost proposed for natural image processing, it
becomes very redundant and inefficient to process HRS images. On the other
hand, nascent Neural Architecture Search (NAS) techniques for dense prediction
tasks are mainly based on encoder-decoder architecture, and just focus on the
automatic design of the encoder, which makes it still difficult to recover the
refined mapping when confronting complicated HRS scenes.
</p>
<p>To overcome their defects and tackle the HRS land-cover classification
problems better, we propose AutoLC which combines the advantages of two
methods. First, we devise a hierarchical search space and gain the lightweight
encoder underlying gradient-based search strategy. Second, we meticulously
design a lightweight but top-performing decoder that is adaptive to the
searched encoder of itself. Finally, experimental results on the LoveDA
land-cover dataset demonstrate that our AutoLC method outperforms the
state-of-art manual and automatic methods with much less computational
consumption.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recurrent Encoder-Decoder Networks for Vessel Trajectory Prediction with Uncertainty Estimation. (arXiv:2205.05404v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05404">
<div class="article-summary-box-inner">
<span><p>Recent deep learning methods for vessel trajectory prediction are able to
learn complex maritime patterns from historical Automatic Identification System
(AIS) data and accurately predict sequences of future vessel positions with a
prediction horizon of several hours. However, in maritime surveillance
applications, reliably quantifying the prediction uncertainty can be as
important as obtaining high accuracy. This paper extends deep learning
frameworks for trajectory prediction tasks by exploring how recurrent
encoder-decoder neural networks can be tasked not only to predict but also to
yield a corresponding prediction uncertainty via Bayesian modeling of epistemic
and aleatoric uncertainties. We compare the prediction performance of two
different models based on labeled or unlabeled input data to highlight how
uncertainty quantification and accuracy can be improved by using, if available,
additional information on the intention of the ship (e.g., its planned
destination).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Objective Method for Pedestrian Occlusion Level Classification. (arXiv:2205.05412v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05412">
<div class="article-summary-box-inner">
<span><p>Pedestrian detection is among the most safety-critical features of driver
assistance systems for autonomous vehicles. One of the most complex detection
challenges is that of partial occlusion, where a target object is only
partially available to the sensor due to obstruction by another foreground
object. A number of current pedestrian detection benchmarks provide annotation
for partial occlusion to assess algorithm performance in these scenarios,
however each benchmark varies greatly in their definition of the occurrence and
severity of occlusion. In addition, current occlusion level annotation methods
contain a high degree of subjectivity by the human annotator. This can lead to
inaccurate or inconsistent reporting of an algorithm's detection performance
for partially occluded pedestrians, depending on which benchmark is used. This
research presents a novel, objective method for pedestrian occlusion level
classification for ground truth annotation. Occlusion level classification is
achieved through the identification of visible pedestrian keypoints and through
the use of a novel, effective method of 2D body surface area estimation.
Experimental results demonstrate that the proposed method reflects the
pixel-wise occlusion level of pedestrians in images and is effective for all
forms of occlusion, including challenging edge cases such as self-occlusion,
truncation and inter-occluding pedestrians.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Label Logo Recognition and Retrieval based on Weighted Fusion of Neural Features. (arXiv:2205.05419v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05419">
<div class="article-summary-box-inner">
<span><p>Logo classification is a particular case of image classification, since these
may contain only text, images, or a combination of both. In this work, we
propose a system for the multi-label classification and similarity search of
logo images. The method allows obtaining the most similar logos on the basis of
their shape, color, business sector, semantics, general characteristics, or a
combination of such features established by the user. This is done by employing
a set of multi-label networks specialized in certain characteristics of logos.
The features extracted from these networks are combined to perform the
similarity search according to the search criteria established. Since the text
of logos is sometimes irrelevant for the classification, a preprocessing stage
is carried out to remove it, thus improving the overall performance. The
proposed approach is evaluated using the European Union Trademark (EUTM)
dataset, structured with the hierarchical Vienna classification system, which
includes a series of metadata with which to index trademarks. We also make a
comparison between well known logo topologies and Vienna in order to help
designers understand their correspondences. The experimentation carried out
attained reliable performance results, both quantitatively and qualitatively,
which outperformed the state-of-the-art results. In addition, since the
semantics and classification of brands can often be subjective, we also
surveyed graphic design students and professionals in order to assess the
reliability of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RustSEG -- Automated segmentation of corrosion using deep learning. (arXiv:2205.05426v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05426">
<div class="article-summary-box-inner">
<span><p>The inspection of infrastructure for corrosion remains a task that is
typically performed manually by qualified engineers or inspectors. This task of
inspection is laborious, slow, and often requires complex access. Recently,
deep learning based algorithms have revealed promise and performance in the
automatic detection of corrosion. However, to date, research regarding the
segmentation of images for automated corrosion detection has been limited, due
to the lack of availability of per-pixel labelled data sets which are required
for model training. Herein, a novel deep learning approach (termed RustSEG) is
presented, that can accurately segment images for automated corrosion
detection, without the requirement of per-pixel labelled data sets for
training. The RustSEG method will first, using deep learning techniques,
determine if corrosion is present in an image (i.e. a classification task), and
then if corrosion is present, the model will examine what pixels in the
original image contributed to that classification decision. Finally, the method
can refine its predictions into a pixel-level segmentation mask. In ideal
cases, the method is able to generate precise masks of corrosion in images,
demonstrating that the automated segmentation of corrosion without per-pixel
training data is possible, addressing a significant hurdle in automated
infrastructure inspection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Continual Deepfake Detection Benchmark: Dataset, Methods, and Essentials. (arXiv:2205.05467v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05467">
<div class="article-summary-box-inner">
<span><p>There have been emerging a number of benchmarks and techniques for the
detection of deepfakes. However, very few works study the detection of
incrementally appearing deepfakes in the real-world scenarios. To simulate the
wild scenes, this paper suggests a continual deepfake detection benchmark
(CDDB) over a new collection of deepfakes from both known and unknown
generative models. The suggested CDDB designs multiple evaluations on the
detection over easy, hard, and long sequence of deepfake tasks, with a set of
appropriate measures. In addition, we exploit multiple approaches to adapt
multiclass incremental learning methods, commonly used in the continual visual
recognition, to the continual deepfake detection problem. We evaluate several
methods, including the adapted ones, on the proposed CDDB. Within the proposed
benchmark, we explore some commonly known essentials of standard continual
learning. Our study provides new insights on these essentials in the context of
continual deepfake detection. The suggested CDDB is clearly more challenging
than the existing benchmarks, which thus offers a suitable evaluation avenue to
the future research. Our benchmark dataset and the source code will be made
publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Supervised Distillation for Continual Representation Learning. (arXiv:2205.05476v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05476">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel training procedure for the continual
representation learning problem in which a neural network model is sequentially
learned to alleviate catastrophic forgetting in visual search tasks. Our
method, called Contrastive Supervised Distillation (CSD), reduces feature
forgetting while learning discriminative features. This is achieved by
leveraging labels information in a distillation setting in which the student
model is contrastively learned from the teacher model. Extensive experiments
show that CSD performs favorably in mitigating catastrophic forgetting by
outperforming current state-of-the-art methods. Our results also provide
further evidence that feature forgetting evaluated in visual retrieval tasks is
not as catastrophic as in classification tasks. Code at:
https://github.com/NiccoBiondi/ContrastiveSupervisedDistillation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scene Consistency Representation Learning for Video Scene Segmentation. (arXiv:2205.05487v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05487">
<div class="article-summary-box-inner">
<span><p>A long-term video, such as a movie or TV show, is composed of various scenes,
each of which represents a series of shots sharing the same semantic story.
Spotting the correct scene boundary from the long-term video is a challenging
task, since a model must understand the storyline of the video to figure out
where a scene starts and ends. To this end, we propose an effective
Self-Supervised Learning (SSL) framework to learn better shot representations
from unlabeled long-term videos. More specifically, we present an SSL scheme to
achieve scene consistency, while exploring considerable data augmentation and
shuffling methods to boost the model generalizability. Instead of explicitly
learning the scene boundary features as in the previous methods, we introduce a
vanilla temporal model with less inductive bias to verify the quality of the
shot features. Our method achieves the state-of-the-art performance on the task
of Video Scene Segmentation. Additionally, we suggest a more fair and
reasonable benchmark to evaluate the performance of Video Scene Segmentation
methods. The code is made available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning and Computer Vision Techniques for Microcirculation Analysis: A Review. (arXiv:2205.05493v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05493">
<div class="article-summary-box-inner">
<span><p>The analysis of microcirculation images has the potential to reveal early
signs of life-threatening diseases like sepsis. Quantifying the capillary
density and the capillary distribution in microcirculation images can be used
as a biological marker to assist critically ill patients. The quantification of
these biological markers is labor-intensive, time-consuming, and subject to
interobserver variability. Several computer vision techniques with varying
performance can be used to automate the analysis of these microcirculation
images in light of the stated challenges. In this paper, we present a survey of
over 50 research papers and present the most relevant and promising computer
vision algorithms to automate the analysis of microcirculation images.
Furthermore, we present a survey of the methods currently used by other
researchers to automate the analysis of microcirculation images. This survey is
of high clinical relevance because it acts as a guidebook of techniques for
other researchers to develop their microcirculation analysis systems and
algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TextMatcher: Cross-Attentional Neural Network to Compare Image and Text. (arXiv:2205.05507v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05507">
<div class="article-summary-box-inner">
<span><p>We study a novel multimodal-learning problem, which we call text matching:
given an image containing a single-line text and a candidate text
transcription, the goal is to assess whether the text represented in the image
corresponds to the candidate text. We devise the first machine-learning model
specifically designed for this problem. The proposed model, termed TextMatcher,
compares the two inputs by applying a cross-attention mechanism over the
embedding representations of image and text, and it is trained in an end-to-end
fashion. We extensively evaluate the empirical performance of TextMatcher on
the popular IAM dataset. Results attest that, compared to a baseline and
existing models designed for related problems, TextMatcher achieves higher
performance on a variety of configurations, while at the same time running
faster at inference time. We also showcase TextMatcher in a real-world
application scenario concerning the automatic processing of bank cheques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">READ: Large-Scale Neural Scene Rendering for Autonomous Driving. (arXiv:2205.05509v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05509">
<div class="article-summary-box-inner">
<span><p>Synthesizing free-view photo-realistic images is an important task in
multimedia. With the development of advanced driver assistance systems~(ADAS)
and their applications in autonomous vehicles, experimenting with different
scenarios becomes a challenge. Although the photo-realistic street scenes can
be synthesized by image-to-image translation methods, which cannot produce
coherent scenes due to the lack of 3D information. In this paper, a large-scale
neural rendering method is proposed to synthesize the autonomous driving
scene~(READ), which makes it possible to synthesize large-scale driving
scenarios on a PC through a variety of sampling schemes. In order to represent
driving scenarios, we propose an {\omega} rendering network to learn neural
descriptors from sparse point clouds. Our model can not only synthesize
realistic driving scenes but also stitch and edit driving scenes. Experiments
show that our model performs well in large-scale driving scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Study Of Self-supervised Learning Approaches For Object Detection With Transformers. (arXiv:2205.05543v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05543">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning (SSL) methods such as masked language modeling have
shown massive performance gains by pretraining transformer models for a variety
of natural language processing tasks. The follow-up research adapted similar
methods like masked image modeling in vision transformer and demonstrated
improvements in the image classification task. Such simple self-supervised
methods are not exhaustively studied for object detection transformers (DETR,
Deformable DETR) as their transformer encoder modules take input in the
convolutional neural network (CNN) extracted feature space rather than the
image space as in general vision transformers. However, the CNN feature maps
still maintain the spatial relationship and we utilize this property to design
self-supervised learning approaches to train the encoder of object detection
transformers in pretraining and multi-task learning settings. We explore common
self-supervised methods based on image reconstruction, masked image modeling
and jigsaw. Preliminary experiments in the iSAID dataset demonstrate faster
convergence of DETR in the initial epochs in both pretraining and multi-task
learning settings; nonetheless, similar improvement is not observed in the case
of multi-task learning with Deformable DETR. The code for our experiments with
DETR and Deformable DETR are available at https://github.com/gokulkarthik/detr
and https://github.com/gokulkarthik/Deformable-DETR respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CNN-LSTM Based Multimodal MRI and Clinical Data Fusion for Predicting Functional Outcome in Stroke Patients. (arXiv:2205.05545v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05545">
<div class="article-summary-box-inner">
<span><p>Clinical outcome prediction plays an important role in stroke patient
management. From a machine learning point-of-view, one of the main challenges
is dealing with heterogeneous data at patient admission, i.e. the image data
which are multidimensional and the clinical data which are scalars. In this
paper, a multimodal convolutional neural network - long short-term memory
(CNN-LSTM) based ensemble model is proposed. For each MR image module, a
dedicated network provides preliminary prediction of the clinical outcome using
the modified Rankin scale (mRS). The final mRS score is obtained by merging the
preliminary probabilities of each module dedicated to a specific type of MR
image weighted by the clinical metadata, here age or the National Institutes of
Health Stroke Scale (NIHSS). The experimental results demonstrate that the
proposed model surpasses the baselines and offers an original way to
automatically encode the spatio-temporal context of MR images in a deep
learning architecture. The highest AUC (0.77) was achieved for the proposed
model with NIHSS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NMR: Neural Manifold Representation for Autonomous Driving. (arXiv:2205.05551v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05551">
<div class="article-summary-box-inner">
<span><p>Autonomous driving requires efficient reasoning about the Spatio-temporal
nature of the semantics of the scene. Recent approaches have successfully
amalgamated the traditional modular architecture of an autonomous driving stack
comprising perception, prediction, and planning in an end-to-end trainable
system. Such a system calls for a shared latent space embedding with
interpretable intermediate trainable projected representation. One such
successfully deployed representation is the Bird's-Eye View(BEV) representation
of the scene in ego-frame. However, a fundamental assumption for an undistorted
BEV is the local coplanarity of the world around the ego-vehicle. This
assumption is highly restrictive, as roads, in general, do have gradients. The
resulting distortions make path planning inefficient and incorrect. To overcome
this limitation, we propose Neural Manifold Representation (NMR), a
representation for the task of autonomous driving that learns to infer
semantics and predict way-points on a manifold over a finite horizon, centered
on the ego-vehicle. We do this using an iterative attention mechanism applied
on a latent high dimensional embedding of surround monocular images and partial
ego-vehicle state. This representation helps generate motion and behavior plans
consistent with and cognizant of the surface geometry. We propose a sampling
algorithm based on edge-adaptive coverage loss of BEV occupancy grid and
associated guidance flow field to generate the surface manifold while incurring
minimal computational overhead. We aim to test the efficacy of our approach on
CARLA and SYNTHIA-SF.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Performance of a deep learning system for detection of referable diabetic retinopathy in real clinical settings. (arXiv:2205.05554v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05554">
<div class="article-summary-box-inner">
<span><p>Background: To determine the ability of a commercially available deep
learning system, RetCAD v.1.3.1 (Thirona, Nijmegen, The Netherlands) for the
automatic detection of referable diabetic retinopathy (DR) on a dataset of
colour fundus images acquired during routine clinical practice in a tertiary
hospital screening program, analyzing the reduction of workload that can be
released incorporating this artificial intelligence-based technology. Methods:
Evaluation of the software was performed on a dataset of 7195 nonmydriatic
fundus images from 6325 eyes of 3189 diabetic patients attending our screening
program between February to December of 2019. The software generated a DR
severity score for each colour fundus image which was combined into an
eye-level score. This score was then compared with a reference standard as set
by a human expert using receiver operating characteristic (ROC) curve analysis.
Results: The artificial intelligence (AI) software achieved an area under the
ROC curve (AUC) value of 0.988 [0.981:0.993] for the detection of referable DR.
At the proposed operating point, the sensitivity of the RetCAD software for DR
is 90.53% and specificity is 97.13%. A workload reduction of 96% could be
achieved at the cost of only 6 false negatives. Conclusions: The AI software
correctly identified the vast majority of referable DR cases, with a workload
reduction of 96% of the cases that would need to be checked, while missing
almost no true cases, so it may therefore be used as an instrument for triage.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Review on Panoramic Imaging and Its Applications in Scene Understanding. (arXiv:2205.05570v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05570">
<div class="article-summary-box-inner">
<span><p>With the rapid development of high-speed communication and artificial
intelligence technologies, human perception of real-world scenes is no longer
limited to the use of small Field of View (FoV) and low-dimensional scene
detection devices. Panoramic imaging emerges as the next generation of
innovative intelligent instruments for environmental perception and
measurement. However, while satisfying the need for large-FoV photographic
imaging, panoramic imaging instruments are expected to have high resolution, no
blind area, miniaturization, and multi-dimensional intelligent perception, and
can be combined with artificial intelligence methods towards the next
generation of intelligent instruments, enabling deeper understanding and more
holistic perception of 360-degree real-world surrounding environments.
Fortunately, recent advances in freeform surfaces, thin-plate optics, and
metasurfaces provide innovative approaches to address human perception of the
environment, offering promising ideas beyond conventional optical imaging. In
this review, we begin with introducing the basic principles of panoramic
imaging systems, and then describe the architectures, features, and functions
of various panoramic imaging systems. Afterwards, we discuss in detail the
broad application prospects and great design potential of freeform surfaces,
thin-plate optics, and metasurfaces in panoramic imaging. We then provide a
detailed analysis on how these techniques can help enhance the performance of
panoramic imaging systems. We further offer a detailed analysis of applications
of panoramic imaging in scene understanding for autonomous driving and
robotics, spanning panoramic semantic image segmentation, panoramic depth
estimation, panoramic visual localization, and so on. Finally, we cast a
perspective on future potential and research directions for panoramic imaging
instruments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Face Detection on Mobile: Five Implementations and Analysis. (arXiv:2205.05572v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05572">
<div class="article-summary-box-inner">
<span><p>In many practical cases face detection on smartphones or other highly
portable devices is a necessity. Applications include mobile face access
control systems, driver status tracking, emotion recognition, etc. Mobile
devices have limited processing power and should have long-enough battery life
even with face detection application running. Thus, striking the right balance
between algorithm quality and complexity is crucial. In this work we adapt 5
algorithms to mobile. These algorithms are based on handcrafted or
neural-network-based features and include: Viola-Jones (Haar cascade), LBP,
HOG, MTCNN, BlazeFace. We analyze inference time of these algorithms on
different devices with different input image resolutions. We provide guidance,
which algorithms are the best fit for mobile face access control systems and
potentially other mobile applications. Interestingly, we note that cascaded
algorithms perform faster on scenes without faces, while BlazeFace is slower on
empty scenes. Exploiting this behavior might be useful in practice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DoubleMatch: Improving Semi-Supervised Learning with Self-Supervision. (arXiv:2205.05575v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05575">
<div class="article-summary-box-inner">
<span><p>Following the success of supervised learning, semi-supervised learning (SSL)
is now becoming increasingly popular. SSL is a family of methods, which in
addition to a labeled training set, also use a sizable collection of unlabeled
data for fitting a model. Most of the recent successful SSL methods are based
on pseudo-labeling approaches: letting confident model predictions act as
training labels. While these methods have shown impressive results on many
benchmark datasets, a drawback of this approach is that not all unlabeled data
are used during training. We propose a new SSL algorithm, DoubleMatch, which
combines the pseudo-labeling technique with a self-supervised loss, enabling
the model to utilize all unlabeled data in the training process. We show that
this method achieves state-of-the-art accuracies on multiple benchmark datasets
while also reducing training times compared to existing SSL methods. Code is
available at https://github.com/walline/doublematch.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TDT: Teaching Detectors to Track without Fully Annotated Videos. (arXiv:2205.05583v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05583">
<div class="article-summary-box-inner">
<span><p>Recently, one-stage trackers that use a joint model to predict both
detections and appearance embeddings in one forward pass received much
attention and achieved state-of-the-art results on the Multi-Object Tracking
(MOT) benchmarks. However, their success depends on the availability of videos
that are fully annotated with tracking data, which is expensive and hard to
obtain. This can limit the model generalization. In comparison, the two-stage
approach, which performs detection and embedding separately, is slower but
easier to train as their data are easier to annotate. We propose to combine the
best of the two worlds through a data distillation approach. Specifically, we
use a teacher embedder, trained on Re-ID datasets, to generate pseudo
appearance embedding labels for the detection datasets. Then, we use the
augmented dataset to train a detector that is also capable of regressing these
pseudo-embeddings in a fully-convolutional fashion. Our proposed one-stage
solution matches the two-stage counterpart in quality but is 3 times faster.
Even though the teacher embedder has not seen any tracking data during
training, our proposed tracker achieves competitive performance with some
popular trackers (e.g. JDE) trained with fully labeled tracking data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Multi-Person Audio/Visual Automatic Speech Recognition. (arXiv:2205.05586v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05586">
<div class="article-summary-box-inner">
<span><p>Traditionally, audio-visual automatic speech recognition has been studied
under the assumption that the speaking face on the visual signal is the face
matching the audio. However, in a more realistic setting, when multiple faces
are potentially on screen one needs to decide which face to feed to the A/V ASR
system. The present work takes the recent progress of A/V ASR one step further
and considers the scenario where multiple people are simultaneously on screen
(multi-person A/V ASR). We propose a fully differentiable A/V ASR model that is
able to handle multiple face tracks in a video. Instead of relying on two
separate models for speaker face selection and audio-visual ASR on a single
face track, we introduce an attention layer to the ASR encoder that is able to
soft-select the appropriate face video track. Experiments carried out on an A/V
system trained on over 30k hours of YouTube videos illustrate that the proposed
approach can automatically select the proper face tracks with minor WER
degradation compared to an oracle selection of the speaking face while still
showing benefits of employing the visual signal instead of the audio alone.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video-ReTime: Learning Temporally Varying Speediness for Time Remapping. (arXiv:2205.05609v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05609">
<div class="article-summary-box-inner">
<span><p>We propose a method for generating a temporally remapped video that matches
the desired target duration while maximally preserving natural video dynamics.
Our approach trains a neural network through self-supervision to recognize and
accurately localize temporally varying changes in the video playback speed. To
re-time videos, we 1. use the model to infer the slowness of individual video
frames, and 2. optimize the temporal frame sub-sampling to be consistent with
the model's slowness predictions. We demonstrate that this model can detect
playback speed variations more accurately while also being orders of magnitude
more efficient than prior approaches. Furthermore, we propose an optimization
for video re-timing that enables precise control over the target duration and
performs more robustly on longer videos than prior methods. We evaluate the
model quantitatively on artificially speed-up videos, through transfer to
action recognition, and qualitatively through user studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RepSR: Training Efficient VGG-style Super-Resolution Networks with Structural Re-Parameterization and Batch Normalization. (arXiv:2205.05671v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05671">
<div class="article-summary-box-inner">
<span><p>This paper explores training efficient VGG-style super-resolution (SR)
networks with the structural re-parameterization technique. The general
pipeline of re-parameterization is to train networks with multi-branch topology
first, and then merge them into standard 3x3 convolutions for efficient
inference. In this work, we revisit those primary designs and investigate
essential components for re-parameterizing SR networks. First of all, we find
that batch normalization (BN) is important to bring training non-linearity and
improve the final performance. However, BN is typically ignored in SR, as it
usually degrades the performance and introduces unpleasant artifacts. We
carefully analyze the cause of BN issue and then propose a straightforward yet
effective solution. In particular, we first train SR networks with mini-batch
statistics as usual, and then switch to using population statistics at the
later training period. While we have successfully re-introduced BN into SR, we
further design a new re-parameterizable block tailored for SR, namely RepSR. It
consists of a clean residual path and two expand-and-squeeze convolution paths
with the modified BN. Extensive experiments demonstrate that our simple RepSR
is capable of achieving superior performance to previous SR re-parameterization
methods among different model sizes. In addition, our RepSR can achieve a
better trade-off between performance and actual running time (throughput) than
previous SR methods. Codes will be available at
https://github.com/TencentARC/RepSR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NTIRE 2022 Challenge on Efficient Super-Resolution: Methods and Results. (arXiv:2205.05675v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05675">
<div class="article-summary-box-inner">
<span><p>This paper reviews the NTIRE 2022 challenge on efficient single image
super-resolution with focus on the proposed solutions and results. The task of
the challenge was to super-resolve an input image with a magnification factor
of $\times$4 based on pairs of low and corresponding high resolution images.
The aim was to design a network for single image super-resolution that achieved
improvement of efficiency measured according to several metrics including
runtime, parameters, FLOPs, activations, and memory consumption while at least
maintaining the PSNR of 29.00dB on DIV2K validation set. IMDN is set as the
baseline for efficiency measurement. The challenge had 3 tracks including the
main track (runtime), sub-track one (model complexity), and sub-track two
(overall performance). In the main track, the practical runtime performance of
the submissions was evaluated. The rank of the teams were determined directly
by the absolute value of the average runtime on the validation set and test
set. In sub-track one, the number of parameters and FLOPs were considered. And
the individual rankings of the two metrics were summed up to determine a final
ranking in this track. In sub-track two, all of the five metrics mentioned in
the description of the challenge including runtime, parameter count, FLOPs,
activations, and memory consumption were considered. Similar to sub-track one,
the rankings of five metrics were summed up to determine a final ranking. The
challenge had 303 registered participants, and 43 teams made valid submissions.
They gauge the state-of-the-art in efficient single image super-resolution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Random Channel Pruning for Neural Network Compression. (arXiv:2205.05676v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05676">
<div class="article-summary-box-inner">
<span><p>Channel (or 3D filter) pruning serves as an effective way to accelerate the
inference of neural networks. There has been a flurry of algorithms that try to
solve this practical problem, each being claimed effective in some ways. Yet, a
benchmark to compare those algorithms directly is lacking, mainly due to the
complexity of the algorithms and some custom settings such as the particular
network configuration or training procedure. A fair benchmark is important for
the further development of channel pruning.
</p>
<p>Meanwhile, recent investigations reveal that the channel configurations
discovered by pruning algorithms are at least as important as the pre-trained
weights. This gives channel pruning a new role, namely searching the optimal
channel configuration. In this paper, we try to determine the channel
configuration of the pruned models by random search. The proposed approach
provides a new way to compare different methods, namely how well they behave
compared with random pruning. We show that this simple strategy works quite
well compared with other channel pruning methods. We also show that under this
setting, there are surprisingly no clear winners among different channel
importance evaluation methods, which then may tilt the research efforts into
advanced channel configuration searching methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HULC: 3D Human Motion Capture with Pose Manifold Sampling and Dense Contact Guidance. (arXiv:2205.05677v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05677">
<div class="article-summary-box-inner">
<span><p>Marker-less monocular 3D human motion capture (MoCap) with scene interactions
is a challenging research topic relevant for extended reality, robotics and
virtual avatar generation. Due to the inherent depth ambiguity of monocular
settings, 3D motions captured with existing methods often contain severe
artefacts such as incorrect body-scene inter-penetrations, jitter and body
floating. To tackle these issues, we propose HULC, a new approach for 3D human
MoCap which is aware of the scene geometry. HULC estimates 3D poses and dense
body-environment surface contacts for improved 3D localisations, as well as the
absolute scale of the subject. Furthermore, we introduce a 3D pose trajectory
optimisation based on a novel pose manifold sampling that resolves erroneous
body-environment inter-penetrations. Although the proposed method requires less
structured inputs compared to existing scene-aware monocular MoCap algorithms,
it produces more physically-plausible poses: HULC significantly and
consistently outperforms the existing approaches in various experiments and on
different metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RISP: Rendering-Invariant State Predictor with Differentiable Simulation and Rendering for Cross-Domain Parameter Estimation. (arXiv:2205.05678v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05678">
<div class="article-summary-box-inner">
<span><p>This work considers identifying parameters characterizing a physical system's
dynamic motion directly from a video whose rendering configurations are
inaccessible. Existing solutions require massive training data or lack
generalizability to unknown rendering configurations. We propose a novel
approach that marries domain randomization and differentiable rendering
gradients to address this problem. Our core idea is to train a
rendering-invariant state-prediction (RISP) network that transforms image
differences into state differences independent of rendering configurations,
e.g., lighting, shadows, or material reflectance. To train this predictor, we
formulate a new loss on rendering variances using gradients from differentiable
rendering. Moreover, we present an efficient, second-order method to compute
the gradients of this loss, allowing it to be integrated seamlessly into modern
deep learning frameworks. We evaluate our method in rigid-body and
deformable-body simulation environments using four tasks: state estimation,
system identification, imitation learning, and visuomotor control. We further
demonstrate the efficacy of our approach on a real-world example: inferring the
state and action sequences of a quadrotor from a video of its motion sequences.
Compared with existing methods, our approach achieves significantly lower
reconstruction errors and has better generalizability among unknown rendering
configurations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DDNet: Dual-path Decoder Network for Occlusion Relationship Reasoning. (arXiv:1911.11582v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1911.11582">
<div class="article-summary-box-inner">
<span><p>Occlusion relationship reasoning based on convolution neural networks
consists of two subtasks: occlusion boundary extraction and occlusion
orientation inference. Due to the essential differences between the two
subtasks in the feature expression at the higher and lower stages, it is
challenging to carry on them simultaneously in one network. To address this
issue, we propose a novel Dual-path Decoder Network, which uniformly extracts
occlusion information at higher stages and separates into two paths to recover
boundary and occlusion orientation respectively in lower stages. Besides,
considering the restriction of occlusion orientation presentation to occlusion
orientation learning, we design a new orthogonal representation for occlusion
orientation and proposed the Orthogonal Orientation Regression loss which can
get rid of the unfitness between occlusion representation and learning and
further prompt the occlusion orientation learning. Finally, we apply a
multi-scale loss together with our proposed orientation regression loss to
guide the boundary and orientation path learning respectively. Experiments
demonstrate that our proposed method achieves state-of-the-art results on PIOD
and BSDS ownership datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DMT: Dynamic Mutual Training for Semi-Supervised Learning. (arXiv:2004.08514v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.08514">
<div class="article-summary-box-inner">
<span><p>Recent semi-supervised learning methods use pseudo supervision as core idea,
especially self-training methods that generate pseudo labels. However, pseudo
labels are unreliable. Self-training methods usually rely on single model
prediction confidence to filter low-confidence pseudo labels, thus remaining
high-confidence errors and wasting many low-confidence correct labels. In this
paper, we point out it is difficult for a model to counter its own errors.
Instead, leveraging inter-model disagreement between different models is a key
to locate pseudo label errors. With this new viewpoint, we propose mutual
training between two different models by a dynamically re-weighted loss
function, called Dynamic Mutual Training (DMT). We quantify inter-model
disagreement by comparing predictions from two different models to dynamically
re-weight loss in training, where a larger disagreement indicates a possible
error and corresponds to a lower loss value. Extensive experiments show that
DMT achieves state-of-the-art performance in both image classification and
semantic segmentation. Our codes are released at
https://github.com/voldemortX/DST-CBC .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Asynchronous Kalman Filter for Hybrid Event Cameras. (arXiv:2012.05590v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.05590">
<div class="article-summary-box-inner">
<span><p>Event cameras are ideally suited to capture HDR visual information without
blur but perform poorly on static or slowly changing scenes. Conversely,
conventional image sensors measure absolute intensity of slowly changing scenes
effectively but do poorly on high dynamic range or quickly changing scenes. In
this paper, we present an event-based video reconstruction pipeline for High
Dynamic Range (HDR) scenarios. The proposed algorithm includes a frame
augmentation pre-processing step that deblurs and temporally interpolates frame
data using events. The augmented frame and event data are then fused using a
novel asynchronous Kalman filter under a unifying uncertainty model for both
sensors. Our experimental results are evaluated on both publicly available
datasets with challenging lighting conditions and fast motions and our new
dataset with HDR reference. The proposed algorithm outperforms state-of-the-art
methods in both absolute intensity error (48% reduction) and image similarity
indexes (average 11% improvement).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Annotated Training Data for 6D Object Pose Estimation in Operational Environments with Minimal User Interaction. (arXiv:2103.09696v3 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.09696">
<div class="article-summary-box-inner">
<span><p>Recently developed deep neural networks achieved state-of-the-art results in
the subject of 6D object pose estimation for robot manipulation. However, those
supervised deep learning methods require expensive annotated training data.
Current methods for reducing those costs frequently use synthetic data from
simulations, but rely on expert knowledge and suffer from the "domain gap" when
shifting to the real world. Here, we present a proof of concept for a novel
approach of autonomously generating annotated training data for 6D object pose
estimation. This approach is designed for learning new objects in operational
environments while requiring little interaction and no expertise on the part of
the user. We evaluate our autonomous data generation approach in two grasping
experiments, where we archive a similar grasping success rate as related work
on a non autonomously generated data set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shadow Generation for Composite Image in Real-world Scenes. (arXiv:2104.10338v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.10338">
<div class="article-summary-box-inner">
<span><p>Image composition targets at inserting a foreground object into a background
image. Most previous image composition methods focus on adjusting the
foreground to make it compatible with background while ignoring the shadow
effect of foreground on the background. In this work, we focus on generating
plausible shadow for the foreground object in the composite image. First, we
contribute a real-world shadow generation dataset DESOBA by generating
synthetic composite images based on paired real images and deshadowed images.
Then, we propose a novel shadow generation network SGRNet, which consists of a
shadow mask prediction stage and a shadow filling stage. In the shadow mask
prediction stage, foreground and background information are thoroughly
interacted to generate foreground shadow mask. In the shadow filling stage,
shadow parameters are predicted to fill the shadow area. Extensive experiments
on our DESOBA dataset and real composite images demonstrate the effectiveness
of our proposed method. Our dataset and code are available at
https://github.com/bcmi/Object-Shadow-Generation-Dataset-DESOBA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Modal Transformer for Accelerated MR Imaging. (arXiv:2106.14248v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.14248">
<div class="article-summary-box-inner">
<span><p>Accelerated multi-modal magnetic resonance (MR) imaging is a new and
effective solution for fast MR imaging, providing superior performance in
restoring the target modality from its undersampled counterpart with guidance
from an auxiliary modality. However, existing works simply combine the
auxiliary modality as prior information, lacking in-depth investigations on the
potential mechanisms for fusing different modalities. Further, they usually
rely on the convolutional neural networks (CNNs), which is limited by the
intrinsic locality in capturing the long-distance dependency. To this end, we
propose a multi-modal transformer (MTrans), which is capable of transferring
multi-scale features from the target modality to the auxiliary modality, for
accelerated MR imaging. To capture deep multi-modal information, our MTrans
utilizes an improved multi-head attention mechanism, named cross attention
module, which absorbs features from the auxiliary modality that contribute to
the target modality. Our framework provides three appealing benefits: (i) Our
MTrans use an improved transformers for multi-modal MR imaging, affording more
global information compared with existing CNN-based methods. (ii) A new cross
attention module is proposed to exploit the useful information in each modality
at different scales. The small patch in the target modality aims to keep more
fine details, the large patch in the auxiliary modality aims to obtain
high-level context features from the larger region and supplement the target
modality effectively. (iii) We evaluate MTrans with various accelerated
multi-modal MR imaging tasks, e.g., MR image reconstruction and
super-resolution, where MTrans outperforms state-of-the-art methods on fastMRI
and real-world clinical datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assessment of Deep Learning-based Heart Rate Estimation using Remote Photoplethysmography under Different Illuminations. (arXiv:2107.13193v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.13193">
<div class="article-summary-box-inner">
<span><p>Remote photoplethysmography (rPPG) monitors heart rate without requiring
physical contact, which allows for a wide variety of applications. Deep
learning-based rPPG have demonstrated superior performance over the traditional
approaches in controlled context. However, the lighting situation in indoor
space is typically complex, with uneven light distribution and frequent
variations in illumination. It lacks a fair comparison of different methods
under different illuminations using the same dataset. In this paper, we present
a public dataset, namely the BH-rPPG dataset, which contains data from thirty
five subjects under three illuminations: low, medium, and high illumination. We
also provide the ground truth heart rate measured by an oximeter. We evaluate
the performance of three deep learning-based methods (Deepphys, rPPGNet, and
Physnet) to that of four traditional methods (CHROM, GREEN, ICA, and POS) using
two public datasets: the UBFC-rPPG dataset and the BH-rPPG dataset. The
experimental results demonstrate that traditional methods are generally more
resistant to fluctuating illuminations. We found that the Physnet achieves
lowest mean absolute error (MAE) among deep learning-based method under medium
illumination, whereas the CHROM achieves 1.04 beats per minute (BPM),
outperforming the Physnet by 80$\%$. Additionally, we investigate potential
methods for improving performance of deep learning-based methods. We find that
brightness augmentation make model more robust to variation illumination. These
findings suggest that while developing deep learning-based heart rate
estimation algorithms, illumination variation should be taken into account.
This work serves as a benchmark for rPPG performance evaluation and it opens a
pathway for future investigation into deep learning-based rPPG under
illumination variations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stereo Hybrid Event-Frame (SHEF) Cameras for 3D Perception. (arXiv:2110.04988v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04988">
<div class="article-summary-box-inner">
<span><p>Stereo camera systems play an important role in robotics applications to
perceive the 3D world. However, conventional cameras have drawbacks such as low
dynamic range, motion blur and latency due to the underlying frame-based
mechanism. Event cameras address these limitations as they report the
brightness changes of each pixel independently with a fine temporal resolution,
but they are unable to acquire absolute intensity information directly.
Although integrated hybrid event-frame sensors (eg., DAVIS) are available, the
quality of data is compromised by coupling at the pixel level in the circuit
fabrication of such cameras. This paper proposes a stereo hybrid event-frame
(SHEF) camera system that offers a sensor modality with separate high-quality
pure event and pure frame cameras, overcoming the limitations of each separate
sensor and allowing for stereo depth estimation. We provide a SHEF dataset
targeted at evaluating disparity estimation algorithms and introduce a stereo
disparity estimation algorithm that uses edge information extracted from the
event stream correlated with the edge detected in the frame data. Our disparity
estimation outperforms the state-of-the-art stereo matching algorithm on the
SHEF dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Synthesis of Diverse Weak Supervision Sources for Behavior Analysis. (arXiv:2111.15186v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.15186">
<div class="article-summary-box-inner">
<span><p>Obtaining annotations for large training sets is expensive, especially in
settings where domain knowledge is required, such as behavior analysis. Weak
supervision has been studied to reduce annotation costs by using weak labels
from task-specific labeling functions (LFs) to augment ground truth labels.
However, domain experts still need to hand-craft different LFs for different
tasks, limiting scalability. To reduce expert effort, we present AutoSWAP: a
framework for automatically synthesizing data-efficient task-level LFs. The key
to our approach is to efficiently represent expert knowledge in a reusable
domain-specific language and more general domain-level LFs, with which we use
state-of-the-art program synthesis techniques and a small labeled dataset to
generate task-level LFs. Additionally, we propose a novel structural diversity
cost that allows for efficient synthesis of diverse sets of LFs, further
improving AutoSWAP's performance. We evaluate AutoSWAP in three behavior
analysis domains and demonstrate that AutoSWAP outperforms existing approaches
using only a fraction of the data. Our results suggest that AutoSWAP is an
effective way to automatically generate LFs that can significantly reduce
expert effort for behavior analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Distance Estimation for Wildlife Camera Trapping. (arXiv:2202.04613v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04613">
<div class="article-summary-box-inner">
<span><p>The ongoing biodiversity crisis calls for accurate estimation of animal
density and abundance to identify sources of biodiversity decline and
effectiveness of conservation interventions. Camera traps together with
abundance estimation methods are often employed for this purpose. The necessary
distances between camera and observed animals are traditionally derived in a
laborious, fully manual or semi-automatic process. Both approaches require
reference image material, which is both difficult to acquire and not available
for existing datasets. We propose a fully automatic approach we call AUtomated
DIstance esTimation (AUDIT) to estimate camera-to-animal distances. We leverage
existing state-of-the-art relative monocular depth estimation and combine it
with a novel alignment procedure to estimate metric distances. AUDIT is fully
automated and requires neither the comparison of observations in camera trap
imagery with reference images nor capturing of reference image material at all.
AUDIT therefore relieves biologists and ecologists from a significant workload.
We evaluate AUDIT on a zoo scenario dataset unseen during training where we
achieve a mean absolute distance estimation error over all animal instances of
only 0.9864 meters and mean relative error (REL) of 0.113. The code and usage
instructions are available at
https://github.com/PJ-cs/DistanceEstimationTracking
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Do Vision Transformers Work?. (arXiv:2202.06709v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06709">
<div class="article-summary-box-inner">
<span><p>The success of multi-head self-attentions (MSAs) for computer vision is now
indisputable. However, little is known about how MSAs work. We present
fundamental explanations to help better understand the nature of MSAs. In
particular, we demonstrate the following properties of MSAs and Vision
Transformers (ViTs): (1) MSAs improve not only accuracy but also generalization
by flattening the loss landscapes. Such improvement is primarily attributable
to their data specificity, not long-range dependency. On the other hand, ViTs
suffer from non-convex losses. Large datasets and loss landscape smoothing
methods alleviate this problem; (2) MSAs and Convs exhibit opposite behaviors.
For example, MSAs are low-pass filters, but Convs are high-pass filters.
Therefore, MSAs and Convs are complementary; (3) Multi-stage neural networks
behave like a series connection of small individual models. In addition, MSAs
at the end of a stage play a key role in prediction. Based on these insights,
we propose AlterNet, a model in which Conv blocks at the end of a stage are
replaced with MSA blocks. AlterNet outperforms CNNs not only in large data
regimes but also in small data regimes. The code is available at
https://github.com/xxxnell/how-do-vits-work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Developing Imperceptible Adversarial Patches to Camouflage Military Assets From Computer Vision Enabled Technologies. (arXiv:2202.08892v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08892">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks (CNNs) have demonstrated rapid progress and a
high level of success in object detection. However, recent evidence has
highlighted their vulnerability to adversarial attacks. These attacks are
calculated image perturbations or adversarial patches that result in object
misclassification or detection suppression. Traditional camouflage methods are
impractical when applied to disguise aircraft and other large mobile assets
from autonomous detection in intelligence, surveillance and reconnaissance
technologies and fifth generation missiles. In this paper we present a unique
method that produces imperceptible patches capable of camouflaging large
military assets from computer vision-enabled technologies. We developed these
patches by maximising object detection loss whilst limiting the patch's colour
perceptibility. This work also aims to further the understanding of adversarial
examples and their effects on object detection algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multimodal German Dataset for Automatic Lip Reading Systems and Transfer Learning. (arXiv:2202.13403v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13403">
<div class="article-summary-box-inner">
<span><p>Large datasets as required for deep learning of lip reading do not exist in
many languages. In this paper we present the dataset GLips (German Lips)
consisting of 250,000 publicly available videos of the faces of speakers of the
Hessian Parliament, which was processed for word-level lip reading using an
automatic pipeline. The format is similar to that of the English language LRW
(Lip Reading in the Wild) dataset, with each video encoding one word of
interest in a context of 1.16 seconds duration, which yields compatibility for
studying transfer learning between both datasets. By training a deep neural
network, we investigate whether lip reading has language-independent features,
so that datasets of different languages can be used to improve lip reading
models. We demonstrate learning from scratch and show that transfer learning
from LRW to GLips and vice versa improves learning speed and performance, in
particular for the validation set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Autoencoder Attractors for Uncertainty Estimation. (arXiv:2204.00382v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00382">
<div class="article-summary-box-inner">
<span><p>The reliability assessment of a machine learning model's prediction is an
important quantity for the deployment in safety critical applications. Not only
can it be used to detect novel sceneries, either as out-of-distribution or
anomaly sample, but it also helps to determine deficiencies in the training
data distribution. A lot of promising research directions have either proposed
traditional methods like Gaussian processes or extended deep learning based
approaches, for example, by interpreting them from a Bayesian point of view. In
this work we propose a novel approach for uncertainty estimation based on
autoencoder models: The recursive application of a previously trained
autoencoder model can be interpreted as a dynamical system storing training
examples as attractors. While input images close to known samples will converge
to the same or similar attractor, input samples containing unknown features are
unstable and converge to different training samples by potentially removing or
changing characteristic features. The use of dropout during training and
inference leads to a family of similar dynamical systems, each one being robust
on samples close to the training distribution but unstable on new features.
Either the model reliably removes these features or the resulting instability
can be exploited to detect problematic input samples. We evaluate our approach
on several dataset combinations as well as on an industrial application for
occupant classification in the vehicle interior for which we additionally
release a new synthetic dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sardino: Ultra-Fast Dynamic Ensemble for Secure Visual Sensing at Mobile Edge. (arXiv:2204.08189v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08189">
<div class="article-summary-box-inner">
<span><p>Adversarial example attack endangers the mobile edge systems such as vehicles
and drones that adopt deep neural networks for visual sensing. This paper
presents {\em Sardino}, an active and dynamic defense approach that renews the
inference ensemble at run time to develop security against the adaptive
adversary who tries to exfiltrate the ensemble and construct the corresponding
effective adversarial examples. By applying consistency check and data fusion
on the ensemble's predictions, Sardino can detect and thwart adversarial
inputs. Compared with the training-based ensemble renewal, we use HyperNet to
achieve {\em one million times} acceleration and per-frame ensemble renewal
that presents the highest level of difficulty to the prerequisite exfiltration
attacks. We design a run-time planner that maximizes the ensemble size in favor
of security while maintaining the processing frame rate. Beyond adversarial
examples, Sardino can also address the issue of out-of-distribution inputs
effectively. This paper presents extensive evaluation of Sardino's performance
in counteracting adversarial examples and applies it to build a real-time
car-borne traffic sign recognition system. Live on-road tests show the built
system's effectiveness in maintaining frame rate and detecting
out-of-distribution inputs due to the false positives of a preceding YOLO-based
traffic sign detector.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The 6th AI City Challenge. (arXiv:2204.10380v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10380">
<div class="article-summary-box-inner">
<span><p>The 6th edition of the AI City Challenge specifically focuses on problems in
two domains where there is tremendous unlocked potential at the intersection of
computer vision and artificial intelligence: Intelligent Traffic Systems (ITS),
and brick and mortar retail businesses. The four challenge tracks of the 2022
AI City Challenge received participation requests from 254 teams across 27
countries. Track 1 addressed city-scale multi-target multi-camera (MTMC)
vehicle tracking. Track 2 addressed natural-language-based vehicle track
retrieval. Track 3 was a brand new track for naturalistic driving analysis,
where the data were captured by several cameras mounted inside the vehicle
focusing on driver safety, and the task was to classify driver actions. Track 4
was another new track aiming to achieve retail store automated checkout using
only a single view camera. We released two leader boards for submissions based
on different methods, including a public leader board for the contest, where no
use of external data is allowed, and a general leader board for all submitted
results. The top performance of participating teams established strong
baselines and even outperformed the state-of-the-art in the proposed challenge
tracks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PolyLoss: A Polynomial Expansion Perspective of Classification Loss Functions. (arXiv:2204.12511v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12511">
<div class="article-summary-box-inner">
<span><p>Cross-entropy loss and focal loss are the most common choices when training
deep neural networks for classification problems. Generally speaking, however,
a good loss function can take on much more flexible forms, and should be
tailored for different tasks and datasets. Motivated by how functions can be
approximated via Taylor expansion, we propose a simple framework, named
PolyLoss, to view and design loss functions as a linear combination of
polynomial functions. Our PolyLoss allows the importance of different
polynomial bases to be easily adjusted depending on the targeting tasks and
datasets, while naturally subsuming the aforementioned cross-entropy loss and
focal loss as special cases. Extensive experimental results show that the
optimal choice within the PolyLoss is indeed dependent on the task and dataset.
Simply by introducing one extra hyperparameter and adding one line of code, our
Poly-1 formulation outperforms the cross-entropy loss and focal loss on 2D
image classification, instance segmentation, object detection, and 3D object
detection tasks, sometimes by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pik-Fix: Restoring and Colorizing Old Photos. (arXiv:2205.01902v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01902">
<div class="article-summary-box-inner">
<span><p>Restoring and inpainting the visual memories that are present, but often
impaired, in old photos remains an intriguing but unsolved research topic.
Decades-old photos often suffer from severe and commingled degradation such as
cracks, defocus, and color-fading, which are difficult to treat individually
and harder to repair when they interact. Deep learning presents a plausible
avenue, but the lack of large-scale datasets of old photos makes addressing
this restoration task very challenging. Here we present a novel reference-based
end-to-end learning framework that is able to both repair and colorize old and
degraded pictures. Our proposed framework consists of three modules: a
restoration sub-network that conducts restoration from degradations, a
similarity sub-network that performs color histogram matching and color
transfer, and a colorization subnet that learns to predict the chroma elements
of images that have been conditioned on chromatic reference signals. The
overall system makes use of color histogram priors from reference images, which
greatly reduces the need for large-scale training data. We have also created a
first-of-a-kind public dataset of real old photos that are paired with ground
truth "pristine" photos that have been that have been manually restored by
PhotoShop experts. We conducted extensive experiments on this dataset and
synthetic datasets, and found that our method significantly outperforms
previous state-of-the-art models using both qualitative comparisons and
quantitative measurements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Iterative Geometry-Aware Cross Guidance Network for Stereo Image Inpainting. (arXiv:2205.03825v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03825">
<div class="article-summary-box-inner">
<span><p>Currently, single image inpainting has achieved promising results based on
deep convolutional neural networks. However, inpainting on stereo images with
missing regions has not been explored thoroughly, which is also a significant
but different problem. One crucial requirement for stereo image inpainting is
stereo consistency. To achieve it, we propose an Iterative Geometry-Aware Cross
Guidance Network (IGGNet). The IGGNet contains two key ingredients, i.e., a
Geometry-Aware Attention (GAA) module and an Iterative Cross Guidance (ICG)
strategy. The GAA module relies on the epipolar geometry cues and learns the
geometry-aware guidance from one view to another, which is beneficial to make
the corresponding regions in two views consistent. However, learning guidance
from co-existing missing regions is challenging. To address this issue, the ICG
strategy is proposed, which can alternately narrow down the missing regions of
the two views in an iterative manner. Experimental results demonstrate that our
proposed network outperforms the latest stereo image inpainting model and
state-of-the-art single image inpainting models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved Evaluation and Generation of Grid Layouts using Distance Preservation Quality and Linear Assignment Sorting. (arXiv:2205.04255v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04255">
<div class="article-summary-box-inner">
<span><p>Images sorted by similarity enables more images to be viewed simultaneously,
and can be very useful for stock photo agencies or e-commerce applications.
Visually sorted grid layouts attempt to arrange images so that their proximity
on the grid corresponds as closely as possible to their similarity. Various
metrics exist for evaluating such arrangements, but there is low experimental
evidence on correlation between human perceived quality and metric value. We
propose Distance Preservation Quality (DPQ) as a new metric to evaluate the
quality of an arrangement. Extensive user testing revealed stronger correlation
of DPQ with user-perceived quality and performance in image retrieval tasks
compared to other metrics. In addition, we introduce Fast Linear Assignment
Sorting (FLAS) as a new algorithm for creating visually sorted grid layouts.
FLAS achieves very good sorting qualities while improving run time and
computational resources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Frequency Attention to Make Adversarial Patch Powerful Against Person Detector. (arXiv:2205.04638v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04638">
<div class="article-summary-box-inner">
<span><p>Deep neural networks (DNNs) are vulnerable to adversarial attacks. In
particular, object detectors may be attacked by applying a particular
adversarial patch to the image. However, because the patch shrinks during
preprocessing, most existing approaches that employ adversarial patches to
attack object detectors would diminish the attack success rate on small and
medium targets. This paper proposes a Frequency Module(FRAN), a
frequency-domain attention module for guiding patch generation. This is the
first study to introduce frequency domain attention to optimize the attack
capabilities of adversarial patches. Our method increases the attack success
rates of small and medium targets by 4.18% and 3.89%, respectively, over the
state-of-the-art attack method for fooling the human detector while assaulting
YOLOv3 without reducing the attack success rate of big targets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">STDC-MA Network for Semantic Segmentation. (arXiv:2205.04639v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04639">
<div class="article-summary-box-inner">
<span><p>Semantic segmentation is applied extensively in autonomous driving and
intelligent transportation with methods that highly demand spatial and semantic
information. Here, an STDC-MA network is proposed to meet these demands. First,
the STDC-Seg structure is employed in STDC-MA to ensure a lightweight and
efficient structure. Subsequently, the feature alignment module (FAM) is
applied to understand the offset between high-level and low-level features,
solving the problem of pixel offset related to upsampling on the high-level
feature map. Our approach implements the effective fusion between high-level
features and low-level features. A hierarchical multiscale attention mechanism
is adopted to reveal the relationship among attention regions from two
different input sizes of one image. Through this relationship, regions
receiving much attention are integrated into the segmentation results, thereby
reducing the unfocused regions of the input image and improving the effective
utilization of multiscale features. STDC- MA maintains the segmentation speed
as an STDC-Seg network while improving the segmentation accuracy of small
objects. STDC-MA was verified on the verification set of Cityscapes. The
segmentation result of STDC-MA attained 76.81% mIOU with the input of 0.5x
scale, 3.61% higher than STDC-Seg.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OTFPF: Optimal Transport-Based Feature Pyramid Fusion Network for Brain Age Estimation with 3D Overlapped ConvNeXt. (arXiv:2205.04684v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04684">
<div class="article-summary-box-inner">
<span><p>Chronological age of healthy brain is able to be predicted using deep neural
networks from T1-weighted magnetic resonance images (T1 MRIs), and the
predicted brain age could serve as an effective biomarker for detecting
aging-related diseases or disorders. In this paper, we propose an end-to-end
neural network architecture, referred to as optimal transport based feature
pyramid fusion (OTFPF) network, for the brain age estimation with T1 MRIs. The
OTFPF consists of three types of modules: Optimal Transport based Feature
Pyramid Fusion (OTFPF) module, 3D overlapped ConvNeXt (3D OL-ConvNeXt) module
and fusion module. These modules strengthen the OTFPF network's understanding
of each brain's semi-multimodal and multi-level feature pyramid information,
and significantly improve its estimation performances. Comparing with recent
state-of-the-art models, the proposed OTFPF converges faster and performs
better. The experiments with 11,728 MRIs aged 3-97 years show that OTFPF
network could provide accurate brain age estimation, yielding mean absolute
error (MAE) of 2.097, Pearson's correlation coefficient (PCC) of 0.993 and
Spearman's rank correlation coefficient (SRCC) of 0.989, between the estimated
and chronological ages. Widespread quantitative experiments and ablation
experiments demonstrate the superiority and rationality of OTFPF network. The
codes and implement details will be released on GitHub:
https://github.com/ZJU-Brain/OTFPF after final decision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Impact of Partial Occlusion on Pedestrian Detectability. (arXiv:2205.04812v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04812">
<div class="article-summary-box-inner">
<span><p>Robust detection of vulnerable road users is a safety critical requirement
for the deployment of autonomous vehicles in heterogeneous traffic. One of the
most complex outstanding challenges is that of partial occlusion where a target
object is only partially available to the sensor due to obstruction by another
foreground object. A number of leading pedestrian detection benchmarks provide
annotation for partial occlusion, however each benchmark varies greatly in
their definition of the occurrence and severity of occlusion. Recent research
demonstrates that a high degree of subjectivity is used to classify occlusion
level in these cases and occlusion is typically categorized into 2 to 3 broad
categories such as partially and heavily occluded. This can lead to inaccurate
or inconsistent reporting of pedestrian detection model performance depending
on which benchmark is used. This research introduces a novel, objective
benchmark for partially occluded pedestrian detection to facilitate the
objective characterization of pedestrian detection models. Characterization is
carried out on seven popular pedestrian detection models for a range of
occlusion levels from 0-99%. Results demonstrate that pedestrian detection
performance degrades, and the number of false negative detections increase as
pedestrian occlusion level increases. Of the seven popular pedestrian detection
routines characterized, CenterNet has the greatest overall performance,
followed by SSDlite. RetinaNet has the lowest overall detection performance
across the range of occlusion levels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Answer Visual Questions from Web Videos. (arXiv:2205.05019v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05019">
<div class="article-summary-box-inner">
<span><p>Recent methods for visual question answering rely on large-scale annotated
datasets. Manual annotation of questions and answers for videos, however, is
tedious, expensive and prevents scalability. In this work, we propose to avoid
manual annotation and generate a large-scale training dataset for video
question answering making use of automatic cross-modal supervision. We leverage
a question generation transformer trained on text data and use it to generate
question-answer pairs from transcribed video narrations. Given narrated videos,
we then automatically generate the HowToVQA69M dataset with 69M
video-question-answer triplets. To handle the open vocabulary of diverse
answers in this dataset, we propose a training procedure based on a contrastive
loss between a video-question multi-modal transformer and an answer
transformer. We introduce the zero-shot VideoQA task and the VideoQA feature
probe evaluation setting and show excellent results, in particular for rare
answers. Furthermore, our method achieves competitive results on MSRVTT-QA,
ActivityNet-QA, MSVD-QA and How2QA datasets. We also show that our VideoQA
dataset generation approach generalizes to another source of web video and text
data. We use our method to generate the WebVidVQA3M dataset from the WebVid
dataset, i.e., videos with alt-text annotations, and show its benefits for
training VideoQA models. Finally, for a detailed evaluation we introduce iVQA,
a new VideoQA dataset with reduced language bias and high-quality manual
annotations. Code, datasets and trained models are available at
https://antoyang.github.io/just-ask.html
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-05-12 23:09:16.764810935 UTC">2022-05-12 23:09:16 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>