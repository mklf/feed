<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-05-05T01:30:00Z">05-05</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving In-Context Few-Shot Learning via Self-Supervised Training. (arXiv:2205.01703v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01703">
<div class="article-summary-box-inner">
<span><p>Self-supervised pretraining has made few-shot learning possible for many NLP
tasks. But the pretraining objectives are not typically adapted specifically
for in-context few-shot learning. In this paper, we propose to use
self-supervision in an intermediate training stage between pretraining and
downstream few-shot usage with the goal to teach the model to perform
in-context few shot learning. We propose and evaluate four self-supervised
objectives on two benchmarks. We find that the intermediate self-supervision
stage produces models that outperform strong baselines. Ablation study shows
that several factors affect the downstream performance, such as the amount of
training data and the diversity of the self-supervised objectives.
Human-annotated cross-task supervision and self-supervision are complementary.
Qualitative analysis suggests that the self-supervised-trained models are
better at following task requirements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Don't sweat the small stuff, classify the rest: Sample Shielding to protect text classifiers against adversarial attacks. (arXiv:2205.01714v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01714">
<div class="article-summary-box-inner">
<span><p>Deep learning (DL) is being used extensively for text classification.
However, researchers have demonstrated the vulnerability of such classifiers to
adversarial attacks. Attackers modify the text in a way which misleads the
classifier while keeping the original meaning close to intact. State-of-the-art
(SOTA) attack algorithms follow the general principle of making minimal changes
to the text so as to not jeopardize semantics. Taking advantage of this we
propose a novel and intuitive defense strategy called Sample Shielding. It is
attacker and classifier agnostic, does not require any reconfiguration of the
classifier or external resources and is simple to implement. Essentially, we
sample subsets of the input text, classify them and summarize these into a
final decision. We shield three popular DL text classifiers with Sample
Shielding, test their resilience against four SOTA attackers across three
datasets in a realistic threat setting. Even when given the advantage of
knowing about our shielding strategy the adversary's attack success rate is
&lt;=10% with only one exception and often &lt; 5%. Additionally, Sample Shielding
maintains near original accuracy when applied to original texts. Crucially, we
show that the `make minimal changes' approach of SOTA attackers leads to
critical vulnerabilities that can be defended against with an intuitive
sampling strategy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quiz Design Task: Helping Teachers Create Quizzes with Automated Question Generation. (arXiv:2205.01730v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01730">
<div class="article-summary-box-inner">
<span><p>Question generation (QGen) models are often evaluated with standardized NLG
metrics that are based on n-gram overlap. In this paper, we measure whether
these metric improvements translate to gains in a practical setting, focusing
on the use case of helping teachers automate the generation of reading
comprehension quizzes. In our study, teachers building a quiz receive question
suggestions, which they can either accept or refuse with a reason. Even though
we find that recent progress in QGen leads to a significant increase in
question acceptance rates, there is still large room for improvement, with the
best model having only 68.4% of its questions accepted by the ten teachers who
participated in our study. We then leverage the annotations we collected to
analyze standard NLG metrics and find that model performance has reached
projected upper-bounds, suggesting new automatic metrics are needed to guide
QGen research forward.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Themes of Revenge: Automatic Identification of Vengeful Content in Textual Data. (arXiv:2205.01731v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01731">
<div class="article-summary-box-inner">
<span><p>Revenge is a powerful motivating force reported to underlie the behavior of
various solo perpetrators, from school shooters to right wing terrorists. In
this paper, we develop an automated methodology for identifying vengeful themes
in textual data. Testing the model on four datasets (vengeful texts from social
media, school shooters, Right Wing terrorist and Islamic terrorists), we
present promising results, even when the methodology is tested on extremely
imbalanced datasets. The paper not only presents a simple and powerful
methodology that may be used for the screening of solo perpetrators but also
validate the simple theoretical model of revenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mixed-effects transformers for hierarchical adaptation. (arXiv:2205.01749v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01749">
<div class="article-summary-box-inner">
<span><p>Language use differs dramatically from context to context. To some degree,
modern language models like GPT-3 are able to account for such variance by
conditioning on a string of previous input text, or prompt. Yet prompting is
ineffective when contexts are sparse, out-of-sample, or extra-textual; for
instance, accounting for when and where the text was produced or who produced
it. In this paper, we introduce the mixed-effects transformer (MET), a novel
approach for learning hierarchically-structured prefixes -- lightweight modules
prepended to the input -- to account for structured variation. Specifically, we
show how the popular class of mixed-effects models may be extended to
transformer-based architectures using a regularized prefix-tuning procedure
with dropout. We evaluate this approach on several domain-adaptation
benchmarks, finding that it efficiently adapts to novel contexts with minimal
data while still effectively generalizing to unseen contexts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On monoaural speech enhancement for automatic recognition of real noisy speech using mixture invariant training. (arXiv:2205.01751v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01751">
<div class="article-summary-box-inner">
<span><p>In this paper, we explore an improved framework to train a monoaural neural
enhancement model for robust speech recognition. The designed training
framework extends the existing mixture invariant training criterion to exploit
both unpaired clean speech and real noisy data. It is found that the unpaired
clean speech is crucial to improve quality of separated speech from real noisy
speech. The proposed method also performs remixing of processed and unprocessed
signals to alleviate the processing artifacts. Experiments on the
single-channel CHiME-3 real test sets show that the proposed method improves
significantly in terms of speech recognition performance over the enhancement
system trained either on the mismatched simulated data in a supervised fashion
or on the matched real data in an unsupervised fashion. Between 16% and 39%
relative WER reduction has been achieved by the proposed system compared to the
unprocessed signal using end-to-end and hybrid acoustic models without
retraining on distorted data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">XLTime: A Cross-Lingual Knowledge Transfer Framework for Temporal Expression Extraction. (arXiv:2205.01757v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01757">
<div class="article-summary-box-inner">
<span><p>Temporal Expression Extraction (TEE) is essential for understanding time in
natural language. It has applications in Natural Language Processing (NLP)
tasks such as question answering, information retrieval, and causal inference.
To date, work in this area has mostly focused on English as there is a scarcity
of labeled data for other languages. We propose XLTime, a novel framework for
multilingual TEE. XLTime works on top of pre-trained language models and
leverages multi-task learning to prompt cross-language knowledge transfer both
from English and within the non-English languages. XLTime alleviates problems
caused by a shortage of data in the target language. We apply XLTime with
different language models and show that it outperforms the previous automatic
SOTA methods on French, Spanish, Portuguese, and Basque, by large margins.
XLTime also closes the gap considerably on the handcrafted HeidelTime method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explain and Conquer: Personalised Text-based Reviews to Achieve Transparency. (arXiv:2205.01759v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01759">
<div class="article-summary-box-inner">
<span><p>There are many contexts where dyadic data is present. Social networking is a
well-known example, where transparency has grown on importance. In these
contexts, pairs of items are linked building a network where interactions play
a crucial role. Explaining why these relationships are established is core to
address transparency. These explanations are often presented using text, thanks
to the spread of the natural language understanding tasks.
</p>
<p>We have focused on the TripAdvisor platform, considering the applicability to
other dyadic data contexts. The items are a subset of users and restaurants and
the interactions the reviews posted by these users. Our aim is to represent and
explain pairs (user, restaurant) established by agents (e.g., a recommender
system or a paid promotion mechanism), so that personalisation is taken into
account. We propose the PTER (Personalised TExt-based Reviews) model. We
predict, from the available reviews for a given restaurant, those that fit to
the specific user interactions.
</p>
<p>PTER leverages the BERT (Bidirectional Encoders Representations from
Transformers) language model. We customised a deep neural network following the
feature-based approach. The performance metrics show the validity of our
labelling proposal. We defined an evaluation framework based on a clustering
process to assess our personalised representation. PTER clearly outperforms the
proposed adversary in 5 of the 6 datasets, with a minimum ratio improvement of
4%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scientific Explanation and Natural Language: A Unified Epistemological-Linguistic Perspective for Explainable AI. (arXiv:2205.01809v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01809">
<div class="article-summary-box-inner">
<span><p>A fundamental research goal for Explainable AI (XAI) is to build models that
are capable of reasoning through the generation of natural language
explanations. However, the methodologies to design and evaluate
explanation-based inference models are still poorly informed by theoretical
accounts on the nature of explanation. As an attempt to provide an
epistemologically grounded characterisation for XAI, this paper focuses on the
scientific domain, aiming to bridge the gap between theory and practice on the
notion of a scientific explanation. Specifically, the paper combines a detailed
survey of the modern accounts of scientific explanation in Philosophy of
Science with a systematic analysis of corpora of natural language explanations,
clarifying the nature and function of explanatory arguments from both a
top-down (categorical) and a bottom-up (corpus-based) perspective. Through a
mixture of quantitative and qualitative methodologies, the presented study
allows deriving the following main conclusions: (1) Explanations cannot be
entirely characterised in terms of inductive or deductive arguments as their
main function is to perform unification; (2) An explanation must cite causes
and mechanisms that are responsible for the occurrence of the event to be
explained; (3) While natural language explanations possess an intrinsic
causal-mechanistic nature, they are not limited to causes and mechanisms, also
accounting for pragmatic elements such as definitions, properties and taxonomic
relations (4) Patterns of unification naturally emerge in corpora of
explanations even if not intentionally modelled; (5) Unification is realised
through a process of abstraction, whose function is to provide the inference
substrate for subsuming the event to be explained under recurring patterns and
high-level regularities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Holistic Framework for Analyzing the COVID-19 Vaccine Debate. (arXiv:2205.01817v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01817">
<div class="article-summary-box-inner">
<span><p>The Covid-19 pandemic has led to infodemic of low quality information leading
to poor health decisions. Combating the outcomes of this infodemic is not only
a question of identifying false claims, but also reasoning about the decisions
individuals make. In this work we propose a holistic analysis framework
connecting stance and reason analysis, and fine-grained entity level moral
sentiment analysis. We study how to model the dependencies between the
different level of analysis and incorporate human insights into the learning
process. Experiments show that our framework provides reliable predictions even
in the low-supervision settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">i-Code: An Integrative and Composable Multimodal Learning Framework. (arXiv:2205.01818v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01818">
<div class="article-summary-box-inner">
<span><p>Human intelligence is multimodal; we integrate visual, linguistic, and
acoustic signals to maintain a holistic worldview. Most current pretraining
methods, however, are limited to one or two modalities. We present i-Code, a
self-supervised pretraining framework where users may flexibly combine the
modalities of vision, speech, and language into unified and general-purpose
vector representations. In this framework, data from each modality are first
given to pretrained single-modality encoders. The encoder outputs are then
integrated with a multimodal fusion network, which uses novel attention
mechanisms and other architectural innovations to effectively combine
information from the different modalities. The entire system is pretrained
end-to-end with new objectives including masked modality unit modeling and
cross-modality contrastive learning. Unlike previous research using only video
for pretraining, the i-Code framework can dynamically process single, dual, and
triple-modality data during training and inference, flexibly projecting
different combinations of modalities into a single representation space.
Experimental results demonstrate how i-Code can outperform state-of-the-art
techniques on five video understanding tasks and the GLUE NLP benchmark,
improving by as much as 11% and demonstrating the power of integrative
multimodal pretraining.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-shot Sonnet Generation with Discourse-level Planning and Aesthetics Features. (arXiv:2205.01821v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01821">
<div class="article-summary-box-inner">
<span><p>Poetry generation, and creative language generation in general, usually
suffers from the lack of large training data. In this paper, we present a novel
framework to generate sonnets that does not require training on poems. We
design a hierarchical framework which plans the poem sketch before decoding.
Specifically, a content planning module is trained on non-poetic texts to
obtain discourse-level coherence; then a rhyme module generates rhyme words and
a polishing module introduces imagery and similes for aesthetics purposes.
Finally, we design a constrained decoding algorithm to impose the
meter-and-rhyme constraint of the generated sonnets. Automatic and human
evaluation show that our multi-stage approach without training on poem corpora
generates more coherent, poetic, and creative sonnets than several strong
baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AmbiPun: Generating Humorous Puns with Ambiguous Context. (arXiv:2205.01825v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01825">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a simple yet effective way to generate pun
sentences that does not require any training on existing puns. Our approach is
inspired by humor theories that ambiguity comes from the context rather than
the pun word itself. Given a pair of definitions of a pun word, our model first
produces a list of related concepts through a reverse dictionary. We then
utilize one-shot GPT3 to generate context words and then generate puns
incorporating context words from both concepts. Human evaluation shows that our
method successfully generates pun 52\% of the time, outperforming well-crafted
baselines and the state-of-the-art models by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified Semantic Typing with Meaningful Label Inference. (arXiv:2205.01826v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01826">
<div class="article-summary-box-inner">
<span><p>Semantic typing aims at classifying tokens or spans of interest in a textual
context into semantic categories such as relations, entity types, and event
types. The inferred labels of semantic categories meaningfully interpret how
machines understand components of text. In this paper, we present UniST, a
unified framework for semantic typing that captures label semantics by
projecting both inputs and labels into a joint semantic embedding space. To
formulate different lexical and relational semantic typing tasks as a unified
task, we incorporate task descriptions to be jointly encoded with the input,
allowing UniST to be adapted to different tasks without introducing
task-specific model components. UniST optimizes a margin ranking loss such that
the semantic relatedness of the input and labels is reflected from their
embedding similarity. Our experiments demonstrate that UniST achieves strong
performance across three semantic typing tasks: entity typing, relation
classification and event typing. Meanwhile, UniST effectively transfers
semantic knowledge of labels and substantially improves generalizability on
inferring rarely seen and unseen types. In addition, multiple semantic typing
tasks can be jointly trained within the unified framework, leading to a single
compact multi-tasking model that performs comparably to dedicated single-task
models, while offering even better transferability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Great Truths are Always Simple: A Rather Simple Knowledge Encoder for Enhancing the Commonsense Reasoning Capacity of Pre-Trained Models. (arXiv:2205.01841v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01841">
<div class="article-summary-box-inner">
<span><p>Commonsense reasoning in natural language is a desired ability of artificial
intelligent systems. For solving complex commonsense reasoning tasks, a typical
solution is to enhance pre-trained language models~(PTMs) with a
knowledge-aware graph neural network~(GNN) encoder that models a commonsense
knowledge graph~(CSKG). Despite the effectiveness, these approaches are built
on heavy architectures, and can't clearly explain how external knowledge
resources improve the reasoning capacity of PTMs. Considering this issue, we
conduct a deep empirical analysis, and find that it is indeed relation features
from CSKGs (but not node features) that mainly contribute to the performance
improvement of PTMs. Based on this finding, we design a simple MLP-based
knowledge encoder that utilizes statistical relation paths as features.
Extensive experiments conducted on five benchmarks demonstrate the
effectiveness of our approach, which also largely reduces the parameters for
encoding CSKGs. Our codes and data are publicly available at
https://github.com/RUCAIBox/SAFE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Seed-Guided Topic Discovery with Out-of-Vocabulary Seeds. (arXiv:2205.01845v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01845">
<div class="article-summary-box-inner">
<span><p>Discovering latent topics from text corpora has been studied for decades.
Many existing topic models adopt a fully unsupervised setting, and their
discovered topics may not cater to users' particular interests due to their
inability of leveraging user guidance. Although there exist seed-guided topic
discovery approaches that leverage user-provided seeds to discover
topic-representative terms, they are less concerned with two factors: (1) the
existence of out-of-vocabulary seeds and (2) the power of pre-trained language
models (PLMs). In this paper, we generalize the task of seed-guided topic
discovery to allow out-of-vocabulary seeds. We propose a novel framework, named
SeeTopic, wherein the general knowledge of PLMs and the local semantics learned
from the input corpus can mutually benefit each other. Experiments on three
real datasets from different domains demonstrate the effectiveness of SeeTopic
in terms of topic coherence, accuracy, and diversity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Commonsense in Pretrained Unimodal and Multimodal Models. (arXiv:2205.01850v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01850">
<div class="article-summary-box-inner">
<span><p>Our commonsense knowledge about objects includes their typical visual
attributes; we know that bananas are typically yellow or green, and not purple.
Text and image corpora, being subject to reporting bias, represent this
world-knowledge to varying degrees of faithfulness. In this paper, we
investigate to what degree unimodal (language-only) and multimodal (image and
language) models capture a broad range of visually salient attributes. To that
end, we create the Visual Commonsense Tests (ViComTe) dataset covering 5
property types (color, shape, material, size, and visual co-occurrence) for
over 5000 subjects. We validate this dataset by showing that our grounded color
data correlates much better than ungrounded text-only data with crowdsourced
color judgments provided by Paik et al. (2021). We then use our dataset to
evaluate pretrained unimodal models and multimodal models. Our results indicate
that multimodal models better reconstruct attribute distributions, but are
still subject to reporting bias. Moreover, increasing model size does not
enhance performance, suggesting that the key to visual commonsense lies in the
data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Provably Confidential Language Modelling. (arXiv:2205.01863v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01863">
<div class="article-summary-box-inner">
<span><p>Large language models are shown to memorize privacy information such as
social security numbers in training data. Given the sheer scale of the training
corpus, it is challenging to screen and filter these privacy data, either
manually or automatically. In this paper, we propose Confidentially Redacted
Training (CRT), a method to train language generation models while protecting
the confidential segments. We borrow ideas from differential privacy (which
solves a related but distinct problem) and show that our method is able to
provably prevent unintended memorization by randomizing parts of the training
process. Moreover, we show that redaction with an approximately correct
screening policy amplifies the confidentiality guarantee. We implement the
method for both LSTM and GPT language models. Our experimental results show
that the models trained by CRT obtain almost the same perplexity while
preserving strong confidentiality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Entity Interactions for Few-Shot Relation Learning (Student Abstract). (arXiv:2205.01878v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01878">
<div class="article-summary-box-inner">
<span><p>Few-shot relation learning refers to infer facts for relations with a limited
number of observed triples. Existing metric-learning methods for this problem
mostly neglect entity interactions within and between triples. In this paper,
we explore this kind of fine-grained semantic meanings and propose our model
TransAM. Specifically, we serialize reference entities and query entities into
sequence and apply transformer structure with local-global attention to capture
both intra- and inter-triple entity interactions. Experiments on two public
benchmark datasets NELL-One and Wiki-One with 1-shot setting prove the
effectiveness of TransAM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">All You May Need for VQA are Image Captions. (arXiv:2205.01883v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01883">
<div class="article-summary-box-inner">
<span><p>Visual Question Answering (VQA) has benefited from increasingly sophisticated
models, but has not enjoyed the same level of engagement in terms of data
creation. In this paper, we propose a method that automatically derives VQA
examples at volume, by leveraging the abundance of existing image-caption
annotations combined with neural models for textual question generation. We
show that the resulting data is of high-quality. VQA models trained on our data
improve state-of-the-art zero-shot accuracy by double digits and achieve a
level of robustness that lacks in the same model trained on human-annotated VQA
data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">P$^3$ Ranker: Mitigating the Gaps between Pre-training and Ranking Fine-tuning with Prompt-based Learning and Pre-finetuning. (arXiv:2205.01886v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01886">
<div class="article-summary-box-inner">
<span><p>Compared to other language tasks, applying pre-trained language models (PLMs)
for search ranking often requires more nuances and training signals. In this
paper, we identify and study the two mismatches between pre-training and
ranking fine-tuning: the training schema gap regarding the differences in
training objectives and model architectures, and the task knowledge gap
considering the discrepancy between the knowledge needed in ranking and that
learned during pre-training. To mitigate these gaps, we propose Pre-trained,
Prompt-learned and Pre-finetuned Neural Ranker (P$^3$ Ranker). P$^3$ Ranker
leverages prompt-based learning to convert the ranking task into a pre-training
like schema and uses pre-finetuning to initialize the model on intermediate
supervised tasks. Experiments on MS MARCO and Robust04 show the superior
performances of P$^3$ Ranker in few-shot ranking. Analyses reveal that P$^3$
Ranker is able to better accustom to the ranking task through prompt-based
learning and retrieve necessary ranking-oriented knowledge gleaned in
pre-finetuning, resulting in data-efficient PLM adaptation. Our code is
available at \url{https://github.com/NEUIR/P3Ranker}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Multi-Document Summarization through Referenced Flexible Extraction with Credit-Awareness. (arXiv:2205.01889v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01889">
<div class="article-summary-box-inner">
<span><p>A notable challenge in Multi-Document Summarization (MDS) is the
extremely-long length of the input. In this paper, we present an
extract-then-abstract Transformer framework to overcome the problem.
Specifically, we leverage pre-trained language models to construct a
hierarchical extractor for salient sentence selection across documents and an
abstractor for rewriting the selected contents as summaries. However, learning
such a framework is challenging since the optimal contents for the abstractor
are generally unknown. Previous works typically create pseudo extraction oracle
to enable the supervised learning for both the extractor and the abstractor.
Nevertheless, we argue that the performance of such methods could be restricted
due to the insufficient information for prediction and inconsistent objectives
between training and testing. To this end, we propose a loss weighting
mechanism that makes the model aware of the unequal importance for the
sentences not in the pseudo extraction oracle, and leverage the fine-tuned
abstractor to generate summary references as auxiliary signals for learning the
extractor. Moreover, we propose a reinforcement learning method that can
efficiently apply to the extractor for harmonizing the optimization between
training and testing. Experiment results show that our framework substantially
outperforms strong baselines with comparable model sizes and achieves the best
results on the Multi-News, Multi-XScience, and WikiCatSum corpora.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Go Back in Time: Generating Flashbacks in Stories with Event Temporal Prompts. (arXiv:2205.01898v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01898">
<div class="article-summary-box-inner">
<span><p>Stories or narratives are comprised of a sequence of events. To compose
interesting stories, professional writers often leverage a creative writing
technique called flashback that inserts past events into current storylines as
we commonly observe in novels and plays. However, it is challenging for
machines to generate flashback as it requires a solid understanding of event
temporal order (e.g. "feeling hungry" before "eat," not vice versa), and the
creativity to arrange storylines so that earlier events do not always appear
first in narrative order. Two major issues in existing systems that exacerbate
the challenges: 1) temporal bias in pertaining and story datasets that leads to
monotonic event temporal orders; 2) lack of explicit guidance that helps
machines decide where to insert flashbacks. We propose to address these issues
using structured storylines to encode events and their pair-wise temporal
relations (before, after and vague) as temporal prompts that guide how stories
should unfold temporally. We leverage a Plan-and-Write framework enhanced by
reinforcement learning to generate storylines and stories end-to-end.
Evaluation results show that the proposed method can generate more interesting
stories with flashbacks while maintaining textual diversity, fluency, and
temporal coherence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-lingual Word Embeddings in Hyperbolic Space. (arXiv:2205.01907v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01907">
<div class="article-summary-box-inner">
<span><p>Cross-lingual word embeddings can be applied to several natural language
processing applications across multiple languages. Unlike prior works that use
word embeddings based on the Euclidean space, this short paper presents a
simple and effective cross-lingual Word2Vec model that adapts to the Poincar\'e
ball model of hyperbolic space to learn unsupervised cross-lingual word
representations from a German-English parallel corpus. It has been shown that
hyperbolic embeddings can capture and preserve hierarchical relationships. We
evaluate the model on both hypernymy and analogy tasks. The proposed model
achieves comparable performance with the vanilla Word2Vec model on the
cross-lingual analogy task, the hypernymy task shows that the cross-lingual
Poincar\'e Word2Vec model can capture latent hierarchical structure from free
text across languages, which are absent from the Euclidean-based Word2Vec
representations. Our results show that by preserving the latent hierarchical
information, hyperbolic spaces can offer better representations for
cross-lingual embeddings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Task Interactions in Document-Level Joint Entity and Relation Extraction. (arXiv:2205.01909v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01909">
<div class="article-summary-box-inner">
<span><p>We target on the document-level relation extraction in an end-to-end setting,
where the model needs to jointly perform mention extraction, coreference
resolution (COREF) and relation extraction (RE) at once, and gets evaluated in
an entity-centric way. Especially, we address the two-way interaction between
COREF and RE that has not been the focus by previous work, and propose to
introduce explicit interaction namely Graph Compatibility (GC) that is
specifically designed to leverage task characteristics, bridging decisions of
two tasks for direct task interference. Our experiments are conducted on DocRED
and DWIE; in addition to GC, we implement and compare different multi-task
settings commonly adopted in previous work, including pipeline, shared
encoders, graph propagation, to examine the effectiveness of different
interactions. The result shows that GC achieves the best performance by up to
2.3/5.1 F1 improvement over the baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lexical Knowledge Internalization for Neural Dialog Generation. (arXiv:2205.01941v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01941">
<div class="article-summary-box-inner">
<span><p>We propose knowledge internalization (KI), which aims to complement the
lexical knowledge into neural dialog models. Instead of further conditioning
the knowledge-grounded dialog (KGD) models on externally retrieved knowledge,
we seek to integrate knowledge about each input token internally into the
model's parameters. To tackle the challenge due to the large scale of lexical
knowledge, we adopt the contrastive learning approach and create an effective
token-level lexical knowledge retriever that requires only weak supervision
mined from Wikipedia. We demonstrate the effectiveness and general
applicability of our approach on various datasets and diversified model
structures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Word Tour: One-dimensional Word Embeddings via the Traveling Salesman Problem. (arXiv:2205.01954v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01954">
<div class="article-summary-box-inner">
<span><p>Word embeddings are one of the most fundamental technologies used in natural
language processing. Existing word embeddings are high-dimensional and consume
considerable computational resources. In this study, we propose WordTour,
unsupervised one-dimensional word embeddings. To achieve the challenging goal,
we propose a decomposition of the desiderata of word embeddings into two parts,
completeness and soundness, and focus on soundness in this paper. Owing to the
single dimensionality, WordTour is extremely efficient and provides a minimal
means to handle word embeddings. We experimentally confirmed the effectiveness
of the proposed method via user study and document classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-Autoregressive Machine Translation: It's Not as Fast as it Seems. (arXiv:2205.01966v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01966">
<div class="article-summary-box-inner">
<span><p>Efficient machine translation models are commercially important as they can
increase inference speeds, and reduce costs and carbon emissions. Recently,
there has been much interest in non-autoregressive (NAR) models, which promise
faster translation. In parallel to the research on NAR models, there have been
successful attempts to create optimized autoregressive models as part of the
WMT shared task on efficient translation. In this paper, we point out flaws in
the evaluation methodology present in the literature on NAR models and we
provide a fair comparison between a state-of-the-art NAR model and the
autoregressive submissions to the shared task. We make the case for consistent
evaluation of NAR models, and also for the importance of comparing NAR models
with other widely used methods for improving efficiency. We run experiments
with a connectionist-temporal-classification-based (CTC) NAR model implemented
in C++ and compare it with AR models using wall clock times. Our results show
that, although NAR models are faster on GPUs, with small batch sizes, they are
almost always slower under more realistic usage conditions. We call for more
realistic and extensive evaluation of NAR models in future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Aligning to Social Norms and Values in Interactive Narratives. (arXiv:2205.01975v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01975">
<div class="article-summary-box-inner">
<span><p>We focus on creating agents that act in alignment with socially beneficial
norms and values in interactive narratives or text-based games -- environments
wherein an agent perceives and interacts with a world through natural language.
Such interactive agents are often trained via reinforcement learning to
optimize task performance, even when such rewards may lead to agent behaviors
that violate societal norms -- causing harm either to the agent itself or other
entities in the environment. Social value alignment refers to creating agents
whose behaviors conform to expected moral and social norms for a given context
and group of people -- in our case, it means agents that behave in a manner
that is less harmful and more beneficial for themselves and others.
</p>
<p>We build on the Jiminy Cricket benchmark (Hendrycks et al. 2021), a set of 25
annotated interactive narratives containing thousands of morally salient
scenarios covering everything from theft and bodily harm to altruism. We
introduce the GALAD (Game-value ALignment through Action Distillation) agent
that uses the social commonsense knowledge present in specially trained
language models to contextually restrict its action space to only those actions
that are aligned with socially beneficial values. An experimental study shows
that the GALAD agent makes decisions efficiently enough to improve
state-of-the-art task performance by 4% while reducing the frequency of
socially harmful behaviors by 25% compared to strong contemporary value
alignment approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ON-TRAC Consortium Systems for the IWSLT 2022 Dialect and Low-resource Speech Translation Tasks. (arXiv:2205.01987v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01987">
<div class="article-summary-box-inner">
<span><p>This paper describes the ON-TRAC Consortium translation systems developed for
two challenge tracks featured in the Evaluation Campaign of IWSLT 2022:
low-resource and dialect speech translation. For the Tunisian Arabic-English
dataset (low-resource and dialect tracks), we build an end-to-end model as our
joint primary submission, and compare it against cascaded models that leverage
a large fine-tuned wav2vec 2.0 model for ASR. Our results show that in our
settings pipeline approaches are still very competitive, and that with the use
of transfer learning, they can outperform end-to-end models for speech
translation (ST). For the Tamasheq-French dataset (low-resource track) our
primary submission leverages intermediate representations from a wav2vec 2.0
model trained on 234 hours of Tamasheq audio, while our contrastive model uses
a French phonetic transcription of the Tamasheq audio as input in a Conformer
speech translation architecture jointly trained on automatic speech
recognition, ST and machine translation losses. Our results highlight that
self-supervised models trained on smaller sets of target data are more
effective to low-resource end-to-end ST fine-tuning, compared to large
off-the-shelf models. Results also illustrate that even approximate phonetic
transcriptions can improve ST scores.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MM-Claims: A Dataset for Multimodal Claim Detection in Social Media. (arXiv:2205.01989v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01989">
<div class="article-summary-box-inner">
<span><p>In recent years, the problem of misinformation on the web has become
widespread across languages, countries, and various social media platforms.
Although there has been much work on automated fake news detection, the role of
images and their variety are not well explored. In this paper, we investigate
the roles of image and text at an earlier stage of the fake news detection
pipeline, called claim detection. For this purpose, we introduce a novel
dataset, MM-Claims, which consists of tweets and corresponding images over
three topics: COVID-19, Climate Change and broadly Technology. The dataset
contains roughly 86000 tweets, out of which 3400 are labeled manually by
multiple annotators for the training and evaluation of multimodal models. We
describe the dataset in detail, evaluate strong unimodal and multimodal
baselines, and analyze the potential and drawbacks of current models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EmoBank: Studying the Impact of Annotation Perspective and Representation Format on Dimensional Emotion Analysis. (arXiv:2205.01996v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01996">
<div class="article-summary-box-inner">
<span><p>We describe EmoBank, a corpus of 10k English sentences balancing multiple
genres, which we annotated with dimensional emotion metadata in the
Valence-Arousal-Dominance (VAD) representation format. EmoBank excels with a
bi-perspectival and bi-representational design. On the one hand, we distinguish
between writer's and reader's emotions, on the other hand, a subset of the
corpus complements dimensional VAD annotations with categorical ones based on
Basic Emotions. We find evidence for the supremacy of the reader's perspective
in terms of IAA and rating intensity, and achieve close-to-human performance
when mapping between dimensional and categorical formats.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Design of a novel Korean learning application for efficient pronunciation correction. (arXiv:2205.02001v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02001">
<div class="article-summary-box-inner">
<span><p>The Korean wave, which denotes the global popularity of South Korea's
cultural economy, contributes to the increasing demand for the Korean language.
However, as there does not exist any application for foreigners to learn
Korean, this paper suggested a design of a novel Korean learning application.
Speech recognition, speech-to-text, and speech-to-waveform are the three key
systems in the proposed system. The Google API and the librosa library will
transform the user's voice into a sentence and MFCC. The software will then
display the user's phrase and answer, with mispronounced elements highlighted
in red, allowing users to more easily recognize the incorrect parts of their
pronunciation. Furthermore, the Siamese network might utilize those translated
spectrograms to provide a similarity score, which could subsequently be used to
offer feedback to the user. Despite the fact that we were unable to collect
sufficient foreigner data for this research, it is notable that we presented a
novel Korean pronunciation correction method for foreigners.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Framework to Generate High-Quality Datapoints for Multiple Novel Intent Detection. (arXiv:2205.02005v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02005">
<div class="article-summary-box-inner">
<span><p>Systems like Voice-command based conversational agents are characterized by a
pre-defined set of skills or intents to perform user specified tasks. In the
course of time, newer intents may emerge requiring retraining. However, the
newer intents may not be explicitly announced and need to be inferred
dynamically. Thus, there are two important tasks at hand (a). identifying
emerging new intents, (b). annotating data of the new intents so that the
underlying classifier can be retrained efficiently. The tasks become specially
challenging when a large number of new intents emerge simultaneously and there
is a limited budget of manual annotation. In this paper, we propose MNID
(Multiple Novel Intent Detection) which is a cluster based framework to detect
multiple novel intents with budgeted human annotation cost. Empirical results
on various benchmark datasets (of different sizes) demonstrate that MNID, by
intelligently using the budget for annotation, outperforms the baseline methods
in terms of accuracy and F1-score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Computational Inflection for Scientific Discovery. (arXiv:2205.02007v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02007">
<div class="article-summary-box-inner">
<span><p>We stand at the foot of a significant inflection in the trajectory of
scientific discovery. As society continues on its fast-paced digital
transformation, so does humankind's collective scientific knowledge and
discourse. We now read and write papers in digitized form, and a great deal of
the formal and informal processes of science are captured digitally --
including papers, preprints and books, code and datasets, conference
presentations, and interactions in social networks and communication platforms.
The transition has led to the growth of a tremendous amount of information,
opening exciting opportunities for computational models and systems that
analyze and harness it. In parallel, exponential growth in data processing
power has fueled remarkable advances in AI, including self-supervised neural
models capable of learning powerful representations from large-scale
unstructured text without costly human supervision. The confluence of societal
and computational trends suggests that computer science is poised to ignite a
revolution in the scientific process itself.
</p>
<p>However, the explosion of scientific data, results and publications stands in
stark contrast to the constancy of human cognitive capacity. While scientific
knowledge is expanding with rapidity, our minds have remained static, with
severe limitations on the capacity for finding, assimilating and manipulating
information. We propose a research agenda of task-guided knowledge retrieval,
in which systems counter humans' bounded capacity by ingesting corpora of
scientific knowledge and retrieving inspirations, explanations, solutions and
evidence synthesized to directly augment human performance on salient tasks in
scientific endeavors. We present initial progress on methods and prototypes,
and lay out important opportunities and challenges ahead with computational
approaches that have the potential to revolutionize science.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Continual Model Refinement in Out-of-Distribution Data Streams. (arXiv:2205.02014v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02014">
<div class="article-summary-box-inner">
<span><p>Real-world natural language processing (NLP) models need to be continually
updated to fix the prediction errors in out-of-distribution (OOD) data streams
while overcoming catastrophic forgetting. However, existing continual learning
(CL) problem setups cannot cover such a realistic and complex scenario. In
response to this, we propose a new CL problem formulation dubbed continual
model refinement (CMR). Compared to prior CL settings, CMR is more practical
and introduces unique challenges (boundary-agnostic and non-stationary
distribution shift, diverse mixtures of multiple OOD data clusters,
error-centric streams, etc.). We extend several existing CL approaches to the
CMR setting and evaluate them extensively. For benchmarking and analysis, we
propose a general sampling algorithm to obtain dynamic OOD data streams with
controllable non-stationarity, as well as a suite of metrics measuring various
aspects of online performance. Our experiments and detailed analysis reveal the
promise and challenges of the CMR problem, supporting that studying CMR in
dynamic OOD streams can benefit the longevity of deployed NLP models in
production.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Few Thousand Translations Go a Long Way! Leveraging Pre-trained Models for African News Translation. (arXiv:2205.02022v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02022">
<div class="article-summary-box-inner">
<span><p>Recent advances in the pre-training of language models leverage large-scale
datasets to create multilingual models. However, low-resource languages are
mostly left out in these datasets. This is primarily because many widely spoken
languages are not well represented on the web and therefore excluded from the
large-scale crawls used to create datasets. Furthermore, downstream users of
these models are restricted to the selection of languages originally chosen for
pre-training. This work investigates how to optimally leverage existing
pre-trained models to create low-resource translation systems for 16 African
languages. We focus on two questions: 1) How can pre-trained models be used for
languages not included in the initial pre-training? and 2) How can the
resulting translation models effectively transfer to new domains? To answer
these questions, we create a new African news corpus covering 16 languages, of
which eight languages are not part of any existing evaluation dataset. We
demonstrate that the most effective strategy for transferring both to
additional languages and to additional domains is to fine-tune large
pre-trained models on small quantities of high-quality translation data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Same Neurons, Different Languages: Probing Morphosyntax in Multilingual Pre-trained Models. (arXiv:2205.02023v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02023">
<div class="article-summary-box-inner">
<span><p>The success of multilingual pre-trained models is underpinned by their
ability to learn representations shared by multiple languages even in absence
of any explicit supervision. However, it remains unclear how these models learn
to generalise across languages. In this work, we conjecture that multilingual
pre-trained models can derive language-universal abstractions about grammar. In
particular, we investigate whether morphosyntactic information is encoded in
the same subset of neurons in different languages. We conduct the first
large-scale empirical study over 43 languages and 14 morphosyntactic categories
with a state-of-the-art neuron-level probe. Our findings show that the
cross-lingual overlap between neurons is significant, but its extent may vary
across categories and depends on language proximity and pre-training data size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CODE-MVP: Learning to Represent Source Code from Multiple Views with Contrastive Pre-Training. (arXiv:2205.02029v1 [cs.PL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02029">
<div class="article-summary-box-inner">
<span><p>Recent years have witnessed increasing interest in code representation
learning, which aims to represent the semantics of source code into distributed
vectors. Currently, various works have been proposed to represent the complex
semantics of source code from different views, including plain text, Abstract
Syntax Tree (AST), and several kinds of code graphs (e.g., Control/Data Flow
Graph). However, most of them only consider a single view of source code
independently, ignoring the correspondences among different views. In this
paper, we propose to integrate different views with the natural-language
description of source code into a unified framework with Multi-View contrastive
Pre-training, and name our model as CODE-MVP. Specifically, we first extract
multiple code views using compiler tools, and learn the complementary
information among them under a contrastive learning framework. Inspired by the
type checking in compilation, we also design a fine-grained type inference
objective in the pre-training. Experiments on three downstream tasks over five
datasets demonstrate the superiority of CODE-MVP when compared with several
state-of-the-art baselines. For example, we achieve 2.4/2.3/1.1 gain in terms
of MRR/MAP/Accuracy metrics on natural language code retrieval, code
similarity, and code defect detection tasks, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Masked Summarization to Generate Factually Inconsistent Summaries for Improved Factual Consistency Checking. (arXiv:2205.02035v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02035">
<div class="article-summary-box-inner">
<span><p>Despite the recent advances in abstractive summarization systems, it is still
difficult to determine whether a generated summary is factual consistent with
the source text. To this end, the latest approach is to train a factual
consistency classifier on factually consistent and inconsistent summaries.
Luckily, the former is readily available as reference summaries in existing
summarization datasets. However, generating the latter remains a challenge, as
they need to be factually inconsistent, yet closely relevant to the source text
to be effective. In this paper, we propose to generate factually inconsistent
summaries using source texts and reference summaries with key information
masked. Experiments on seven benchmark datasets demonstrate that factual
consistency classifiers trained on summaries generated using our method
generally outperform existing models and show a competitive correlation with
human judgments. We also analyze the characteristics of the summaries generated
using our method. We will release the pre-trained model and the code at
https://github.com/hwanheelee1993/MFMA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hyperbolic Relevance Matching for Neural Keyphrase Extraction. (arXiv:2205.02047v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02047">
<div class="article-summary-box-inner">
<span><p>Keyphrase extraction is a fundamental task in natural language processing and
information retrieval that aims to extract a set of phrases with important
information from a source document. Identifying important keyphrase is the
central component of the keyphrase extraction task, and its main challenge is
how to represent information comprehensively and discriminate importance
accurately. In this paper, to address these issues, we design a new hyperbolic
matching model (HyperMatch) to represent phrases and documents in the same
hyperbolic space and explicitly estimate the phrase-document relevance via the
Poincar\'e distance as the important score of each phrase. Specifically, to
capture the hierarchical syntactic and semantic structure information,
HyperMatch takes advantage of the hidden representations in multiple layers of
RoBERTa and integrates them as the word embeddings via an adaptive mixing
layer. Meanwhile, considering the hierarchical structure hidden in the
document, HyperMatch embeds both phrases and documents in the same hyperbolic
space via a hyperbolic phrase encoder and a hyperbolic document encoder. This
strategy can further enhance the estimation of phrase-document relevance due to
the good properties of hyperbolic space. In this setting, the keyphrase
extraction can be taken as a matching problem and effectively implemented by
minimizing a hyperbolic margin-based triplet loss. Extensive experiments are
conducted on six benchmarks and demonstrate that HyperMatch outperforms the
state-of-the-art baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-Shot Document-Level Relation Extraction. (arXiv:2205.02048v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02048">
<div class="article-summary-box-inner">
<span><p>We present FREDo, a few-shot document-level relation extraction (FSDLRE)
benchmark. As opposed to existing benchmarks which are built on sentence-level
relation extraction corpora, we argue that document-level corpora provide more
realism, particularly regarding none-of-the-above (NOTA) distributions.
Therefore, we propose a set of FSDLRE tasks and construct a benchmark based on
two existing supervised learning data sets, DocRED and sciERC. We adapt the
state-of-the-art sentence-level method MNAV to the document-level and develop
it further for improved domain adaptation. We find FSDLRE to be a challenging
setting with interesting new characteristics such as the ability to sample NOTA
instances from the support set. The data, code, and trained models are
available online (https://github.com/nicpopovic/FREDo).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring and Improving Compositional Generalization in Text-to-SQL via Component Alignment. (arXiv:2205.02054v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02054">
<div class="article-summary-box-inner">
<span><p>In text-to-SQL tasks -- as in much of NLP -- compositional generalization is
a major challenge: neural networks struggle with compositional generalization
where training and test distributions differ. However, most recent attempts to
improve this are based on word-level synthetic data or specific dataset splits
to generate compositional biases. In this work, we propose a clause-level
compositional example generation method. We first split the sentences in the
Spider text-to-SQL dataset into sub-sentences, annotating each sub-sentence
with its corresponding SQL clause, resulting in a new dataset Spider-SS. We
then construct a further dataset, Spider-CG, by composing Spider-SS
sub-sentences in different combinations, to test the ability of models to
generalize compositionally. Experiments show that existing models suffer
significant performance degradation when evaluated on Spider-CG, even though
every sub-sentence is seen during training. To deal with this problem, we
modify a number of state-of-the-art models to train on the segmented data of
Spider-SS, and we show that this method improves the generalization
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compositional Task-Oriented Parsing as Abstractive Question Answering. (arXiv:2205.02068v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02068">
<div class="article-summary-box-inner">
<span><p>Task-oriented parsing (TOP) aims to convert natural language into
machine-readable representations of specific tasks, such as setting an alarm. A
popular approach to TOP is to apply seq2seq models to generate linearized parse
trees. A more recent line of work argues that pretrained seq2seq models are
better at generating outputs that are themselves natural language, so they
replace linearized parse trees with canonical natural-language paraphrases that
can then be easily translated into parse trees, resulting in so-called
naturalized parsers. In this work we continue to explore naturalized semantic
parsing by presenting a general reduction of TOP to abstractive question
answering that overcomes some limitations of canonical paraphrasing.
Experimental results show that our QA-based technique outperforms
state-of-the-art methods in full-data settings while achieving dramatic
improvements in few-shot settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improve Discourse Dependency Parsing with Contextualized Representations. (arXiv:2205.02090v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02090">
<div class="article-summary-box-inner">
<span><p>Recent works show that discourse analysis benefits from modeling intra- and
inter-sentential levels separately, where proper representations for text units
of different granularities are desired to capture both the meaning of text
units and their relations to the context. In this paper, we propose to take
advantage of transformers to encode contextualized representations of units of
different levels to dynamically capture the information required for discourse
dependency analysis on intra- and inter-sentential levels. Motivated by the
observation of writing patterns commonly shared across articles, we propose a
novel method that treats discourse relation identification as a sequence
labelling task, which takes advantage of structural information from the
context of extracted discourse trees, and substantially outperforms traditional
direct-classification methods. Experiments show that our model achieves
state-of-the-art results on both English and Chinese datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are All the Datasets in Benchmark Necessary? A Pilot Study of Dataset Evaluation for Text Classification. (arXiv:2205.02129v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02129">
<div class="article-summary-box-inner">
<span><p>In this paper, we ask the research question of whether all the datasets in
the benchmark are necessary. We approach this by first characterizing the
distinguishability of datasets when comparing different systems. Experiments on
9 datasets and 36 systems show that several existing benchmark datasets
contribute little to discriminating top-scoring systems, while those less used
datasets exhibit impressive discriminative power. We further, taking the text
classification task as a case study, investigate the possibility of predicting
dataset discrimination based on its properties (e.g., average sentence length).
Our preliminary experiments promisingly show that given a sufficient number of
training experimental records, a meaningful predictor can be learned to
estimate dataset discrimination over unseen datasets. We released all datasets
with features explored in this work on DataLab:
\url{https://datalab.nlpedia.ai}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Limits of Word Level Differential Privacy. (arXiv:2205.02130v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02130">
<div class="article-summary-box-inner">
<span><p>As the issues of privacy and trust are receiving increasing attention within
the research community, various attempts have been made to anonymize textual
data. A significant subset of these approaches incorporate differentially
private mechanisms to perturb word embeddings, thus replacing individual words
in a sentence. While these methods represent very important contributions, have
various advantages over other techniques and do show anonymization
capabilities, they have several shortcomings. In this paper, we investigate
these weaknesses and demonstrate significant mathematical constraints
diminishing the theoretical privacy guarantee as well as major practical
shortcomings with regard to the protection against deanonymization attacks, the
preservation of content of the original sentences as well as the quality of the
language output. Finally, we propose a new method for text anonymization based
on transformer based language models fine-tuned for paraphrasing that
circumvents most of the identified weaknesses and also offers a formal privacy
guarantee. We evaluate the performance of our method via thorough
experimentation and demonstrate superior performance over the discussed
mechanisms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Granularity Semantic Aware Graph Model for Reducing Position Bias in Emotion-Cause Pair Extraction. (arXiv:2205.02132v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02132">
<div class="article-summary-box-inner">
<span><p>The Emotion-Cause Pair Extraction (ECPE) task aims to extract emotions and
causes as pairs from documents. We observe that the relative distance
distribution of emotions and causes is extremely imbalanced in the typical ECPE
dataset. Existing methods have set a fixed size window to capture relations
between neighboring clauses. However, they neglect the effective semantic
connections between distant clauses, leading to poor generalization ability
towards position-insensitive data. To alleviate the problem, we propose a novel
\textbf{M}ulti-\textbf{G}ranularity \textbf{S}emantic \textbf{A}ware
\textbf{G}raph model (MGSAG) to incorporate fine-grained and coarse-grained
semantic features jointly, without regard to distance limitation. In
particular, we first explore semantic dependencies between clauses and keywords
extracted from the document that convey fine-grained semantic features,
obtaining keywords enhanced clause representations. Besides, a clause graph is
also established to model coarse-grained semantic relations between clauses.
Experimental results indicate that MGSAG surpasses the existing
state-of-the-art ECPE models. Especially, MGSAG outperforms other models
significantly in the condition of position-insensitive data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Few-Shot Fine-Tuning for Opinion Summarization. (arXiv:2205.02170v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02170">
<div class="article-summary-box-inner">
<span><p>Abstractive summarization models are typically pre-trained on large amounts
of generic texts, then fine-tuned on tens or hundreds of thousands of annotated
samples. However, in opinion summarization, large annotated datasets of reviews
paired with reference summaries are not available and would be expensive to
create. This calls for fine-tuning methods robust to overfitting on small
datasets. In addition, generically pre-trained models are often not accustomed
to the specifics of customer reviews and, after fine-tuning, yield summaries
with disfluencies and semantic mistakes. To address these problems, we utilize
an efficient few-shot method based on adapters which, as we show, can easily
store in-domain knowledge. Instead of fine-tuning the entire model, we add
adapters and pre-train them in a task-specific way on a large corpus of
unannotated customer reviews, using held-out reviews as pseudo summaries. Then,
fine-tune the adapters on the small available human-annotated dataset. We show
that this self-supervised adapter pre-training improves summary quality over
standard fine-tuning by 2.0 and 1.3 ROUGE-L points on the Amazon and Yelp
datasets, respectively. Finally, for summary personalization, we condition on
aspect keyword queries, automatically created from generic datasets. In the
same vein, we pre-train the adapters in a query-based manner on customer
reviews and then fine-tune them on annotated datasets. This results in
better-organized summary content reflected in improved coherence and fewer
redundancies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using virtual edges to extract keywords from texts modeled as complex networks. (arXiv:2205.02172v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02172">
<div class="article-summary-box-inner">
<span><p>Detecting keywords in texts is important for many text mining applications.
Graph-based methods have been commonly used to automatically find the key
concepts in texts, however, relevant information provided by embeddings has not
been widely used to enrich the graph structure. Here we modeled texts
co-occurrence networks, where nodes are words and edges are established either
by contextual or semantical similarity. We compared two embedding approaches --
Word2vec and BERT -- to check whether edges created via word embeddings can
improve the quality of the keyword extraction method. We found that, in fact,
the use of virtual edges can improve the discriminability of co-occurrence
networks. The best performance was obtained when we considered low percentages
of addition of virtual (embedding) edges. A comparative analysis of structural
and dynamical network metrics revealed the degree, PageRank, and accessibility
are the metrics displaying the best performance in the model enriched with
virtual edges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reproducibility Beyond the Research Community: Experience from NLP Beginners. (arXiv:2205.02182v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02182">
<div class="article-summary-box-inner">
<span><p>As NLP research attracts public attention and excitement, it becomes
increasingly important for it to be accessible to a broad audience. As the
research community works to democratize NLP, it remains unclear whether
beginners to the field can easily apply the latest developments. To understand
their needs, we conducted a study with 93 students in an introductory NLP
course, where students reproduced results of recent NLP papers. Surprisingly,
our results suggest that their technical skill (i.e., programming experience)
has limited impact on their effort spent completing the exercise. Instead, we
find accessibility efforts by research authors to be key to a successful
experience, including thorough documentation and easy access to required models
and datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">User-Centric Gender Rewriting. (arXiv:2205.02211v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02211">
<div class="article-summary-box-inner">
<span><p>In this paper, we define the task of gender rewriting in contexts involving
two users (I and/or You) - first and second grammatical persons with
independent grammatical gender preferences. We focus on Arabic, a
gender-marking morphologically rich language. We develop a multi-step system
that combines the positive aspects of both rule-based and neural rewriting
models. Our results successfully demonstrate the viability of this approach on
a recently created corpus for Arabic gender rewriting, achieving 88.42 M2 F0.5
on a blind test set. Our proposed system improves over previous work on the
first-person-only version of this task, by 3.05 absolute increase in M2 F0.5.
We demonstrate a use case of our gender rewriting system by using it to
post-edit the output of a commercial MT system to provide personalized outputs
based on the users' grammatical gender preferences. We make our code, data, and
models publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-supervised learning approaches for predicting South African political sentiment for local government elections. (arXiv:2205.02223v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02223">
<div class="article-summary-box-inner">
<span><p>This study aims to understand the South African political context by
analysing the sentiments shared on Twitter during the local government
elections. An emphasis on the analysis was placed on understanding the
discussions led around four predominant political parties ANC, DA, EFF and
ActionSA. A semi-supervised approach by means of a graph-based technique to
label the vast accessible Twitter data for the classification of tweets into
negative and positive sentiment was used. The tweets expressing negative
sentiment were further analysed through latent topic extraction to uncover
hidden topics of concern associated with each of the political parties. Our
findings demonstrated that the general sentiment across South African Twitter
users is negative towards all four predominant parties with the worst negative
sentiment among users projected towards the current ruling party, ANC, relating
to concerns cantered around corruption, incompetence and loadshedding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HiURE: Hierarchical Exemplar Contrastive Learning for Unsupervised Relation Extraction. (arXiv:2205.02225v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02225">
<div class="article-summary-box-inner">
<span><p>Unsupervised relation extraction aims to extract the relationship between
entities from natural language sentences without prior information on
relational scope or distribution. Existing works either utilize self-supervised
schemes to refine relational feature signals by iteratively leveraging adaptive
clustering and classification that provoke gradual drift problems, or adopt
instance-wise contrastive learning which unreasonably pushes apart those
sentence pairs that are semantically similar. To overcome these defects, we
propose a novel contrastive learning framework named HiURE, which has the
capability to derive hierarchical signals from relational feature space using
cross hierarchy attention and effectively optimize relation representation of
sentences under exemplar-wise contrastive learning. Experimental results on two
public datasets demonstrate the advanced effectiveness and robustness of HiURE
on unsupervised relation extraction when compared with state-of-the-art models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adjusting for Confounders with Text: Challenges and an Empirical Evaluation Framework for Causal Inference. (arXiv:2009.09961v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.09961">
<div class="article-summary-box-inner">
<span><p>Leveraging text, such as social media posts, for causal inferences requires
the use of NLP models to 'learn' and adjust for confounders, which could
otherwise impart bias. However, evaluating such models is challenging, as
ground truth is almost never available. We demonstrate the need for empirical
evaluation frameworks for causal inference in natural language by showing that
existing, commonly used models regularly disagree with one another on real
world tasks. We contribute the first such framework, generalizing several
challenges across these real world tasks. Using this framework, we evaluate a
large set of commonly used causal inference models based on propensity scores
and identify their strengths and weaknesses to inform future improvements. We
make all tasks, data, and models public to inform applications and encourage
additional research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniK-QA: Unified Representations of Structured and Unstructured Knowledge for Open-Domain Question Answering. (arXiv:2012.14610v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.14610">
<div class="article-summary-box-inner">
<span><p>We study open-domain question answering with structured, unstructured and
semi-structured knowledge sources, including text, tables, lists and knowledge
bases. Departing from prior work, we propose a unifying approach that
homogenizes all sources by reducing them to text and applies the
retriever-reader model which has so far been limited to text sources only. Our
approach greatly improves the results on knowledge-base QA tasks by 11 points,
compared to latest graph-based methods. More importantly, we demonstrate that
our unified knowledge (UniK-QA) model is a simple and yet effective way to
combine heterogeneous sources of knowledge, advancing the state-of-the-art
results on two popular question answering benchmarks, NaturalQuestions and
WebQuestions, by 3.5 and 2.6 points, respectively.
</p>
<p>The code of UniK-QA is available at:
https://github.com/facebookresearch/UniK-QA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diagnosing Vision-and-Language Navigation: What Really Matters. (arXiv:2103.16561v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.16561">
<div class="article-summary-box-inner">
<span><p>Vision-and-language navigation (VLN) is a multimodal task where an agent
follows natural language instructions and navigates in visual environments.
Multiple setups have been proposed, and researchers apply new model
architectures or training techniques to boost navigation performance. However,
there still exist non-negligible gaps between machines' performance and human
benchmarks. Moreover, the agents' inner mechanisms for navigation decisions
remain unclear. To the best of our knowledge, how the agents perceive the
multimodal input is under-studied and needs investigation. In this work, we
conduct a series of diagnostic experiments to unveil agents' focus during
navigation. Results show that indoor navigation agents refer to both object and
direction tokens when making decisions. In contrast, outdoor navigation agents
heavily rely on direction tokens and poorly understand the object tokens.
Transformer-based agents acquire a better cross-modal understanding of objects
and display strong numerical reasoning ability than non-Transformer-based
agents. When it comes to vision-and-language alignments, many models claim that
they can align object tokens with specific visual targets. We find unbalanced
attention on the vision and text input and doubt the reliability of such
cross-modal alignments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Thematic Fit Bits: Annotation Quality and Quantity Interplay for Event Participant Representation. (arXiv:2105.06097v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.06097">
<div class="article-summary-box-inner">
<span><p>Modeling thematic fit (a verb--argument compositional semantics task)
currently requires a very large burden of labeled data. We take a
linguistically machine-annotated large corpus and replace corpus layers with
output from higher-quality, more modern taggers. We compare the old and new
corpus versions' impact on a verb--argument fit modeling task, using a
high-performing neural approach. We discover that higher annotation quality
dramatically reduces our data requirement while demonstrating better supervised
predicate-argument classification. But in applying the model to
psycholinguistic tasks outside the training objective, we see clear gains at
scale, but only in one of two thematic fit estimation tasks, and no clear gains
on the other. We also see that quality improves with training size, but perhaps
plateauing or even declining in one task. Last, we tested the effect of role
set size. All this suggests that the quality/quantity interplay is not all you
need. We replicate previous studies while modifying certain role representation
details and set a new state-of-the-art in event modeling, using a fraction of
the data. We make the new corpus version public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CausalNLP: A Practical Toolkit for Causal Inference with Text. (arXiv:2106.08043v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.08043">
<div class="article-summary-box-inner">
<span><p>Causal inference is the process of estimating the effect or impact of a
treatment on an outcome with other covariates as potential confounders (and
mediators) that may need to be controlled. The vast majority of existing
methods and systems for causal inference assume that all variables under
consideration are categorical or numerical (e.g., gender, price, enrollment).
In this paper, we present CausalNLP, a toolkit for inferring causality with
observational data that includes text in addition to traditional numerical and
categorical variables. CausalNLP employs the use of meta learners for treatment
effect estimation and supports using raw text and its linguistic properties as
a treatment, an outcome, or a "controlled-for" variable (e.g., confounder). The
library is open source and available at: https://github.com/amaiya/causalnlp.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Language to Learn Program Abstractions and Search Heuristics. (arXiv:2106.11053v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.11053">
<div class="article-summary-box-inner">
<span><p>Inductive program synthesis, or inferring programs from examples of desired
behavior, offers a general paradigm for building interpretable, robust, and
generalizable machine learning systems. Effective program synthesis depends on
two key ingredients: a strong library of functions from which to build
programs, and an efficient search strategy for finding programs that solve a
given task. We introduce LAPS (Language for Abstraction and Program Search), a
technique for using natural language annotations to guide joint learning of
libraries and neurally-guided search models for synthesis. When integrated into
a state-of-the-art library learning system (DreamCoder), LAPS produces
higher-quality libraries and improves search efficiency and generalization on
three domains -- string editing, image composition, and abstract reasoning
about scenes -- even when no natural language hints are available at test time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Political Ideology and Polarization of Policy Positions: A Multi-dimensional Approach. (arXiv:2106.14387v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.14387">
<div class="article-summary-box-inner">
<span><p>Analyzing ideology and polarization is of critical importance in advancing
our grasp of modern politics. Recent research has made great strides towards
understanding the ideological bias (i.e., stance) of news media along the
left-right spectrum. In this work, we instead take a novel and more nuanced
approach for the study of ideology based on its left or right positions on the
issue being discussed. Aligned with the theoretical accounts in political
science, we treat ideology as a multi-dimensional construct, and introduce the
first diachronic dataset of news articles whose ideological positions are
annotated by trained political scientists and linguists at the paragraph level.
We showcase that, by controlling for the author's stance, our method allows for
the quantitative and temporal measurement and analysis of polarization as a
multidimensional ideological distance. We further present baseline models for
ideology prediction, outlining a challenging task distinct from stance
detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hybrid deep learning methods for phenotype prediction from clinical notes. (arXiv:2108.10682v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10682">
<div class="article-summary-box-inner">
<span><p>Identifying patient cohorts from clinical notes in secondary electronic
health records is a fundamental task in clinical information management.
However, with the growing number of clinical notes, it becomes challenging to
analyze the data manually for phenotype detection. Automatic extraction of
clinical concepts would helps to identify the patient phenotypes correctly.
This paper proposes a novel hybrid model for automatically extracting patient
phenotypes using natural language processing and deep learning models to
determine the patient phenotypes without dictionaries and human intervention.
The model is based on a neural bidirectional sequence model (BiLSTM or BiGRU)
and a CNN layer for phenotypes identification. An extra CNN layer is run
parallel to the hybrid model to extract more features related to each
phenotype. We used pre-trained embeddings such as FastText and Word2vec
separately as the input layers to evaluate other embedding's performance.
Experimental results using MIMIC III database in internal comparison
demonstrate that the proposed model achieved significant performance
improvement over existing models. The enhanced version of our model with an
extra CNN layer obtained a relatively higher F1-score than the original hybrid
model. We also showed that BiGRU layer with FastText embedding had better
performance than BiLSTM layer to identify patient phenotypes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DEGREE: A Data-Efficient Generation-Based Event Extraction Model. (arXiv:2108.12724v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12724">
<div class="article-summary-box-inner">
<span><p>Event extraction requires high-quality expert human annotations, which are
usually expensive. Therefore, learning a data-efficient event extraction model
that can be trained with only a few labeled examples has become a crucial
challenge. In this paper, we focus on low-resource end-to-end event extraction
and propose DEGREE, a data-efficient model that formulates event extraction as
a conditional generation problem. Given a passage and a manually designed
prompt, DEGREE learns to summarize the events mentioned in the passage into a
natural sentence that follows a predefined pattern. The final event predictions
are then extracted from the generated sentence with a deterministic algorithm.
DEGREE has three advantages to learn well with less training data. First, our
designed prompts provide semantic guidance for DEGREE to leverage DEGREE and
thus better capture the event arguments. Moreover, DEGREE is capable of using
additional weakly-supervised information, such as the description of events
encoded in the prompts. Finally, DEGREE learns triggers and arguments jointly
in an end-to-end manner, which encourages the model to better utilize the
shared knowledge and dependencies among them. Our experimental results
demonstrate the strong performance of DEGREE for low-resource event extraction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Length Divergence Bias in Textual Matching Models. (arXiv:2109.02431v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02431">
<div class="article-summary-box-inner">
<span><p>Despite the remarkable success deep models have achieved in Textual Matching
(TM) tasks, it still remains unclear whether they truly understand language or
measure the semantic similarity of texts by exploiting statistical bias in
datasets. In this work, we provide a new perspective to study this issue -- via
the length divergence bias. We find the length divergence heuristic widely
exists in prevalent TM datasets, providing direct cues for prediction. To
determine whether TM models have adopted such heuristic, we introduce an
adversarial evaluation scheme which invalidates the heuristic. In this
adversarial setting, all TM models perform worse, indicating they have indeed
adopted this heuristic. Through a well-designed probing experiment, we
empirically validate that the bias of TM models can be attributed in part to
extracting the text length information during training. To alleviate the length
divergence bias, we propose an adversarial training method. The results
demonstrate we successfully improve the robustness and generalization ability
of models at the same time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MOVER: Mask, Over-generate and Rank for Hyperbole Generation. (arXiv:2109.07726v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07726">
<div class="article-summary-box-inner">
<span><p>Despite being a common figure of speech, hyperbole is under-researched in
Figurative Language Processing. In this paper, we tackle the challenging task
of hyperbole generation to transfer a literal sentence into its hyperbolic
paraphrase. To address the lack of available hyperbolic sentences, we construct
HYPO-XL, the first large-scale English hyperbole corpus containing 17,862
hyperbolic sentences in a non-trivial way. Based on our corpus, we propose an
unsupervised method for hyperbole generation that does not require parallel
literal-hyperbole pairs. During training, we fine-tune BART to infill masked
hyperbolic spans of sentences from HYPO-XL. During inference, we mask part of
an input literal sentence and over-generate multiple possible hyperbolic
versions. Then a BERT-based ranker selects the best candidate by hyperbolicity
and paraphrase quality. Automatic and human evaluation results show that our
model is effective at generating hyperbolic paraphrase sentences and
outperforms several baseline systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PPL-MCTS: Constrained Textual Generation Through Discriminator-Guided MCTS Decoding. (arXiv:2109.13582v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13582">
<div class="article-summary-box-inner">
<span><p>Large language models (LM) based on Transformers allow to generate plausible
long texts. In this paper, we explore how this generation can be further
controlled at decoding time to satisfy certain constraints (e.g. being
non-toxic, conveying certain emotions, using a specific writing style, etc.)
without fine-tuning the LM. Precisely, we formalize constrained generation as a
tree exploration process guided by a discriminator that indicates how well the
associated sequence respects the constraint. This approach, in addition to
being easier and cheaper to train than fine-tuning the LM, allows to apply the
constraint more finely and dynamically. We propose several original methods to
search this generation tree, notably the Monte Carlo Tree Search (MCTS) which
provides theoretical guarantees on the search efficiency, but also simpler
methods based on re-ranking a pool of diverse sequences using the discriminator
scores. These methods are evaluated, with automatic and human-based metrics, on
two types of constraints and languages: review polarity and emotion control in
French and English. We show that discriminator-guided MCTS decoding achieves
state-of-the-art results without having to tune the language model, in both
tasks and languages. We also demonstrate that other proposed decoding methods
based on re-ranking can be really effective when diversity among the generated
propositions is encouraged.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UserIdentifier: Implicit User Representations for Simple and Effective Personalized Sentiment Analysis. (arXiv:2110.00135v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00135">
<div class="article-summary-box-inner">
<span><p>Global models are trained to be as generalizable as possible, with user
invariance considered desirable since the models are shared across multitudes
of users. As such, these models are often unable to produce personalized
responses for individual users, based on their data. Contrary to widely-used
personalization techniques based on few-shot learning, we propose
UserIdentifier, a novel scheme for training a single shared model for all
users. Our approach produces personalized responses by adding fixed,
non-trainable user identifiers to the input data. We empirically demonstrate
that this proposed method outperforms the prefix-tuning based state-of-the-art
approach by up to 13%, on a suite of sentiment analysis datasets. We also show
that, unlike prior work, this method needs neither any additional model
parameters nor any extra rounds of few-shot fine-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Robustness of Reading Comprehension Models to Entity Renaming. (arXiv:2110.08555v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08555">
<div class="article-summary-box-inner">
<span><p>We study the robustness of machine reading comprehension (MRC) models to
entity renaming -- do models make more wrong predictions when the same
questions are asked about an entity whose name has been changed? Such failures
imply that models overly rely on entity information to answer questions, and
thus may generalize poorly when facts about the world change or questions are
asked about novel entities. To systematically audit this issue, we present a
pipeline to automatically generate test examples at scale, by replacing entity
names in the original test sample with names from a variety of sources, ranging
from names in the same test set, to common names in life, to arbitrary strings.
Across five datasets and three pretrained model architectures, MRC models
consistently perform worse when entities are renamed, with particularly large
accuracy drops on datasets constructed via distant supervision. We also find
large differences between models: SpanBERT, which is pretrained with span-level
masking, is more robust than RoBERTa, despite having similar accuracy on
unperturbed test data. We further experiment with different masking strategies
as the continual pretraining objective and find that entity-based masking can
improve the robustness of MRC models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GNN-LM: Language Modeling based on Global Contexts via GNN. (arXiv:2110.08743v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08743">
<div class="article-summary-box-inner">
<span><p>Inspired by the notion that ``{\it to copy is easier than to memorize}``, in
this work, we introduce GNN-LM, which extends the vanilla neural language model
(LM) by allowing to reference similar contexts in the entire training corpus.
We build a directed heterogeneous graph between an input context and its
semantically related neighbors selected from the training corpus, where nodes
are tokens in the input context and retrieved neighbor contexts, and edges
represent connections between nodes. Graph neural networks (GNNs) are
constructed upon the graph to aggregate information from similar contexts to
decode the token. This learning paradigm provides direct access to the
reference contexts and helps improve a model's generalization ability. We
conduct comprehensive experiments to validate the effectiveness of the GNN-LM:
GNN-LM achieves a new state-of-the-art perplexity of 14.8 on WikiText-103 (a
3.9 point improvement over its counterpart of the vanilla LM model), and shows
substantial improvement on One Billion Word and Enwiki8 datasets against strong
baselines. In-depth ablation studies are performed to understand the mechanics
of GNN-LM. \footnote{The code can be found at
https://github.com/ShannonAI/GNN-LM
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When is BERT Multilingual? Isolating Crucial Ingredients for Cross-lingual Transfer. (arXiv:2110.14782v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14782">
<div class="article-summary-box-inner">
<span><p>While recent work on multilingual language models has demonstrated their
capacity for cross-lingual zero-shot transfer on downstream tasks, there is a
lack of consensus in the community as to what shared properties between
languages enable such transfer. Analyses involving pairs of natural languages
are often inconclusive and contradictory since languages simultaneously differ
in many linguistic aspects. In this paper, we perform a large-scale empirical
study to isolate the effects of various linguistic properties by measuring
zero-shot transfer between four diverse natural languages and their
counterparts constructed by modifying aspects such as the script, word order,
and syntax. Among other things, our experiments show that the absence of
sub-word overlap significantly affects zero-shot transfer when languages differ
in their word order, and there is a strong correlation between transfer
performance and word embedding alignment between languages (e.g., R=0.94 on the
task of NLI). Our results call for focus in multilingual models on explicitly
improving word embedding alignment between languages rather than relying on its
implicit emergence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity. (arXiv:2111.08366v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.08366">
<div class="article-summary-box-inner">
<span><p>We present a new scientific document similarity model based on matching
fine-grained aspects of texts. To train our model, we exploit a
naturally-occurring source of supervision: sentences in the full-text of papers
that cite multiple papers together (co-citations). Such co-citations not only
reflect close paper relatedness, but also provide textual descriptions of how
the co-cited papers are related. This novel form of textual supervision is used
for learning to match aspects across papers. We develop multi-vector
representations where vectors correspond to sentence-level aspects of
documents, and present two methods for aspect matching: (1) A fast method that
only matches single aspects, and (2) a method that makes sparse multiple
matches with an Optimal Transport mechanism that computes an Earth Mover's
Distance between aspects. Our approach improves performance on document
similarity tasks in four datasets. Further, our fast single-match method
achieves competitive results, paving the way for applying fine-grained
similarity to large scientific corpora. Code, data, and models available at:
https://github.com/allenai/aspire
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RoBERTuito: a pre-trained language model for social media text in Spanish. (arXiv:2111.09453v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.09453">
<div class="article-summary-box-inner">
<span><p>Since BERT appeared, Transformer language models and transfer learning have
become state-of-the-art for Natural Language Understanding tasks. Recently,
some works geared towards pre-training specially-crafted models for particular
domains, such as scientific papers, medical documents, user-generated texts,
among others. These domain-specific models have been shown to improve
performance significantly in most tasks. However, for languages other than
English such models are not widely available.
</p>
<p>In this work, we present RoBERTuito, a pre-trained language model for
user-generated text in Spanish, trained on over 500 million tweets. Experiments
on a benchmark of tasks involving user-generated text showed that RoBERTuito
outperformed other pre-trained language models in Spanish. In addition to this,
our model achieves top results for some English-Spanish tasks of the Linguistic
Code-Switching Evaluation benchmark (LinCE) and has also competitive
performance against monolingual models in English tasks. To facilitate further
research, we make RoBERTuito publicly available at the HuggingFace model hub
together with the dataset used to pre-train it.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERTMap: A BERT-based Ontology Alignment System. (arXiv:2112.02682v4 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02682">
<div class="article-summary-box-inner">
<span><p>Ontology alignment (a.k.a ontology matching (OM)) plays a critical role in
knowledge integration. Owing to the success of machine learning in many
domains, it has been applied in OM. However, the existing methods, which often
adopt ad-hoc feature engineering or non-contextual word embeddings, have not
yet outperformed rule-based systems especially in an unsupervised setting. In
this paper, we propose a novel OM system named BERTMap which can support both
unsupervised and semi-supervised settings. It first predicts mappings using a
classifier based on fine-tuning the contextual embedding model BERT on text
semantics corpora extracted from ontologies, and then refines the mappings
through extension and repair by utilizing the ontology structure and logic. Our
evaluation with three alignment tasks on biomedical ontologies demonstrates
that BERTMap can often perform better than the leading OM systems LogMap and
AML.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WECHSEL: Effective initialization of subword embeddings for cross-lingual transfer of monolingual language models. (arXiv:2112.06598v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06598">
<div class="article-summary-box-inner">
<span><p>Large pretrained language models (LMs) have become the central building block
of many NLP applications. Training these models requires ever more
computational resources and most of the existing models are trained on English
text only. It is exceedingly expensive to train these models in other
languages. To alleviate this problem, we introduce a novel method -- called
WECHSEL -- to efficiently and effectively transfer pretrained LMs to new
languages. WECHSEL can be applied to any model which uses subword-based
tokenization and learns an embedding for each subword. The tokenizer of the
source model (in English) is replaced with a tokenizer in the target language
and token embeddings are initialized such that they are semantically similar to
the English tokens by utilizing multilingual static word embeddings covering
English and the target language. We use WECHSEL to transfer the English RoBERTa
and GPT-2 models to four languages (French, German, Chinese and Swahili). We
also study the benefits of our method on very low-resource languages. WECHSEL
improves over proposed methods for cross-lingual parameter transfer and
outperforms models of comparable size trained from scratch with up to 64x less
training effort. Our method makes training large language models for new
languages more accessible and less damaging to the environment. We make our
code and models publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simple Local Attentions Remain Competitive for Long-Context Tasks. (arXiv:2112.07210v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07210">
<div class="article-summary-box-inner">
<span><p>Many NLP tasks require processing long contexts beyond the length limit of
pretrained models. In order to scale these models to longer text sequences,
many efficient long-range attention variants have been proposed. Despite the
abundance of research along this direction, it is still difficult to gauge the
relative effectiveness of these models in practical use cases, e.g., if we
apply these models following the pretrain-and-finetune paradigm. In this work,
we aim to conduct a thorough analysis of these emerging models with large-scale
and controlled experiments. For each attention variant, we pretrain large-size
models using the same long-doc corpus and then finetune these models for
real-world long-context tasks. Our findings reveal pitfalls of an existing
widely-used long-range benchmark and show none of the tested efficient
attentions can beat a simple local window attention under standard pretraining
paradigms. Further analysis on local attention variants suggests that even the
commonly used attention-window overlap is not necessary to achieve good
downstream results -- using disjoint local attentions, we are able to build a
simpler and more efficient long-doc QA model that matches the performance of
Longformer~\citep{longformer} with half of its pretraining compute.
</p>
<p>The code to replicate our experiments can be found at
https://github.com/pytorch/fairseq/tree/main/examples/xformers
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Cross-Lingual IR from an English Retriever. (arXiv:2112.08185v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08185">
<div class="article-summary-box-inner">
<span><p>We present DR.DECR (Dense Retrieval with Distillation-Enhanced Cross-Lingual
Representation), a new cross-lingual information retrieval (CLIR) system
trained using multi-stage knowledge distillation (KD). The teacher of DR.DECR
relies on a highly effective but computationally expensive two-stage inference
process consisting of query translation and monolingual IR, while the student,
DR.DECR, executes a single CLIR step. We teach DR.DECR powerful multilingual
representations as well as CLIR by optimizing two corresponding KD objectives.
Learning useful representations of non-English text from an English-only
retriever is accomplished through a cross-lingual token alignment algorithm
that relies on the representation capabilities of the underlying multilingual
encoders. In both in-domain and zero-shot out-of-domain evaluation, DR.DECR
demonstrates far superior accuracy over direct fine-tuning with labeled CLIR
data. It is also the best single-model retriever on the XOR-TyDi benchmark at
the time of this writing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt Waywardness: The Curious Case of Discretized Interpretation of Continuous Prompts. (arXiv:2112.08348v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08348">
<div class="article-summary-box-inner">
<span><p>Fine-tuning continuous prompts for target tasks has recently emerged as a
compact alternative to full model fine-tuning. Motivated by these promising
results, we investigate the feasibility of extracting a discrete (textual)
interpretation of continuous prompts that is faithful to the problem they
solve. In practice, we observe a "wayward" behavior between the task solved by
continuous prompts and their nearest neighbor discrete projections: We can find
continuous prompts that solve a task while being projected to an arbitrary text
(e.g., definition of a different or even a contradictory task), while being
within a very small (2%) margin of the best continuous prompt of the same size
for the task. We provide intuitions behind this odd and surprising behavior, as
well as extensive empirical analyses quantifying the effect of various
parameters. For instance, for larger model sizes we observe higher waywardness,
i.e, we can find prompts that more closely map to any arbitrary text with a
smaller drop in accuracy. These findings have important implications relating
to the difficulty of faithfully interpreting continuous prompts and their
generalization across models and tasks, providing guidance for future progress
in prompting language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ErAConD : Error Annotated Conversational Dialog Dataset for Grammatical Error Correction. (arXiv:2112.08466v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08466">
<div class="article-summary-box-inner">
<span><p>Currently available grammatical error correction (GEC) datasets are compiled
using well-formed written text, limiting the applicability of these datasets to
other domains such as informal writing and dialog. In this paper, we present a
novel parallel GEC dataset drawn from open-domain chatbot conversations; this
dataset is, to our knowledge, the first GEC dataset targeted to a
conversational setting. To demonstrate the utility of the dataset, we use our
annotated data to fine-tune a state-of-the-art GEC model, resulting in a 16
point increase in model precision. This is of particular importance in a GEC
model, as model precision is considered more important than recall in GEC tasks
since false positives could lead to serious confusion in language learners. We
also present a detailed annotation scheme which ranks errors by perceived
impact on comprehensibility, making our dataset both reproducible and
extensible. Experimental results show the effectiveness of our data in
improving GEC model performance in conversational scenario.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MuMuQA: Multimedia Multi-Hop News Question Answering via Cross-Media Knowledge Extraction and Grounding. (arXiv:2112.10728v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10728">
<div class="article-summary-box-inner">
<span><p>Recently, there has been an increasing interest in building question
answering (QA) models that reason across multiple modalities, such as text and
images. However, QA using images is often limited to just picking the answer
from a pre-defined set of options. In addition, images in the real world,
especially in news, have objects that are co-referential to the text, with
complementary information from both modalities. In this paper, we present a new
QA evaluation benchmark with 1,384 questions over news articles that require
cross-media grounding of objects in images onto text. Specifically, the task
involves multi-hop questions that require reasoning over image-caption pairs to
identify the grounded visual object being referred to and then predicting a
span from the news body text to answer the question. In addition, we introduce
a novel multimedia data augmentation framework, based on cross-media knowledge
extraction and synthetic question-answer generation, to automatically augment
data that can provide weak supervision for this task. We evaluate both
pipeline-based and end-to-end pretraining-based multimedia QA models on our
benchmark, and show that they achieve promising performance, while considerably
lagging behind human performance hence leaving large room for future work on
this challenging new task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Variant Consistency based Self-supervised Learning for Robust Automatic Speech Recognition. (arXiv:2112.12522v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.12522">
<div class="article-summary-box-inner">
<span><p>Automatic speech recognition (ASR) has shown rapid advances in recent years
but still degrades significantly in far-field and noisy environments. The
recent development of self-supervised learning (SSL) technology can improve the
ASR performance by pre-training the model with additional unlabeled speech and
the SSL pre-trained model has achieved the state-of-the-art result on several
speech benchmarks. Nevertheless, most of the previous SSL methods ignore the
influence of the background noise or reverberation, which is crucial to
deploying ASR systems in real-world speech applications. This study addresses
the robust ASR by introducing a multi-variant consistency (MVC) based SSL
method that adapts to different environments. The MVC-SSL is a robust SSL
pre-training method designed for noisy and distant-talking speech in real-world
applications. Compared to the previous SSL method, the MVC-SSL can calculate
the contrastive loss among audios from different acoustic conditions or
channels and can learn invariant representations with the change in the
environment or the recording equipment. We also explore different SSL training
pipelines to balance the noisy distant-talking speech and extra high resource
clean speech. We evaluate the proposed method on the commercially-motivated
dataset, CHiME-4, and the meeting dataset, AMI. With the help of the MVC-SSL
and appropriate training pipeline, we can achieve up to 30% relative word error
rate reductions over the baseline wav2vec2.0, one of the most successful SSL
methods for ASR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anticipation-Free Training for Simultaneous Machine Translation. (arXiv:2201.12868v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12868">
<div class="article-summary-box-inner">
<span><p>Simultaneous machine translation (SimulMT) speeds up the translation process
by starting to translate before the source sentence is completely available. It
is difficult due to limited context and word order difference between
languages. Existing methods increase latency or introduce adaptive read-write
policies for SimulMT models to handle local reordering and improve translation
quality. However, the long-distance reordering would make the SimulMT models
learn translation mistakenly. Specifically, the model may be forced to predict
target tokens when the corresponding source tokens have not been read. This
leads to aggressive anticipation during inference, resulting in the
hallucination phenomenon. To mitigate this problem, we propose a new framework
that decompose the translation process into the monotonic translation step and
the reordering step, and we model the latter by the auxiliary sorting network
(ASN). The ASN rearranges the hidden states to match the order in the target
language, so that the SimulMT model could learn to translate more reasonably.
The entire model is optimized end-to-end and does not rely on external aligners
or data. During inference, ASN is removed to achieve streaming. Experiments
show the proposed framework could outperform previous methods with less
latency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Temporal Attention for Language Models. (arXiv:2202.02093v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02093">
<div class="article-summary-box-inner">
<span><p>Pretrained language models based on the transformer architecture have shown
great success in NLP. Textual training data often comes from the web and is
thus tagged with time-specific information, but most language models ignore
this information. They are trained on the textual data alone, limiting their
ability to generalize temporally. In this work, we extend the key component of
the transformer architecture, i.e., the self-attention mechanism, and propose
temporal attention - a time-aware self-attention mechanism. Temporal attention
can be applied to any transformer model and requires the input texts to be
accompanied with their relevant time points. It allows the transformer to
capture this temporal information and create time-specific contextualized word
representations. We leverage these representations for the task of semantic
change detection; we apply our proposed mechanism to BERT and experiment on
three datasets in different languages (English, German, and Latin) that also
vary in time, size, and genre. Our proposed model achieves state-of-the-art
results on all the datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Systematic Evaluation of Large Language Models of Code. (arXiv:2202.13169v3 [cs.PL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13169">
<div class="article-summary-box-inner">
<span><p>Large language models (LMs) of code have recently shown tremendous promise in
completing code and synthesizing code from natural language descriptions.
However, the current state-of-the-art code LMs (e.g., Codex (Chen et al.,
2021)) are not publicly available, leaving many questions about their model and
data design decisions. We aim to fill in some of these blanks through a
systematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo,
GPT-NeoX-20B, and CodeParrot, across various programming languages. Although
Codex itself is not open-source, we find that existing open-source models do
achieve close results in some programming languages, although targeted mainly
for natural language modeling. We further identify an important missing piece
in the form of a large open-source model trained exclusively on a multi-lingual
corpus of code. We release a new model, PolyCoder, with 2.7B parameters based
on the GPT-2 architecture, which was trained on 249GB of code across 12
programming languages on a single machine. In the C programming language,
PolyCoder outperforms all models including Codex. Our trained models are
open-source and publicly available at https://github.com/VHellendoorn/Code-LMs,
which enables future research and application in this area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AIFB-WebScience at SemEval-2022 Task 12: Relation Extraction First -- Using Relation Extraction to Identify Entities. (arXiv:2203.05325v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05325">
<div class="article-summary-box-inner">
<span><p>In this paper, we present an end-to-end joint entity and relation extraction
approach based on transformer-based language models. We apply the model to the
task of linking mathematical symbols to their descriptions in LaTeX documents.
In contrast to existing approaches, which perform entity and relation
extraction in sequence, our system incorporates information from relation
extraction into entity extraction. This means that the system can be trained
even on data sets where only a subset of all valid entity spans is annotated.
We provide an extensive evaluation of the proposed system and its strengths and
weaknesses. Our approach, which can be scaled dynamically in computational
complexity at inference time, produces predictions with high precision and
reaches 3rd place in the leaderboard of SemEval-2022 Task 12. For inputs in the
domain of physics and math, it achieves high relation extraction macro F1
scores of 95.43% and 79.17%, respectively. The code used for training and
evaluating our models is available at: https://github.com/nicpopovic/RE1st
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph Pre-training for AMR Parsing and Generation. (arXiv:2203.07836v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07836">
<div class="article-summary-box-inner">
<span><p>Abstract meaning representation (AMR) highlights the core semantic
information of text in a graph structure. Recently, pre-trained language models
(PLMs) have advanced tasks of AMR parsing and AMR-to-text generation,
respectively. However, PLMs are typically pre-trained on textual data, thus are
sub-optimal for modeling structural knowledge. To this end, we investigate
graph self-supervised training to improve the structure awareness of PLMs over
AMR graphs. In particular, we introduce two graph auto-encoding strategies for
graph-to-graph pre-training and four tasks to integrate text and graph
information during pre-training. We further design a unified framework to
bridge the gap between pre-training and fine-tuning tasks. Experiments on both
AMR parsing and AMR-to-text generation show the superiority of our model. To
our knowledge, we are the first to consider pre-training on semantic graphs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt-based Pre-trained Model for Personality and Interpersonal Reactivity Prediction. (arXiv:2203.12481v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12481">
<div class="article-summary-box-inner">
<span><p>This paper describes our proposed method for the Workshop on Computational
Approaches to Subjectivity, Sentiment &amp; Social Media Analysis (WASSA) 2022
shared task on Personality Prediction (PER) and Reactivity Index Prediction
(IRI). In this paper, we adopt the prompt-based learning method with the
pre-trained language model to accomplish these tasks. Specifically, the prompt
is designed to provide knowledge of the extra personalized information for
enhancing the pre-trained model. Data augmentation and model ensemble are
adopted for obtaining better results. Moreover, we also provided the online
software demonstration and the codes of the software for further research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Exploration of Prompt Tuning on Generative Spoken Language Model for Speech Processing Tasks. (arXiv:2203.16773v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16773">
<div class="article-summary-box-inner">
<span><p>Speech representations learned from Self-supervised learning (SSL) models can
benefit various speech processing tasks. However, utilizing SSL representations
usually requires fine-tuning the pre-trained models or designing task-specific
downstream models and loss functions, causing much memory usage and human
labor. Recently, prompting in Natural Language Processing (NLP) has been found
to be an efficient technique to leverage pre-trained language models (LMs).
Specifically, prompt tuning optimizes a limited number of task-specific
parameters with a fixed pre-trained model; as a result, only a small set of
parameters is needed to be stored for each task. Prompt tuning improves
computation and memory efficiency by leveraging the pre-trained LM's prediction
ability. Nevertheless, such a paradigm is little studied in the speech
community. We report in this paper the first exploration of the prompt tuning
paradigm for speech processing tasks based on Generative Spoken Language Model
(GSLM). Experiment results show that the prompt tuning technique achieves
competitive performance in speech classification tasks with fewer trainable
parameters than fine-tuning specialized downstream models. We further study the
technique in challenging sequence generation tasks. Prompt tuning also
demonstrates its potential, while the limitation and possible research
directions are discussed in this paper. The source code is available on
https://github.com/ga642381/SpeechPrompt.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quick Starting Dialog Systems with Paraphrase Generation. (arXiv:2204.02546v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02546">
<div class="article-summary-box-inner">
<span><p>Acquiring training data to improve the robustness of dialog systems can be a
painstakingly long process. In this work, we propose a method to reduce the
cost and effort of creating new conversational agents by artificially
generating more data from existing examples, using paraphrase generation. Our
proposed approach can kick-start a dialog system with little human effort, and
brings its performance to a level satisfactory enough for allowing actual
interactions with real end-users. We experimented with two neural paraphrasing
approaches, namely Neural Machine Translation and a Transformer-based seq2seq
model. We present the results obtained with two datasets in English and in
French:~a crowd-sourced public intent classification dataset and our own
corporate dialog system dataset. We show that our proposed approach increased
the generalization capabilities of the intent classification model on both
datasets, reducing the effort required to initialize a new dialog system and
helping to deploy this technology at scale within an organization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Informativeness and Invariance: Two Perspectives on Spurious Correlations in Natural Language. (arXiv:2204.04487v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04487">
<div class="article-summary-box-inner">
<span><p>Spurious correlations are a threat to the trustworthiness of natural language
processing systems, motivating research into methods for identifying and
eliminating them. However, addressing the problem of spurious correlations
requires more clarity on what they are and how they arise in language data.
Gardner et al (2021) argue that the compositional nature of language implies
that \emph{all} correlations between labels and individual "input features" are
spurious. This paper analyzes this proposal in the context of a toy example,
demonstrating three distinct conditions that can give rise to feature-label
correlations in a simple PCFG. Linking the toy example to a structured causal
model shows that (1) feature-label correlations can arise even when the label
is invariant to interventions on the feature, and (2) feature-label
correlations may be absent even when the label is sensitive to interventions on
the feature. Because input features will be individually correlated with labels
in all but very rare circumstances, domain knowledge must be applied to
identify spurious correlations that pose genuine robustness threats.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Curriculum: A Broad-Coverage Benchmark for Linguistic Phenomena in Natural Language Understanding. (arXiv:2204.06283v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06283">
<div class="article-summary-box-inner">
<span><p>In the age of large transformer language models, linguistic evaluation play
an important role in diagnosing models' abilities and limitations on natural
language understanding. However, current evaluation methods show some
significant shortcomings. In particular, they do not provide insight into how
well a language model captures distinct linguistic skills essential for
language understanding and reasoning. Thus they fail to effectively map out the
aspects of language understanding that remain challenging to existing models,
which makes it hard to discover potential limitations in models and datasets.
In this paper, we introduce Curriculum as a new format of NLI benchmark for
evaluation of broad-coverage linguistic phenomena. Curriculum contains a
collection of datasets that covers 36 types of major linguistic phenomena and
an evaluation procedure for diagnosing how well a language model captures
reasoning skills for distinct types of linguistic phenomena. We show that this
linguistic-phenomena-driven benchmark can serve as an effective tool for
diagnosing model behavior and verifying model learning quality. In addition,
our experiments provide insight into the limitation of existing benchmark
datasets and state-of-the-art models that may encourage future research on
re-designing datasets, model architectures, and learning objectives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CRUSH: Contextually Regularized and User anchored Self-supervised Hate speech Detection. (arXiv:2204.06389v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06389">
<div class="article-summary-box-inner">
<span><p>The last decade has witnessed a surge in the interaction of people through
social networking platforms. While there are several positive aspects of these
social platforms, the proliferation has led them to become the breeding ground
for cyber-bullying and hate speech. Recent advances in NLP have often been used
to mitigate the spread of such hateful content. Since the task of hate speech
detection is usually applicable in the context of social networks, we introduce
CRUSH, a framework for hate speech detection using user-anchored
self-supervision and contextual regularization. Our proposed approach secures ~
1-12% improvement in test set metrics over best performing previous approaches
on two types of tasks and multiple popular english social media datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shedding New Light on the Language of the Dark Web. (arXiv:2204.06885v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06885">
<div class="article-summary-box-inner">
<span><p>The hidden nature and the limited accessibility of the Dark Web, combined
with the lack of public datasets in this domain, make it difficult to study its
inherent characteristics such as linguistic properties. Previous works on text
classification of Dark Web domain have suggested that the use of deep neural
models may be ineffective, potentially due to the linguistic differences
between the Dark and Surface Webs. However, not much work has been done to
uncover the linguistic characteristics of the Dark Web. This paper introduces
CoDA, a publicly available Dark Web dataset consisting of 10000 web documents
tailored towards text-based Dark Web analysis. By leveraging CoDA, we conduct a
thorough linguistic analysis of the Dark Web and examine the textual
differences between the Dark Web and the Surface Web. We also assess the
performance of various methods of Dark Web page classification. Finally, we
compare CoDA with an existing public Dark Web dataset and evaluate their
suitability for various use cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision-and-Language Pretrained Models: A Survey. (arXiv:2204.07356v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07356">
<div class="article-summary-box-inner">
<span><p>Pretrained models have produced great success in both Computer Vision (CV)
and Natural Language Processing (NLP). This progress leads to learning joint
representations of vision and language pretraining by feeding visual and
linguistic contents into a multi-layer transformer, Visual-Language Pretrained
Models (VLPMs). In this paper, we present an overview of the major advances
achieved in VLPMs for producing joint representations of vision and language.
As the preliminaries, we briefly describe the general task definition and
genetic architecture of VLPMs. We first discuss the language and vision data
encoding methods and then present the mainstream VLPM structure as the core
content. We further summarise several essential pretraining and fine-tuning
strategies. Finally, we highlight three future directions for both CV and NLP
researchers to provide insightful guidance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MuCGEC: a Multi-Reference Multi-Source Evaluation Dataset for Chinese Grammatical Error Correction. (arXiv:2204.10994v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10994">
<div class="article-summary-box-inner">
<span><p>This paper presents MuCGEC, a multi-reference multi-source evaluation dataset
for Chinese Grammatical Error Correction (CGEC), consisting of 7,063 sentences
collected from three Chinese-as-a-Second-Language (CSL) learner sources. Each
sentence is corrected by three annotators, and their corrections are carefully
reviewed by a senior annotator, resulting in 2.3 references per sentence. We
conduct experiments with two mainstream CGEC models, i.e., the
sequence-to-sequence model and the sequence-to-edit model, both enhanced with
large pretrained language models, achieving competitive benchmark performance
on previous and our datasets. We also discuss CGEC evaluation methodologies,
including the effect of multiple references and using a char-based metric. Our
annotation guidelines, data, and code are available at
\url{https://github.com/HillZhang1999/MuCGEC}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Local Hypergraph-based Nested Named Entity Recognition as Query-based Sequence Labeling. (arXiv:2204.11467v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11467">
<div class="article-summary-box-inner">
<span><p>There has been a growing academic interest in the recognition of nested named
entities in many domains. We tackle the task with a novel local
hypergraph-based method: We first propose start token candidates and generate
corresponding queries with their surrounding context, then use a query-based
sequence labeling module to form a local hypergraph for each candidate. An end
token estimator is used to correct the hypergraphs and get the final
predictions. Compared to span-based approaches, our method is free of the high
computation cost of span sampling and the risk of losing long entities.
Sequential prediction makes it easier to leverage information in word order
inside nested structures, and richer representations are built with a local
hypergraph. Experiments show that our proposed method outperforms all the
previous hypergraph-based and sequence labeling approaches with large margins
on all four nested datasets. It achieves a new state-of-the-art F1 score on the
ACE 2004 dataset and competitive F1 scores with previous state-of-the-art
methods on three other nested NER datasets: ACE 2005, GENIA, and KBP 2017.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Rationalization Improve Robustness?. (arXiv:2204.11790v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11790">
<div class="article-summary-box-inner">
<span><p>A growing line of work has investigated the development of neural NLP models
that can produce rationales--subsets of input that can explain their model
predictions. In this paper, we ask whether such rationale models can also
provide robustness to adversarial attacks in addition to their interpretable
nature. Since these models need to first generate rationales ("rationalizer")
before making predictions ("predictor"), they have the potential to ignore
noise or adversarially added text by simply masking it out of the generated
rationale. To this end, we systematically generate various types of 'AddText'
attacks for both token and sentence-level rationalization tasks, and perform an
extensive empirical evaluation of state-of-the-art rationale models across five
different tasks. Our experiments reveal that the rationale models show the
promise to improve robustness, while they struggle in certain scenarios--when
the rationalizer is sensitive to positional bias or lexical choices of attack
text. Further, leveraging human rationale as supervision does not always
translate to better performance. Our study is a first step towards exploring
the interplay between interpretability and robustness in the
rationalize-then-predict framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OPERA:Operation-Pivoted Discrete Reasoning over Text. (arXiv:2204.14166v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14166">
<div class="article-summary-box-inner">
<span><p>Machine reading comprehension (MRC) that requires discrete reasoning
involving symbolic operations, e.g., addition, sorting, and counting, is a
challenging task. According to this nature, semantic parsing-based methods
predict interpretable but complex logical forms. However, logical form
generation is nontrivial and even a little perturbation in a logical form will
lead to wrong answers. To alleviate this issue, multi-predictor -based methods
are proposed to directly predict different types of answers and achieve
improvements. However, they ignore the utilization of symbolic operations and
encounter a lack of reasoning ability and interpretability. To inherit the
advantages of these two types of methods, we propose OPERA, an
operation-pivoted discrete reasoning framework, where lightweight symbolic
operations (compared with logical forms) as neural modules are utilized to
facilitate the reasoning ability and interpretability. Specifically, operations
are first selected and then softly executed to simulate the answer reasoning
procedure. Extensive experiments on both DROP and RACENum datasets show the
reasoning ability of OPERA. Moreover, further analysis verifies its
interpretability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training Naturalized Semantic Parsers with Very Little Data. (arXiv:2204.14243v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.14243">
<div class="article-summary-box-inner">
<span><p>Semantic parsing is an important NLP problem, particularly for voice
assistants such as Alexa and Google Assistant. State-of-the-art (SOTA) semantic
parsers are seq2seq architectures based on large language models that have been
pretrained on vast amounts of text. To better leverage that pretraining, recent
work has explored a reformulation of semantic parsing whereby the output
sequences are themselves natural language sentences, but in a controlled
fragment of natural language. This approach delivers strong results,
particularly for few-shot semantic parsing, which is of key importance in
practice and the focus of our paper. We push this line of work forward by
introducing an automated methodology that delivers very significant additional
improvements by utilizing modest amounts of unannotated data, which is
typically easy to obtain. Our method is based on a novel synthesis of four
techniques: joint training with auxiliary unsupervised tasks; constrained
decoding; self-training; and paraphrasing. We show that this method delivers
new SOTA few-shot performance on the Overnight dataset, particularly in very
low-resource settings, and very compelling few-shot results on a new semantic
parsing dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Brainish: Formalizing A Multimodal Language for Intelligence and Consciousness. (arXiv:2205.00001v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00001">
<div class="article-summary-box-inner">
<span><p>Having a rich multimodal inner language is an important component of human
intelligence that enables several necessary core cognitive functions such as
multimodal prediction, translation, and generation. Building upon the Conscious
Turing Machine (CTM), a machine model for consciousness proposed by Blum and
Blum (2021), we describe the desiderata of a multimodal language called
Brainish, comprising words, images, audio, and sensations combined in
representations that the CTM's processors use to communicate with each other.
We define the syntax and semantics of Brainish before operationalizing this
language through the lens of multimodal artificial intelligence, a vibrant
research area studying the computational tools necessary for processing and
relating information from heterogeneous signals. Our general framework for
learning Brainish involves designing (1) unimodal encoders to segment and
represent unimodal data, (2) a coordinated representation space that relates
and composes unimodal features to derive holistic meaning across multimodal
inputs, and (3) decoders to map multimodal representations into predictions
(for fusion) or raw data (for translation or generation). Through discussing
how Brainish is crucial for communication and coordination in order to achieve
consciousness in the CTM, and by implementing a simple version of Brainish and
evaluating its capability of demonstrating intelligence on multimodal
prediction and retrieval tasks on several real-world image, text, and audio
datasets, we argue that such an inner language will be important for advances
in machine models of intelligence and consciousness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What do we Really Know about State of the Art NER?. (arXiv:2205.00034v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00034">
<div class="article-summary-box-inner">
<span><p>Named Entity Recognition (NER) is a well researched NLP task and is widely
used in real world NLP scenarios. NER research typically focuses on the
creation of new ways of training NER, with relatively less emphasis on
resources and evaluation. Further, state of the art (SOTA) NER models, trained
on standard datasets, typically report only a single performance measure
(F-score) and we don't really know how well they do for different entity types
and genres of text, or how robust are they to new, unseen entities. In this
paper, we perform a broad evaluation of NER using a popular dataset, that takes
into consideration various text genres and sources constituting the dataset at
hand. Additionally, we generate six new adversarial test sets through small
perturbations in the original test set, replacing select entities while
retaining the context. We also train and test our models on randomly generated
train/dev/test splits followed by an experiment where the models are trained on
a select set of genres but tested genres not seen in training. These
comprehensive evaluation strategies were performed using three SOTA NER models.
Based on our results, we recommend some useful reporting practices for NER
researchers, that could help in providing a better understanding of a SOTA
model's performance in future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gender Bias in Masked Language Models for Multiple Languages. (arXiv:2205.00551v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00551">
<div class="article-summary-box-inner">
<span><p>Masked Language Models (MLMs) pre-trained by predicting masked tokens on
large corpora have been used successfully in natural language processing tasks
for a variety of languages. Unfortunately, it was reported that MLMs also learn
discriminative biases regarding attributes such as gender and race. Because
most studies have focused on MLMs in English, the bias of MLMs in other
languages has rarely been investigated. Manual annotation of evaluation data
for languages other than English has been challenging due to the cost and
difficulty in recruiting annotators. Moreover, the existing bias evaluation
methods require the stereotypical sentence pairs consisting of the same context
with attribute words (e.g. He/She is a nurse). We propose Multilingual Bias
Evaluation (MBE) score, to evaluate bias in various languages using only
English attribute word lists and parallel corpora between the target language
and English without requiring manually annotated data. We evaluated MLMs in
eight languages using the MBE and confirmed that gender-related biases are
encoded in MLMs for all those languages. We manually created datasets for
gender bias in Japanese and Russian to evaluate the validity of the MBE. The
results show that the bias scores reported by the MBE significantly correlates
with that computed from the above manually created datasets and the existing
English datasets for gender bias.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COSPLAY: Concept Set Guided Personalized Dialogue Generation Across Both Party Personas. (arXiv:2205.00872v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00872">
<div class="article-summary-box-inner">
<span><p>Maintaining a consistent persona is essential for building a human-like
conversational model. However, the lack of attention to the partner makes the
model more egocentric: they tend to show their persona by all means such as
twisting the topic stiffly, pulling the conversation to their own interests
regardless, and rambling their persona with little curiosity to the partner. In
this work, we propose COSPLAY(COncept Set guided PersonaLized dialogue
generation Across both partY personas) that considers both parties as a "team":
expressing self-persona while keeping curiosity toward the partner, leading
responses around mutual personas, and finding the common ground. Specifically,
we first represent self-persona, partner persona and mutual dialogue all in the
concept sets. Then, we propose the Concept Set framework with a suite of
knowledge-enhanced operations to process them such as set algebras, set
expansion, and set distance. Based on these operations as medium, we train the
model by utilizing 1) concepts of both party personas, 2) concept relationship
between them, and 3) their relationship to the future dialogue. Extensive
experiments on a large public dataset, Persona-Chat, demonstrate that our model
outperforms state-of-the-art baselines for generating less egocentric, more
human-like, and higher quality responses in both automatic and human
evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Finding patterns in Knowledge Attribution for Transformers. (arXiv:2205.01366v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01366">
<div class="article-summary-box-inner">
<span><p>We analyze the Knowledge Neurons framework for the attribution of factual and
relational knowledge to particular neurons in the transformer network. We use a
12-layer multi-lingual BERT model for our experiments. Our study reveals
various interesting phenomena. We observe that mostly factual knowledge can be
attributed to middle and higher layers of the network($\ge 6$). Further
analysis reveals that the middle layers($6-9$) are mostly responsible for
relational information, which is further refined into actual factual knowledge
or the "correct answer" in the last few layers($10-12$). Our experiments also
show that the model handles prompts in different languages, but representing
the same fact, similarly, providing further evidence for effectiveness of
multi-lingual pre-training. Applying the attribution scheme for grammatical
knowledge, we find that grammatical knowledge is far more dispersed among the
neurons than factual knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exact Paired-Permutation Testing for Structured Test Statistics. (arXiv:2205.01416v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01416">
<div class="article-summary-box-inner">
<span><p>Significance testing -- especially the paired-permutation test -- has played
a vital role in developing NLP systems to provide confidence that the
difference in performance between two systems (i.e., the test statistic) is not
due to luck. However, practitioners rely on Monte Carlo approximation to
perform this test due to a lack of a suitable exact algorithm. In this paper,
we provide an efficient exact algorithm for the paired-permutation test for a
family of structured test statistics. Our algorithm runs in $\mathcal{O}(GN
(\log GN )(\log N ))$ time where $N$ is the dataset size and $G$ is the range
of the test statistic. We found that our exact algorithm was $10$x faster than
the Monte Carlo approximation with $20000$ samples on a common dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Training for High-Stakes Reliability. (arXiv:2205.01663v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01663">
<div class="article-summary-box-inner">
<span><p>In the future, powerful AI systems may be deployed in high-stakes settings,
where a single failure could be catastrophic. One technique for improving AI
safety in high-stakes settings is adversarial training, which uses an adversary
to generate examples to train on in order to achieve better worst-case
performance.
</p>
<p>In this work, we used a language generation task as a testbed for achieving
high reliability through adversarial training. We created a series of
adversarial training techniques -- including a tool that assists human
adversaries -- to find and eliminate failures in a classifier that filters text
completions suggested by a generator. In our simple "avoid injuries" task, we
determined that we can set very conservative classifier thresholds without
significantly impacting the quality of the filtered outputs. With our chosen
thresholds, filtering with our baseline classifier decreases the rate of unsafe
completions from about 2.4% to 0.003% on in-distribution data, which is near
the limit of our ability to measure. We found that adversarial training
significantly increased robustness to the adversarial attacks that we trained
on, without affecting in-distribution performance. We hope to see further work
in the high-stakes reliability setting, including more powerful tools for
enhancing human adversaries and better ways to measure high levels of
reliability, until we can confidently rule out the possibility of catastrophic
deployment-time failures of powerful models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TIB's Visual Analytics Group at MediaEval '20: Detecting Fake News on Corona Virus and 5G Conspiracy. (arXiv:2101.03529v1 [cs.SI] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.03529">
<div class="article-summary-box-inner">
<span><p>Fake news on social media has become a hot topic of research as it negatively
impacts the discourse of real news in the public. Specifically, the ongoing
COVID-19 pandemic has seen a rise of inaccurate and misleading information due
to the surrounding controversies and unknown details at the beginning of the
pandemic. The FakeNews task at MediaEval 2020 tackles this problem by creating
a challenge to automatically detect tweets containing misinformation based on
text and structure from Twitter follower network. In this paper, we present a
simple approach that uses BERT embeddings and a shallow neural network for
classifying tweets using only text, and discuss our findings and limitations of
the approach in text-based misinformation detection.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">The scope for AI-augmented interpretation of building blueprints in commercial and industrial property insurance. (arXiv:2205.01671v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01671">
<div class="article-summary-box-inner">
<span><p>This report, commissioned by the WTW research network, investigates the use
of AI in property risk assessment. It (i) reviews existing work on risk
assessment in commercial and industrial properties and automated information
extraction from building blueprints; and (ii) presents an exploratory 'proof-of
concept-solution' exploring the feasibility of using machine learning for the
automated extraction of information from building blueprints to support
insurance risk assessment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Deep Learning-based Integrated Framework for Quality-aware Undersampled Cine Cardiac MRI Reconstruction and Analysis. (arXiv:2205.01673v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01673">
<div class="article-summary-box-inner">
<span><p>Cine cardiac magnetic resonance (CMR) imaging is considered the gold standard
for cardiac function evaluation. However, cine CMR acquisition is inherently
slow and in recent decades considerable effort has been put into accelerating
scan times without compromising image quality or the accuracy of derived
results. In this paper, we present a fully-automated, quality-controlled
integrated framework for reconstruction, segmentation and downstream analysis
of undersampled cine CMR data. The framework enables active acquisition of
radial k-space data, in which acquisition can be stopped as soon as acquired
data are sufficient to produce high quality reconstructions and segmentations.
This results in reduced scan times and automated analysis, enabling robust and
accurate estimation of functional biomarkers. To demonstrate the feasibility of
the proposed approach, we perform realistic simulations of radial k-space
acquisitions on a dataset of subjects from the UK Biobank and present results
on in-vivo cine CMR k-space data collected from healthy subjects. The results
demonstrate that our method can produce quality-controlled images in a mean
scan time reduced from 12 to 4 seconds per slice, and that image quality is
sufficient to allow clinically relevant parameters to be automatically
estimated to within 5% mean absolute difference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MIRST-DM: Multi-Instance RST with Drop-Max Layer for Robust Classification of Breast Cancer. (arXiv:2205.01674v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01674">
<div class="article-summary-box-inner">
<span><p>Robust self-training (RST) can augment the adversarial robustness of image
classification models without significantly sacrificing models'
generalizability. However, RST and other state-of-the-art defense approaches
failed to preserve the generalizability and reproduce their good adversarial
robustness on small medical image sets. In this work, we propose the
Multi-instance RST with a drop-max layer, namely MIRST-DM, which involves a
sequence of iteratively generated adversarial instances during training to
learn smoother decision boundaries on small datasets. The proposed drop-max
layer eliminates unstable features and helps learn representations that are
robust to image perturbations. The proposed approach was validated using a
small breast ultrasound dataset with 1,190 images. The results demonstrate that
the proposed approach achieves state-of-the-art adversarial robustness against
three prevalent attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning Framework for Real-time Fetal Brain Segmentation in MRI. (arXiv:2205.01675v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01675">
<div class="article-summary-box-inner">
<span><p>Fetal brain segmentation is an important first step for slice-level motion
correction and slice-to-volume reconstruction in fetal MRI. Fast and accurate
segmentation of the fetal brain on fetal MRI is required to achieve real-time
fetal head pose estimation and motion tracking for slice re-acquisition and
steering. To address this critical unmet need, in this work we analyzed the
speed-accuracy performance of a variety of deep neural network models, and
devised a symbolically small convolutional neural network that combines spatial
details at high resolution with context features extracted at lower
resolutions. We used multiple branches with skip connections to maintain high
accuracy while devising a parallel combination of convolution and pooling
operations as an input downsampling module to further reduce inference time. We
trained our model as well as eight alternative, state-of-the-art networks with
manually-labeled fetal brain MRI slices and tested on two sets of normal and
challenging test cases. Experimental results show that our network achieved the
highest accuracy and lowest inference time among all of the compared
state-of-the-art real-time segmentation methods. We achieved average Dice
scores of 97.99\% and 84.04\% on the normal and challenging test sets,
respectively, with an inference time of 3.36 milliseconds per image on an
NVIDIA GeForce RTX 2080 Ti. Code, data, and the trained models are available at
https://github.com/bchimagine/real_time_fetal_brain_segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FundusQ-Net: a Regression Quality Assessment Deep Learning Algorithm for Fundus Images Quality Grading. (arXiv:2205.01676v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01676">
<div class="article-summary-box-inner">
<span><p>Objective: Ophthalmological pathologies such as glaucoma, diabetic
retinopathy and age-related macular degeneration are major causes of blindness
and vision impairment. There is a need for novel decision support tools that
can simplify and speed up the diagnosis of these pathologies. A key step in
this process is to automatically estimate the quality of the fundus images to
make sure these are interpretable by a human operator or a machine learning
model. We present a novel fundus image quality scale and deep learning (DL)
model that can estimate fundus image quality relative to this new scale.
</p>
<p>Methods: A total of 1,245 images were graded for quality by two
ophthalmologists within the range 1-10, with a resolution of 0.5. A DL
regression model was trained for fundus image quality assessment. The
architecture used was Inception-V3. The model was developed using a total of
89,947 images from 6 databases, of which 1,245 were labeled by the specialists
and the remaining 88,702 images were used for pre-training and semi-supervised
learning. The final DL model was evaluated on an internal test set (n=209) as
well as an external test set (n=194).
</p>
<p>Results: The final DL model, denoted FundusQ-Net, achieved a mean absolute
error of 0.61 (0.54-0.68) on the internal test set. When evaluated as a binary
classification model on the public DRIMDB database as an external test set the
model obtained an accuracy of 99%.
</p>
<p>Significance: the proposed algorithm provides a new robust tool for automated
quality grading of fundus images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Physics to the Rescue: Deep Non-line-of-sight Reconstruction for High-speed Imaging. (arXiv:2205.01679v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01679">
<div class="article-summary-box-inner">
<span><p>Computational approach to imaging around the corner, or non-line-of-sight
(NLOS) imaging, is becoming a reality thanks to major advances in imaging
hardware and reconstruction algorithms. A recent development towards practical
NLOS imaging, Nam et al. demonstrated a high-speed non-confocal imaging system
that operates at 5Hz, 100x faster than the prior art. This enormous gain in
acquisition rate, however, necessitates numerous approximations in light
transport, breaking many existing NLOS reconstruction methods that assume an
idealized image formation model. To bridge the gap, we present a novel deep
model that incorporates the complementary physics priors of wave propagation
and volume rendering into a neural network for high-quality and robust NLOS
reconstruction. This orchestrated design regularizes the solution space by
relaxing the image formation model, resulting in a deep model that generalizes
well on real captures despite being exclusively trained on synthetic data.
Further, we devise a unified learning framework that enables our model to be
flexibly trained using diverse supervision signals, including target intensity
images or even raw NLOS transient measurements. Once trained, our model renders
both intensity and depth images at inference time in a single forward pass,
capable of processing more than 5 captures per second on a high-end GPU.
Through extensive qualitative and quantitative experiments, we show that our
method outperforms prior physics and learning based approaches on both
synthetic and real measurements. We anticipate that our method along with the
fast capturing system will accelerate future development of NLOS imaging for
real world applications that require high-speed imaging.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SpineNetV2: Automated Detection, Labelling and Radiological Grading Of Clinical MR Scans. (arXiv:2205.01683v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01683">
<div class="article-summary-box-inner">
<span><p>This technical report presents SpineNetV2, an automated tool which: (i)
detects and labels vertebral bodies in clinical spinal magnetic resonance (MR)
scans across a range of commonly used sequences; and (ii) performs radiological
grading of lumbar intervertebral discs in T2-weighted scans for a range of
common degenerative changes. SpineNetV2 improves over the original SpineNet
software in two ways: (1) The vertebral body detection stage is significantly
faster, more accurate and works across a range of fields-of-view (as opposed to
just lumbar scans). (2) Radiological grading adopts a more powerful
architecture, adding several new grading schemes without loss in performance. A
demo of the software is available at the project website:
<a href="http://zeus.robots.ox.ac.uk/spinenet2/.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effect of Random Histogram Equalization on Breast Calcification Analysis Using Deep Learning. (arXiv:2205.01684v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01684">
<div class="article-summary-box-inner">
<span><p>Early detection and analysis of calcifications in mammogram images is crucial
in a breast cancer diagnosis workflow. Management of calcifications that
require immediate follow-up and further analyzing its benignancy or malignancy
can result in a better prognosis. Recent studies have shown that deep
learning-based algorithms can learn robust representations to analyze
suspicious calcifications in mammography. In this work, we demonstrate that
randomly equalizing the histograms of calcification patches as a data
augmentation technique can significantly improve the classification performance
for analyzing suspicious calcifications. We validate our approach by using the
CBIS-DDSM dataset for two classification tasks. The results on both the tasks
show that the proposed methodology gains more than 1% mean accuracy and
F1-score when equalizing the data with a probability of 0.4 when compared to
not using histogram equalization. This is further supported by the t-tests,
where we obtain a p-value of p&lt;0.0001, thus showing the statistical
significance of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Smart City Intersections: Intelligence Nodes for Future Metropolises. (arXiv:2205.01686v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01686">
<div class="article-summary-box-inner">
<span><p>Traffic intersections are the most suitable locations for the deployment of
computing, communications, and intelligence services for smart cities of the
future. The abundance of data to be collected and processed, in combination
with privacy and security concerns, motivates the use of the edge-computing
paradigm which aligns well with physical intersections in metropolises. This
paper focuses on high-bandwidth, low-latency applications, and in that context
it describes: (i) system design considerations for smart city intersection
intelligence nodes; (ii) key technological components including sensors,
networking, edge computing, low latency design, and AI-based intelligence; and
(iii) applications such as privacy preservation, cloud-connected vehicles, a
real-time "radar-screen", traffic management, and monitoring of pedestrian
behavior during pandemics. The results of the experimental studies performed on
the COSMOS testbed located in New York City are illustrated. Future challenges
in designing human-centered smart city intersections are summarized.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End2End Multi-View Feature Matching using Differentiable Pose Optimization. (arXiv:2205.01694v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01694">
<div class="article-summary-box-inner">
<span><p>Learning-based approaches have become indispensable for camera pose
estimation. However, feature detection, description, matching, and pose
optimization are often approached in an isolated fashion. In particular,
erroneous feature matches have severe impact on subsequent camera pose
estimation and often require additional measures such as outlier rejection. Our
method tackles this challenge by addressing feature matching and pose
optimization jointly: first, we integrate information from multiple views into
the matching by spanning a graph attention network across multiple frames to
predict their matches all at once. Second, the resulting matches along with
their predicted confidences are used for robust pose optimization with a
differentiable Gauss-Newton solver. End-to-end training combined with
multi-view feature matching boosts the pose estimation metrics compared to
SuperGlue by 8.9% on ScanNet and 10.7% on MegaDepth on average. Our approach
improves both pose estimation and matching accuracy over state-of-the-art
matching networks. Training feature matching across multiple views with
gradients from pose optimization naturally learns to disregard outliers,
thereby rendering additional outlier handling unnecessary, which is highly
desirable for pose estimation systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Object Class Aware Video Anomaly Detection through Image Translation. (arXiv:2205.01706v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01706">
<div class="article-summary-box-inner">
<span><p>Semi-supervised video anomaly detection (VAD) methods formulate the task of
anomaly detection as detection of deviations from the learned normal patterns.
Previous works in the field (reconstruction or prediction-based methods) suffer
from two drawbacks: 1) They focus on low-level features, and they (especially
holistic approaches) do not effectively consider the object classes. 2)
Object-centric approaches neglect some of the context information (such as
location). To tackle these challenges, this paper proposes a novel two-stream
object-aware VAD method that learns the normal appearance and motion patterns
through image translation tasks. The appearance branch translates the input
image to the target semantic segmentation map produced by Mask-RCNN, and the
motion branch associates each frame with its expected optical flow magnitude.
Any deviation from the expected appearance or motion in the inference stage
shows the degree of potential abnormality. We evaluated our proposed method on
the ShanghaiTech, UCSD-Ped1, and UCSD-Ped2 datasets and the results show
competitive performance compared with state-of-the-art works. Most importantly,
the results show that, as significant improvements to previous methods,
detections by our method are completely explainable and anomalies are localized
accurately in the frames.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In Defense of Image Pre-Training for Spatiotemporal Recognition. (arXiv:2205.01721v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01721">
<div class="article-summary-box-inner">
<span><p>Image pre-training, the current de-facto paradigm for a wide range of visual
tasks, is generally less favored in the field of video recognition. By
contrast, a common strategy is to directly train with spatiotemporal
convolutional neural networks (CNNs) from scratch. Nonetheless, interestingly,
by taking a closer look at these from-scratch learned CNNs, we note there exist
certain 3D kernels that exhibit much stronger appearance modeling ability than
others, arguably suggesting appearance information is already well disentangled
in learning. Inspired by this observation, we hypothesize that the key to
effectively leveraging image pre-training lies in the decomposition of learning
spatial and temporal features, and revisiting image pre-training as the
appearance prior to initializing 3D kernels. In addition, we propose
Spatial-Temporal Separable (STS) convolution, which explicitly splits the
feature channels into spatial and temporal groups, to further enable a more
thorough decomposition of spatiotemporal features for fine-tuning 3D CNNs. Our
experiments show that simply replacing 3D convolution with STS notably improves
a wide range of 3D CNNs without increasing parameters and computation on both
Kinetics-400 and Something-Something V2. Moreover, this new training pipeline
consistently achieves better results on video recognition with significant
speedup. For instance, we achieve +0.6% top-1 of Slowfast on Kinetics-400 over
the strong 256-epoch 128-GPU baseline while fine-tuning for only 50 epochs with
4 GPUs. The code and models are available at
https://github.com/UCSC-VLAA/Image-Pretraining-for-Video.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">License Plate Privacy in Collaborative Visual Analysis of Traffic Scenes. (arXiv:2205.01724v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01724">
<div class="article-summary-box-inner">
<span><p>Traffic scene analysis is important for emerging technologies such as smart
traffic management and autonomous vehicles. However, such analysis also poses
potential privacy threats. For example, a system that can recognize license
plates may construct patterns of behavior of the corresponding vehicles' owners
and use that for various illegal purposes. In this paper we present a system
that enables traffic scene analysis while at the same time preserving license
plate privacy. The system is based on a multi-task model whose latent space is
selectively compressed depending on the amount of information the specific
features carry about analysis tasks and private information. Effectiveness of
the proposed method is illustrated by experiments on the Cityscapes dataset,
for which we also provide license plate annotations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Application of belief functions to medical image segmentation: A review. (arXiv:2205.01733v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01733">
<div class="article-summary-box-inner">
<span><p>Belief function theory, a formal framework for uncertainty analysis and
multiple evidence fusion, has made significant contributions in the medical
domain, especially since the development of deep learning. Medical image
segmentation with belief function theory has shown significant benefits in
clinical diagnosis and medical image research. In this paper, we provide a
review of medical image segmentation methods using belief function theory. We
classify the methods according to the fusion step and explain how information
with uncertainty or imprecision is modeled and fused with belief function
theory. In addition, we discuss the challenges and limitations of present
belief function-based medical image segmentation and propose orientations for
future research. Future research could investigate both belief function theory
and deep learning to achieve more promising and reliable segmentation results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparison of CoModGANs, LaMa and GLIDE for Art Inpainting- Completing M.C Escher's Print Gallery. (arXiv:2205.01741v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01741">
<div class="article-summary-box-inner">
<span><p>Digital art restoration has benefited from inpainting models to correct the
degradation or missing sections of a painting. This work compares three current
state-of-the art models for inpainting of large missing regions. We provide
qualitative and quantitative comparison of the performance by CoModGANs, LaMa
and GLIDE in inpainting of blurry and missing sections of images. We use
Escher's incomplete painting Print Gallery as our test study since it presents
several of the challenges commonly present in restorative inpainting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data-Consistent Non-Cartesian Deep Subspace Learning for Efficient Dynamic MR Image Reconstruction. (arXiv:2205.01770v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01770">
<div class="article-summary-box-inner">
<span><p>Non-Cartesian sampling with subspace-constrained image reconstruction is a
popular approach to dynamic MRI, but slow iterative reconstruction limits its
clinical application. Data-consistent (DC) deep learning can accelerate
reconstruction with good image quality, but has not been formulated for
non-Cartesian subspace imaging. In this study, we propose a DC non-Cartesian
deep subspace learning framework for fast, accurate dynamic MR image
reconstruction. Four novel DC formulations are developed and evaluated: two
gradient decent approaches, a directly solved approach, and a conjugate
gradient approach. We applied a U-Net model with and without DC layers to
reconstruct T1-weighted images for cardiac MR Multitasking (an advanced
multidimensional imaging method), comparing our results to the iteratively
reconstructed reference. Experimental results show that the proposed framework
significantly improves reconstruction accuracy over the U-Net model without DC,
while significantly accelerating the reconstruction over conventional iterative
reconstruction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Os Dados dos Brasileiros sob Risco na Era da Intelig\^encia Artificial?. (arXiv:2205.01772v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01772">
<div class="article-summary-box-inner">
<span><p>Advances in image processing and analysis as well as machine learning
techniques have contributed to the use of biometric recognition systems in
daily people tasks. These tasks range from simple access to mobile devices to
tagging friends in photos shared on social networks and complex financial
operations on self-service devices for banking transactions. In China, the use
of these systems goes beyond personal use becoming a country's government
policy with the objective of monitoring the behavior of its population. On July
05th 2021, the Brazilian government announced acquisition of a biometric
recognition system to be used nationwide. In the opposite direction to China,
Europe and some American cities have already started the discussion about the
legality of using biometric systems in public places, even banning this
practice in their territory. In order to open a deeper discussion about the
risks and legality of using these systems, this work exposes the
vulnerabilities of biometric recognition systems, focusing its efforts on the
face modality. Furthermore, it shows how it is possible to fool a biometric
system through a well-known presentation attack approach in the literature
called morphing. Finally, a list of ten concerns was created to start the
discussion about the security of citizen data and data privacy law in the Age
of Artificial Intelligence (AI).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Multi-Scale U-Net Architecture and Noise-Robust Training Strategies for Histopathological Image Segmentation. (arXiv:2205.01777v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01777">
<div class="article-summary-box-inner">
<span><p>Although the U-Net architecture has been extensively used for segmentation of
medical images, we address two of its shortcomings in this work. Firstly, the
accuracy of vanilla U-Net degrades when the target regions for segmentation
exhibit significant variations in shape and size. Even though the U-Net already
possesses some capability to analyze features at various scales, we propose to
explicitly add multi-scale feature maps in each convolutional module of the
U-Net encoder to improve segmentation of histology images. Secondly, the
accuracy of a U-Net model also suffers when the annotations for supervised
learning are noisy or incomplete. This can happen due to the inherent
difficulty for a human expert to identify and delineate all instances of
specific pathology very precisely and accurately. We address this challenge by
introducing auxiliary confidence maps that emphasize less on the boundaries of
the given target regions. Further, we utilize the bootstrapping properties of
the deep network to address the missing annotation problem intelligently. In
our experiments on a private dataset of breast cancer lymph nodes, where the
primary task was to segment germinal centres and sinus histiocytosis, we
observed substantial improvement over a U-Net baseline based on the two
proposed augmentations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Multi-dimensional Edge Feature-based AU Relation Graph for Facial Action Unit Recognition. (arXiv:2205.01782v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01782">
<div class="article-summary-box-inner">
<span><p>The activations of Facial Action Units (AUs) mutually influence one another.
While the relationship between a pair of AUs can be complex and unique,
existing approaches fail to specifically and explicitly represent such cues for
each pair of AUs in each facial display. This paper proposes an AU relationship
modelling approach that deep learns a unique graph to explicitly describe the
relationship between each pair of AUs of the target facial display. Our
approach first encodes each AU's activation status and its association with
other AUs into a node feature. Then, it learns a pair of multi-dimensional edge
features to describe multiple task-specific relationship cues between each pair
of AUs. During both node and edge feature learning, our approach also considers
the influence of the unique facial display on AUs' relationship by taking the
full face representation as an input. Experimental results on BP4D and DISFA
datasets show that both node and edge feature learning modules provide large
performance improvements for CNN and transformer-based backbones, with our best
systems achieving the state-of-the-art AU recognition results. Our approach not
only has a strong capability in modelling relationship cues for AU recognition
but also can be easily incorporated into various backbones. Our PyTorch code is
made available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Pre-study on Data Processing Pipelines for Roadside Object Detection Systems Towards Safer Road Infrastructure. (arXiv:2205.01783v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01783">
<div class="article-summary-box-inner">
<span><p>Single-vehicle accidents are the most common type of fatal accidents in
Sweden, where a car drives off the road and runs into hazardous roadside
objects. Proper installation and maintenance of protective objects, such as
crash cushions and guard rails, may reduce the chance and severity of such
accidents. Moreover, efficient detection and management of hazardous roadside
objects also plays an important role in improving road safety. To better
understand the state-of-the-art and system requirements, in this pre-study, we
investigate the feasibility, implementation, limitations and scaling up of data
processing pipelines for roadside object detection. In particular, we divide
our investigation into three parts: the target of interest, the sensors of
choice and the algorithm design. The data sources we consider in this study
cover two common setups: 1) road surveying fleet - annual scans conducted by
Trafikverket, the Swedish Transport Administration, and 2) consumer vehicle -
data collected using a research vehicle from the laboratory of Resource for
vehicle research at Chalmers (REVERE). The goal of this report is to
investigate how to implement a scalable roadside object detection system
towards safe road infrastructure and Sweden's Vision Zero.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synthesized Speech Detection Using Convolutional Transformer-Based Spectrogram Analysis. (arXiv:2205.01800v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01800">
<div class="article-summary-box-inner">
<span><p>Synthesized speech is common today due to the prevalence of virtual
assistants, easy-to-use tools for generating and modifying speech signals, and
remote work practices. Synthesized speech can also be used for nefarious
purposes, including creating a purported speech signal and attributing it to
someone who did not speak the content of the signal. We need methods to detect
if a speech signal is synthesized. In this paper, we analyze speech signals in
the form of spectrograms with a Compact Convolutional Transformer (CCT) for
synthesized speech detection. A CCT utilizes a convolutional layer that
introduces inductive biases and shared weights into a network, allowing a
transformer architecture to perform well with fewer data samples used for
training. The CCT uses an attention mechanism to incorporate information from
all parts of a signal under analysis. Trained on both genuine human voice
signals and synthesized human voice signals, we demonstrate that our CCT
approach successfully differentiates between genuine and synthesized speech
signals.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Splicing Detection and Localization In Satellite Imagery Using Conditional GANs. (arXiv:2205.01805v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01805">
<div class="article-summary-box-inner">
<span><p>The widespread availability of image editing tools and improvements in image
processing techniques allow image manipulation to be very easy. Oftentimes,
easy-to-use yet sophisticated image manipulation tools yields
distortions/changes imperceptible to the human observer. Distribution of forged
images can have drastic ramifications, especially when coupled with the speed
and vastness of the Internet. Therefore, verifying image integrity poses an
immense and important challenge to the digital forensic community. Satellite
images specifically can be modified in a number of ways, including the
insertion of objects to hide existing scenes and structures. In this paper, we
describe the use of a Conditional Generative Adversarial Network (cGAN) to
identify the presence of such spliced forgeries within satellite images.
Additionally, we identify their locations and shapes. Trained on pristine and
falsified images, our method achieves high success on these detection and
localization objectives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Frequency Domain-Based Detection of Generated Audio. (arXiv:2205.01806v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01806">
<div class="article-summary-box-inner">
<span><p>Attackers may manipulate audio with the intent of presenting falsified
reports, changing an opinion of a public figure, and winning influence and
power. The prevalence of inauthentic multimedia continues to rise, so it is
imperative to develop a set of tools that determines the legitimacy of media.
We present a method that analyzes audio signals to determine whether they
contain real human voices or fake human voices (i.e., voices generated by
neural acoustic and waveform models). Instead of analyzing the audio signals
directly, the proposed approach converts the audio signals into spectrogram
images displaying frequency, intensity, and temporal content and evaluates them
with a Convolutional Neural Network (CNN). Trained on both genuine human voice
signals and synthesized voice signals, we show our approach achieves high
accuracy on this classification task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assessing Dataset Bias in Computer Vision. (arXiv:2205.01811v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01811">
<div class="article-summary-box-inner">
<span><p>A biased dataset is a dataset that generally has attributes with an uneven
class distribution. These biases have the tendency to propagate to the models
that train on them, often leading to a poor performance in the minority class.
In this project, we will explore the extent to which various data augmentation
methods alleviate intrinsic biases within the dataset. We will apply several
augmentation techniques on a sample of the UTKFace dataset, such as
undersampling, geometric transformations, variational autoencoders (VAEs), and
generative adversarial networks (GANs). We then trained a classifier for each
of the augmented datasets and evaluated their performance on the native test
set and on external facial recognition datasets. We have also compared their
performance to the state-of-the-art attribute classifier trained on the
FairFace dataset. Through experimentation, we were able to find that training
the model on StarGAN-generated images led to the best overall performance. We
also found that training on geometrically transformed images lead to a similar
performance with a much quicker training time. Additionally, the best
performing models also exhibit a uniform performance across the classes within
each attribute. This signifies that the model was also able to mitigate the
biases present in the baseline model that was trained on the original training
set. Finally, we were able to show that our model has a better overall
performance and consistency on age and ethnicity classification on multiple
datasets when compared with the FairFace model. Our final model has an accuracy
on the UTKFace test set of 91.75%, 91.30%, and 87.20% for the gender, age, and
ethnicity attribute respectively, with a standard deviation of less than 0.1
between the accuracies of the classes of each attribute.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diverse Image Captioning with Grounded Style. (arXiv:2205.01813v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01813">
<div class="article-summary-box-inner">
<span><p>Stylized image captioning as presented in prior work aims to generate
captions that reflect characteristics beyond a factual description of the scene
composition, such as sentiments. Such prior work relies on given sentiment
identifiers, which are used to express a certain global style in the caption,
e.g. positive or negative, however without taking into account the stylistic
content of the visual scene. To address this shortcoming, we first analyze the
limitations of current stylized captioning datasets and propose COCO
attribute-based augmentations to obtain varied stylized captions from COCO
annotations. Furthermore, we encode the stylized information in the latent
space of a Variational Autoencoder; specifically, we leverage extracted image
attributes to explicitly structure its sequential latent space according to
different localized style characteristics. Our experiments on the Senticap and
COCO datasets show the ability of our approach to generate accurate captions
with diversity in styles that are grounded in the image.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">i-Code: An Integrative and Composable Multimodal Learning Framework. (arXiv:2205.01818v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01818">
<div class="article-summary-box-inner">
<span><p>Human intelligence is multimodal; we integrate visual, linguistic, and
acoustic signals to maintain a holistic worldview. Most current pretraining
methods, however, are limited to one or two modalities. We present i-Code, a
self-supervised pretraining framework where users may flexibly combine the
modalities of vision, speech, and language into unified and general-purpose
vector representations. In this framework, data from each modality are first
given to pretrained single-modality encoders. The encoder outputs are then
integrated with a multimodal fusion network, which uses novel attention
mechanisms and other architectural innovations to effectively combine
information from the different modalities. The entire system is pretrained
end-to-end with new objectives including masked modality unit modeling and
cross-modality contrastive learning. Unlike previous research using only video
for pretraining, the i-Code framework can dynamically process single, dual, and
triple-modality data during training and inference, flexibly projecting
different combinations of modalities into a single representation space.
Experimental results demonstrate how i-Code can outperform state-of-the-art
techniques on five video understanding tasks and the GLUE NLP benchmark,
improving by as much as 11% and demonstrating the power of integrative
multimodal pretraining.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FedMix: Mixed Supervised Federated Learning for Medical Image Segmentation. (arXiv:2205.01840v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01840">
<div class="article-summary-box-inner">
<span><p>The purpose of federated learning is to enable multiple clients to jointly
train a machine learning model without sharing data. However, the existing
methods for training an image segmentation model have been based on an
unrealistic assumption that the training set for each local client is annotated
in a similar fashion and thus follows the same image supervision level. To
relax this assumption, in this work, we propose a label-agnostic unified
federated learning framework, named FedMix, for medical image segmentation
based on mixed image labels. In FedMix, each client updates the federated model
by integrating and effectively making use of all available labeled data ranging
from strong pixel-level labels, weak bounding box labels, to weakest
image-level class labels. Based on these local models, we further propose an
adaptive weight assignment procedure across local clients, where each client
learns an aggregation weight during the global model update. Compared to the
existing methods, FedMix not only breaks through the constraint of a single
level of image supervision, but also can dynamically adjust the aggregation
weight of each local client, achieving rich yet discriminative feature
representations. To evaluate its effectiveness, experiments have been carried
out on two challenging medical image segmentation tasks, i.e., breast tumor
segmentation and skin lesion segmentation. The results validate that our
proposed FedMix outperforms the state-of-the-art method by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Commonsense in Pretrained Unimodal and Multimodal Models. (arXiv:2205.01850v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01850">
<div class="article-summary-box-inner">
<span><p>Our commonsense knowledge about objects includes their typical visual
attributes; we know that bananas are typically yellow or green, and not purple.
Text and image corpora, being subject to reporting bias, represent this
world-knowledge to varying degrees of faithfulness. In this paper, we
investigate to what degree unimodal (language-only) and multimodal (image and
language) models capture a broad range of visually salient attributes. To that
end, we create the Visual Commonsense Tests (ViComTe) dataset covering 5
property types (color, shape, material, size, and visual co-occurrence) for
over 5000 subjects. We validate this dataset by showing that our grounded color
data correlates much better than ungrounded text-only data with crowdsourced
color judgments provided by Paik et al. (2021). We then use our dataset to
evaluate pretrained unimodal models and multimodal models. Our results indicate
that multimodal models better reconstruct attribute distributions, but are
still subject to reporting bias. Moreover, increasing model size does not
enhance performance, suggesting that the key to visual commonsense lies in the
data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UCL-Dehaze: Towards Real-world Image Dehazing via Unsupervised Contrastive Learning. (arXiv:2205.01871v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01871">
<div class="article-summary-box-inner">
<span><p>While the wisdom of training an image dehazing model on synthetic hazy data
can alleviate the difficulty of collecting real-world hazy/clean image pairs,
it brings the well-known domain shift problem. From a different yet new
perspective, this paper explores contrastive learning with an adversarial
training effort to leverage unpaired real-world hazy and clean images, thus
bridging the gap between synthetic and real-world haze is avoided. We propose
an effective unsupervised contrastive learning paradigm for image dehazing,
dubbed UCL-Dehaze. Unpaired real-world clean and hazy images are easily
captured, and will serve as the important positive and negative samples
respectively when training our UCL-Dehaze network. To train the network more
effectively, we formulate a new self-contrastive perceptual loss function,
which encourages the restored images to approach the positive samples and keep
away from the negative samples in the embedding space. Besides the overall
network architecture of UCL-Dehaze, adversarial training is utilized to align
the distributions between the positive samples and the dehazed images. Compared
with recent image dehazing works, UCL-Dehaze does not require paired data
during training and utilizes unpaired positive/negative data to better enhance
the dehazing performance. We conduct comprehensive experiments to evaluate our
UCL-Dehaze and demonstrate its superiority over the state-of-the-arts, even
only 1,800 unpaired real-world images are used to train our network. Source
code has been available at https://github.com/yz-wang/UCL-Dehaze.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Image Compression and Denoising via Latent-Space Scalability. (arXiv:2205.01874v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01874">
<div class="article-summary-box-inner">
<span><p>When it comes to image compression in digital cameras, denoising is
traditionally performed prior to compression. However, there are applications
where image noise may be necessary to demonstrate the trustworthiness of the
image, such as court evidence and image forensics. This means that noise itself
needs to be coded, in addition to the clean image itself. In this paper, we
present a learnt image compression framework where image denoising and
compression are performed jointly. The latent space of the image codec is
organized in a scalable manner such that the clean image can be decoded from a
subset of the latent space at a lower rate, while the noisy image is decoded
from the full latent space at a higher rate. The proposed codec is compared
against established compression and denoising benchmarks, and the experiments
reveal considerable bitrate savings of up to 80% compared to cascade
compression and denoising.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">All You May Need for VQA are Image Captions. (arXiv:2205.01883v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01883">
<div class="article-summary-box-inner">
<span><p>Visual Question Answering (VQA) has benefited from increasingly sophisticated
models, but has not enjoyed the same level of engagement in terms of data
creation. In this paper, we propose a method that automatically derives VQA
examples at volume, by leveraging the abundance of existing image-caption
annotations combined with neural models for textual question generation. We
show that the resulting data is of high-quality. VQA models trained on our data
improve state-of-the-art zero-shot accuracy by double digits and achieve a
level of robustness that lacks in the same model trained on human-annotated VQA
data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Domain Adaptation Learning for Hierarchical Infant Pose Recognition with Synthetic Data. (arXiv:2205.01892v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01892">
<div class="article-summary-box-inner">
<span><p>The Alberta Infant Motor Scale (AIMS) is a well-known assessment scheme that
evaluates the gross motor development of infants by recording the number of
specific poses achieved. With the aid of the image-based pose recognition
model, the AIMS evaluation procedure can be shortened and automated, providing
early diagnosis or indicator of potential developmental disorder. Due to
limited public infant-related datasets, many works use the SMIL-based method to
generate synthetic infant images for training. However, this domain mismatch
between real and synthetic training samples often leads to performance
degradation during inference. In this paper, we present a CNN-based model which
takes any infant image as input and predicts the coarse and fine-level pose
labels. The model consists of an image branch and a pose branch, which
respectively generates the coarse-level logits facilitated by the unsupervised
domain adaptation and the 3D keypoints using the HRNet with SMPLify
optimization. Then the outputs of these branches will be sent into the
hierarchical pose recognition module to estimate the fine-level pose labels. We
also collect and label a new AIMS dataset, which contains 750 real and 4000
synthetic infants images with AIMS pose labels. Our experimental results show
that the proposed method can significantly align the distribution of synthetic
and real-world datasets, thus achieving accurate performance on fine-grained
infant pose recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pik-Fix: Restoring and Colorizing Old Photo. (arXiv:2205.01902v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01902">
<div class="article-summary-box-inner">
<span><p>Restoring and inpainting the visual memories that are present, but often
impaired, in old photos remains an intriguing but unsolved research topic.
Decades-old photos often suffer from severe and commingled degradation such as
cracks, defocus, and color-fading, which are difficult to treat individually
and harder to repair when they interact. Deep learning presents a plausible
avenue, but the lack of large-scale datasets of old photos makes addressing
this restoration task very challenging. Here we present a novel reference-based
end-to-end learning framework that is able to both repair and colorize old and
degraded pictures. Our proposed framework consists of three modules: a
restoration sub-network that conducts restoration from degradations, a
similarity sub-network that performs color histogram matching and color
transfer, and a colorization subnet that learns to predict the chroma elements
of images that have been conditioned on chromatic reference signals. The
overall system makes uses of color histogram priors from reference images,
which greatly reduces the need for large-scale training data. We have also
created a first-of-a-kind public dataset of real old photos that are paired
with ground truth "pristine" photos that have been that have been manually
restored by PhotoShop experts. We conducted extensive experiments on this
dataset and synthetic datasets, and found that our method significantly
outperforms previous state-of-the-art models using both qualitative comparisons
and quantitative measurements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Taught Metric Learning without Labels. (arXiv:2205.01903v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01903">
<div class="article-summary-box-inner">
<span><p>We present a novel self-taught framework for unsupervised metric learning,
which alternates between predicting class-equivalence relations between data
through a moving average of an embedding model and learning the model with the
predicted relations as pseudo labels. At the heart of our framework lies an
algorithm that investigates contexts of data on the embedding space to predict
their class-equivalence relations as pseudo labels. The algorithm enables
efficient end-to-end training since it demands no off-the-shelf module for
pseudo labeling. Also, the class-equivalence relations provide rich supervisory
signals for learning an embedding space. On standard benchmarks for metric
learning, it clearly outperforms existing unsupervised learning methods and
sometimes even beats supervised learning models using the same backbone
network. It is also applied to semi-supervised metric learning as a way of
exploiting additional unlabeled data, and achieves the state of the art by
boosting performance of supervised learning substantially.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalized Knowledge Distillation via Relationship Matching. (arXiv:2205.01915v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01915">
<div class="article-summary-box-inner">
<span><p>The knowledge of a well-trained deep neural network (a.k.a. the "teacher") is
valuable for learning similar tasks. Knowledge distillation extracts knowledge
from the teacher and integrates it with the target model (a.k.a. the
"student"), which expands the student's knowledge and improves its learning
efficacy. Instead of enforcing the teacher to work on the same task as the
student, we borrow the knowledge from a teacher trained from a general label
space -- in this "Generalized Knowledge Distillation (GKD)", the classes of the
teacher and the student may be the same, completely different, or partially
overlapped. We claim that the comparison ability between instances acts as an
essential factor threading knowledge across tasks, and propose the RElationship
FacIlitated Local cLassifiEr Distillation (REFILLED) approach, which decouples
the GKD flow of the embedding and the top-layer classifier. In particular,
different from reconciling the instance-label confidence between models,
REFILLED requires the teacher to reweight the hard tuples pushed forward by the
student and then matches the similarity comparison levels between instances. An
embedding-induced classifier based on the teacher model supervises the
student's classification confidence and adaptively emphasizes the most related
supervision from the teacher. REFILLED demonstrates strong discriminative
ability when the classes of the teacher vary from the same to a fully
non-overlapped set w.r.t. the student. It also achieves state-of-the-art
performance on standard knowledge distillation, one-step incremental learning,
and few-shot learning tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoCa: Contrastive Captioners are Image-Text Foundation Models. (arXiv:2205.01917v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01917">
<div class="article-summary-box-inner">
<span><p>Exploring large-scale pretrained foundation models is of significant interest
in computer vision because these models can be quickly transferred to many
downstream tasks. This paper presents Contrastive Captioner (CoCa), a
minimalist design to pretrain an image-text encoder-decoder foundation model
jointly with contrastive loss and captioning loss, thereby subsuming model
capabilities from contrastive approaches like CLIP and generative methods like
SimVLM. In contrast to standard encoder-decoder transformers where all decoder
layers attend to encoder outputs, CoCa omits cross-attention in the first half
of decoder layers to encode unimodal text representations, and cascades the
remaining decoder layers which cross-attend to the image encoder for multimodal
image-text representations. We apply a contrastive loss between unimodal image
and text embeddings, in addition to a captioning loss on the multimodal decoder
outputs which predicts text tokens autoregressively. By sharing the same
computational graph, the two training objectives are computed efficiently with
minimal overhead. CoCa is pretrained end-to-end and from scratch on both
web-scale alt-text data and annotated images by treating all labels simply as
text, seamlessly unifying natural language supervision for representation
learning. Empirically, CoCa achieves state-of-the-art performance with
zero-shot transfer or minimal task-specific adaptation on a broad range of
downstream tasks, spanning visual recognition (ImageNet, Kinetics-400/600/700,
Moments-in-Time), crossmodal retrieval (MSCOCO, Flickr30K, MSR-VTT), multimodal
understanding (VQA, SNLI-VE, NLVR2), and image captioning (MSCOCO, NoCaps).
Notably on ImageNet classification, CoCa obtains 86.3% zero-shot top-1
accuracy, 90.6% with a frozen encoder and learned classification head, and new
state-of-the-art 91.0% top-1 accuracy on ImageNet with a finetuned encoder.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scene Clustering Based Pseudo-labeling Strategy for Multi-modal Aerial View Object Classification. (arXiv:2205.01920v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01920">
<div class="article-summary-box-inner">
<span><p>Multi-modal aerial view object classification (MAVOC) in Automatic target
recognition (ATR), although an important and challenging problem, has been
under studied. This paper firstly finds that fine-grained data, class imbalance
and various shooting conditions preclude the representational ability of
general image classification. Moreover, the MAVOC dataset has scene aggregation
characteristics. By exploiting these properties, we propose Scene Clustering
Based Pseudo-labeling Strategy (SCP-Label), a simple yet effective method to
employ in post-processing. The SCP-Label brings greater accuracy by assigning
the same label to objects within the same scene while also mitigating bias and
confusion with model ensembles. Its performance surpasses the official baseline
by a large margin of +20.57% Accuracy on Track 1 (SAR), and +31.86% Accuracy on
Track 2 (SAR+EO), demonstrating the potential of SCP-Label as post-processing.
Finally, we win the championship both on Track1 and Track2 in the CVPR 2022
Perception Beyond the Visible Spectrum (PBVS) Workshop MAVOC Challenge. Our
code is available at https://github.com/HowieChangchn/SCP-Label.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Episode Few-Shot Contrastive Predictive Coding: Solving intelligence tests without prior training. (arXiv:2205.01924v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01924">
<div class="article-summary-box-inner">
<span><p>Video prediction models often combine three components: an encoder from pixel
space to a small latent space, a latent space prediction model, and a
generative model back to pixel space. However, the large and unpredictable
pixel space makes training such models difficult, requiring many training
examples. We argue that finding a predictive latent variable and using it to
evaluate the consistency of a future image enables data-efficient predictions
because it precludes the necessity of a generative model training. To
demonstrate it, we created sequence completion intelligence tests in which the
task is to identify a predictably changing feature in a sequence of images and
use this prediction to select the subsequent image. We show that a
one-dimensional Markov Contrastive Predictive Coding (M-CPC_1D) model solves
these tests efficiently, with only five examples. Finally, we demonstrate the
usefulness of M-CPC_1D in solving two tasks without prior training: anomaly
detection and stochastic movement video prediction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised learning unveils morphological clusters behind lung cancer types and prognosis. (arXiv:2205.01931v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01931">
<div class="article-summary-box-inner">
<span><p>Histopathological images of tumors contain abundant information about how
tumors grow and how they interact with their micro-environment. Characterizing
and improving our understanding of phenotypes could reveal factors related to
tumor progression and their underpinning biological processes, ultimately
improving diagnosis and treatment. In recent years, the field of histological
deep learning applications has seen great progress, yet most of these
applications focus on a supervised approach, relating tissue and associated
sample annotations. Supervised approaches have their impact limited by two
factors. Firstly, high-quality labels are expensive in time and effort, which
makes them not easily scalable. Secondly, these methods focus on predicting
annotations from histological images, fundamentally restricting the discovery
of new tissue phenotypes. These limitations emphasize the importance of using
new methods that can characterize tissue by the features enclosed in the image,
without pre-defined annotation or supervision. We present Phenotype
Representation Learning (PRL), a methodology to extract histomorphological
phenotypes through self-supervised learning and community detection. PRL
creates phenotype clusters by identifying tissue patterns that share common
morphological and cellular features, allowing to describe whole slide images
through compositional representations of cluster contributions. We used this
framework to analyze histopathology slides of LUAD and LUSC lung cancer
subtypes from TCGA and NYU cohorts. We show that PRL achieves a robust lung
subtype prediction providing statistically relevant phenotypes for each lung
subtype. We further demonstrate the significance of these phenotypes in lung
adenocarcinoma overall and recurrence free survival, relating clusters with
patient outcomes, cell types, grown patterns, and omic-based immune signatures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Homography-Based Loss Function for Camera Pose Regression. (arXiv:2205.01937v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01937">
<div class="article-summary-box-inner">
<span><p>Some recent visual-based relocalization algorithms rely on deep learning
methods to perform camera pose regression from image data. This paper focuses
on the loss functions that embed the error between two poses to perform deep
learning based camera pose regression. Existing loss functions are either
difficult-to-tune multi-objective functions or present unstable reprojection
errors that rely on ground truth 3D scene points and require a two-step
training. To deal with these issues, we introduce a novel loss function which
is based on a multiplane homography integration. This new function does not
require prior initialization and only depends on physically interpretable
hyperparameters. Furthermore, the experiments carried out on well established
relocalization datasets show that it minimizes best the mean square
reprojection error during training when compared with existing loss functions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EllSeg-Gen, towards Domain Generalization for head-mounted eyetracking. (arXiv:2205.01947v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01947">
<div class="article-summary-box-inner">
<span><p>The study of human gaze behavior in natural contexts requires algorithms for
gaze estimation that are robust to a wide range of imaging conditions. However,
algorithms often fail to identify features such as the iris and pupil centroid
in the presence of reflective artifacts and occlusions. Previous work has shown
that convolutional networks excel at extracting gaze features despite the
presence of such artifacts. However, these networks often perform poorly on
data unseen during training. This work follows the intuition that jointly
training a convolutional network with multiple datasets learns a generalized
representation of eye parts. We compare the performance of a single model
trained with multiple datasets against a pool of models trained on individual
datasets. Results indicate that models tested on datasets in which eye images
exhibit higher appearance variability benefit from multiset training. In
contrast, dataset-specific models generalize better onto eye images with lower
appearance variability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequencer: Deep LSTM for Image Classification. (arXiv:2205.01972v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01972">
<div class="article-summary-box-inner">
<span><p>In recent computer vision research, the advent of the Vision Transformer
(ViT) has rapidly revolutionized various architectural design efforts: ViT
achieved state-of-the-art image classification performance using self-attention
found in natural language processing, and MLP-Mixer achieved competitive
performance using simple multi-layer perceptrons. In contrast, several studies
have also suggested that carefully redesigned convolutional neural networks
(CNNs) can achieve advanced performance comparable to ViT without resorting to
these new ideas. Against this background, there is growing interest in what
inductive bias is suitable for computer vision. Here we propose Sequencer, a
novel and competitive architecture alternative to ViT that provides a new
perspective on these issues. Unlike ViTs, Sequencer models long-range
dependencies using LSTMs rather than self-attention layers. We also propose a
two-dimensional version of Sequencer module, where an LSTM is decomposed into
vertical and horizontal LSTMs to enhance performance. Despite its simplicity,
several experiments demonstrate that Sequencer performs impressively well:
Sequencer2D-L, with 54M parameters, realizes 84.6\% top-1 accuracy on only
ImageNet-1K. Not only that, we show that it has good transferability and the
robust resolution adaptability on double resolution-band.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MM-Claims: A Dataset for Multimodal Claim Detection in Social Media. (arXiv:2205.01989v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01989">
<div class="article-summary-box-inner">
<span><p>In recent years, the problem of misinformation on the web has become
widespread across languages, countries, and various social media platforms.
Although there has been much work on automated fake news detection, the role of
images and their variety are not well explored. In this paper, we investigate
the roles of image and text at an earlier stage of the fake news detection
pipeline, called claim detection. For this purpose, we introduce a novel
dataset, MM-Claims, which consists of tweets and corresponding images over
three topics: COVID-19, Climate Change and broadly Technology. The dataset
contains roughly 86000 tweets, out of which 3400 are labeled manually by
multiple annotators for the training and evaluation of multimodal models. We
describe the dataset in detail, evaluate strong unimodal and multimodal
baselines, and analyze the potential and drawbacks of current models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Impact of a DCT-driven Loss in Attention-based Knowledge-Distillation for Scene Recognition. (arXiv:2205.01997v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01997">
<div class="article-summary-box-inner">
<span><p>Knowledge Distillation (KD) is a strategy for the definition of a set of
transferability gangways to improve the efficiency of Convolutional Neural
Networks. Feature-based Knowledge Distillation is a subfield of KD that relies
on intermediate network representations, either unaltered or depth-reduced via
maximum activation maps, as the source knowledge. In this paper, we propose and
analyse the use of a 2D frequency transform of the activation maps before
transferring them. We pose that\textemdash by using global image cues rather
than pixel estimates, this strategy enhances knowledge transferability in tasks
such as scene recognition, defined by strong spatial and contextual
relationships between multiple and varied concepts. To validate the proposed
method, an extensive evaluation of the state-of-the-art in scene recognition is
presented. Experimental results provide strong evidences that the proposed
strategy enables the student network to better focus on the relevant image
areas learnt by the teacher network, hence leading to better descriptive
features and higher transferred performance than every other state-of-the-art
alternative. We publicly release the training and evaluation framework used
along this paper at
<a href="http://www-vpu.eps.uam.es/publications/DCTBasedKDForSceneRecognition.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransRank: Self-supervised Video Representation Learning via Ranking-based Transformation Recognition. (arXiv:2205.02028v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02028">
<div class="article-summary-box-inner">
<span><p>Recognizing transformation types applied to a video clip (RecogTrans) is a
long-established paradigm for self-supervised video representation learning,
which achieves much inferior performance compared to instance discrimination
approaches (InstDisc) in recent works. However, based on a thorough comparison
of representative RecogTrans and InstDisc methods, we observe the great
potential of RecogTrans on both semantic-related and temporal-related
downstream tasks. Based on hard-label classification, existing RecogTrans
approaches suffer from noisy supervision signals in pre-training. To mitigate
this problem, we developed TransRank, a unified framework for recognizing
Transformations in a Ranking formulation. TransRank provides accurate
supervision signals by recognizing transformations relatively, consistently
outperforming the classification-based formulation. Meanwhile, the unified
framework can be instantiated with an arbitrary set of temporal or spatial
transformations, demonstrating good generality. With a ranking-based
formulation and several empirical practices, we achieve competitive performance
on video retrieval and action recognition. Under the same setting, TransRank
surpasses the previous state-of-the-art method by 6.4% on UCF101 and 8.3% on
HMDB51 for action recognition (Top1 Acc); improves video retrieval on UCF101 by
20.4% (R@1). The promising results validate that RecogTrans is still a worth
exploring paradigm for video self-supervised learning. Codes will be released
at https://github.com/kennymckormick/TransRank.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Super-Resolution for Multi-Exposure Push-Frame Satellites. (arXiv:2205.02031v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02031">
<div class="article-summary-box-inner">
<span><p>Modern Earth observation satellites capture multi-exposure bursts of
push-frame images that can be super-resolved via computational means. In this
work, we propose a super-resolution method for such multi-exposure sequences, a
problem that has received very little attention in the literature. The proposed
method can handle the signal-dependent noise in the inputs, process sequences
of any length, and be robust to inaccuracies in the exposure times.
Furthermore, it can be trained end-to-end with self-supervision, without
requiring ground truth high resolution frames, which makes it especially suited
to handle real data. Central to our method are three key contributions: i) a
base-detail decomposition for handling errors in the exposure times, ii) a
noise-level-aware feature encoding for improved fusion of frames with varying
signal-to-noise ratio and iii) a permutation invariant fusion strategy by
temporal pooling operators. We evaluate the proposed method on synthetic and
real data and show that it outperforms by a significant margin existing
single-exposure approaches that we adapted to the multi-exposure case.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Learning for Invariant Representations from Multi-Spectral and SAR Images. (arXiv:2205.02049v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02049">
<div class="article-summary-box-inner">
<span><p>Self-Supervised learning (SSL) has become the new state-of-art in several
domain classification and segmentation tasks. Of these, one popular category in
SSL is distillation networks such as BYOL. This work proposes RSDnet, which
applies the distillation network (BYOL) in the remote sensing (RS) domain where
data is non-trivially different from natural RGB images. Since Multi-spectral
(MS) and synthetic aperture radar (SAR) sensors provide varied spectral and
spatial resolution information, we utilised them as an implicit augmentation to
learn invariant feature embeddings. In order to learn RS based invariant
features with SSL, we trained RSDnet in two ways, i.e., single channel feature
learning and three channel feature learning. This work explores the usefulness
of single channel feature learning from random MS and SAR bands compared to the
common notion of using three or more bands. In our linear evaluation, these
single channel features reached a 0.92 F1 score on the EuroSAT classification
task and 59.6 mIoU on the DFC segmentation task for certain single bands. We
also compared our results with ImageNet weights and showed that the RS based
SSL model outperforms the supervised ImageNet based model. We further explored
the usefulness of multi-modal data compared to single modality data, and it is
shown that utilising MS and SAR data learn better invariant representations
than utilising only MS data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SVTS: Scalable Video-to-Speech Synthesis. (arXiv:2205.02058v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02058">
<div class="article-summary-box-inner">
<span><p>Video-to-speech synthesis (also known as lip-to-speech) refers to the
translation of silent lip movements into the corresponding audio. This task has
received an increasing amount of attention due to its self-supervised nature
(i.e., can be trained without manual labelling) combined with the ever-growing
collection of audio-visual data available online. Despite these strong
motivations, contemporary video-to-speech works focus mainly on small- to
medium-sized corpora with substantial constraints in both vocabulary and
setting. In this work, we introduce a scalable video-to-speech framework
consisting of two components: a video-to-spectrogram predictor and a
pre-trained neural vocoder, which converts the mel-frequency spectrograms into
waveform audio. We achieve state-of-the art results for GRID and considerably
outperform previous approaches on LRW. More importantly, by focusing on
spectrogram prediction using a simple feedforward model, we can efficiently and
effectively scale our method to very large and unconstrained datasets: To the
best of our knowledge, we are the first to show intelligible results on the
challenging LRS3 dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mobile-URSONet: an Embeddable Neural Network for Onboard Spacecraft Pose Estimation. (arXiv:2205.02065v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02065">
<div class="article-summary-box-inner">
<span><p>Spacecraft pose estimation is an essential computer vision application that
can improve the autonomy of in-orbit operations. An ESA/Stanford competition
brought out solutions that seem hardly compatible with the constraints imposed
on spacecraft onboard computers. URSONet is among the best in the competition
for its generalization capabilities but at the cost of a tremendous number of
parameters and high computational complexity. In this paper, we propose
Mobile-URSONet: a spacecraft pose estimation convolutional neural network with
178 times fewer parameters while degrading accuracy by no more than four times
compared to URSONet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual Branch Neural Network for Sea Fog Detection in Geostationary Ocean Color Imager. (arXiv:2205.02069v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02069">
<div class="article-summary-box-inner">
<span><p>Sea fog significantly threatens the safety of maritime activities. This paper
develops a sea fog dataset (SFDD) and a dual branch sea fog detection network
(DB-SFNet). We investigate all the observed sea fog events in the Yellow Sea
and the Bohai Sea (118.1{\deg}E-128.1{\deg}E, 29.5{\deg}N-43.8{\deg}N) from
2010 to 2020, and collect the sea fog images for each event from the
Geostationary Ocean Color Imager (GOCI) to comprise the dataset SFDD. The
location of the sea fog in each image in SFDD is accurately marked. The
proposed dataset is characterized by a long-time span, large number of samples,
and accurate labeling, that can substantially improve the robustness of various
sea fog detection models. Furthermore, this paper proposes a dual branch sea
fog detection network to achieve accurate and holistic sea fog detection. The
poporsed DB-SFNet is composed of a knowledge extraction module and a dual
branch optional encoding decoding module. The two modules jointly extracts
discriminative features from both visual and statistical domain. Experiments
show promising sea fog detection results with an F1-score of 0.77 and a
critical success index of 0.63. Compared with existing advanced deep learning
networks, DB-SFNet is superior in detection performance and stability,
particularly in the mixed cloud and fog areas.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepPortraitDrawing: Generating Human Body Images from Freehand Sketches. (arXiv:2205.02070v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02070">
<div class="article-summary-box-inner">
<span><p>Researchers have explored various ways to generate realistic images from
freehand sketches, e.g., for objects and human faces. However, how to generate
realistic human body images from sketches is still a challenging problem. It
is, first because of the sensitivity to human shapes, second because of the
complexity of human images caused by body shape and pose changes, and third
because of the domain gap between realistic images and freehand sketches. In
this work, we present DeepPortraitDrawing, a deep generative framework for
converting roughly drawn sketches to realistic human body images. To encode
complicated body shapes under various poses, we take a local-to-global
approach. Locally, we employ semantic part auto-encoders to construct
part-level shape spaces, which are useful for refining the geometry of an input
pre-segmented hand-drawn sketch. Globally, we employ a cascaded spatial
transformer network to refine the structure of body parts by adjusting their
spatial locations and relative proportions. Finally, we use a global synthesis
network for the sketch-to-image translation task, and a face refinement network
to enhance facial details. Extensive experiments have shown that given roughly
sketched human portraits, our method produces more realistic images than the
state-of-the-art sketch-to-image synthesis techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ANUBIS: Review and Benchmark Skeleton-Based Action Recognition Methods with a New Dataset. (arXiv:2205.02071v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02071">
<div class="article-summary-box-inner">
<span><p>Skeleton-based action recognition, as a subarea of action recognition, is
swiftly accumulating attention and popularity. The task is to recognize actions
performed by human articulation points. Compared with other data modalities, 3D
human skeleton representations have extensive unique desirable characteristics,
including succinctness, robustness, racial-impartiality, and many more. We aim
to provide a roadmap for new and existing researchers a on the landscapes of
skeleton-based action recognition for new and existing researchers. To this
end, we present a review in the form of a taxonomy on existing works of
skeleton-based action recognition. We partition them into four major
categories: (1) datasets; (2) extracting spatial features; (3) capturing
temporal patterns; (4) improving signal quality. For each method, we provide
concise yet informatively-sufficient descriptions. To promote more fair and
comprehensive evaluation on existing approaches of skeleton-based action
recognition, we collect ANUBIS, a large-scale human skeleton dataset. Compared
with previously collected dataset, ANUBIS are advantageous in the following
four aspects: (1) employing more recently released sensors; (2) containing
novel back view; (3) encouraging high enthusiasm of subjects; (4) including
actions of the COVID pandemic era. Using ANUBIS, we comparably benchmark
performance of current skeleton-based action recognizers. At the end of this
paper, we outlook future development of skeleton-based action recognition by
listing several new technical problems. We believe they are valuable to solve
in order to commercialize skeleton-based action recognition in the near future.
The dataset of ANUBIS is available at:
<a href="http://hcc-workshop.anu.edu.au/webs/anu101/home.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SDF-based RGB-D Camera Tracking in Neural Scene Representations. (arXiv:2205.02079v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02079">
<div class="article-summary-box-inner">
<span><p>We consider the problem of tracking the 6D pose of a moving RGB-D camera in a
neural scene representation. Different such representations have recently
emerged, and we investigate the suitability of them for the task of camera
tracking. In particular, we propose to track an RGB-D camera using a signed
distance field-based representation and show that compared to density-based
representations, tracking can be sped up, which enables more robust and
accurate pose estimates when computation time is limited.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Extrapolationin Space and Time. (arXiv:2205.02084v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02084">
<div class="article-summary-box-inner">
<span><p>Novel view synthesis (NVS) and video prediction (VP) are typically considered
disjoint tasks in computer vision. However, they can both be seen as ways to
observe the spatial-temporal world: NVS aims to synthesize a scene from a new
point of view, while VP aims to see a scene from a new point of time. These two
tasks provide complementary signals to obtain a scene representation, as
viewpoint changes from spatial observations inform depth, and temporal
observations inform the motion of cameras and individual objects. Inspired by
these observations, we propose to study the problem of Video Extrapolation in
Space and Time (VEST). We propose a model that leverages the self-supervision
and the complementary cues from both tasks, while existing methods can only
solve one of them. Experiments show that our method achieves performance better
than or comparable to several state-of-the-art NVS and VP methods on indoor and
outdoor real-world datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hypercomplex Image-to-Image Translation. (arXiv:2205.02087v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02087">
<div class="article-summary-box-inner">
<span><p>Image-to-image translation (I2I) aims at transferring the content
representation from an input domain to an output one, bouncing along different
target domains. Recent I2I generative models, which gain outstanding results in
this task, comprise a set of diverse deep networks each with tens of million
parameters. Moreover, images are usually three-dimensional being composed of
RGB channels and common neural models do not take dimensions correlation into
account, losing beneficial information. In this paper, we propose to leverage
hypercomplex algebra properties to define lightweight I2I generative models
capable of preserving pre-existing relations among image dimensions, thus
exploiting additional input information. On manifold I2I benchmarks, we show
how the proposed Quaternion StarGANv2 and parameterized hypercomplex StarGANv2
(PHStarGANv2) reduce parameters and storage memory amount while ensuring high
domain translation performance and good image quality as measured by FID and
LPIPS scores. Full code is available at: https://github.com/ispamm/HI2I.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Fully Annotated Thermal Infrared Face Dataset: Recorded in Various Environment Conditions and Distances From The Camera. (arXiv:2205.02093v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02093">
<div class="article-summary-box-inner">
<span><p>Facial thermography is one of the most popular research areas in infrared
thermal imaging, with diverse applications in medical, surveillance, and
environmental monitoring. However, in contrast to facial imagery in the visual
spectrum, the lack of public datasets on facial thermal images is an obstacle
to research improvement in this area. Thermal face imagery is still a
relatively new research area to be evaluated and studied in different
domains.The current thermal face datasets are limited in regards to the
subjects' distance from the camera, the ambient temperature variation, and
facial landmarks' localization. We address these gaps by presenting a new
facial thermography dataset. This article makes two main contributions to the
body of knowledge. First, it presents a comprehensive review and comparison of
current public datasets in facial thermography. Second, it introduces and
studies a novel public dataset on facial thermography, which we call it
Charlotte-ThermalFace. Charlotte-ThermalFace contains more than10000 infrared
thermal images in varying thermal conditions, several distances from the
camera, and different head positions. The data is fully annotated with the
facial landmarks, ambient temperature, relative humidity, the air speed of the
room, distance to the camera, and subject thermal sensation at the time of
capturing each image. Our dataset is the first publicly available thermal
dataset annotated with the thermal sensation of each subject in different
thermal conditions and one of the few datasets in raw 16-bit format. Finally,
we present a preliminary analysis of the dataset to show the applicability and
importance of the thermal conditions in facial thermography. The full dataset,
including annotations, are freely available for research purpose at
https://github.com/TeCSAR-UNCC/UNCC-ThermalFace
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Sparse R-CNN. (arXiv:2205.02101v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02101">
<div class="article-summary-box-inner">
<span><p>Sparse R-CNN is a recent strong object detection baseline by set prediction
on sparse, learnable proposal boxes and proposal features. In this work, we
propose to improve Sparse R-CNN with two dynamic designs. First, Sparse R-CNN
adopts a one-to-one label assignment scheme, where the Hungarian algorithm is
applied to match only one positive sample for each ground truth. Such
one-to-one assignment may not be optimal for the matching between the learned
proposal boxes and ground truths. To address this problem, we propose dynamic
label assignment (DLA) based on the optimal transport algorithm to assign
increasing positive samples in the iterative training stages of Sparse R-CNN.
We constrain the matching to be gradually looser in the sequential stages as
the later stage produces the refined proposals with improved precision. Second,
the learned proposal boxes and features remain fixed for different images in
the inference process of Sparse R-CNN. Motivated by dynamic convolution, we
propose dynamic proposal generation (DPG) to assemble multiple proposal experts
dynamically for providing better initial proposal boxes and features for the
consecutive training stages. DPG thereby can derive sample-dependent proposal
boxes and features for inference. Experiments demonstrate that our method,
named Dynamic Sparse R-CNN, can boost the strong Sparse R-CNN baseline with
different backbones for object detection. Particularly, Dynamic Sparse R-CNN
reaches the state-of-the-art 47.2% AP on the COCO 2017 validation set,
surpassing Sparse R-CNN by 2.2% AP with the same ResNet-50 backbone.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Concept Activation Vectors for Generating User-Defined 3D Shapes. (arXiv:2205.02102v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02102">
<div class="article-summary-box-inner">
<span><p>We explore the interpretability of 3D geometric deep learning models in the
context of Computer-Aided Design (CAD). The field of parametric CAD can be
limited by the difficulty of expressing high-level design concepts in terms of
a few numeric parameters. In this paper, we use a deep learning architectures
to encode high dimensional 3D shapes into a vectorized latent representation
that can be used to describe arbitrary concepts. Specifically, we train a
simple auto-encoder to parameterize a dataset of complex shapes. To understand
the latent encoded space, we use the idea of Concept Activation Vectors (CAV)
to reinterpret the latent space in terms of user-defined concepts. This allows
modification of a reference design to exhibit more or fewer characteristics of
a chosen concept or group of concepts. We also test the statistical
significance of the identified concepts and determine the sensitivity of a
physical quantity of interest across the dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neuroevolutionary Multi-objective approaches to Trajectory Prediction in Autonomous Vehicles. (arXiv:2205.02105v1 [cs.NE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02105">
<div class="article-summary-box-inner">
<span><p>The incentive for using Evolutionary Algorithms (EAs) for the automated
optimization and training of deep neural networks (DNNs), a process referred to
as neuroevolution, has gained momentum in recent years. The configuration and
training of these networks can be posed as optimization problems. Indeed, most
of the recent works on neuroevolution have focused their attention on
single-objective optimization. Moreover, from the little research that has been
done at the intersection of neuroevolution and evolutionary multi-objective
optimization (EMO), all the research that has been carried out has focused
predominantly on the use of one type of DNN: convolutional neural networks
(CNNs), using well-established standard benchmark problems such as MNIST. In
this work, we make a leap in the understanding of these two areas
(neuroevolution and EMO), regarded in this work as neuroevolutionary
multi-objective, by using and studying a rich DNN composed of a CNN and
Long-short Term Memory network. Moreover, we use a robust and challenging
vehicle trajectory prediction problem. By using the well-known Non-dominated
Sorting Genetic Algorithm-II, we study the effects of five different
objectives, tested in categories of three, allowing us to show how these
objectives have either a positive or detrimental effect in neuroevolution for
trajectory prediction in autonomous vehicles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prediction of fish location by combining fisheries data and sea bottom temperature forecasting. (arXiv:2205.02107v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02107">
<div class="article-summary-box-inner">
<span><p>This paper combines fisheries dependent data and environmental data to be
used in a machine learning pipeline to predict the spatio-temporal abundance of
two species (plaice and sole) commonly caught by the Belgian fishery in the
North Sea. By combining fisheries related features with environmental data, sea
bottom temperature derived from remote sensing, a higher accuracy can be
achieved. In a forecast setting, the predictive accuracy is further improved by
predicting, using a recurrent deep neural network, the sea bottom temperature
up to four days in advance instead of relying on the last previous temperature
measurement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved Orientation Estimation and Detection with Hybrid Object Detection Networks for Automotive Radar. (arXiv:2205.02111v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02111">
<div class="article-summary-box-inner">
<span><p>This paper presents novel hybrid architectures that combine grid- and
point-based processing to improve the detection performance and orientation
estimation of radar-based object detection networks. Purely grid-based
detection models operate on a bird's-eye-view (BEV) projection of the input
point cloud. These approaches suffer from a loss of detailed information
through the discrete grid resolution. This applies in particular to radar
object detection, where relatively coarse grid resolutions are commonly used to
account for the sparsity of radar point clouds. In contrast, point-based models
are not affected by this problem as they continuously process point clouds.
However, they generally exhibit worse detection performances than grid-based
methods.
</p>
<p>We show that a point-based model can extract neighborhood features,
leveraging the exact relative positions of points, before grid rendering. This
has significant benefits for a following convolutional detection backbone. In
experiments on the public nuScenes dataset our hybrid architecture achieves
improvements in terms of detection performance and orientation estimates over
networks from previous literature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Engineering deep learning methods on automatic detection of damage in infrastructure due to extreme events. (arXiv:2205.02125v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02125">
<div class="article-summary-box-inner">
<span><p>This paper presents a few comprehensive experimental studies for automated
Structural Damage Detection (SDD) in extreme events using deep learning methods
for processing 2D images. In the first study, a 152-layer Residual network
(ResNet) is utilized to classify multiple classes in eight SDD tasks, which
include identification of scene levels, damage levels, material types, etc. The
proposed ResNet achieved high accuracy for each task while the positions of the
damage are not identifiable. In the second study, the existing ResNet and a
segmentation network (U-Net) are combined into a new pipeline, cascaded
networks, for categorizing and locating structural damage. The results show
that the accuracy of damage detection is significantly improved compared to
only using a segmentation network. In the third and fourth studies, end-to-end
networks are developed and tested as a new solution to directly detect cracks
and spalling in the image collections of recent large earthquakes. One of the
proposed networks can achieve an accuracy above 67.6% for all tested images at
various scales and resolutions, and shows its robustness for these human-free
detection tasks. As a preliminary field study, we applied the proposed method
to detect damage in a concrete structure that was tested to study its
progressive collapse performance. The experiments indicate that these solutions
for automatic detection of structural damage using deep learning methods are
feasible and promising. The training datasets and codes will be made available
for the public upon the publication of this paper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domino Saliency Metrics: Improving Existing Channel Saliency Metrics with Structural Information. (arXiv:2205.02131v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02131">
<div class="article-summary-box-inner">
<span><p>Channel pruning is used to reduce the number of weights in a Convolutional
Neural Network (CNN). Channel pruning removes slices of the weight tensor so
that the convolution layer remains dense. The removal of these weight slices
from a single layer causes mismatching number of feature maps between layers of
the network. A simple solution is to force the number of feature map between
layers to match through the removal of weight slices from subsequent layers.
This additional constraint becomes more apparent in DNNs with branches where
multiple channels need to be pruned together to keep the network dense. Popular
pruning saliency metrics do not factor in the structural dependencies that
arise in DNNs with branches. We propose Domino metrics (built on existing
channel saliency metrics) to reflect these structural constraints. We test
Domino saliency metrics against the baseline channel saliency metrics on
multiple networks with branches. Domino saliency metrics improved pruning rates
in most tested networks and up to 25% in AlexNet on CIFAR-10.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RecipeSnap -- a lightweight image-to-recipe model. (arXiv:2205.02141v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02141">
<div class="article-summary-box-inner">
<span><p>In this paper we want to address the problem of automation for recognition of
photographed cooking dishes and generating the corresponding food recipes.
Current image-to-recipe models are computation expensive and require powerful
GPUs for model training and implementation. High computational cost prevents
those existing models from being deployed on portable devices, like smart
phones. To solve this issue we introduce a lightweight image-to-recipe
prediction model, RecipeSnap, that reduces memory cost and computational cost
by more than 90% while still achieving 2.0 MedR, which is in line with the
state-of-the-art model. A pre-trained recipe encoder was used to compute recipe
embeddings. Recipes from Recipe1M dataset and corresponding recipe embeddings
are collected as a recipe library, which are used for image encoder training
and image query later. We use MobileNet-V2 as image encoder backbone, which
makes our model suitable to portable devices. This model can be further
developed into an application for smart phones with a few effort. A comparison
of the performance between this lightweight model to other heavy models are
presented in this paper. Code, data and models are publicly accessible on
github.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Analysis of Generative Methods for Multiple Image Inpainting. (arXiv:2205.02146v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02146">
<div class="article-summary-box-inner">
<span><p>Image inpainting refers to the restoration of an image with missing regions
in a way that is not detectable by the observer. The inpainting regions can be
of any size and shape. This is an ill-posed inverse problem that does not have
a unique solution. In this work, we focus on learning-based image completion
methods for multiple and diverse inpainting which goal is to provide a set of
distinct solutions for a given damaged image. These methods capitalize on the
probabilistic nature of certain generative models to sample various solutions
that coherently restore the missing content. Along the chapter, we will analyze
the underlying theory and analyze the recent proposals for multiple inpainting.
To investigate the pros and cons of each method, we present quantitative and
qualitative comparisons, on common datasets, regarding both the quality and the
diversity of the set of inpainted solutions. Our analysis allows us to identify
the most successful generative strategies in both inpainting quality and
inpainting diversity. This task is closely related to the learning of an
accurate probability distribution of images. Depending on the dataset in use,
the challenges that entail the training of such a model will be discussed
through the analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual Cross-Attention Learning for Fine-Grained Visual Categorization and Object Re-Identification. (arXiv:2205.02151v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02151">
<div class="article-summary-box-inner">
<span><p>Recently, self-attention mechanisms have shown impressive performance in
various NLP and CV tasks, which can help capture sequential characteristics and
derive global information. In this work, we explore how to extend
self-attention modules to better learn subtle feature embeddings for
recognizing fine-grained objects, e.g., different bird species or person
identities. To this end, we propose a dual cross-attention learning (DCAL)
algorithm to coordinate with self-attention learning. First, we propose
global-local cross-attention (GLCA) to enhance the interactions between global
images and local high-response regions, which can help reinforce the
spatial-wise discriminative clues for recognition. Second, we propose pair-wise
cross-attention (PWCA) to establish the interactions between image pairs. PWCA
can regularize the attention learning of an image by treating another image as
distractor and will be removed during inference. We observe that DCAL can
reduce misleading attentions and diffuse the attention response to discover
more complementary parts for recognition. We conduct extensive evaluations on
fine-grained visual categorization and object re-identification. Experiments
demonstrate that DCAL performs on par with state-of-the-art methods and
consistently improves multiple self-attention baselines, e.g., surpassing
DeiT-Tiny and ViT-Base by 2.8% and 2.4% mAP on MSMT17, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Transferability for Covid 3D Localization Using CT SARS-CoV-2 segmentation models. (arXiv:2205.02152v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02152">
<div class="article-summary-box-inner">
<span><p>Recent studies indicate that detecting radiographic patterns on CT scans can
yield high sensitivity and specificity for COVID-19 localization. In this
paper, we investigate the appropriateness of deep learning models
transferability, for semantic segmentation of pneumonia-infected areas in CT
images. Transfer learning allows for the fast initialization/ reutilization of
detection models, given that large volumes of training are not available. Our
work explores the efficacy of using pre-trained U-Net architectures, on a
specific CT data set, for identifying Covid-19 side-effects over images from
different datasets. Experimental results indicate improvement in the
segmentation accuracy of identifying COVID-19 infected regions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UnrealNAS: Can We Search Neural Architectures with Unreal Data?. (arXiv:2205.02162v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02162">
<div class="article-summary-box-inner">
<span><p>Neural architecture search (NAS) has shown great success in the automatic
design of deep neural networks (DNNs). However, the best way to use data to
search network architectures is still unclear and under exploration. Previous
work [19, 46] has analyzed the necessity of having ground-truth labels in NAS
and inspired broad interest. In this work, we take a further step to question
whether real data is necessary for NAS to be effective. The answer to this
question is important for applications with limited amount of accessible data,
and can help people improve NAS by leveraging the extra flexibility of data
generation. To explore if NAS needs real data, we construct three types of
unreal datasets using: 1) randomly labeled real images; 2) generated images and
labels; and 3) generated Gaussian noise with random labels. These datasets
facilitate to analyze the generalization and expressivity of the searched
architectures. We study the performance of architectures searched on these
constructed datasets using popular differentiable NAS methods. Extensive
experiments on CIFAR, ImageNet and CheXpert [12] show that the searched
architectures can achieve promising results compared with those derived from
the conventional NAS pipeline with real labeled data, suggesting the
feasibility of performing NAS with unreal data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compound virtual screening by learning-to-rank with gradient boosting decision tree and enrichment-based cumulative gain. (arXiv:2205.02169v1 [q-bio.BM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02169">
<div class="article-summary-box-inner">
<span><p>Learning-to-rank, a machine learning technique widely used in information
retrieval, has recently been applied to the problem of ligand-based virtual
screening, to accelerate the early stages of new drug development. Ranking
prediction models learn based on ordinal relationships, making them suitable
for integrating assay data from various environments. Existing studies of rank
prediction in compound screening have generally used a learning-to-rank method
called RankSVM. However, they have not been compared with or validated against
the gradient boosting decision tree (GBDT)-based learning-to-rank methods that
have gained popularity recently. Furthermore, although the ranking metric
called Normalized Discounted Cumulative Gain (NDCG) is widely used in
information retrieval, it only determines whether the predictions are better
than those of other models. In other words, NDCG is incapable of recognizing
when a prediction model produces worse than random results. Nevertheless, NDCG
is still used in the performance evaluation of compound screening using
learning-to-rank. This study used the GBDT model with ranking loss functions,
called lambdarank and lambdaloss, for ligand-based virtual screening; results
were compared with existing RankSVM methods and GBDT models using regression.
We also proposed a new ranking metric, Normalized Enrichment Discounted
Cumulative Gain (NEDCG), which aims to properly evaluate the goodness of
ranking predictions. Results showed that the GBDT model with learning-to-rank
outperformed existing regression methods using GBDT and RankSVM on diverse
datasets. Moreover, NEDCG showed that predictions by regression were comparable
to random predictions in multi-assay, multi-family datasets, demonstrating its
usefulness for a more direct assessment of compound screening performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COOPERNAUT: End-to-End Driving with Cooperative Perception for Networked Vehicles. (arXiv:2205.02222v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02222">
<div class="article-summary-box-inner">
<span><p>Optical sensors and learning algorithms for autonomous vehicles have
dramatically advanced in the past few years. Nonetheless, the reliability of
today's autonomous vehicles is hindered by the limited line-of-sight sensing
capability and the brittleness of data-driven methods in handling extreme
situations. With recent developments of telecommunication technologies,
cooperative perception with vehicle-to-vehicle communications has become a
promising paradigm to enhance autonomous driving in dangerous or emergency
situations. We introduce COOPERNAUT, an end-to-end learning model that uses
cross-vehicle perception for vision-based cooperative driving. Our model
encodes LiDAR information into compact point-based representations that can be
transmitted as messages between vehicles via realistic wireless channels. To
evaluate our model, we develop AutoCastSim, a network-augmented driving
simulation framework with example accident-prone scenarios. Our experiments on
AutoCastSim suggest that our cooperative perception driving models lead to a
40% improvement in average success rate over egocentric driving models in these
challenging driving situations and a 5 times smaller bandwidth requirement than
prior work V2VNet. COOPERNAUT and AUTOCASTSIM are available at
https://ut-austin-rpl.github.io/Coopernaut/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Similarity Attention. (arXiv:1911.07381v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1911.07381">
<div class="article-summary-box-inner">
<span><p>While there has been substantial progress in learning suitable distance
metrics, these techniques in general lack transparency and decision reasoning,
i.e., explaining why the input set of images is similar or dissimilar. In this
work, we solve this key problem by proposing the first method to generate
generic visual similarity explanations with gradient-based attention. We
demonstrate that our technique is agnostic to the specific similarity model
type, e.g., we show applicability to Siamese, triplet, and quadruplet models.
Furthermore, we make our proposed similarity attention a principled part of the
learning process, resulting in a new paradigm for learning similarity
functions. We demonstrate that our learning mechanism results in more
generalizable, as well as explainable, similarity models. Finally, we
demonstrate the generality of our framework by means of experiments on a
variety of tasks, including image retrieval, person re-identification, and
low-shot semantic segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EllSeg: An Ellipse Segmentation Framework for Robust Gaze Tracking. (arXiv:2007.09600v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.09600">
<div class="article-summary-box-inner">
<span><p>Ellipse fitting, an essential component in pupil or iris tracking based video
oculography, is performed on previously segmented eye parts generated using
various computer vision techniques. Several factors, such as occlusions due to
eyelid shape, camera position or eyelashes, frequently break ellipse fitting
algorithms that rely on well-defined pupil or iris edge segments. In this work,
we propose training a convolutional neural network to directly segment entire
elliptical structures and demonstrate that such a framework is robust to
occlusions and offers superior pupil and iris tracking performance (at least
10$\%$ and 24$\%$ increase in pupil and iris center detection rate respectively
within a two-pixel error margin) compared to using standard eye parts
segmentation for multiple publicly available synthetic segmentation datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse Representations of Positive Functions via First and Second-Order Pseudo-Mirror Descent. (arXiv:2011.07142v4 [stat.ML] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.07142">
<div class="article-summary-box-inner">
<span><p>We consider expected risk minimization problems when the range of the
estimator is required to be nonnegative, motivated by the settings of maximum
likelihood estimation (MLE) and trajectory optimization. To facilitate
nonlinear interpolation, we hypothesize that the search space is a Reproducing
Kernel Hilbert Space (RKHS). We develop first and second-order variants of
stochastic mirror descent employing (i) \emph{pseudo-gradients} and (ii)
complexity-reducing projections. Compressive projection in the first-order
scheme is executed via kernel orthogonal matching pursuit (KOMP), which
overcomes the fact that the vanilla RKHS parameterization grows unbounded with
the iteration index in the stochastic setting. Moreover, pseudo-gradients are
needed when gradient estimates for cost are only computable up to some
numerical error, which arise in, e.g., integral approximations. Under constant
step-size and compression budget, we establish tradeoffs between the radius of
convergence of the expected sub-optimality and the projection budget parameter,
as well as non-asymptotic bounds on the model complexity. To refine the
solution's precision, we develop a second-order extension which employs
recursively averaged pseudo-gradient outer-products to approximate the Hessian
inverse, whose convergence in mean is established under an additional
eigenvalue decay condition on the Hessian of the optimal RKHS element, which is
unique to this work. Experiments demonstrate favorable performance on
inhomogeneous Poisson Process intensity estimation in practice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Two-Stream CNN for Multi-Modal Age-related Macular Degeneration Categorization. (arXiv:2012.01879v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.01879">
<div class="article-summary-box-inner">
<span><p>This paper tackles automated categorization of Age-related Macular
Degeneration (AMD), a common macular disease among people over 50. Previous
research efforts mainly focus on AMD categorization with a single-modal input,
let it be a color fundus photograph (CFP) or an OCT B-scan image. By contrast,
we consider AMD categorization given a multi-modal input, a direction that is
clinically meaningful yet mostly unexplored. Contrary to the prior art that
takes a traditional approach of feature extraction plus classifier training
that cannot be jointly optimized, we opt for end-to-end multi-modal
Convolutional Neural Networks (MM-CNN). Our MM-CNN is instantiated by a
two-stream CNN, with spatially-invariant fusion to combine information from the
CFP and OCT streams. In order to visually interpret the contribution of the
individual modalities to the final prediction, we extend the class activation
mapping (CAM) technique to the multi-modal scenario. For effective training of
MM-CNN, we develop two data augmentation methods. One is GAN-based CFP/OCT
image synthesis, with our novel use of CAMs as conditional input of a
high-resolution image-to-image translation GAN. The other method is Loose
Pairing, which pairs a CFP image and an OCT image on the basis of their classes
instead of eye identities. Experiments on a clinical dataset consisting of
1,094 CFP images and 1,289 OCT images acquired from 1,093 distinct eyes show
that the proposed solution obtains better F1 and Accuracy than multiple
baselines for multi-modal AMD categorization. Code and data are available at
https://github.com/li-xirong/mmc-amd.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conditional Generation of Medical Images via Disentangled Adversarial Inference. (arXiv:2012.04764v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.04764">
<div class="article-summary-box-inner">
<span><p>Synthetic medical image generation has a huge potential for improving
healthcare through many applications, from data augmentation for training
machine learning systems to preserving patient privacy. Conditional Adversarial
Generative Networks (cGANs) use a conditioning factor to generate images and
have shown great success in recent years. Intuitively, the information in an
image can be divided into two parts: 1) content which is presented through the
conditioning vector and 2) style which is the undiscovered information missing
from the conditioning vector. Current practices in using cGANs for medical
image generation, only use a single variable for image generation (i.e.,
content) and therefore, do not provide much flexibility nor control over the
generated image. In this work we propose a methodology to learn from the image
itself, disentangled representations of style and content, and use this
information to impose control over the generation process. In this framework,
style is learned in a fully unsupervised manner, while content is learned
through both supervised learning (using the conditioning vector) and
unsupervised learning (with the inference mechanism). We undergo two novel
regularization steps to ensure content-style disentanglement. First, we
minimize the shared information between content and style by introducing a
novel application of the gradient reverse layer (GRL); second, we introduce a
self-supervised regularization method to further separate information in the
content and style variables. We show that in general, two latent variable
models achieve better performance and give more control over the generated
image. We also show that our proposed model (DRAI) achieves the best
disentanglement score and has the best overall performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Universal Deep Learning Framework for Real-Time Denoising of Ultrasound Images. (arXiv:2101.09122v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.09122">
<div class="article-summary-box-inner">
<span><p>Ultrasound images are widespread in medical diagnosis for muscle-skeletal,
cardiac, and obstetrical diseases, due to the efficiency and non-invasiveness
of the acquisition methodology. However, ultrasound acquisition introduces
noise in the signal, which corrupts the resulting image and affects further
processing steps, e.g., segmentation and quantitative analysis. We define a
novel deep learning framework for the real-time denoising of ultrasound images.
Firstly, we compare state-of-the-art methods for denoising (e.g., spectral,
low-rank methods) and select WNNM (Weighted Nuclear Norm Minimisation) as the
best denoising in terms of accuracy, preservation of anatomical features, and
edge enhancement. Then, we propose a tuned version of WNNM (tuned-WNNM) that
improves the quality of the denoised images and extends its applicability to
ultrasound images. Through a deep learning framework, the tuned-WNNM
qualitatively and quantitatively replicates WNNM results in real-time. Finally,
our approach is general in terms of its building blocks and parameters of the
deep learning and high-performance computing framework; in fact, we can select
different denoising algorithms and deep learning architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Purified Feature Representations from Task-irrelevant Labels. (arXiv:2102.10955v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.10955">
<div class="article-summary-box-inner">
<span><p>Learning an empirically effective model with generalization using limited
data is a challenging task for deep neural networks. In this paper, we propose
a novel learning framework called PurifiedLearning to exploit task-irrelevant
features extracted from task-irrelevant labels when training models on
small-scale datasets. Particularly, we purify feature representations by using
the expression of task-irrelevant information, thus facilitating the learning
process of classification. Our work is built on solid theoretical analysis and
extensive experiments, which demonstrate the effectiveness of PurifiedLearning.
According to the theory we proved, PurifiedLearning is model-agnostic and
doesn't have any restrictions on the model needed, so it can be combined with
any existing deep neural networks with ease to achieve better performance. The
source code of this paper will be available in the future for reproducibility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diagnosing Vision-and-Language Navigation: What Really Matters. (arXiv:2103.16561v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.16561">
<div class="article-summary-box-inner">
<span><p>Vision-and-language navigation (VLN) is a multimodal task where an agent
follows natural language instructions and navigates in visual environments.
Multiple setups have been proposed, and researchers apply new model
architectures or training techniques to boost navigation performance. However,
there still exist non-negligible gaps between machines' performance and human
benchmarks. Moreover, the agents' inner mechanisms for navigation decisions
remain unclear. To the best of our knowledge, how the agents perceive the
multimodal input is under-studied and needs investigation. In this work, we
conduct a series of diagnostic experiments to unveil agents' focus during
navigation. Results show that indoor navigation agents refer to both object and
direction tokens when making decisions. In contrast, outdoor navigation agents
heavily rely on direction tokens and poorly understand the object tokens.
Transformer-based agents acquire a better cross-modal understanding of objects
and display strong numerical reasoning ability than non-Transformer-based
agents. When it comes to vision-and-language alignments, many models claim that
they can align object tokens with specific visual targets. We find unbalanced
attention on the vision and text input and doubt the reliability of such
cross-modal alignments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating the Impact of Multi-LiDAR Placement on Object Detection for Autonomous Driving. (arXiv:2105.00373v4 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.00373">
<div class="article-summary-box-inner">
<span><p>The past few years have witnessed an increasing interest in improving the
perception performance of LiDARs on autonomous vehicles. While most of the
existing works focus on developing new deep learning algorithms or model
architectures, we study the problem from the physical design perspective, i.e.,
how different placements of multiple LiDARs influence the learning-based
perception. To this end, we introduce an easy-to-compute information-theoretic
surrogate metric to quantitatively and fast evaluate LiDAR placement for 3D
detection of different types of objects. We also present a new data collection,
detection model training and evaluation framework in the realistic CARLA
simulator to evaluate disparate multi-LiDAR configurations. Using several
prevalent placements inspired by the designs of self-driving companies, we show
the correlation between our surrogate metric and object detection performance
of different representative algorithms on KITTI through extensive experiments,
validating the effectiveness of our LiDAR placement evaluation approach. Our
results show that sensor placement is non-negligible in 3D point cloud-based
object detection, which will contribute up to 10% performance discrepancy in
terms of average precision in challenging 3D object detection settings. We
believe that this is one of the first studies to quantitatively investigate the
influence of LiDAR placement on perception performance. The code is available
on https://github.com/HanjiangHu/Multi-LiDAR-Placement-for-3D-Detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enabling 3D Object Detection with a Low-Resolution LiDAR. (arXiv:2105.01765v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.01765">
<div class="article-summary-box-inner">
<span><p>Light Detection And Ranging (LiDAR) has been widely used in autonomous
vehicles for perception and localization. However, the cost of a
high-resolution LiDAR is still prohibitively expensive, while its
low-resolution counterpart is much more affordable. Therefore, using
low-resolution LiDAR for autonomous driving is an economically viable solution,
but the point cloud sparsity makes it extremely challenging. In this paper, we
propose a two-stage neural network framework that enables 3D object detection
using a low-resolution LiDAR. Taking input from a low-resolution LiDAR point
cloud and a monocular camera image, a depth completion network is employed to
produce dense point cloud that is subsequently processed by a voxel-based
network for 3D object detection. Evaluated with KITTI dataset for 3D object
detection in Bird-Eye View (BEV), the experimental result shows that the
proposed approach performs significantly better than directly applying the
16-line LiDAR point cloud for object detection. For both easy and moderate
cases, our 3D vehicle detection results are close to those using 64-line
high-resolution LiDARs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comprehensive Survey and Taxonomy on Image Dehazing Based on Deep Learning. (arXiv:2106.03323v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03323">
<div class="article-summary-box-inner">
<span><p>With the development of convolutional neural networks, hundreds of deep
learning based dehazing methods have been proposed. In this paper, we provide a
comprehensive survey on supervised, semi-supervised, and unsupervised dehazing.
We first discuss the physical model, datasets, network modules, loss functions,
and evaluation metrics that are commonly used. Then, the main contributions of
various dehazing algorithms are categorized and summarized. Further,
quantitative and qualitative experiments of various baseline methods are
carried out. Finally, the unsolved issues and challenges that can inspire the
future research are pointed out. A collection of useful dehazing materials is
available at https://github.com/Xiaofeng-life/AwesomeDehazing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Deep Neural Network Calibration by Regularization and its Impact on Refinement. (arXiv:2106.09385v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.09385">
<div class="article-summary-box-inner">
<span><p>Deep neural networks have been shown to be highly miscalibrated. often they
tend to be overconfident in their predictions. It poses a significant challenge
for safety-critical systems to utilise deep neural networks (DNNs), reliably.
Many recently proposed approaches to mitigate this have demonstrated
substantial progress in improving DNN calibration. However, they hardly touch
upon refinement, which historically has been an essential aspect of
calibration. Refinement indicates separability of a network's correct and
incorrect predictions. This paper presents a theoretically and empirically
supported exposition reviewing refinement of a calibrated model. Firstly, we
show the breakdown of expected calibration error (ECE), into predicted
confidence and refinement under the assumption of over-confident predictions.
Secondly, linking with this result, we highlight that regularization based
calibration only focuses on naively reducing a model's confidence. This
logically has a severe downside to a model's refinement as correct and
incorrect predictions become tightly coupled. Lastly, connecting refinement
with ECE also provides support to existing refinement based approaches which
improve calibration but do not explain the reasoning behind it. We support our
claims through rigorous empirical evaluations of many state of the art
calibration approaches on widely used datasets and neural networks. We find
that many calibration approaches with the likes of label smoothing, mixup etc.
lower the usefulness of a DNN by degrading its refinement. Even under natural
data shift, this calibration-refinement trade-off holds for the majority of
calibration methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-domain Few-shot Learning with Task-specific Adapters. (arXiv:2107.00358v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.00358">
<div class="article-summary-box-inner">
<span><p>In this paper, we look at the problem of cross-domain few-shot classification
that aims to learn a classifier from previously unseen classes and domains with
few labeled samples. Recent approaches broadly solve this problem by
parameterizing their few-shot classifiers with task-agnostic and task-specific
weights where the former is typically learned on a large training set and the
latter is dynamically predicted through an auxiliary network conditioned on a
small support set. In this work, we focus on the estimation of the latter, and
propose to learn task-specific weights from scratch directly on a small support
set, in contrast to dynamically estimating them. In particular, through
systematic analysis, we show that task-specific weights through parametric
adapters in matrix form with residual connections to multiple intermediate
layers of a backbone network significantly improves the performance of the
state-of-the-art models in the Meta-Dataset benchmark with minor additional
cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Slap Fingerprint Segmentation for Juveniles and Adults. (arXiv:2110.04067v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04067">
<div class="article-summary-box-inner">
<span><p>Many fingerprint recognition systems capture four fingerprints in one image.
In such systems, the fingerprint processing pipeline must first segment each
four-fingerprint slap into individual fingerprints. Note that most of the
current fingerprint segmentation algorithms have been designed and evaluated
using only adult fingerprint datasets. In this work, we have developed a
human-annotated in-house dataset of 15790 slaps of which 9084 are adult samples
and 6706 are samples drawn from children from ages 4 to 12. Subsequently, the
dataset is used to evaluate the matching performance of the NFSEG, a slap
fingerprint segmentation system developed by NIST, on slaps from adults and
juvenile subjects. Our results reveal the lower performance of NFSEG on slaps
from juvenile subjects. Finally, we utilized our novel dataset to develop the
Mask-RCNN based Clarkson Fingerprint Segmentation (CFSEG). Our matching results
using the Verifinger fingerprint matcher indicate that CFSEG outperforms NFSEG
for both adults and juvenile slaps. The CFSEG model is publicly available at
\url{https://github.com/keivanB/Clarkson_Finger_Segment}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Authentication Attacks on Projection-based Cancelable Biometric Schemes (long version). (arXiv:2110.15163v3 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15163">
<div class="article-summary-box-inner">
<span><p>Cancelable biometric schemes aim at generating secure biometric templates by
combining user specific tokens, such as password, stored secret or salt, along
with biometric data. This type of transformation is constructed as a
composition of a biometric transformation with a feature extraction algorithm.
The security requirements of cancelable biometric schemes concern the
irreversibility, unlinkability and revocability of templates, without losing in
accuracy of comparison. While several schemes were recently attacked regarding
these requirements, full reversibility of such a composition in order to
produce colliding biometric characteristics, and specifically presentation
attacks, were never demonstrated to the best of our knowledge. In this paper,
we formalize these attacks for a traditional cancelable scheme with the help of
integer linear programming (ILP) and quadratically constrained quadratic
programming (QCQP). Solving these optimization problems allows an adversary to
slightly alter its fingerprint image in order to impersonate any individual.
Moreover, in an even more severe scenario, it is possible to simultaneously
impersonate several individuals.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Palette: Image-to-Image Diffusion Models. (arXiv:2111.05826v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.05826">
<div class="article-summary-box-inner">
<span><p>This paper develops a unified framework for image-to-image translation based
on conditional diffusion models and evaluates this framework on four
challenging image-to-image translation tasks, namely colorization, inpainting,
uncropping, and JPEG restoration. Our simple implementation of image-to-image
diffusion models outperforms strong GAN and regression baselines on all tasks,
without task-specific hyper-parameter tuning, architecture customization, or
any auxiliary loss or sophisticated new techniques needed. We uncover the
impact of an L2 vs. L1 loss in the denoising diffusion objective on sample
diversity, and demonstrate the importance of self-attention in the neural
architecture through empirical studies. Importantly, we advocate a unified
evaluation protocol based on ImageNet, with human evaluation and sample quality
scores (FID, Inception Score, Classification Accuracy of a pre-trained
ResNet-50, and Perceptual Distance against original images). We expect this
standardized evaluation protocol to play a role in advancing image-to-image
translation research. Finally, we show that a generalist, multi-task diffusion
model performs as well or better than task-specific specialist counterparts.
Check out https://diffusion-palette.github.io for an overview of the results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">U-shape Transformer for Underwater Image Enhancement. (arXiv:2111.11843v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11843">
<div class="article-summary-box-inner">
<span><p>The light absorption and scattering of underwater impurities lead to poor
underwater imaging quality. The existing data-driven based underwater image
enhancement (UIE) techniques suffer from the lack of a large-scale dataset
containing various underwater scenes and high-fidelity reference images.
Besides, the inconsistent attenuation in different color channels and space
areas is not fully considered for boosted enhancement. In this work, we
constructed a large-scale underwater image (LSUI) dataset including 5004 image
pairs, and reported an U-shape Transformer network where the transformer model
is for the first time introduced to the UIE task. The U-shape Transformer is
integrated with a channel-wise multi-scale feature fusion transformer (CMSFFT)
module and a spatial-wise global feature modeling transformer (SGFMT) module,
which reinforce the network's attention to the color channels and space areas
with more serious attenuation. Meanwhile, in order to further improve the
contrast and saturation, a novel loss function combining RGB, LAB and LCH color
spaces is designed following the human vision principle. The extensive
experiments on available datasets validate the state-of-the-art performance of
the reported technique with more than 2dB superiority.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Multiple Dense Prediction Tasks from Partially Annotated Data. (arXiv:2111.14893v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14893">
<div class="article-summary-box-inner">
<span><p>Despite the recent advances in multi-task learning of dense prediction
problems, most methods rely on expensive labelled datasets. In this paper, we
present a label efficient approach and look at jointly learning of multiple
dense prediction tasks on partially annotated data (i.e. not all the task
labels are available for each image), which we call multi-task
partially-supervised learning. We propose a multi-task training procedure that
successfully leverages task relations to supervise its multi-task learning when
data is partially annotated. In particular, we learn to map each task pair to a
joint pairwise task-space which enables sharing information between them in a
computationally efficient way through another network conditioned on task
pairs, and avoids learning trivial cross-task relations by retaining high-level
information about the input image. We rigorously demonstrate that our proposed
method effectively exploits the images with unlabelled tasks and outperforms
existing semi-supervised learning approaches and related methods on three
standard benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Text-Guided Object Generation with Dream Fields. (arXiv:2112.01455v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01455">
<div class="article-summary-box-inner">
<span><p>We combine neural rendering with multi-modal image and text representations
to synthesize diverse 3D objects solely from natural language descriptions. Our
method, Dream Fields, can generate the geometry and color of a wide range of
objects without 3D supervision. Due to the scarcity of diverse, captioned 3D
data, prior methods only generate objects from a handful of categories, such as
ShapeNet. Instead, we guide generation with image-text models pre-trained on
large datasets of captioned images from the web. Our method optimizes a Neural
Radiance Field from many camera views so that rendered images score highly with
a target caption according to a pre-trained CLIP model. To improve fidelity and
visual quality, we introduce simple geometric priors, including
sparsity-inducing transmittance regularization, scene bounds, and new MLP
architectures. In experiments, Dream Fields produce realistic, multi-view
consistent object geometry and color from a variety of natural language
captions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Music-to-Dance Generation with Optimal Transport. (arXiv:2112.01806v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01806">
<div class="article-summary-box-inner">
<span><p>Dance choreography for a piece of music is a challenging task, having to be
creative in presenting distinctive stylistic dance elements while taking into
account the musical theme and rhythm. It has been tackled by different
approaches such as similarity retrieval, sequence-to-sequence modeling and
generative adversarial networks, but their generated dance sequences are often
short of motion realism, diversity and music consistency. In this paper, we
propose a Music-to-Dance with Optimal Transport Network (MDOT-Net) for learning
to generate 3D dance choreographies from music. We introduce an optimal
transport distance for evaluating the authenticity of the generated dance
distribution and a Gromov-Wasserstein distance to measure the correspondence
between the dance distribution and the input music. This gives a well defined
and non-divergent training objective that mitigates the limitation of standard
GAN training which is frequently plagued with instability and divergent
generator loss issues. Extensive experiments demonstrate that our MDOT-Net can
synthesize realistic and diverse dances which achieve an organic unity with the
input music, reflecting the shared intentionality and matching the rhythmic
articulation. Sample results are found at
https://www.youtube.com/watch?v=dErfBkrlUO8.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controllable Animation of Fluid Elements in Still Images. (arXiv:2112.03051v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03051">
<div class="article-summary-box-inner">
<span><p>We propose a method to interactively control the animation of fluid elements
in still images to generate cinemagraphs. Specifically, we focus on the
animation of fluid elements like water, smoke, fire, which have the properties
of repeating textures and continuous fluid motion. Taking inspiration from
prior works, we represent the motion of such fluid elements in the image in the
form of a constant 2D optical flow map. To this end, we allow the user to
provide any number of arrow directions and their associated speeds along with a
mask of the regions the user wants to animate. The user-provided input arrow
directions, their corresponding speed values, and the mask are then converted
into a dense flow map representing a constant optical flow map (FD). We observe
that FD, obtained using simple exponential operations can closely approximate
the plausible motion of elements in the image. We further refine computed dense
optical flow map FD using a generative-adversarial network (GAN) to obtain a
more realistic flow map. We devise a novel UNet based architecture to
autoregressively generate future frames using the refined optical flow map by
forward-warping the input image features at different resolutions. We conduct
extensive experiments on a publicly available dataset and show that our method
is superior to the baselines in terms of qualitative and quantitative metrics.
In addition, we show the qualitative animations of the objects in directions
that did not exist in the training set and provide a way to synthesize videos
that otherwise would not exist in the real world.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ensembling Off-the-shelf Models for GAN Training. (arXiv:2112.09130v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09130">
<div class="article-summary-box-inner">
<span><p>The advent of large-scale training has produced a cornucopia of powerful
visual recognition models. However, generative models, such as GANs, have
traditionally been trained from scratch in an unsupervised manner. Can the
collective "knowledge" from a large bank of pretrained vision models be
leveraged to improve GAN training? If so, with so many models to choose from,
which one(s) should be selected, and in what manner are they most effective? We
find that pretrained computer vision models can significantly improve
performance when used in an ensemble of discriminators. Notably, the particular
subset of selected models greatly affects performance. We propose an effective
selection mechanism, by probing the linear separability between real and fake
samples in pretrained model embeddings, choosing the most accurate model, and
progressively adding it to the discriminator ensemble. Interestingly, our
method can improve GAN training in both limited data and large-scale settings.
Given only 10k training samples, our FID on LSUN Cat matches the StyleGAN2
trained on 1.6M images. On the full dataset, our method improves FID by 1.5x to
2x on cat, church, and horse categories of LSUN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Streaming Volumetric Image Generation Framework for Development and Evaluation of Out-of-Core Methods. (arXiv:2112.09809v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09809">
<div class="article-summary-box-inner">
<span><p>Advances in 3D imaging technology in recent years have allowed for
increasingly high resolution volumetric images of large specimen. The resulting
datasets of hundreds of Gigabytes in size call for new scalable and memory
efficient approaches in the field of image processing, where some progress has
been made already. At the same time, quantitative evaluation of these new
methods is difficult both in terms of the availability of specific data sizes
and in the generation of associated ground truth data. In this paper we present
an algorithmic framework that can be used to efficiently generate test (and
ground truth) volume data, optionally even in a streaming fashion. As the
proposed nested sweeps algorithm is fast, it can be used to generate test data
on demand. We analyze the asymptotic run time of the presented algorithm and
compare it experimentally to alternative approaches as well as a hypothetical
best-case baseline method. In a case study, the framework is applied to the
popular VascuSynth software for vascular image generation, making it capable of
efficiently producing larger-than-main memory volumes which is demonstrated by
generating a trillion voxel (1TB) image. Implementations of the presented
framework are available online in the form of the modified version of
Vascusynth and the code used for the experimental evaluation. In addition, the
test data generation procedure has been integrated into the popular volume
rendering and processing framework Voreen.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MuMuQA: Multimedia Multi-Hop News Question Answering via Cross-Media Knowledge Extraction and Grounding. (arXiv:2112.10728v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10728">
<div class="article-summary-box-inner">
<span><p>Recently, there has been an increasing interest in building question
answering (QA) models that reason across multiple modalities, such as text and
images. However, QA using images is often limited to just picking the answer
from a pre-defined set of options. In addition, images in the real world,
especially in news, have objects that are co-referential to the text, with
complementary information from both modalities. In this paper, we present a new
QA evaluation benchmark with 1,384 questions over news articles that require
cross-media grounding of objects in images onto text. Specifically, the task
involves multi-hop questions that require reasoning over image-caption pairs to
identify the grounded visual object being referred to and then predicting a
span from the news body text to answer the question. In addition, we introduce
a novel multimedia data augmentation framework, based on cross-media knowledge
extraction and synthetic question-answer generation, to automatically augment
data that can provide weak supervision for this task. We evaluate both
pipeline-based and end-to-end pretraining-based multimedia QA models on our
benchmark, and show that they achieve promising performance, while considerably
lagging behind human performance hence leaving large room for future work on
this challenging new task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep learning for brain metastasis detection and segmentation in longitudinal MRI data. (arXiv:2112.11833v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11833">
<div class="article-summary-box-inner">
<span><p>Brain metastases occur frequently in patients with metastatic cancer. Early
and accurate detection of brain metastases is very essential for treatment
planning and prognosis in radiation therapy. To improve brain metastasis
detection performance with deep learning, a custom detection loss called
volume-level sensitivity-specificity (VSS) is proposed, which rates individual
metastasis detection sensitivity and specificity in (sub-)volume levels. As
sensitivity and precision are always a trade-off in a metastasis level, either
a high sensitivity or a high precision can be achieved by adjusting the weights
in the VSS loss without decline in dice score coefficient for segmented
metastases. To reduce metastasis-like structures being detected as false
positive metastases, a temporal prior volume is proposed as an additional input
of DeepMedic. The modified network is called DeepMedic+ for distinction. Our
proposed VSS loss improves the sensitivity of brain metastasis detection for
DeepMedic, increasing the sensitivity from 85.3% to 97.5%. Alternatively, it
improves the precision from 69.1% to 98.7%. Comparing DeepMedic+ with DeepMedic
with the same VSS loss, 44.4% of the false positive metastases are reduced in
the high sensitivity model and the precision reaches 99.6% for the high
specificity model. The mean dice coefficient for all metastases is about 0.81.
With the ensemble of the high sensitivity and high specificity models, on
average only 1.5 false positive metastases per patient needs further check,
while the majority of true positive metastases are confirmed. The ensemble
learning is able to distinguish high confidence true positive metastases from
metastases candidates that require special expert review or further follow-up,
being particularly well-fit to the requirements of expert support in real
clinical practice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Instant Neural Graphics Primitives with a Multiresolution Hash Encoding. (arXiv:2201.05989v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05989">
<div class="article-summary-box-inner">
<span><p>Neural graphics primitives, parameterized by fully connected neural networks,
can be costly to train and evaluate. We reduce this cost with a versatile new
input encoding that permits the use of a smaller network without sacrificing
quality, thus significantly reducing the number of floating point and memory
access operations: a small neural network is augmented by a multiresolution
hash table of trainable feature vectors whose values are optimized through
stochastic gradient descent. The multiresolution structure allows the network
to disambiguate hash collisions, making for a simple architecture that is
trivial to parallelize on modern GPUs. We leverage this parallelism by
implementing the whole system using fully-fused CUDA kernels with a focus on
minimizing wasted bandwidth and compute operations. We achieve a combined
speedup of several orders of magnitude, enabling training of high-quality
neural graphics primitives in a matter of seconds, and rendering in tens of
milliseconds at a resolution of ${1920\!\times\!1080}$.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-Shot Backdoor Attacks on Visual Object Tracking. (arXiv:2201.13178v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.13178">
<div class="article-summary-box-inner">
<span><p>Visual object tracking (VOT) has been widely adopted in mission-critical
applications, such as autonomous driving and intelligent surveillance systems.
In current practice, third-party resources such as datasets, backbone networks,
and training platforms are frequently used to train high-performance VOT
models. Whilst these resources bring certain convenience, they also introduce
new security threats into VOT models. In this paper, we reveal such a threat
where an adversary can easily implant hidden backdoors into VOT models by
tempering with the training process. Specifically, we propose a simple yet
effective few-shot backdoor attack (FSBA) that optimizes two losses
alternately: 1) a \emph{feature loss} defined in the hidden feature space, and
2) the standard \emph{tracking loss}. We show that, once the backdoor is
embedded into the target model by our FSBA, it can trick the model to lose
track of specific objects even when the \emph{trigger} only appears in one or a
few frames. We examine our attack in both digital and physical-world settings
and show that it can significantly degrade the performance of state-of-the-art
VOT trackers. We also show that our attack is resistant to potential defenses,
highlighting the vulnerability of VOT models to potential backdoor attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AssistQ: Affordance-centric Question-driven Task Completion for Egocentric Assistant. (arXiv:2203.04203v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04203">
<div class="article-summary-box-inner">
<span><p>A long-standing goal of intelligent assistants such as AR glasses/robots has
been to assist users in affordance-centric real-world scenarios, such as "how
can I run the microwave for 1 minute?". However, there is still no clear task
definition and suitable benchmarks. In this paper, we define a new task called
Affordance-centric Question-driven Task Completion, where the AI assistant
should learn from instructional videos and scripts to guide the user
step-by-step. To support the task, we constructed AssistQ, a new dataset
comprising 531 question-answer samples derived from 100 newly filmed
first-person videos. Each question should be completed with multi-step
guidances by inferring from visual details (e.g., buttons' position) and
textural details (e.g., actions like press/turn). To address this unique task,
we developed a Question-to-Actions (Q2A) model that significantly outperforms
several baseline methods while still having large room for improvement. We
expect our task and dataset to advance Egocentric AI Assistant's development.
Our project page is available at: https://showlab.github.io/assistq
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MDsrv -- visual sharing and analysis of molecular dynamics simulations. (arXiv:2203.13658v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13658">
<div class="article-summary-box-inner">
<span><p>Molecular dynamics simulation is a proven technique for computing and
visualizing the time-resolved motion of macromolecules at atomic resolution.
The MDsrv is a tool that streams MD trajectories and displays them
interactively in web browsers without requiring advanced skills, facilitating
interactive exploration and collaborative visual analysis. We have now enhanced
the MDsrv to further simplify the upload and sharing of MD trajectories and
improve their online viewing and analysis. With the new instance, the MDsrv
simplifies the creation of sessions, which allows the exchange of MD
trajectories with preset representations and perspectives. An important
innovation is that the MDsrv can now access and visualize trajectories from
remote datasets, which greatly expands its applicability and use, as the data
no longer needs to be accessible on a local server. In addition, initial
analyses such as sequence or structure alignments, distance measurements, or
RMSD calculations have been implemented, which optionally support visual
analysis. Finally, the MDsrv now offers a faster and more efficient
visualization of even large trajectories.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">iSDF: Real-Time Neural Signed Distance Fields for Robot Perception. (arXiv:2204.02296v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02296">
<div class="article-summary-box-inner">
<span><p>We present iSDF, a continual learning system for real-time signed distance
field (SDF) reconstruction. Given a stream of posed depth images from a moving
camera, it trains a randomly initialised neural network to map input 3D
coordinate to approximate signed distance. The model is self-supervised by
minimising a loss that bounds the predicted signed distance using the distance
to the closest sampled point in a batch of query points that are actively
sampled. In contrast to prior work based on voxel grids, our neural method is
able to provide adaptive levels of detail with plausible filling in of
partially observed regions and denoising of observations, all while having a
more compact representation. In evaluations against alternative methods on real
and synthetic datasets of indoor environments, we find that iSDF produces more
accurate reconstructions, and better approximations of collision costs and
gradients useful for downstream planners in domains from navigation to
manipulation. Code and video results can be found at our project page:
https://joeaortiz.github.io/iSDF/ .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FastMapSVM: Classifying Complex Objects Using the FastMap Algorithm and Support-Vector Machines. (arXiv:2204.05112v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05112">
<div class="article-summary-box-inner">
<span><p>Neural Networks and related Deep Learning methods are currently at the
leading edge of technologies used for classifying objects. However, they
generally demand large amounts of time and data for model training; and their
learned models can sometimes be difficult to interpret. In this paper, we
re-introduce FastMapSVM, an interpretable Machine Learning framework for
classifying complex objects. FastMapSVM combines the strengths of FastMap and
Support-Vector Machines. FastMap is an efficient linear-time algorithm that
maps complex objects to points in a Euclidean space, while preserving pairwise
non-Euclidean distances between them. We demonstrate the efficiency and
effectiveness of FastMapSVM in the context of classifying seismograms. We show
that its performance, in terms of precision, recall, and accuracy, is
comparable to that of other state-of-the-art methods. However, compared to
other methods, FastMapSVM uses significantly smaller amounts of time and data
for model training. It also provides a perspicuous visualization of the objects
and the classification boundaries between them. We expect FastMapSVM to be
viable for classification tasks in many other real-world domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision-and-Language Pretrained Models: A Survey. (arXiv:2204.07356v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07356">
<div class="article-summary-box-inner">
<span><p>Pretrained models have produced great success in both Computer Vision (CV)
and Natural Language Processing (NLP). This progress leads to learning joint
representations of vision and language pretraining by feeding visual and
linguistic contents into a multi-layer transformer, Visual-Language Pretrained
Models (VLPMs). In this paper, we present an overview of the major advances
achieved in VLPMs for producing joint representations of vision and language.
As the preliminaries, we briefly describe the general task definition and
genetic architecture of VLPMs. We first discuss the language and vision data
encoding methods and then present the mainstream VLPM structure as the core
content. We further summarise several essential pretraining and fine-tuning
strategies. Finally, we highlight three future directions for both CV and NLP
researchers to provide insightful guidance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Share With Thy Neighbors: Single-View Reconstruction by Cross-Instance Consistency. (arXiv:2204.10310v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10310">
<div class="article-summary-box-inner">
<span><p>Approaches to single-view reconstruction typically rely on viewpoint
annotations, silhouettes, the absence of background, multiple views of the same
instance, a template shape, or symmetry. We avoid all of these supervisions and
hypotheses by leveraging explicitly the consistency between images of different
object instances. As a result, our method can learn from large collections of
unlabelled images depicting the same object category. Our main contributions
are two approaches to leverage cross-instance consistency: (i) progressive
conditioning, a training strategy to gradually specialize the model from
category to instances in a curriculum learning fashion; (ii) swap
reconstruction, a loss enforcing consistency between instances having similar
shape or texture. Critical to the success of our method are also: our
structured autoencoding architecture decomposing an image into explicit shape,
texture, pose, and background; an adapted formulation of differential
rendering, and; a new optimization scheme alternating between 3D and pose
learning. We compare our approach, UNICORN, both on the diverse synthetic
ShapeNet dataset - the classical benchmark for methods requiring multiple views
as supervision - and on standard real-image benchmarks (Pascal3D+ Car, CUB-200)
for which most methods require known templates and silhouette annotations. We
also showcase applicability to more challenging real-world collections
(CompCars, LSUN), where silhouettes are not available and images are not
cropped around the object.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Dynamic View Synthesis With Few RGBD Cameras. (arXiv:2204.10477v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10477">
<div class="article-summary-box-inner">
<span><p>There have been significant advancements in dynamic novel view synthesis in
recent years. However, current deep learning models often require (1) prior
models (e.g., SMPL human models), (2) heavy pre-processing, or (3) per-scene
optimization. We propose to utilize RGBD cameras to remove these limitations
and synthesize free-viewpoint videos of dynamic indoor scenes. We generate
feature point clouds from RGBD frames and then render them into free-viewpoint
videos via a neural renderer. However, the inaccurate, unstable, and incomplete
depth measurements induce severe distortions, flickering, and ghosting
artifacts. We enforce spatial-temporal consistency via the proposed Cycle
Reconstruction Consistency and Temporal Stabilization module to reduce these
artifacts. We introduce a simple Regional Depth-Inpainting module that
adaptively inpaints missing depth values to render complete novel views.
Additionally, we present a Human-Things Interactions dataset to validate our
approach and facilitate future research. The dataset consists of 43 multi-view
RGBD video sequences of everyday activities, capturing complex interactions
between human subjects and their surroundings. Experiments on the HTI dataset
show that our method outperforms the baseline per-frame image fidelity and
spatial-temporal consistency. We will release our code, and the dataset on the
website soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controllable Image Captioning. (arXiv:2204.13324v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13324">
<div class="article-summary-box-inner">
<span><p>State-of-the-art image captioners can generate accurate sentences to describe
images in a sequence to sequence manner without considering the controllability
and interpretability. This, however, is far from making image captioning widely
used as an image can be interpreted in infinite ways depending on the target
and the context at hand. Achieving controllability is important especially when
the image captioner is used by different people with different way of
interpreting the images. In this paper, we introduce a novel framework for
image captioning which can generate diverse descriptions by capturing the
co-dependence between Part-Of-Speech tags and semantics. Our model decouples
direct dependence between successive variables. In this way, it allows the
decoder to exhaustively search through the latent Part-Of-Speech choices, while
keeping decoding speed proportional to the size of the POS vocabulary. Given a
control signal in the form of a sequence of Part-Of-Speech tags, we propose a
method to generate captions through a Transformer network, which predicts words
based on the input Part-Of-Speech tag sequences. Experiments on publicly
available datasets show that our model significantly outperforms
state-of-the-art methods on generating diverse image captions with high
qualities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual networks based 3D Multi-Person Pose Estimation from Monocular Video. (arXiv:2205.00748v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00748">
<div class="article-summary-box-inner">
<span><p>Monocular 3D human pose estimation has made progress in recent years. Most of
the methods focus on single persons, which estimate the poses in the
person-centric coordinates, i.e., the coordinates based on the center of the
target person. Hence, these methods are inapplicable for multi-person 3D pose
estimation, where the absolute coordinates (e.g., the camera coordinates) are
required. Moreover, multi-person pose estimation is more challenging than
single pose estimation, due to inter-person occlusion and close human
interactions. Existing top-down multi-person methods rely on human detection
(i.e., top-down approach), and thus suffer from the detection errors and cannot
produce reliable pose estimation in multi-person scenes. Meanwhile, existing
bottom-up methods that do not use human detection are not affected by detection
errors, but since they process all persons in a scene at once, they are prone
to errors, particularly for persons in small scales. To address all these
challenges, we propose the integration of top-down and bottom-up approaches to
exploit their strengths. Our top-down network estimates human joints from all
persons instead of one in an image patch, making it robust to possible
erroneous bounding boxes. Our bottom-up network incorporates human-detection
based normalized heatmaps, allowing the network to be more robust in handling
scale variations. Finally, the estimated 3D poses from the top-down and
bottom-up networks are fed into our integration network for final 3D poses. To
address the common gaps between training and testing data, we do optimization
during the test time, by refining the estimated 3D human poses using high-order
temporal constraint, re-projection loss, and bone length regularizations. Our
evaluations demonstrate the effectiveness of the proposed method. Code and
models are available: https://github.com/3dpose/3D-Multi-Person-Pose.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding CNNs from excitations. (arXiv:2205.00932v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00932">
<div class="article-summary-box-inner">
<span><p>For instance-level explanation, in order to reveal the relations between
high-level semantics and detailed spatial information, this paper proposes a
novel cognitive approach to neural networks, which named PANE. Under the
guidance of PANE, a novel saliency map representation method, named IOM, is
proposed for CNN-like models. We make the comparison with eight
state-of-the-art saliency map representation methods. The experimental results
show that IOM far outperforms baselines. The work of this paper may bring a new
perspective to understand deep neural networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HL-Net: Heterophily Learning Network for Scene Graph Generation. (arXiv:2205.01316v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01316">
<div class="article-summary-box-inner">
<span><p>Scene graph generation (SGG) aims to detect objects and predict their
pairwise relationships within an image. Current SGG methods typically utilize
graph neural networks (GNNs) to acquire context information between
objects/relationships. Despite their effectiveness, however, current SGG
methods only assume scene graph homophily while ignoring heterophily.
Accordingly, in this paper, we propose a novel Heterophily Learning Network
(HL-Net) to comprehensively explore the homophily and heterophily between
objects/relationships in scene graphs. More specifically, HL-Net comprises the
following 1) an adaptive reweighting transformer module, which adaptively
integrates the information from different layers to exploit both the
heterophily and homophily in objects; 2) a relationship feature propagation
module that efficiently explores the connections between relationships by
considering heterophily in order to refine the relationship representation; 3)
a heterophily-aware message-passing scheme to further distinguish the
heterophily and homophily between objects/relationships, thereby facilitating
improved message passing in graphs. We conducted extensive experiments on two
public datasets: Visual Genome (VG) and Open Images (OI). The experimental
results demonstrate the superiority of our proposed HL-Net over existing
state-of-the-art approaches. In more detail, HL-Net outperforms the second-best
competitors by 2.1$\%$ on the VG dataset for scene graph classification and
1.2$\%$ on the IO dataset for the final score. Code is available at
https://github.com/siml3/HL-Net.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Point Cloud Semantic Segmentation using Multi Scale Sparse Convolution Neural Network. (arXiv:2205.01550v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01550">
<div class="article-summary-box-inner">
<span><p>Point clouds have the characteristics of disorder, unstructured and
sparseness.Aiming at the problem of the non-structural nature of point clouds,
thanks to the excellent performance of convolutional neural networks in image
processing, one of the solutions is to extract features from point clouds based
on two-dimensional convolutional neural networks. The three-dimensional
information carried in the point cloud can be converted to two-dimensional, and
then processed by a two-dimensional convolutional neural network, and finally
back-projected to three-dimensional.In the process of projecting 3D information
to 2D and back-projection, certain information loss will inevitably be caused
to the point cloud and category inconsistency will be introduced in the
back-projection stage;Another solution is the voxel-based point cloud
segmentation method, which divides the point cloud into small grids one by
one.However, the point cloud is sparse, and the direct use of 3D convolutional
neural network inevitably wastes computing resources. In this paper, we propose
a feature extraction module based on multi-scale ultra-sparse convolution and a
feature selection module based on channel attention, and build a point cloud
segmentation network framework based on this.By introducing multi-scale sparse
convolution, network could capture richer feature information based on
convolution kernels of different sizes, improving the segmentation result of
point cloud segmentation.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-05-05 23:09:00.147124826 UTC">2022-05-05 23:09:00 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>