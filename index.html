<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-07-15T01:30:00Z">07-15</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Long-term Dependencies and Short-term Correlations in Patient Journey Data with Temporal Attention Networks for Health Prediction. (arXiv:2207.06414v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06414">
<div class="article-summary-box-inner">
<span><p>Building models for health prediction based on Electronic Health Records
(EHR) has become an active research area. EHR patient journey data consists of
patient time-ordered clinical events/visits from patients. Most existing
studies focus on modeling long-term dependencies between visits, without
explicitly taking short-term correlations between consecutive visits into
account, where irregular time intervals, incorporated as auxiliary information,
are fed into health prediction models to capture latent progressive patterns of
patient journeys. We present a novel deep neural network with four modules to
take into account the contributions of various variables for health prediction:
i) the Stacked Attention module strengthens the deep semantics in clinical
events within each patient journey and generates visit embeddings, ii) the
Short-Term Temporal Attention module models short-term correlations between
consecutive visit embeddings while capturing the impact of time intervals
within those visit embeddings, iii) the Long-Term Temporal Attention module
models long-term dependencies between visit embeddings while capturing the
impact of time intervals within those visit embeddings, iv) and finally, the
Coupled Attention module adaptively aggregates the outputs of Short-Term
Temporal Attention and Long-Term Temporal Attention modules to make health
predictions. Experimental results on MIMIC-III demonstrate superior predictive
accuracy of our model compared to existing state-of-the-art methods, as well as
the interpretability and robustness of this approach. Furthermore, we found
that modeling short-term correlations contributes to local priors generation,
leading to improved predictive modeling of patient journeys.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Robustly Optimized Long Text to Math Models for Numerical Reasoning On FinQA. (arXiv:2207.06490v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06490">
<div class="article-summary-box-inner">
<span><p>Numerical reasoning is required when solving most problems in our life, but
it has been neglected in previous artificial intelligence researches. FinQA
challenge has been organized to strengthen the study on numerical reasoning
where the participants are asked to predict the numerical reasoning program to
solve financial question. The result of FinQA will be evaluated by both
execution accuracy and program accuracy. In this paper, we present our approach
to tackle the task objective by developing models with different specialized
capabilities and fusing their strength. Overall, our approach achieves the 1st
place in FinQA challenge, with 71.93% execution accuracy and 67.03% program
accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A tool to overcome technical barriers for bias assessment in human language technologies. (arXiv:2207.06591v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06591">
<div class="article-summary-box-inner">
<span><p>Automatic processing of language is becoming pervasive in our lives, often
taking central roles in our decision making, like choosing the wording for our
messages and mails, translating our readings, or even having full conversations
with us. Word embeddings are a key component of modern natural language
processing systems. They provide a representation of words that has boosted the
performance of many applications, working as a semblance of meaning. Word
embeddings seem to capture a semblance of the meaning of words from raw text,
but, at the same time, they also distill stereotypes and societal biases which
are subsequently relayed to the final applications. Such biases can be
discriminatory. It is very important to detect and mitigate those biases, to
prevent discriminatory behaviors of automated processes, which can be much more
harmful than in the case of humans because their of their scale. There are
currently many tools and techniques to detect and mitigate biases in word
embeddings, but they present many barriers for the engagement of people without
technical skills. As it happens, most of the experts in bias, either social
scientists or people with deep knowledge of the context where bias is harmful,
do not have such skills, and they cannot engage in the processes of bias
detection because of the technical barriers. We have studied the barriers in
existing tools and have explored their possibilities and limitations with
different kinds of users. With this exploration, we propose to develop a tool
that is specially aimed to lower the technical barriers and provide the
exploration power to address the requirements of experts, scientists and people
in general who are willing to audit these technologies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Two-Pass Low Latency End-to-End Spoken Language Understanding. (arXiv:2207.06670v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06670">
<div class="article-summary-box-inner">
<span><p>End-to-end (E2E) models are becoming increasingly popular for spoken language
understanding (SLU) systems and are beginning to achieve competitive
performance to pipeline-based approaches. However, recent work has shown that
these models struggle to generalize to new phrasings for the same intent
indicating that models cannot understand the semantic content of the given
utterance. In this work, we incorporated language models pre-trained on
unlabeled text data inside E2E-SLU frameworks to build strong semantic
representations. Incorporating both semantic and acoustic information can
increase the inference time, leading to high latency when deployed for
applications like voice assistants. We developed a 2-pass SLU system that makes
low latency prediction using acoustic information from the few seconds of the
audio in the first pass and makes higher quality prediction in the second pass
by combining semantic and acoustic representations. We take inspiration from
prior work on 2-pass end-to-end speech recognition systems that attends on both
audio and first-pass hypothesis using a deliberation network. The proposed
2-pass SLU system outperforms the acoustic-based SLU model on the Fluent Speech
Commands Challenge Set and SLURP dataset and reduces latency, thus improving
user experience. Our code and models are publicly available as part of the
ESPnet-SLU toolkit.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Overview of Abusive and Threatening Language Detection in Urdu at FIRE 2021. (arXiv:2207.06710v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06710">
<div class="article-summary-box-inner">
<span><p>With the growth of social media platform influence, the effect of their
misuse becomes more and more impactful. The importance of automatic detection
of threatening and abusive language can not be overestimated. However, most of
the existing studies and state-of-the-art methods focus on English as the
target language, with limited work on low- and medium-resource languages. In
this paper, we present two shared tasks of abusive and threatening language
detection for the Urdu language which has more than 170 million speakers
worldwide. Both are posed as binary classification tasks where participating
systems are required to classify tweets in Urdu into two classes, namely: (i)
Abusive and Non-Abusive for the first task, and (ii) Threatening and
Non-Threatening for the second. We present two manually annotated datasets
containing tweets labelled as (i) Abusive and Non-Abusive, and (ii) Threatening
and Non-Threatening. The abusive dataset contains 2400 annotated tweets in the
train part and 1100 annotated tweets in the test part. The threatening dataset
contains 6000 annotated tweets in the train part and 3950 annotated tweets in
the test part. We also provide logistic regression and BERT-based baseline
classifiers for both tasks. In this shared task, 21 teams from six countries
registered for participation (India, Pakistan, China, Malaysia, United Arab
Emirates, and Taiwan), 10 teams submitted their runs for Subtask A, which is
Abusive Language Detection and 9 teams submitted their runs for Subtask B,
which is Threatening Language detection, and seven teams submitted their
technical reports. The best performing system achieved an F1-score value of
0.880 for Subtask A and 0.545 for Subtask B. For both subtasks, m-Bert based
transformer model showed the best performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Layout-Aware Information Extraction for Document-Grounded Dialogue: Dataset, Method and Demonstration. (arXiv:2207.06717v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06717">
<div class="article-summary-box-inner">
<span><p>Building document-grounded dialogue systems have received growing interest as
documents convey a wealth of human knowledge and commonly exist in enterprises.
Wherein, how to comprehend and retrieve information from documents is a
challenging research problem. Previous work ignores the visual property of
documents and treats them as plain text, resulting in incomplete modality. In
this paper, we propose a Layout-aware document-level Information Extraction
dataset, LIE, to facilitate the study of extracting both structural and
semantic knowledge from visually rich documents (VRDs), so as to generate
accurate responses in dialogue systems. LIE contains 62k annotations of three
extraction tasks from 4,061 pages in product and official documents, becoming
the largest VRD-based information extraction dataset to the best of our
knowledge. We also develop benchmark methods that extend the token-based
language model to consider layout features like humans. Empirical results show
that layout is critical for VRD-based extraction, and system demonstration also
verifies that the extracted knowledge can help locate the answers that users
care about.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open Terminology Management and Sharing Toolkit for Federation of Terminology Databases. (arXiv:2207.06729v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06729">
<div class="article-summary-box-inner">
<span><p>Consolidated access to current and reliable terms from different subject
fields and languages is necessary for content creators and translators.
Terminology is also needed in AI applications such as machine translation,
speech recognition, information extraction, and other natural language
processing tools. In this work, we facilitate standards-based sharing and
management of terminology resources by providing an open terminology management
solution - the EuroTermBank Toolkit. It allows organisations to manage and
search their terms, create term collections, and share them within and outside
the organisation by participating in the network of federated databases. The
data curated in the federated databases are automatically shared with
EuroTermBank, the largest multilingual terminology resource in Europe, allowing
translators and language service providers as well as researchers and students
to access terminology resources in their most current version.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FFTc: An MLIR Dialect for Developing HPC Fast Fourier Transform Libraries. (arXiv:2207.06803v1 [cs.MS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06803">
<div class="article-summary-box-inner">
<span><p>Discrete Fourier Transform (DFT) libraries are one of the most critical
software components for scientific computing. Inspired by FFTW, a widely used
library for DFT HPC calculations, we apply compiler technologies for the
development of HPC Fourier transform libraries. In this work, we introduce
FFTc, a domain-specific language, based on Multi-Level Intermediate
Representation (MLIR), for expressing Fourier Transform algorithms. We present
the initial design, implementation, and preliminary results of FFTc.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERTIN: Efficient Pre-Training of a Spanish Language Model using Perplexity Sampling. (arXiv:2207.06814v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06814">
<div class="article-summary-box-inner">
<span><p>The pre-training of large language models usually requires massive amounts of
resources, both in terms of computation and data. Frequently used web sources
such as Common Crawl might contain enough noise to make this pre-training
sub-optimal. In this work, we experiment with different sampling methods from
the Spanish version of mC4, and present a novel data-centric technique which we
name $\textit{perplexity sampling}$ that enables the pre-training of language
models in roughly half the amount of steps and using one fifth of the data. The
resulting models are comparable to the current state-of-the-art, and even
achieve better results for certain tasks. Our work is proof of the versatility
of Transformers, and paves the way for small teams to train their models on a
limited budget. Our models are available at this
$\href{https://huggingface.co/bertin-project}{URL}$.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Data-to-Text Generation Based on Small Datasets: Comparing the Added Value of Two Semi-Supervised Learning Approaches on Top of a Large Language Model. (arXiv:2207.06839v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06839">
<div class="article-summary-box-inner">
<span><p>This study discusses the effect of semi-supervised learning in combination
with pretrained language models for data-to-text generation. It is not known
whether semi-supervised learning is still helpful when a large-scale language
model is also supplemented. This study aims to answer this question by
comparing a data-to-text system only supplemented with a language model, to two
data-to-text systems that are additionally enriched by a data augmentation or a
pseudo-labeling semi-supervised learning approach.
</p>
<p>Results show that semi-supervised learning results in higher scores on
diversity metrics. In terms of output quality, extending the training set of a
data-to-text system with a language model using the pseudo-labeling approach
did increase text quality scores, but the data augmentation approach yielded
similar scores to the system without training set extension. These results
indicate that semi-supervised learning approaches can bolster output quality
and diversity, even when a language model is also present.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep versus Wide: An Analysis of Student Architectures for Task-Agnostic Knowledge Distillation of Self-Supervised Speech Models. (arXiv:2207.06867v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06867">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning (SSL) is seen as a very promising approach with high
performance for several speech downstream tasks. Since the parameters of SSL
models are generally so large that training and inference require a lot of
memory and computational cost, it is desirable to produce compact SSL models
without a significant performance degradation by applying compression methods
such as knowledge distillation (KD). Although the KD approach is able to shrink
the depth and/or width of SSL model structures, there has been little research
on how varying the depth and width impacts the internal representation of the
small-footprint model. This paper provides an empirical study that addresses
the question. We investigate the performance on SUPERB while varying the
structure and KD methods so as to keep the number of parameters constant; this
allows us to analyze the contribution of the representation introduced by
varying the model architecture. Experiments demonstrate that a certain depth is
essential for solving content-oriented tasks (e.g. automatic speech
recognition) accurately, whereas a certain width is necessary for achieving
high performance on several speaker-oriented tasks (e.g. speaker
identification). Based on these observations, we identify, for SUPERB, a more
compressed model with better performance than previous studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Augmentation for Low-Resource Quechua ASR Improvement. (arXiv:2207.06872v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06872">
<div class="article-summary-box-inner">
<span><p>Automatic Speech Recognition (ASR) is a key element in new services that
helps users to interact with an automated system. Deep learning methods have
made it possible to deploy systems with word error rates below 5% for ASR of
English. However, the use of these methods is only available for languages with
hundreds or thousands of hours of audio and their corresponding transcriptions.
For the so-called low-resource languages to speed up the availability of
resources that can improve the performance of their ASR systems, methods of
creating new resources on the basis of existing ones are being investigated. In
this paper we describe our data augmentation approach to improve the results of
ASR models for low-resource and agglutinative languages. We carry out
experiments developing an ASR for Quechua using the wav2letter++ model. We
reduced WER by 8.73% through our approach to the base model. The resulting ASR
model obtained 22.75% WER and was trained with 99 hours of original resources
and 99 hours of synthetic data obtained with a combination of text augmentation
and synthetic speech generati
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recurrent Memory Transformer. (arXiv:2207.06881v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06881">
<div class="article-summary-box-inner">
<span><p>Transformer-based models show their effectiveness across multiple domains and
tasks. The self-attention allows to combine information from all sequence
elements into context-aware representations. However, global and local
information has to be stored mostly in the same element-wise representations.
Moreover, the length of an input sequence is limited by quadratic computational
complexity of self-attention.
</p>
<p>In this work, we propose and study a memory-augmented segment-level recurrent
Transformer (Recurrent Memory Transformer). Memory allows to store and process
local and global information as well as to pass information between segments of
the long sequence with the help of recurrence. We implement a memory mechanism
with no changes to Transformer model by adding special memory tokens to the
input or output sequence. Then Transformer is trained to control both memory
operations and sequence representations processing.
</p>
<p>Results of experiments show that our model performs on par with the
Transformer-XL on language modeling for smaller memory sizes and outperforms it
for tasks that require longer sequence processing. We show that adding memory
tokens to Tr-XL is able to improve it performance. This makes Recurrent Memory
Transformer a promising architecture for applications that require learning of
long-term dependencies and general purpose in memory processing, such as
algorithmic tasks and reasoning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilinguals at SemEval-2022 Task 11: Complex NER in Semantically Ambiguous Settings for Low Resource Languages. (arXiv:2207.06882v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06882">
<div class="article-summary-box-inner">
<span><p>We leverage pre-trained language models to solve the task of complex NER for
two low-resource languages: Chinese and Spanish. We use the technique of Whole
Word Masking(WWM) to boost the performance of masked language modeling
objective on large and unsupervised corpora. We experiment with multiple neural
network architectures, incorporating CRF, BiLSTMs, and Linear Classifiers on
top of a fine-tuned BERT layer. All our models outperform the baseline by a
significant margin and our best performing model obtains a competitive position
on the evaluation leaderboard for the blind test set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beware the Rationalization Trap! When Language Model Explainability Diverges from our Mental Models of Language. (arXiv:2207.06897v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06897">
<div class="article-summary-box-inner">
<span><p>Language models learn and represent language differently than humans; they
learn the form and not the meaning. Thus, to assess the success of language
model explainability, we need to consider the impact of its divergence from a
user's mental model of language. In this position paper, we argue that in order
to avoid harmful rationalization and achieve truthful understanding of language
models, explanation processes must satisfy three main conditions: (1)
explanations have to truthfully represent the model behavior, i.e., have a high
fidelity; (2) explanations must be complete, as missing information distorts
the truth; and (3) explanations have to take the user's mental model into
account, progressively verifying a person's knowledge and adapting their
understanding. We introduce a decision tree model to showcase potential reasons
why current explanations fail to reach their objectives. We further emphasize
the need for human-centered design to explain the model from multiple
perspectives, progressively adapting explanations to changing user
expectations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Forming Trees with Treeformers. (arXiv:2207.06960v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06960">
<div class="article-summary-box-inner">
<span><p>Popular models such as Transformers and LSTMs use tokens as its unit of
information. That is, each token is encoded into a vector representation, and
those vectors are used directly in a computation. However, humans frequently
consider spans of tokens (i.e., phrases) instead of their constituent tokens.
In this paper we introduce Treeformer, an architecture inspired by the CKY
algorithm and Transformer which learns a composition operator and pooling
function in order to construct hierarchical encodings for phrases and
sentences. Our extensive experiments demonstrate the benefits of incorporating
a hierarchical structure into the Transformer, and show significant
improvements compared to a baseline Transformer in machine translation,
abstractive summarization, and various natural language understanding tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scene Text Recognition with Permuted Autoregressive Sequence Models. (arXiv:2207.06966v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06966">
<div class="article-summary-box-inner">
<span><p>Context-aware STR methods typically use internal autoregressive (AR) language
models (LM). Inherent limitations of AR models motivated two-stage methods
which employ an external LM. The conditional independence of the external LM on
the input image may cause it to erroneously rectify correct predictions,
leading to significant inefficiencies. Our method, PARSeq, learns an ensemble
of internal AR LMs with shared weights using Permutation Language Modeling. It
unifies context-free non-AR and context-aware AR inference, and iterative
refinement using bidirectional context. Using synthetic training data, PARSeq
achieves state-of-the-art (SOTA) results in STR benchmarks (91.9% accuracy) and
more challenging datasets. It establishes new SOTA results (96.0% accuracy)
when trained on real data. PARSeq is optimal on accuracy vs parameter count,
FLOPS, and latency because of its simple, unified structure and parallel token
processing. Due to its extensive use of attention, it is robust on
arbitrarily-oriented text which is common in real-world images. Code,
pretrained weights, and data are available at: https://github.com/baudm/parseq.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Modelling with Pixels. (arXiv:2207.06991v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06991">
<div class="article-summary-box-inner">
<span><p>Language models are defined over a finite set of inputs, which creates a
vocabulary bottleneck when we attempt to scale the number of supported
languages. Tackling this bottleneck results in a trade-off between what can be
represented in the embedding matrix and computational issues in the output
layer. This paper introduces PIXEL, the Pixel-based Encoder of Language, which
suffers from neither of these issues. PIXEL is a pretrained language model that
renders text as images, making it possible to transfer representations across
languages based on orthographic similarity or the co-activation of pixels.
PIXEL is trained to reconstruct the pixels of masked patches, instead of
predicting a distribution over tokens. We pretrain the 86M parameter PIXEL
model on the same English data as BERT and evaluate on syntactic and semantic
tasks in typologically diverse languages, including various non-Latin scripts.
We find that PIXEL substantially outperforms BERT on syntactic and semantic
processing tasks on scripts that are not found in the pretraining data, but
PIXEL is slightly weaker than BERT when working with Latin scripts.
Furthermore, we find that PIXEL is more robust to noisy text inputs than BERT,
further confirming the benefits of modelling language with pixels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to translate by learning to communicate. (arXiv:2207.07025v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07025">
<div class="article-summary-box-inner">
<span><p>We formulate and test a technique to use Emergent Communication (EC) with a
pretrained multilingual model to improve on modern Unsupervised NMT systems,
especially for low-resource languages. It has been argued that the currently
dominant paradigm in NLP of pretraining on text-only corpora will not yield
robust natural language understanding systems, and the need for grounded,
goal-oriented, and interactive language learning has been highlighted. In our
approach, we embed a modern multilingual model (mBART, Liu et. al. 2020) into
an EC image-reference game, in which the model is incentivized to use
multilingual generations to accomplish a vision-grounded task, with the
hypothesis that this will align multiple languages to a shared task space. We
present two variants of EC Fine-Tuning (Steinert-Threlkeld et. al. 2022), one
of which outperforms a backtranslation-based baseline in 6/8 translation
settings, and proves especially beneficial for the very low-resource languages
of Nepali and Sinhala.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Single Self-Supervised Model for Many Speech Modalities Enables Zero-Shot Modality Transfer. (arXiv:2207.07036v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07036">
<div class="article-summary-box-inner">
<span><p>While audio-visual speech models can yield superior performance and
robustness compared to audio-only models, their development and adoption are
hindered by the lack of labeled and unlabeled audio-visual data and the cost to
deploy one model per modality. In this paper, we present u-HuBERT, a
self-supervised pre-training framework that can leverage both multimodal and
unimodal speech with a unified masked cluster prediction objective. By
utilizing modality dropout during pre-training, we demonstrate that a single
fine-tuned model can achieve performance on par or better than the
state-of-the-art modality-specific models. Moreover, our model fine-tuned only
on audio can perform well with audio-visual and visual speech input, achieving
zero-shot modality generalization for speech recognition and speaker
verification. In particular, our single model yields 1.2%/1.4%/27.2% speech
recognition word error rate on LRS3 with audio-visual/audio/visual input.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language models show human-like content effects on reasoning. (arXiv:2207.07051v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07051">
<div class="article-summary-box-inner">
<span><p>Abstract reasoning is a key ability for an intelligent system. Large language
models achieve above-chance performance on abstract reasoning tasks, but
exhibit many imperfections. However, human abstract reasoning is also
imperfect, and depends on our knowledge and beliefs about the content of the
reasoning problem. For example, humans reason much more reliably about logical
rules that are grounded in everyday situations than arbitrary rules about
abstract attributes. The training experiences of language models similarly
endow them with prior expectations that reflect human knowledge and beliefs. We
therefore hypothesized that language models would show human-like content
effects on abstract reasoning problems. We explored this hypothesis across
three logical reasoning tasks: natural language inference, judging the logical
validity of syllogisms, and the Wason selection task (Wason, 1968). We find
that state of the art large language models (with 7 or 70 billion parameters;
Hoffman et al., 2022) reflect many of the same patterns observed in humans
across these tasks -- like humans, models reason more effectively about
believable situations than unrealistic or abstract ones. Our findings have
implications for understanding both these cognitive effects, and the factors
that contribute to language model performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Confident Adaptive Language Modeling. (arXiv:2207.07061v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07061">
<div class="article-summary-box-inner">
<span><p>Recent advances in Transformer-based large language models (LLMs) have led to
significant performance improvements across many tasks. These gains come with a
drastic increase in the models' size, potentially leading to slow and costly
use at inference time. In practice, however, the series of generations made by
LLMs is composed of varying levels of difficulty. While certain predictions
truly benefit from the models' full capacity, other continuations are more
trivial and can be solved with reduced compute. In this work, we introduce
Confident Adaptive Language Modeling (CALM), a framework for dynamically
allocating different amounts of compute per input and generation timestep.
Early exit decoding involves several challenges that we address here, such as:
(1) what confidence measure to use; (2) connecting sequence-level constraints
to local per-token exit decisions; and (3) attending back to missing hidden
representations due to early exits in previous tokens. Through theoretical
analysis and empirical experiments on three diverse text generation tasks, we
demonstrate the efficacy of our framework in reducing compute -- potential
speedup of up to $\times 3$ -- while provably maintaining high performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parameter-Efficient Prompt Tuning Makes Generalized and Calibrated Neural Text Retrievers. (arXiv:2207.07087v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07087">
<div class="article-summary-box-inner">
<span><p>Prompt tuning attempts to update few task-specific parameters in pre-trained
models. It has achieved comparable performance to fine-tuning of the full
parameter set on both language understanding and generation tasks. In this
work, we study the problem of prompt tuning for neural text retrievers. We
introduce parameter-efficient prompt tuning for text retrieval across
in-domain, cross-domain, and cross-topic settings. Through an extensive
analysis, we show that the strategy can mitigate the two issues --
parameter-inefficiency and weak generalizability -- faced by fine-tuning based
retrieval methods. Notably, it can significantly improve the out-of-domain
zero-shot generalization of the retrieval models. By updating only 0.1% of the
model parameters, the prompt tuning strategy can help retrieval models achieve
better generalization performance than traditional methods in which all
parameters are updated. Finally, to facilitate research on retrievers'
cross-topic generalizability, we curate and release an academic retrieval
dataset with 18K query-results pairs in 87 topics, making it the largest
topic-specific one to date.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A comparison of latent semantic analysis and correspondence analysis of document-term matrices. (arXiv:2108.06197v3 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06197">
<div class="article-summary-box-inner">
<span><p>Latent semantic analysis (LSA) and correspondence analysis (CA) are two
techniques that use a singular value decomposition (SVD) for dimensionality
reduction. LSA has been extensively used to obtain low-dimensional
representations that capture relationships among documents and terms. In this
article, we present a theoretical analysis and comparison of the two techniques
in the context of document-term matrices. We show that CA has some attractive
properties as compared to LSA, for instance that effects of margins arising
from differing document-lengths and term-frequencies are effectively
eliminated, so that the CA solution is optimally suited to focus on
relationships among documents and terms. A unifying framework is proposed that
includes both CA and LSA as special cases. We empirically compare CA to various
LSA based methods on text categorization in English and authorship attribution
on historical Dutch texts, and find that CA performs significantly better. We
also apply CA to a long-standing question regarding the authorship of the Dutch
national anthem Wilhelmus and provide further support that it can be attributed
to the author Datheen, amongst several contenders.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">2020 U.S. presidential election in swing states: Gender differences in Twitter conversations. (arXiv:2108.09416v2 [cs.SI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09416">
<div class="article-summary-box-inner">
<span><p>Social media is commonly used by the public during election campaigns to
express their opinions regarding different issues. Among various social media
channels, Twitter provides an efficient platform for researchers and
politicians to explore public opinion regarding a wide range of topics such as
the economy and foreign policy. Current literature mainly focuses on analyzing
the content of tweets without considering the gender of users. This research
collects and analyzes a large number of tweets and uses computational, human
coding, and statistical analyses to identify topics in more than 300,000 tweets
posted during the 2020 U.S. presidential election and to compare female and
male users regarding the average weight of the discussed topics. Our findings
are based upon a wide range of topics, such as tax, climate change, and the
COVID-19 pandemic. Out of the topics, there exists a significant difference
between female and male users for more than 70% of topics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Security Analysis Based on Random Geometry Theory for Satellite-Terrestrial-Vehicle Network. (arXiv:2112.14192v2 [cs.IT] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14192">
<div class="article-summary-box-inner">
<span><p>Driven by B5G and 6G technologies, multi-network fusion is an indispensable
tendency for future communications. In this paper, we focus on and analyze the
\emph{security performance} (SP) of the \emph{satellite-terrestrial downlink
transmission} (STDT). Here, the STDT is composed of a satellite network and a
vehicular network with a legitimate mobile receiver and an mobile eavesdropper
distributing. To theoretically analyze the SP of this system from the
perspective of mobile terminals better, the random geometry theory is adopted,
which assumes that both terrestrial vehicles are distributed stochastically in
one beam of the satellite. Furthermore, based on this theory, the closed-form
analytical expressions for two crucial and specific indicators in the STDT are
derived, respectively, the secrecy outage probability and the ergodic secrecy
capacity. Additionally, several related variables restricting the SP of the
STDT are discussed, and specific schemes are presented to enhance the SP. Then,
the asymptotic property is investigated in the high signal-to-noise ratio
scenario, and accurate and asymptotic closed-form expressions are given.
Finally, simulation results show that, under the precondition of guaranteeing
the reliability of the STDT, the asymptotic solutions outperform the
corresponding accurate results significantly in the effectiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection. (arXiv:2203.09509v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09509">
<div class="article-summary-box-inner">
<span><p>Toxic language detection systems often falsely flag text that contains
minority group mentions as toxic, as those groups are often the targets of
online hate. Such over-reliance on spurious correlations also causes systems to
struggle with detecting implicitly toxic language. To help mitigate these
issues, we create ToxiGen, a new large-scale and machine-generated dataset of
274k toxic and benign statements about 13 minority groups. We develop a
demonstration-based prompting framework and an adversarial
classifier-in-the-loop decoding method to generate subtly toxic and benign text
with a massive pretrained language model. Controlling machine generation in
this way allows ToxiGen to cover implicitly toxic text at a larger scale, and
about more demographic groups, than previous resources of human-written text.
We conduct a human evaluation on a challenging subset of ToxiGen and find that
annotators struggle to distinguish machine-generated text from human-written
language. We also find that 94.5% of toxic examples are labeled as hate speech
by human annotators. Using three publicly-available datasets, we show that
finetuning a toxicity classifier on our data improves its performance on
human-written data substantially. We also demonstrate that ToxiGen can be used
to fight machine-generated toxicity as finetuning improves the classifier
significantly on our evaluation subset. Our code and data can be found at
https://github.com/microsoft/ToxiGen.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Meta-learning for Low-resource Text Classification and Generation via Memory Imitation. (arXiv:2203.11670v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11670">
<div class="article-summary-box-inner">
<span><p>Building models of natural language processing (NLP) is challenging in
low-resource scenarios where only limited data are available.
Optimization-based meta-learning algorithms achieve promising results in
low-resource scenarios by adapting a well-generalized model initialization to
handle new tasks. Nonetheless, these approaches suffer from the memorization
overfitting issue, where the model tends to memorize the meta-training tasks
while ignoring support sets when adapting to new tasks. To address this issue,
we propose a memory imitation meta-learning (MemIML) method that enhances the
model's reliance on support sets for task adaptation. Specifically, we
introduce a task-specific memory module to store support set information and
construct an imitation module to force query sets to imitate the behaviors of
some representative support-set samples stored in the memory. A theoretical
analysis is provided to prove the effectiveness of our method, and empirical
results also demonstrate that our method outperforms competitive baselines on
both text classification and generation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speech-enhanced and Noise-aware Networks for Robust Speech Recognition. (arXiv:2203.13696v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13696">
<div class="article-summary-box-inner">
<span><p>Compensation for channel mismatch and noise interference is essential for
robust automatic speech recognition. Enhanced speech has been introduced into
the multi-condition training of acoustic models to improve their generalization
ability. In this paper, a noise-aware training framework based on two cascaded
neural structures is proposed to jointly optimize speech enhancement and speech
recognition. The feature enhancement module is composed of a multi-task
autoencoder, where noisy speech is decomposed into clean speech and noise. By
concatenating its enhanced, noise-aware, and noisy features for each frame, the
acoustic-modeling module maps each feature-augmented frame into a triphone
state by optimizing the lattice-free maximum mutual information and cross
entropy between the predicted and actual state sequences. On top of the
factorized time delay neural network (TDNN-F) and its convolutional variant
(CNN-TDNNF), both with SpecAug, the two proposed systems achieve word error
rate (WER) of 3.90% and 3.55%, respectively, on the Aurora-4 task. Compared
with the best existing systems that use bigram and trigram language models for
decoding, the proposed CNN-TDNNF-based system achieves a relative WER reduction
of 15.20% and 33.53%, respectively. In addition, the proposed CNN-TDNNF-based
system also outperforms the baseline CNN-TDNNF system on the AMI task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Parallelize in a Shared-Memory Environment with Transformers. (arXiv:2204.12835v4 [cs.DC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.12835">
<div class="article-summary-box-inner">
<span><p>In past years, the world has switched to many-core and multi-core shared
memory architectures. As a result, there is a growing need to utilize these
architectures by introducing shared memory parallelization schemes to software
applications. OpenMP is the most comprehensive API that implements such
schemes, characterized by a readable interface. Nevertheless, introducing
OpenMP into code is challenging due to pervasive pitfalls in management of
parallel shared memory. To facilitate the performance of this task, many
source-to-source (S2S) compilers have been created over the years, tasked with
inserting OpenMP directives into code automatically. In addition to having
limited robustness to their input format, these compilers still do not achieve
satisfactory coverage and precision in locating parallelizable code and
generating appropriate directives. In this work, we propose leveraging recent
advances in ML techniques, specifically in natural language processing (NLP),
to replace S2S compilers altogether. We create a database (corpus), Open-OMP,
specifically for this goal. Open-OMP contains over 28,000 code snippets, half
of which contain OpenMP directives while the other half do not need
parallelization at all with high probability. We use the corpus to train
systems to automatically classify code segments in need of parallelization, as
well as suggest individual OpenMP clauses. We train several transformer models,
named PragFormer, for these tasks, and show that they outperform
statistically-trained baselines and automatic S2S parallelization compilers in
both classifying the overall need for an OpenMP directive and the introduction
of private and reduction clauses.
</p>
<p>Our source code and database are available at:
https://github.com/Scientific-Computing-Lab-NRCN/PragFormer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-Parametric Domain Adaptation for End-to-End Speech Translation. (arXiv:2205.11211v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11211">
<div class="article-summary-box-inner">
<span><p>End-to-End Speech Translation (E2E-ST) has received increasing attention due
to the potential of its less error propagation, lower latency, and fewer
parameters. However, the effectiveness of neural-based approaches to this task
is severely limited by the available training corpus, especially for domain
adaptation where in-domain triplet training data is scarce or nonexistent. In
this paper, we propose a novel non-parametric method that leverages
domain-specific text translation corpus to achieve domain adaptation for the
E2E-ST system. To this end, we first incorporate an additional encoder into the
pre-trained E2E-ST model to realize text translation modelling, and then unify
the decoder's output representation for text and speech translation tasks by
reducing the correspondent representation mismatch in available triplet
training data. During domain adaptation, a k-nearest-neighbor (kNN) classifier
is introduced to produce the final translation distribution using the external
datastore built by the domain-specific text translation corpus, while the
universal output representation is adopted to perform a similarity search.
Experiments on the Europarl-ST benchmark demonstrate that when in-domain text
translation data is involved only, our proposed approach significantly improves
baseline by 12.82 BLEU on average in all translation directions, even
outperforming the strong in-domain fine-tuning method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Medical Scientific Table-to-Text Generation with Human-in-the-Loop under the Data Sparsity Constraint. (arXiv:2205.12368v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12368">
<div class="article-summary-box-inner">
<span><p>Structured (tabular) data in the preclinical and clinical domains contains
valuable information about individuals and an efficient table-to-text
summarization system can drastically reduce manual efforts to condense this
data into reports. However, in practice, the problem is heavily impeded by the
data paucity, data sparsity and inability of the state-of-the-art natural
language generation models (including T5, PEGASUS and GPT-Neo) to produce
accurate and reliable outputs. In this paper, we propose a novel table-to-text
approach and tackle these problems with a novel two-step architecture which is
enhanced by auto-correction, copy mechanism and synthetic data augmentation.
The study shows that the proposed approach selects salient biomedical entities
and values from structured data with improved precision (up to 0.13 absolute
increase) of copying the tabular values to generate coherent and accurate text
for assay validation reports and toxicology reports. Moreover, we also
demonstrate a light-weight adaptation of the proposed system to new datasets by
fine-tuning with as little as 40\% training examples. The outputs of our model
are validated by human experts in the Human-in-the-Loop scenario.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language with Vision: a Study on Grounded Word and Sentence Embeddings. (arXiv:2206.08823v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08823">
<div class="article-summary-box-inner">
<span><p>Language grounding to vision is an active field of research aiming to enrich
text-based representations of word meanings by leveraging perceptual knowledge
from vision. Despite many attempts at language grounding, it is still unclear
how to effectively inject visual knowledge into the word embeddings of a
language in such a way that a proper balance of textual and visual knowledge is
maintained. Some common concerns are the following. Is visual grounding
beneficial for abstract words or is its contribution only limited to concrete
words? What is the optimal way of bridging the gap between text and vision? How
much do we gain by visually grounding textual embeddings? The present study
addresses these questions by proposing a simple yet very effective grounding
approach for pre-trained word embeddings. Our model aligns textual embeddings
with vision while largely preserving the distributional statistics that
characterize word use in text corpora. By applying a learned alignment, we are
able to generate visually grounded embeddings for unseen words, including
abstract words. A series of evaluations on word similarity benchmarks shows
that visual grounding is beneficial not only for concrete words, but also for
abstract words. We also show that our method for visual grounding offers
advantages for contextualized embeddings, but only when these are trained on
corpora of relatively modest size. Code and grounded embeddings for English are
available at https://github.com/Hazel1994/Visually_Grounded_Word_Embeddings_2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models (Mostly) Know What They Know. (arXiv:2207.05221v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05221">
<div class="article-summary-box-inner">
<span><p>We study whether language models can evaluate the validity of their own
claims and predict which questions they will be able to answer correctly. We
first show that larger models are well-calibrated on diverse multiple choice
and true/false questions when they are provided in the right format. Thus we
can approach self-evaluation on open-ended sampling tasks by asking models to
first propose answers, and then to evaluate the probability "P(True)" that
their answers are correct. We find encouraging performance, calibration, and
scaling for P(True) on a diverse array of tasks. Performance at self-evaluation
further improves when we allow models to consider many of their own samples
before predicting the validity of one specific possibility. Next, we
investigate whether models can be trained to predict "P(IK)", the probability
that "I know" the answer to a question, without reference to any particular
proposed answer. Models perform well at predicting P(IK) and partially
generalize across tasks, though they struggle with calibration of P(IK) on new
tasks. The predicted P(IK) probabilities also increase appropriately in the
presence of relevant source materials in the context, and in the presence of
hints towards the solution of mathematical word problems. We hope these
observations lay the groundwork for training more honest models, and for
investigating how honesty generalizes to cases where models are trained on
objectives other than the imitation of human writing.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Open High-Resolution Satellite Imagery: The WorldStrat Dataset -- With Application to Super-Resolution. (arXiv:2207.06418v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06418">
<div class="article-summary-box-inner">
<span><p>Analyzing the planet at scale with satellite imagery and machine learning is
a dream that has been constantly hindered by the cost of difficult-to-access
highly-representative high-resolution imagery. To remediate this, we introduce
here the WorldStrat dataset. The largest and most varied such publicly
available dataset, at Airbus SPOT 6/7 satellites' high resolution of up to 1.5
m/pixel, empowered by European Space Agency's Phi-Lab as part of the ESA-funded
QueryPlanet project, we curate nearly 10,000 sqkm of unique locations to ensure
stratified representation of all types of land-use across the world: from
agriculture to ice caps, from forests to multiple urbanization densities. We
also enrich those with locations typically under-represented in ML datasets:
sites of humanitarian interest, illegal mining sites, and settlements of
persons at risk. We temporally-match each high-resolution image with multiple
low-resolution images from the freely accessible lower-resolution Sentinel-2
satellites at 10 m/pixel. We accompany this dataset with an open-source Python
package to: rebuild or extend the WorldStrat dataset, train and infer baseline
algorithms, and learn with abundant tutorials, all compatible with the popular
EO-learn toolbox. We hereby hope to foster broad-spectrum applications of ML to
satellite imagery, and possibly develop from free public low-resolution
Sentinel2 imagery the same power of analysis allowed by costly private
high-resolution imagery. We illustrate this specific point by training and
releasing several highly compute-efficient baselines on the task of Multi-Frame
Super-Resolution. High-resolution Airbus imagery is CC BY-NC, while the labels
and Sentinel2 imagery are CC BY, and the source code and pre-trained models
under BSD. The dataset is available at https://zenodo.org/record/6810792 and
the software package at https://github.com/worldstrat/worldstrat .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph CNN for Moving Object Detection in Complex Environments from Unseen Videos. (arXiv:2207.06440v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06440">
<div class="article-summary-box-inner">
<span><p>Moving Object Detection (MOD) is a fundamental step for many computer vision
applications. MOD becomes very challenging when a video sequence captured from
a static or moving camera suffers from the challenges: camouflage, shadow,
dynamic backgrounds, and lighting variations, to name a few. Deep learning
methods have been successfully applied to address MOD with competitive
performance. However, in order to handle the overfitting problem, deep learning
methods require a large amount of labeled data which is a laborious task as
exhaustive annotations are always not available. Moreover, some MOD deep
learning methods show performance degradation in the presence of unseen video
sequences because the testing and training splits of the same sequences are
involved during the network learning process. In this work, we pose the problem
of MOD as a node classification problem using Graph Convolutional Neural
Networks (GCNNs). Our algorithm, dubbed as GraphMOD-Net, encompasses instance
segmentation, background initialization, feature extraction, and graph
construction. GraphMOD-Net is tested on unseen videos and outperforms
state-of-the-art methods in unsupervised, semi-supervised, and supervised
learning in several challenges of the Change Detection 2014 (CDNet2014) and
UCSD background subtraction datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Imaging through the Atmosphere using Turbulence Mitigation Transformer. (arXiv:2207.06465v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06465">
<div class="article-summary-box-inner">
<span><p>Restoring images distorted by atmospheric turbulence is a long-standing
problem due to the spatially varying nature of the distortion, nonlinearity of
the image formation process, and scarcity of training and testing data.
Existing methods often have strong statistical assumptions on the distortion
model which in many cases will lead to a limited performance in real-world
scenarios as they do not generalize. To overcome the challenge, this paper
presents an end-to-end physics-driven approach that is efficient and can
generalize to real-world turbulence. On the data synthesis front, we
significantly increase the image resolution that can be handled by the SOTA
turbulence simulator by approximating the random field via wide-sense
stationarity. The new data synthesis process enables the generation of
large-scale multi-level turbulence and ground truth pairs for training. On the
network design front, we propose the turbulence mitigation transformer (TMT), a
two stage U-Net shaped multi-frame restoration network which has a noval
efficient self-attention mechanism named temporal channel joint attention
(TCJA). We also introduce a new training scheme that is enabled by the new
simulator, and we design new transformer units to reduce the memory
consumption. Experimental results on both static and dynamic scenes are
promising, including various real turbulence scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analysis of face detection, face landmarking, and face recognition performance with masked face images. (arXiv:2207.06478v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06478">
<div class="article-summary-box-inner">
<span><p>Face recognition has become an essential task in our lives. However, the
current COVID-19 pandemic has led to the widespread use of face masks. The
effect of wearing face masks is currently an understudied issue. The aim of
this paper is to analyze face detection, face landmarking, and face recognition
performance with masked face images. HOG and CNN face detectors are used for
face detection in combination with 5-point and 68-point face landmark
predictors and VGG16 face recognition model is used for face recognition on
masked and unmasked images. We found that the performance of face detection,
face landmarking, and face recognition is negatively impacted by face masks
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Study on Image Filtering -- Techniques, Algorithm and Applications. (arXiv:2207.06481v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06481">
<div class="article-summary-box-inner">
<span><p>Image processing is one of the most immerging and widely growing techniques
making it a lively research field. Image processing is converting an image to a
digital format and then doing different operations on it, such as improving the
image or extracting various valuable data. Image filtering is one of the
fascinating applications of image processing. Image filtering is a technique
for altering the size, shape, color, depth, smoothness, and other image
properties. It alters the pixels of the image to transform it into the desired
form using different types of graphical editing methods through graphic design
and editing software. This paper introduces various image filtering techniques
and their wide applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Data-Efficient Deep Learning Framework for Segmentation and Classification of Histopathology Images. (arXiv:2207.06489v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06489">
<div class="article-summary-box-inner">
<span><p>The current study of cell architecture of inflammation in histopathology
images commonly performed for diagnosis and research purposes excludes a lot of
information available on the biopsy slide. In autoimmune diseases, major
outstanding research questions remain regarding which cell types participate in
inflammation at the tissue level,and how they interact with each other. While
these questions can be partially answered using traditional methods, artificial
intelligence approaches for segmentation and classification provide a much more
efficient method to understand the architecture of inflammation in autoimmune
disease, holding a great promise for novel insights. In this paper, we
empirically develop deep learning approaches that uses dermatomyositis biopsies
of human tissue to detect and identify inflammatory cells. Our approach
improves classification performance by 26% and segmentation performance by 5%.
We also propose a novel post-processing autoencoder architecture that improves
segmentation performance by an additional 3%. We have open-sourced our approach
and architecture at https://github.com/pranavsinghps1/DEDL
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Model to Unite Them All: Personalized Federated Learning of Multi-Contrast MRI Synthesis. (arXiv:2207.06509v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06509">
<div class="article-summary-box-inner">
<span><p>Learning-based MRI translation involves a synthesis model that maps a
source-contrast onto a target-contrast image. Multi-institutional
collaborations are key to training synthesis models across broad datasets, yet
centralized training involves privacy risks. Federated learning (FL) is a
collaboration framework that instead adopts decentralized training to avoid
sharing imaging data and mitigate privacy concerns. However, FL-trained models
can be impaired by the inherent heterogeneity in the distribution of imaging
data. On the one hand, implicit shifts in image distribution are evident across
sites, even for a common translation task with fixed source-target
configuration. Conversely, explicit shifts arise within and across sites when
diverse translation tasks with varying source-target configurations are
prescribed. To improve reliability against domain shifts, here we introduce the
first personalized FL method for MRI Synthesis (pFLSynth). pFLSynth is based on
an adversarial model equipped with a mapper that produces latents specific to
individual sites and source-target contrasts. It leverages novel
personalization blocks that adaptively tune the statistics and weighting of
feature maps across the generator based on these latents. To further promote
site-specificity, partial model aggregation is employed over downstream layers
of the generator while upstream layers are retained locally. As such, pFLSynth
enables training of a unified synthesis model that can reliably generalize
across multiple sites and translation tasks. Comprehensive experiments on
multi-site datasets clearly demonstrate the enhanced performance of pFLSynth
against prior federated methods in multi-contrast MRI synthesis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lipschitz Continuity Retained Binary Neural Network. (arXiv:2207.06540v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06540">
<div class="article-summary-box-inner">
<span><p>Relying on the premise that the performance of a binary neural network can be
largely restored with eliminated quantization error between full-precision
weight vectors and their corresponding binary vectors, existing works of
network binarization frequently adopt the idea of model robustness to reach the
aforementioned objective. However, robustness remains to be an ill-defined
concept without solid theoretical support. In this work, we introduce the
Lipschitz continuity, a well-defined functional property, as the rigorous
criteria to define the model robustness for BNN. We then propose to retain the
Lipschitz continuity as a regularization term to improve the model robustness.
Particularly, while the popular Lipschitz-involved regularization methods often
collapse in BNN due to its extreme sparsity, we design the Retention Matrices
to approximate spectral norms of the targeted weight matrices, which can be
deployed as the approximation for the Lipschitz constant of BNNs without the
exact Lipschitz constant computation (NP-hard). Our experiments prove that our
BNN-specific regularization method can effectively strengthen the robustness of
BNN (testified on ImageNet-C), achieving state-of-the-art performance on CIFAR
and ImageNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Body Composition Assessment with Limited Field-of-view Computed Tomography: A Semantic Image Extension Perspective. (arXiv:2207.06551v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06551">
<div class="article-summary-box-inner">
<span><p>Field-of-view (FOV) tissue truncation beyond the lungs is common in routine
lung screening computed tomography (CT). This poses limitations for
opportunistic CT- based body composition (BC) assessment as key anatomical
structures are missing. Traditionally, extending the FOV of CT is considered as
a CT reconstruction problem using limited data. However, this approach relies
on the projection domain data which might not be available in application. In
this work, we formulate the problem from the semantic image extension
perspective which only requires image data as inputs. The proposed two-stage
method identifies a new FOV border based on the estimated extent of the
complete body and imputes missing tissues in the truncated region. The training
samples are simulated using CT slices with complete body in FOV, making the
model development self-supervised. We evaluate the validity of the proposed
method in automatic BC assessment using lung screening CT with limited FOV. The
proposed method effectively restores the missing tissues and reduces BC
assessment error introduced by FOV tissue truncation. In the BC assessment for
a large-scale lung screening CT dataset, this correction improves both the
intra-subject consistency and the correlation with anthropometric
approximations. The developed method is available at
https://github.com/MASILab/S-EFOV.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QML for Argoverse 2 Motion Forecasting Challenge. (arXiv:2207.06553v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06553">
<div class="article-summary-box-inner">
<span><p>To safely navigate in various complex traffic scenarios, autonomous driving
systems are generally equipped with a motion forecasting module to provide
vital information for the downstream planning module. For the real-world
onboard applications, both accuracy and latency of a motion forecasting model
are essential. In this report, we present an effective and efficient solution,
which ranks the 3rd place in the Argoverse 2 Motion Forecasting Challenge 2022.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Supervised Attribute Information Removal and Reconstruction for Image Manipulation. (arXiv:2207.06555v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06555">
<div class="article-summary-box-inner">
<span><p>The goal of attribute manipulation is to control specified attribute(s) in
given images. Prior work approaches this problem by learning disentangled
representations for each attribute that enables it to manipulate the encoded
source attributes to the target attributes. However, encoded attributes are
often correlated with relevant image content. Thus, the source attribute
information can often be hidden in the disentangled features, leading to
unwanted image editing effects. In this paper, we propose an Attribute
Information Removal and Reconstruction (AIRR) network that prevents such
information hiding by learning how to remove the attribute information
entirely, creating attribute excluded features, and then learns to directly
inject the desired attributes in a reconstructed image. We evaluate our
approach on four diverse datasets with a variety of attributes including
DeepFashion Synthesis, DeepFashion Fine-grained Attribute, CelebA and
CelebA-HQ, where our model improves attribute manipulation accuracy and top-k
retrieval rate by 10% on average over prior work. A user study also reports
that AIRR manipulated images are preferred over prior work in up to 76% of
cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving the diagnosis of breast cancer based on biophysical ultrasound features utilizing machine learning. (arXiv:2207.06560v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06560">
<div class="article-summary-box-inner">
<span><p>The improved diagnostic accuracy of ultrasound breast examinations remains an
important goal. In this study, we propose a biophysical feature based machine
learning method for breast cancer detection to improve the performance beyond a
benchmark deep learning algorithm and to furthermore provide a color overlay
visual map of the probability of malignancy within a lesion. This overall
framework is termed disease specific imaging. Previously, 150 breast lesions
were segmented and classified utilizing a modified fully convolutional network
and a modified GoogLeNet, respectively. In this study multiparametric analysis
was performed within the contoured lesions. Features were extracted from
ultrasound radiofrequency, envelope, and log compressed data based on
biophysical and morphological models. The support vector machine with a
Gaussian kernel constructed a nonlinear hyperplane, and we calculated the
distance between the hyperplane and data point of each feature in
multiparametric space. The distance can quantitatively assess a lesion, and
suggest the probability of malignancy that is color coded and overlaid onto B
mode images. Training and evaluation were performed on in vivo patient data.
The overall accuracy for the most common types and sizes of breast lesions in
our study exceeded 98.0% for classification and 0.98 for an area under the
receiver operating characteristic curve, which is more precise than the
performance of radiologists and a deep learning system. Further, the
correlation between the probability and BI RADS enables a quantitative
guideline to predict breast cancer. Therefore, we anticipate that the proposed
framework can help radiologists achieve more accurate and convenient breast
cancer classification and detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Benign, Tempered, or Catastrophic: A Taxonomy of Overfitting. (arXiv:2207.06569v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06569">
<div class="article-summary-box-inner">
<span><p>The practical success of overparameterized neural networks has motivated the
recent scientific study of interpolating methods, which perfectly fit their
training data. Certain interpolating methods, including neural networks, can
fit noisy training data without catastrophically bad test performance, in
defiance of standard intuitions from statistical learning theory. Aiming to
explain this, a body of recent work has studied $\textit{benign overfitting}$,
a phenomenon where some interpolating methods approach Bayes optimality, even
in the presence of noise. In this work we argue that while benign overfitting
has been instructive and fruitful to study, many real interpolating methods
like neural networks $\textit{do not fit benignly}$: modest noise in the
training set causes nonzero (but non-infinite) excess risk at test time,
implying these models are neither benign nor catastrophic but rather fall in an
intermediate regime. We call this intermediate regime $\textit{tempered
overfitting}$, and we initiate its systematic study. We first explore this
phenomenon in the context of kernel (ridge) regression (KR) by obtaining
conditions on the ridge parameter and kernel eigenspectrum under which KR
exhibits each of the three behaviors. We find that kernels with powerlaw
spectra, including Laplace kernels and ReLU neural tangent kernels, exhibit
tempered overfitting. We then empirically study deep neural networks through
the lens of our taxonomy, and find that those trained to interpolation are
tempered, while those stopped early are benign. We hope our work leads to a
more refined understanding of overfitting in modern learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Virtual stain transfer in histology via cascaded deep neural networks. (arXiv:2207.06578v1 [physics.med-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06578">
<div class="article-summary-box-inner">
<span><p>Pathological diagnosis relies on the visual inspection of histologically
stained thin tissue specimens, where different types of stains are applied to
bring contrast to and highlight various desired histological features. However,
the destructive histochemical staining procedures are usually irreversible,
making it very difficult to obtain multiple stains on the same tissue section.
Here, we demonstrate a virtual stain transfer framework via a cascaded deep
neural network (C-DNN) to digitally transform hematoxylin and eosin (H&amp;E)
stained tissue images into other types of histological stains. Unlike a single
neural network structure which only takes one stain type as input to digitally
output images of another stain type, C-DNN first uses virtual staining to
transform autofluorescence microscopy images into H&amp;E and then performs stain
transfer from H&amp;E to the domain of the other stain in a cascaded manner. This
cascaded structure in the training phase allows the model to directly exploit
histochemically stained image data on both H&amp;E and the target special stain of
interest. This advantage alleviates the challenge of paired data acquisition
and improves the image quality and color accuracy of the virtual stain transfer
from H&amp;E to another stain. We validated the superior performance of this C-DNN
approach using kidney needle core biopsy tissue sections and successfully
transferred the H&amp;E-stained tissue images into virtual PAS (periodic
acid-Schiff) stain. This method provides high-quality virtual images of special
stains using existing, histochemically stained slides and creates new
opportunities in digital pathology by performing highly accurate stain-to-stain
transformations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Temporal Action Detection with Global Segmentation Mask Learning. (arXiv:2207.06580v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06580">
<div class="article-summary-box-inner">
<span><p>Existing temporal action detection (TAD) methods rely on generating an
overwhelmingly large number of proposals per video. This leads to complex model
designs due to proposal generation and/or per-proposal action instance
evaluation and the resultant high computational cost. In this work, for the
first time, we propose a proposal-free Temporal Action detection model with
Global Segmentation mask (TAGS). Our core idea is to learn a global
segmentation mask of each action instance jointly at the full video length. The
TAGS model differs significantly from the conventional proposal-based methods
by focusing on global temporal representation learning to directly detect local
start and end points of action instances without proposals. Further, by
modeling TAD holistically rather than locally at the individual proposal level,
TAGS needs a much simpler model architecture with lower computational cost.
Extensive experiments show that despite its simpler design, TAGS outperforms
existing TAD methods, achieving new state-of-the-art performance on two
benchmarks. Importantly, it is ~ 20x faster to train and ~1.6x more efficient
for inference. Our PyTorch implementation of TAGS is available at
https://github.com/sauradip/TAGS .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer-based Context Condensation for Boosting Feature Pyramids in Object Detection. (arXiv:2207.06603v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06603">
<div class="article-summary-box-inner">
<span><p>Current object detectors typically have a feature pyramid (FP) module for
multi-level feature fusion (MFF) which aims to mitigate the gap between
features from different levels and form a comprehensive object representation
to achieve better detection performance. However, they usually require heavy
cross-level connections or iterative refinement to obtain better MFF result,
making them complicated in structure and inefficient in computation. To address
these issues, we propose a novel and efficient context modeling mechanism that
can help existing FPs deliver better MFF results while reducing the
computational costs effectively. In particular, we introduce a novel insight
that comprehensive contexts can be decomposed and condensed into two types of
representations for higher efficiency. The two representations include a
locally concentrated representation and a globally summarized representation,
where the former focuses on extracting context cues from nearby areas while the
latter extracts key representations of the whole image scene as global context
cues. By collecting the condensed contexts, we employ a Transformer decoder to
investigate the relations between them and each local feature from the FP and
then refine the MFF results accordingly. As a result, we obtain a simple and
light-weight Transformer-based Context Condensation (TCC) module, which can
boost various FPs and lower their computational costs simultaneously. Extensive
experimental results on the challenging MS COCO dataset show that TCC is
compatible to four representative FPs and consistently improves their detection
accuracy by up to 7.8 % in terms of average precision and reduce their
complexities by up to around 20% in terms of GFLOPs, helping them achieve
state-of-the-art performance more efficiently. Code will be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Super-Resolution as Text-Guided Details Generation. (arXiv:2207.06604v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06604">
<div class="article-summary-box-inner">
<span><p>Deep neural networks have greatly promoted the performance of single image
super-resolution (SISR). Conventional methods still resort to restoring the
single high-resolution (HR) solution only based on the input of image modality.
However, the image-level information is insufficient to predict adequate
details and photo-realistic visual quality facing large upscaling factors (x8,
x16). In this paper, we propose a new perspective that regards the SISR as a
semantic image detail enhancement problem to generate semantically reasonable
HR image that are faithful to the ground truth. To enhance the semantic
accuracy and the visual quality of the reconstructed image, we explore the
multi-modal fusion learning in SISR by proposing a Text-Guided Super-Resolution
(TGSR) framework, which can effectively utilize the information from the text
and image modalities. Different from existing methods, the proposed TGSR could
generate HR image details that match the text descriptions through a
coarse-to-fine process. Extensive experiments and ablation studies demonstrate
the effect of the TGSR, which exploits the text reference to recover realistic
images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deepfake Video Detection with Spatiotemporal Dropout Transformer. (arXiv:2207.06612v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06612">
<div class="article-summary-box-inner">
<span><p>While the abuse of deepfake technology has caused serious concerns recently,
how to detect deepfake videos is still a challenge due to the high
photo-realistic synthesis of each frame. Existing image-level approaches often
focus on single frame and ignore the spatiotemporal cues hidden in deepfake
videos, resulting in poor generalization and robustness. The key of a
video-level detector is to fully exploit the spatiotemporal inconsistency
distributed in local facial regions across different frames in deepfake videos.
Inspired by that, this paper proposes a simple yet effective patch-level
approach to facilitate deepfake video detection via spatiotemporal dropout
transformer. The approach reorganizes each input video into bag of patches that
is then fed into a vision transformer to achieve robust representation.
Specifically, a spatiotemporal dropout operation is proposed to fully explore
patch-level spatiotemporal cues and serve as effective data augmentation to
further enhance model's robustness and generalization ability. The operation is
flexible and can be easily plugged into existing vision transformers. Extensive
experiments demonstrate the effectiveness of our approach against 25
state-of-the-arts with impressive robustness, generalizability, and
representation ability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">T-RECX: Tiny-Resource Efficient Convolutional Neural Networks with Early-Exit. (arXiv:2207.06613v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06613">
<div class="article-summary-box-inner">
<span><p>Deploying Machine learning (ML) on the milliwatt-scale edge devices (tinyML)
is gaining popularity due to recent breakthroughs in ML and IoT. However, the
capabilities of tinyML are restricted by strict power and compute constraints.
The majority of the contemporary research in tinyML focuses on model
compression techniques such as model pruning and quantization to fit ML models
on low-end devices. Nevertheless, the improvements in energy consumption and
inference time obtained by existing techniques are limited because aggressive
compression quickly shrinks model capacity and accuracy. Another approach to
improve inference time and/or reduce power while preserving its model capacity
is through early-exit networks. These networks place intermediate classifiers
along a baseline neural network that facilitate early exit from neural network
computation if an intermediate classifier exhibits sufficient confidence in its
prediction. Previous work on early-exit networks have focused on large
networks, beyond what would typically be used for tinyML applications. In this
paper, we discuss the challenges of adding early-exits to state-of-the-art
tiny-CNNs and devise an early-exit architecture, T-RECX, that addresses these
challenges. In addition, we develop a method to alleviate the effect of network
overthinking at the final exit by leveraging the high-level representations
learned by the early-exit. We evaluate T-RECX on three CNNs from the MLPerf
tiny benchmark suite for image classification, keyword spotting and visual wake
word detection tasks. Our results demonstrate that T-RECX improves the accuracy
of baseline network and significantly reduces the average inference time of
tiny-CNNs. T-RECX achieves 32.58% average reduction in FLOPS in exchange for 1%
accuracy across all evaluated models. Also, our techniques increase the
accuracy of baseline network in two out of three models we evaluate
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perception-Oriented Stereo Image Super-Resolution. (arXiv:2207.06617v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06617">
<div class="article-summary-box-inner">
<span><p>Recent studies of deep learning based stereo image super-resolution
(StereoSR) have promoted the development of StereoSR. However, existing
StereoSR models mainly concentrate on improving quantitative evaluation metrics
and neglect the visual quality of super-resolved stereo images. To improve the
perceptual performance, this paper proposes the first perception-oriented
stereo image super-resolution approach by exploiting the feedback, provided by
the evaluation on the perceptual quality of StereoSR results. To provide
accurate guidance for the StereoSR model, we develop the first special stereo
image super-resolution quality assessment (StereoSRQA) model, and further
construct a StereoSRQA database. Extensive experiments demonstrate that our
StereoSR approach significantly improves the perceptual quality and enhances
the reliability of stereo images for disparity estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continuous Facial Motion Deblurring. (arXiv:2207.06626v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06626">
<div class="article-summary-box-inner">
<span><p>We introduce a novel framework for continuous facial motion deblurring that
restores the continuous sharp moment latent in a single motion-blurred face
image via a moment control factor. Although a motion-blurred image is the
accumulated signal of continuous sharp moments during the exposure time, most
existing single image deblurring approaches aim to restore a fixed number of
frames using multiple networks and training stages. To address this problem, we
propose a continuous facial motion deblurring network based on GAN (CFMD-GAN),
which is a novel framework for restoring the continuous moment latent in a
single motion-blurred face image with a single network and a single training
stage. To stabilize the network training, we train the generator to restore
continuous moments in the order determined by our facial motion-based
reordering process (FMR) utilizing domain-specific knowledge of the face.
Moreover, we propose an auxiliary regressor that helps our generator produce
more accurate images by estimating continuous sharp moments. Furthermore, we
introduce a control-adaptive (ContAda) block that performs spatially deformable
convolution and channel-wise attention as a function of the control factor.
Extensive experiments on the 300VW datasets demonstrate that the proposed
framework generates a various number of continuous output frames by varying the
moment control factor. Compared with the recent single-to-single image
deblurring networks trained with the same 300VW training set, the proposed
method show the superior performance in restoring the central sharp frame in
terms of perceptual metrics, including LPIPS, FID and Arcface identity
distance. The proposed method outperforms the existing single-to-video
deblurring method for both qualitative and quantitative comparisons.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EGSDE: Unpaired Image-to-Image Translation via Energy-Guided Stochastic Differential Equations. (arXiv:2207.06635v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06635">
<div class="article-summary-box-inner">
<span><p>Score-based diffusion generative models (SDGMs) have achieved the SOTA FID
results in unpaired image-to-image translation (I2I). However, we notice that
existing methods totally ignore the training data in the source domain, leading
to sub-optimal solutions for unpaired I2I. To this end, we propose
energy-guided stochastic differential equations (EGSDE) that employs an energy
function pretrained on both the source and target domains to guide the
inference process of a pretrained SDE for realistic and faithful unpaired I2I.
Building upon two feature extractors, we carefully design the energy function
such that it encourages the transferred image to preserve the
domain-independent features and discard domainspecific ones. Further, we
provide an alternative explanation of the EGSDE as a product of experts, where
each of the three experts (corresponding to the SDE and two feature extractors)
solely contributes to faithfulness or realism. Empirically, we compare EGSDE to
a large family of baselines on three widely-adopted unpaired I2I tasks under
four metrics. EGSDE not only consistently outperforms existing SDGMs-based
methods in almost all settings but also achieves the SOTA realism results
(e.g., FID of 65.82 in Cat to Dog and FID of 59.75 in Wild to Dog on AFHQ)
without harming the faithful performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Source-Free Domain Adaptation for Real-world Image Dehazing. (arXiv:2207.06644v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06644">
<div class="article-summary-box-inner">
<span><p>Deep learning-based source dehazing methods trained on synthetic datasets
have achieved remarkable performance but suffer from dramatic performance
degradation on real hazy images due to domain shift. Although certain Domain
Adaptation (DA) dehazing methods have been presented, they inevitably require
access to the source dataset to reduce the gap between the source synthetic and
target real domains. To address these issues, we present a novel Source-Free
Unsupervised Domain Adaptation (SFUDA) image dehazing paradigm, in which only a
well-trained source model and an unlabeled target real hazy dataset are
available. Specifically, we devise the Domain Representation Normalization
(DRN) module to make the representation of real hazy domain features match that
of the synthetic domain to bridge the gaps. With our plug-and-play DRN module,
unlabeled real hazy images can adapt existing well-trained source networks.
Besides, the unsupervised losses are applied to guide the learning of the DRN
module, which consists of frequency losses and physical prior losses. Frequency
losses provide structure and style constraints, while the prior loss explores
the inherent statistic property of haze-free images. Equipped with our DRN
module and unsupervised loss, existing source dehazing models are able to
dehaze unlabeled real hazy images. Extensive experiments on multiple baselines
demonstrate the validity and superiority of our method visually and
quantitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prototypical Contrast Adaptation for Domain Adaptive Semantic Segmentation. (arXiv:2207.06654v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06654">
<div class="article-summary-box-inner">
<span><p>Unsupervised Domain Adaptation (UDA) aims to adapt the model trained on the
labeled source domain to an unlabeled target domain. In this paper, we present
Prototypical Contrast Adaptation (ProCA), a simple and efficient contrastive
learning method for unsupervised domain adaptive semantic segmentation.
Previous domain adaptation methods merely consider the alignment of the
intra-class representational distributions across various domains, while the
inter-class structural relationship is insufficiently explored, resulting in
the aligned representations on the target domain might not be as easily
discriminated as done on the source domain anymore. Instead, ProCA incorporates
inter-class information into class-wise prototypes, and adopts the
class-centered distribution alignment for adaptation. By considering the same
class prototypes as positives and other class prototypes as negatives to
achieve class-centered distribution alignment, ProCA achieves state-of-the-art
performance on classical domain adaptation tasks, {\em i.e., GTA5 $\to$
Cityscapes \text{and} SYNTHIA $\to$ Cityscapes}. Code is available at
\href{https://github.com/jiangzhengkai/ProCA}{ProCA}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploration of an End-to-End Automatic Number-plate Recognition neural network for Indian datasets. (arXiv:2207.06657v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06657">
<div class="article-summary-box-inner">
<span><p>Indian vehicle number plates have wide variety in terms of size, font, script
and shape. Development of Automatic Number Plate Recognition (ANPR) solutions
is therefore challenging, necessitating a diverse dataset to serve as a
collection of examples. However, a comprehensive dataset of Indian scenario is
missing, thereby, hampering the progress towards publicly available and
reproducible ANPR solutions. Many countries have invested efforts to develop
comprehensive ANPR datasets like Chinese City Parking Dataset (CCPD) for China
and Application-oriented License Plate (AOLP) dataset for US. In this work, we
release an expanding dataset presently consisting of 1.5k images and a scalable
and reproducible procedure of enhancing this dataset towards development of
ANPR solution for Indian conditions. We have leveraged this dataset to explore
an End-to-End (E2E) ANPR architecture for Indian scenario which was originally
proposed for Chinese Vehicle number-plate recognition based on the CCPD
dataset. As we customized the architecture for our dataset, we came across
insights, which we have discussed in this paper. We report the hindrances in
direct reusability of the model provided by the authors of CCPD because of the
extreme diversity in Indian number plates and differences in distribution with
respect to the CCPD dataset. An improvement of 42.86% was observed in LP
detection after aligning the characteristics of Indian dataset with Chinese
dataset. In this work, we have also compared the performance of the E2E
number-plate detection model with YOLOv5 model, pre-trained on COCO dataset and
fine-tuned on Indian vehicle images. Given that the number Indian vehicle
images used for fine-tuning the detection module and yolov5 were same, we
concluded that it is more sample efficient to develop an ANPR solution for
Indian conditions based on COCO dataset rather than CCPD dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Universal Adaptive Data Augmentation. (arXiv:2207.06658v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06658">
<div class="article-summary-box-inner">
<span><p>Existing automatic data augmentation (DA) methods either ignore updating DA's
parameters according to the target model's state during training or adopt
update strategies that are not effective enough. In this work, we design a
novel data augmentation strategy called "Universal Adaptive Data Augmentation"
(UADA). Different from existing methods, UADA would adaptively update DA's
parameters according to the target model's gradient information during
training: given a pre-defined set of DA operations, we randomly decide types
and magnitudes of DA operations for every data batch during training, and
adaptively update DA's parameters along the gradient direction of the loss
concerning DA's parameters. In this way, UADA can increase the training loss of
the target networks, and the target networks would learn features from harder
samples to improve the generalization. Moreover, UADA is very general and can
be utilized in numerous tasks, e.g., image classification, semantic
segmentation and object detection. Extensive experiments with various models
are conducted on CIFAR-10, CIFAR-100, ImageNet, tiny-ImageNet, Cityscapes, and
VOC07+12 to prove the significant performance improvements brought by our
proposed adaptive augmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Forcing the Whole Video as Background: An Adversarial Learning Strategy for Weakly Temporal Action Localization. (arXiv:2207.06659v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06659">
<div class="article-summary-box-inner">
<span><p>With video-level labels, weakly supervised temporal action localization
(WTAL) applies a localization-by-classification paradigm to detect and classify
the action in untrimmed videos. Due to the characteristic of classification,
class-specific background snippets are inevitably mis-activated to improve the
discriminability of the classifier in WTAL. To alleviate the disturbance of
background, existing methods try to enlarge the discrepancy between action and
background through modeling background snippets with pseudo-snippet-level
annotations, which largely rely on artificial hypotheticals. Distinct from the
previous works, we present an adversarial learning strategy to break the
limitation of mining pseudo background snippets. Concretely, the background
classification loss forces the whole video to be regarded as the background by
a background gradient reinforcement strategy, confusing the recognition model.
Reversely, the foreground(action) loss guides the model to focus on action
snippets under such conditions. As a result, competition between the two
classification losses drives the model to boost its ability for action
modeling. Simultaneously, a novel temporal enhancement network is designed to
facilitate the model to construct temporal relation of affinity snippets based
on the proposed strategy, for further improving the performance of action
localization. Finally, extensive experiments conducted on THUMOS14 and
ActivityNet1.2 demonstrate the effectiveness of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Point-to-Plane Registration by Efficient Backpropagation for Error Minimizing Function. (arXiv:2207.06661v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06661">
<div class="article-summary-box-inner">
<span><p>Traditional algorithms of point set registration minimizing point-to-plane
distances often achieve a better estimation of rigid transformation than those
minimizing point-to-point distances. Nevertheless, recent deep-learning-based
methods minimize the point-to-point distances. In contrast to these methods,
this paper proposes the first deep-learning-based approach to point-to-plane
registration. A challenging part of this problem is that a typical solution for
point-to-plane registration requires an iterative process of accumulating small
transformations obtained by minimizing a linearized energy function. The
iteration significantly increases the size of the computation graph needed for
backpropagation and can slow down both forward and backward network
evaluations. To solve this problem, we consider the estimated rigid
transformation as a function of input point clouds and derive its analytic
gradients using the implicit function theorem. The analytic gradient that we
introduce is independent of how the error minimizing function (i.e., the rigid
transformation) is obtained, thus allowing us to calculate both the rigid
transformation and its gradient efficiently. We implement the proposed
point-to-plane registration module over several previous methods that minimize
point-to-point distances and demonstrate that the extensions outperform the
base methods even with point clouds with noise and low-quality point normals
estimated with local point distributions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Volunteer Cotton Plants in a Corn Field with Deep Learning on UAV Remote-Sensing Imagery. (arXiv:2207.06673v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06673">
<div class="article-summary-box-inner">
<span><p>The cotton boll weevil, Anthonomus grandis Boheman is a serious pest to the
U.S. cotton industry that has cost more than 16 billion USD in damages since it
entered the United States from Mexico in the late 1800s. This pest has been
nearly eradicated; however, southern part of Texas still faces this issue and
is always prone to the pest reinfestation each year due to its sub-tropical
climate where cotton plants can grow year-round. Volunteer cotton (VC) plants
growing in the fields of inter-seasonal crops, like corn, can serve as hosts to
these pests once they reach pin-head square stage (5-6 leaf stage) and
therefore need to be detected, located, and destroyed or sprayed . In this
paper, we present a study to detect VC plants in a corn field using YOLOv3 on
three band aerial images collected by unmanned aircraft system (UAS). The
two-fold objectives of this paper were : (i) to determine whether YOLOv3 can be
used for VC detection in a corn field using RGB (red, green, and blue) aerial
images collected by UAS and (ii) to investigate the behavior of YOLOv3 on
images at three different scales (320 x 320, S1; 416 x 416, S2; and 512 x 512,
S3 pixels) based on average precision (AP), mean average precision (mAP) and
F1-score at 95% confidence level. No significant differences existed for mAP
among the three scales, while a significant difference was found for AP between
S1 and S3 (p = 0.04) and S2 and S3 (p = 0.02). A significant difference was
also found for F1-score between S2 and S3 (p = 0.02). The lack of significant
differences of mAP at all the three scales indicated that the trained YOLOv3
model can be used on a computer vision-based remotely piloted aerial
application system (RPAAS) for VC detection and spray application in near
real-time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Subgraph Frequency Distribution Estimation using Graph Neural Networks. (arXiv:2207.06684v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06684">
<div class="article-summary-box-inner">
<span><p>Small subgraphs (graphlets) are important features to describe fundamental
units of a large network. The calculation of the subgraph frequency
distributions has a wide application in multiple domains including biology and
engineering. Unfortunately due to the inherent complexity of this task, most of
the existing methods are computationally intensive and inefficient. In this
work, we propose GNNS, a novel representational learning framework that
utilizes graph neural networks to sample subgraphs efficiently for estimating
their frequency distribution. Our framework includes an inference model and a
generative model that learns hierarchical embeddings of nodes, subgraphs, and
graph types. With the learned model and embeddings, subgraphs are sampled in a
highly scalable and parallel way and the frequency distribution estimation is
then performed based on these sampled subgraphs. Eventually, our methods
achieve comparable accuracy and a significant speedup by three orders of
magnitude compared to existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Low-Resolution Distillation for Cost-Efficient End-to-End Text Spotting. (arXiv:2207.06694v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06694">
<div class="article-summary-box-inner">
<span><p>End-to-end text spotting has attached great attention recently due to its
benefits on global optimization and high maintainability for real applications.
However, the input scale has always been a tough trade-off since recognizing a
small text instance usually requires enlarging the whole image, which brings
high computational costs. In this paper, to address this problem, we propose a
novel cost-efficient Dynamic Low-resolution Distillation (DLD) text spotting
framework, which aims to infer images in different small but recognizable
resolutions and achieve a better balance between accuracy and efficiency.
Concretely, we adopt a resolution selector to dynamically decide the input
resolutions for different images, which is constraint by both inference
accuracy and computational cost. Another sequential knowledge distillation
strategy is conducted on the text recognition branch, making the low-res input
obtains comparable performance to a high-res image. The proposed method can be
optimized end-to-end and adopted in any current text spotting framework to
improve the practicability. Extensive experiments on several text spotting
benchmarks show that the proposed method vastly improves the usability of
low-res models. The code is available at
https://github.com/hikopensource/DAVAR-Lab-OCR/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DavarOCR: A Toolbox for OCR and Multi-Modal Document Understanding. (arXiv:2207.06695v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06695">
<div class="article-summary-box-inner">
<span><p>This paper presents DavarOCR, an open-source toolbox for OCR and document
understanding tasks. DavarOCR currently implements 19 advanced algorithms,
covering 9 different task forms. DavarOCR provides detailed usage instructions
and the trained models for each algorithm. Compared with the previous
opensource OCR toolbox, DavarOCR has relatively more complete support for the
sub-tasks of the cutting-edge technology of document understanding. In order to
promote the development and application of OCR technology in academia and
industry, we pay more attention to the use of modules that different
sub-domains of technology can share. DavarOCR is publicly released at
https://github.com/hikopensource/Davar-Lab-OCR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SHREC 2022 Track on Online Detection of Heterogeneous Gestures. (arXiv:2207.06706v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06706">
<div class="article-summary-box-inner">
<span><p>This paper presents the outcomes of a contest organized to evaluate methods
for the online recognition of heterogeneous gestures from sequences of 3D hand
poses. The task is the detection of gestures belonging to a dictionary of 16
classes characterized by different pose and motion features. The dataset
features continuous sequences of hand tracking data where the gestures are
interleaved with non-significant motions. The data have been captured using the
Hololens 2 finger tracking system in a realistic use-case of mixed reality
interaction. The evaluation is based not only on the detection performances but
also on the latency and the false positives, making it possible to understand
the feasibility of practical interaction tools based on the algorithms
proposed. The outcomes of the contest's evaluation demonstrate the necessity of
further research to reduce recognition errors, while the computational cost of
the algorithms proposed is sufficiently low.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Octuplet Loss: Make Face Recognition Robust to Image Resolution. (arXiv:2207.06726v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06726">
<div class="article-summary-box-inner">
<span><p>Image resolution, or in general, image quality, plays an essential role in
the performance of today's face recognition systems. To address this problem,
we propose a novel combination of the popular triplet loss to improve
robustness against image resolution via fine-tuning of existing face
recognition models. With octuplet loss, we leverage the relationship between
high-resolution images and their synthetically down-sampled variants jointly
with their identity labels. Fine-tuning several state-of-the-art approaches
with our method proves that we can significantly boost performance for
cross-resolution (high-to-low resolution) face verification on various datasets
without meaningfully exacerbating the performance on high-to-high resolution
images. Our method applied on the FaceTransformer network achieves 95.12% face
verification accuracy on the challenging XQLFW dataset while reaching 99.73% on
the LFW database. Moreover, the low-to-low face verification accuracy benefits
from our method. We release our code to allow seamless integration of the
octuplet loss into existing frameworks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ConCL: Concept Contrastive Learning for Dense Prediction Pre-training in Pathology Images. (arXiv:2207.06733v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06733">
<div class="article-summary-box-inner">
<span><p>Detectingandsegmentingobjectswithinwholeslideimagesis essential in
computational pathology workflow. Self-supervised learning (SSL) is appealing
to such annotation-heavy tasks. Despite the extensive benchmarks in natural
images for dense tasks, such studies are, unfortunately, absent in current
works for pathology. Our paper intends to narrow this gap. We first benchmark
representative SSL methods for dense prediction tasks in pathology images.
Then, we propose concept contrastive learning (ConCL), an SSL framework for
dense pre-training. We explore how ConCL performs with concepts provided by
different sources and end up with proposing a simple dependency-free concept
generating method that does not rely on external segmentation algorithms or
saliency detection models. Extensive experiments demonstrate the superiority of
ConCL over previous state-of-the-art SSL methods across different settings.
Along our exploration, we distll several important and intriguing components
contributing to the success of dense pre-training for pathology images. We hope
this work could provide useful data points and encourage the community to
conduct ConCL pre-training for problems of interest. Code is available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-supervised Vector-Quantization in Visual SLAM using HGCN. (arXiv:2207.06738v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06738">
<div class="article-summary-box-inner">
<span><p>In this paper, two semi-supervised appearance based loop closure detection
technique, HGCN-FABMAP and HGCN-BoW are introduced. Furthermore an extension to
the current state of the art localization SLAM algorithm, ORB-SLAM, is
presented. The proposed HGCN-FABMAP method is implemented in an off-line manner
incorporating Bayesian probabilistic schema for loop detection decision making.
Specifically, we let a Hyperbolic Graph Convolutional Neural Network (HGCN) to
operate over the SURF features graph space, and perform vector quantization
part of the SLAM procedure. This part previously was performed in an
unsupervised manner using algorithms like HKmeans, kmeans++,..etc. The main
Advantage of using HGCN, is that it scales linearly in number of graph edges.
Experimental results shows that HGCN-FABMAP algorithm needs far more cluster
centroids than HGCN-ORB, otherwise it fails to detect loop closures. Therefore
we consider HGCN-ORB to be more efficient in terms of memory consumption, also
we conclude the superiority of HGCN-BoW and HGCN-FABMAP with respect to other
algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TRIE++: Towards End-to-End Information Extraction from Visually Rich Documents. (arXiv:2207.06744v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06744">
<div class="article-summary-box-inner">
<span><p>Recently, automatically extracting information from visually rich documents
(e.g., tickets and resumes) has become a hot and vital research topic due to
its widespread commercial value. Most existing methods divide this task into
two subparts: the text reading part for obtaining the plain text from the
original document images and the information extraction part for extracting key
contents. These methods mainly focus on improving the second, while neglecting
that the two parts are highly correlated. This paper proposes a unified
end-to-end information extraction framework from visually rich documents, where
text reading and information extraction can reinforce each other via a
well-designed multi-modal context block. Specifically, the text reading part
provides multi-modal features like visual, textual and layout features. The
multi-modal context block is developed to fuse the generated multi-modal
features and even the prior knowledge from the pre-trained language model for
better semantic representation. The information extraction part is responsible
for generating key contents with the fused context features. The framework can
be trained in an end-to-end trainable manner, achieving global optimization.
What is more, we define and group visually rich documents into four categories
across two dimensions, the layout and text type. For each document category, we
provide or recommend the corresponding benchmarks, experimental settings and
strong baselines for remedying the problem that this research area lacks the
uniform evaluation standard. Extensive experiments on four kinds of benchmarks
(from fixed layout to variable layout, from full-structured text to
semi-unstructured text) are reported, demonstrating the proposed method's
effectiveness. Data, source code and models are available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Single-Pixel Image Reconstruction Based on Block Compressive Sensing and Deep Learning. (arXiv:2207.06746v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06746">
<div class="article-summary-box-inner">
<span><p>Single-pixel imaging (SPI) is a novel imaging technique whose working
principle is based on the compressive sensing (CS) theory. In SPI, data is
obtained through a series of compressive measurements and the corresponding
image is reconstructed. Typically, the reconstruction algorithm such as basis
pursuit relies on the sparsity assumption in images. However, recent advances
in deep learning have found its uses in reconstructing CS images. Despite
showing a promising result in simulations, it is often unclear how such an
algorithm can be implemented in an actual SPI setup. In this paper, we
demonstrate the use of deep learning on the reconstruction of SPI images in
conjunction with block compressive sensing (BCS). We also proposed a novel
reconstruction model based on convolutional neural networks that outperforms
other competitive CS reconstruction algorithms. Besides, by incorporating BCS
in our deep learning model, we were able to reconstruct images of any size
above a certain smallest image size. In addition, we show that our model is
capable of reconstructing images obtained from an SPI setup while being priorly
trained on natural images, which can be vastly different from the SPI images.
This opens up opportunity for the feasibility of pretrained deep learning
models for CS reconstructions of images from various domain areas.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">E2-AEN: End-to-End Incremental Learning with Adaptively Expandable Network. (arXiv:2207.06754v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06754">
<div class="article-summary-box-inner">
<span><p>Expandable networks have demonstrated their advantages in dealing with
catastrophic forgetting problem in incremental learning. Considering that
different tasks may need different structures, recent methods design dynamic
structures adapted to different tasks via sophisticated skills. Their routine
is to search expandable structures first and then train on the new tasks,
which, however, breaks tasks into multiple training stages, leading to
suboptimal or overmuch computational cost. In this paper, we propose an
end-to-end trainable adaptively expandable network named E2-AEN, which
dynamically generates lightweight structures for new tasks without any accuracy
drop in previous tasks. Specifically, the network contains a serial of powerful
feature adapters for augmenting the previously learned representations to new
tasks, and avoiding task interference. These adapters are controlled via an
adaptive gate-based pruning strategy which decides whether the expanded
structures can be pruned, making the network structure dynamically changeable
according to the complexity of the new tasks. Moreover, we introduce a novel
sparsity-activation regularization to encourage the model to learn
discriminative features with limited parameters. E2-AEN reduces cost and can be
built upon any feed-forward architectures in an end-to-end manner. Extensive
experiments on both classification (i.e., CIFAR and VDD) and detection (i.e.,
COCO, VOC and ICCV2021 SSLAD challenge) benchmarks demonstrate the
effectiveness of the proposed method, which achieves the new remarkable
results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neighbor Correspondence Matching for Flow-based Video Frame Synthesis. (arXiv:2207.06763v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06763">
<div class="article-summary-box-inner">
<span><p>Video frame synthesis, which consists of interpolation and extrapolation, is
an essential video processing technique that can be applied to various
scenarios. However, most existing methods cannot handle small objects or large
motion well, especially in high-resolution videos such as 4K videos. To
eliminate such limitations, we introduce a neighbor correspondence matching
(NCM) algorithm for flow-based frame synthesis. Since the current frame is not
available in video frame synthesis, NCM is performed in a
current-frame-agnostic fashion to establish multi-scale correspondences in the
spatial-temporal neighborhoods of each pixel. Based on the powerful motion
representation capability of NCM, we further propose to estimate intermediate
flows for frame synthesis in a heterogeneous coarse-to-fine scheme.
Specifically, the coarse-scale module is designed to leverage neighbor
correspondences to capture large motion, while the fine-scale module is more
computationally efficient to speed up the estimation process. Both modules are
trained progressively to eliminate the resolution gap between training dataset
and real-world videos. Experimental results show that NCM achieves
state-of-the-art performance on several benchmarks. In addition, NCM can be
applied to various practical scenarios such as video compression to achieve
better performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GeoSegNet: Point Cloud Semantic Segmentation via Geometric Encoder-Decoder Modeling. (arXiv:2207.06766v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06766">
<div class="article-summary-box-inner">
<span><p>Semantic segmentation of point clouds, aiming to assign each point a semantic
category, is critical to 3D scene understanding.Despite of significant advances
in recent years, most of existing methods still suffer from either the
object-level misclassification or the boundary-level ambiguity. In this paper,
we present a robust semantic segmentation network by deeply exploring the
geometry of point clouds, dubbed GeoSegNet. Our GeoSegNet consists of a
multi-geometry based encoder and a boundary-guided decoder. In the encoder, we
develop a new residual geometry module from multi-geometry perspectives to
extract object-level features. In the decoder, we introduce a contrastive
boundary learning module to enhance the geometric representation of boundary
points. Benefiting from the geometric encoder-decoder modeling, our GeoSegNet
can infer the segmentation of objects effectively while making the
intersections (boundaries) of two or more objects clear. Experiments show
obvious improvements of our method over its competitors in terms of the overall
segmentation accuracy and object boundary clearness. Code is available at
https://github.com/Chen-yuiyui/GeoSegNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Evaluation of Four Off-the-Shelf Proprietary Visual-Inertial Odometry Systems. (arXiv:2207.06780v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06780">
<div class="article-summary-box-inner">
<span><p>Commercial visual-inertial odometry (VIO) systems have been gaining attention
as cost-effective, off-the-shelf six degrees of freedom (6-DoF) ego-motion
tracking methods for estimating accurate and consistent camera pose data, in
addition to their ability to operate without external localization from motion
capture or global positioning systems. It is unclear from existing results,
however, which commercial VIO platforms are the most stable, consistent, and
accurate in terms of state estimation for indoor and outdoor robotic
applications. We assess four popular proprietary VIO systems (Apple ARKit,
Google ARCore, Intel RealSense T265, and Stereolabs ZED 2) through a series of
both indoor and outdoor experiments where we show their positioning stability,
consistency, and accuracy. We present our complete results as a benchmark
comparison for the research community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inertial Hallucinations -- When Wearable Inertial Devices Start Seeing Things. (arXiv:2207.06789v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06789">
<div class="article-summary-box-inner">
<span><p>We propose a novel approach to multimodal sensor fusion for Ambient Assisted
Living (AAL) which takes advantage of learning using privileged information
(LUPI). We address two major shortcomings of standard multimodal approaches,
limited area coverage and reduced reliability. Our new framework fuses the
concept of modality hallucination with triplet learning to train a model with
different modalities to handle missing sensors at inference time. We evaluate
the proposed model on inertial data from a wearable accelerometer device, using
RGB videos and skeletons as privileged modalities, and show an improvement of
accuracy of an average 6.6% on the UTD-MHAD dataset and an average 5.5% on the
Berkeley MHAD dataset, reaching a new state-of-the-art for inertial-only
classification accuracy on these datasets. We validate our framework through
several ablation studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural apparent BRDF fields for multiview photometric stereo. (arXiv:2207.06793v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06793">
<div class="article-summary-box-inner">
<span><p>We propose to tackle the multiview photometric stereo problem using an
extension of Neural Radiance Fields (NeRFs), conditioned on light source
direction. The geometric part of our neural representation predicts surface
normal direction, allowing us to reason about local surface reflectance. The
appearance part of our neural representation is decomposed into a neural
bidirectional reflectance function (BRDF), learnt as part of the fitting
process, and a shadow prediction network (conditioned on light source
direction) allowing us to model the apparent BRDF. This balance of learnt
components with inductive biases based on physical image formation models
allows us to extrapolate far from the light source and viewer directions
observed during training. We demonstrate our approach on a multiview
photometric stereo benchmark and show that competitive performance can be
obtained with the neural density representation of a NeRF.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multi-Modality Ovarian Tumor Ultrasound Image Dataset for Unsupervised Cross-Domain Semantic Segmentation. (arXiv:2207.06799v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06799">
<div class="article-summary-box-inner">
<span><p>Ovarian cancer is one of the most harmful gynecological diseases. Detecting
ovarian tumors in early stage with computer-aided techniques can efficiently
decrease the mortality rate. With the improvement of medical treatment
standard, ultrasound images are widely applied in clinical treatment. However,
recent notable methods mainly focus on single-modality ultrasound ovarian tumor
segmentation or recognition, which means there still lacks of researches on
exploring the representation capability of multi-modality ultrasound ovarian
tumor images. To solve this problem, we propose a Multi-Modality Ovarian Tumor
Ultrasound (MMOTU) image dataset containing 1469 2d ultrasound images and 170
contrast enhanced ultrasonography (CEUS) images with pixel-wise and global-wise
annotations. Based on MMOTU, we mainly focus on unsupervised cross-domain
semantic segmentation task. To solve the domain shift problem, we propose a
feature alignment based architecture named Dual-Scheme Domain-Selected Network
(DS$^2$Net). Specifically, we first design source-encoder and target-encoder to
extract two-style features of source and target images. Then, we propose
Domain-Distinct Selected Module (DDSM) and Domain-Universal Selected Module
(DUSM) to extract the distinct and universal features in two styles
(source-style or target-style). Finally, we fuse these two kinds of features
and feed them into the source-decoder and target-decoder to generate final
predictions. Extensive comparison experiments and analysis on MMOTU image
dataset show that DS$^2$Net can boost the segmentation performance for
bidirectional cross-domain adaptation of 2d ultrasound images and CEUS images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pseudo-Labeling Based Practical Semi-Supervised Meta-Training for Few-Shot Learning. (arXiv:2207.06817v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06817">
<div class="article-summary-box-inner">
<span><p>Most existing few-shot learning (FSL) methods require a large amount of
labeled data in meta-training, which is a major limit. To reduce the
requirement of labels, a semi-supervised meta-training setting has been
proposed for FSL, which includes only a few labeled samples and numbers of
unlabeled samples in base classes. However, existing methods under this setting
require class-aware sample selection from the unlabeled set, which violates the
assumption of unlabeled set. In this paper, we propose a practical
semi-supervised meta-training setting with truly unlabeled data. Under the new
setting, the performance of existing methods drops notably. To better utilize
both the labeled and truly unlabeled data, we propose a simple and effective
meta-training framework, called pseudo-labeling based on meta-learning (PLML).
Firstly, we train a classifier via common semi-supervised learning (SSL) and
use it to obtain the pseudo-labels of unlabeled data. Then we build few-shot
tasks from labeled and pseudo-labeled data and run meta-learning over the
constructed tasks to learn the FSL model. Surprisingly, through extensive
experiments across two FSL datasets, we find that this simple meta-training
framework effectively prevents the performance degradation of FSL under limited
labeled data. Besides, benefiting from meta-training, the proposed method
improves the classifiers learned by two representative SSL algorithms as well.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DEXTER: An end-to-end system to extract table contents from electronic medical health documents. (arXiv:2207.06823v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06823">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose DEXTER, an end to end system to extract information
from tables present in medical health documents, such as electronic health
records (EHR) and explanation of benefits (EOB). DEXTER consists of four
sub-system stages: i) table detection ii) table type classification iii) cell
detection; and iv) cell content extraction. We propose a two-stage transfer
learning-based approach using CDeC-Net architecture along with Non-Maximal
suppression for table detection. We design a conventional computer vision-based
approach for table type classification and cell detection using parameterized
kernels based on image size for detecting rows and columns. Finally, we extract
the text from the detected cells using pre-existing OCR engine Tessaract. To
evaluate our system, we manually annotated a sample of the real-world medical
dataset (referred to as Meddata) consisting of wide variations of documents (in
terms of appearance) covering different table structures, such as bordered,
partially bordered, borderless, or coloured tables. We experimentally show that
DEXTER outperforms the commercially available Amazon Textract and Microsoft
Azure Form Recognizer systems on the annotated real-world medical dataset
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Refign: Align and Refine for Adaptation of Semantic Segmentation to Adverse Conditions. (arXiv:2207.06825v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06825">
<div class="article-summary-box-inner">
<span><p>Due to the scarcity of dense pixel-level semantic annotations for images
recorded in adverse visual conditions, there has been a keen interest in
unsupervised domain adaptation (UDA) for the semantic segmentation of such
images. UDA adapts models trained on normal conditions to the target
adverse-condition domains. Meanwhile, multiple datasets with driving scenes
provide corresponding images of the same scenes across multiple conditions,
which can serve as a form of weak supervision for domain adaptation. We propose
Refign, a generic extension to self-training-based UDA methods which leverages
these cross-domain correspondences. Refign consists of two steps: (1) aligning
the normal-condition image to the corresponding adverse-condition image using
an uncertainty-aware dense matching network, and (2) refining the adverse
prediction with the normal prediction using an adaptive label correction
mechanism. We design custom modules to streamline both steps and set the new
state of the art for domain-adaptive semantic segmentation on several
adverse-condition benchmarks, including ACDC and Dark Zurich. The approach
introduces no extra training parameters, minimal computational overhead --
during training only -- and can be used as a drop-in extension to improve any
given self-training-based UDA method. Code is available at
https://github.com/brdav/refign.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Point-to-Box Network for Accurate Object Detection via Single Point Supervision. (arXiv:2207.06827v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06827">
<div class="article-summary-box-inner">
<span><p>Object detection using single point supervision has received increasing
attention over the years. In this paper, we attribute such a large performance
gap to the failure of generating high-quality proposal bags which are crucial
for multiple instance learning (MIL). To address this problem, we introduce a
lightweight alternative to the off-the-shelf proposal (OTSP) method and thereby
create the Point-to-Box Network (P2BNet), which can construct an inter-objects
balanced proposal bag by generating proposals in an anchor-like way. By fully
investigating the accurate position information, P2BNet further constructs an
instance-level bag, avoiding the mixture of multiple objects. Finally, a
coarse-to-fine policy in a cascade fashion is utilized to improve the IoU
between proposals and ground-truth (GT). Benefiting from these strategies,
P2BNet is able to produce high-quality instance-level bags for object
detection. P2BNet improves the mean average precision (AP) by more than 50%
relative to the previous best PSOD method on the MS COCO dataset. It also
demonstrates the great potential to bridge the performance gap between point
supervised and bounding-box supervised detectors. The code will be released at
github.com/ucas-vg/P2BNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pose-based Tremor Classification for Parkinson's Disease Diagnosis from Video. (arXiv:2207.06828v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06828">
<div class="article-summary-box-inner">
<span><p>Parkinson's disease (PD) is a progressive neurodegenerative disorder that
results in a variety of motor dysfunction symptoms, including tremors,
bradykinesia, rigidity and postural instability. The diagnosis of PD mainly
relies on clinical experience rather than a definite medical test, and the
diagnostic accuracy is only about 73-84% since it is challenged by the
subjective opinions or experiences of different medical experts. Therefore, an
efficient and interpretable automatic PD diagnosis system is valuable for
supporting clinicians with more robust diagnostic decision-making. To this end,
we propose to classify Parkinson's tremor since it is one of the most
predominant symptoms of PD with strong generalizability. Different from other
computer-aided time and resource-consuming Parkinson's Tremor (PT)
classification systems that rely on wearable sensors, we propose SPAPNet, which
only requires consumer-grade non-intrusive video recording of camera-facing
human movements as input to provide undiagnosed patients with low-cost PT
classification results as a PD warning sign. For the first time, we propose to
use a novel attention module with a lightweight pyramidal
channel-squeezing-fusion architecture to extract relevant PT information and
filter the noise efficiently. This design aids in improving both classification
performance and system interpretability. Experimental results show that our
system outperforms state-of-the-arts by achieving a balanced accuracy of 90.9%
and an F1-score of 90.6% in classifying PT with the non-PT class.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">iColoriT: Towards Propagating Local Hint to the Right Region in Interactive Colorization by Leveraging Vision Transformer. (arXiv:2207.06831v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06831">
<div class="article-summary-box-inner">
<span><p>Point-interactive image colorization aims to colorize grayscale images when a
user provides the colors for specific locations. It is essential for
point-interactive colorization methods to appropriately propagate user-provided
colors (i.e., user hints) in the entire image to obtain a reasonably colorized
image with minimal user effort. However, existing approaches often produce
partially colorized results due to the inefficient design of stacking
convolutional layers to propagate hints to distant relevant regions. To address
this problem, we present iColoriT, a novel point-interactive colorization
Vision Transformer capable of propagating user hints to relevant regions,
leveraging the global receptive field of Transformers. The self-attention
mechanism of Transformers enables iColoriT to selectively colorize relevant
regions with only a few local hints. Our approach colorizes images in real-time
by utilizing pixel shuffling, an efficient upsampling technique that replaces
the decoder architecture. Also, in order to mitigate the artifacts caused by
pixel shuffling with large upsampling ratios, we present the local stabilizing
layer. Extensive quantitative and qualitative results demonstrate that our
approach highly outperforms existing methods for point-interactive
colorization, producing accurately colorized images with a user's minimal
effort.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enforcing connectivity of 3D linear structures using their 2D projections. (arXiv:2207.06832v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06832">
<div class="article-summary-box-inner">
<span><p>Many biological and medical tasks require the delineation of 3D curvilinear
structures such as blood vessels and neurites from image volumes. This is
typically done using neural networks trained by minimizing voxel-wise loss
functions that do not capture the topological properties of these structures.
As a result, the connectivity of the recovered structures is often wrong, which
lessens their usefulness. In this paper, we propose to improve the 3D
connectivity of our results by minimizing a sum of topology-aware losses on
their 2D projections. This suffices to increase the accuracy and to reduce the
annotation effort required to provide the required annotated training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Dictionary Learning with An Intra-class Constraint. (arXiv:2207.06841v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06841">
<div class="article-summary-box-inner">
<span><p>In recent years, deep dictionary learning (DDL)has attracted a great amount
of attention due to its effectiveness for representation learning and visual
recognition.~However, most existing methods focus on unsupervised deep
dictionary learning, failing to further explore the category information.~To
make full use of the category information of different samples, we propose a
novel deep dictionary learning model with an intra-class constraint (DDLIC) for
visual classification. Specifically, we design the intra-class compactness
constraint on the intermediate representation at different levels to encourage
the intra-class representations to be closer to each other, and eventually the
learned representation becomes more discriminative.~Unlike the traditional DDL
methods, during the classification stage, our DDLIC performs a layer-wise
greedy optimization in a similar way to the training stage. Experimental
results on four image datasets show that our method is superior to the
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AIParsing: Anchor-free Instance-level Human Parsing. (arXiv:2207.06854v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06854">
<div class="article-summary-box-inner">
<span><p>Most state-of-the-art instance-level human parsing models adopt two-stage
anchor-based detectors and, therefore, cannot avoid the heuristic anchor box
design and the lack of analysis on a pixel level. To address these two issues,
we have designed an instance-level human parsing network which is anchor-free
and solvable on a pixel level. It consists of two simple sub-networks: an
anchor-free detection head for bounding box predictions and an edge-guided
parsing head for human segmentation. The anchor-free detector head inherits the
pixel-like merits and effectively avoids the sensitivity of hyper-parameters as
proved in object detection applications. By introducing the part-aware boundary
clue, the edge-guided parsing head is capable to distinguish adjacent human
parts from among each other up to 58 parts in a single human instance, even
overlapping instances. Meanwhile, a refinement head integrating box-level score
and part-level parsing quality is exploited to improve the quality of the
parsing results. Experiments on two multiple human parsing datasets (i.e., CIHP
and LV-MHP-v2.0) and one video instance-level human parsing dataset (i.e., VIP)
show that our method achieves the best global-level and instance-level
performance over state-of-the-art one-stage top-down alternatives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Immunofluorescence Capillary Imaging Segmentation: Cases Study. (arXiv:2207.06861v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06861">
<div class="article-summary-box-inner">
<span><p>Nonunion is one of the challenges faced by orthopedics clinics for the
technical difficulties and high costs in photographing interosseous
capillaries. Segmenting vessels and filling capillaries are critical in
understanding the obstacles encountered in capillary growth. However, existing
datasets for blood vessel segmentation mainly focus on the large blood vessels
of the body, and the lack of labeled capillary image datasets greatly limits
the methodological development and applications of vessel segmentation and
capillary filling. Here, we present a benchmark dataset, named IFCIS-155,
consisting of 155 2D capillary images with segmentation boundaries and vessel
fillings annotated by biomedical experts, and 19 large-scale, high-resolution
3D capillary images. To obtain better images of interosseous capillaries, we
leverage state-of-the-art immunofluorescence imaging techniques to highlight
the rich vascular morphology of interosseous capillaries. We conduct
comprehensive experiments to verify the effectiveness of the dataset and the
benchmarking deep learning models (\eg UNet/UNet++ and the modified
UNet/UNet++). Our work offers a benchmark dataset for training deep learning
models for capillary image segmentation and provides a potential tool for
future capillary research. The IFCIS-155 dataset and code are all publicly
available at \url{https://github.com/ncclabsustech/IFCIS-55}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BayesCap: Bayesian Identity Cap for Calibrated Uncertainty in Frozen Neural Networks. (arXiv:2207.06873v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06873">
<div class="article-summary-box-inner">
<span><p>High-quality calibrated uncertainty estimates are crucial for numerous
real-world applications, especially for deep learning-based deployed ML
systems. While Bayesian deep learning techniques allow uncertainty estimation,
training them with large-scale datasets is an expensive process that does not
always yield models competitive with non-Bayesian counterparts. Moreover, many
of the high-performing deep learning models that are already trained and
deployed are non-Bayesian in nature and do not provide uncertainty estimates.
To address these issues, we propose BayesCap that learns a Bayesian identity
mapping for the frozen model, allowing uncertainty estimation. BayesCap is a
memory-efficient method that can be trained on a small fraction of the original
dataset, enhancing pretrained non-Bayesian computer vision models by providing
calibrated uncertainty estimates for the predictions without (i) hampering the
performance of the model and (ii) the need for expensive retraining the model
from scratch. The proposed method is agnostic to various architectures and
tasks. We show the efficacy of our method on a wide variety of tasks with a
diverse set of architectures, including image super-resolution, deblurring,
inpainting, and crucial application such as medical image translation.
Moreover, we apply the derived uncertainty estimates to detect
out-of-distribution samples in critical scenarios like depth estimation in
autonomous driving. Code is available at
https://github.com/ExplainableML/BayesCap.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">E2FIF: Push the limit of Binarized Deep Imagery Super-resolution using End-to-end Full-precision Information Flow. (arXiv:2207.06893v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06893">
<div class="article-summary-box-inner">
<span><p>Binary neural network (BNN) provides a promising solution to deploy
parameter-intensive deep single image super-resolution (SISR) models onto real
devices with limited storage and computational resources. To achieve comparable
performance with the full-precision counterpart, most existing BNNs for SISR
mainly focus on compensating the information loss incurred by binarizing
weights and activations in the network through better approximations to the
binarized convolution. In this study, we revisit the difference between BNNs
and their full-precision counterparts and argue that the key for good
generalization performance of BNNs lies on preserving a complete full-precision
information flow as well as an accurate gradient flow passing through each
binarized convolution layer. Inspired by this, we propose to introduce a
full-precision skip connection or its variant over each binarized convolution
layer across the entire network, which can increase the forward expressive
capability and the accuracy of back-propagated gradient, thus enhancing the
generalization performance. More importantly, such a scheme is applicable to
any existing BNN backbones for SISR without introducing any additional
computation cost. To testify its efficacy, we evaluate it using four different
backbones for SISR on four benchmark datasets and report obviously superior
performance over existing BNNs and even some 4-bit competitors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Factorized and Controllable Neural Re-Rendering of Outdoor Scene for Photo Extrapolation. (arXiv:2207.06899v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06899">
<div class="article-summary-box-inner">
<span><p>Expanding an existing tourist photo from a partially captured scene to a full
scene is one of the desired experiences for photography applications. Although
photo extrapolation has been well studied, it is much more challenging to
extrapolate a photo (i.e., selfie) from a narrow field of view to a wider one
while maintaining a similar visual style. In this paper, we propose a
factorized neural re-rendering model to produce photorealistic novel views from
cluttered outdoor Internet photo collections, which enables the applications
including controllable scene re-rendering, photo extrapolation and even
extrapolated 3D photo generation. Specifically, we first develop a novel
factorized re-rendering pipeline to handle the ambiguity in the decomposition
of geometry, appearance and illumination. We also propose a composited training
strategy to tackle the unexpected occlusion in Internet images. Moreover, to
enhance photo-realism when extrapolating tourist photographs, we propose a
novel realism augmentation process to complement appearance details, which
automatically propagates the texture details from a narrow captured photo to
the extrapolated neural rendered image. The experiments and photo editing
examples on outdoor scenes demonstrate the superior performance of our proposed
method in both photo-realism and downstream applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-time Streaming Video Denoising with Bidirectional Buffers. (arXiv:2207.06937v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06937">
<div class="article-summary-box-inner">
<span><p>Video streams are delivered continuously to save the cost of storage and
device memory. Real-time denoising algorithms are typically adopted on the user
device to remove the noise involved during the shooting and transmission of
video streams. However, sliding-window-based methods feed multiple input frames
for a single output and lack computation efficiency. Recent multi-output
inference works propagate the bidirectional temporal feature with a parallel or
recurrent framework, which either suffers from performance drops on the
temporal edges of clips or can not achieve online inference. In this paper, we
propose a Bidirectional Streaming Video Denoising (BSVD) framework, to achieve
high-fidelity real-time denoising for streaming videos with both past and
future temporal receptive fields. The bidirectional temporal fusion for online
inference is considered not applicable in the MoViNet. However, we introduce a
novel Bidirectional Buffer Block as the core module of our BSVD, which makes it
possible during our pipeline-style inference. In addition, our method is
concise and flexible to be utilized in both non-blind and blind video
denoising. We compare our model with various state-of-the-art video denoising
models qualitatively and quantitatively on synthetic and real noise. Our method
outperforms previous methods in terms of restoration fidelity and runtime. Our
source code is publicly available at https://github.com/ChenyangQiQi/BSVD
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Insurgency as Complex Network: Image Co-Appearance and Hierarchy in the PKK. (arXiv:2207.06946v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06946">
<div class="article-summary-box-inner">
<span><p>Despite a growing recognition of the importance of insurgent group structure
on conflict outcomes, there is very little empirical research thereon. Though
this problem is rooted in the inaccessibility of data on militant group
structure, insurgents frequently publish large volumes of image data on the
internet. In this paper, I develop a new methodology that leverages this
abundant but underutilized source of data by automating the creation of a
social network graph based on co-appearance in photographs using deep learning.
Using a trove of 19,115 obituary images published online by the PKK, a Kurdish
militant group in Turkey, I demonstrate that an individual's centrality in the
resulting co-appearance network is closely correlated with their rank in the
insurgent group.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tackling Background Distraction in Video Object Segmentation. (arXiv:2207.06953v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06953">
<div class="article-summary-box-inner">
<span><p>Semi-supervised video object segmentation (VOS) aims to densely track certain
designated objects in videos. One of the main challenges in this task is the
existence of background distractors that appear similar to the target objects.
We propose three novel strategies to suppress such distractors: 1) a
spatio-temporally diversified template construction scheme to obtain
generalized properties of the target objects; 2) a learnable distance-scoring
function to exclude spatially-distant distractors by exploiting the temporal
consistency between two consecutive frames; 3) swap-and-attach augmentation to
force each object to have unique features by providing training samples
containing entangled objects. On all public benchmark datasets, our model
achieves a comparable performance to contemporary state-of-the-art approaches,
even with real-time performance. Qualitative results also demonstrate the
superiority of our approach over existing methods. We believe our approach will
be widely used for future VOS research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Implicit Templates for Point-Based Clothed Human Modeling. (arXiv:2207.06955v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06955">
<div class="article-summary-box-inner">
<span><p>We present FITE, a First-Implicit-Then-Explicit framework for modeling human
avatars in clothing. Our framework first learns implicit surface templates
representing the coarse clothing topology, and then employs the templates to
guide the generation of point sets which further capture pose-dependent
clothing deformations such as wrinkles. Our pipeline incorporates the merits of
both implicit and explicit representations, namely, the ability to handle
varying topology and the ability to efficiently capture fine details. We also
propose diffused skinning to facilitate template training especially for loose
clothing, and projection-based pose-encoding to extract pose information from
mesh templates without predefined UV map or connectivity. Our code is publicly
available at https://github.com/jsnln/fite.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoMerge: A Framework for Map Assembling and Smoothing in City-scale Environments. (arXiv:2207.06965v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06965">
<div class="article-summary-box-inner">
<span><p>We present AutoMerge, a LiDAR data processing framework for assembling a
large number of map segments into a complete map. Traditional large-scale map
merging methods are fragile to incorrect data associations, and are primarily
limited to working only offline. AutoMerge utilizes multi-perspective fusion
and adaptive loop closure detection for accurate data associations, and it uses
incremental merging to assemble large maps from individual trajectory segments
given in random order and with no initial estimations. Furthermore, after
assembling the segments, AutoMerge performs fine matching and pose-graph
optimization to globally smooth the merged map. We demonstrate AutoMerge on
both city-scale merging (120km) and campus-scale repeated merging (4.5km x 8).
The experiments show that AutoMerge (i) surpasses the second- and third- best
methods by 14% and 24% recall in segment retrieval, (ii) achieves comparable 3D
mapping accuracy for 120 km large-scale map assembly, (iii) and it is robust to
temporally-spaced revisits. To the best of our knowledge, AutoMerge is the
first mapping approach that can merge hundreds of kilometers of individual
segments without the aid of GPS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scene Text Recognition with Permuted Autoregressive Sequence Models. (arXiv:2207.06966v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06966">
<div class="article-summary-box-inner">
<span><p>Context-aware STR methods typically use internal autoregressive (AR) language
models (LM). Inherent limitations of AR models motivated two-stage methods
which employ an external LM. The conditional independence of the external LM on
the input image may cause it to erroneously rectify correct predictions,
leading to significant inefficiencies. Our method, PARSeq, learns an ensemble
of internal AR LMs with shared weights using Permutation Language Modeling. It
unifies context-free non-AR and context-aware AR inference, and iterative
refinement using bidirectional context. Using synthetic training data, PARSeq
achieves state-of-the-art (SOTA) results in STR benchmarks (91.9% accuracy) and
more challenging datasets. It establishes new SOTA results (96.0% accuracy)
when trained on real data. PARSeq is optimal on accuracy vs parameter count,
FLOPS, and latency because of its simple, unified structure and parallel token
processing. Due to its extensive use of attention, it is robust on
arbitrarily-oriented text which is common in real-world images. Code,
pretrained weights, and data are available at: https://github.com/baudm/parseq.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PR-DARTS: Pruning-Based Differentiable Architecture Search. (arXiv:2207.06968v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06968">
<div class="article-summary-box-inner">
<span><p>The deployment of Convolutional Neural Networks (CNNs) on edge devices is
hindered by the substantial gap between performance requirements and available
processing power. While recent research has made large strides in developing
network pruning methods for reducing the computing overhead of CNNs, there
remains considerable accuracy loss, especially at high pruning ratios.
Questioning that the architectures designed for non-pruned networks might not
be effective for pruned networks, we propose to search architectures for
pruning methods by defining a new search space and a novel search objective. To
improve the generalization of the pruned networks, we propose two novel
PrunedConv and PrunedLinear operations. Specifically, these operations mitigate
the problem of unstable gradients by regularizing the objective function of the
pruned networks. The proposed search objective enables us to train architecture
parameters regarding the pruned weight elements. Quantitative analyses
demonstrate that our searched architectures outperform those used in the
state-of-the-art pruning networks on CIFAR-10 and ImageNet. In terms of
hardware effectiveness, PR-DARTS increases MobileNet-v2's accuracy from 73.44%
to 81.35% (+7.91% improvement) and runs 3.87$\times$ faster.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Discriminative Representation via Metric Learning for Imbalanced Medical Image Classification. (arXiv:2207.06975v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06975">
<div class="article-summary-box-inner">
<span><p>Data imbalance between common and rare diseases during model training often
causes intelligent diagnosis systems to have biased predictions towards common
diseases. The state-of-the-art approaches apply a two-stage learning framework
to alleviate the class-imbalance issue, where the first stage focuses on
training of a general feature extractor and the second stage focuses on
fine-tuning the classifier head for class rebalancing. However, existing
two-stage approaches do not consider the fine-grained property between
different diseases, often causing the first stage less effective for medical
image classification than for natural image classification tasks. In this
study, we propose embedding metric learning into the first stage of the
two-stage framework specially to help the feature extractor learn to extract
more discriminative feature representations. Extensive experiments mainly on
three medical image datasets show that the proposed approach consistently
outperforms existing onestage and two-stage approaches, suggesting that metric
learning can be used as an effective plug-in component in the two-stage
framework for fine-grained class-imbalanced image classification tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ObjectBox: From Centers to Boxes for Anchor-Free Object Detection. (arXiv:2207.06985v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06985">
<div class="article-summary-box-inner">
<span><p>We present ObjectBox, a novel single-stage anchor-free and highly
generalizable object detection approach. As opposed to both existing
anchor-based and anchor-free detectors, which are more biased toward specific
object scales in their label assignments, we use only object center locations
as positive samples and treat all objects equally in different feature levels
regardless of the objects' sizes or shapes. Specifically, our label assignment
strategy considers the object center locations as shape- and size-agnostic
anchors in an anchor-free fashion, and allows learning to occur at all scales
for every object. To support this, we define new regression targets as the
distances from two corners of the center cell location to the four sides of the
bounding box. Moreover, to handle scale-variant objects, we propose a tailored
IoU loss to deal with boxes with different sizes. As a result, our proposed
object detector does not need any dataset-dependent hyperparameters to be tuned
across datasets. We evaluate our method on MS-COCO 2017 and PASCAL VOC 2012
datasets, and compare our results to state-of-the-art methods. We observe that
ObjectBox performs favorably in comparison to prior works. Furthermore, we
perform rigorous ablation experiments to evaluate different components of our
method. Our code is available at: https://github.com/MohsenZand/ObjectBox.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tree Structure-Aware Few-Shot Image Classification via Hierarchical Aggregation. (arXiv:2207.06989v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06989">
<div class="article-summary-box-inner">
<span><p>In this paper, we mainly focus on the problem of how to learn additional
feature representations for few-shot image classification through pretext tasks
(e.g., rotation or color permutation and so on). This additional knowledge
generated by pretext tasks can further improve the performance of few-shot
learning (FSL) as it differs from human-annotated supervision (i.e., class
labels of FSL tasks). To solve this problem, we present a plug-in Hierarchical
Tree Structure-aware (HTS) method, which not only learns the relationship of
FSL and pretext tasks, but more importantly, can adaptively select and
aggregate feature representations generated by pretext tasks to maximize the
performance of FSL tasks. A hierarchical tree constructing component and a
gated selection aggregating component is introduced to construct the tree
structure and find richer transferable knowledge that can rapidly adapt to
novel classes with a few labeled images. Extensive experiments show that our
HTS can significantly enhance multiple few-shot methods to achieve new
state-of-the-art performance on four benchmark datasets. The code is available
at: https://github.com/remiMZ/HTS-ECCV22.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Modelling with Pixels. (arXiv:2207.06991v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06991">
<div class="article-summary-box-inner">
<span><p>Language models are defined over a finite set of inputs, which creates a
vocabulary bottleneck when we attempt to scale the number of supported
languages. Tackling this bottleneck results in a trade-off between what can be
represented in the embedding matrix and computational issues in the output
layer. This paper introduces PIXEL, the Pixel-based Encoder of Language, which
suffers from neither of these issues. PIXEL is a pretrained language model that
renders text as images, making it possible to transfer representations across
languages based on orthographic similarity or the co-activation of pixels.
PIXEL is trained to reconstruct the pixels of masked patches, instead of
predicting a distribution over tokens. We pretrain the 86M parameter PIXEL
model on the same English data as BERT and evaluate on syntactic and semantic
tasks in typologically diverse languages, including various non-Latin scripts.
We find that PIXEL substantially outperforms BERT on syntactic and semantic
processing tasks on scripts that are not found in the pretraining data, but
PIXEL is slightly weaker than BERT when working with Latin scripts.
Furthermore, we find that PIXEL is more robust to noisy text inputs than BERT,
further confirming the benefits of modelling language with pixels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accurate Ground-Truth Depth Image Generation via Overfit Training of Point Cloud Registration using Local Frame Sets. (arXiv:2207.07016v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07016">
<div class="article-summary-box-inner">
<span><p>Accurate three-dimensional perception is a fundamental task in several
computer vision applications. Recently, commercial RGB-depth (RGB-D) cameras
have been widely adopted as single-view depth-sensing devices owing to their
efficient depth-sensing abilities. However, the depth quality of most RGB-D
sensors remains insufficient owing to the inherent noise from a single-view
environment. Recently, several studies have focused on the single-view depth
enhancement of RGB-D cameras. Recent research has proposed deep-learning-based
approaches that typically train networks using high-quality supervised depth
datasets, which indicates that the quality of the ground-truth (GT) depth
dataset is a top-most important factor for accurate system; however, such
high-quality GT datasets are difficult to obtain. In this study, we developed a
novel method for high-quality GT depth generation based on an RGB-D stream
dataset. First, we defined consecutive depth frames in a local spatial region
as a local frame set. Then, the depth frames were aligned to a certain frame in
the local frame set using an unsupervised point cloud registration scheme. The
registration parameters were trained based on an overfit-training scheme, which
was primarily used to construct a single GT depth image for each frame set. The
final GT depth dataset was constructed using several local frame sets, and each
local frame set was trained independently. The primary advantage of this study
is that a high-quality GT depth dataset can be constructed under various
scanning environments using only the RGB-D stream dataset. Moreover, our
proposed method can be used as a new benchmark GT dataset for accurate
performance evaluations. We evaluated our GT dataset on previously benchmarked
GT depth datasets and demonstrated that our method is superior to
state-of-the-art depth enhancement frameworks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MedFuse: Multi-modal fusion with clinical time-series data and chest X-ray images. (arXiv:2207.07027v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07027">
<div class="article-summary-box-inner">
<span><p>Multi-modal fusion approaches aim to integrate information from different
data sources. Unlike natural datasets, such as in audio-visual applications,
where samples consist of "paired" modalities, data in healthcare is often
collected asynchronously. Hence, requiring the presence of all modalities for a
given sample is not realistic for clinical tasks and significantly limits the
size of the dataset during training. In this paper, we propose MedFuse, a
conceptually simple yet promising LSTM-based fusion module that can accommodate
uni-modal as well as multi-modal input. We evaluate the fusion method and
introduce new benchmark results for in-hospital mortality prediction and
phenotype classification, using clinical time-series data in the MIMIC-IV
dataset and corresponding chest X-ray images in MIMIC-CXR. Compared to more
complex multi-modal fusion strategies, MedFuse provides a performance
improvement by a large margin on the fully paired test set. It also remains
robust across the partially paired test set containing samples with missing
chest X-ray images. We release our code for reproducibility and to enable the
evaluation of competing models in the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Attacks on Monocular Pose Estimation. (arXiv:2207.07032v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07032">
<div class="article-summary-box-inner">
<span><p>Advances in deep learning have resulted in steady progress in computer vision
with improved accuracy on tasks such as object detection and semantic
segmentation. Nevertheless, deep neural networks are vulnerable to adversarial
attacks, thus presenting a challenge in reliable deployment. Two of the
prominent tasks in 3D scene-understanding for robotics and advanced drive
assistance systems are monocular depth and pose estimation, often learned
together in an unsupervised manner. While studies evaluating the impact of
adversarial attacks on monocular depth estimation exist, a systematic
demonstration and analysis of adversarial perturbations against pose estimation
are lacking. We show how additive imperceptible perturbations can not only
change predictions to increase the trajectory drift but also catastrophically
alter its geometry. We also study the relation between adversarial
perturbations targeting monocular depth and pose estimation networks, as well
as the transferability of perturbations to other networks with different
architectures and losses. Our experiments show how the generated perturbations
lead to notable errors in relative rotation and translation predictions and
elucidate vulnerabilities of the networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Single Self-Supervised Model for Many Speech Modalities Enables Zero-Shot Modality Transfer. (arXiv:2207.07036v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07036">
<div class="article-summary-box-inner">
<span><p>While audio-visual speech models can yield superior performance and
robustness compared to audio-only models, their development and adoption are
hindered by the lack of labeled and unlabeled audio-visual data and the cost to
deploy one model per modality. In this paper, we present u-HuBERT, a
self-supervised pre-training framework that can leverage both multimodal and
unimodal speech with a unified masked cluster prediction objective. By
utilizing modality dropout during pre-training, we demonstrate that a single
fine-tuned model can achieve performance on par or better than the
state-of-the-art modality-specific models. Moreover, our model fine-tuned only
on audio can perform well with audio-visual and visual speech input, achieving
zero-shot modality generalization for speech recognition and speaker
verification. In particular, our single model yields 1.2%/1.4%/27.2% speech
recognition word error rate on LRS3 with audio-visual/audio/visual input.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Convolutional Bypasses Are Better Vision Transformer Adapters. (arXiv:2207.07039v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07039">
<div class="article-summary-box-inner">
<span><p>The pretrain-then-finetune paradigm has been widely adopted in computer
vision. But as the size of Vision Transformer (ViT) grows exponentially, the
full finetuning becomes prohibitive in view of the heavier storage overhead.
Motivated by parameter-efficient transfer learning (PETL) on language
transformers, recent studies attempt to insert lightweight adaptation modules
(e.g., adapter layers or prompt tokens) to pretrained ViT and only finetune
these modules while the pretrained weights are frozen. However, these modules
were originally proposed to finetune language models. Although ported well to
ViT, their design lacks prior knowledge for visual tasks. In this paper, we
propose to construct Convolutional Bypasses (Convpass) in ViT as adaptation
modules, introducing only a small amount (less than 0.5% of model parameters)
of trainable parameters to adapt the large ViT. Different from other PETL
methods, Convpass benefits from the hard-coded inductive bias of convolutional
layers and thus is more suitable for visual tasks, especially in the low-data
regime. Experimental results on VTAB-1k benchmark and few-shot learning
datasets demonstrate that Convpass outperforms current language-oriented
adaptation modules, demonstrating the necessity to tailor vision-oriented
adaptation modules for vision models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Supervised Temporal Action Detection with Proposal-Free Masking. (arXiv:2207.07059v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07059">
<div class="article-summary-box-inner">
<span><p>Existing temporal action detection (TAD) methods rely on a large number of
training data with segment-level annotations. Collecting and annotating such a
training set is thus highly expensive and unscalable. Semi-supervised TAD
(SS-TAD) alleviates this problem by leveraging unlabeled videos freely
available at scale. However, SS-TAD is also a much more challenging problem
than supervised TAD, and consequently much under-studied. Prior SS-TAD methods
directly combine an existing proposal-based TAD method and a SSL method. Due to
their sequential localization (e.g, proposal generation) and classification
design, they are prone to proposal error propagation. To overcome this
limitation, in this work we propose a novel Semi-supervised Temporal action
detection model based on PropOsal-free Temporal mask (SPOT) with a parallel
localization (mask generation) and classification architecture. Such a novel
design effectively eliminates the dependence between localization and
classification by cutting off the route for error propagation in-between. We
further introduce an interaction mechanism between classification and
localization for prediction refinement, and a new pretext task for
self-supervised model pre-training. Extensive experiments on two standard
benchmarks show that our SPOT outperforms state-of-the-art alternatives, often
by a large margin. The PyTorch implementation of SPOT is available at
https://github.com/sauradip/SPOT
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Egocentric Scene Understanding via Multimodal Spatial Rectifier. (arXiv:2207.07077v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07077">
<div class="article-summary-box-inner">
<span><p>In this paper, we study a problem of egocentric scene understanding, i.e.,
predicting depths and surface normals from an egocentric image. Egocentric
scene understanding poses unprecedented challenges: (1) due to large head
movements, the images are taken from non-canonical viewpoints (i.e., tilted
images) where existing models of geometry prediction do not apply; (2) dynamic
foreground objects including hands constitute a large proportion of visual
scenes. These challenges limit the performance of the existing models learned
from large indoor datasets, such as ScanNet and NYUv2, which comprise
predominantly upright images of static scenes. We present a multimodal spatial
rectifier that stabilizes the egocentric images to a set of reference
directions, which allows learning a coherent visual representation. Unlike
unimodal spatial rectifier that often produces excessive perspective warp for
egocentric images, the multimodal spatial rectifier learns from multiple
directions that can minimize the impact of the perspective warp. To learn
visual representations of the dynamic foreground objects, we present a new
dataset called EDINA (Egocentric Depth on everyday INdoor Activities) that
comprises more than 500K synchronized RGBD frames and gravity directions.
Equipped with the multimodal spatial rectifier and the EDINA dataset, our
proposed method on single-view depth and surface normal estimation
significantly outperforms the baselines not only on our EDINA dataset, but also
on other popular egocentric datasets, such as First Person Hand Action (FPHA)
and EPIC-KITCHENS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Grand Unification of Object Tracking. (arXiv:2207.07078v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07078">
<div class="article-summary-box-inner">
<span><p>We present a unified method, termed Unicorn, that can simultaneously solve
four tracking problems (SOT, MOT, VOS, MOTS) with a single network using the
same model parameters. Due to the fragmented definitions of the object tracking
problem itself, most existing trackers are developed to address a single or
part of tasks and overspecialize on the characteristics of specific tasks. By
contrast, Unicorn provides a unified solution, adopting the same input,
backbone, embedding, and head across all tracking tasks. For the first time, we
accomplish the great unification of the tracking network architecture and
learning paradigm. Unicorn performs on-par or better than its task-specific
counterparts in 8 tracking datasets, including LaSOT, TrackingNet, MOT17,
BDD100K, DAVIS16-17, MOTS20, and BDD100K MOTS. We believe that Unicorn will
serve as a solid step towards the general vision model. Code is available at
https://github.com/MasterBin-IIAU/Unicorn.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Asymmetric Contrastive Loss for Handling Imbalanced Datasets. (arXiv:2207.07080v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07080">
<div class="article-summary-box-inner">
<span><p>Contrastive learning is a representation learning method performed by
contrasting a sample to other similar samples so that they are brought closely
together, forming clusters in the feature space. The learning process is
typically conducted using a two-stage training architecture, and it utilizes
the contrastive loss (CL) for its feature learning. Contrastive learning has
been shown to be quite successful in handling imbalanced datasets, in which
some classes are overrepresented while some others are underrepresented.
However, previous studies have not specifically modified CL for imbalanced
datasets. In this work, we introduce an asymmetric version of CL, referred to
as ACL, in order to directly address the problem of class imbalance. In
addition, we propose the asymmetric focal contrastive loss (AFCL) as a further
generalization of both ACL and focal contrastive loss (FCL). Results on the
FMNIST and ISIC 2018 imbalanced datasets show that AFCL is capable of
outperforming CL and FCL in terms of both weighted and unweighted
classification accuracies. In the appendix, we provide a full axiomatic
treatment on entropy, along with complete proofs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Personalized Zero-Shot ECG Arrhythmia Monitoring System: From Sparse Representation Based Domain Adaption to Energy Efficient Abnormal Beat Detection for Practical ECG Surveillance. (arXiv:2207.07089v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07089">
<div class="article-summary-box-inner">
<span><p>This paper proposes a low-cost and highly accurate ECG-monitoring system
intended for personalized early arrhythmia detection for wearable mobile
sensors. Earlier supervised approaches for personalized ECG monitoring require
both abnormal and normal heartbeats for the training of the dedicated
classifier. However, in a real-world scenario where the personalized algorithm
is embedded in a wearable device, such training data is not available for
healthy people with no cardiac disorder history. In this study, (i) we propose
a null space analysis on the healthy signal space obtained via sparse
dictionary learning, and investigate how a simple null space projection or
alternatively regularized least squares-based classification methods can reduce
the computational complexity, without sacrificing the detection accuracy, when
compared to sparse representation-based classification. (ii) Then we introduce
a sparse representation-based domain adaptation technique in order to project
other existing users' abnormal and normal signals onto the new user's signal
space, enabling us to train the dedicated classifier without having any
abnormal heartbeat of the new user. Therefore, zero-shot learning can be
achieved without the need for synthetic abnormal heartbeat generation. An
extensive set of experiments performed on the benchmark MIT-BIH ECG dataset
shows that when this domain adaptation-based training data generator is used
with a simple 1-D CNN classifier, the method outperforms the prior work by a
significant margin. (iii) Then, by combining (i) and (ii), we propose an
ensemble classifier that further improves the performance. This approach for
zero-shot arrhythmia detection achieves an average accuracy level of 98.2% and
an F1-Score of 92.8%. Finally, a personalized energy-efficient ECG monitoring
scheme is proposed using the above-mentioned innovations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explaining Image Enhancement Black-Box Methods through a Path Planning Based Algorithm. (arXiv:2207.07092v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07092">
<div class="article-summary-box-inner">
<span><p>Nowadays, image-to-image translation methods, are the state of the art for
the enhancement of natural images. Even if they usually show high performance
in terms of accuracy, they often suffer from several limitations such as the
generation of artifacts and the scalability to high resolutions. Moreover,
their main drawback is the completely black-box approach that does not allow to
provide the final user with any insight about the enhancement processes
applied. In this paper we present a path planning algorithm which provides a
step-by-step explanation of the output produced by state of the art enhancement
methods, overcoming black-box limitation. This algorithm, called eXIE, uses a
variant of the A* algorithm to emulate the enhancement process of another
method through the application of an equivalent sequence of enhancing
operators. We applied eXIE to explain the output of several state-of-the-art
models trained on the Five-K dataset, obtaining sequences of enhancing
operators able to produce very similar results in terms of performance and
overcoming the huge limitation of poor interpretability of the best performing
algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReAct: Temporal Action Detection with Relational Queries. (arXiv:2207.07097v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07097">
<div class="article-summary-box-inner">
<span><p>This work aims at advancing temporal action detection (TAD) using an
encoder-decoder framework with action queries, similar to DETR, which has shown
great success in object detection. However, the framework suffers from several
problems if directly applied to TAD: the insufficient exploration of
inter-query relation in the decoder, the inadequate classification training due
to a limited number of training samples, and the unreliable classification
scores at inference. To this end, we first propose a relational attention
mechanism in the decoder, which guides the attention among queries based on
their relations. Moreover, we propose two losses to facilitate and stabilize
the training of action classification. Lastly, we propose to predict the
localization quality of each action query at inference in order to distinguish
high-quality queries. The proposed method, named ReAct, achieves the
state-of-the-art performance on THUMOS14, with much lower computational costs
than previous methods. Besides, extensive ablation studies are conducted to
verify the effectiveness of each proposed component. The code is available at
https://github.com/sssste/React.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relighting4D: Neural Relightable Human from Videos. (arXiv:2207.07104v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07104">
<div class="article-summary-box-inner">
<span><p>Human relighting is a highly desirable yet challenging task. Existing works
either require expensive one-light-at-a-time (OLAT) captured data using light
stage or cannot freely change the viewpoints of the rendered body. In this
work, we propose a principled framework, Relighting4D, that enables
free-viewpoints relighting from only human videos under unknown illuminations.
Our key insight is that the space-time varying geometry and reflectance of the
human body can be decomposed as a set of neural fields of normal, occlusion,
diffuse, and specular maps. These neural fields are further integrated into
reflectance-aware physically based rendering, where each vertex in the neural
field absorbs and reflects the light from the environment. The whole framework
can be learned from videos in a self-supervised manner, with physically
informed priors designed for regularization. Extensive experiments on both real
and synthetic datasets demonstrate that our framework is capable of relighting
dynamic human actors with free-viewpoints.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Benchmarking Omni-Vision Representation through the Lens of Visual Realms. (arXiv:2207.07106v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07106">
<div class="article-summary-box-inner">
<span><p>Though impressive performance has been achieved in specific visual realms
(e.g. faces, dogs, and places), an omni-vision representation generalizing to
many natural visual domains is highly desirable. But, existing benchmarks are
biased and inefficient to evaluate the omni-vision representation -- these
benchmarks either only include several specific realms, or cover most realms at
the expense of subsuming numerous datasets that have extensive realm
overlapping. In this paper, we propose Omni-Realm Benchmark (OmniBenchmark). It
includes 21 realm-wise datasets with 7,372 concepts and 1,074,346 images.
Without semantic overlapping, these datasets cover most visual realms
comprehensively and meanwhile efficiently. In addition, we propose a new
supervised contrastive learning framework, namely Relational Contrastive
learning (ReCo), for a better omni-vision representation. Beyond pulling two
instances from the same concept closer -- the typical supervised contrastive
learning framework -- ReCo also pulls two instances from the same semantic
realm closer, encoding the semantic relation between concepts, and facilitating
omni-vision representation learning. We benchmark ReCo and other advances in
omni-vision representation studies that are different in architectures (from
CNNs to transformers) and in learning paradigms (from supervised learning to
self-supervised learning) on OmniBenchmark. We illustrate the superior of ReCo
to other supervised contrastive learning methods and reveal multiple practical
observations to facilitate future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-grained Few-shot Recognition by Deep Object Parsing. (arXiv:2207.07110v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07110">
<div class="article-summary-box-inner">
<span><p>In our framework, an object is made up of K distinct parts or units, and we
parse a test instance by inferring the K parts, where each part occupies a
distinct location in the feature space, and the instance features at this
location, manifest as an active subset of part templates shared across all
instances. We recognize test instances by comparing its active templates and
the relative geometry of its part locations against those of the presented
few-shot instances. We propose an end-to-end training method to learn part
templates on-top of a convolutional backbone. To combat visual distortions such
as orientation, pose and size, we learn multi-scale templates, and at test-time
parse and match instances across these scales. We show that our method is
competitive with the state-of-the-art, and by virtue of parsing enjoys
interpretability as well.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">XMem: Long-Term Video Object Segmentation with an Atkinson-Shiffrin Memory Model. (arXiv:2207.07115v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07115">
<div class="article-summary-box-inner">
<span><p>We present XMem, a video object segmentation architecture for long videos
with unified feature memory stores inspired by the Atkinson-Shiffrin memory
model. Prior work on video object segmentation typically only uses one type of
feature memory. For videos longer than a minute, a single feature memory model
tightly links memory consumption and accuracy. In contrast, following the
Atkinson-Shiffrin model, we develop an architecture that incorporates multiple
independent yet deeply-connected feature memory stores: a rapidly updated
sensory memory, a high-resolution working memory, and a compact thus sustained
long-term memory. Crucially, we develop a memory potentiation algorithm that
routinely consolidates actively used working memory elements into the long-term
memory, which avoids memory explosion and minimizes performance decay for
long-term prediction. Combined with a new memory reading mechanism, XMem
greatly exceeds state-of-the-art performance on long-video datasets while being
on par with state-of-the-art methods (that do not work on long videos) on
short-video datasets. Code is available at https://hkchengrex.github.io/XMem
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bootstrapped Masked Autoencoders for Vision BERT Pretraining. (arXiv:2207.07116v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07116">
<div class="article-summary-box-inner">
<span><p>We propose bootstrapped masked autoencoders (BootMAE), a new approach for
vision BERT pretraining. BootMAE improves the original masked autoencoders
(MAE) with two core designs: 1) momentum encoder that provides online feature
as extra BERT prediction targets; 2) target-aware decoder that tries to reduce
the pressure on the encoder to memorize target-specific information in BERT
pretraining. The first design is motivated by the observation that using a
pretrained MAE to extract the features as the BERT prediction target for masked
tokens can achieve better pretraining performance. Therefore, we add a momentum
encoder in parallel with the original MAE encoder, which bootstraps the
pretraining performance by using its own representation as the BERT prediction
target. In the second design, we introduce target-specific information (e.g.,
pixel values of unmasked patches) from the encoder directly to the decoder to
reduce the pressure on the encoder of memorizing the target-specific
information. Thus, the encoder focuses on semantic modeling, which is the goal
of BERT pretraining, and does not need to waste its capacity in memorizing the
information of unmasked tokens related to the prediction target. Through
extensive experiments, our BootMAE achieves $84.2\%$ Top-1 accuracy on
ImageNet-1K with ViT-B backbone, outperforming MAE by $+0.8\%$ under the same
pre-training epochs. BootMAE also gets $+1.0$ mIoU improvements on semantic
segmentation on ADE20K and $+1.3$ box AP, $+1.4$ mask AP improvement on object
detection and segmentation on COCO dataset. Code is released at
https://github.com/LightDXY/BootMAE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Implementation of Machine Learning for the Efficient, Explainable Diagnosis of COVID-19 from Chest CT. (arXiv:2207.07117v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07117">
<div class="article-summary-box-inner">
<span><p>In a worldwide health crisis as exigent as COVID-19, there has become a
pressing need for rapid, reliable diagnostics. Currently, popular testing
methods such as reverse transcription polymerase chain reaction (RT-PCR) can
have high false negative rates. Consequently, COVID-19 patients are not
accurately identified nor treated quickly enough to prevent transmission of the
virus. However, the recent rise of medical CT data has presented promising
avenues, since CT manifestations contain key characteristics indicative of
COVID-19. This study aimed to take a novel approach in the machine
learning-based detection of COVID-19 from chest CT scans. First, the dataset
utilized in this study was derived from three major sources, comprising a total
of 17,698 chest CT slices across 923 patient cases. Image preprocessing
algorithms were then developed to reduce noise by excluding irrelevant
features. Transfer learning was also implemented with the EfficientNetB7
pre-trained model to provide a backbone architecture and save computational
resources. Lastly, several explainability techniques were leveraged to
qualitatively validate model performance by localizing infected regions and
highlighting fine-grained pixel details. The proposed model attained an overall
accuracy of 0.927 and a sensitivity of 0.958. Explainability measures showed
that the model correctly distinguished between relevant, critical features
pertaining to COVID-19 chest CT images and normal controls. Deep learning
frameworks provide efficient, human-interpretable COVID-19 diagnostics that
could complement radiologist decisions or serve as an alternative screening
tool. Future endeavors may provide insight into infection severity, patient
risk stratification, and prognosis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Deep Epipolar Flow for Stationary or Dynamic Scenes. (arXiv:1904.03848v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1904.03848">
<div class="article-summary-box-inner">
<span><p>Unsupervised deep learning for optical flow computation has achieved
promising results. Most existing deep-net based methods rely on image
brightness consistency and local smoothness constraint to train the networks.
Their performance degrades at regions where repetitive textures or occlusions
occur. In this paper, we propose Deep Epipolar Flow, an unsupervised optical
flow method which incorporates global geometric constraints into network
learning. In particular, we investigate multiple ways of enforcing the epipolar
constraint in flow estimation. To alleviate a "chicken-and-egg" type of problem
encountered in dynamic scenes where multiple motions may be present, we propose
a low-rank constraint as well as a union-of-subspaces constraint for training.
Experimental results on various benchmarking datasets show that our method
achieves competitive performance compared with supervised methods and
outperforms state-of-the-art unsupervised deep-learning methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Surface Normal Estimation of Tilted Images via Spatial Rectifier. (arXiv:2007.09264v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.09264">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a spatial rectifier to estimate surface normals of
tilted images. Tilted images are of particular interest as more visual data are
captured by arbitrarily oriented sensors such as body-/robot-mounted cameras.
Existing approaches exhibit bounded performance on predicting surface normals
because they were trained using gravity-aligned images. Our two main hypotheses
are: (1) visual scene layout is indicative of the gravity direction; and (2)
not all surfaces are equally represented by a learned estimator due to the
structured distribution of the training data, thus, there exists a
transformation for each tilted image that is more responsive to the learned
estimator than others. We design a spatial rectifier that is learned to
transform the surface normal distribution of a tilted image to the rectified
one that matches the gravity-aligned training data distribution. Along with the
spatial rectifier, we propose a novel truncated angular loss that offers a
stronger gradient at smaller angular errors and robustness to outliers. The
resulting estimator outperforms the state-of-the-art methods including data
augmentation baselines not only on ScanNet and NYUv2 but also on a new dataset
called Tilt-RGBD that includes considerable roll and pitch camera motion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RGB-D Salient Object Detection: A Survey. (arXiv:2008.00230v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.00230">
<div class="article-summary-box-inner">
<span><p>Salient object detection (SOD), which simulates the human visual perception
system to locate the most attractive object(s) in a scene, has been widely
applied to various computer vision tasks. Now, with the advent of depth
sensors, depth maps with affluent spatial information that can be beneficial in
boosting the performance of SOD, can easily be captured. Although various RGB-D
based SOD models with promising performance have been proposed over the past
several years, an in-depth understanding of these models and challenges in this
topic remains lacking. In this paper, we provide a comprehensive survey of
RGB-D based SOD models from various perspectives, and review related benchmark
datasets in detail. Further, considering that the light field can also provide
depth maps, we review SOD models and popular benchmark datasets from this
domain as well. Moreover, to investigate the SOD ability of existing models, we
carry out a comprehensive evaluation, as well as attribute-based evaluation of
several representative RGB-D based SOD models. Finally, we discuss several
challenges and open directions of RGB-D based SOD for future research. All
collected models, benchmark datasets, source code links, datasets constructed
for attribute-based evaluation, and codes for evaluation will be made publicly
available at https://github.com/taozh2017/RGBDSODsurvey
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BANet: Blur-aware Attention Networks for Dynamic Scene Deblurring. (arXiv:2101.07518v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.07518">
<div class="article-summary-box-inner">
<span><p>Image motion blur usually results from moving objects or camera shakes. Such
blur is generally directional and non-uniform. Previous research efforts
attempt to solve non-uniform blur by using self-recurrent multi-scale or
multi-patch architectures accompanying with self-attention. However, using
self-recurrent frameworks typically leads to a longer inference time, while
inter-pixel or inter-channel self-attention may cause excessive memory usage.
This paper proposes blur-aware attention networks (BANet) that accomplish
accurate and efficient deblurring via a single forward pass. Our BANet utilizes
region-based self-attention with multi-kernel strip pooling to disentangle blur
patterns of different degrees and with cascaded parallel dilated convolution to
aggregate multi-scale content features. Extensive experimental results on the
GoPro and HIDE benchmarks demonstrate that the proposed BANet performs
favorably against the state-of-the-art in blurred image restoration and can
provide deblurred results in real-time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Blurs Behave Like Ensembles: Spatial Smoothings to Improve Accuracy, Uncertainty, and Robustness. (arXiv:2105.12639v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.12639">
<div class="article-summary-box-inner">
<span><p>Neural network ensembles, such as Bayesian neural networks (BNNs), have shown
success in the areas of uncertainty estimation and robustness. However, a
crucial challenge prohibits their use in practice. BNNs require a large number
of predictions to produce reliable results, leading to a significant increase
in computational cost. To alleviate this issue, we propose spatial smoothing, a
method that spatially ensembles neighboring feature map points of convolutional
neural networks. By simply adding a few blur layers to the models, we
empirically show that spatial smoothing improves accuracy, uncertainty
estimation, and robustness of BNNs across a whole range of ensemble sizes. In
particular, BNNs incorporating spatial smoothing achieve high predictive
performance merely with a handful of ensembles. Moreover, this method also can
be applied to canonical deterministic neural networks to improve the
performances. A number of evidences suggest that the improvements can be
attributed to the stabilized feature maps and the smoothing of the loss
landscape. In addition, we provide a fundamental explanation for prior works -
namely, global average pooling, pre-activation, and ReLU6 - by addressing them
as special cases of spatial smoothing. These not only enhance accuracy, but
also improve uncertainty estimation and robustness by making the loss landscape
smoother in the same manner as spatial smoothing. The code is available at
https://github.com/xxxnell/spatial-smoothing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual Contrastive Learning for Image Classification. (arXiv:2107.01776v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.01776">
<div class="article-summary-box-inner">
<span><p>For artificial learning systems, continual learning over time from a stream
of data is essential. The burgeoning studies on supervised continual learning
have achieved great progress, while the study of catastrophic forgetting in
unsupervised learning is still blank. Among unsupervised learning methods,
self-supervise learning method shows tremendous potential on visual
representation without any labeled data at scale. To improve the visual
representation of self-supervised learning, larger and more varied data is
needed. In the real world, unlabeled data is generated at all times. This
circumstance provides a huge advantage for the learning of the self-supervised
method. However, in the current paradigm, packing previous data and current
data together and training it again is a waste of time and resources. Thus, a
continual self-supervised learning method is badly needed. In this paper, we
make the first attempt to implement the continual contrastive self-supervised
learning by proposing a rehearsal method, which keeps a few exemplars from the
previous data. Instead of directly combining saved exemplars with the current
data set for training, we leverage self-supervised knowledge distillation to
transfer contrastive information among previous data to the current network by
mimicking similarity score distribution inferred by the old network over a set
of saved exemplars. Moreover, we build an extra sample queue to assist the
network to distinguish between previous and current data and prevent mutual
interference while learning their own feature representation. Experimental
results show that our method performs well on CIFAR100 and ImageNet-Sub.
Compared with the baselines, which learning tasks without taking any technique,
we improve the image classification top-1 accuracy by 1.60% on CIFAR100, 2.86%
on ImageNet-Sub and 1.29% on ImageNet-Full under 10 incremental steps setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FOCUS: Familiar Objects in Common and Uncommon Settings. (arXiv:2110.03804v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03804">
<div class="article-summary-box-inner">
<span><p>Standard training datasets for deep learning often contain objects in common
settings (e.g., "a horse on grass" or "a ship in water") since they are usually
collected by randomly scraping the web. Uncommon and rare settings (e.g., "a
plane on water", "a car in snowy weather") are thus severely under-represented
in the training data. This can lead to an undesirable bias in model predictions
towards common settings and create a false sense of accuracy. In this paper, we
introduce FOCUS (Familiar Objects in Common and Uncommon Settings), a dataset
for stress-testing the generalization power of deep image classifiers. By
leveraging the power of modern search engines, we deliberately gather data
containing objects in common and uncommon settings in a wide range of
locations, weather conditions, and time of day. We present a detailed analysis
of the performance of various popular image classifiers on our dataset and
demonstrate a clear drop in performance when classifying images in uncommon
settings. By analyzing deep features of these models, we show that such errors
can be due to the use of spurious features in model predictions. We believe
that our dataset will aid researchers in understanding the inability of deep
models to generalize well to uncommon settings and drive future work on
improving their distributional robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wide Neural Networks Forget Less Catastrophically. (arXiv:2110.11526v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11526">
<div class="article-summary-box-inner">
<span><p>A primary focus area in continual learning research is alleviating the
"catastrophic forgetting" problem in neural networks by designing new
algorithms that are more robust to the distribution shifts. While the recent
progress in continual learning literature is encouraging, our understanding of
what properties of neural networks contribute to catastrophic forgetting is
still limited. To address this, instead of focusing on continual learning
algorithms, in this work, we focus on the model itself and study the impact of
"width" of the neural network architecture on catastrophic forgetting, and show
that width has a surprisingly significant effect on forgetting. To explain this
effect, we study the learning dynamics of the network from various perspectives
such as gradient orthogonality, sparsity, and lazy training regime. We provide
potential explanations that are consistent with the empirical results across
different architectures and continual learning benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Class-agnostic Object Detection with Multi-modal Transformer. (arXiv:2111.11430v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11430">
<div class="article-summary-box-inner">
<span><p>What constitutes an object? This has been a long-standing question in
computer vision. Towards this goal, numerous learning-free and learning-based
approaches have been developed to score objectness. However, they generally do
not scale well across new domains and novel objects. In this paper, we advocate
that existing methods lack a top-down supervision signal governed by
human-understandable semantics. For the first time in literature, we
demonstrate that Multi-modal Vision Transformers (MViT) trained with aligned
image-text pairs can effectively bridge this gap. Our extensive experiments
across various domains and novel objects show the state-of-the-art performance
of MViTs to localize generic objects in images. Based on the observation that
existing MViTs do not include multi-scale feature processing and usually
require longer training schedules, we develop an efficient MViT architecture
using multi-scale deformable attention and late vision-language fusion. We show
the significance of MViT proposals in a diverse range of applications including
open-world object detection, salient and camouflage object detection,
supervised and self-supervised detection tasks. Further, MViTs can adaptively
generate proposals given a specific language query and thus offer enhanced
interactability. Code: \url{https://git.io/J1HPY}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph Modularity: Towards Understanding the Cross-Layer Transition of Feature Representations in Deep Neural Networks. (arXiv:2111.12485v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12485">
<div class="article-summary-box-inner">
<span><p>There are good arguments to support the claim that deep neural networks
(DNNs) capture better feature representations than the previous hand-crafted
feature engineering, which leads to a significant performance improvement. In
this paper, we move a tiny step towards understanding the dynamics of feature
representations over layers. Specifically, we model the process of class
separation of intermediate representations in pre-trained DNNs as the evolution
of communities in dynamic graphs. Then, we introduce modularity, a generic
metric in graph theory, to quantify the evolution of communities. In the
preliminary experiment, we find that modularity roughly tends to increase as
the layer goes deeper and the degradation and plateau arise when the model
complexity is great relative to the dataset. Through an asymptotic analysis, we
prove that modularity can be broadly used for different applications. For
example, modularity provides new insights to quantify the difference between
feature representations. More crucially, we demonstrate that the degradation
and plateau in modularity curves represent redundant layers in DNNs and can be
pruned with minimal impact on performance, which provides theoretical guidance
for layer pruning. Our code is available at
https://github.com/yaolu-zjut/Dynamic-Graphs-Construction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lightweight Attentional Feature Fusion: A New Baseline for Text-to-Video Retrieval. (arXiv:2112.01832v2 [cs.MM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01832">
<div class="article-summary-box-inner">
<span><p>In this paper we revisit \emph{feature fusion}, an old-fashioned topic, in
the new context of text-to-video retrieval. Different from previous research
that considers feature fusion only at one end, let it be video or text, we aim
for feature fusion for both ends within a unified framework. We hypothesize
that optimizing the convex combination of the features is preferred to modeling
their correlations by computationally heavy multi-head self attention. We
propose Lightweight Attentional Feature Fusion (LAFF). LAFF performs feature
fusion at both early and late stages and at both video and text ends, making it
a powerful method for exploiting diverse (off-the-shelf) features. The
interpretability of LAFF can be used for feature selection. Extensive
experiments on five public benchmark sets (MSR-VTT, MSVD, TGIF, VATEX and
TRECVID AVS 2016-2020) justify LAFF as a new baseline for text-to-video
retrieval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data-Free Neural Architecture Search via Recursive Label Calibration. (arXiv:2112.02086v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02086">
<div class="article-summary-box-inner">
<span><p>This paper aims to explore the feasibility of neural architecture search
(NAS) given only a pre-trained model without using any original training data.
This is an important circumstance for privacy protection, bias avoidance, etc.,
in real-world scenarios. To achieve this, we start by synthesizing usable data
through recovering the knowledge from a pre-trained deep neural network. Then
we use the synthesized data and their predicted soft-labels to guide neural
architecture search. We identify that the NAS task requires the synthesized
data (we target at image domain here) with enough semantics, diversity, and a
minimal domain gap from the natural images. For semantics, we propose recursive
label calibration to produce more informative outputs. For diversity, we
propose a regional update strategy to generate more diverse and
semantically-enriched synthetic data. For minimal domain gap, we use input and
feature-level regularization to mimic the original data distribution in latent
space. We instantiate our proposed framework with three popular NAS algorithms:
DARTS, ProxylessNAS and SPOS. Surprisingly, our results demonstrate that the
architectures discovered by searching with our synthetic data achieve accuracy
that is comparable to, or even higher than, architectures discovered by
searching from the original ones, for the first time, deriving the conclusion
that NAS can be done effectively with no need of access to the original or
called natural data if the synthesis method is well designed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Feature Interpolation for Low-Shot Image Generation. (arXiv:2112.02450v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02450">
<div class="article-summary-box-inner">
<span><p>Training of generative models especially Generative Adversarial Networks can
easily diverge in low-data setting. To mitigate this issue, we propose a novel
implicit data augmentation approach which facilitates stable training and
synthesize high-quality samples without need of label information.
Specifically, we view the discriminator as a metric embedding of the real data
manifold, which offers proper distances between real data points. We then
utilize information in the feature space to develop a fully unsupervised and
data-driven augmentation method. Experiments on few-shot generation tasks show
the proposed method significantly improve results from strong baselines with
hundreds of training samples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pose2Room: Understanding 3D Scenes from Human Activities. (arXiv:2112.03030v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03030">
<div class="article-summary-box-inner">
<span><p>With wearable IMU sensors, one can estimate human poses from wearable devices
without requiring visual input~\cite{von2017sparse}. In this work, we pose the
question: Can we reason about object structure in real-world environments
solely from human trajectory information? Crucially, we observe that human
motion and interactions tend to give strong information about the objects in a
scene -- for instance a person sitting indicates the likely presence of a chair
or sofa. To this end, we propose P2R-Net to learn a probabilistic 3D model of
the objects in a scene characterized by their class categories and oriented 3D
bounding boxes, based on an input observed human trajectory in the environment.
P2R-Net models the probability distribution of object class as well as a deep
Gaussian mixture model for object boxes, enabling sampling of multiple,
diverse, likely modes of object configurations from an observed human
trajectory. In our experiments we show that P2R-Net can effectively learn
multi-modal distributions of likely objects for human motions, and produce a
variety of plausible object structures of the environment, even without any
visual information. The results demonstrate that P2R-Net consistently
outperforms the baselines on the PROX dataset and the VirtualHome platform.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep learning for brain metastasis detection and segmentation in longitudinal MRI data. (arXiv:2112.11833v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11833">
<div class="article-summary-box-inner">
<span><p>Brain metastases occur frequently in patients with metastatic cancer. Early
and accurate detection of brain metastases is very essential for treatment
planning and prognosis in radiation therapy. To improve brain metastasis
detection performance with deep learning, a custom detection loss called
volume-level sensitivity-specificity (VSS) is proposed, which rates individual
metastasis detection sensitivity and specificity in (sub-)volume levels. As
sensitivity and precision are always a trade-off in a metastasis level, either
a high sensitivity or a high precision can be achieved by adjusting the weights
in the VSS loss without decline in dice score coefficient for segmented
metastases. To reduce metastasis-like structures being detected as false
positive metastases, a temporal prior volume is proposed as an additional input
of DeepMedic. The modified network is called DeepMedic+ for distinction. Our
proposed VSS loss improves the sensitivity of brain metastasis detection for
DeepMedic, increasing the sensitivity from 85.3% to 97.5%. Alternatively, it
improves the precision from 69.1% to 98.7%. Comparing DeepMedic+ with DeepMedic
with the same VSS loss, 44.4% of the false positive metastases are reduced in
the high sensitivity model and the precision reaches 99.6% for the high
specificity model. The mean dice coefficient for all metastases is about 0.81.
With the ensemble of the high sensitivity and high specificity models, on
average only 1.5 false positive metastases per patient needs further check,
while the majority of true positive metastases are confirmed. The ensemble
learning is able to distinguish high confidence true positive metastases from
metastases candidates that require special expert review or further follow-up,
being particularly well-fit to the requirements of expert support in real
clinical practice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving the Behaviour of Vision Transformers with Token-consistent Stochastic Layers. (arXiv:2112.15111v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15111">
<div class="article-summary-box-inner">
<span><p>We introduce token-consistent stochastic layers in vision transformers,
without causing any severe drop in performance. The added stochasticity
improves network calibration, robustness and strengthens privacy. We use linear
layers with token-consistent stochastic parameters inside the multilayer
perceptron blocks, without altering the architecture of the transformer. The
stochastic parameters are sampled from the uniform distribution, both during
training and inference. The applied linear operations preserve the topological
structure, formed by the set of tokens passing through the shared multilayer
perceptron. This operation encourages the learning of the recognition task to
rely on the topological structures of the tokens, instead of their values,
which in turn offers the desired robustness and privacy of the visual features.
The effectiveness of the token-consistent stochasticity is demonstrated on
three different applications, namely, network calibration, adversarial
robustness, and feature privacy, by boosting the performance of the respective
established baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Iterative training of robust k-space interpolation networks for improved image reconstruction with limited scan specific training samples. (arXiv:2201.03560v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03560">
<div class="article-summary-box-inner">
<span><p>Purpose: To evaluate an iterative learning approach for enhanced performance
of Robust Artificial-neural-networks for K-space Interpolation (RAKI), when
only a limited amount of training data (auto-calibration signals, ACS) are
available for accelerated standard 2D imaging. Methods: In a first step, the
RAKI model was optimized for the case of strongly limited training data amount.
In the iterative learning approach (termed iterative RAKI), the optimized RAKI
model is initially trained using original and augmented ACS obtained from a
linear parallel imaging reconstruction. Subsequently, the RAKI convolution
filters are refined iteratively using original and augmented ACS extracted from
the previous RAKI reconstruction. Evaluation was carried out on 200
retrospectively undersampled in-vivo datasets from the fastMRI neuro database
with different contrast settings. Results: For limited training data (18 and 22
ACS lines for R=4 and R=5, respectively), iterative RAKI outperforms standard
RAKI by reducing residual artefacts and yields strong noise suppression when
compared to standard parallel imaging, underlined by quantitative
reconstruction quality metrics. In combination with a phase constraint, further
reconstruction improvements can be achieved. Additionally, iterative RAKI shows
better performance than both GRAPPA and RAKI in case of pre-scan calibration
with varying contrast between training- and undersampled data. Conclusion: The
iterative learning approach with RAKI benefits from standard RAKIs well known
noise suppression feature but requires less original training data for the
accurate reconstruction of standard 2D images thereby improving net
acceleration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning. (arXiv:2201.04182v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04182">
<div class="article-summary-box-inner">
<span><p>In this work we propose a HyperTransformer, a Transformer-based model for
supervised and semi-supervised few-shot learning that generates weights of a
convolutional neural network (CNN) directly from support samples. Since the
dependence of a small generated CNN model on a specific task is encoded by a
high-capacity Transformer model, we effectively decouple the complexity of the
large task space from the complexity of individual tasks. Our method is
particularly effective for small target CNN architectures where learning a
fixed universal task-independent embedding is not optimal and better
performance is attained when the information about the task can modulate all
model parameters. For larger models we discover that generating the last layer
alone allows us to produce competitive or better results than those obtained
with state-of-the-art methods while being end-to-end differentiable.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Pretraining for Echocardiography Segmentation with Limited Data. (arXiv:2201.07219v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07219">
<div class="article-summary-box-inner">
<span><p>Contrastive learning has proven useful in many applications where access to
labelled data is limited. The lack of annotated data is particularly
problematic in medical image segmentation as it is difficult to have clinical
experts manually annotate large volumes of data such as cardiac structures in
ultrasound images of the heart. In this paper, We propose a self supervised
contrastive learning method to segment the left ventricle from echocardiography
when limited annotated images exist. Furthermore, we study the effect of
contrastive pretraining on two well-known segmentation networks, UNet and
DeepLabV3. Our results show that contrastive pretraining helps improve the
performance on left ventricle segmentation, particularly when annotated data is
scarce. We show how to achieve comparable results to state-of-the-art fully
supervised algorithms when we train our models in a self-supervised fashion
followed by fine-tuning on just 5\% of the data. We show that our solution
outperforms what is currently published on a large public dataset
(EchoNet-Dynamic) achieving a Dice score of 0.9252. We also compare the
performance of our solution on another smaller dataset (CAMUS) to demonstrate
the generalizability of our proposed solution. The code is available at
(https://github.com/BioMedIA-MBZUAI/contrastive-echo).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing variational generation through self-decomposition. (arXiv:2202.02738v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02738">
<div class="article-summary-box-inner">
<span><p>In this article we introduce the notion of Split Variational Autoencoder
(SVAE), whose output $\hat{x}$ is obtained as a weighted sum $\sigma \odot
\hat{x_1} + (1-\sigma) \odot \hat{x_2}$ of two generated images
$\hat{x_1},\hat{x_2}$, and $\sigma$ is a {\em learned} compositional map. The
composing images $\hat{x_1},\hat{x_2}$, as well as the $\sigma$-map are
automatically synthesized by the model. The network is trained as a usual
Variational Autoencoder with a negative loglikelihood loss between training and
reconstructed images. No additional loss is required for $\hat{x_1},\hat{x_2}$
or $\sigma$, neither any form of human tuning. The decomposition is
nondeterministic, but follows two main schemes, that we may roughly categorize
as either \say{syntactic} or \say{semantic}. In the first case, the map tends
to exploit the strong correlation between adjacent pixels, splitting the image
in two complementary high frequency sub-images. In the second case, the map
typically focuses on the contours of objects, splitting the image in
interesting variations of its content, with more marked and distinctive
features. In this case, according to empirical observations, the Fr\'echet
Inception Distance (FID) of $\hat{x_1}$ and $\hat{x_2}$ is usually lower (hence
better) than that of $\hat{x}$, that clearly suffers from being the average of
the former. In a sense, a SVAE forces the Variational Autoencoder to make
choices, in contrast with its intrinsic tendency to {\em average} between
alternatives with the aim to minimize the reconstruction loss towards a
specific sample. According to the FID metric, our technique, tested on typical
datasets such as Mnist, Cifar10 and CelebA, allows us to outperform all
previous purely variational architectures (not relying on normalization flows).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Machines Help Us Answering Question 16 in Datasheets, and In Turn Reflecting on Inappropriate Content?. (arXiv:2202.06675v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06675">
<div class="article-summary-box-inner">
<span><p>Large datasets underlying much of current machine learning raise serious
issues concerning inappropriate content such as offensive, insulting,
threatening, or might otherwise cause anxiety. This calls for increased dataset
documentation, e.g., using datasheets. They, among other topics, encourage to
reflect on the composition of the datasets. So far, this documentation,
however, is done manually and therefore can be tedious and error-prone,
especially for large image datasets. Here we ask the arguably "circular"
question of whether a machine can help us reflect on inappropriate content,
answering Question 16 in Datasheets. To this end, we propose to use the
information stored in pre-trained transformer models to assist us in the
documentation process. Specifically, prompt-tuning based on a dataset of
socio-moral values steers CLIP to identify potentially inappropriate content,
therefore reducing human labor. We then document the inappropriate images found
using word clouds, based on captions generated using a vision-language model.
The documentations of two popular, large-scale computer vision datasets --
ImageNet and OpenImages -- produced this way suggest that machines can indeed
help dataset creators to answer Question 16 on inappropriate image content.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CAR: Class-aware Regularizations for Semantic Segmentation. (arXiv:2203.07160v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07160">
<div class="article-summary-box-inner">
<span><p>Recent segmentation methods, such as OCR and CPNet, utilizing "class level"
information in addition to pixel features, have achieved notable success for
boosting the accuracy of existing network modules. However, the extracted
class-level information was simply concatenated to pixel features, without
explicitly being exploited for better pixel representation learning. Moreover,
these approaches learn soft class centers based on coarse mask prediction,
which is prone to error accumulation. In this paper, aiming to use class level
information more effectively, we propose a universal Class-Aware Regularization
(CAR) approach to optimize the intra-class variance and inter-class distance
during feature learning, motivated by the fact that humans can recognize an
object by itself no matter which other objects it appears with. Three novel
loss functions are proposed. The first loss function encourages more compact
class representations within each class, the second directly maximizes the
distance between different class centers, and the third further pushes the
distance between inter-class centers and pixels. Furthermore, the class center
in our approach is directly generated from ground truth instead of from the
error-prone coarse prediction. Our method can be easily applied to most
existing segmentation models during training, including OCR and CPNet, and can
largely improve their accuracy at no additional inference overhead. Extensive
experiments and ablation studies conducted on multiple benchmark datasets
demonstrate that the proposed CAR can boost the accuracy of all baseline models
by up to 2.23% mIOU with superior generalization ability. The complete code is
available at https://github.com/edwardyehuang/CAR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Animatable Implicit Neural Representations for Creating Realistic Avatars from Videos. (arXiv:2203.08133v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08133">
<div class="article-summary-box-inner">
<span><p>This paper addresses the challenge of reconstructing an animatable human
model from a multi-view video. Some recent works have proposed to decompose a
non-rigidly deforming scene into a canonical neural radiance field and a set of
deformation fields that map observation-space points to the canonical space,
thereby enabling them to learn the dynamic scene from images. However, they
represent the deformation field as translational vector field or SE(3) field,
which makes the optimization highly under-constrained. Moreover, these
representations cannot be explicitly controlled by input motions. Instead, we
introduce a pose-driven deformation field based on the linear blend skinning
algorithm, which combines the blend weight field and the 3D human skeleton to
produce observation-to-canonical correspondences. Since 3D human skeletons are
more observable, they can regularize the learning of the deformation field.
Moreover, the pose-driven deformation field can be controlled by input skeletal
motions to generate new deformation fields to animate the canonical human
model. Experiments show that our approach significantly outperforms recent
human modeling methods. The code is available at
https://zju3dv.github.io/animatable_nerf/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Adaptive Hand Keypoint and Pixel Localization in the Wild. (arXiv:2203.08344v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08344">
<div class="article-summary-box-inner">
<span><p>We aim to improve the performance of regressing hand keypoints and segmenting
pixel-level hand masks under new imaging conditions (e.g., outdoors) when we
only have labeled images taken under very different conditions (e.g., indoors).
In the real world, it is important that the model trained for both tasks works
under various imaging conditions. However, their variation covered by existing
labeled hand datasets is limited. Thus, it is necessary to adapt the model
trained on the labeled images (source) to unlabeled images (target) with unseen
imaging conditions. While self-training domain adaptation methods (i.e.,
learning from the unlabeled target images in a self-supervised manner) have
been developed for both tasks, their training may degrade performance when the
predictions on the target images are noisy. To avoid this, it is crucial to
assign a low importance (confidence) weight to the noisy predictions during
self-training. In this paper, we propose to utilize the divergence of two
predictions to estimate the confidence of the target image for both tasks.
These predictions are given from two separate networks, and their divergence
helps identify the noisy predictions. To integrate our proposed confidence
estimation into self-training, we propose a teacher-student framework where the
two networks (teachers) provide supervision to a network (student) for
self-training, and the teachers are learned from the student by knowledge
distillation. Our experiments show its superiority over state-of-the-art
methods in adaptation settings with different lighting, grasping objects,
backgrounds, and camera viewpoints. Our method improves by 4% the multi-task
score on HO3D compared to the latest adversarial adaptation method. We also
validate our method on Ego4D, egocentric videos with rapid changes in imaging
conditions outdoors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Video Text Spotting with Transformer. (arXiv:2203.10539v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10539">
<div class="article-summary-box-inner">
<span><p>Recent video text spotting methods usually require the three-staged pipeline,
i.e., detecting text in individual images, recognizing localized text, tracking
text streams with post-processing to generate final results. These methods
typically follow the tracking-by-match paradigm and develop sophisticated
pipelines. In this paper, rooted in Transformer sequence modeling, we propose a
simple, but effective end-to-end video text DEtection, Tracking, and
Recognition framework (TransDETR). TransDETR mainly includes two advantages: 1)
Different from the explicit match paradigm in the adjacent frame, TransDETR
tracks and recognizes each text implicitly by the different query termed text
query over long-range temporal sequence (more than 7 frames). 2) TransDETR is
the first end-to-end trainable video text spotting framework, which
simultaneously addresses the three sub-tasks (e.g., text detection, tracking,
recognition). Extensive experiments in four video text datasets (i.e.,ICDAR2013
Video, ICDAR2015 Video, Minetto, and YouTube Video Text) are conducted to
demonstrate that TransDETR achieves state-of-the-art performance with up to
around 8.0% improvements on video text spotting tasks. The code of TransDETR
can be found at https://github.com/weijiawu/TransDETR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatially Multi-conditional Image Generation. (arXiv:2203.13812v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13812">
<div class="article-summary-box-inner">
<span><p>In most scenarios, conditional image generation can be thought of as an
inversion of the image understanding process. Since generic image understanding
involves solving multiple tasks, it is natural to aim at generating images via
multi-conditioning. However, multi-conditional image generation is a very
challenging problem due to the heterogeneity and the sparsity of the (in
practice) available conditioning labels. In this work, we propose a novel
neural architecture to address the problem of heterogeneity and sparsity of the
spatially multi-conditional labels. Our choice of spatial conditioning, such as
by semantics and depth, is driven by the promise it holds for better control of
the image generation process. The proposed method uses a transformer-like
architecture operating pixel-wise, which receives the available labels as input
tokens to merge them in a learned homogeneous space of labels. The merged
labels are then used for image generation via conditional generative
adversarial training. In this process, the sparsity of the labels is handled by
simply dropping the input tokens corresponding to the missing labels at the
desired locations, thanks to the proposed pixel-wise operating architecture.
Our experiments on three benchmark datasets demonstrate the clear superiority
of our method over the state-of-the-art and compared baselines. The source code
will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature robustness and sex differences in medical imaging: a case study in MRI-based Alzheimer's disease detection. (arXiv:2204.01737v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01737">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks have enabled significant improvements in
medical image-based diagnosis. It is, however, increasingly clear that these
models are susceptible to performance degradation when facing spurious
correlations and dataset shift, leading, e.g., to underperformance on
underrepresented patient groups. In this paper, we compare two classification
schemes on the ADNI MRI dataset: a simple logistic regression model using
manually selected volumetric features, and a convolutional neural network
trained on 3D MRI data. We assess the robustness of the trained models in the
face of varying dataset splits, training set sex composition, and stage of
disease. In contrast to earlier work in other imaging modalities, we do not
observe a clear pattern of improved model performance for the majority group in
the training dataset. Instead, while logistic regression is fully robust to
dataset composition, we find that CNN performance is generally improved for
both male and female subjects when including more female subjects in the
training dataset. We hypothesize that this might be due to inherent differences
in the pathology of the two sexes. Moreover, in our analysis, the logistic
regression model outperforms the 3D CNN, emphasizing the utility of manual
feature specification based on prior knowledge, and the need for more robust
automatic feature selection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Unlearning via Randomized Conditionally Independent Hessians. (arXiv:2204.07655v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07655">
<div class="article-summary-box-inner">
<span><p>Recent legislation has led to interest in machine unlearning, i.e., removing
specific training samples from a predictive model as if they never existed in
the training dataset. Unlearning may also be required due to
corrupted/adversarial data or simply a user's updated privacy requirement. For
models which require no training (k-NN), simply deleting the closest original
sample can be effective. But this idea is inapplicable to models which learn
richer representations. Recent ideas leveraging optimization-based updates
scale poorly with the model dimension d, due to inverting the Hessian of the
loss function. We use a variant of a new conditional independence coefficient,
L-CODEC, to identify a subset of the model parameters with the most semantic
overlap on an individual sample level. Our approach completely avoids the need
to invert a (possibly) huge matrix. By utilizing a Markov blanket selection, we
premise that L-CODEC is also suitable for deep unlearning, as well as other
applications in vision. Compared to alternatives, L-CODEC makes approximate
unlearning possible in settings that would otherwise be infeasible, including
vision models used for face recognition, person re-identification and NLP
models that may require unlearning samples identified for exclusion. Code can
be found at https://github.com/vsingh-group/LCODEC-deep-unlearning/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MMRotate: A Rotated Object Detection Benchmark using Pytorch. (arXiv:2204.13317v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13317">
<div class="article-summary-box-inner">
<span><p>We present an open-source toolbox, named MMRotate, which provides a coherent
algorithm framework of training, inferring, and evaluation for the popular
rotated object detection algorithm based on deep learning. MMRotate implements
18 state-of-the-art algorithms and supports the three most frequently used
angle definition methods. To facilitate future research and industrial
applications of rotated object detection-related problems, we also provide a
large number of trained models and detailed benchmarks to give insights into
the performance of rotated object detection. MMRotate is publicly released at
https://github.com/open-mmlab/mmrotate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Privacy Preserving Image Registration. (arXiv:2205.10120v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10120">
<div class="article-summary-box-inner">
<span><p>Image registration is a key task in medical imaging applications, allowing to
represent medical images in a common spatial reference frame. Current
literature on image registration is generally based on the assumption that
images are usually accessible to the researcher, from which the spatial
transformation is subsequently estimated. This common assumption may not be met
in current practical applications, since the sensitive nature of medical images
may ultimately require their analysis under privacy constraints, preventing to
share the image content in clear form. In this work, we formulate the problem
of image registration under a privacy preserving regime, where images are
assumed to be confidential and cannot be disclosed in clear. We derive our
privacy preserving image registration framework by extending classical
registration paradigms to account for advanced cryptographic tools, such as
secure multi-party computation and homomorphic encryption, that enable the
execution of operations without leaking the underlying data. To overcome the
problem of performance and scalability of cryptographic tools in high
dimensions, we first propose to optimize the underlying image registration
operations using gradient approximations. We further revisit the use of
homomorphic encryption and use a packing method to allow the encryption and
multiplication of large matrices more efficiently. We demonstrate our privacy
preserving framework in linear and non-linear registration problems, evaluating
its accuracy and scalability with respect to standard image registration. Our
results show that privacy preserving image registration is feasible and can be
adopted in sensitive medical imaging applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Superclass Adversarial Attack. (arXiv:2205.14629v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14629">
<div class="article-summary-box-inner">
<span><p>Adversarial attacks have only focused on changing the predictions of the
classifier, but their danger greatly depends on how the class is mistaken. For
example, when an automatic driving system mistakes a Persian cat for a Siamese
cat, it is hardly a problem. However, if it mistakes a cat for a 120km/h
minimum speed sign, serious problems can arise. As a stepping stone to more
threatening adversarial attacks, we consider the superclass adversarial attack,
which causes misclassification of not only fine classes, but also superclasses.
We conducted the first comprehensive analysis of superclass adversarial attacks
(an existing and 19 new methods) in terms of accuracy, speed, and stability,
and identified several strategies to achieve better performance. Although this
study is aimed at superclass misclassification, the findings can be applied to
other problem settings involving multiple classes, such as top-k and
multi-label classification attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Strongly Augmented Contrastive Clustering. (arXiv:2206.00380v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00380">
<div class="article-summary-box-inner">
<span><p>Deep clustering has attracted increasing attention in recent years due to its
capability of joint representation learning and clustering via deep neural
networks. In its latest developments, the contrastive learning has emerged as
an effective technique to substantially enhance the deep clustering
performance. However, the existing contrastive learning based deep clustering
algorithms mostly focus on some carefully-designed augmentations (often with
limited transformations to preserve the structure), referred to as weak
augmentations, but cannot go beyond the weak augmentations to explore the more
opportunities in stronger augmentations (with more aggressive transformations
or even severe distortions). In this paper, we present an end-to-end deep
clustering approach termed Strongly Augmented Contrastive Clustering (SACC),
which extends the conventional two-augmentation-view paradigm to multiple views
and jointly leverages strong and weak augmentations for strengthened deep
clustering. Particularly, we utilize a backbone network with triply-shared
weights, where a strongly augmented view and two weakly augmented views are
incorporated. Based on the representations produced by the backbone, the
weak-weak view pair and the strong-weak view pairs are simultaneously exploited
for the instance-level contrastive learning (via an instance projector) and the
cluster-level contrastive learning (via a cluster projector), which, together
with the backbone, can be jointly optimized in a purely unsupervised manner.
Experimental results on five challenging image datasets have shown the
superiority of our SACC approach over the state-of-the-art. The code is
available at https://github.com/dengxiaozhi/SACC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Modal Transformer GAN: A Brain Structure-Function Deep Fusing Framework for Alzheimer's Disease. (arXiv:2206.13393v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.13393">
<div class="article-summary-box-inner">
<span><p>Cross-modal fusion of different types of neuroimaging data has shown great
promise for predicting the progression of Alzheimer's Disease(AD). However,
most existing methods applied in neuroimaging can not efficiently fuse the
functional and structural information from multi-modal neuroimages. In this
work, a novel cross-modal transformer generative adversarial network(CT-GAN) is
proposed to fuse functional information contained in resting-state functional
magnetic resonance imaging (rs-fMRI) and structural information contained in
Diffusion Tensor Imaging (DTI). The developed bi-attention mechanism can match
functional information to structural information efficiently and maximize the
capability of extracting complementary information from rs-fMRI and DTI. By
capturing the deep complementary information between structural features and
functional features, the proposed CT-GAN can detect the AD-related brain
connectivity, which could be used as a bio-marker of AD. Experimental results
show that the proposed model can not only improve classification performance
but also detect the AD-related brain connectivity effectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On-Device Training Under 256KB Memory. (arXiv:2206.15472v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.15472">
<div class="article-summary-box-inner">
<span><p>On-device training enables the model to adapt to new data collected from the
sensors by fine-tuning a pre-trained model. However, the training memory
consumption is prohibitive for IoT devices that have tiny memory resources. We
propose an algorithm-system co-design framework to make on-device training
possible with only 256KB of memory. On-device training faces two unique
challenges: (1) the quantized graphs of neural networks are hard to optimize
due to mixed bit-precision and the lack of normalization; (2) the limited
hardware resource (memory and computation) does not allow full backward
computation. To cope with the optimization difficulty, we propose
Quantization-Aware Scaling to calibrate the gradient scales and stabilize
quantized training. To reduce the memory footprint, we propose Sparse Update to
skip the gradient computation of less important layers and sub-tensors. The
algorithm innovation is implemented by a lightweight training system, Tiny
Training Engine, which prunes the backward computation graph to support sparse
updates and offloads the runtime auto-differentiation to compile time. Our
framework is the first practical solution for on-device transfer learning of
visual recognition on tiny IoT devices (e.g., a microcontroller with only 256KB
SRAM), using less than 1/100 of the memory of existing frameworks while
matching the accuracy of cloud training+edge deployment for the tinyML
application VWW. Our study enables IoT devices to not only perform inference
but also continuously adapt to new data for on-device lifelong learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detection of ADHD based on Eye Movements during Natural Viewing. (arXiv:2207.01377v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01377">
<div class="article-summary-box-inner">
<span><p>Attention-deficit/hyperactivity disorder (ADHD) is a neurodevelopmental
disorder that is highly prevalent and requires clinical specialists to
diagnose. It is known that an individual's viewing behavior, reflected in their
eye movements, is directly related to attentional mechanisms and higher-order
cognitive processes. We therefore explore whether ADHD can be detected based on
recorded eye movements together with information about the video stimulus in a
free-viewing task. To this end, we develop an end-to-end deep learning-based
sequence model which we pre-train on a related task for which more data are
available. We find that the method is in fact able to detect ADHD and
outperforms relevant baselines. We investigate the relevance of the input
features in an ablation study. Interestingly, we find that the model's
performance is closely related to the content of the video, which provides
insights for future experimental designs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Patch-wise Deep Metric Learning for Unsupervised Low-Dose CT Denoising. (arXiv:2207.02377v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02377">
<div class="article-summary-box-inner">
<span><p>The acquisition conditions for low-dose and high-dose CT images are usually
different, so that the shifts in the CT numbers often occur. Accordingly,
unsupervised deep learning-based approaches, which learn the target image
distribution, often introduce CT number distortions and result in detrimental
effects in diagnostic performance. To address this, here we propose a novel
unsupervised learning approach for lowdose CT reconstruction using patch-wise
deep metric learning. The key idea is to learn embedding space by pulling the
positive pairs of image patches which shares the same anatomical structure, and
pushing the negative pairs which have same noise level each other. Thereby, the
network is trained to suppress the noise level, while retaining the original
global CT number distributions even after the image translation. Experimental
results confirm that our deep metric learning plays a critical role in
producing high quality denoised images without CT number shift.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Makes for Automatic Reconstruction of Pulmonary Segments. (arXiv:2207.03078v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03078">
<div class="article-summary-box-inner">
<span><p>3D reconstruction of pulmonary segments plays an important role in surgical
treatment planning of lung cancer, which facilitates preservation of pulmonary
function and helps ensure low recurrence rates. However, automatic
reconstruction of pulmonary segments remains unexplored in the era of deep
learning. In this paper, we investigate what makes for automatic reconstruction
of pulmonary segments. First and foremost, we formulate, clinically and
geometrically, the anatomical definitions of pulmonary segments, and propose
evaluation metrics adhering to these definitions. Second, we propose ImPulSe
(Implicit Pulmonary Segment), a deep implicit surface model designed for
pulmonary segment reconstruction. The automatic reconstruction of pulmonary
segments by ImPulSe is accurate in metrics and visually appealing. Compared
with canonical segmentation methods, ImPulSe outputs continuous predictions of
arbitrary resolutions with higher training efficiency and fewer parameters.
Lastly, we experiment with different network inputs to analyze what matters in
the task of pulmonary segment reconstruction. Our code is available at
https://github.com/M3DV/ImPulSe.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">D2HNet: Joint Denoising and Deblurring with Hierarchical Network for Robust Night Image Restoration. (arXiv:2207.03294v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03294">
<div class="article-summary-box-inner">
<span><p>Night imaging with modern smartphone cameras is troublesome due to low photon
count and unavoidable noise in the imaging system. Directly adjusting exposure
time and ISO ratings cannot obtain sharp and noise-free images at the same time
in low-light conditions. Though many methods have been proposed to enhance
noisy or blurry night images, their performances on real-world night photos are
still unsatisfactory due to two main reasons: 1) Limited information in a
single image and 2) Domain gap between synthetic training images and real-world
photos (e.g., differences in blur area and resolution). To exploit the
information from successive long- and short-exposure images, we propose a
learning-based pipeline to fuse them. A D2HNet framework is developed to
recover a high-quality image by deblurring and enhancing a long-exposure image
under the guidance of a short-exposure image. To shrink the domain gap, we
leverage a two-phase DeblurNet-EnhanceNet architecture, which performs accurate
blur removal on a fixed low resolution so that it is able to handle large
ranges of blur in different resolution inputs. In addition, we synthesize a
D2-Dataset from HD videos and experiment on it. The results on the validation
set and real photos demonstrate our methods achieve better visual quality and
state-of-the-art quantitative scores. The D2HNet codes and D2-Dataset can be
found at https://github.com/zhaoyuzhi/D2HNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A simple normalization technique using window statistics to improve the out-of-distribution generalization on medical images. (arXiv:2207.03366v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03366">
<div class="article-summary-box-inner">
<span><p>Since data scarcity and data heterogeneity are prevailing for medical images,
well-trained Convolutional Neural Networks (CNNs) using previous normalization
methods may perform poorly when deployed to a new site. However, a reliable
model for real-world clinical applications should be able to generalize well
both on in-distribution (IND) and out-of-distribution (OOD) data (e.g., the new
site data). In this study, we present a novel normalization technique called
window normalization (WIN) to improve the model generalization on heterogeneous
medical images, which is a simple yet effective alternative to existing
normalization methods. Specifically, WIN perturbs the normalizing statistics
with the local statistics computed on the window of features. This
feature-level augmentation technique regularizes the models well and improves
their OOD generalization significantly. Taking its advantage, we propose a
novel self-distillation method called WIN-WIN for classification tasks. WIN-WIN
is easily implemented with twice forward passes and a consistency constraint,
which can be a simple extension for existing methods. Extensive experimental
results on various tasks (6 tasks) and datasets (24 datasets) demonstrate the
generality and effectiveness of our methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Direct Handheld Burst Imaging to Simulated Defocus. (arXiv:2207.04175v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04175">
<div class="article-summary-box-inner">
<span><p>A shallow depth-of-field image keeps the subject in focus, and the foreground
and background contexts blurred. This effect requires much larger lens
apertures than those of smartphone cameras. Conventional methods acquire RGB-D
images and blur image regions based on their depth. However, this approach is
not suitable for reflective or transparent surfaces, or finely detailed object
silhouettes, where the depth value is inaccurate or ambiguous.
</p>
<p>We present a learning-based method to synthesize the defocus blur in shallow
depth-of-field images from handheld bursts acquired with a single small
aperture lens. Our deep learning model directly produces the shallow
depth-of-field image, avoiding explicit depth-based blurring. The simulated
aperture diameter equals the camera translation during burst acquisition. Our
method does not suffer from artifacts due to inaccurate or ambiguous depth
estimation, and it is well-suited to portrait photography.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Radiomics-Guided Global-Local Transformer for Weakly Supervised Pathology Localization in Chest X-Rays. (arXiv:2207.04394v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04394">
<div class="article-summary-box-inner">
<span><p>Before the recent success of deep learning methods for automated medical
image analysis, practitioners used handcrafted radiomic features to
quantitatively describe local patches of medical images. However, extracting
discriminative radiomic features relies on accurate pathology localization,
which is difficult to acquire in real-world settings. Despite advances in
disease classification and localization from chest X-rays, many approaches fail
to incorporate clinically-informed domain knowledge. For these reasons, we
propose a Radiomics-Guided Transformer (RGT) that fuses \textit{global} image
information with \textit{local} knowledge-guided radiomics information to
provide accurate cardiopulmonary pathology localization and classification
\textit{without any bounding box annotations}. RGT consists of an image
Transformer branch, a radiomics Transformer branch, and fusion layers that
aggregate image and radiomic information. Using the learned self-attention of
its image branch, RGT extracts a bounding box for which to compute radiomic
features, which are further processed by the radiomics branch; learned image
and radiomic features are then fused and mutually interact via cross-attention
layers. Thus, RGT utilizes a novel end-to-end feedback loop that can bootstrap
accurate pathology localization only using image-level disease labels.
Experiments on the NIH ChestXRay dataset demonstrate that RGT outperforms prior
works in weakly supervised disease localization (by an average margin of 3.6\%
over various intersection-over-union thresholds) and classification (by 1.1\%
in average area under the receiver operating characteristic curve). We publicly
release our codes and pre-trained models at
\url{https://github.com/VITA-Group/chext}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Waste Copper Granules Rating System Based on Machine Vision. (arXiv:2207.04575v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04575">
<div class="article-summary-box-inner">
<span><p>In the field of waste copper granules recycling, engineers should be able to
identify all different sorts of impurities in waste copper granules and
estimate their mass proportion relying on experience before rating. This manual
rating method is costly, lacking in objectivity and comprehensiveness. To
tackle this problem, we propose a waste copper granules rating system based on
machine vision and deep learning. We firstly formulate the rating task into a
2D image recognition and purity regression task. Then we design a two-stage
convolutional rating network to compute the mass purity and rating level of
waste copper granules. Our rating network includes a segmentation network and a
purity regression network, which respectively calculate the semantic
segmentation heatmaps and purity results of the waste copper granules. After
training the rating network on the augmented datasets, experiments on real
waste copper granules demonstrate the effectiveness and superiority of the
proposed network. Specifically, our system is superior to the manual method in
terms of accuracy, effectiveness, robustness, and objectivity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Susceptibility of Continual Learning Against Adversarial Attacks. (arXiv:2207.05225v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05225">
<div class="article-summary-box-inner">
<span><p>The recent advances in continual (incremental or lifelong) learning have
concentrated on the prevention of forgetting that can lead to catastrophic
consequences, but there are two outstanding challenges that must be addressed.
The first is the evaluation of the robustness of the proposed methods. The
second is ensuring the security of learned tasks remains largely unexplored.
This paper presents a comprehensive study of the susceptibility of the
continually learned tasks (including both current and previously learned tasks)
that are vulnerable to forgetting. Such vulnerability of tasks against
adversarial attacks raises profound issues in data integrity and privacy. We
consider all three scenarios (i.e, task-incremental leaning, domain-incremental
learning and class-incremental learning) of continual learning and explore
three regularization-based experiments, three replay-based experiments, and one
hybrid technique based on the reply and exemplar approach. We examine the
robustness of these methods. In particular, we consider cases where we
demonstrate that any class belonging to the current or previously learned tasks
is prone to misclassification. Our observations, we identify potential
limitations in continual learning approaches against adversarial attacks. Our
empirical study recommends that the research community consider the robustness
of the proposed continual learning approaches and invest extensive efforts in
mitigating catastrophic forgetting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Fairness of Visual Attribute Predictors. (arXiv:2207.05727v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05727">
<div class="article-summary-box-inner">
<span><p>The performance of deep neural networks for image recognition tasks such as
predicting a smiling face is known to degrade with under-represented classes of
sensitive attributes. We address this problem by introducing fairness-aware
regularization losses based on batch estimates of Demographic Parity, Equalized
Odds, and a novel Intersection-over-Union measure. The experiments performed on
facial and medical images from CelebA, UTKFace, and the SIIM-ISIC melanoma
classification challenge show the effectiveness of our proposed fairness losses
for bias mitigation as they improve model fairness while maintaining high
classification performance. To the best of our knowledge, our work is the first
attempt to incorporate these types of losses in an end-to-end training scheme
for mitigating biases of visual attribute predictors. Our code is available at
https://github.com/nish03/FVAP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Collaborative Quantization Embeddings for Intra-Subject Prostate MR Image Registration. (arXiv:2207.06189v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06189">
<div class="article-summary-box-inner">
<span><p>Image registration is useful for quantifying morphological changes in
longitudinal MR images from prostate cancer patients. This paper describes a
development in improving the learning-based registration algorithms, for this
challenging clinical application often with highly variable yet limited
training data. First, we report that the latent space can be clustered into a
much lower dimensional space than that commonly found as bottleneck features at
the deep layer of a trained registration network. Based on this observation, we
propose a hierarchical quantization method, discretizing the learned feature
vectors using a jointly-trained dictionary with a constrained size, in order to
improve the generalisation of the registration networks. Furthermore, a novel
collaborative dictionary is independently optimised to incorporate additional
prior information, such as the segmentation of the gland or other regions of
interest, in the latent quantized space. Based on 216 real clinical images from
86 prostate cancer patients, we show the efficacy of both the designed
components. Improved registration accuracy was obtained with statistical
significance, in terms of both Dice on gland and target registration error on
corresponding landmarks, the latter of which achieved 5.46 mm, an improvement
of 28.7\% from the baseline without quantization. Experimental results also
show that the difference in performance was indeed minimised between training
and testing data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarially-Aware Robust Object Detector. (arXiv:2207.06202v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06202">
<div class="article-summary-box-inner">
<span><p>Object detection, as a fundamental computer vision task, has achieved a
remarkable progress with the emergence of deep neural networks. Nevertheless,
few works explore the adversarial robustness of object detectors to resist
adversarial attacks for practical applications in various real-world scenarios.
Detectors have been greatly challenged by unnoticeable perturbation, with sharp
performance drop on clean images and extremely poor performance on adversarial
images. In this work, we empirically explore the model training for adversarial
robustness in object detection, which greatly attributes to the conflict
between learning clean images and adversarial images. To mitigate this issue,
we propose a Robust Detector (RobustDet) based on adversarially-aware
convolution to disentangle gradients for model learning on clean and
adversarial images. RobustDet also employs the Adversarial Image Discriminator
(AID) and Consistent Features with Reconstruction (CFR) to ensure a reliable
robustness. Extensive experiments on PASCAL VOC and MS-COCO demonstrate that
our model effectively disentangles gradients and significantly enhances the
detection robustness with maintaining the detection ability on clean images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Entry-Flipped Transformer for Inference and Prediction of Participant Behavior. (arXiv:2207.06235v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06235">
<div class="article-summary-box-inner">
<span><p>Some group activities, such as team sports and choreographed dances, involve
closely coupled interaction between participants. Here we investigate the tasks
of inferring and predicting participant behavior, in terms of motion paths and
actions, under such conditions. We narrow the problem to that of estimating how
a set target participants react to the behavior of other observed participants.
Our key idea is to model the spatio-temporal relations among participants in a
manner that is robust to error accumulation during frame-wise inference and
prediction. We propose a novel Entry-Flipped Transformer (EF-Transformer),
which models the relations of participants by attention mechanisms on both
spatial and temporal domains. Unlike typical transformers, we tackle the
problem of error accumulation by flipping the order of query, key, and value
entries, to increase the importance and fidelity of observed features in the
current frame. Comparative experiments show that our EF-Transformer achieves
the best performance on a newly-collected tennis doubles dataset, a Ceilidh
dance dataset, and two pedestrian datasets. Furthermore, it is also
demonstrated that our EF-Transformer is better at limiting accumulated errors
and recovering from wrong estimations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Organic Priors in Non-Rigid Structure from Motion. (arXiv:2207.06262v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06262">
<div class="article-summary-box-inner">
<span><p>This paper advocates the use of organic priors in classical non-rigid
structure from motion (NRSfM). By organic priors, we mean invaluable
intermediate prior information intrinsic to the NRSfM matrix factorization
theory. It is shown that such priors reside in the factorized matrices, and
quite surprisingly, existing methods generally disregard them. The paper's main
contribution is to put forward a simple, methodical, and practical method that
can effectively exploit such organic priors to solve NRSfM. The proposed method
does not make assumptions other than the popular one on the low-rank shape and
offers a reliable solution to NRSfM under orthographic projection. Our work
reveals that the accessibility of organic priors is independent of the camera
motion and shape deformation type. Besides that, the paper provides insights
into the NRSfM factorization -- both in terms of shape and motion -- and is the
first approach to show the benefit of single rotation averaging for NRSfM.
Furthermore, we outline how to effectively recover motion and non-rigid 3D
shape using the proposed organic prior based approach and demonstrate results
that outperform prior-free NRSfM performance by a significant margin. Finally,
we present the benefits of our method via extensive experiments and evaluations
on several benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Adaptive Unknown Authentication for Universal Domain Adaptation by Classifier Paradox. (arXiv:2207.04494v1 [cs.CV] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04494">
<div class="article-summary-box-inner">
<span><p>Universal domain adaptation (UniDA) is a general unsupervised domain
adaptation setting, which addresses both domain and label shifts in adaptation.
Its main challenge lies in how to identify target samples in unshared or
unknown classes. Previous methods commonly strive to depict sample "confidence"
along with a threshold for rejecting unknowns, and align feature distributions
of shared classes across domains. However, it is still hard to pre-specify a
"confidence" criterion and threshold which are adaptive to various real tasks,
and a mis-prediction of unknowns further incurs misalignment of features in
shared classes. In this paper, we propose a new UniDA method with adaptive
Unknown Authentication by Classifier Paradox (UACP), considering that samples
with paradoxical predictions are probably unknowns belonging to none of the
source classes. In UACP, a composite classifier is jointly designed with two
types of predictors. That is, a multi-class (MC) predictor classifies samples
to one of the multiple source classes, while a binary one-vs-all (OVA)
predictor further verifies the prediction by MC predictor. Samples with
verification failure or paradox are identified as unknowns. Further, instead of
feature alignment for shared classes, implicit domain alignment is conducted in
output space such that samples across domains share the same decision boundary,
though with feature discrepancy. Empirical results validate UACP under both
open-set and universal UDA settings.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-07-16 23:07:27.407888654 UTC">2022-07-16 23:07:27 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>