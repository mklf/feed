<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-03-25T01:30:00Z">03-25</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Linearizing Transformer with Key-Value Memory Bank. (arXiv:2203.12644v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12644">
<div class="article-summary-box-inner">
<span><p>Transformer has brought great success to a wide range of natural language
processing tasks. Nevertheless, the computational overhead of the vanilla
transformer scales quadratically with sequence length. Many efforts have been
made to develop more efficient transformer variants. A line of work (e.g.,
Linformer) projects the input sequence into a low-rank space, achieving linear
time complexity. However, Linformer does not suit well for text generation
tasks as the sequence length must be pre-specified. We propose MemSizer, an
approach also projects the source sequence into lower dimension representation
but can take input with dynamic length, with a different perspective of the
attention mechanism. MemSizer not only achieves the same linear time complexity
but also enjoys efficient recurrent-style autoregressive generation, which
yields constant memory complexity and reduced computation at inference. We
demonstrate that MemSizer provides an improved tradeoff between efficiency and
accuracy over the vanilla transformer and other linear variants in language
modeling and machine translation tasks, revealing a viable direction towards
further inference efficiency improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions. (arXiv:2203.12667v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12667">
<div class="article-summary-box-inner">
<span><p>A long-term goal of AI research is to build intelligent agents that can
communicate with humans in natural language, perceive the environment, and
perform real-world tasks. Vision-and-Language Navigation (VLN) is a fundamental
and interdisciplinary research topic towards this goal, and receives increasing
attention from natural language processing, computer vision, robotics, and
machine learning communities. In this paper, we review contemporary studies in
the emerging field of VLN, covering tasks, evaluation metrics, methods, etc.
Through structured analysis of current progress and challenges, we highlight
the limitations of current VLN and opportunities for future work. This paper
serves as a thorough reference for the VLN research community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pseudo Label Is Better Than Human Label. (arXiv:2203.12668v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12668">
<div class="article-summary-box-inner">
<span><p>State-of-the-art automatic speech recognition (ASR) systems are trained with
tens of thousands of hours of labeled speech data. Human transcription is
expensive and time consuming. Factors such as the quality and consistency of
the transcription can greatly affect the performance of the ASR models trained
with these data. In this paper, we show that we can train a strong teacher
model to produce high quality pseudo labels by utilizing recent self-supervised
and semi-supervised learning techniques. Specifically, we use JUST (Joint
Unsupervised/Supervised Training) and iterative noisy student teacher training
to train a 600 million parameter bi-directional teacher model. This model
achieved 4.0% word error rate (WER) on a voice search task, 11.1% relatively
better than a baseline. We further show that by using this strong teacher model
to generate high-quality pseudo labels for training, we can achieve 13.6%
relative WER reduction (5.9% to 5.1%) for a streaming model compared to using
human labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Training for Improving Model Robustness? Look at Both Prediction and Interpretation. (arXiv:2203.12709v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12709">
<div class="article-summary-box-inner">
<span><p>Neural language models show vulnerability to adversarial examples which are
semantically similar to their original counterparts with a few words replaced
by their synonyms. A common way to improve model robustness is adversarial
training which follows two steps-collecting adversarial examples by attacking a
target model, and fine-tuning the model on the augmented dataset with these
adversarial examples. The objective of traditional adversarial training is to
make a model produce the same correct predictions on an original/adversarial
example pair. However, the consistency between model decision-makings on two
similar texts is ignored. We argue that a robust model should behave
consistently on original/adversarial example pairs, that is making the same
predictions (what) based on the same reasons (how) which can be reflected by
consistent interpretations. In this work, we propose a novel feature-level
adversarial training method named FLAT. FLAT aims at improving model robustness
in terms of both predictions and interpretations. FLAT incorporates variational
word masks in neural networks to learn global word importance and play as a
bottleneck teaching the model to make predictions based on important words.
FLAT explicitly shoots at the vulnerability problem caused by the mismatch
between model understandings on the replaced words and their synonyms in
original/adversarial example pairs by regularizing the corresponding global
word importance scores. Experiments show the effectiveness of FLAT in improving
the robustness with respect to both predictions and interpretations of four
neural network models (LSTM, CNN, BERT, and DeBERTa) to two adversarial attacks
on four text classification tasks. The models trained via FLAT also show better
robustness than baseline models on unforeseen adversarial examples across
different attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ThingTalk: An Extensible, Executable Representation Language for Task-Oriented Dialogues. (arXiv:2203.12751v1 [cs.PL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12751">
<div class="article-summary-box-inner">
<span><p>Task-oriented conversational agents rely on semantic parsers to translate
natural language to formal representations. In this paper, we propose the
design and rationale of the ThingTalk formal representation, and how the design
improves the development of transactional task-oriented agents.
</p>
<p>ThingTalk is built on four core principles: (1) representing user requests
directly as executable statements, covering all the functionality of the agent,
(2) representing dialogues formally and succinctly to support accurate
contextual semantic parsing, (3) standardizing types and interfaces to maximize
reuse between agents, and (4) allowing multiple, independently-developed agents
to be composed in a single virtual assistant. ThingTalk is developed as part of
the Genie Framework that allows developers to quickly build transactional
agents given a database and APIs.
</p>
<p>We compare ThingTalk to existing representations: SMCalFlow, SGD, TreeDST.
Compared to the others, the ThingTalk design is both more general and more
cost-effective. Evaluated on the MultiWOZ benchmark, using ThingTalk and
associated tools yields a new state of the art accuracy of 79% turn-by-turn.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Classifying Cyber-Risky Clinical Notes by Employing Natural Language Processing. (arXiv:2203.12781v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12781">
<div class="article-summary-box-inner">
<span><p>Clinical notes, which can be embedded into electronic medical records,
document patient care delivery and summarize interactions between healthcare
providers and patients. These clinical notes directly inform patient care and
can also indirectly inform research and quality/safety metrics, among other
indirect metrics. Recently, some states within the United States of America
require patients to have open access to their clinical notes to improve the
exchange of patient information for patient care. Thus, developing methods to
assess the cyber risks of clinical notes before sharing and exchanging data is
critical. While existing natural language processing techniques are geared to
de-identify clinical notes, to the best of our knowledge, few have focused on
classifying sensitive-information risk, which is a fundamental step toward
developing effective, widespread protection of patient health information. To
bridge this gap, this research investigates methods for identifying
security/privacy risks within clinical notes. The classification either can be
used upstream to identify areas within notes that likely contain sensitive
information or downstream to improve the identification of clinical notes that
have not been entirely de-identified. We develop several models using unigram
and word2vec features with different classifiers to categorize sentence risk.
Experiments on i2b2 de-identification dataset show that the SVM classifier
using word2vec features obtained a maximum F1-score of 0.792. Future research
involves articulation and differentiation of risk in terms of different global
regulatory requirements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Distributional Distortion in Neural Language Modeling. (arXiv:2203.12788v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12788">
<div class="article-summary-box-inner">
<span><p>A fundamental characteristic of natural language is the high rate at which
speakers produce novel expressions. Because of this novelty, a heavy-tail of
rare events accounts for a significant amount of the total probability mass of
distributions in language (Baayen, 2001). Standard language modeling metrics
such as perplexity quantify the performance of language models (LM) in
aggregate. As a result, we have relatively little understanding of whether
neural LMs accurately estimate the probability of sequences in this heavy-tail
of rare events. To address this gap, we develop a controlled evaluation scheme
which uses generative models trained on natural data as artificial languages
from which we can exactly compute sequence probabilities. Training LMs on
generations from these artificial languages, we compare the sequence-level
probability estimates given by LMs to the true probabilities in the target
language. Our experiments reveal that LSTM and Transformer language models (i)
systematically underestimate the probability of sequences drawn from the target
language, and (ii) do so more severely for less-probable sequences.
Investigating where this probability mass went, (iii) we find that LMs tend to
overestimate the probability of ill formed (perturbed) sequences. In addition,
we find that this underestimation behaviour (iv) is weakened, but not
eliminated by greater amounts of training data, and (v) is exacerbated for
target distributions with lower entropy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Disentangleing Content and Fine-grained Prosody Information via Hybrid ASR Bottleneck Features for Voice Conversion. (arXiv:2203.12813v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12813">
<div class="article-summary-box-inner">
<span><p>Non-parallel data voice conversion (VC) have achieved considerable
breakthroughs recently through introducing bottleneck features (BNFs) extracted
by the automatic speech recognition(ASR) model. However, selection of BNFs have
a significant impact on VC result. For example, when extracting BNFs from ASR
trained with Cross Entropy loss (CE-BNFs) and feeding into neural network to
train a VC system, the timbre similarity of converted speech is significantly
degraded. If BNFs are extracted from ASR trained using Connectionist Temporal
Classification loss (CTC-BNFs), the naturalness of the converted speech may
decrease. This phenomenon is caused by the difference of information contained
in BNFs. In this paper, we proposed an any-to-one VC method using hybrid
bottleneck features extracted from CTC-BNFs and CE-BNFs to complement each
other advantages. Gradient reversal layer and instance normalization were used
to extract prosody information from CE-BNFs and content information from
CTC-BNFs. Auto-regressive decoder and Hifi-GAN vocoder were used to generate
high-quality waveform. Experimental results show that our proposed method
achieves higher similarity, naturalness, quality than baseline method and
reveals the differences between the information contained in CE-BNFs and
CTC-BNFs as well as the influence they have on the converted speech.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting the Effects of Leakage on Dependency Parsing. (arXiv:2203.12815v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12815">
<div class="article-summary-box-inner">
<span><p>Recent work by S{\o}gaard (2020) showed that, treebank size aside, overlap
between training and test graphs (termed leakage) explains more of the observed
variation in dependency parsing performance than other explanations. In this
work we revisit this claim, testing it on more models and languages. We find
that it only holds for zero-shot cross-lingual settings. We then propose a more
fine-grained measure of such leakage which, unlike the original measure, not
only explains but also correlates with observed performance variation. Code and
data are available here: https://github.com/miriamwanner/reu-nlp-project
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual CheckList: Generation and Evaluation. (arXiv:2203.12865v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12865">
<div class="article-summary-box-inner">
<span><p>The recently proposed CheckList (Riberio et al,. 2020) approach to evaluation
of NLP systems has revealed high failure rates for basic capabilities for
multiple state-of-the-art and commercial models. However, the CheckList
creation process is manual which creates a bottleneck towards creation of
multilingual CheckLists catering 100s of languages. In this work, we explore
multiple approaches to generate and evaluate the quality of Multilingual
CheckList. We device an algorithm -- Automated Multilingual Checklist
Generation (AMCG) for automatically transferring a CheckList from a source to a
target language that relies on a reasonable machine translation system. We then
compare the CheckList generated by AMCG with CheckLists generated with
different levels of human intervention. Through in-depth crosslingual
experiments between English and Hindi, and broad multilingual experiments
spanning 11 languages, we show that the automatic approach can provide accurate
estimates of failure rates of a model across capabilities, as would a
human-verified CheckList, and better than CheckLists generated by humans from
scratch.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Unsupervised Knowledge Transfer from Social Discussions Help Argument Mining?. (arXiv:2203.12881v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12881">
<div class="article-summary-box-inner">
<span><p>Identifying argument components from unstructured texts and predicting the
relationships expressed among them are two primary steps of argument mining.
The intrinsic complexity of these tasks demands powerful learning models. While
pretrained Transformer-based Language Models (LM) have been shown to provide
state-of-the-art results over different NLP tasks, the scarcity of manually
annotated data and the highly domain-dependent nature of argumentation restrict
the capabilities of such models. In this work, we propose a novel transfer
learning strategy to overcome these challenges. We utilize argumentation-rich
social discussions from the ChangeMyView subreddit as a source of unsupervised,
argumentative discourse-aware knowledge by finetuning pretrained LMs on a
selectively masked language modeling task. Furthermore, we introduce a novel
prompt-based strategy for inter-component relation prediction that compliments
our proposed finetuning method while leveraging on the discourse context.
Exhaustive experiments show the generalization capability of our method on
these two tasks over within-domain as well as out-of-domain datasets,
outperforming several existing and employed strong baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Speech recognition for Speech Assessment of Preschool Children. (arXiv:2203.12886v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12886">
<div class="article-summary-box-inner">
<span><p>The acoustic and linguistic features of preschool speech are investigated in
this study to design an automated speech recognition (ASR) system. Acoustic
fluctuation has been highlighted as a significant barrier to developing
high-performance ASR applications for youngsters. Because of the epidemic,
preschool speech assessment should be conducted online. Accordingly, there is a
need for an automatic speech recognition system. We were confronted with new
challenges in our cognitive system, including converting meaningless words from
speech to text and recognizing word sequence. After testing and experimenting
with several models we obtained a 3.1\% phoneme error rate in Persian. Wav2Vec
2.0 is a paradigm that could be used to build a robust end-to-end speech
recognition system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lahjoita puhetta -- a large-scale corpus of spoken Finnish with some benchmarks. (arXiv:2203.12906v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12906">
<div class="article-summary-box-inner">
<span><p>The Donate Speech campaign has so far succeeded in gathering approximately
3600 hours of ordinary, colloquial Finnish speech into the Lahjoita puhetta
(Donate Speech) corpus. The corpus includes over twenty thousand speakers from
all the regions of Finland and from all age brackets. The primary goals of the
collection were to create a representative, large-scale resource to study
spontaneous spoken Finnish and to accelerate the development of language
technology and speech-based services. In this paper, we present the collection
process and the collected corpus, and showcase its versatility through multiple
use cases. The evaluated use cases include: automatic speech recognition of
spontaneous speech, detection of age, gender, dialect and topic and metadata
analysis. We provide benchmarks for the use cases, as well down loadable,
trained baseline systems with open-source code for reproducibility. One further
use case is to verify the metadata and transcripts given in this corpus itself,
and to suggest artificial metadata and transcripts for the part of the corpus
where it is missing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mono vs Multilingual BERT: A Case Study in Hindi and Marathi Named Entity Recognition. (arXiv:2203.12907v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12907">
<div class="article-summary-box-inner">
<span><p>Named entity recognition (NER) is the process of recognising and classifying
important information (entities) in text. Proper nouns, such as a person's
name, an organization's name, or a location's name, are examples of entities.
The NER is one of the important modules in applications like human resources,
customer support, search engines, content classification, and academia. In this
work, we consider NER for low-resource Indian languages like Hindi and Marathi.
The transformer-based models have been widely used for NER tasks. We consider
different variations of BERT like base-BERT, RoBERTa, and AlBERT and benchmark
them on publicly available Hindi and Marathi NER datasets. We provide an
exhaustive comparison of different monolingual and multilingual
transformer-based models and establish simple baselines currently missing in
the literature. We show that the monolingual MahaRoBERTa model performs the
best for Marathi NER whereas the multilingual XLM-RoBERTa performs the best for
Hindi NER. We also perform cross-language evaluation and present mixed
observations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Rationale-Centric Framework for Human-in-the-loop Machine Learning. (arXiv:2203.12918v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12918">
<div class="article-summary-box-inner">
<span><p>We present a novel rationale-centric framework with human-in-the-loop --
Rationales-centric Double-robustness Learning (RDL) -- to boost model
out-of-distribution performance in few-shot learning scenarios. By using static
semi-factual generation and dynamic human-intervened correction, RDL exploits
rationales (i.e. phrases that cause the prediction), human interventions and
semi-factual augmentations to decouple spurious associations and bias models
towards generally applicable underlying distributions, which enables fast and
accurate generalisation. Experimental results show that RDL leads to
significant prediction benefits on both in-distribution and out-of-distribution
tests compared to many state-of-the-art benchmarks -- especially for few-shot
learning scenarios. We also perform extensive ablation studies to support
in-depth analyses of each component in our framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multitasking Framework for Unsupervised Simple Definition Generation. (arXiv:2203.12926v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12926">
<div class="article-summary-box-inner">
<span><p>The definition generation task can help language learners by providing
explanations for unfamiliar words. This task has attracted much attention in
recent years. We propose a novel task of Simple Definition Generation (SDG) to
help language learners and low literacy readers. A significant challenge of
this task is the lack of learner's dictionaries in many languages, and
therefore the lack of data for supervised training. We explore this task and
propose a multitasking framework SimpDefiner that only requires a standard
dictionary with complex definitions and a corpus containing arbitrary simple
texts. We disentangle the complexity factors from the text by carefully
designing a parameter sharing scheme between two decoders. By jointly training
these components, the framework can generate both complex and simple
definitions simultaneously. We demonstrate that the framework can generate
relevant, simple definitions for the target words through automatic and manual
evaluations on English and Chinese datasets. Our method outperforms the
baseline model by a 1.77 SARI score on the English dataset, and raises the
proportion of the low level (HSK level 1-3) words in Chinese definitions by
3.87%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">mcBERT: Momentum Contrastive Learning with BERT for Zero-Shot Slot Filling. (arXiv:2203.12940v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12940">
<div class="article-summary-box-inner">
<span><p>Zero-shot slot filling has received considerable attention to cope with the
problem of limited available data for the target domain. One of the important
factors in zero-shot learning is to make the model learn generalized and
reliable representations. For this purpose, we present mcBERT, which stands for
momentum contrastive learning with BERT, to develop a robust zero-shot slot
filling model. mcBERT uses BERT to initialize the two encoders, the query
encoder and key encoder, and is trained by applying momentum contrastive
learning. Our experimental results on the SNIPS benchmark show that mcBERT
substantially outperforms the previous models, recording a new
state-of-the-art. Besides, we also show that each component composing mcBERT
contributes to the performance improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Data to Mitigate Spurious Correlations in Natural Language Inference Datasets. (arXiv:2203.12942v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12942">
<div class="article-summary-box-inner">
<span><p>Natural language processing models often exploit spurious correlations
between task-independent features and labels in datasets to perform well only
within the distributions they are trained on, while not generalising to
different task distributions. We propose to tackle this problem by generating a
debiased version of a dataset, which can then be used to train a debiased,
off-the-shelf model, by simply replacing its training data. Our approach
consists of 1) a method for training data generators to generate high-quality,
label-consistent data samples; and 2) a filtering mechanism for removing data
points that contribute to spurious correlations, measured in terms of
z-statistics. We generate debiased versions of the SNLI and MNLI datasets, and
we evaluate on a large suite of debiased, out-of-distribution, and adversarial
test sets. Results show that models trained on our debiased datasets generalise
better than those trained on the original datasets in all settings. On the
majority of the datasets, our method outperforms or performs comparably to
previous state-of-the-art debiasing strategies, and when combined with an
orthogonal technique, product-of-experts, it improves further and outperforms
previous best results of SNLI-hard and MNLI-hard.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Duality-Induced Regularizer for Semantic Matching Knowledge Graph Embeddings. (arXiv:2203.12949v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12949">
<div class="article-summary-box-inner">
<span><p>Semantic matching models -- which assume that entities with similar semantics
have similar embeddings -- have shown great power in knowledge graph embeddings
(KGE). Many existing semantic matching models use inner products in embedding
spaces to measure the plausibility of triples and quadruples in static and
temporal knowledge graphs. However, vectors that have the same inner products
with another vector can still be orthogonal to each other, which implies that
entities with similar semantics may have dissimilar embeddings. This property
of inner products significantly limits the performance of semantic matching
models. To address this challenge, we propose a novel regularizer -- namely,
DUality-induced RegulArizer (DURA) -- which effectively encourages the entities
with similar semantics to have similar embeddings. The major novelty of DURA is
based on the observation that, for an existing semantic matching KGE model
(primal), there is often another distance based KGE model (dual) closely
associated with it, which can be used as effective constraints for entity
embeddings. Experiments demonstrate that DURA consistently and significantly
improves the performance of state-of-the-art semantic matching models on both
static and temporal knowledge graph benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing for Labeled Dependency Trees. (arXiv:2203.12971v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12971">
<div class="article-summary-box-inner">
<span><p>Probing has become an important tool for analyzing representations in Natural
Language Processing (NLP). For graphical NLP tasks such as dependency parsing,
linear probes are currently limited to extracting undirected or unlabeled parse
trees which do not capture the full task. This work introduces DepProbe, a
linear probe which can extract labeled and directed dependency parse trees from
embeddings while using fewer parameters and compute than prior methods.
Leveraging its full task coverage and lightweight parametrization, we
investigate its predictive power for selecting the best transfer language for
training a full biaffine attention parser. Across 13 languages, our proposed
method identifies the best source treebank 94% of the time, outperforming
competitive baselines and prior work. Finally, we analyze the informativeness
of task-specific subspaces in contextual embeddings as well as which benefits a
full parser's non-linear parametrization provides.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Scientific Claims for Zero-Shot Scientific Fact Checking. (arXiv:2203.12990v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12990">
<div class="article-summary-box-inner">
<span><p>Automated scientific fact checking is difficult due to the complexity of
scientific language and a lack of significant amounts of training data, as
annotation requires domain expertise. To address this challenge, we propose
scientific claim generation, the task of generating one or more atomic and
verifiable claims from scientific sentences, and demonstrate its usefulness in
zero-shot fact checking for biomedical claims. We propose CLAIMGEN-BART, a new
supervised method for generating claims supported by the literature, as well as
KBIN, a novel method for generating claim negations. Additionally, we adapt an
existing unsupervised entity-centric method of claim generation to biomedical
claims, which we call CLAIMGEN-ENTITY. Experiments on zero-shot fact checking
demonstrate that both CLAIMGEN-ENTITY and CLAIMGEN-BART, coupled with KBIN,
achieve up to 90% performance of fully supervised models trained on manually
annotated claims and evidence. A rigorous evaluation study demonstrates
significant improvement in generated claim and negation quality over existing
baselines
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Kratt: Developing an Automatic Subject Indexing Tool for The National Library of Estonia. (arXiv:2203.12998v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12998">
<div class="article-summary-box-inner">
<span><p>Manual subject indexing in libraries is a time-consuming and costly process
and the quality of the assigned subjects is affected by the cataloguer's
knowledge on the specific topics contained in the book. Trying to solve these
issues, we exploited the opportunities arising from artificial intelligence to
develop Kratt: a prototype of an automatic subject indexing tool. Kratt is able
to subject index a book independent of its extent and genre with a set of
keywords present in the Estonian Subject Thesaurus. It takes Kratt
approximately 1 minute to subject index a book, outperforming humans 10-15
times. Although the resulting keywords were not considered satisfactory by the
cataloguers, the ratings of a small sample of regular library users showed more
promise. We also argue that the results can be enhanced by including a bigger
corpus for training the model and applying more careful preprocessing
techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ensembling and Knowledge Distilling of Large Sequence Taggers for Grammatical Error Correction. (arXiv:2203.13064v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13064">
<div class="article-summary-box-inner">
<span><p>In this paper, we investigate improvements to the GEC sequence tagging
architecture with a focus on ensembling of recent cutting-edge
Transformer-based encoders in Large configurations. We encourage ensembling
models by majority votes on span-level edits because this approach is tolerant
to the model architecture and vocabulary size. Our best ensemble achieves a new
SOTA result with an $F_{0.5}$ score of 76.05 on BEA-2019 (test), even without
pre-training on synthetic datasets. In addition, we perform knowledge
distillation with a trained ensemble to generate new synthetic training
datasets, "Troy-Blogs" and "Troy-1BW". Our best single sequence tagging model
that is pretrained on the generated Troy-datasets in combination with the
publicly available synthetic PIE dataset achieves a near-SOTA (To the best of
our knowledge, our best single model gives way only to much heavier T5 model
result with an $F_{0.5}$ score of 73.21 on BEA-2019 (test). The code, datasets,
and trained models are publicly available).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Introducing Neural Bag of Whole-Words with ColBERTer: Contextualized Late Interactions using Enhanced Reduction. (arXiv:2203.13088v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13088">
<div class="article-summary-box-inner">
<span><p>Recent progress in neural information retrieval has demonstrated large gains
in effectiveness, while often sacrificing the efficiency and interpretability
of the neural model compared to classical approaches. This paper proposes
ColBERTer, a neural retrieval model using contextualized late interaction
(ColBERT) with enhanced reduction. Along the effectiveness Pareto frontier,
ColBERTer's reductions dramatically lower ColBERT's storage requirements while
simultaneously improving the interpretability of its token-matching scores. To
this end, ColBERTer fuses single-vector retrieval, multi-vector refinement, and
optional lexical matching components into one model. For its multi-vector
component, ColBERTer reduces the number of stored vectors per document by
learning unique whole-word representations for the terms in each document and
learning to identify and remove word representations that are not essential to
effective scoring. We employ an explicit multi-task, multi-stage training to
facilitate using very small vector dimensions. Results on the MS MARCO and
TREC-DL collection show that ColBERTer can reduce the storage footprint by up
to 2.5x, while maintaining effectiveness. With just one dimension per token in
its smallest setting, ColBERTer achieves index storage parity with the
plaintext size, with very strong effectiveness results. Finally, we demonstrate
ColBERTer's robustness on seven high-quality out-of-domain collections,
yielding statistically significant gains over traditional retrieval baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">minicons: Enabling Flexible Behavioral and Representational Analyses of Transformer Language Models. (arXiv:2203.13112v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13112">
<div class="article-summary-box-inner">
<span><p>We present minicons, an open source library that provides a standard API for
researchers interested in conducting behavioral and representational analyses
of transformer-based language models (LMs). Specifically, minicons enables
researchers to apply analysis methods at two levels: (1) at the prediction
level -- by providing functions to efficiently extract word/sentence level
probabilities; and (2) at the representational level -- by also facilitating
efficient extraction of word/phrase level vectors from one or more layers. In
this paper, we describe the library and apply it to two motivating case
studies: One focusing on the learning dynamics of the BERT architecture on
relative grammatical judgments, and the other on benchmarking 23 different LMs
on zero-shot abductive reasoning. minicons is available at
https://github.com/kanishkamisra/minicons
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors. (arXiv:2203.13131v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13131">
<div class="article-summary-box-inner">
<span><p>Recent text-to-image generation methods provide a simple yet exciting
conversion capability between text and image domains. While these methods have
incrementally improved the generated image fidelity and text relevancy, several
pivotal gaps remain unanswered, limiting applicability and quality. We propose
a novel text-to-image method that addresses these gaps by (i) enabling a simple
control mechanism complementary to text in the form of a scene, (ii)
introducing elements that substantially improve the tokenization process by
employing domain-specific knowledge over key image regions (faces and salient
objects), and (iii) adapting classifier-free guidance for the transformer use
case. Our model achieves state-of-the-art FID and human evaluation results,
unlocking the ability to generate high fidelity images in a resolution of
512x512 pixels, significantly improving visual quality. Through scene
controllability, we introduce several new capabilities: (i) Scene editing, (ii)
text editing with anchor scenes, (iii) overcoming out-of-distribution text
prompts, and (iv) story illustration generation, as demonstrated in the story
we wrote.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-armed bandits for online optimization of language model pre-training: the use case of dynamic masking. (arXiv:2203.13151v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13151">
<div class="article-summary-box-inner">
<span><p>Transformer-based language models (TLMs) provide state-of-the-art performance
in many modern natural language processing applications. TLM training is
conducted in two phases. First, the model is pre-trained over large volumes of
text to minimize a generic objective function, such as the Masked Language
Model (MLM). Second, the model is fine-tuned in specific downstream tasks.
Pre-training requires large volumes of data and high computational resources,
while introducing many still unresolved design choices. For instance, selecting
hyperparameters for language model pre-training is often carried out based on
heuristics or grid-based searches. In this work, we propose a multi-armed
bandit-based online optimization framework for the sequential selection of
pre-training hyperparameters to optimize language model performance. We pose
the pre-training procedure as a sequential decision-making task, where at each
pre-training step, an agent must determine what hyperparameters to use towards
optimizing the pre-training objective. We propose a Thompson sampling bandit
algorithm, based on a surrogate Gaussian process reward model of the MLM
pre-training objective, for its sequential minimization. We empirically show
how the proposed Gaussian process based Thompson sampling pre-trains robust and
well-performing language models. Namely, by sequentially selecting masking
hyperparameters of the TLM, we achieve satisfactory performance in less epochs,
not only in terms of the pre-training MLM objective, but in diverse downstream
fine-tuning tasks. The proposed bandit-based technique provides an automated
hyperparameter selection method for pre-training TLMs of interest to
practitioners. In addition, our results indicate that, instead of MLM
pre-training with fixed masking probabilities, sequentially adapting the
masking hyperparameters improves both pre-training loss and downstream task
metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emergence of hierarchical reference systems in multi-agent communication. (arXiv:2203.13176v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13176">
<div class="article-summary-box-inner">
<span><p>In natural language, referencing objects at different levels of specificity
is a fundamental pragmatic mechanism for efficient communication in context. We
develop a novel communication game, the hierarchical reference game, to study
the emergence of such reference systems in artificial agents. We consider a
simplified world, in which concepts are abstractions over a set of primitive
attributes (e.g., color, style, shape). Depending on how many attributes are
combined, concepts are more general ("circle") or more specific ("red dotted
circle"). Based on the context, the agents have to communicate at different
levels of this hierarchy. Our results show, that the agents learn to play the
game successfully and can even generalize to novel concepts. To achieve
abstraction, they use implicit (omitting irrelevant information) and explicit
(indicating that attributes are irrelevant) strategies. In addition, the
compositional structure underlying the concept hierarchy is reflected in the
emergent protocols, indicating that the need to develop hierarchical reference
systems supports the emergence of compositionality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Direct parsing to sentiment graphs. (arXiv:2203.13209v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13209">
<div class="article-summary-box-inner">
<span><p>This paper demonstrates how a graph-based semantic parser can be applied to
the task of structured sentiment analysis, directly predicting sentiment graphs
from text. We advance the state of the art on 4 out of 5 standard benchmark
sets. We release the source code, models and predictions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models that Seek for Knowledge: Modular Search & Generation for Dialogue and Prompt Completion. (arXiv:2203.13224v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13224">
<div class="article-summary-box-inner">
<span><p>Language models (LMs) have recently been shown to generate more factual
responses by employing modularity (Zhou et al., 2021) in combination with
retrieval (Adolphs et al., 2021). We extend the recent approach of Adolphs et
al. (2021) to include internet search as a module. Our SeeKeR (Search
engine-&gt;Knowledge-&gt;Response) method thus applies a single LM to three modular
tasks in succession: search, generating knowledge, and generating a final
response. We show that, when using SeeKeR as a dialogue model, it outperforms
the state-of-the-art model BlenderBot 2 (Chen et al., 2021) on open-domain
knowledge-grounded conversations for the same number of parameters, in terms of
consistency, knowledge and per-turn engagingness. SeeKeR applied to topical
prompt completions as a standard language model outperforms GPT2 (Radford et
al., 2019) and GPT3 (Brown et al., 2020) in terms of factuality and topicality,
despite GPT3 being a vastly larger model. Our code and models are made publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SMARAGD: Synthesized sMatch for Accurate and Rapid AMR Graph Distance. (arXiv:2203.13226v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13226">
<div class="article-summary-box-inner">
<span><p>The semantic similarity of graph-based meaning representations, such as
Abstract Meaning Representation (AMR), is typically assessed using graph
matching algorithms, such as SMATCH (Cai and Knight, 2013). However, SMATCH
suffers from NP-completeness, making its large-scale application, e.g., for AMR
clustering or semantic search, infeasible. To mitigate this issue, we propose
SMARAGD (Synthesized sMatch for accurate and rapid AMR graph distance). We show
the potential of neural networks to approximate the SMATCH scores and graph
alignments, i) in linear time using a machine translation framework to predict
the alignments, or ii) in constant time using a Siamese CNN to directly predict
SMATCH scores. We show that the approximation error can be substantially
reduced by applying data augmentation and AMR graph anonymization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Token Dropping for Efficient BERT Pretraining. (arXiv:2203.13240v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13240">
<div class="article-summary-box-inner">
<span><p>Transformer-based models generally allocate the same amount of computation
for each token in a given sequence. We develop a simple but effective "token
dropping" method to accelerate the pretraining of transformer models, such as
BERT, without degrading its performance on downstream tasks. In short, we drop
unimportant tokens starting from an intermediate layer in the model to make the
model focus on important tokens; the dropped tokens are later picked up by the
last layer of the model so that the model still produces full-length sequences.
We leverage the already built-in masked language modeling (MLM) loss to
identify unimportant tokens with practically no computational overhead. In our
experiments, this simple approach reduces the pretraining cost of BERT by 25%
while achieving similar overall fine-tuning performance on standard downstream
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PCP Theorems, SETH and More: Towards Proving Sub-linear Time Inapproximability. (arXiv:2011.02320v4 [cs.CC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.02320">
<div class="article-summary-box-inner">
<span><p>In this paper we propose the PCP-like theorem for sub-linear time
inapproximability. Abboud et al. have devised the distributed PCP framework for
sub-quadratic time inapproximability. We show that the distributed PCP theorem
can be generalized for proving arbitrary polynomial time inapproximability, but
fails in the linear case. We prove the sub-linear PCP theorem by adapting from
an MA-protocol for the Set Containment problem, and show how to use the theorem
to prove both existing and new inapproximability results, exhibiting the power
of the sub-linear PCP theorem. Considering the emerging research works on
sub-linear time algorithms, the sub-linear PCP theorem is important in guiding
the research in sub-linear time approximation algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning From Human Correction For Data-Centric Deep Learning. (arXiv:2102.00225v7 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.00225">
<div class="article-summary-box-inner">
<span><p>In industry NLP application, our manually labeled data has a certain number
of noisy data. We present a simple method to find the noisy data and relabel
them manually, meanwhile we collect the correction information. Then we present
novel method to incorporate the human correction information into deep learning
model. Human know how to correct noisy data. So the correction information can
be inject into deep learning model. We do the experiment on our own text
classification dataset, which is manually labeled, because we relabel the noisy
data in our dataset for our industry application. The experiment result shows
that our method improve the classification accuracy from 91.7% to 92.5%. The
91.7% accuracy is trained on the corrected dataset, which improve the baseline
from 83.3% to 91.7%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Hate Speech with GPT-3. (arXiv:2103.12407v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.12407">
<div class="article-summary-box-inner">
<span><p>Sophisticated language models such as OpenAI's GPT-3 can generate hateful
text that targets marginalized groups. Given this capacity, we are interested
in whether large language models can be used to identify hate speech and
classify text as sexist or racist. We use GPT-3 to identify sexist and racist
text passages with zero-, one-, and few-shot learning. We find that with zero-
and one-shot learning, GPT-3 can identify sexist or racist text with an average
accuracy between 55 per cent and 67 per cent, depending on the category of text
and type of learning. With few-shot learning, the model's accuracy can be as
high as 85 per cent. Large language models have a role to play in hate speech
detection, and with further development they could eventually be used to
counter hate speech.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLIPScore: A Reference-free Evaluation Metric for Image Captioning. (arXiv:2104.08718v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08718">
<div class="article-summary-box-inner">
<span><p>Image captioning has conventionally relied on reference-based automatic
evaluations, where machine captions are compared against captions written by
humans. This is in contrast to the reference-free manner in which humans assess
caption quality.
</p>
<p>In this paper, we report the surprising empirical finding that CLIP (Radford
et al., 2021), a cross-modal model pretrained on 400M image+caption pairs from
the web, can be used for robust automatic evaluation of image captioning
without the need for references. Experiments spanning several corpora
demonstrate that our new reference-free metric, CLIPScore, achieves the highest
correlation with human judgements, outperforming existing reference-based
metrics like CIDEr and SPICE. Information gain experiments demonstrate that
CLIPScore, with its tight focus on image-text compatibility, is complementary
to existing reference-based metrics that emphasize text-text similarities.
Thus, we also present a reference-augmented version, RefCLIPScore, which
achieves even higher correlation. Beyond literal description tasks, several
case studies reveal domains where CLIPScore performs well (clip-art images,
alt-text rating), but also where it is relatively weaker in comparison to
reference-based metrics, e.g., news captions that require richer contextual
knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the proper role of linguistically-oriented deep net analysis in linguistic theorizing. (arXiv:2106.08694v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.08694">
<div class="article-summary-box-inner">
<span><p>A lively research field has recently emerged that uses experimental methods
to probe the linguistic behavior of modern deep networks. While work in this
tradition often reports intriguing results about the grammatical skills of deep
nets, it is not clear what their implications for linguistic theorizing should
be. As a consequence, linguistically-oriented deep net analysis has had very
little impact on linguistics at large. In this chapter, I suggest that deep
networks should be treated as theories making explicit predictions about the
acceptability of linguistic utterances. I argue that, if we overcome some
obstacles standing in the way of seriously pursuing this idea, we will gain a
powerful new theoretical tool, complementary to mainstream algebraic
approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PALRACE: Reading Comprehension Dataset with Human Data and Labeled Rationales. (arXiv:2106.12373v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.12373">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models achieves high performance on machine reading
comprehension (MRC) tasks but the results are hard to explain. An appealing
approach to make models explainable is to provide rationales for its decision.
To investigate whether human rationales can further improve current models and
to facilitate supervised learning of human rationales, here we present PALRACE
(Pruned And Labeled RACE), a new MRC dataset with human labeled rationales for
800 passages selected from the RACE dataset. We further classified the question
to each passage into 6 types. Each passage was read by at least 26 human
readers, who labeled their rationales to answer the question. It is
demonstrated that models such as RoBERTa-large outperforms human readers in all
6 types of questions, including inference questions, but its performance can be
further improved when having access to the human rationales. Simpler models and
pre-trained models that are not fine-tuned based on the task benefit more from
human rationales, and their performance can be boosted by more than 30% by
rationales. With access to human rationales, a simple model based on the GloVe
word embedding can reach the performance of BERT-base.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Efficacy of Automatically Generated Counterfactuals for Sentiment Analysis. (arXiv:2106.15231v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.15231">
<div class="article-summary-box-inner">
<span><p>While state-of-the-art NLP models have been achieving the excellent
performance of a wide range of tasks in recent years, important questions are
being raised about their robustness and their underlying sensitivity to
systematic biases that may exist in their training and test data. Such issues
come to be manifest in performance problems when faced with out-of-distribution
data in the field. One recent solution has been to use counterfactually
augmented datasets in order to reduce any reliance on spurious patterns that
may exist in the original data. Producing high-quality augmented data can be
costly and time-consuming as it usually needs to involve human feedback and
crowdsourcing efforts. In this work, we propose an alternative by describing
and evaluating an approach to automatically generating counterfactual data for
data augmentation and explanation. A comprehensive evaluation on several
different datasets and using a variety of state-of-the-art benchmarks
demonstrate how our approach can achieve significant improvements in model
performance when compared to models training on the original data and even when
compared to models trained with the benefit of human-generated augmented data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepSTL -- From English Requirements to Signal Temporal Logic. (arXiv:2109.10294v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10294">
<div class="article-summary-box-inner">
<span><p>Formal methods provide very powerful tools and techniques for the design and
analysis of complex systems. Their practical application remains however
limited, due to the widely accepted belief that formal methods require
extensive expertise and a steep learning curve. Writing correct formal
specifications in form of logical formulas is still considered to be a
difficult and error prone task.
</p>
<p>In this paper we propose DeepSTL, a tool and technique for the translation of
informal requirements, given as free English sentences, into Signal Temporal
Logic (STL), a formal specification language for cyber-physical systems, used
both by academia and advanced research labs in industry. A major challenge to
devise such a translator is the lack of publicly available informal
requirements and formal specifications. We propose a two-step workflow to
address this challenge. We first design a grammar-based generation technique of
synthetic data, where each output is a random STL formula and its associated
set of possible English translations. In the second step, we use a
state-of-the-art transformer-based neural translation technique, to train an
accurate attentional translator of English to STL. The experimental results
show high translation quality for patterns of English requirements that have
been well trained, making this workflow promising to be extended for processing
more complex translation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MReD: A Meta-Review Dataset for Controllable Text Generation. (arXiv:2110.07474v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07474">
<div class="article-summary-box-inner">
<span><p>When directly using existing text generation datasets for controllable
generation, we are facing the problem of not having the domain knowledge and
thus the aspects that could be controlled are limited. A typical example is
when using CNN/Daily Mail dataset for controllable text summarization, there is
no guided information on the emphasis of summary sentences. A more useful text
generator should leverage both the input text and the control signal to guide
the generation, which can only be built with a deep understanding of the domain
knowledge. Motivated by this vision, our paper introduces a new text generation
dataset, named MReD. Our new dataset consists of 7,089 meta-reviews and all its
45k meta-review sentences are manually annotated with one of the 9 carefully
defined categories, including abstract, strength, decision, etc. We present
experimental results on start-of-the-art summarization models, and propose
methods for structure-controlled generation with both extractive and
abstractive models using our annotated data. By exploring various settings and
analyzing the model behavior with respect to the control signal, we demonstrate
the challenges of our proposed task and the values of our dataset MReD.
Meanwhile, MReD also allows us to have a better understanding of the
meta-review domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DialFact: A Benchmark for Fact-Checking in Dialogue. (arXiv:2110.08222v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08222">
<div class="article-summary-box-inner">
<span><p>Fact-checking is an essential tool to mitigate the spread of misinformation
and disinformation. We introduce the task of fact-checking in dialogue, which
is a relatively unexplored area. We construct DialFact, a testing benchmark
dataset of 22,245 annotated conversational claims, paired with pieces of
evidence from Wikipedia. There are three sub-tasks in DialFact: 1) Verifiable
claim detection task distinguishes whether a response carries verifiable
factual information; 2) Evidence retrieval task retrieves the most relevant
Wikipedia snippets as evidence; 3) Claim verification task predicts a dialogue
response to be supported, refuted, or not enough information. We found that
existing fact-checking models trained on non-dialogue data like FEVER fail to
perform well on our task, and thus, we propose a simple yet data-efficient
solution to effectively improve fact-checking performance in dialogue. We point
out unique challenges in DialFact such as handling the colloquialisms,
coreferences and retrieval ambiguities in the error analysis to shed light on
future research in this direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">More than Words: In-the-Wild Visually-Driven Prosody for Text-to-Speech. (arXiv:2111.10139v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10139">
<div class="article-summary-box-inner">
<span><p>In this paper we present VDTTS, a Visually-Driven Text-to-Speech model.
Motivated by dubbing, VDTTS takes advantage of video frames as an additional
input alongside text, and generates speech that matches the video signal. We
demonstrate how this allows VDTTS to, unlike plain TTS models, generate speech
that not only has prosodic variations like natural pauses and pitch, but is
also synchronized to the input video. Experimentally, we show our model
produces well-synchronized outputs, approaching the video-speech
synchronization quality of the ground-truth, on several challenging benchmarks
including "in-the-wild" content from VoxCeleb2. Supplementary demo videos
demonstrating video-speech synchronization, robustness to speaker ID swapping,
and prosody, presented at the project page.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Show and Write: Entity-aware Article Generation with Image Information. (arXiv:2112.05917v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05917">
<div class="article-summary-box-inner">
<span><p>Many vision-language applications contain long articles of text paired with
images (e.g., news or Wikipedia articles). Prior work learning to encode and/or
generate these articles has primarily focused on understanding the article
itself and some related metadata like the title or date it was written.
However, the images and their captions or alt-text often contain crucial
information such as named entities that are difficult to be correctly
recognized and predicted by language models. To address this shortcoming, this
paper introduces an ENtity-aware article Generation method with Image
iNformation, ENGIN, to incorporate an article's image information into language
models. ENGIN represents articles that can be conditioned on metadata used by
prior work and information such as captions and named entities extracted from
images. Our key contribution is a novel Entity-aware mechanism to help our
model better recognize and predict the entity names in articles. We perform
experiments on three public datasets, GoodNews, VisualNews, and WikiText.
Quantitative results show that our approach improves generated article
perplexity by 4-5 points over the base models. Qualitative results demonstrate
the text generated by ENGIN is more consistent with embedded article images. We
also perform article quality annotation experiments on the generated articles
to validate that our model produces higher-quality articles. Finally, we
investigate the effect ENGIN has on methods that automatically detect
machine-generated articles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks. (arXiv:2112.06825v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06825">
<div class="article-summary-box-inner">
<span><p>Recently, fine-tuning language models pre-trained on large text corpora have
provided huge improvements on vision-and-language (V&amp;L) tasks as well as on
pure language tasks. However, fine-tuning the entire parameter set of
pre-trained models becomes impractical since the model size is growing rapidly.
Hence, in this paper, we introduce adapter-based parameter-efficient transfer
learning techniques to V&amp;L models such as VL-BART and VLT5. We evaluate our
methods in a unified multi-task setup on both image-text and video-text
benchmarks. For the image-text tasks, we use four diverse V&amp;L datasets: VQAv2,
GQA, NLVR2 , and MSCOCO image captioning. For video-text tasks, we use TVQA,
How2QA, TVC, and YC2C. With careful training and thorough experiments, we
benchmark three popular adapter-based methods (Adapter, Hyperformer, Compacter)
against the standard full fine-tuning and the recently proposed prompt-tuning
approach. We also enhance the efficiency and performance of adapters by sharing
their weights to attain knowledge across tasks. Our results demonstrate that
training the adapter with the weight-sharing technique (4.18% of total
parameters for image-text tasks and 3.39% for video-text tasks) can match the
performance of fine-tuning the entire model. Lastly, we present a comprehensive
analysis including the combination of adapter and task-specific prompts and the
impact of V&amp;L pre-training on adapters. Our code is available at:
https://github.com/ylsung/VL_adapter.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SGPT: GPT Sentence Embeddings for Semantic Search. (arXiv:2202.08904v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08904">
<div class="article-summary-box-inner">
<span><p>GPT transformers are the largest language models available, yet semantic
search is dominated by BERT transformers. We present SGPT-BE and SGPT-CE for
applying GPT models as Bi-Encoders or Cross-Encoders to symmetric or asymmetric
search.
</p>
<p>SGPT-BE produces semantically meaningful sentence embeddings by contrastive
fine-tuning of only bias tensors and a novel pooling method. A 5.8 billion
parameter SGPT-BE outperforms the best available sentence embeddings by 6%
setting a new state-of-the-art on BEIR. It outperforms the concurrently
proposed OpenAI Embeddings of the 175B Davinci endpoint, which fine-tunes
250,000 times more parameters.
</p>
<p>SGPT-CE uses log probabilities from GPT models without any fine-tuning. A 6.1
billion parameter SGPT-CE sets an unsupervised state-of-the-art on BEIR. It
beats the supervised state-of-the-art on 7 datasets, but significantly loses on
other datasets. We show how this can be alleviated by adapting the prompt.
</p>
<p>SGPT-BE and SGPT-CE performance scales with model size. Yet, increased
latency, storage and compute costs should be considered. Code, models and
result files are freely available at https://github.com/Muennighoff/sgpt.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FormNet: Structural Encoding beyond Sequential Modeling in Form Document Information Extraction. (arXiv:2203.08411v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08411">
<div class="article-summary-box-inner">
<span><p>Sequence modeling has demonstrated state-of-the-art performance on natural
language and document understanding tasks. However, it is challenging to
correctly serialize tokens in form-like documents in practice due to their
variety of layout patterns. We propose FormNet, a structure-aware sequence
model to mitigate the suboptimal serialization of forms. First, we design Rich
Attention that leverages the spatial relationship between tokens in a form for
more precise attention score calculation. Second, we construct Super-Tokens for
each word by embedding representations from their neighboring tokens through
graph convolutions. FormNet therefore explicitly recovers local syntactic
information that may have been lost during serialization. In experiments,
FormNet outperforms existing methods with a more compact model size and less
pre-training data, establishing new state-of-the-art performance on CORD, FUNSD
and Payment benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">g2pW: A Conditional Weighted Softmax BERT for Polyphone Disambiguation in Mandarin. (arXiv:2203.10430v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10430">
<div class="article-summary-box-inner">
<span><p>Polyphone disambiguation is the most crucial task in Mandarin
grapheme-to-phoneme (g2p) conversion. Previous studies have approached this
problem using pre-trained language models, restricted output, and extra
information from Part-Of-Speech (POS) tagging. Inspired by these strategies, we
propose a novel approach, called g2pW, which adapts learnable softmax-weights
to condition the outputs of BERT with the polyphonic character of interest and
its POS tagging. Rather than using the hard mask as in previous works, our
experiments show that learning a soft-weighting function for the candidate
phonemes benefits performance. In addition, our proposed g2pW does not require
extra pre-trained POS tagging models while using POS tags as auxiliary features
since we train the POS tagging model simultaneously with the unified encoder.
Experimental results show that our g2pW outperforms existing methods on the
public CPP dataset. All codes, model weights, and a user-friendly package are
publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WeSinger: Data-augmented Singing Voice Synthesis with Auxiliary Losses. (arXiv:2203.10750v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10750">
<div class="article-summary-box-inner">
<span><p>In this paper, we develop a new multi-singer Chinese neural singing voice
synthesis (SVS) system named WeSinger. To improve the accuracy and naturalness
of synthesized singing voice, we design several specifical modules and
techniques: 1) A deep bi-directional LSTM based duration model with multi-scale
rhythm loss and post-processing step; 2) A Transformer-alike acoustic model
with progressive pitch-weighted decoder loss; 3) a 24 kHz pitch-aware LPCNet
neural vocoder to produce high-quality singing waveforms; 4) A novel data
augmentation method with multi-singer pre-training for stronger robustness and
naturalness. Both quantitative and qualitative evaluation results demonstrate
the effectiveness of WeSinger in terms of accuracy and naturalness, and
WeSinger achieves state-of-the-art performance on the public corpus Opencpop.
Some synthesized singing samples are available online
(https://zzw922cn.github.io/WeSinger/).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IAM: A Comprehensive and Large-Scale Dataset for Integrated Argument Mining Tasks. (arXiv:2203.12257v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12257">
<div class="article-summary-box-inner">
<span><p>Traditionally, a debate usually requires a manual preparation process,
including reading plenty of articles, selecting the claims, identifying the
stances of the claims, seeking the evidence for the claims, etc. As the AI
debate attracts more attention these years, it is worth exploring the methods
to automate the tedious process involved in the debating system. In this work,
we introduce a comprehensive and large dataset named IAM, which can be applied
to a series of argument mining tasks, including claim extraction, stance
classification, evidence extraction, etc. Our dataset is collected from over 1k
articles related to 123 topics. Near 70k sentences in the dataset are fully
annotated based on their argument properties (e.g., claims, stances, evidence,
etc.). We further propose two new integrated argument mining tasks associated
with the debate preparation process: (1) claim extraction with stance
classification (CESC) and (2) claim-evidence pair extraction (CEPE). We adopt a
pipeline approach and an end-to-end method for each integrated task separately.
Promising experimental results are reported to show the values and challenges
of our proposed tasks, and motivate future research on argument mining.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluation of Non-Invasive Thermal Imaging for detection of Viability of Onchocerciasis worms. (arXiv:2203.12620v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12620">
<div class="article-summary-box-inner">
<span><p>Onchocerciasis is causing blindness in over half a million people in the
world today. Drug development for the disease is crippled as there is no way of
measuring effectiveness of the drug without an invasive procedure. Drug
efficacy measurement through assessment of viability of onchocerca worms
requires the patients to undergo nodulectomy which is invasive, expensive,
time-consuming, skill-dependent, infrastructure dependent and lengthy process.
In this paper, we discuss the first-ever study that proposes use of machine
learning over thermal imaging to non-invasively and accurately predict the
viability of worms. The key contributions of the paper are (i) a unique thermal
imaging protocol along with pre-processing steps such as alignment,
registration and segmentation to extract interpretable features (ii) extraction
of relevant semantic features (iii) development of accurate classifiers for
detecting the existence of viable worms in a nodule. When tested on a
prospective test data of 30 participants with 48 palpable nodules, we achieved
an Area Under the Curve (AUC) of 0.85.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MR Image Denoising and Super-Resolution Using Regularized Reverse Diffusion. (arXiv:2203.12621v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12621">
<div class="article-summary-box-inner">
<span><p>Patient scans from MRI often suffer from noise, which hampers the diagnostic
capability of such images. As a method to mitigate such artifact, denoising is
largely studied both within the medical imaging community and beyond the
community as a general subject. However, recent deep neural network-based
approaches mostly rely on the minimum mean squared error (MMSE) estimates,
which tend to produce a blurred output. Moreover, such models suffer when
deployed in real-world sitautions: out-of-distribution data, and complex noise
distributions that deviate from the usual parametric noise models. In this
work, we propose a new denoising method based on score-based reverse diffusion
sampling, which overcomes all the aforementioned drawbacks. Our network,
trained only with coronal knee scans, excels even on out-of-distribution in
vivo liver MRI data, contaminated with complex mixture of noise. Even more, we
propose a method to enhance the resolution of the denoised image with the same
network. With extensive experiments, we show that our method establishes
state-of-the-art performance, while having desirable properties which prior
MMSE denoisers did not have: flexibly choosing the extent of denoising, and
quantifying uncertainty.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Q-FW: A Hybrid Classical-Quantum Frank-Wolfe for Quadratic Binary Optimization. (arXiv:2203.12633v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12633">
<div class="article-summary-box-inner">
<span><p>We present a hybrid classical-quantum framework based on the Frank-Wolfe
algorithm, Q-FW, for solving quadratic, linearly-constrained, binary
optimization problems on quantum annealers (QA). The computational premise of
quantum computers has cultivated the re-design of various existing vision
problems into quantum-friendly forms. Experimental QA realizations can solve a
particular non-convex problem known as the quadratic unconstrained binary
optimization (QUBO). Yet a naive-QUBO cannot take into account the restrictions
on the parameters. To introduce additional structure in the parameter space,
researchers have crafted ad-hoc solutions incorporating (linear) constraints in
the form of regularizers. However, this comes at the expense of a
hyper-parameter, balancing the impact of regularization. To date, a true
constrained solver of quadratic binary optimization (QBO) problems has lacked.
Q-FW first reformulates constrained-QBO as a copositive program (CP), then
employs Frank-Wolfe iterations to solve CP while satisfying linear (in)equality
constraints. This procedure unrolls the original constrained-QBO into a set of
unconstrained QUBOs all of which are solved, in a sequel, on a QA. We use
D-Wave Advantage QA to conduct synthetic and real experiments on two important
computer vision problems, graph matching and permutation synchronization, which
demonstrate that our approach is effective in alleviating the need for an
explicit regularization coefficient.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Scene Flow in 3D Point Clouds with Noisy Pseudo Labels. (arXiv:2203.12655v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12655">
<div class="article-summary-box-inner">
<span><p>We propose a novel scene flow method that captures 3D motions from point
clouds without relying on ground-truth scene flow annotations. Due to the
irregularity and sparsity of point clouds, it is expensive and time-consuming
to acquire ground-truth scene flow annotations. Some state-of-the-art
approaches train scene flow networks in a self-supervised learning manner via
approximating pseudo scene flow labels from point clouds. However, these
methods fail to achieve the performance level of fully supervised methods, due
to the limitations of point cloud such as sparsity and lacking color
information. To provide an alternative, we propose a novel approach that
utilizes monocular RGB images and point clouds to generate pseudo scene flow
labels for training scene flow networks. Our pseudo label generation module
infers pseudo scene labels for point clouds by jointly leveraging rich
appearance information in monocular images and geometric information of point
clouds. To further reduce the negative effect of noisy pseudo labels on the
training, we propose a noisy-label-aware training scheme by exploiting the
geometric relations of points. Experiment results show that our method not only
outperforms state-of-the-art self-supervised approaches, but also outperforms
some supervised approaches that use accurate ground-truth flows.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Computed Tomography Reconstruction using Generative Energy-Based Priors. (arXiv:2203.12658v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12658">
<div class="article-summary-box-inner">
<span><p>In the past decades, Computed Tomography (CT) has established itself as one
of the most important imaging techniques in medicine. Today, the applicability
of CT is only limited by the deposited radiation dose, reduction of which
manifests in noisy or incomplete measurements. Thus, the need for robust
reconstruction algorithms arises. In this work, we learn a parametric
regularizer with a global receptive field by maximizing it's likelihood on
reference CT data. Due to this unsupervised learning strategy, our trained
regularizer truly represents higher-level domain statistics, which we
empirically demonstrate by synthesizing CT images. Moreover, this regularizer
can easily be applied to different CT reconstruction problems by embedding it
in a variational framework, which increases flexibility and interpretability
compared to feed-forward learning-based approaches. In addition, the
accompanying probabilistic perspective enables experts to explore the full
posterior distribution and may quantify uncertainty of the reconstruction
approach. We apply the regularizer to limited-angle and few-view CT
reconstruction problems, where it outperforms traditional reconstruction
algorithms by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions. (arXiv:2203.12667v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12667">
<div class="article-summary-box-inner">
<span><p>A long-term goal of AI research is to build intelligent agents that can
communicate with humans in natural language, perceive the environment, and
perform real-world tasks. Vision-and-Language Navigation (VLN) is a fundamental
and interdisciplinary research topic towards this goal, and receives increasing
attention from natural language processing, computer vision, robotics, and
machine learning communities. In this paper, we review contemporary studies in
the emerging field of VLN, covering tasks, evaluation metrics, methods, etc.
Through structured analysis of current progress and challenges, we highlight
the limitations of current VLN and opportunities for future work. This paper
serves as a thorough reference for the VLN research community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision-Based Manipulators Need to Also See from Their Hands. (arXiv:2203.12677v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12677">
<div class="article-summary-box-inner">
<span><p>We study how the choice of visual perspective affects learning and
generalization in the context of physical manipulation from raw sensor
observations. Compared with the more commonly used global third-person
perspective, a hand-centric (eye-in-hand) perspective affords reduced
observability, but we find that it consistently improves training efficiency
and out-of-distribution generalization. These benefits hold across a variety of
learning algorithms, experimental settings, and distribution shifts, and for
both simulated and real robot apparatuses. However, this is only the case when
hand-centric observability is sufficient; otherwise, including a third-person
perspective is necessary for learning, but also harms out-of-distribution
generalization. To mitigate this, we propose to regularize the third-person
information stream via a variational information bottleneck. On six
representative manipulation tasks with varying hand-centric observability
adapted from the Meta-World benchmark, this results in a state-of-the-art
reinforcement learning agent operating from both perspectives improving its
out-of-distribution generalization on every task. While some practitioners have
long put cameras in the hands of robots, our work systematically analyzes the
benefits of doing so and provides simple and broadly applicable insights for
improving end-to-end learned vision-based robotic manipulation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Multi-Scale Feature Fusion for Semantic Segmentation. (arXiv:2203.12683v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12683">
<div class="article-summary-box-inner">
<span><p>It is commonly believed that high internal resolution combined with expensive
operations (e.g. atrous convolutions) are necessary for accurate semantic
segmentation, resulting in slow speed and large memory usage. In this paper, we
question this belief and demonstrate that neither high internal resolution nor
atrous convolutions are necessary. Our intuition is that although segmentation
is a dense per-pixel prediction task, the semantics of each pixel often depend
on both nearby neighbors and far-away context; therefore, a more powerful
multi-scale feature fusion network plays a critical role. Following this
intuition, we revisit the conventional multi-scale feature space (typically
capped at P5) and extend it to a much richer space, up to P9, where the
smallest features are only 1/512 of the input size and thus have very large
receptive fields. To process such a rich feature space, we leverage the recent
BiFPN to fuse the multi-scale features. Based on these insights, we develop a
simplified segmentation model, named ESeg, which has neither high internal
resolution nor expensive atrous convolutions. Perhaps surprisingly, our simple
method can achieve better accuracy with faster speed than prior art across
multiple datasets. In real-time settings, ESeg-Lite-S achieves 76.0% mIoU on
CityScapes [12] at 189 FPS, outperforming FasterSeg [9] (73.1% mIoU at 170
FPS). Our ESeg-Lite-L runs at 79 FPS and achieves 80.1% mIoU, largely closing
the gap between real-time and high-performance segmentation models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to generate line drawings that convey geometry and semantics. (arXiv:2203.12691v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12691">
<div class="article-summary-box-inner">
<span><p>This paper presents an unpaired method for creating line drawings from
photographs. Current methods often rely on high quality paired datasets to
generate line drawings. However, these datasets often have limitations due to
the subjects of the drawings belonging to a specific domain, or in the amount
of data collected. Although recent work in unsupervised image-to-image
translation has shown much progress, the latest methods still struggle to
generate compelling line drawings. We observe that line drawings are encodings
of scene information and seek to convey 3D shape and semantic meaning. We build
these observations into a set of objectives and train an image translation to
map photographs into line drawings. We introduce a geometry loss which predicts
depth information from the image features of a line drawing, and a semantic
loss which matches the CLIP features of a line drawing with its corresponding
photograph. Our approach outperforms state-of-the-art unpaired image
translation and line drawing generation methods on creating line drawings from
arbitrary photographs. For code and demo visit our webpage
carolineec.github.io/informative_drawings
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Affective Feedback Synthesis Towards Multimodal Text and Image Data. (arXiv:2203.12692v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12692">
<div class="article-summary-box-inner">
<span><p>In this paper, we have defined a novel task of affective feedback synthesis
that deals with generating feedback for input text &amp; corresponding image in a
similar way as humans respond towards the multimodal data. A feedback synthesis
system has been proposed and trained using ground-truth human comments along
with image-text input. We have also constructed a large-scale dataset
consisting of image, text, Twitter user comments, and the number of likes for
the comments by crawling the news articles through Twitter feeds. The proposed
system extracts textual features using a transformer-based textual encoder
while the visual features have been extracted using a Faster region-based
convolutional neural networks model. The textual and visual features have been
concatenated to construct the multimodal features using which the decoder
synthesizes the feedback. We have compared the results of the proposed system
with the baseline models using quantitative and qualitative measures. The
generated feedbacks have been analyzed using automatic and human evaluation.
They have been found to be semantically similar to the ground-truth comments
and relevant to the given text-image input.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Classifier Conservativeness and Robustness by Polynomiality. (arXiv:2203.12693v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12693">
<div class="article-summary-box-inner">
<span><p>We illustrate the detrimental effect, such as overconfident decisions, that
exponential behavior can have in methods like classical LDA and logistic
regression. We then show how polynomiality can remedy the situation. This,
among others, leads purposefully to random-level performance in the tails, away
from the bulk of the training data. A directly related, simple, yet important
technical novelty we subsequently present is softRmax: a reasoned alternative
to the standard softmax function employed in contemporary (deep) neural
networks. It is derived through linking the standard softmax to Gaussian
class-conditional models, as employed in LDA, and replacing those by a
polynomial alternative. We show that two aspects of softRmax, conservativeness
and inherent gradient regularization, lead to robustness against adversarial
attacks without gradient obfuscation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Maximum Spatial Perturbation Consistency for Unpaired Image-to-Image Translation. (arXiv:2203.12707v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12707">
<div class="article-summary-box-inner">
<span><p>Unpaired image-to-image translation (I2I) is an ill-posed problem, as an
infinite number of translation functions can map the source domain distribution
to the target distribution. Therefore, much effort has been put into designing
suitable constraints, e.g., cycle consistency (CycleGAN), geometry consistency
(GCGAN), and contrastive learning-based constraints (CUTGAN), that help better
pose the problem. However, these well-known constraints have limitations: (1)
they are either too restrictive or too weak for specific I2I tasks; (2) these
methods result in content distortion when there is a significant spatial
variation between the source and target domains. This paper proposes a
universal regularization technique called maximum spatial perturbation
consistency (MSPC), which enforces a spatial perturbation function (T ) and the
translation operator (G) to be commutative (i.e., TG = GT ). In addition, we
introduce two adversarial training components for learning the spatial
perturbation function. The first one lets T compete with G to achieve maximum
perturbation. The second one lets G and T compete with discriminators to align
the spatial variations caused by the change of object size, object distortion,
background interruptions, etc. Our method outperforms the state-of-the-art
methods on most I2I benchmarks. We also introduce a new benchmark, namely the
front face to profile face dataset, to emphasize the underlying challenges of
I2I for real-world applications. We finally perform ablation experiments to
study the sensitivity of our method to the severity of spatial perturbation and
its effectiveness for distribution alignment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Challenges of Continuous Self-Supervised Learning. (arXiv:2203.12710v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12710">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning (SSL) aims to eliminate one of the major bottlenecks
in representation learning - the need for human annotations. As a result, SSL
holds the promise to learn representations from data in-the-wild, i.e., without
the need for finite and static datasets. Instead, true SSL algorithms should be
able to exploit the continuous stream of data being generated on the internet
or by agents exploring their environments. But do traditional self-supervised
learning approaches work in this setup? In this work, we investigate this
question by conducting experiments on the continuous self-supervised learning
problem. While learning in the wild, we expect to see a continuous (infinite)
non-IID data stream that follows a non-stationary distribution of visual
concepts. The goal is to learn a representation that can be robust, adaptive
yet not forgetful of concepts seen in the past. We show that a direct
application of current methods to such continuous setup is 1) inefficient both
computationally and in the amount of data required, 2) leads to inferior
representations due to temporal correlations (non-IID data) in some sources of
streaming data and 3) exhibits signs of catastrophic forgetting when trained on
sources with non-stationary data distributions. We propose the use of replay
buffers as an approach to alleviate the issues of inefficiency and temporal
correlations. We further propose a novel method to enhance the replay buffer by
maintaining the least redundant samples. Minimum redundancy (MinRed) buffers
allow us to learn effective representations even in the most challenging
streaming scenarios composed of sequential visual data obtained from a single
embodied agent, and alleviates the problem of catastrophic forgetting when
learning from data with non-stationary semantic distributions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What to Hide from Your Students: Attention-Guided Masked Image Modeling. (arXiv:2203.12719v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12719">
<div class="article-summary-box-inner">
<span><p>Transformers and masked language modeling are quickly being adopted and
explored in computer vision as vision transformers and masked image modeling
(MIM). In this work, we argue that image token masking is fundamentally
different from token masking in text, due to the amount and correlation of
tokens in an image. In particular, to generate a challenging pretext task for
MIM, we advocate a shift from random masking to informed masking. We develop
and exhibit this idea in the context of distillation-based MIM, where a teacher
transformer encoder generates an attention map, which we use to guide masking
for the student encoder. We thus introduce a novel masking strategy, called
attention-guided masking (AttMask), and we demonstrate its effectiveness over
random masking for dense distillation-based MIM as well as plain
distillation-based self-supervised learning on classification tokens. We
confirm that AttMask accelerates the learning process and improves the
performance on a variety of downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UMT: Unified Multi-modal Transformers for Joint Video Moment Retrieval and Highlight Detection. (arXiv:2203.12745v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12745">
<div class="article-summary-box-inner">
<span><p>Finding relevant moments and highlights in videos according to natural
language queries is a natural and highly valuable common need in the current
video content explosion era. Nevertheless, jointly conducting moment retrieval
and highlight detection is an emerging research topic, even though its
component problems and some related tasks have already been studied for a
while. In this paper, we present the first unified framework, named Unified
Multi-modal Transformers (UMT), capable of realizing such joint optimization
while can also be easily degenerated for solving individual problems. As far as
we are aware, this is the first scheme to integrate multi-modal (visual-audio)
learning for either joint optimization or the individual moment retrieval task,
and tackles moment retrieval as a keypoint detection problem using a novel
query generator and query decoder. Extensive comparisons with existing methods
and ablation studies on QVHighlights, Charades-STA, YouTube Highlights, and
TVSum datasets demonstrate the effectiveness, superiority, and flexibility of
the proposed method under various settings. Source code and pre-trained models
are available at https://github.com/TencentARC/UMT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multidimensional Belief Quantification for Label-Efficient Meta-Learning. (arXiv:2203.12768v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12768">
<div class="article-summary-box-inner">
<span><p>Optimization-based meta-learning offers a promising direction for few-shot
learning that is essential for many real-world computer vision applications.
However, learning from few samples introduces uncertainty, and quantifying
model confidence for few-shot predictions is essential for many critical
domains. Furthermore, few-shot tasks used in meta training are usually sampled
randomly from a task distribution for an iterative model update, leading to
high labeling costs and computational overhead in meta-training. We propose a
novel uncertainty-aware task selection model for label efficient meta-learning.
The proposed model formulates a multidimensional belief measure, which can
quantify the known uncertainty and lower bound the unknown uncertainty of any
given task. Our theoretical result establishes an important relationship
between the conflicting belief and the incorrect belief. The theoretical result
allows us to estimate the total uncertainty of a task, which provides a
principled criterion for task selection. A novel multi-query task formulation
is further developed to improve both the computational and labeling efficiency
of meta-learning. Experiments conducted over multiple real-world few-shot image
classification tasks demonstrate the effectiveness of the proposed model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Motion-Dependent Appearance for High-Fidelity Rendering of Dynamic Humans from a Single Camera. (arXiv:2203.12780v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12780">
<div class="article-summary-box-inner">
<span><p>Appearance of dressed humans undergoes a complex geometric transformation
induced not only by the static pose but also by its dynamics, i.e., there
exists a number of cloth geometric configurations given a pose depending on the
way it has moved. Such appearance modeling conditioned on motion has been
largely neglected in existing human rendering methods, resulting in rendering
of physically implausible motion. A key challenge of learning the dynamics of
the appearance lies in the requirement of a prohibitively large amount of
observations. In this paper, we present a compact motion representation by
enforcing equivariance -- a representation is expected to be transformed in the
way that the pose is transformed. We model an equivariant encoder that can
generate the generalizable representation from the spatial and temporal
derivatives of the 3D body surface. This learned representation is decoded by a
compositional multi-task decoder that renders high fidelity time-varying
appearance. Our experiments show that our method can generate a temporally
coherent video of dynamic humans for unseen body poses and novel views given a
single view video.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When Accuracy Meets Privacy: Two-Stage Federated Transfer Learning Framework in Classification of Medical Images on Limited Data: A COVID-19 Case Study. (arXiv:2203.12803v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12803">
<div class="article-summary-box-inner">
<span><p>COVID-19 pandemic has spread rapidly and caused a shortage of global medical
resources. The efficiency of COVID-19 diagnosis has become highly significant.
As deep learning and convolutional neural network (CNN) has been widely
utilized and been verified in analyzing medical images, it has become a
powerful tool for computer-assisted diagnosis. However, there are two most
significant challenges in medical image classification with the help of deep
learning and neural networks, one of them is the difficulty of acquiring enough
samples, which may lead to model overfitting. Privacy concerns mainly bring the
other challenge since medical-related records are often deemed patients'
private information and protected by laws such as GDPR and HIPPA. Federated
learning can ensure the model training is decentralized on different devices
and no data is shared among them, which guarantees privacy. However, with data
located on different devices, the accessible data of each device could be
limited. Since transfer learning has been verified in dealing with limited data
with good performance, therefore, in this paper, We made a trial to implement
federated learning and transfer learning techniques using CNNs to classify
COVID-19 using lung CT scans. We also explored the impact of dataset
distribution at the client-side in federated learning and the number of
training epochs a model is trained. Finally, we obtained very high performance
with federated learning, demonstrating our success in leveraging accuracy and
privacy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Simultaneous Learning for Camera Re-Localization and Depth Estimation from Video. (arXiv:2203.12804v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12804">
<div class="article-summary-box-inner">
<span><p>We present an unsupervised simultaneous learning framework for the task of
monocular camera re-localization and depth estimation from unlabeled video
sequences. Monocular camera re-localization refers to the task of estimating
the absolute camera pose from an instance image in a known environment, which
has been intensively studied for alternative localization in GPS-denied
environments. In recent works, camera re-localization methods are trained via
supervised learning from pairs of camera images and camera poses. In contrast
to previous works, we propose a completely unsupervised learning framework for
camera re-localization and depth estimation, requiring only monocular video
sequences for training. In our framework, we train two networks that estimate
the scene coordinates using directions and the depth map from each image which
are then combined to estimate the camera pose. The networks can be trained
through the minimization of loss functions based on our loop closed view
synthesis. In experiments with the 7-scenes dataset, the proposed method
outperformed the re-localization of the state-of-the-art visual SLAM,
ORB-SLAM3. Our method also outperforms state-of-the-art monocular depth
estimation in a trained environment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Efficient and Elastic Visual Question Answering with Doubly Slimmable Transformer. (arXiv:2203.12814v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12814">
<div class="article-summary-box-inner">
<span><p>Transformer-based approaches have shown great success in visual question
answering (VQA). However, they usually require deep and wide models to
guarantee good performance, making it difficult to deploy on
capacity-restricted platforms. It is a challenging yet valuable task to design
an elastic VQA model that supports adaptive pruning at runtime to meet the
efficiency constraints of diverse platforms. In this paper, we present the
Doubly Slimmable Transformer (DST), a general framework that can be seamlessly
integrated into arbitrary Transformer-based VQA models to train one single
model once and obtain various slimmed submodels of different widths and depths.
Taking two typical Transformer-based VQA approaches, i.e., MCAN and UNITER, as
the reference models, the obtained slimmable MCAN_DST and UNITER_DST models
outperform the state-of-the-art methods trained independently on two benchmark
datasets. In particular, one slimmed MCAN_DST submodel achieves a comparable
accuracy on VQA-v2, while being 0.38x smaller in model size and having 0.27x
fewer FLOPs than the reference MCAN model. The smallest MCAN_DST submodel has
9M parameters and 0.16G FLOPs in the inference stage, making it possible to be
deployed on edge devices.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ViT-FOD: A Vision Transformer based Fine-grained Object Discriminator. (arXiv:2203.12816v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12816">
<div class="article-summary-box-inner">
<span><p>Recently, several Vision Transformer (ViT) based methods have been proposed
for Fine-Grained Visual Classification (FGVC).These methods significantly
surpass existing CNN-based ones, demonstrating the effectiveness of ViT in FGVC
tasks.However, there are some limitations when applying ViT directly to
FGVC.First, ViT needs to split images into patches and calculate the attention
of every pair, which may result in heavy redundant calculation and unsatisfying
performance when handling fine-grained images with complex background and small
objects.Second, a standard ViT only utilizes the class token in the final layer
for classification, which is not enough to extract comprehensive fine-grained
information. To address these issues, we propose a novel ViT based fine-grained
object discriminator for FGVC tasks, ViT-FOD for short. Specifically, besides a
ViT backbone, it further introduces three novel components, i.e, Attention
Patch Combination (APC), Critical Regions Filter (CRF), and Complementary
Tokens Integration (CTI). Thereinto, APC pieces informative patches from two
images to generate a new image so that the redundant calculation can be
reduced. CRF emphasizes tokens corresponding to discriminative regions to
generate a new class token for subtle feature learning. To extract
comprehensive information, CTI integrates complementary information captured by
class tokens in different ViT layers. We conduct comprehensive experiments on
widely used datasets and the results demonstrate that ViT-FOD is able to
achieve state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Random Forest Regression for continuous affect using Facial Action Units. (arXiv:2203.12818v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12818">
<div class="article-summary-box-inner">
<span><p>In this paper we describe our approach to the arousal and valence track of
the 3rd Workshop and Competition on Affective Behavior Analysis in-the-wild
(ABAW). We extracted facial features using OpenFace and used them to train a
multiple output random forest regressor. Our approach performed comparable to
the baseline approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Subjective and Objective Analysis of Streamed Gaming Videos. (arXiv:2203.12824v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12824">
<div class="article-summary-box-inner">
<span><p>The rising popularity of online User-Generated-Content (UGC) in the form of
streamed and shared videos, has hastened the development of perceptual Video
Quality Assessment (VQA) models, which can be used to help optimize their
delivery. Gaming videos, which are a relatively new type of UGC videos, are
created when skilled gamers post videos of their gameplay. These kinds of
screenshots of UGC gameplay videos have become extremely popular on major
streaming platforms like YouTube and Twitch. Synthetically-generated gaming
content presents challenges to existing VQA algorithms, including those based
on natural scene/video statistics models. Synthetically generated gaming
content presents different statistical behavior than naturalistic videos. A
number of studies have been directed towards understanding the perceptual
characteristics of professionally generated gaming videos arising in gaming
video streaming, online gaming, and cloud gaming. However, little work has been
done on understanding the quality of UGC gaming videos, and how it can be
characterized and predicted. Towards boosting the progress of gaming video VQA
model development, we conducted a comprehensive study of subjective and
objective VQA models on UGC gaming videos. To do this, we created a novel UGC
gaming video resource, called the LIVE-YouTube Gaming video quality
(LIVE-YT-Gaming) database, comprised of 600 real UGC gaming videos. We
conducted a subjective human study on this data, yielding 18,600 human quality
ratings recorded by 61 human subjects. We also evaluated a number of
state-of-the-art (SOTA) VQA models on the new database, including a new one,
called GAME-VQP, based on both natural video statistics and CNN-learned
features. To help support work in this field, we are making the new
LIVE-YT-Gaming Database, publicly available through the link:
https://live.ece.utexas.edu/research/LIVE-YT-Gaming/index.html .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HMFS: Hybrid Masking for Few-Shot Segmentation. (arXiv:2203.12826v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12826">
<div class="article-summary-box-inner">
<span><p>We study few-shot semantic segmentation that aims to segment a target object
from a query image when provided with a few annotated support images of the
target class. Several recent methods resort to a feature masking (FM)
technique, introduced by [1], to discard irrelevant feature activations to
facilitate reliable segmentation mask prediction. A fundamental limitation of
FM is the inability to preserve the fine-grained spatial details that affect
the accuracy of segmentation mask, especially for small target objects. In this
paper, we develop a simple, effective, and efficient approach to enhance
feature masking (FM). We dub the enhanced FM as hybrid masking (HM).
Specifically, we compensate for the loss of fine-grained spatial details in FM
technique by investigating and leveraging a complementary basic input masking
method [2]. To validate the effectiveness of HM, we instantiate it into a
strong baseline [3], and coin the resulting framework as HMFS. Experimental
results on three publicly available benchmarks reveal that HMFS outperforms the
current state-of-the-art methods by visible margins.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse Instance Activation for Real-Time Instance Segmentation. (arXiv:2203.12827v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12827">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a conceptually novel, efficient, and fully
convolutional framework for real-time instance segmentation. Previously, most
instance segmentation methods heavily rely on object detection and perform mask
prediction based on bounding boxes or dense centers. In contrast, we propose a
sparse set of instance activation maps, as a new object representation, to
highlight informative regions for each foreground object. Then instance-level
features are obtained by aggregating features according to the highlighted
regions for recognition and segmentation. Moreover, based on bipartite
matching, the instance activation maps can predict objects in a one-to-one
style, thus avoiding non-maximum suppression (NMS) in post-processing. Owing to
the simple yet effective designs with instance activation maps, SparseInst has
extremely fast inference speed and achieves 40 FPS and 37.9 AP on the COCO
benchmark, which significantly outperforms the counterparts in terms of speed
and accuracy. Code and models are available at
https://github.com/hustvl/SparseInst.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AIMusicGuru: Music Assisted Human Pose Correction. (arXiv:2203.12829v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12829">
<div class="article-summary-box-inner">
<span><p>Pose Estimation techniques rely on visual cues available through observations
represented in the form of pixels. But the performance is bounded by the frame
rate of the video and struggles from motion blur, occlusions, and temporal
coherence. This issue is magnified when people are interacting with objects and
instruments, for example playing the violin. Standard approaches for
postprocessing use interpolation and smoothing functions to filter noise and
fill gaps, but they cannot model highly non-linear motion. We present a method
that leverages our understanding of the high degree of a causal relationship
between the sound produced and the motion that produces them. We use the audio
signature to refine and predict accurate human body pose motion models. We
propose MAPnet (Music Assisted Pose network) for generating a fine grain motion
model from sparse input pose sequences but continuous audio. To accelerate
further research in this domain, we also open-source MAPdat, a new multi-modal
dataset of 3D violin playing motion with music. We perform a comparison of
different standard machine learning models and perform analysis on input
modalities, sampling techniques, and audio and motion features. Experiments on
MAPdat suggest multi-modal approaches like ours as a promising direction for
tasks previously approached with visual methods only. Our results show both
qualitatively and quantitatively how audio can be combined with visual
observation to help improve any pose estimation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Industrial Style Transfer with Large-scale Geometric Warping and Content Preservation. (arXiv:2203.12835v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12835">
<div class="article-summary-box-inner">
<span><p>We propose a novel style transfer method to quickly create a new visual
product with a nice appearance for industrial designers' reference. Given a
source product, a target product, and an art style image, our method produces a
neural warping field that warps the source shape to imitate the geometric style
of the target and a neural texture transformation network that transfers the
artistic style to the warped source product. Our model, Industrial Style
Transfer (InST), consists of large-scale geometric warping (LGW) and
interest-consistency texture transfer (ICTT). LGW aims to explore an
unsupervised transformation between the shape masks of the source and target
products for fitting large-scale shape warping. Furthermore, we introduce a
mask smoothness regularization term to prevent the abrupt changes of the
details of the source product. ICTT introduces an interest regularization term
to maintain important contents of the warped product when it is stylized by
using the art style image. Extensive experimental results demonstrate that InST
achieves state-of-the-art performance on multiple visual product design tasks,
e.g., companies' snail logos and classical bottles (please see Fig. 1). To the
best of our knowledge, we are the first to extend the neural style transfer
method to create industrial product appearances. Project page:
\ulr{https://jcyang98.github.io/InST/home.html}. Code available at:
\url{https://github.com/jcyang98/InST}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bayesian Nonparametric Submodular Video Partition for Robust Anomaly Detection. (arXiv:2203.12840v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12840">
<div class="article-summary-box-inner">
<span><p>Multiple-instance learning (MIL) provides an effective way to tackle the
video anomaly detection problem by modeling it as a weakly supervised problem
as the labels are usually only available at the video level while missing for
frames due to expensive labeling cost. We propose to conduct novel Bayesian
non-parametric submodular video partition (BN-SVP) to significantly improve MIL
model training that can offer a highly reliable solution for robust anomaly
detection in practical settings that include outlier segments or multiple types
of abnormal events. BN-SVP essentially performs dynamic non-parametric
hierarchical clustering with an enhanced self-transition that groups segments
in a video into temporally consistent and semantically coherent hidden states
that can be naturally interpreted as scenes. Each segment is assumed to be
generated through a non-parametric mixture process that allows variations of
segments within the same scenes to accommodate the dynamic and noisy nature of
many real-world surveillance videos. The scene and mixture component assignment
of BN-SVP also induces a pairwise similarity among segments, resulting in
non-parametric construction of a submodular set function. Integrating this
function with an MIL loss effectively exposes the model to a diverse set of
potentially positive instances to improve its training. A greedy algorithm is
developed to optimize the submodular function and support efficient model
training. Our theoretical analysis ensures a strong performance guarantee of
the proposed algorithm. The effectiveness of the proposed approach is
demonstrated over multiple real-world anomaly video datasets with robust
detection performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Steganalysis of Image with Adaptively Parametric Activation. (arXiv:2203.12843v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12843">
<div class="article-summary-box-inner">
<span><p>Steganalysis as a method to detect whether image contains se-cret message, is
a crucial study avoiding the imperils from abus-ing steganography. The point of
steganalysis is to detect the weak embedding signals which is hardly learned by
convolution-al layer and easily suppressed. In this paper, to enhance
embed-ding signals, we study the insufficiencies of activation function,
filters and loss function from the aspects of reduce embedding signal loss and
enhance embedding signal capture ability. Adap-tive Parametric Activation
Module is designed to reserve nega-tive embedding signal. For embedding signal
capture ability enhancement, we add constraints on the high-pass filters to
im-prove residual diversity which enables the filters extracts rich embedding
signals. Besides, a loss function based on contrastive learning is applied to
overcome the limitations of cross-entropy loss by maximum inter-class distance.
It helps the network make a distinction between embedding signals and semantic
edges. We use images from BOSSbase 1.01 and make stegos by WOW and S-UNIWARD
for experiments. Compared to state-of-the-art methods, our method has a
competitive performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiple Emotion Descriptors Estimation at the ABAW3 Challenge. (arXiv:2203.12845v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12845">
<div class="article-summary-box-inner">
<span><p>To describe complex emotional states, psychologists have proposed multiple
emotion descriptors: sparse descriptors like facial action units; continuous
descriptors like valence and arousal; and discrete class descriptors like
happiness and anger. According to Ekman and Friesen, 1969, facial action units
are sign vehicles that convey the emotion message, while discrete or continuous
emotion descriptors are the messages perceived and expressed by human.
</p>
<p>In this paper, we designed an architecture for multiple emotion descriptors
estimation in participating the ABAW3 Challenge. Based on the theory of Ekman
and Friesen, 1969, we designed distinct architectures to measure the sign
vehicles (i.e., facial action units) and the message (i.e., discrete emotions,
valence and arousal) given their different properties. The quantitative
experiments on the ABAW3 challenge dataset has shown the superior performance
of our approach over two baseline models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Keypoints Tracking via Transformer Networks. (arXiv:2203.12848v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12848">
<div class="article-summary-box-inner">
<span><p>In this thesis, we propose a pioneering work on sparse keypoints tracking
across images using transformer networks. While deep learning-based keypoints
matching have been widely investigated using graph neural networks - and more
recently transformer networks, they remain relatively too slow to operate in
real-time and are particularly sensitive to the poor repeatability of the
keypoints detectors. In order to address these shortcomings, we propose to
study the particular case of real-time and robust keypoints tracking.
Specifically, we propose a novel architecture which ensures a fast and robust
estimation of the keypoints tracking between successive images of a video
sequence. Our method takes advantage of a recent breakthrough in computer
vision, namely, visual transformer networks. Our method consists of two
successive stages, a coarse matching followed by a fine localization of the
keypoints' correspondences prediction. Through various experiments, we
demonstrate that our approach achieves competitive results and demonstrates
high robustness against adverse conditions, such as illumination change,
occlusion and viewpoint differences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Image Manipulation with Background-guided Internal Learning. (arXiv:2203.12849v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12849">
<div class="article-summary-box-inner">
<span><p>Image manipulation has attracted a lot of interest due to its wide range of
applications. Prior work modifies images either from low-level manipulation,
such as image inpainting or through manual edits via paintbrushes and
scribbles, or from high-level manipulation, employing deep generative networks
to output an image conditioned on high-level semantic input. In this study, we
propose Semantic Image Manipulation with Background-guided Internal Learning
(SIMBIL), which combines high-level and low-level manipulation. Specifically,
users can edit an image at the semantic level by applying changes on a scene
graph. Then our model manipulates the image at the pixel level according to the
modified scene graph. There are two major advantages of our approach. First,
high-level manipulation of scene graphs requires less manual effort from the
user compared to manipulating raw image pixels. Second, our low-level internal
learning approach is scalable to images of various sizes without reliance on
external visual datasets for training. We outperform the state-of-the-art in a
quantitative and qualitative evaluation on the CLEVR and Visual Genome
datasets. Experiments show 8 points improvement on FID scores (CLEVR) and 27%
improvement on user evaluation (Visual Genome), demonstrating the effectiveness
of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Direct evaluation of progression or regression of disease burden in brain metastatic disease with Deep Neuroevolution. (arXiv:2203.12853v1 [cs.NE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12853">
<div class="article-summary-box-inner">
<span><p>Purpose: A core component of advancing cancer treatment research is assessing
response to therapy. Doing so by hand, for example as per RECIST or RANO
criteria, is tedious, time-consuming, and can miss important tumor response
information; most notably, they exclude non-target lesions. We wish to assess
change in a holistic fashion that includes all lesions, obtaining simple,
informative, and automated assessments of tumor progression or regression. Due
to often low patient enrolments in clinical trials, we wish to make response
assessments with small training sets. Deep neuroevolution (DNE) can produce
radiology artificial intelligence (AI) that performs well on small training
sets. Here we use DNE for function approximation that predicts progression
versus regression of metastatic brain disease.
</p>
<p>Methods: We analyzed 50 pairs of MRI contrast-enhanced images as our training
set. Half of these pairs, separated in time, qualified as disease progression,
while the other 25 images constituted regression. We trained the parameters of
a relatively small CNN via mutations that consisted of random CNN weight
adjustments and mutation fitness. We then incorporated the best mutations into
the next generations CNN, repeating this process for approximately 50,000
generations. We applied the CNNs to our training set, as well as a separate
testing set with the same class balance of 25 progression and 25 regression
images.
</p>
<p>Results: DNE achieved monotonic convergence to 100% training set accuracy.
DNE also converged monotonically to 100% testing set accuracy.
</p>
<p>Conclusion: DNE can accurately classify brain-metastatic disease progression
versus regression. Future work will extend the input from 2D image slices to
full 3D volumes, and include the category of no change. We believe that an
approach such as our could ultimately provide a useful adjunct to RANO/RECIST
assessment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Fixation: Dynamic Window Visual Transformer. (arXiv:2203.12856v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12856">
<div class="article-summary-box-inner">
<span><p>Recently, a surge of interest in visual transformers is to reduce the
computational cost by limiting the calculation of self-attention to a local
window. Most current work uses a fixed single-scale window for modeling by
default, ignoring the impact of window size on model performance. However, this
may limit the modeling potential of these window-based models for multi-scale
information. In this paper, we propose a novel method, named Dynamic Window
Vision Transformer (DW-ViT). The dynamic window strategy proposed by DW-ViT
goes beyond the model that employs a fixed single window setting. To the best
of our knowledge, we are the first to use dynamic multi-scale windows to
explore the upper limit of the effect of window settings on model performance.
In DW-ViT, multi-scale information is obtained by assigning windows of
different sizes to different head groups of window multi-head self-attention.
Then, the information is dynamically fused by assigning different weights to
the multi-scale window branches. We conducted a detailed performance evaluation
on three datasets, ImageNet-1K, ADE20K, and COCO. Compared with related
state-of-the-art (SoTA) methods, DW-ViT obtains the best performance.
Specifically, compared with the current SoTA Swin Transformers
\cite{liu2021swin}, DW-ViT has achieved consistent and substantial improvements
on all three datasets with similar parameters and computational costs. In
addition, DW-ViT exhibits good scalability and can be easily inserted into any
window-based visual transformers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer Compressed Sensing via Global Image Tokens. (arXiv:2203.12861v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12861">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks (CNN) have demonstrated outstanding Compressed
Sensing (CS) performance compared to traditional, hand-crafted methods.
However, they are broadly limited in terms of generalisability, inductive bias
and difficulty to model long distance relationships. Transformer neural
networks (TNN) overcome such issues by implementing an attention mechanism
designed to capture dependencies between inputs. However, high-resolution tasks
typically require vision Transformers (ViT) to decompose an image into
patch-based tokens, limiting inputs to inherently local contexts. We propose a
novel image decomposition that naturally embeds images into low-resolution
inputs. These Kaleidoscope tokens (KD) provide a mechanism for global
attention, at the same computational cost as a patch-based approach. To
showcase this development, we replace CNN components in a well-known CS-MRI
neural network with TNN blocks and demonstrate the improvements afforded by KD.
We also propose an ensemble of image tokens, which enhance overall image
quality and reduces model size. Supplementary material is available:
https://github.com/uqmarlonbran/TCS.git}{https://github.com/uqmarlonbran/TCS.git
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DyRep: Bootstrapping Training with Dynamic Re-parameterization. (arXiv:2203.12868v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12868">
<div class="article-summary-box-inner">
<span><p>Structural re-parameterization (Rep) methods achieve noticeable improvements
on simple VGG-style networks. Despite the prevalence, current Rep methods
simply re-parameterize all operations into an augmented network, including
those that rarely contribute to the model's performance. As such, the price to
pay is an expensive computational overhead to manipulate these unnecessary
behaviors. To eliminate the above caveats, we aim to bootstrap the training
with minimal cost by devising a dynamic re-parameterization (DyRep) method,
which encodes Rep technique into the training process that dynamically evolves
the network structures. Concretely, our proposal adaptively finds the
operations which contribute most to the loss in the network, and applies Rep to
enhance their representational capacity. Besides, to suppress the noisy and
redundant operations introduced by Rep, we devise a de-parameterization
technique for a more compact re-parameterization. With this regard, DyRep is
more efficient than Rep since it smoothly evolves the given network instead of
constructing an over-parameterized network. Experimental results demonstrate
our effectiveness, e.g., DyRep improves the accuracy of ResNet-18 by $2.04\%$
on ImageNet and reduces $22\%$ runtime over the baseline. Code is available at:
https://github.com/hunto/DyRep.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RNNPose: Recurrent 6-DoF Object Pose Refinement with Robust Correspondence Field Estimation and Pose Optimization. (arXiv:2203.12870v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12870">
<div class="article-summary-box-inner">
<span><p>Direct estimating the 6-DoF object pose from a single color image is
challenging, and post-refinement is generally needed to achieve high-precision
estimation. In this paper, we propose a framework based on a recurrent neural
network (RNN) for object pose refinement, which is robust to erroneous initial
poses and occlusions. During the recurrent iterations, object pose refinement
is formulated as a non-linear least squares problem based on the estimated
correspondence field (between a rendered image and the observed image). The
problem is then solved by a differentiable Levenberg-Marquardt (LM) algorithm
for end-toend training. The correspondence field estimation and pose refinement
are conducted alternatively in each iteration to recover accurate object poses.
Furthermore, to improve the robustness to occlusions, we introduce a
consistencycheck mechanism based on the learned descriptors of the 3D model and
observed 2D image, which downweights the unreliable correspondences during pose
optimization. Extensive experiments on LINEMOD, Occlusion-LINEMOD, and
YCB-Video datasets validate the effectiveness of our method and demonstrate
state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Intrinsic Bias Identification on Medical Image Datasets. (arXiv:2203.12872v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12872">
<div class="article-summary-box-inner">
<span><p>Machine learning based medical image analysis highly depends on datasets.
Biases in the dataset can be learned by the model and degrade the
generalizability of the applications. There are studies on debiased models.
However, scientists and practitioners are difficult to identify implicit biases
in the datasets, which causes lack of reliable unbias test datasets to valid
models. To tackle this issue, we first define the data intrinsic bias
attribute, and then propose a novel bias identification framework for medical
image datasets. The framework contains two major components, KlotskiNet and
Bias Discriminant Direction Analysis(bdda), where KlostkiNet is to build the
mapping which makes backgrounds to distinguish positive and negative samples
and bdda provides a theoretical solution on determining bias attributes.
Experimental results on three datasets show the effectiveness of the bias
attributes discovered by the framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly-Supervised End-to-End CAD Retrieval to Scan Objects. (arXiv:2203.12873v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12873">
<div class="article-summary-box-inner">
<span><p>CAD model retrieval to real-world scene observations has shown strong promise
as a basis for 3D perception of objects and a clean, lightweight mesh-based
scene representation; however, current approaches to retrieve CAD models to a
query scan rely on expensive manual annotations of 1:1 associations of CAD-scan
objects, which typically contain strong lower-level geometric differences. We
thus propose a new weakly-supervised approach to retrieve semantically and
structurally similar CAD models to a query 3D scanned scene without requiring
any CAD-scan associations, and only object detection information as oriented
bounding boxes. Our approach leverages a fully-differentiable top-$k$ retrieval
layer, enabling end-to-end training guided by geometric and perceptual
similarity of the top retrieved CAD models to the scan queries. We demonstrate
that our weakly-supervised approach can outperform fully-supervised retrieval
methods on challenging real-world ScanNet scans, and maintain robustness for
unseen class categories, achieving significantly improved performance over
fully-supervised state of the art in zero-shot CAD retrieval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Ensemble Approach for Facial Expression Analysis in Video. (arXiv:2203.12891v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12891">
<div class="article-summary-box-inner">
<span><p>Human emotions recognization contributes to the development of human-computer
interaction. The machines understanding human emotions in the real world will
significantly contribute to life in the future. This paper will introduce the
Affective Behavior Analysis in-the-wild (ABAW3) 2022 challenge. The paper
focuses on solving the problem of the valence-arousal estimation and action
unit detection. For valence-arousal estimation, we conducted two stages:
creating new features from multimodel and temporal learning to predict
valence-arousal. First, we make new features; the Gated Recurrent Unit (GRU)
and Transformer are combined using a Regular Networks (RegNet) feature, which
is extracted from the image. The next step is the GRU combined with Local
Attention to predict valence-arousal. The Concordance Correlation Coefficient
(CCC) was used to evaluate the model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Making Heads or Tails: Towards Semantically Consistent Visual Counterfactuals. (arXiv:2203.12892v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12892">
<div class="article-summary-box-inner">
<span><p>A visual counterfactual explanation replaces image regions in a query image
with regions from a distractor image such that the system's decision on the
transformed image changes to the distractor class. In this work, we present a
novel framework for computing visual counterfactual explanations based on two
key ideas. First, we enforce that the \textit{replaced} and \textit{replacer}
regions contain the same semantic part, resulting in more semantically
consistent explanations. Second, we use multiple distractor images in a
computationally efficient way and obtain more discriminative explanations with
fewer region replacements. Our approach is $\mathbf{27\%}$ more semantically
consistent and an order of magnitude faster than a competing method on three
fine-grained image recognition datasets. We highlight the utility of our
counterfactuals over existing works through machine teaching experiments where
we teach humans to classify different bird species. We also complement our
explanations with the vocabulary of parts and attributes that contributed the
most to the system's decision. In this task as well, we obtain state-of-the-art
results when using our counterfactual explanations relative to existing works,
reinforcing the importance of semantically consistent explanations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FAMLP: A Frequency-Aware MLP-Like Architecture For Domain Generalization. (arXiv:2203.12893v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12893">
<div class="article-summary-box-inner">
<span><p>MLP-like models built entirely upon multi-layer perceptrons have recently
been revisited, exhibiting the comparable performance with transformers. It is
one of most promising architectures due to the excellent trade-off between
network capability and efficiency in the large-scale recognition tasks.
However, its generalization performance to heterogeneous tasks is inferior to
other architectures (e.g., CNNs and transformers) due to the extensive
retention of domain information. To address this problem, we propose a novel
frequency-aware MLP architecture, in which the domain-specific features are
filtered out in the transformed frequency domain, augmenting the invariant
descriptor for label prediction. Specifically, we design an adaptive Fourier
filter layer, in which a learnable frequency filter is utilized to adjust the
amplitude distribution by optimizing both the real and imaginary parts. A
low-rank enhancement module is further proposed to rectify the filtered
features by adding the low-frequency components from SVD decomposition.
Finally, a momentum update strategy is utilized to stabilize the optimization
to fluctuation of model parameters and inputs by the output distillation with
weighted historical states. To our best knowledge, we are the first to propose
a MLP-like backbone for domain generalization. Extensive experiments on three
benchmarks demonstrate significant generalization performance, outperforming
the state-of-the-art methods by a margin of 3%, 4% and 9%, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Expression Classification using Concatenation of Deep Neural Network for the 3rd ABAW3 Competition. (arXiv:2203.12899v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12899">
<div class="article-summary-box-inner">
<span><p>For computers to recognize human emotions, expression classification is an
equally important problem in the human-computer interaction area. In the 3rd
Affective Behavior Analysis In-The-Wild competition, the task of expression
classification includes 8 classes including 6 basic expressions of human faces
from videos. In this paper, we perform combination representation from RegNet,
Attention module, and Transformer Encoder for the expression classification
task. We achieve 35.87 \% for F1-score on the validation set of Aff-Wild2
dataset. This result shows the effectiveness of the proposed architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Privileged Attribution Constrained Deep Networks for Facial Expression Recognition. (arXiv:2203.12905v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12905">
<div class="article-summary-box-inner">
<span><p>Facial Expression Recognition (FER) is crucial in many research domains
because it enables machines to better understand human behaviours. FER methods
face the problems of relatively small datasets and noisy data that don't allow
classical networks to generalize well. To alleviate these issues, we guide the
model to concentrate on specific facial areas like the eyes, the mouth or the
eyebrows, which we argue are decisive to recognise facial expressions. We
propose the Privileged Attribution Loss (PAL), a method that directs the
attention of the model towards the most salient facial regions by encouraging
its attribution maps to correspond to a heatmap formed by facial landmarks.
Furthermore, we introduce several channel strategies that allow the model to
have more degrees of freedom. The proposed method is independent of the
backbone architecture and doesn't need additional semantic information at test
time. Finally, experimental results show that the proposed PAL method
outperforms current state-of-the-art methods on both RAF-DB and AffectNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Reflectance for Shape Recovery with Shadow Handling. (arXiv:2203.12909v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12909">
<div class="article-summary-box-inner">
<span><p>This paper aims at recovering the shape of a scene with unknown,
non-Lambertian, and possibly spatially-varying surface materials. When the
shape of the object is highly complex and that shadows cast on the surface, the
task becomes very challenging. To overcome these challenges, we propose a
coordinate-based deep MLP (multilayer perceptron) to parameterize both the
unknown 3D shape and the unknown reflectance at every surface point. This
network is able to leverage the observed photometric variance and shadows on
the surface, and recover both surface shape and general non-Lambertian
reflectance. We explicitly predict cast shadows, mitigating possible artifacts
on these shadowing regions, leading to higher estimation accuracy. Our
framework is entirely self-supervised, in the sense that it requires neither
ground truth shape nor BRDF. Tests on real-world images demonstrate that our
method outperform existing methods by a significant margin. Thanks to the small
size of the MLP-net, our method is an order of magnitude faster than previous
CNN-based methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NPC: Neuron Path Coverage via Characterizing Decision Logic of Deep Neural Networks. (arXiv:2203.12915v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12915">
<div class="article-summary-box-inner">
<span><p>Deep learning has recently been widely applied to many applications across
different domains, e.g., image classification and audio recognition. However,
the quality of Deep Neural Networks (DNNs) still raises concerns in the
practical operational environment, which calls for systematic testing,
especially in safety-critical scenarios. Inspired by software testing, a number
of structural coverage criteria are designed and proposed to measure the test
adequacy of DNNs. However, due to the blackbox nature of DNN, the existing
structural coverage criteria are difficult to interpret, making it hard to
understand the underlying principles of these criteria. The relationship
between the structural coverage and the decision logic of DNNs is unknown.
Moreover, recent studies have further revealed the non-existence of correlation
between the structural coverage and DNN defect detection, which further posts
concerns on what a suitable DNN testing criterion should be.
</p>
<p>In this paper, we propose the interpretable coverage criteria through
constructing the decision structure of a DNN. Mirroring the control flow graph
of the traditional program, we first extract a decision graph from a DNN based
on its interpretation, where a path of the decision graph represents a decision
logic of the DNN. Based on the control flow and data flow of the decision
graph, we propose two variants of path coverage to measure the adequacy of the
test cases in exercising the decision logic. The higher the path coverage, the
more diverse decision logic the DNN is expected to be explored. Our large-scale
evaluation results demonstrate that: the path in the decision graph is
effective in characterizing the decision of the DNN, and the proposed coverage
criteria are also sensitive with errors including natural errors and
adversarial examples, and strongly correlated with the output impartiality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WarpingGAN: Warping Multiple Uniform Priors for Adversarial 3D Point Cloud Generation. (arXiv:2203.12917v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12917">
<div class="article-summary-box-inner">
<span><p>We propose WarpingGAN, an effective and efficient 3D point cloud generation
network. Unlike existing methods that generate point clouds by directly
learning the mapping functions between latent codes and 3D shapes, Warping-GAN
learns a unified local-warping function to warp multiple identical pre-defined
priors (i.e., sets of points uniformly distributed on regular 3D grids) into 3D
shapes driven by local structure-aware semantics. In addition, we also
ingeniously utilize the principle of the discriminator and tailor a stitching
loss to eliminate the gaps between different partitions of a generated shape
corresponding to different priors for boosting quality. Owing to the novel
generating mechanism, WarpingGAN, a single lightweight network after one-time
training, is capable of efficiently generating uniformly distributed 3D point
clouds with various resolutions. Extensive experimental results demonstrate the
superiority of our WarpingGAN over state-of-the-art methods in terms of
quantitative metrics, visual quality, and efficiency. The source code is
publicly available at https://github.com/yztang4/WarpingGAN.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Dense Correspondence from Synthetic Environments. (arXiv:2203.12919v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12919">
<div class="article-summary-box-inner">
<span><p>Estimation of human shape and pose from a single image is a challenging task.
It is an even more difficult problem to map the identified human shape onto a
3D human model. Existing methods map manually labelled human pixels in real 2D
images onto the 3D surface, which is prone to human error, and the sparsity of
available annotated data often leads to sub-optimal results. We propose to
solve the problem of data scarcity by training 2D-3D human mapping algorithms
using automatically generated synthetic data for which exact and dense 2D-3D
correspondence is known. Such a learning strategy using synthetic environments
has a high generalisation potential towards real-world data. Using different
camera parameter variations, background and lighting settings, we created
precise ground truth data that constitutes a wider distribution. We evaluate
the performance of models trained on synthetic using the COCO dataset and
validation framework. Results show that training 2D-3D mapping network models
on synthetic data is a viable alternative to using real data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Fixed Sub-Center: A Better Way to Capture Data Complexity. (arXiv:2203.12928v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12928">
<div class="article-summary-box-inner">
<span><p>Treating class with a single center may hardly capture data distribution
complexities. Using multiple sub-centers is an alternative way to address this
problem. However, highly correlated sub-classes, the classifier's parameters
grow linearly with the number of classes, and lack of intra-class compactness
are three typical issues that need to be addressed in existing multi-subclass
methods. To this end, we propose to use Fixed Sub-Center (F-SC), which allows
the model to create more discrepant sub-centers while saving memory and cutting
computational costs considerably. The F-SC specifically, first samples a class
center Ui for each class from a uniform distribution, and then generates a
normal distribution for each class, where the mean is equal to Ui. Finally, the
sub-centers are sampled based on the normal distribution corresponding to each
class, and the sub-centers are fixed during the training process avoiding the
overhead of gradient calculation. Moreover, F-SC penalizes the Euclidean
distance between the samples and their corresponding sub-centers, it helps
remain intra-compactness. The experimental results show that F-SC significantly
improves the accuracy of both image classification and fine-grained recognition
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Escaping from Language Bias and OCR Error: Semantics-Centered Text Visual Question Answering. (arXiv:2203.12929v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12929">
<div class="article-summary-box-inner">
<span><p>Texts in scene images convey critical information for scene understanding and
reasoning. The abilities of reading and reasoning matter for the model in the
text-based visual question answering (TextVQA) process. However, current
TextVQA models do not center on the text and suffer from several limitations.
The model is easily dominated by language biases and optical character
recognition (OCR) errors due to the absence of semantic guidance in the answer
prediction process. In this paper, we propose a novel Semantics-Centered
Network (SC-Net) that consists of an instance-level contrastive semantic
prediction module (ICSP) and a semantics-centered transformer module (SCT).
Equipped with the two modules, the semantics-centered model can resist the
language biases and the accumulated errors from OCR. Extensive experiments on
TextVQA and ST-VQA datasets show the effectiveness of our model. SC-Net
surpasses previous works with a noticeable margin and is more reasonable for
the TextVQA task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformers Meet Visual Learning Understanding: A Comprehensive Review. (arXiv:2203.12944v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12944">
<div class="article-summary-box-inner">
<span><p>Dynamic attention mechanism and global modeling ability make Transformer show
strong feature learning ability. In recent years, Transformer has become
comparable to CNNs methods in computer vision. This review mainly investigates
the current research progress of Transformer in image and video applications,
which makes a comprehensive overview of Transformer in visual learning
understanding. First, the attention mechanism is reviewed, which plays an
essential part in Transformer. And then, the visual Transformer model and the
principle of each module are introduced. Thirdly, the existing
Transformer-based models are investigated, and their performance is compared in
visual learning understanding applications. Three image tasks and two video
tasks of computer vision are investigated. The former mainly includes image
classification, object detection, and image segmentation. The latter contains
object tracking and video classification. It is significant for comparing
different models' performance in various tasks on several public benchmark data
sets. Finally, ten general problems are summarized, and the developing
prospects of the visual Transformer are given in this review.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Focus-and-Detect: A Small Object Detection Framework for Aerial Images. (arXiv:2203.12976v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12976">
<div class="article-summary-box-inner">
<span><p>Despite recent advances, object detection in aerial images is still a
challenging task. Specific problems in aerial images makes the detection
problem harder, such as small objects, densely packed objects, objects in
different sizes and with different orientations. To address small object
detection problem, we propose a two-stage object detection framework called
"Focus-and-Detect". The first stage which consists of an object detector
network supervised by a Gaussian Mixture Model, generates clusters of objects
constituting the focused regions. The second stage, which is also an object
detector network, predicts objects within the focal regions. Incomplete Box
Suppression (IBS) method is also proposed to overcome the truncation effect of
region search approach. Results indicate that the proposed two-stage framework
achieves an AP score of 42.06 on VisDrone validation dataset, surpassing all
other state-of-the-art small object detection methods reported in the
literature, to the best of authors' knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is Geometry Enough for Matching in Visual Localization?. (arXiv:2203.12979v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12979">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose to go beyond the well-established approach to
vision-based localization that relies on visual descriptor matching between a
query image and a 3D point cloud. While matching keypoints via visual
descriptors makes localization highly accurate, it has significant storage
demands, raises privacy concerns and increases map maintenance complexity. To
elegantly address those practical challenges for large-scale localization, we
present GoMatch, an alternative to visual-based matching that solely relies on
geometric information for matching image keypoints to maps, represented as sets
of bearing vectors. Our novel bearing vectors representation of 3D points,
significantly relieves the cross-domain challenge in geometric-based matching
that prevented prior work to tackle localization in a realistic environment.
With additional careful architecture design, GoMatch improves over prior
geometric-based matching work with a reduction of ($10.67m, 95.7^{\circ}$) and
($1.43m$, $34.7^{\circ}$) in average median pose errors on Cambridge Landmarks
and 7-Scenes, while requiring as little as $1.5/1.7\%$ of storage capacity in
comparison to the best visual-based matching methods. This confirms its
potential and feasibility for real-world localization and opens the door to
future efforts in advancing city-scale visual localization methods that do not
require storing visual descriptors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Disentangled Representation for One-shot Progressive Face Swapping. (arXiv:2203.12985v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12985">
<div class="article-summary-box-inner">
<span><p>Although face swapping has attracted much attention in recent years, it
remains a challenging problem. The existing methods leverage a large number of
data samples to explore the intrinsic properties of face swapping without
taking into account the semantic information of face images. Moreover, the
representation of the identity information tends to be fixed, leading to
suboptimal face swapping. In this paper, we present a simple yet efficient
method named FaceSwapper, for one-shot face swapping based on Generative
Adversarial Networks. Our method consists of a disentangled representation
module and a semantic-guided fusion module. The disentangled representation
module is composed of an attribute encoder and an identity encoder, which aims
to achieve the disentanglement of the identity and the attribute information.
The identity encoder is more flexible and the attribute encoder contains more
details of the attributes than its competitors. Benefiting from the
disentangled representation, FaceSwapper can swap face images progressively. In
addition, semantic information is introduced into the semantic-guided fusion
module to control the swapped area and model the pose and expression more
accurately. The experimental results show that our method achieves
state-of-the-art results on benchmark datasets with fewer training samples. Our
code is publicly available at https://github.com/liqi-casia/FaceSwapper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Nearest Neighbor Graph Embedding for Efficient Dimensionality Reduction. (arXiv:2203.12997v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12997">
<div class="article-summary-box-inner">
<span><p>Dimensionality reduction is crucial both for visualization and preprocessing
high dimensional data for machine learning. We introduce a novel method based
on a hierarchy built on 1-nearest neighbor graphs in the original space which
is used to preserve the grouping properties of the data distribution on
multiple levels. The core of the proposal is an optimization-free projection
that is competitive with the latest versions of t-SNE and UMAP in performance
and visualization quality while being an order of magnitude faster in run-time.
Furthermore, its interpretable mechanics, the ability to project new data, and
the natural separation of data clusters in visualizations make it a general
purpose unsupervised dimension reduction technique. In the paper, we argue
about the soundness of the proposed method and evaluate it on a diverse
collection of datasets with sizes varying from 1K to 11M samples and dimensions
from 28 to 16K. We perform comparisons with other state-of-the-art methods on
multiple metrics and target dimensions highlighting its efficiency and
performance. Code is available at https://github.com/koulakis/h-nne
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Deep-Discrete Learning Framework for Spherical Surface Registration. (arXiv:2203.12999v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12999">
<div class="article-summary-box-inner">
<span><p>Cortical surface registration is a fundamental tool for neuroimaging analysis
that has been shown to improve the alignment of functional regions relative to
volumetric approaches. Classically, image registration is performed by
optimizing a complex objective similarity function, leading to long run times.
This contributes to a convention for aligning all data to a global average
reference frame that poorly reflects the underlying cortical heterogeneity. In
this paper, we propose a novel unsupervised learning-based framework that
converts registration to a multi-label classification problem, where each point
in a low-resolution control grid deforms to one of fixed, finite number of
endpoints. This is learned using a spherical geometric deep learning
architecture, in an end-to-end unsupervised way, with regularization imposed
using a deep Conditional Random Field (CRF). Experiments show that our proposed
framework performs competitively, in terms of similarity and areal distortion,
relative to the most popular classical surface registration algorithms and
generates smoother deformations than other learning-based surface registration
methods, even in subjects with atypical cortical morphology.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compound Domain Generalization via Meta-Knowledge Encoding. (arXiv:2203.13006v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13006">
<div class="article-summary-box-inner">
<span><p>Domain generalization (DG) aims to improve the generalization performance for
an unseen target domain by using the knowledge of multiple seen source domains.
Mainstream DG methods typically assume that the domain label of each source
sample is known a priori, which is challenged to be satisfied in many
real-world applications. In this paper, we study a practical problem of
compound DG, which relaxes the discrete domain assumption to the mixed source
domains setting. On the other hand, current DG algorithms prioritize the focus
on semantic invariance across domains (one-vs-one), while paying less attention
to the holistic semantic structure (many-vs-many). Such holistic semantic
structure, referred to as meta-knowledge here, is crucial for learning
generalizable representations. To this end, we present Compound Domain
Generalization via Meta-Knowledge Encoding (COMEN), a general approach to
automatically discover and model latent domains in two steps. Firstly, we
introduce Style-induced Domain-specific Normalization (SDNorm) to re-normalize
the multi-modal underlying distributions, thereby dividing the mixture of
source domains into latent clusters. Secondly, we harness the prototype
representations, the centroids of classes, to perform relational modeling in
the embedding space with two parallel and complementary modules, which
explicitly encode the semantic structure for the out-of-distribution
generalization. Experiments on four standard DG benchmarks reveal that COMEN
exceeds the state-of-the-art performance without the need of domain
supervision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CVF-SID: Cyclic multi-Variate Function for Self-Supervised Image Denoising by Disentangling Noise from Image. (arXiv:2203.13009v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13009">
<div class="article-summary-box-inner">
<span><p>Recently, significant progress has been made on image denoising with strong
supervision from large-scale datasets. However, obtaining well-aligned
noisy-clean training image pairs for each specific scenario is complicated and
costly in practice. Consequently, applying a conventional supervised denoising
network on in-the-wild noisy inputs is not straightforward. Although several
studies have challenged this problem without strong supervision, they rely on
less practical assumptions and cannot be applied to practical situations
directly. To address the aforementioned challenges, we propose a novel and
powerful self-supervised denoising method called CVF-SID based on a Cyclic
multi-Variate Function (CVF) module and a self-supervised image disentangling
(SID) framework. The CVF module can output multiple decomposed variables of the
input and take a combination of the outputs back as an input in a cyclic
manner. Our CVF-SID can disentangle a clean image and noise maps from the input
by leveraging various self-supervised loss terms. Unlike several methods that
only consider the signal-independent noise models, we also deal with
signal-dependent noise components for real-world applications. Furthermore, we
do not rely on any prior assumptions about the underlying noise distribution,
making CVF-SID more generalizable toward realistic noise. Extensive experiments
on real-world datasets show that CVF-SID achieves state-of-the-art
self-supervised image denoising performance and is comparable to other existing
approaches. The code is publicly available from
https://github.com/Reyhanehne/CVF-SID_PyTorch .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continuous Emotion Recognition using Visual-audio-linguistic information: A Technical Report for ABAW3. (arXiv:2203.13031v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13031">
<div class="article-summary-box-inner">
<span><p>We propose a cross-modal co-attention model for continuous emotion
recognition using visual-audio-linguistic information. The model consists of
four blocks. The visual, audio, and linguistic blocks are used to learn the
spatial-temporal features of the multimodal input. A co-attention block is
designed to fuse the learned enbeddings with the multihead co-attention
mechanism. The visual encoding from the visual block is concatenated with the
attention feature to emphasize the visual information. To make full use of the
data and alleviate over-fitting, the cross-validation is carried out on the
training and validation set. The concordance correlation coefficient (CCC)
centering is used to merge the results from each fold. The achieved CCC on
validation set is 0.450 for valence and 0.651 for arousal, which significantly
outperforms the baseline method with the corresponding CCC of 0.310 and 0.170,
respectively. The code is available at https://github.com/sucv/ABAW3.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-modal Emotion Estimation for in-the-wild Videos. (arXiv:2203.13032v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13032">
<div class="article-summary-box-inner">
<span><p>In this paper, we briefly introduce our submission to the Valence-Arousal
Estimation Challenge of the 3rd Affective Behavior Analysis in-the-wild (ABAW)
competition. Our method utilizes the multi-modal information, i.e., the visual
and audio information, and employs a temporal encoder to model the temporal
context in the videos. Besides, a smooth processor is applied to get more
reasonable predictions, and a model ensemble strategy is used to improve the
performance of our proposed method. The experiment results show that our method
achieves 65.55% ccc for valence and 70.88% ccc for arousal on the validation
set of the Aff-Wild2 dataset, which prove the effectiveness of our proposed
method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpretable Prediction of Pulmonary Hypertension in Newborns using Echocardiograms. (arXiv:2203.13038v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13038">
<div class="article-summary-box-inner">
<span><p>Pulmonary hypertension (PH) in newborns and infants is a complex condition
associated with several pulmonary, cardiac, and systemic diseases contributing
to morbidity and mortality. Therefore, accurate and early detection of PH is
crucial for successful management. Using echocardiography, the primary
diagnostic tool in pediatrics, human assessment is both time-consuming and
expertise-demanding, raising the need for an automated approach. In this work,
we present an interpretable multi-view video-based deep learning approach to
predict PH for a cohort of 194 newborns using echocardiograms. We use
spatio-temporal convolutional architectures for the prediction of PH from each
view, and aggregate the predictions of the different views using majority
voting. To the best of our knowledge, this is the first work for an automated
assessment of PH in newborns using echocardiograms. Our results show a mean
F1-score of 0.84 for severity prediction and 0.92 for binary detection using
10-fold cross-validation. We complement our predictions with saliency maps and
show that the learned model focuses on clinically relevant cardiac structures,
motivating its usage in clinical practice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Facial Action Unit Recognition With Multi-models Ensembling. (arXiv:2203.13046v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13046">
<div class="article-summary-box-inner">
<span><p>The Affective Behavior Analysis in-the-wild (ABAW) 2022 Competition gives
Affective Computing a large promotion. In this paper, we present our method of
AU challenge in this Competition. We use improved IResnet100 as backbone. Then
we train AU dataset in Aff-Wild2 on three pertained models pretrained by our
private au and expression dataset, and Glint360K respectively. Finally, we
ensemble the results of our models. We achieved F1 score (macro) 0.731 on AU
validation set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simulation Benchmark for Vision-based Autonomous Navigation. (arXiv:2203.13048v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13048">
<div class="article-summary-box-inner">
<span><p>This work introduces a simulator benchmark for vision-based autonomous
navigation. The simulator offers control over real world variables such as the
environment, time of day, weather and traffic. The benchmark includes a modular
integration of different components of a full autonomous visual navigation
stack. In the experimental part of the paper, state-of-the-art visual
localization methods are evaluated as a part of the stack in realistic
navigation tasks. To the authors' best knowledge, the proposed benchmark is the
first to study modern visual localization methods as part of a full autonomous
visual navigation stack.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compositional Temporal Grounding with Structured Variational Cross-Graph Correspondence Learning. (arXiv:2203.13049v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13049">
<div class="article-summary-box-inner">
<span><p>Temporal grounding in videos aims to localize one target video segment that
semantically corresponds to a given query sentence. Thanks to the semantic
diversity of natural language descriptions, temporal grounding allows activity
grounding beyond pre-defined classes and has received increasing attention in
recent years. The semantic diversity is rooted in the principle of
compositionality in linguistics, where novel semantics can be systematically
described by combining known words in novel ways (compositional
generalization). However, current temporal grounding datasets do not
specifically test for the compositional generalizability. To systematically
measure the compositional generalizability of temporal grounding models, we
introduce a new Compositional Temporal Grounding task and construct two new
dataset splits, i.e., Charades-CG and ActivityNet-CG. Evaluating the
state-of-the-art methods on our new dataset splits, we empirically find that
they fail to generalize to queries with novel combinations of seen words. To
tackle this challenge, we propose a variational cross-graph reasoning framework
that explicitly decomposes video and language into multiple structured
hierarchies and learns fine-grained semantic correspondence among them.
Experiments illustrate the superior compositional generalizability of our
approach. The repository of this work is at https://github.com/YYJMJC/
Compositional-Temporal-Grounding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Coarse-to-Fine Cascaded Networks with Smooth Predicting for Video Facial Expression Recognition. (arXiv:2203.13052v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13052">
<div class="article-summary-box-inner">
<span><p>Facial expression recognition plays an important role in human-computer
interaction. In this paper, we propose the Coarse-to-Fine Cascaded networks
with Smooth Predicting (CFC-SP) to improve the performance of facial expression
recognition. CFC-SP contains two core components, namely Coarse-to-Fine
Cascaded networks (CFC) and Smooth Predicting (SP). For CFC, it first groups
several similar emotions to form a rough category, and then employs a network
to conduct a coarse but accurate classification. Later, Then, an additional
network for these grouped emotions is further used to obtain fine-grained
predictions. For SP, it improves the recognition capability of the model by
capturing both universal and unique effective features. To be specific, the
universal features denote the general characteristic of facial emotions and the
unique features denote the specific characteristic of each facial expression.
Experiments on Aff-Wild2 show the effectiveness of the proposed CFSP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bailando: 3D Dance Generation by Actor-Critic GPT with Choreographic Memory. (arXiv:2203.13055v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13055">
<div class="article-summary-box-inner">
<span><p>Driving 3D characters to dance following a piece of music is highly
challenging due to the spatial constraints applied to poses by choreography
norms. In addition, the generated dance sequence also needs to maintain
temporal coherency with different music genres. To tackle these challenges, we
propose a novel music-to-dance framework, Bailando, with two powerful
components: 1) a choreographic memory that learns to summarize meaningful
dancing units from 3D pose sequence to a quantized codebook, 2) an actor-critic
Generative Pre-trained Transformer (GPT) that composes these units to a fluent
dance coherent to the music. With the learned choreographic memory, dance
generation is realized on the quantized units that meet high choreography
standards, such that the generated dancing sequences are confined within the
spatial constraints. To achieve synchronized alignment between diverse motion
tempos and music beats, we introduce an actor-critic-based reinforcement
learning scheme to the GPT with a newly-designed beat-align reward function.
Extensive experiments on the standard benchmark demonstrate that our proposed
framework achieves state-of-the-art performance both qualitatively and
quantitatively. Notably, the learned choreographic memory is shown to discover
human-interpretable dancing-style poses in an unsupervised manner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SIFT and SURF based feature extraction for the anomaly detection. (arXiv:2203.13068v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13068">
<div class="article-summary-box-inner">
<span><p>In this paper, we suggest a way, how to use SIFT and SURF algorithms to
extract the image features for anomaly detection. We use those feature vectors
to train various classifiers on a real-world dataset in the semi -supervised
(with a small number of faulty samples) manner with a large number of
classifiers and in the one-class (with no faulty samples) manner using the SVDD
and SVM classifier. We prove, that the SIFT and SURF algorithms could be used
as feature extractors, that they could be used to train a semi-supervised and
one-class classifier with an accuracy around 89\% and that the performance of
the one-class classifier could be comparable to the semi-supervised one. We
also made our dataset and source code publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multitask Emotion Recognition Model with Knowledge Distillation and Task Discriminator. (arXiv:2203.13072v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13072">
<div class="article-summary-box-inner">
<span><p>Due to the collection of big data and the development of deep learning,
research to predict human emotions in the wild is being actively conducted. We
designed a multi-task model using ABAW dataset to predict valence-arousal,
expression, and action unit through audio data and face images at in real
world. We trained model from the incomplete label by applying the knowledge
distillation technique. The teacher model was trained as a supervised learning
method, and the student model was trained by using the output of the teacher
model as a soft label. As a result we achieved 2.40 in Multi Task Learning task
validation dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AziNorm: Exploiting the Radial Symmetry of Point Cloud for Azimuth-Normalized 3D Perception. (arXiv:2203.13090v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13090">
<div class="article-summary-box-inner">
<span><p>Studying the inherent symmetry of data is of great importance in machine
learning. Point cloud, the most important data format for 3D environmental
perception, is naturally endowed with strong radial symmetry. In this work, we
exploit this radial symmetry via a divide-and-conquer strategy to boost 3D
perception performance and ease optimization. We propose Azimuth Normalization
(AziNorm), which normalizes the point clouds along the radial direction and
eliminates the variability brought by the difference of azimuth. AziNorm can be
flexibly incorporated into most LiDAR-based perception methods. To validate its
effectiveness and generalization ability, we apply AziNorm in both object
detection and semantic segmentation. For detection, we integrate AziNorm into
two representative detection methods, the one-stage SECOND detector and the
state-of-the-art two-stage PV-RCNN detector. Experiments on Waymo Open Dataset
demonstrate that AziNorm improves SECOND and PV-RCNN by 7.03 mAPH and 3.01 mAPH
respectively. For segmentation, we integrate AziNorm into KPConv. On
SemanticKitti dataset, AziNorm improves KPConv by 1.6/1.1 mIoU on val/test set.
Besides, AziNorm remarkably improves data efficiency and accelerates
convergence, reducing the requirement of data amounts or training epochs by an
order of magnitude. SECOND w/ AziNorm can significantly outperform fully
trained vanilla SECOND, even trained with only 10% data or 10% epochs. Code and
models are available at https://github.com/hustvl/AziNorm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Preliminary Research on Space Situational Awareness Based on Event Cameras. (arXiv:2203.13093v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13093">
<div class="article-summary-box-inner">
<span><p>Event camera is a new type of sensor that is different from traditional
cameras. Each pixel is triggered asynchronously by an event. The trigger event
is the change of the brightness irradiated on the pixel. If the increment or
decrement is higher than a certain threshold, the event is output. Compared
with traditional cameras, event cameras have the advantages of high temporal
resolution, low latency, high dynamic range, low bandwidth and low power
consumption. We carried out a series of observation experiments in a simulated
space lighting environment. The experimental results show that the event camera
can give full play to the above advantages in space situational awareness. This
article first introduces the basic principles of the event camera, then
analyzes its advantages and disadvantages, then introduces the observation
experiment and analyzes the experimental results, and finally, a workflow of
space situational awareness based on event cameras is given.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IA-FaceS, Bidirectional Method, Disentangled Attribute Manipulation, Flexible Component Editing. (arXiv:2203.13097v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13097">
<div class="article-summary-box-inner">
<span><p>Semantic face editing has achieved substantial progress in recent years.
Known as a growingly popular method, latent space manipulation performs face
editing by changing the latent code of an input face to liberate users from
painting skills. However, previous latent space manipulation methods usually
encode an entire face into a single low-dimensional embedding, which constrains
the reconstruction capacity and the control flexibility of facial components,
such as eyes and nose. This paper proposes IA-FaceS as a bidirectional method
for disentangled face attribute manipulation as well as flexible, controllable
component editing without the need for segmentation masks or sketches in the
original image. To strike a balance between the reconstruction capacity and the
control flexibility, the encoder is designed as a multi-head structure to yield
embeddings for reconstruction and control, respectively: a high-dimensional
tensor with spatial properties for consistent reconstruction and four
low-dimensional facial component embeddings for semantic face editing.
Manipulating the separate component embeddings can help achieve disentangled
attribute manipulation and flexible control of facial components. To further
disentangle the highly-correlated components, a component adaptive modulation
(CAM) module is proposed for the decoder. The semantic single-eye editing is
developed for the first time without any input visual guidance, such as
segmentation masks or sketches. According to the experimental results, IA-FaceS
establishes a good balance between maintaining image details and performing
flexible face manipulation. Both quantitative and qualitative results indicate
that the proposed method outperforms the other techniques in reconstruction,
face attribute manipulation, and component transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">R-DFCIL: Relation-Guided Representation Learning for Data-Free Class Incremental Learning. (arXiv:2203.13104v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13104">
<div class="article-summary-box-inner">
<span><p>Class-Incremental Learning (CIL) struggles with catastrophic forgetting when
learning new knowledge, and Data-Free CIL (DFCIL) is even more challenging
without access to the training data of previous classes. Though recent DFCIL
works introduce techniques such as model inversion to synthesize data for
previous classes, they fail to overcome forgetting due to the severe domain gap
between the synthetic and real data. To address this issue, this paper proposes
relation-guided representation learning (RRL) for DFCIL, dubbed R-DFCIL. In
RRL, we introduce relational knowledge distillation to flexibly transfer the
structural relation of new data from the old model to the current model. Our
RRL-boosted DFCIL can guide the current model to learn representations of new
classes better compatible with representations of previous classes, which
greatly reduces forgetting while improving plasticity. To avoid the mutual
interference between representation and classifier learning, we employ local
rather than global classification loss during RRL. After RRL, the
classification head is fine-tuned with global class-balanced classification
loss to address the data imbalance issue as well as learn the decision boundary
between new and previous classes. Extensive experiments on CIFAR100,
Tiny-ImageNet200, and ImageNet100 demonstrate that our R-DFCIL significantly
surpasses previous approaches and achieves a new state-of-the-art performance
for DFCIL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Egocentric Prediction of Action Target in 3D. (arXiv:2203.13116v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13116">
<div class="article-summary-box-inner">
<span><p>We are interested in anticipating as early as possible the target location of
a person's object manipulation action in a 3D workspace from egocentric vision.
It is important in fields like human-robot collaboration, but has not yet
received enough attention from vision and learning communities. To stimulate
more research on this challenging egocentric vision task, we propose a large
multimodality dataset of more than 1 million frames of RGB-D and IMU streams,
and provide evaluation metrics based on our high-quality 2D and 3D labels from
semi-automatic annotation. Meanwhile, we design baseline methods using
recurrent neural networks and conduct various ablation studies to validate
their effectiveness. Our results demonstrate that this new task is worthy of
further study by researchers in robotics, vision, and learning communities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">X-ray Dissectography Improves Lung Nodule Detection. (arXiv:2203.13118v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13118">
<div class="article-summary-box-inner">
<span><p>Although radiographs are the most frequently used worldwide due to their
cost-effectiveness and widespread accessibility, the structural superposition
along the x-ray paths often renders suspicious or concerning lung nodules
difficult to detect. In this study, we apply "X-ray dissectography" to dissect
lungs digitally from a few radiographic projections, suppress the interference
of irrelevant structures, and improve lung nodule detectability. For this
purpose, a collaborative detection network is designed to localize lung nodules
in 2D dissected projections and 3D physical space. Our experimental results
show that our approach can significantly improve the average precision by 20+%
in comparison with the common baseline that detects lung nodules from original
projections using a popular detection network. Potentially, this approach could
help re-design the current X-ray imaging protocols and workflows and improve
the diagnostic performance of chest radiographs in lung diseases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature visualization for convolutional neural network models trained on neuroimaging data. (arXiv:2203.13120v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13120">
<div class="article-summary-box-inner">
<span><p>A major prerequisite for the application of machine learning models in
clinical decision making is trust and interpretability. Current explainability
studies in the neuroimaging community have mostly focused on explaining
individual decisions of trained models, e.g. obtained by a convolutional neural
network (CNN). Using attribution methods such as layer-wise relevance
propagation or SHAP heatmaps can be created that highlight which regions of an
input are more relevant for the decision than others. While this allows the
detection of potential data set biases and can be used as a guide for a human
expert, it does not allow an understanding of the underlying principles the
model has learned. In this study, we instead show, to the best of our
knowledge, for the first time results using feature visualization of
neuroimaging CNNs. Particularly, we have trained CNNs for different tasks
including sex classification and artificial lesion classification based on
structural magnetic resonance imaging (MRI) data. We have then iteratively
generated images that maximally activate specific neurons, in order to
visualize the patterns they respond to. To improve the visualizations we
compared several regularization strategies. The resulting images reveal the
learned concepts of the artificial lesions, including their shapes, but remain
hard to interpret for abstract features in the sex classification task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Moving Window Regression: A Novel Approach to Ordinal Regression. (arXiv:2203.13122v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13122">
<div class="article-summary-box-inner">
<span><p>A novel ordinal regression algorithm, called moving window regression (MWR),
is proposed in this paper. First, we propose the notion of relative rank
($\rho$-rank), which is a new order representation scheme for input and
reference instances. Second, we develop global and local relative regressors
($\rho$-regressors) to predict $\rho$-ranks within entire and specific rank
ranges, respectively. Third, we refine an initial rank estimate iteratively by
selecting two reference instances to form a search window and then estimating
the $\rho$-rank within the window. Extensive experiments results show that the
proposed algorithm achieves the state-of-the-art performances on various
benchmark datasets for facial age estimation and historical color image
classification. The codes are available at https://github.com/nhshin-mcl/MWR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors. (arXiv:2203.13131v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13131">
<div class="article-summary-box-inner">
<span><p>Recent text-to-image generation methods provide a simple yet exciting
conversion capability between text and image domains. While these methods have
incrementally improved the generated image fidelity and text relevancy, several
pivotal gaps remain unanswered, limiting applicability and quality. We propose
a novel text-to-image method that addresses these gaps by (i) enabling a simple
control mechanism complementary to text in the form of a scene, (ii)
introducing elements that substantially improve the tokenization process by
employing domain-specific knowledge over key image regions (faces and salient
objects), and (iii) adapting classifier-free guidance for the transformer use
case. Our model achieves state-of-the-art FID and human evaluation results,
unlocking the ability to generate high fidelity images in a resolution of
512x512 pixels, significantly improving visual quality. Through scene
controllability, we introduce several new capabilities: (i) Scene editing, (ii)
text editing with anchor scenes, (iii) overcoming out-of-distribution text
prompts, and (iv) story illustration generation, as demonstrated in the story
we wrote.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Physics-based Learning of Parameterized Thermodynamics from Real-time Thermography. (arXiv:2203.13148v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13148">
<div class="article-summary-box-inner">
<span><p>Progress in automatic control of thermal processes has long been limited by
the difficulty of obtaining high-fidelity thermodynamic models. Traditionally,
in complex thermodynamic systems, it is often infeasible to estimate the
thermophysical parameters of spatiotemporally varying processes, forcing the
adoption of model-free control architectures. This comes at the cost of losing
any robustness guarantees, and implies a need for extensive real-life testing.
In recent years, however, infrared cameras and other thermographic equipment
have become readily applicable to these processes, allowing for a real-time,
non-invasive means of sensing the thermal state of a process. In this work, we
present a novel physics-based approach to learning a thermal process's dynamics
directly from such real-time thermographic data, while focusing attention on
regions with high thermal activity. We call this process, which applies to any
higher-dimensional scalar field, attention-based noise robust averaging (ANRA).
Given a partial-differential equation model structure, we show that our
approach is robust against noise, and can be used to initialize optimization
routines to further refine parameter estimates. We demonstrate our method on
several simulation examples, as well as by applying it to electrosurgical
thermal response data on in vivo porcine skin tissue.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Hierarchical Cross-Modal Association for Co-Speech Gesture Generation. (arXiv:2203.13161v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13161">
<div class="article-summary-box-inner">
<span><p>Generating speech-consistent body and gesture movements is a long-standing
problem in virtual avatar creation. Previous studies often synthesize pose
movement in a holistic manner, where poses of all joints are generated
simultaneously. Such a straightforward pipeline fails to generate fine-grained
co-speech gestures. One observation is that the hierarchical semantics in
speech and the hierarchical structures of human gestures can be naturally
described into multiple granularities and associated together. To fully utilize
the rich connections between speech audio and human gestures, we propose a
novel framework named Hierarchical Audio-to-Gesture (HA2G) for co-speech
gesture generation. In HA2G, a Hierarchical Audio Learner extracts audio
representations across semantic granularities. A Hierarchical Pose Inferer
subsequently renders the entire human pose gradually in a hierarchical manner.
To enhance the quality of synthesized gestures, we develop a contrastive
learning strategy based on audio-text alignment for better audio
representations. Extensive experiments and human evaluation demonstrate that
the proposed method renders realistic co-speech gestures and outperforms
previous methods in a clear margin. Project page:
https://alvinliu0.github.io/projects/HA2G
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Video-centralised Transformer for Video Face Clustering. (arXiv:2203.13166v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13166">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel method for face clustering in videos using a
video-centralised transformer. Previous works often employed contrastive
learning to learn frame-level representation and used average pooling to
aggregate the features along the temporal dimension. This approach may not
fully capture the complicated video dynamics. In addition, despite the recent
progress in video-based contrastive learning, few have attempted to learn a
self-supervised clustering-friendly face representation that benefits the video
face clustering task. To overcome these limitations, our method employs a
transformer to directly learn video-level representations that can better
reflect the temporally-varying property of faces in videos, while we also
propose a video-centralised self-supervised framework to train the transformer
model. We also investigate face clustering in egocentric videos, a
fast-emerging field that has not been studied yet in works related to face
clustering. To this end, we present and release the first large-scale
egocentric video face clustering dataset named EasyCom-Clustering. We evaluate
our proposed method on both the widely used Big Bang Theory (BBT) dataset and
the new EasyCom-Clustering dataset. Results show the performance of our
video-centralised transformer has surpassed all previous state-of-the-art
methods on both benchmarks, exhibiting a self-attentive understanding of face
videos.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Exemplar-Free Continual Learning in Vision Transformers: an Account of Attention, Functional and Weight Regularization. (arXiv:2203.13167v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13167">
<div class="article-summary-box-inner">
<span><p>In this paper, we investigate the continual learning of Vision Transformers
(ViT) for the challenging exemplar-free scenario, with special focus on how to
efficiently distill the knowledge of its crucial self-attention mechanism
(SAM). Our work takes an initial step towards a surgical investigation of SAM
for designing coherent continual learning methods in ViTs. We first carry out
an evaluation of established continual learning regularization techniques. We
then examine the effect of regularization when applied to two key enablers of
SAM: (a) the contextualized embedding layers, for their ability to capture
well-scaled representations with respect to the values, and (b) the prescaled
attention maps, for carrying value-independent global contextual information.
We depict the perks of each distilling strategy on two image recognition
benchmarks (CIFAR100 and ImageNet-32) -- while (a) leads to a better overall
accuracy, (b) helps enhance the rigidity by maintaining competitive
performances. Furthermore, we identify the limitation imposed by the symmetric
nature of regularization losses. To alleviate this, we propose an asymmetric
variant and apply it to the pooled output distillation (POD) loss adapted for
ViTs. Our experiments confirm that introducing asymmetry to POD boosts its
plasticity while retaining stability across (a) and (b). Moreover, we
acknowledge low forgetting measures for all the compared methods, indicating
that ViTs might be naturally inclined continual learner
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantum Motion Segmentation. (arXiv:2203.13185v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13185">
<div class="article-summary-box-inner">
<span><p>Motion segmentation is a challenging problem that seeks to identify
independent motions in two or several input images. This paper introduces the
first algorithm for motion segmentation that relies on adiabatic quantum
optimization of the objective function. The proposed method achieves on-par
performance with the state of the art on problem instances which can be mapped
to modern quantum annealers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decouple-and-Sample: Protecting sensitive information in task agnostic data release. (arXiv:2203.13204v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13204">
<div class="article-summary-box-inner">
<span><p>We propose sanitizer, a framework for secure and task-agnostic data release.
While releasing datasets continues to make a big impact in various applications
of computer vision, its impact is mostly realized when data sharing is not
inhibited by privacy concerns. We alleviate these concerns by sanitizing
datasets in a two-stage process. First, we introduce a global decoupling stage
for decomposing raw data into sensitive and non-sensitive latent
representations. Secondly, we design a local sampling stage to synthetically
generate sensitive information with differential privacy and merge it with
non-sensitive latent features to create a useful representation while
preserving the privacy. This newly formed latent information is a task-agnostic
representation of the original dataset with anonymized sensitive information.
While most algorithms sanitize data in a task-dependent manner, a few
task-agnostic sanitization techniques sanitize data by censoring sensitive
information. In this work, we show that a better privacy-utility trade-off is
achieved if sensitive information can be synthesized privately. We validate the
effectiveness of the sanitizer by outperforming state-of-the-art baselines on
the existing benchmark tasks and demonstrating tasks that are not possible
using existing techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Perturbation Constrained Adversarial Attack for Evaluating the Robustness of Optical Flow. (arXiv:2203.13214v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13214">
<div class="article-summary-box-inner">
<span><p>Recent optical flow methods are almost exclusively judged in terms of
accuracy, while analyzing their robustness is often neglected. Although
adversarial attacks offer a useful tool to perform such an analysis, current
attacks on optical flow methods rather focus on real-world attacking scenarios
than on a worst case robustness assessment. Hence, in this work, we propose a
novel adversarial attack - the Perturbation Constrained Flow Attack (PCFA) -
that emphasizes destructivity over applicability as a real-world attack. More
precisely, PCFA is a global attack that optimizes adversarial perturbations to
shift the predicted flow towards a specified target flow, while keeping the L2
norm of the perturbation below a chosen bound. Our experiments not only
demonstrate PCFA's applicability in white- and black-box settings, but also
show that it finds stronger adversarial samples for optical flow than previous
attacking frameworks. Moreover, based on these strong samples, we provide the
first common ranking of optical flow methods in the literature considering both
prediction quality and adversarial robustness, indicating that high quality
methods are not necessarily robust. Our source code will be publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Neighbor Style Transfer. (arXiv:2203.13215v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13215">
<div class="article-summary-box-inner">
<span><p>We propose Neural Neighbor Style Transfer (NNST), a pipeline that offers
state-of-the-art quality, generalization, and competitive efficiency for
artistic style transfer. Our approach is based on explicitly replacing neural
features extracted from the content input (to be stylized) with those from a
style exemplar, then synthesizing the final output based on these rearranged
features. While the spirit of our approach is similar to prior work, we show
that our design decisions dramatically improve the final visual quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Facial Expression Recognition based on Multi-head Cross Attention Network. (arXiv:2203.13235v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13235">
<div class="article-summary-box-inner">
<span><p>Facial expression in-the-wild is essential for various interactive computing
domains. In this paper, we proposed an extended version of DAN model to address
the VA estimation and facial expression challenges introduced in ABAW 2022. Our
method produced preliminary results of 0.44 of mean CCC value for the VA
estimation task, and 0.33 of the average F1 score for the expression
classification task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open-set Recognition via Augmentation-based Similarity Learning. (arXiv:2203.13238v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13238">
<div class="article-summary-box-inner">
<span><p>The primary assumption of conventional supervised learning or classification
is that the test samples are drawn from the same distribution as the training
samples, which is called closed set learning or classification. In many
practical scenarios, this is not the case because there are unknowns or unseen
class samples in the test data, which is called the open set scenario, and the
unknowns need to be detected. This problem is referred to as the open set
recognition problem and is important in safety-critical applications. We
propose to detect unknowns (or unseen class samples) through learning pairwise
similarities. The proposed method works in two steps. It first learns a closed
set classifier using the seen classes that have appeared in training and then
learns how to compare seen classes with pseudo-unseen (automatically generated
unseen class samples). The pseudo-unseen generation is carried out by
performing distribution shifting augmentations on the seen or training samples.
We call our method OPG (Open set recognition based on Pseudo unseen data
Generation). The experimental evaluation shows that the learned
similarity-based features can successfully distinguish seen from unseen in
benchmark datasets for open set recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Representation Separation Perspective to Correspondences-free Unsupervised 3D Point Cloud Registration. (arXiv:2203.13239v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13239">
<div class="article-summary-box-inner">
<span><p>3D point cloud registration in remote sensing field has been greatly advanced
by deep learning based methods, where the rigid transformation is either
directly regressed from the two point clouds (correspondences-free approaches)
or computed from the learned correspondences (correspondences-based
approaches). Existing correspondences-free methods generally learn the holistic
representation of the entire point cloud, which is fragile for partial and
noisy point clouds. In this paper, we propose a correspondences-free
unsupervised point cloud registration (UPCR) method from the representation
separation perspective. First, we model the input point cloud as a combination
of pose-invariant representation and pose-related representation. Second, the
pose-related representation is used to learn the relative pose wrt a "latent
canonical shape" for the source and target point clouds respectively. Third,
the rigid transformation is obtained from the above two learned relative poses.
Our method not only filters out the disturbance in pose-invariant
representation but also is robust to partial-to-partial point clouds or noise.
Experiments on benchmark datasets demonstrate that our unsupervised method
achieves comparable if not better performance than state-of-the-art supervised
registration methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VRNet: Learning the Rectified Virtual Corresponding Points for 3D Point Cloud Registration. (arXiv:2203.13241v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13241">
<div class="article-summary-box-inner">
<span><p>3D point cloud registration is fragile to outliers, which are labeled as the
points without corresponding points. To handle this problem, a widely adopted
strategy is to estimate the relative pose based only on some accurate
correspondences, which is achieved by building correspondences on the
identified inliers or by selecting reliable ones. However, these approaches are
usually complicated and time-consuming. By contrast, the virtual point-based
methods learn the virtual corresponding points (VCPs) for all source points
uniformly without distinguishing the outliers and the inliers. Although this
strategy is time-efficient, the learned VCPs usually exhibit serious collapse
degeneration due to insufficient supervision and the inherent distribution
limitation. In this paper, we propose to exploit the best of both worlds and
present a novel robust 3D point cloud registration framework. We follow the
idea of the virtual point-based methods but learn a new type of virtual points
called rectified virtual corresponding points (RCPs), which are defined as the
point set with the same shape as the source and with the same pose as the
target. Hence, a pair of consistent point clouds, i.e. source and RCPs, is
formed by rectifying VCPs to RCPs (VRNet), through which reliable
correspondences between source and RCPs can be accurately obtained. Since the
relative pose between source and RCPs is the same as the relative pose between
source and target, the input point clouds can be registered naturally.
Specifically, we first construct the initial VCPs by using an estimated soft
matching matrix to perform a weighted average on the target points. Then, we
design a correction-walk module to learn an offset to rectify VCPs to RCPs,
which effectively breaks the distribution limitation of VCPs. Finally, we
develop a hybrid loss function to enforce the shape and geometry structure
consistency ...
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pastiche Master: Exemplar-Based High-Resolution Portrait Style Transfer. (arXiv:2203.13248v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13248">
<div class="article-summary-box-inner">
<span><p>Recent studies on StyleGAN show high performance on artistic portrait
generation by transfer learning with limited data. In this paper, we explore
more challenging exemplar-based high-resolution portrait style transfer by
introducing a novel DualStyleGAN with flexible control of dual styles of the
original face domain and the extended artistic portrait domain. Different from
StyleGAN, DualStyleGAN provides a natural way of style transfer by
characterizing the content and style of a portrait with an intrinsic style path
and a new extrinsic style path, respectively. The delicately designed extrinsic
style path enables our model to modulate both the color and complex structural
styles hierarchically to precisely pastiche the style example. Furthermore, a
novel progressive fine-tuning scheme is introduced to smoothly transform the
generative space of the model to the target domain, even with the above
modifications on the network architecture. Experiments demonstrate the
superiority of DualStyleGAN over state-of-the-art methods in high-quality
portrait style transfer and flexible style control.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BigDetection: A Large-scale Benchmark for Improved Object Detector Pre-training. (arXiv:2203.13249v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13249">
<div class="article-summary-box-inner">
<span><p>Multiple datasets and open challenges for object detection have been
introduced in recent years. To build more general and powerful object detection
systems, in this paper, we construct a new large-scale benchmark termed
BigDetection. Our goal is to simply leverage the training data from existing
datasets (LVIS, OpenImages and Object365) with carefully designed principles,
and curate a larger dataset for improved detector pre-training. Specifically,
we generate a new taxonomy which unifies the heterogeneous label spaces from
different sources. Our BigDetection dataset has 600 object categories and
contains over 3.4M training images with 36M bounding boxes. It is much larger
in multiple dimensions than previous benchmarks, which offers both
opportunities and challenges. Extensive experiments demonstrate its validity as
a new benchmark for evaluating different object detection methods, and its
effectiveness as a pre-training dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Global Tracking Transformers. (arXiv:2203.13250v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13250">
<div class="article-summary-box-inner">
<span><p>We present a novel transformer-based architecture for global multi-object
tracking. Our network takes a short sequence of frames as input and produces
global trajectories for all objects. The core component is a global tracking
transformer that operates on objects from all frames in the sequence. The
transformer encodes object features from all frames, and uses trajectory
queries to group them into trajectories. The trajectory queries are object
features from a single frame and naturally produce unique trajectories. Our
global tracking transformer does not require intermediate pairwise grouping or
combinatorial association, and can be jointly trained with an object detector.
It achieves competitive performance on the popular MOT17 benchmark, with 75.3
MOTA and 59.1 HOTA. More importantly, our framework seamlessly integrates into
state-of-the-art large-vocabulary detectors to track any objects. Experiments
on the challenging TAO dataset show that our framework consistently improves
upon baselines that are based on pairwise association, outperforming published
works by a significant 7.7 tracking mAP. Code is available at
https://github.com/xingyizhou/GTR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dexterous Imitation Made Easy: A Learning-Based Framework for Efficient Dexterous Manipulation. (arXiv:2203.13251v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13251">
<div class="article-summary-box-inner">
<span><p>Optimizing behaviors for dexterous manipulation has been a longstanding
challenge in robotics, with a variety of methods from model-based control to
model-free reinforcement learning having been previously explored in
literature. Perhaps one of the most powerful techniques to learn complex
manipulation strategies is imitation learning. However, collecting and learning
from demonstrations in dexterous manipulation is quite challenging. The
complex, high-dimensional action-space involved with multi-finger control often
leads to poor sample efficiency of learning-based methods. In this work, we
propose 'Dexterous Imitation Made Easy' (DIME) a new imitation learning
framework for dexterous manipulation. DIME only requires a single RGB camera to
observe a human operator and teleoperate our robotic hand. Once demonstrations
are collected, DIME employs standard imitation learning methods to train
dexterous manipulation policies. On both simulation and real robot benchmarks
we demonstrate that DIME can be used to solve complex, in-hand manipulation
tasks such as 'flipping', 'spinning', and 'rotating' objects with the Allegro
hand. Our framework along with pre-collected demonstrations is publicly
available at https://nyu-robot-learning.github.io/dime.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Instance Segmentation via Multi-scale Spatio-temporal Split Attention Transformer. (arXiv:2203.13253v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13253">
<div class="article-summary-box-inner">
<span><p>State-of-the-art transformer-based video instance segmentation (VIS)
approaches typically utilize either single-scale spatio-temporal features or
per-frame multi-scale features during the attention computations. We argue that
such an attention computation ignores the multi-scale spatio-temporal feature
relationships that are crucial to tackle target appearance deformations in
videos. To address this issue, we propose a transformer-based VIS framework,
named MS-STS VIS, that comprises a novel multi-scale spatio-temporal split
(MS-STS) attention module in the encoder. The proposed MS-STS module
effectively captures spatio-temporal feature relationships at multiple scales
across frames in a video. We further introduce an attention block in the
decoder to enhance the temporal consistency of the detected instances in
different frames of a video. Moreover, an auxiliary discriminator is introduced
during training to ensure better foreground-background separability within the
multi-scale spatio-temporal feature space. We conduct extensive experiments on
two benchmarks: Youtube-VIS (2019 and 2021). Our MS-STS VIS achieves
state-of-the-art performance on both benchmarks. When using the ResNet50
backbone, our MS-STS achieves a mask AP of 50.1 %, outperforming the best
reported results in literature by 2.7 % and by 4.8 % at higher overlap
threshold of AP_75, while being comparable in model size and speed on
Youtube-VIS 2019 val. set. When using the Swin Transformer backbone, MS-STS VIS
achieves mask AP of 61.0 % on Youtube-VIS 2019 val. set. Our code and models
are available at https://github.com/OmkarThawakar/MSSTS-VIS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EPro-PnP: Generalized End-to-End Probabilistic Perspective-n-Points for Monocular Object Pose Estimation. (arXiv:2203.13254v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13254">
<div class="article-summary-box-inner">
<span><p>Locating 3D objects from a single RGB image via Perspective-n-Points (PnP) is
a long-standing problem in computer vision. Driven by end-to-end deep learning,
recent studies suggest interpreting PnP as a differentiable layer, so that
2D-3D point correspondences can be partly learned by backpropagating the
gradient w.r.t. object pose. Yet, learning the entire set of unrestricted 2D-3D
points from scratch fails to converge with existing approaches, since the
deterministic pose is inherently non-differentiable. In this paper, we propose
the EPro-PnP, a probabilistic PnP layer for general end-to-end pose estimation,
which outputs a distribution of pose on the SE(3) manifold, essentially
bringing categorical Softmax to the continuous domain. The 2D-3D coordinates
and corresponding weights are treated as intermediate variables learned by
minimizing the KL divergence between the predicted and target pose
distribution. The underlying principle unifies the existing approaches and
resembles the attention mechanism. EPro-PnP significantly outperforms
competitive baselines, closing the gap between PnP-based method and the
task-specific leaders on the LineMOD 6DoF pose estimation and nuScenes 3D
object detection benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A real-time and unsupervised face Re-Identification system for Human-Robot Interaction. (arXiv:1804.03547v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1804.03547">
<div class="article-summary-box-inner">
<span><p>In the context of Human-Robot Interaction (HRI), face Re-Identification (face
Re-ID) aims to verify if certain detected faces have already been observed by
robots. The ability of distinguishing between different users is crucial in
social robots as it will enable the robot to tailor the interaction strategy
toward the users' individual preferences. So far face recognition research has
achieved great success, however little attention has been paid to the realistic
applications of Face Re-ID in social robots. In this paper, we present an
effective and unsupervised face Re-ID system which simultaneously re-identifies
multiple faces for HRI. This Re-ID system employs Deep Convolutional Neural
Networks to extract features, and an online clustering algorithm to determine
the face's ID. Its performance is evaluated on two datasets: the TERESA video
dataset collected by the TERESA robot, and the YouTube Face Dataset (YTF
Dataset). We demonstrate that the optimised combination of techniques achieves
an overall 93.55% accuracy on TERESA dataset and an overall 90.41% accuracy on
YTF dataset. We have implemented the proposed method into a software module in
the HCI^2 Framework for it to be further integrated into the TERESA robot, and
has achieved real-time performance at 10~26 Frames per second.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Answer-Driven Visual State Estimator for Goal-Oriented Visual Dialogue. (arXiv:2010.00361v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.00361">
<div class="article-summary-box-inner">
<span><p>A goal-oriented visual dialogue involves multi-turn interactions between two
agents, Questioner and Oracle. During which, the answer given by Oracle is of
great significance, as it provides golden response to what Questioner concerns.
Based on the answer, Questioner updates its belief on target visual content and
further raises another question. Notably, different answers drive into
different visual beliefs and future questions. However, existing methods always
indiscriminately encode answers after much longer questions, resulting in a
weak utilization of answers. In this paper, we propose an Answer-Driven Visual
State Estimator (ADVSE) to impose the effects of different answers on visual
states. First, we propose an Answer-Driven Focusing Attention (ADFA) to capture
the answer-driven effect on visual attention by sharpening question-related
attention and adjusting it by answer-based logical operation at each turn. Then
based on the focusing attention, we get the visual state estimation by
Conditional Visual Information Fusion (CVIF), where overall information and
difference information are fused conditioning on the question-answer state. We
evaluate the proposed ADVSE to both question generator and guesser tasks on the
large-scale GuessWhat?! dataset and achieve the state-of-the-art performances
on both tasks. The qualitative results indicate that the ADVSE boosts the agent
to generate highly efficient questions and obtains reliable visual attentions
during the reasonable question generation and guess processes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">XraySyn: Realistic View Synthesis From a Single Radiograph Through CT Priors. (arXiv:2012.02407v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.02407">
<div class="article-summary-box-inner">
<span><p>A radiograph visualizes the internal anatomy of a patient through the use of
X-ray, which projects 3D information onto a 2D plane. Hence, radiograph
analysis naturally requires physicians to relate the prior about 3D human
anatomy to 2D radiographs. Synthesizing novel radiographic views in a small
range can assist physicians in interpreting anatomy more reliably; however,
radiograph view synthesis is heavily ill-posed, lacking in paired data, and
lacking in differentiable operations to leverage learning-based approaches. To
address these problems, we use Computed Tomography (CT) for radiograph
simulation and design a differentiable projection algorithm, which enables us
to achieve geometrically consistent transformations between the radiography and
CT domains. Our method, XraySyn, can synthesize novel views on real radiographs
through a combination of realistic simulation and finetuning on real
radiographs. To the best of our knowledge, this is the first work on radiograph
view synthesis. We show that by gaining an understanding of radiography in 3D
space, our method can be applied to radiograph bone extraction and suppression
without groundtruth bone labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep One-Class Classification via Interpolated Gaussian Descriptor. (arXiv:2101.10043v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.10043">
<div class="article-summary-box-inner">
<span><p>One-class classification (OCC) aims to learn an effective data description to
enclose all normal training samples and detect anomalies based on the deviation
from the data description. Current state-of-the-art OCC models learn a compact
normality description by hyper-sphere minimisation, but they often suffer from
overfitting the training data, especially when the training set is small or
contaminated with anomalous samples. To address this issue, we introduce the
interpolated Gaussian descriptor (IGD) method, a novel OCC model that learns a
one-class Gaussian anomaly classifier trained with adversarially interpolated
training samples. The Gaussian anomaly classifier differentiates the training
samples based on their distance to the Gaussian centre and the standard
deviation of these distances, offering the model a discriminability w.r.t. the
given samples during training. The adversarial interpolation is enforced to
consistently learn a smooth Gaussian descriptor, even when the training data is
small or contaminated with anomalous samples. This enables our model to learn
the data description based on the representative normal samples rather than
fringe or anomalous samples, resulting in significantly improved normality
description. In extensive experiments on diverse popular benchmarks, including
MNIST, Fashion MNIST, CIFAR10, MVTec AD and two medical datasets, IGD achieves
better detection accuracy than current state-of-the-art models. IGD also shows
better robustness in problems with small or contaminated training sets. Code is
available at https://github.com/tianyu0207/IGD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RoRD: Rotation-Robust Descriptors and Orthographic Views for Local Feature Matching. (arXiv:2103.08573v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.08573">
<div class="article-summary-box-inner">
<span><p>The use of local detectors and descriptors in typical computer vision
pipelines work well until variations in viewpoint and appearance change become
extreme. Past research in this area has typically focused on one of two
approaches to this challenge: the use of projections into spaces more suitable
for feature matching under extreme viewpoint changes, and attempting to learn
features that are inherently more robust to viewpoint change. In this paper, we
present a novel framework that combines learning of invariant descriptors
through data augmentation and orthographic viewpoint projection. We propose
rotation-robust local descriptors, learnt through training data augmentation
based on rotation homographies, and a correspondence ensemble technique that
combines vanilla feature correspondences with those obtained through
rotation-robust features. Using a range of benchmark datasets as well as
contributing a new bespoke dataset for this research domain, we evaluate the
effectiveness of the proposed approach on key tasks including pose estimation
and visual place recognition. Our system outperforms a range of baseline and
state-of-the-art techniques, including enabling higher levels of place
recognition precision across opposing place viewpoints and achieves
practically-useful performance levels even under extreme viewpoint changes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QueryDet: Cascaded Sparse Query for Accelerating High-Resolution Small Object Detection. (arXiv:2103.09136v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.09136">
<div class="article-summary-box-inner">
<span><p>While general object detection with deep learning has achieved great success
in the past few years, the performance and efficiency of detecting small
objects are far from satisfactory. The most common and effective way to promote
small object detection is to use high-resolution images or feature maps.
However, both approaches induce costly computation since the computational cost
grows squarely as the size of images and features increases. To get the best of
two worlds, we propose QueryDet that uses a novel query mechanism to accelerate
the inference speed of feature-pyramid based object detectors. The pipeline
composes two steps: it first predicts the coarse locations of small objects on
low-resolution features and then computes the accurate detection results using
high-resolution features sparsely guided by those coarse positions. In this
way, we can not only harvest the benefit of high-resolution feature maps but
also avoid useless computation for the background area. On the popular COCO
dataset, the proposed method improves the detection mAP by 1.0 and mAP-small by
2.0, and the high-resolution inference speed is improved to 3.0x on average. On
VisDrone dataset, which contains more small objects, we create a new
state-of-the-art while gaining a 2.3x high-resolution acceleration on average.
Code is available at https://github.com/ChenhongyiYang/QueryDet-PyTorch.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text to Image Generation with Semantic-Spatial Aware GAN. (arXiv:2104.00567v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.00567">
<div class="article-summary-box-inner">
<span><p>Text-to-image synthesis (T2I) aims to generate photo-realistic images which
are semantically consistent with the text descriptions. Existing methods are
usually built upon conditional generative adversarial networks (GANs) and
initialize an image from noise with sentence embedding, and then refine the
features with fine-grained word embedding iteratively. A close inspection of
their generated images reveals a major limitation: even though the generated
image holistically matches the description, individual image regions or parts
of somethings are often not recognizable or consistent with words in the
sentence, e.g. "a white crown". To address this problem, we propose a novel
framework Semantic-Spatial Aware GAN for synthesizing images from input text.
Concretely, we introduce a simple and effective Semantic-Spatial Aware block,
which (1) learns semantic-adaptive transformation conditioned on text to
effectively fuse text features and image features, and (2) learns a semantic
mask in a weakly-supervised way that depends on the current text-image fusion
process in order to guide the transformation spatially. Experiments on the
challenging COCO and CUB bird datasets demonstrate the advantage of our method
over the recent state-of-the-art approaches, regarding both visual fidelity and
alignment with input text description.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLIPScore: A Reference-free Evaluation Metric for Image Captioning. (arXiv:2104.08718v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08718">
<div class="article-summary-box-inner">
<span><p>Image captioning has conventionally relied on reference-based automatic
evaluations, where machine captions are compared against captions written by
humans. This is in contrast to the reference-free manner in which humans assess
caption quality.
</p>
<p>In this paper, we report the surprising empirical finding that CLIP (Radford
et al., 2021), a cross-modal model pretrained on 400M image+caption pairs from
the web, can be used for robust automatic evaluation of image captioning
without the need for references. Experiments spanning several corpora
demonstrate that our new reference-free metric, CLIPScore, achieves the highest
correlation with human judgements, outperforming existing reference-based
metrics like CIDEr and SPICE. Information gain experiments demonstrate that
CLIPScore, with its tight focus on image-text compatibility, is complementary
to existing reference-based metrics that emphasize text-text similarities.
Thus, we also present a reference-augmented version, RefCLIPScore, which
achieves even higher correlation. Beyond literal description tasks, several
case studies reveal domains where CLIPScore performs well (clip-art images,
alt-text rating), but also where it is relatively weaker in comparison to
reference-based metrics, e.g., news captions that require richer contextual
knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Oriented RepPoints for Aerial Object Detection. (arXiv:2105.11111v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.11111">
<div class="article-summary-box-inner">
<span><p>In contrast to the generic object, aerial targets are often non-axis aligned
with arbitrary orientations having the cluttered surroundings. Unlike the
mainstreamed approaches regressing the bounding box orientations, this paper
proposes an effective adaptive points learning approach to aerial object
detection by taking advantage of the adaptive points representation, which is
able to capture the geometric information of the arbitrary-oriented instances.
To this end, three oriented conversion functions are presented to facilitate
the classification and localization with accurate orientation. Moreover, we
propose an effective quality assessment and sample assignment scheme for
adaptive points learning toward choosing the representative oriented reppoints
samples during training, which is able to capture the non-axis aligned features
from adjacent objects or background noises. A spatial constraint is introduced
to penalize the outlier points for roust adaptive learning. Experimental
results on four challenging aerial datasets including DOTA, HRSC2016, UCAS-AOD
and DIOR-R, demonstrate the efficacy of our proposed approach. The source code
is availabel at: https://github.com/LiWentomng/OrientedRepPoints.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PolarStream: Streaming Lidar Object Detection and Segmentation with Polar Pillars. (arXiv:2106.07545v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07545">
<div class="article-summary-box-inner">
<span><p>Recent works recognized lidars as an inherently streaming data source and
showed that the end-to-end latency of lidar perception models can be reduced
significantly by operating on wedge-shaped point cloud sectors rather then the
full point cloud. However, due to use of cartesian coordinate systems these
methods represent the sectors as rectangular regions, wasting memory and
compute. In this work we propose using a polar coordinate system and make two
key improvements on this design. First, we increase the spatial context by
using multi-scale padding from neighboring sectors: preceding sector from the
current scan and/or the following sector from the past scan. Second, we improve
the core polar convolutional architecture by introducing feature undistortion
and range stratified convolutions. Experimental results on the nuScenes dataset
show significant improvements over other streaming based methods. We also
achieve comparable results to existing non-streaming methods but with lower
latencies. The code and pretrained models are available at
\url{https://github.com/motional/polarstream}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepMesh: Differentiable Iso-Surface Extraction. (arXiv:2106.11795v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.11795">
<div class="article-summary-box-inner">
<span><p>Geometric Deep Learning has recently made striking progress with the advent
of continuous deep implicit fields. They allow for detailed modeling of
watertight surfaces of arbitrary topology while not relying on a 3D Euclidean
grid, resulting in a learnable parameterization that is unlimited in
resolution.
</p>
<p>Unfortunately, these methods are often unsuitable for applications that
require an explicit mesh-based surface representation because converting an
implicit field to such a representation relies on the Marching Cubes algorithm,
which cannot be differentiated with respect to the underlying implicit field.
</p>
<p>In this work, we remove this limitation and introduce a differentiable way to
produce explicit surface mesh representations from Deep Implicit Fields. Our
key insight is that by reasoning on how implicit field perturbations impact
local surface geometry, one can ultimately differentiate the 3D location of
surface samples with respect to the underlying deep implicit field. We exploit
this to define DeepMesh - an end-to-end differentiable mesh representation that
can vary its topology.
</p>
<p>We validate our theoretical insight through several applications: Single view
3D Reconstruction via Differentiable Rendering, Physically-Driven Shape
Optimization, Full Scene 3D Reconstruction from Scans and End-to-End Training.
In all cases our end-to-end differentiable parameterization gives us an edge
over state-of-the-art algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Applications of Artificial Neural Networks in Microorganism Image Analysis: A Comprehensive Review from Conventional Multilayer Perceptron to Popular Convolutional Neural Network and Potential Visual Transformer. (arXiv:2108.00358v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.00358">
<div class="article-summary-box-inner">
<span><p>Microorganisms are widely distributed in the human daily living environment.
They play an essential role in environmental pollution control, disease
prevention and treatment, and food and drug production. The analysis of
microorganisms is essential for making full use of different microorganisms.
The conventional analysis methods are laborious and time-consuming. Therefore,
the automatic image analysis based on artificial neural networks is introduced
to optimize it. However, the automatic microorganism image analysis faces many
challenges, such as the requirement of a robust algorithm caused by various
application occasions, insignificant features and easy under-segmentation
caused by the image characteristic, and various analysis tasks. Therefore, we
conduct this review to comprehensively discuss the characteristics of
microorganism image analysis based on artificial neural networks. In this
review, the background and motivation are introduced first. Then, the
development of artificial neural networks and representative networks are
presented. After that, the papers related to microorganism image analysis based
on classical and deep neural networks are reviewed from the perspectives of
different tasks. In the end, the methodology analysis and potential direction
are discussed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unpaired Deep Image Deraining Using Dual Contrastive Learning. (arXiv:2109.02973v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02973">
<div class="article-summary-box-inner">
<span><p>Learning single image deraining (SID) networks from an unpaired set of clean
and rainy images is practical and valuable as acquiring paired real-world data
is almost infeasible. However, without the paired data as the supervision,
learning a SID network is challenging. Moreover, simply using existing unpaired
learning methods (e.g., unpaired adversarial learning and cycle-consistency
constraints) in the SID task is insufficient to learn the underlying
relationship from rainy inputs to clean outputs as there exists significant
domain gap between the rainy and clean images. In this paper, we develop an
effective unpaired SID adversarial framework which explores mutual properties
of the unpaired exemplars by a dual contrastive learning manner in a deep
feature space, named as DCD-GAN. The proposed method mainly consists of two
cooperative branches: Bidirectional Translation Branch (BTB) and Contrastive
Guidance Branch (CGB). Specifically, BTB exploits full advantage of the
circulatory architecture of adversarial consistency to generate abundant
exemplar pairs and excavates latent feature distributions between two domains
by equipping it with bidirectional mapping. Simultaneously, CGB implicitly
constrains the embeddings of different exemplars in the deep feature space by
encouraging the similar feature distributions closer while pushing the
dissimilar further away, in order to better facilitate rain removal and help
image restoration. Extensive experiments demonstrate that our method performs
favorably against existing unpaired deraining approaches on both synthetic and
real-world datasets, and generates comparable results against several
fully-supervised or semi-supervised models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ErfAct and Pserf: Non-monotonic Smooth Trainable Activation Functions. (arXiv:2109.04386v4 [cs.NE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04386">
<div class="article-summary-box-inner">
<span><p>An activation function is a crucial component of a neural network that
introduces non-linearity in the network. The state-of-the-art performance of a
neural network depends also on the perfect choice of an activation function. We
propose two novel non-monotonic smooth trainable activation functions, called
ErfAct and Pserf. Experiments suggest that the proposed functions improve the
network performance significantly compared to the widely used activations like
ReLU, Swish, and Mish. Replacing ReLU by ErfAct and Pserf, we have 5.68% and
5.42% improvement for top-1 accuracy on Shufflenet V2 (2.0x) network in
CIFAR100 dataset, 2.11% and 1.96% improvement for top-1 accuracy on Shufflenet
V2 (2.0x) network in CIFAR10 dataset, 1.0%, and 1.0% improvement on mean
average precision (mAP) on SSD300 model in Pascal VOC dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">High-Resolution Image Harmonization via Collaborative Dual Transformations. (arXiv:2109.06671v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06671">
<div class="article-summary-box-inner">
<span><p>Given a composite image, image harmonization aims to adjust the foreground to
make it compatible with the background. High-resolution image harmonization is
in high demand, but still remains unexplored. Conventional image harmonization
methods learn global RGB-to-RGB transformation which could effortlessly scale
to high resolution, but ignore diverse local context. Recent deep learning
methods learn the dense pixel-to-pixel transformation which could generate
harmonious outputs, but are highly constrained in low resolution. In this work,
we propose a high-resolution image harmonization network with Collaborative
Dual Transformation (CDTNet) to combine pixel-to-pixel transformation and
RGB-to-RGB transformation coherently in an end-to-end network. Our CDTNet
consists of a low-resolution generator for pixel-to-pixel transformation, a
color mapping module for RGB-to-RGB transformation, and a refinement module to
take advantage of both. Extensive experiments on high-resolution benchmark
dataset and our created high-resolution real composite images demonstrate that
our CDTNet strikes a good balance between efficiency and effectiveness. Our
used datasets can be found in
https://github.com/bcmi/CDTNet-High-Resolution-Image-Harmonization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Representation Learning for Reliable Robotic Monitoring of Fruit Anomalies. (arXiv:2109.10135v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10135">
<div class="article-summary-box-inner">
<span><p>Data augmentation can be a simple yet powerful tool for autonomous robots to
fully utilise available data for selfsupervised identification of atypical
scenes or objects. State-of-the-art augmentation methods arbitrarily embed
"structural" peculiarity on typical images so that classifying these artefacts
can provide guidance for learning representations for the detection of
anomalous visual signals. In this paper, however, we argue that learning such
structure-sensitive representations can be a suboptimal approach to some
classes of anomaly (e.g., unhealthy fruits) which could be better recognised by
a different type of visual element such as "colour". We thus propose Channel
Randomisation as a novel data augmentation method for restricting neural
networks to learn encoding of "colour irregularity" whilst predicting
channel-randomised images to ultimately build reliable fruit-monitoring robots
identifying atypical fruit qualities. Our experiments show that (1) this
colour-based alternative can better learn representations for consistently
accurate identification of fruit anomalies in various fruit species, and also,
(2) unlike other methods, the validation accuracy can be utilised as a
criterion for early stopping of training in practice due to positive
correlation between the performance in the self-supervised
colour-differentiation task and the subsequent detection rate of actual
anomalous fruits. Also, the proposed approach is evaluated on a new
agricultural dataset, Riseholme-2021, consisting of 3.5K strawberry images
gathered by a mobile robot, which we share online to encourage active
agri-robotics research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Algorithm Fairness in AI for Medicine and Healthcare. (arXiv:2110.00603v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00603">
<div class="article-summary-box-inner">
<span><p>In the current development and deployment of many artificial intelligence
(AI) systems in healthcare, algorithm fairness is a challenging problem in
delivering equitable care. Recent evaluation of AI models stratified across
race sub-populations have revealed inequalities in how patients are diagnosed,
given treatments, and billed for healthcare costs. In this perspective article,
we summarize the intersectional field of fairness in machine learning through
the context of current issues in healthcare, outline how algorithmic biases
(e.g. - image acquisition, genetic variation, intra-observer labeling
variability) arise in current clinical workflows and their resulting healthcare
disparities. Lastly, we also review emerging technology for mitigating bias via
federated learning, disentanglement, and model explainability, and their role
in AI-SaMD development.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PatchFormer: An Efficient Point Transformer with Patch Attention. (arXiv:2111.00207v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00207">
<div class="article-summary-box-inner">
<span><p>The point cloud learning community witnesses a modeling shift from CNNs to
Transformers, where pure Transformer architectures have achieved top accuracy
on the major learning benchmarks. However, existing point Transformers are
computationally expensive since they need to generate a large attention map,
which has quadratic complexity (both in space and time) with respect to input
size. To solve this shortcoming, we introduce Patch ATtention (PAT) to
adaptively learn a much smaller set of bases upon which the attention maps are
computed. By a weighted summation upon these bases, PAT not only captures the
global shape context but also achieves linear complexity to input size. In
addition, we propose a lightweight Multi-Scale aTtention (MST) block to build
attentions among features of different scales, providing the model with
multi-scale features. Equipped with the PAT and MST, we construct our neural
architecture called PatchFormer that integrates both modules into a joint
framework for point cloud learning. Extensive experiments demonstrate that our
network achieves comparable accuracy on general point cloud learning tasks with
9.2x speed-up than previous point Transformers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synthetic Document Generator for Annotation-free Layout Recognition. (arXiv:2111.06016v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.06016">
<div class="article-summary-box-inner">
<span><p>Analyzing the layout of a document to identify headers, sections, tables,
figures etc. is critical to understanding its content. Deep learning based
approaches for detecting the layout structure of document images have been
promising. However, these methods require a large number of annotated examples
during training, which are both expensive and time consuming to obtain. We
describe here a synthetic document generator that automatically produces
realistic documents with labels for spatial positions, extents and categories
of the layout elements. The proposed generative process treats every physical
component of a document as a random variable and models their intrinsic
dependencies using a Bayesian Network graph. Our hierarchical formulation using
stochastic templates allow parameter sharing between documents for retaining
broad themes and yet the distributional characteristics produces visually
unique samples, thereby capturing complex and diverse layouts. We empirically
illustrate that a deep layout detection model trained purely on the synthetic
documents can match the performance of a model that uses real documents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">More than Words: In-the-Wild Visually-Driven Prosody for Text-to-Speech. (arXiv:2111.10139v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10139">
<div class="article-summary-box-inner">
<span><p>In this paper we present VDTTS, a Visually-Driven Text-to-Speech model.
Motivated by dubbing, VDTTS takes advantage of video frames as an additional
input alongside text, and generates speech that matches the video signal. We
demonstrate how this allows VDTTS to, unlike plain TTS models, generate speech
that not only has prosodic variations like natural pauses and pitch, but is
also synchronized to the input video. Experimentally, we show our model
produces well-synchronized outputs, approaching the video-speech
synchronization quality of the ground-truth, on several challenging benchmarks
including "in-the-wild" content from VoxCeleb2. Supplementary demo videos
demonstrating video-speech synchronization, robustness to speaker ID swapping,
and prosody, presented at the project page.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Imperceptible Transfer Attack and Defense on 3D Point Cloud Classification. (arXiv:2111.10990v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10990">
<div class="article-summary-box-inner">
<span><p>Although many efforts have been made into attack and defense on the 2D image
domain in recent years, few methods explore the vulnerability of 3D models.
Existing 3D attackers generally perform point-wise perturbation over point
clouds, resulting in deformed structures or outliers, which is easily
perceivable by humans. Moreover, their adversarial examples are generated under
the white-box setting, which frequently suffers from low success rates when
transferred to attack remote black-box models. In this paper, we study 3D point
cloud attacks from two new and challenging perspectives by proposing a novel
Imperceptible Transfer Attack (ITA): 1) Imperceptibility: we constrain the
perturbation direction of each point along its normal vector of the
neighborhood surface, leading to generated examples with similar geometric
properties and thus enhancing the imperceptibility. 2) Transferability: we
develop an adversarial transformation model to generate the most harmful
distortions and enforce the adversarial examples to resist it, improving their
transferability to unknown black-box models. Further, we propose to train more
robust black-box 3D models to defend against such ITA attacks by learning more
discriminative point cloud representations. Extensive evaluations demonstrate
that our ITA attack is more imperceptible and transferable than
state-of-the-arts and validate the superiority of our defense strategy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pixel-wise Energy-biased Abstention Learning for Anomaly Segmentation on Complex Urban Driving Scenes. (arXiv:2111.12264v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12264">
<div class="article-summary-box-inner">
<span><p>State-of-the-art (SOTA) anomaly segmentation approaches on complex urban
driving scenes explore pixel-wise classification uncertainty learned from
outlier exposure, or external reconstruction models. However, previous
uncertainty approaches that directly associate high uncertainty to anomaly may
sometimes lead to incorrect anomaly predictions, and external reconstruction
models tend to be too inefficient for real-time self-driving embedded systems.
In this paper, we propose a new anomaly segmentation method, named pixel-wise
energy-biased abstention learning (PEBAL), that explores pixel-wise abstention
learning (AL) with a model that learns an adaptive pixel-level anomaly class,
and an energy-based model (EBM) that learns inlier pixel distribution. More
specifically, PEBAL is based on a non-trivial joint training of EBM and AL,
where EBM is trained to output high-energy for anomaly pixels (from outlier
exposure) and AL is trained such that these high-energy pixels receive adaptive
low penalty for being included to the anomaly class. We extensively evaluate
PEBAL against the SOTA and show that it achieves the best performance across
four benchmarks. Code is available at https://github.com/tianyu0207/PEBAL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Shape Variational Autoencoder Latent Disentanglement via Mini-Batch Feature Swapping for Bodies and Faces. (arXiv:2111.12448v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12448">
<div class="article-summary-box-inner">
<span><p>Learning a disentangled, interpretable, and structured latent representation
in 3D generative models of faces and bodies is still an open problem. The
problem is particularly acute when control over identity features is required.
In this paper, we propose an intuitive yet effective self-supervised approach
to train a 3D shape variational autoencoder (VAE) which encourages a
disentangled latent representation of identity features. Curating the
mini-batch generation by swapping arbitrary features across different shapes
allows to define a loss function leveraging known differences and similarities
in the latent representations. Experimental results conducted on 3D meshes show
that state-of-the-art methods for latent disentanglement are not able to
disentangle identity features of faces and bodies. Our proposed method properly
decouples the generation of such features while maintaining good representation
and reconstruction capabilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perturbed and Strict Mean Teachers for Semi-supervised Semantic Segmentation. (arXiv:2111.12903v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12903">
<div class="article-summary-box-inner">
<span><p>Consistency learning using input image, feature, or network perturbations has
shown remarkable results in semi-supervised semantic segmentation, but this
approach can be seriously affected by inaccurate predictions of unlabelled
training images. There are two consequences of these inaccurate predictions: 1)
the training based on the "strict" cross-entropy (CE) loss can easily overfit
prediction mistakes, leading to confirmation bias; and 2) the perturbations
applied to these inaccurate predictions will use potentially erroneous
predictions as training signals, degrading consistency learning. In this paper,
we address the prediction accuracy problem of consistency learning methods with
novel extensions of the mean-teacher (MT) model, which include a new auxiliary
teacher, and the replacement of MT's mean square error (MSE) by a stricter
confidence-weighted cross-entropy (Conf-CE) loss. The accurate prediction by
this model allows us to use a challenging combination of network, input data
and feature perturbations to improve the consistency learning generalisation,
where the feature perturbations consist of a new adversarial perturbation.
Results on public benchmarks show that our approach achieves remarkable
improvements over the previous SOTA methods in the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predict, Prevent, and Evaluate: Disentangled Text-Driven Image Manipulation Empowered by Pre-Trained Vision-Language Model. (arXiv:2111.13333v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13333">
<div class="article-summary-box-inner">
<span><p>To achieve disentangled image manipulation, previous works depend heavily on
manual annotation. Meanwhile, the available manipulations are limited to a
pre-defined set the models were trained for. We propose a novel framework,
i.e., Predict, Prevent, and Evaluate (PPE), for disentangled text-driven image
manipulation that requires little manual annotation while being applicable to a
wide variety of manipulations. Our method approaches the targets by deeply
exploiting the power of the large-scale pre-trained vision-language model CLIP.
Concretely, we firstly Predict the possibly entangled attributes for a given
text command. Then, based on the predicted attributes, we introduce an
entanglement loss to Prevent entanglements during training. Finally, we propose
a new evaluation metric to Evaluate the disentangled image manipulation. We
verify the effectiveness of our method on the challenging face editing task.
Extensive experiments show that the proposed PPE framework achieves much better
quantitative and qualitative results than the up-to-date StyleCLIP baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LAFITE: Towards Language-Free Training for Text-to-Image Generation. (arXiv:2111.13792v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13792">
<div class="article-summary-box-inner">
<span><p>One of the major challenges in training text-to-image generation models is
the need of a large number of high-quality image-text pairs. While image
samples are often easily accessible, the associated text descriptions typically
require careful human captioning, which is particularly time- and
cost-consuming. In this paper, we propose the first work to train text-to-image
generation models without any text data. Our method leverages the well-aligned
multi-modal semantic space of the powerful pre-trained CLIP model: the
requirement of text-conditioning is seamlessly alleviated via generating text
features from image features. Extensive experiments are conducted to illustrate
the effectiveness of the proposed method. We obtain state-of-the-art results in
the standard text-to-image generation tasks. Importantly, the proposed
language-free model outperforms most existing models trained with full
image-text pairs. Furthermore, our method can be applied in fine-tuning
pre-trained models, which saves both training time and cost in training
text-to-image generation models. Our pre-trained model obtains competitive
results in zero-shot text-to-image generation on the MS-COCO dataset, yet with
around only 1% of the model size and training data size relative to the
recently proposed large DALL-E model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FCAF3D: Fully Convolutional Anchor-Free 3D Object Detection. (arXiv:2112.00322v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00322">
<div class="article-summary-box-inner">
<span><p>Recently, promising applications in robotics and augmented reality have
attracted considerable attention to 3D object detection from point clouds. In
this paper, we present FCAF3D - a first-in-class fully convolutional
anchor-free indoor 3D object detection method. It is a simple yet effective
method that uses a voxel representation of a point cloud and processes voxels
with sparse convolutions. FCAF3D can handle large-scale scenes with minimal
runtime through a single fully convolutional feed-forward pass. Existing 3D
object detection methods make prior assumptions on the geometry of objects, and
we argue that it limits their generalization ability. To get rid of any prior
assumptions, we propose a novel parametrization of oriented bounding boxes that
allows obtaining better results in a purely data-driven way. The proposed
method achieves state-of-the-art 3D object detection results in terms of
mAP@0.5 on ScanNet V2 (+4.5), SUN RGB-D (+3.5), and S3DIS (+20.5) datasets. The
code and models are available at https://github.com/samsunglabs/fcaf3d.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Encouraging Disentangled and Convex Representation with Controllable Interpolation Regularization. (arXiv:2112.03163v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03163">
<div class="article-summary-box-inner">
<span><p>We focus on controllable disentangled representation learning (C-Dis-RL),
where users can control the partition of the disentangled latent space to
factorize dataset attributes (concepts) for downstream tasks. Two general
problems remain under-explored in current methods: (1) They lack comprehensive
disentanglement constraints, especially missing the minimization of mutual
information between different attributes across latent and observation domains.
(2) They lack convexity constraints, which is important for meaningfully
manipulating specific attributes for downstream tasks. To encourage both
comprehensive C-Dis-RL and convexity simultaneously, we propose a simple yet
efficient method: Controllable Interpolation Regularization (CIR), which
creates a positive loop where disentanglement and convexity can help each
other. Specifically, we conduct controlled interpolation in latent space during
training, and we reuse the encoder to help form a 'perfect disentanglement'
regularization. In that case, (a) disentanglement loss implicitly enlarges the
potential understandable distribution to encourage convexity; (b) convexity can
in turn improve robust and precise disentanglement. CIR is a general module and
we merge CIR with three different algorithms: ELEGANT, I2I-Dis, and GZS-Net to
show the compatibility and effectiveness. Qualitative and quantitative
experiments show improvement in C-Dis-RL and latent convexity by CIR. This
further improves downstream tasks: controllable image synthesis, cross-modality
image translation, and zero-shot synthesis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BT-Unet: A self-supervised learning framework for biomedical image segmentation using Barlow Twins with U-Net models. (arXiv:2112.03916v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03916">
<div class="article-summary-box-inner">
<span><p>Deep learning has brought the most profound contribution towards biomedical
image segmentation to automate the process of delineation in medical imaging.
To accomplish such task, the models are required to be trained using huge
amount of annotated or labelled data that highlights the region of interest
with a binary mask. However, efficient generation of the annotations for such
huge data requires expert biomedical analysts and extensive manual effort. It
is a tedious and expensive task, while also being vulnerable to human error. To
address this problem, a self-supervised learning framework, BT-Unet is proposed
that uses the Barlow Twins approach to pre-train the encoder of a U-Net model
via redundancy reduction in an unsupervised manner to learn data
representation. Later, complete network is fine-tuned to perform actual
segmentation. The BT-Unet framework can be trained with a limited number of
annotated samples while having high number of unannotated samples, which is
mostly the case in real-world problems. This framework is validated over
multiple U-Net models over diverse datasets by generating scenarios of a
limited number of labelled samples using standard evaluation metrics. With
exhaustive experiment trials, it is observed that the BT-Unet framework
enhances the performance of the U-Net models with significant margin under such
circumstances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hyperdimensional Feature Fusion for Interpretable Out-Of-Distribution Detection. (arXiv:2112.05341v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05341">
<div class="article-summary-box-inner">
<span><p>We introduce powerful ideas from Hyperdimensional Computing into the
challenging field of Out-of-Distribution (OOD) detection. In contrast to most
existing work that performs OOD detection based on only a single layer of a
neural network, we use similarity-preserving semi-orthogonal projection
matrices to project the feature maps from multiple layers into a common vector
space. By repeatedly applying the bundling operation $\oplus$, we create
expressive class-specific descriptor vectors for all in-distribution classes.
At test time, a simple and efficient cosine similarity calculation between
descriptor vectors consistently identifies OOD samples with better performance
than the current state-of-the-art. We show that the hyperdimensional fusion of
multiple network layers is critical to achieve best general performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph-based Generative Face Anonymisation with Pose Preservation. (arXiv:2112.05496v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05496">
<div class="article-summary-box-inner">
<span><p>We propose AnonyGAN, a GAN-based solution for face anonymisation which
replaces the visual information corresponding to a source identity with a
condition identity provided as any single image. With the goal to maintain the
geometric attributes of the source face, i.e., the facial pose and expression,
and to promote more natural face generation, we propose to exploit a Bipartite
Graph to explicitly model the relations between the facial landmarks of the
source identity and the ones of the condition identity through a deep model. We
further propose a landmark attention model to relax the manual selection of
facial landmarks, allowing the network to weight the landmarks for the best
visual naturalness and pose preservation. Finally, to facilitate the appearance
learning, we propose a hybrid training strategy to address the challenge caused
by the lack of direct pixel-level supervision. We evaluate our method and its
variants on two public datasets, CelebA and LFW, in terms of visual
naturalness, facial pose preservation and of its impacts on face detection and
re-identification. We prove that AnonyGAN significantly outperforms the
state-of-the-art methods in terms of visual naturalness, face detection and
pose preservation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">gACSON software for automated segmentation and morphology analyses of myelinated axons in 3D electron microscopy. (arXiv:2112.06476v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06476">
<div class="article-summary-box-inner">
<span><p>Background and Objective: Advances in electron microscopy (EM) now allow
three-dimensional (3D) imaging of hundreds of micrometers of tissue with
nanometer-scale resolution, providing new opportunities to study the
ultrastructure of the brain. In this work, we introduce a freely available
Matlab-based gACSON software for visualization, segmentation, assessment, and
morphology analysis of myelinated axons in 3D-EM volumes of brain tissue
samples. Methods: The software is equipped with a graphical user interface
(GUI). It automatically segments the intra-axonal space of myelinated axons and
their corresponding myelin sheaths and allows manual segmentation,
proofreading, and interactive correction of the segmented components. gACSON
analyzes the morphology of myelinated axons, such as axonal diameter, axonal
eccentricity, myelin thickness, or g-ratio. Results: We illustrate the use of
the software by segmenting and analyzing myelinated axons in six 3D-EM volumes
of rat somatosensory cortex after sham surgery or traumatic brain injury (TBI).
Our results suggest that the equivalent diameter of myelinated axons in
somatosensory cortex was decreased in TBI animals five months after the injury.
Conclusions: Our results indicate that gACSON is a valuable tool for
visualization, segmentation, assessment, and morphology analysis of myelinated
axons in 3D-EM volumes. It is freely available at
https://github.com/AndreaBehan/g-ACSON under the MIT license.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks. (arXiv:2112.06825v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06825">
<div class="article-summary-box-inner">
<span><p>Recently, fine-tuning language models pre-trained on large text corpora have
provided huge improvements on vision-and-language (V&amp;L) tasks as well as on
pure language tasks. However, fine-tuning the entire parameter set of
pre-trained models becomes impractical since the model size is growing rapidly.
Hence, in this paper, we introduce adapter-based parameter-efficient transfer
learning techniques to V&amp;L models such as VL-BART and VLT5. We evaluate our
methods in a unified multi-task setup on both image-text and video-text
benchmarks. For the image-text tasks, we use four diverse V&amp;L datasets: VQAv2,
GQA, NLVR2 , and MSCOCO image captioning. For video-text tasks, we use TVQA,
How2QA, TVC, and YC2C. With careful training and thorough experiments, we
benchmark three popular adapter-based methods (Adapter, Hyperformer, Compacter)
against the standard full fine-tuning and the recently proposed prompt-tuning
approach. We also enhance the efficiency and performance of adapters by sharing
their weights to attain knowledge across tasks. Our results demonstrate that
training the adapter with the weight-sharing technique (4.18% of total
parameters for image-text tasks and 3.39% for video-text tasks) can match the
performance of fine-tuning the entire model. Lastly, we present a comprehensive
analysis including the combination of adapter and task-specific prompts and the
impact of V&amp;L pre-training on adapters. Our code is available at:
https://github.com/ylsung/VL_adapter.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CrossLoc: Scalable Aerial Localization Assisted by Multimodal Synthetic Data. (arXiv:2112.09081v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09081">
<div class="article-summary-box-inner">
<span><p>We present a visual localization system that learns to estimate camera poses
in the real world with the help of synthetic data. Despite significant progress
in recent years, most learning-based approaches to visual localization target
at a single domain and require a dense database of geo-tagged images to
function well. To mitigate the data scarcity issue and improve the scalability
of the neural localization models, we introduce TOPO-DataGen, a versatile
synthetic data generation tool that traverses smoothly between the real and
virtual world, hinged on the geographic camera viewpoint. New large-scale
sim-to-real benchmark datasets are proposed to showcase and evaluate the
utility of the said synthetic data. Our experiments reveal that synthetic data
generically enhances the neural network performance on real data. Furthermore,
we introduce CrossLoc, a cross-modal visual representation learning approach to
pose estimation that makes full use of the scene coordinate ground truth via
self-supervision. Without any extra data, CrossLoc significantly outperforms
the state-of-the-art methods and achieves substantially higher real-data sample
efficiency. Our code and datasets are all available at
https://crossloc.github.io/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Microfossil Identification via Deep Metric Learning. (arXiv:2112.09490v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09490">
<div class="article-summary-box-inner">
<span><p>We apply deep metric learning for the first time to the problem of
classifying planktic foraminifer shells on microscopic images. This species
recognition task is an important information source and scientific pillar for
reconstructing past climates. All foraminifer CNN recognition pipelines in the
literature produce black-box classifiers that lack visualization options for
human experts and cannot be applied to open-set problems. Here, we benchmark
metric learning against these pipelines, produce the first scientific
visualization of the phenotypic planktic foraminifer morphology space, and
demonstrate that metric learning can be used to cluster species unseen during
training. We show that metric learning outperforms all published CNN-based
state-of-the-art benchmarks in this domain. We evaluate our approach on the
34,640 expert-annotated images of the Endless Forams public library of 35
modern planktic foraminifera species. Our results on this data show leading 92%
accuracy (at 0.84 F1-score) in reproducing expert labels on withheld test data,
and 66.5% accuracy (at 0.70 F1-score) when clustering species never encountered
in training. We conclude that metric learning is highly effective for this
domain and serves as an important tool towards expert-in-the-loop automation of
microfossil identification. Keycode, network weights, and data splits are
published with this paper for full reproducibility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HyperSegNAS: Bridging One-Shot Neural Architecture Search with 3D Medical Image Segmentation using HyperNet. (arXiv:2112.10652v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10652">
<div class="article-summary-box-inner">
<span><p>Semantic segmentation of 3D medical images is a challenging task due to the
high variability of the shape and pattern of objects (such as organs or
tumors). Given the recent success of deep learning in medical image
segmentation, Neural Architecture Search (NAS) has been introduced to find
high-performance 3D segmentation network architectures. However, because of the
massive computational requirements of 3D data and the discrete optimization
nature of architecture search, previous NAS methods require a long search time
or necessary continuous relaxation, and commonly lead to sub-optimal network
architectures. While one-shot NAS can potentially address these disadvantages,
its application in the segmentation domain has not been well studied in the
expansive multi-scale multi-path search space. To enable one-shot NAS for
medical image segmentation, our method, named HyperSegNAS, introduces a
HyperNet to assist super-net training by incorporating architecture topology
information. Such a HyperNet can be removed once the super-net is trained and
introduces no overhead during architecture search. We show that HyperSegNAS
yields better performing and more intuitive architectures compared to the
previous state-of-the-art (SOTA) segmentation networks; furthermore, it can
quickly and accurately find good architecture candidates under different
computing constraints. Our method is evaluated on public datasets from the
Medical Segmentation Decathlon (MSD) challenge, and achieves SOTA performances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalized Few-Shot Semantic Segmentation: All You Need is Fine-Tuning. (arXiv:2112.10982v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10982">
<div class="article-summary-box-inner">
<span><p>Generalized few-shot semantic segmentation was introduced to move beyond only
evaluating few-shot segmentation models on novel classes to include testing
their ability to remember base classes. While the current state-of-the-art
approach is based on meta-learning, it performs poorly and saturates in
learning after observing only a few shots. We propose the first fine-tuning
solution, and demonstrate that it addresses the saturation problem while
achieving state-of-the-art results on two datasets, PASCAL-5i and COCO-20i. We
also show that it outperforms existing methods, whether fine-tuning multiple
final layers or only the final layer. Finally, we present a triplet loss
regularization that shows how to redistribute the balance of performance
between novel and base categories so that there is a smaller gap between them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QuadTree Attention for Vision Transformers. (arXiv:2201.02767v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02767">
<div class="article-summary-box-inner">
<span><p>Transformers have been successful in many vision tasks, thanks to their
capability of capturing long-range dependency. However, their quadratic
computational complexity poses a major obstacle for applying them to vision
tasks requiring dense predictions, such as object detection, feature matching,
stereo, etc. We introduce QuadTree Attention, which reduces the computational
complexity from quadratic to linear. Our quadtree transformer builds token
pyramids and computes attention in a coarse-to-fine manner. At each level, the
top K patches with the highest attention scores are selected, such that at the
next level, attention is only evaluated within the relevant regions
corresponding to these top K patches. We demonstrate that quadtree attention
achieves state-of-the-art performance in various vision tasks, e.g. with 4.0%
improvement in feature matching on ScanNet, about 50% flops reduction in stereo
matching, 0.4-1.5% improvement in top-1 accuracy on ImageNet classification,
1.2-1.8% improvement on COCO object detection, and 0.7-2.4% improvement on
semantic segmentation over previous state-of-the-art transformers. The codes
are available at https://github.com/Tangshitao/QuadtreeAttention.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CrossMoDA 2021 challenge: Benchmark of Cross-Modality Domain Adaptation techniques for Vestibular Schwannoma and Cochlea Segmentation. (arXiv:2201.02831v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02831">
<div class="article-summary-box-inner">
<span><p>Domain Adaptation (DA) has recently raised strong interests in the medical
imaging community. While a large variety of DA techniques has been proposed for
image segmentation, most of these techniques have been validated either on
private datasets or on small publicly available datasets. Moreover, these
datasets mostly addressed single-class problems. To tackle these limitations,
the Cross-Modality Domain Adaptation (crossMoDA) challenge was organised in
conjunction with the 24th International Conference on Medical Image Computing
and Computer Assisted Intervention (MICCAI 2021). CrossMoDA is the first large
and multi-class benchmark for unsupervised cross-modality DA. The challenge's
goal is to segment two key brain structures involved in the follow-up and
treatment planning of vestibular schwannoma (VS): the VS and the cochleas.
Currently, the diagnosis and surveillance in patients with VS are performed
using contrast-enhanced T1 (ceT1) MRI. However, there is growing interest in
using non-contrast sequences such as high-resolution T2 (hrT2) MRI. Therefore,
we created an unsupervised cross-modality segmentation benchmark. The training
set provides annotated ceT1 (N=105) and unpaired non-annotated hrT2 (N=105).
The aim was to automatically perform unilateral VS and bilateral cochlea
segmentation on hrT2 as provided in the testing set (N=137). A total of 16
teams submitted their algorithm for the evaluation phase. The level of
performance reached by the top-performing teams is strikingly high (best median
Dice - VS:88.4%; Cochleas:85.7%) and close to full supervision (median Dice -
VS:92.5%; Cochleas:87.7%). All top-performing methods made use of an
image-to-image translation approach to transform the source-domain images into
pseudo-target-domain images. A segmentation network was then trained using
these generated images and the manual annotations provided for the source
image.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Transformers for Unsupervised Object Discovery using Normalized Cut. (arXiv:2202.11539v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11539">
<div class="article-summary-box-inner">
<span><p>Transformers trained with self-supervised learning using self-distillation
loss (DINO) have been shown to produce attention maps that highlight salient
foreground objects. In this paper, we demonstrate a graph-based approach that
uses the self-supervised transformer features to discover an object from an
image. Visual tokens are viewed as nodes in a weighted graph with edges
representing a connectivity score based on the similarity of tokens. Foreground
objects can then be segmented using a normalized graph-cut to group
self-similar regions. We solve the graph-cut problem using spectral clustering
with generalized eigen-decomposition and show that the second smallest
eigenvector provides a cutting solution since its absolute value indicates the
likelihood that a token belongs to a foreground object. Despite its simplicity,
this approach significantly boosts the performance of unsupervised object
discovery: we improve over the recent state of the art LOST by a margin of
6.9%, 8.1%, and 8.1% respectively on the VOC07, VOC12, and COCO20K. The
performance can be further improved by adding a second stage class-agnostic
detector (CAD). Our proposed method can be easily extended to unsupervised
saliency detection and weakly supervised object detection. For unsupervised
saliency detection, we improve IoU for 4.9%, 5.2%, 12.9% on ECSSD, DUTS,
DUT-OMRON respectively compared to previous state of the art. For weakly
supervised object detection, we achieve competitive performance on CUB and
ImageNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Avalanche RL: a Continual Reinforcement Learning Library. (arXiv:2202.13657v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13657">
<div class="article-summary-box-inner">
<span><p>Continual Reinforcement Learning (CRL) is a challenging setting where an
agent learns to interact with an environment that is constantly changing over
time (the stream of experiences). In this paper, we describe Avalanche RL, a
library for Continual Reinforcement Learning which allows to easily train
agents on a continuous stream of tasks. Avalanche RL is based on PyTorch and
supports any OpenAI Gym environment. Its design is based on Avalanche, one of
the more popular continual learning libraries, which allow us to reuse a large
number of continual learning strategies and improve the interaction between
reinforcement learning and continual learning researchers. Additionally, we
propose Continual Habitat-Lab, a novel benchmark and a high-level library which
enables the usage of the photorealistic simulator Habitat-Sim for CRL research.
Overall, Avalanche RL attempts to unify under a common framework continual
reinforcement learning applications, which we hope will foster the growth of
the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CrossPoint: Self-Supervised Cross-Modal Contrastive Learning for 3D Point Cloud Understanding. (arXiv:2203.00680v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00680">
<div class="article-summary-box-inner">
<span><p>Manual annotation of large-scale point cloud dataset for varying tasks such
as 3D object classification, segmentation and detection is often laborious
owing to the irregular structure of point clouds. Self-supervised learning,
which operates without any human labeling, is a promising approach to address
this issue. We observe in the real world that humans are capable of mapping the
visual concepts learnt from 2D images to understand the 3D world. Encouraged by
this insight, we propose CrossPoint, a simple cross-modal contrastive learning
approach to learn transferable 3D point cloud representations. It enables a
3D-2D correspondence of objects by maximizing agreement between point clouds
and the corresponding rendered 2D image in the invariant space, while
encouraging invariance to transformations in the point cloud modality. Our
joint training objective combines the feature correspondences within and across
modalities, thus ensembles a rich learning signal from both 3D point cloud and
2D image modalities in a self-supervised fashion. Experimental results show
that our approach outperforms the previous unsupervised learning methods on a
diverse range of downstream tasks including 3D object classification and
segmentation. Further, the ablation studies validate the potency of our
approach for a better point cloud understanding. Code and pretrained models are
available at <a href="http://github.com/MohamedAfham/CrossPoint.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Class-Aware Contrastive Semi-Supervised Learning. (arXiv:2203.02261v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02261">
<div class="article-summary-box-inner">
<span><p>Pseudo-label-based semi-supervised learning (SSL) has achieved great success
on raw data utilization. However, its training procedure suffers from
confirmation bias due to the noise contained in self-generated artificial
labels. Moreover, the model's judgment becomes noisier in real-world
applications with extensive out-of-distribution data. To address this issue, we
propose a general method named Class-aware Contrastive Semi-Supervised Learning
(CCSSL), which is a drop-in helper to improve the pseudo-label quality and
enhance the model's robustness in the real-world setting. Rather than treating
real-world data as a union set, our method separately handles reliable
in-distribution data with class-wise clustering for blending into downstream
tasks and noisy out-of-distribution data with image-wise contrastive for better
generalization. Furthermore, by applying target re-weighting, we successfully
emphasize clean label learning and simultaneously reduce noisy label learning.
Despite its simplicity, our proposed CCSSL has significant performance
improvements over the state-of-the-art SSL methods on the standard datasets
CIFAR100 and STL10. On the real-world dataset Semi-iNat 2021, we improve
FixMatch by 9.80% and CoMatch by 3.18%. Code is available
https://github.com/TencentYoutuResearch/Classification-SemiCLS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nuclei instance segmentation and classification in histopathology images with StarDist. (arXiv:2203.02284v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02284">
<div class="article-summary-box-inner">
<span><p>Instance segmentation and classification of nuclei is an important task in
computational pathology. We show that StarDist, a deep learning based nuclei
segmentation method originally developed for fluorescence microscopy, can be
extended and successfully applied to histopathology images. This is
substantiated by conducting experiments on the Lizard dataset, and through
entering the Colon Nuclei Identification and Counting (CoNIC) challenge 2022.
At the end of the preliminary test phase of CoNIC, our approach ranked first on
the leaderboard for the segmentation and classification task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Frequency-driven Imperceptible Adversarial Attack on Semantic Similarity. (arXiv:2203.05151v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05151">
<div class="article-summary-box-inner">
<span><p>Current adversarial attack research reveals the vulnerability of
learning-based classifiers against carefully crafted perturbations. However,
most existing attack methods have inherent limitations in cross-dataset
generalization as they rely on a classification layer with a closed set of
categories. Furthermore, the perturbations generated by these methods may
appear in regions easily perceptible to the human visual system (HVS). To
circumvent the former problem, we propose a novel algorithm that attacks
semantic similarity on feature representations. In this way, we are able to
fool classifiers without limiting attacks to a specific dataset. For
imperceptibility, we introduce the low-frequency constraint to limit
perturbations within high-frequency components, ensuring perceptual similarity
between adversarial examples and originals. Extensive experiments on three
datasets (CIFAR-10, CIFAR-100, and ImageNet-1K) and three public online
platforms indicate that our attack can yield misleading and transferable
adversarial examples across architectures and datasets. Additionally,
visualization results and quantitative performance (in terms of four different
metrics) show that the proposed algorithm generates more imperceptible
perturbations than the state-of-the-art methods. Code is made available at.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mixed-Precision Neural Network Quantization via Learned Layer-wise Importance. (arXiv:2203.08368v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08368">
<div class="article-summary-box-inner">
<span><p>The exponentially large discrete search space in mixed-precision quantization
(MPQ) makes it hard to determine the optimal bit-width for each layer. Previous
works usually resort to iterative search methods on the training set, which
consume hundreds or even thousands of GPU-hours. In this study, we reveal that
some unique learnable parameters in quantization, namely the scale factors in
the quantizer, can serve as importance indicators of a layer, reflecting the
contribution of that layer to the final accuracy at certain bit-widths. These
importance indicators naturally perceive the numerical transformation during
quantization-aware training, which can precisely and correctly provide
quantization sensitivity metrics of layers. However, a deep network always
contains hundreds of such indicators, and training them one by one would lead
to an excessive time cost. To overcome this issue, we propose a joint training
scheme that can obtain all indicators at once. It considerably speeds up the
indicators training process by parallelizing the original sequential training
processes. With these learned importance indicators, we formulate the MPQ
search problem as a one-time integer linear programming (ILP) problem. That
avoids the iterative search and significantly reduces search time without
limiting the bit-width search space. For example, MPQ search on ResNet18 with
our indicators takes only 0.06 seconds. Also, extensive experiments show our
approach can achieve SOTA accuracy on ImageNet for far-ranging models with
various constraints (e.g., BitOps, compress rate).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FormNet: Structural Encoding beyond Sequential Modeling in Form Document Information Extraction. (arXiv:2203.08411v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08411">
<div class="article-summary-box-inner">
<span><p>Sequence modeling has demonstrated state-of-the-art performance on natural
language and document understanding tasks. However, it is challenging to
correctly serialize tokens in form-like documents in practice due to their
variety of layout patterns. We propose FormNet, a structure-aware sequence
model to mitigate the suboptimal serialization of forms. First, we design Rich
Attention that leverages the spatial relationship between tokens in a form for
more precise attention score calculation. Second, we construct Super-Tokens for
each word by embedding representations from their neighboring tokens through
graph convolutions. FormNet therefore explicitly recovers local syntactic
information that may have been lost during serialization. In experiments,
FormNet outperforms existing methods with a more compact model size and less
pre-training data, establishing new state-of-the-art performance on CORD, FUNSD
and Payment benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ContrastMask: Contrastive Learning to Segment Every Thing. (arXiv:2203.09775v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09775">
<div class="article-summary-box-inner">
<span><p>Partially-supervised instance segmentation is a task which requests
segmenting objects from novel unseen categories via learning on limited seen
categories with annotated masks thus eliminating demands of heavy annotation
burden. The key to addressing this task is to build an effective class-agnostic
mask segmentation model. Unlike previous methods that learn such models only on
seen categories, in this paper, we propose a new method, named ContrastMask,
which learns a mask segmentation model on both seen and unseen categories under
a unified pixel-level contrastive learning framework. In this framework,
annotated masks of seen categories and pseudo masks of unseen categories serve
as a prior for contrastive learning, where features from the mask regions
(foreground) are pulled together, and are contrasted against those from the
background, and vice versa. Through this framework, feature discrimination
between foreground and background is largely improved, facilitating learning of
the class-agnostic mask segmentation model. Exhaustive experiments on the COCO
dataset demonstrate the superiority of our method, which outperforms previous
state-of-the-arts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Human-Gaze-Target Detection with Transformers. (arXiv:2203.10433v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10433">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose an effective and efficient method for
Human-Gaze-Target (HGT) detection, i.e., gaze following. Current approaches
decouple the HGT detection task into separate branches of salient object
detection and human gaze prediction, employing a two-stage framework where
human head locations must first be detected and then be fed into the next gaze
target prediction sub-network. In contrast, we redefine the HGT detection task
as detecting human head locations and their gaze targets, simultaneously. By
this way, our method, named Human-Gaze-Target detection TRansformer or HGTTR,
streamlines the HGT detection pipeline by eliminating all other additional
components. HGTTR reasons about the relations of salient objects and human gaze
from the global image context. Moreover, unlike existing two-stage methods that
require human head locations as input and can predict only one human's gaze
target at a time, HGTTR can directly predict the locations of all people and
their gaze targets at one time in an end-to-end manner. The effectiveness and
robustness of our proposed method are verified with extensive experiments on
the two standard benchmark datasets, GazeFollowing and VideoAttentionTarget.
Without bells and whistles, HGTTR outperforms existing state-of-the-art methods
by large margins (6.4 mAP gain on GazeFollowing and 10.3 mAP gain on
VideoAttentionTarget) with a much simpler architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Associating Objects with Scalable Transformers for Video Object Segmentation. (arXiv:2203.11442v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11442">
<div class="article-summary-box-inner">
<span><p>This paper investigates how to realize better and more efficient embedding
learning to tackle the semi-supervised video object segmentation under
challenging multi-object scenarios. The state-of-the-art methods learn to
decode features with a single positive object and thus have to match and
segment each target separately under multi-object scenarios, consuming multiple
times computation resources. To solve the problem, we propose an Associating
Objects with Transformers (AOT) approach to match and decode multiple objects
jointly and collaboratively. In detail, AOT employs an identification mechanism
to associate multiple targets into the same high-dimensional embedding space.
Thus, we can simultaneously process multiple objects' matching and segmentation
decoding as efficiently as processing a single object. To sufficiently model
multi-object association, a Long Short-Term Transformer (LSTT) is devised to
construct hierarchical matching and propagation. Based on AOT, we further
propose a more flexible and robust framework, Associating Objects with Scalable
Transformers (AOST), in which a scalable version of LSTT is designed to enable
run-time adaptation of accuracy-efficiency trade-offs. Besides, AOST introduces
a better layer-wise manner to couple identification and vision embeddings. We
conduct extensive experiments on multi-object and single-object benchmarks to
examine AOT series frameworks. Compared to the state-of-the-art competitors,
our methods can maintain times of run-time efficiency with superior
performance. Notably, we achieve new state-of-the-art performance on three
popular benchmarks, i.e., YouTube-VOS (86.5%), DAVIS 2017 Val/Test
(87.0%/84.7%), and DAVIS 2016 (93.0%). Project page:
https://github.com/z-x-yang/AOT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AP-BSN: Self-Supervised Denoising for Real-World Images via Asymmetric PD and Blind-Spot Network. (arXiv:2203.11799v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11799">
<div class="article-summary-box-inner">
<span><p>Blind-spot network (BSN) and its variants have made significant advances in
self-supervised denoising. Nevertheless, they are still bound to synthetic
noisy inputs due to less practical assumptions like pixel-wise independent
noise. Hence, it is challenging to deal with spatially correlated real-world
noise using self-supervised BSN. Recently, pixel-shuffle downsampling (PD) has
been proposed to remove the spatial correlation of real-world noise. However,
it is not trivial to integrate PD and BSN directly, which prevents the fully
self-supervised denoising model on real-world images. We propose an Asymmetric
PD (AP) to address this issue, which introduces different PD stride factors for
training and inference. We systematically demonstrate that the proposed AP can
resolve inherent trade-offs caused by specific PD stride factors and make BSN
applicable to practical scenarios. To this end, we develop AP-BSN, a
state-of-the-art self-supervised denoising method for real-world sRGB images.
We further propose random-replacing refinement, which significantly improves
the performance of our AP-BSN without any additional parameters. Extensive
studies demonstrate that our method outperforms the other self-supervised and
even unpaired denoising methods by a large margin, without using any additional
knowledge, e.g., noise level, regarding the underlying unknown noise.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Generalization in Federated Learning by Seeking Flat Minima. (arXiv:2203.11834v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11834">
<div class="article-summary-box-inner">
<span><p>Models trained in federated settings often suffer from degraded performances
and fail at generalizing, especially when facing heterogeneous scenarios. In
this work, we investigate such behavior through the lens of geometry of the
loss and Hessian eigenspectrum, linking the model's lack of generalization
capacity to the sharpness of the solution. Motivated by prior studies
connecting the sharpness of the loss surface and the generalization gap, we
show that i) training clients locally with Sharpness-Aware Minimization (SAM)
or its adaptive version (ASAM) and ii) averaging stochastic weights (SWA) on
the server-side can substantially improve generalization in Federated Learning
and help bridging the gap with centralized models. By seeking parameters in
neighborhoods having uniform low loss, the model converges towards flatter
minima and its generalization significantly improves in both homogeneous and
heterogeneous scenarios. Empirical results demonstrate the effectiveness of
those optimizers across a variety of benchmark vision datasets (e.g.
CIFAR10/100, Landmarks-User-160k, IDDA) and tasks (large scale classification,
semantic segmentation, domain generalization).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GradViT: Gradient Inversion of Vision Transformers. (arXiv:2203.11894v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11894">
<div class="article-summary-box-inner">
<span><p>In this work we demonstrate the vulnerability of vision transformers (ViTs)
to gradient-based inversion attacks. During this attack, the original data
batch is reconstructed given model weights and the corresponding gradients. We
introduce a method, named GradViT, that optimizes random noise into naturally
looking images via an iterative process. The optimization objective consists of
(i) a loss on matching the gradients, (ii) image prior in the form of distance
to batch-normalization statistics of a pretrained CNN model, and (iii) a total
variation regularization on patches to guide correct recovery locations. We
propose a unique loss scheduling function to overcome local minima during
optimization. We evaluate GadViT on ImageNet1K and MS-Celeb-1M datasets, and
observe unprecedentedly high fidelity and closeness to the original (hidden)
data. During the analysis we find that vision transformers are significantly
more vulnerable than previously studied CNNs due to the presence of the
attention mechanism. Our method demonstrates new state-of-the-art results for
gradient inversion in both qualitative and quantitative metrics. Project page
at https://gradvit.github.io/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Feature Learning and Relation Modeling for Tracking: A One-Stream Framework. (arXiv:2203.11991v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11991">
<div class="article-summary-box-inner">
<span><p>The current popular two-stream, two-stage tracking framework extracts the
template and the search region features separately and then performs relation
modeling, thus the extracted features lack the awareness of the target and have
limited target-background discriminability. To tackle the above issue, we
propose a novel one-stream tracking (OSTrack) framework that unifies feature
learning and relation modeling by bridging the template-search image pairs with
bidirectional information flows. In this way, discriminative target-oriented
features can be dynamically extracted by mutual guidance. Since no extra heavy
relation modeling module is needed and the implementation is highly
parallelized, the proposed tracker runs at a fast speed. To further improve the
inference efficiency, an in-network candidate early elimination module is
proposed based on the strong similarity prior calculated in the one-stream
framework. As a unified framework, OSTrack achieves state-of-the-art
performance on multiple benchmarks, in particular, it shows impressive results
on the one-shot tracking benchmark GOT-10k, i.e., achieving 73.7% AO, improving
the existing best result (SwinTrack) by 4.3%. Besides, our method maintains a
good performance-speed trade-off and shows faster convergence. The code and
models will be available at https://github.com/botaoye/OSTrack.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Portrait Delighting. (arXiv:2203.12088v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12088">
<div class="article-summary-box-inner">
<span><p>We present a deep neural network for removing undesirable shading features
from an unconstrained portrait image, recovering the underlying texture. Our
training scheme incorporates three regularization strategies: masked loss, to
emphasize high-frequency shading features; soft-shadow loss, which improves
sensitivity to subtle changes in lighting; and shading-offset estimation, to
supervise separation of shading and texture. Our method demonstrates improved
delighting quality and generalization when compared with the state-of-the-art.
We further demonstrate how our delighting method can enhance the performance of
light-sensitive computer vision tasks such as face relighting and semantic
parsing, allowing them to handle extreme lighting conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Semi-Supervised Deep Facial Expression Recognition with An Adaptive Confidence Margin. (arXiv:2203.12341v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12341">
<div class="article-summary-box-inner">
<span><p>Only parts of unlabeled data are selected to train models for most
semi-supervised learning methods, whose confidence scores are usually higher
than the pre-defined threshold (i.e., the confidence margin). We argue that the
recognition performance should be further improved by making full use of all
unlabeled data. In this paper, we learn an Adaptive Confidence Margin (Ada-CM)
to fully leverage all unlabeled data for semi-supervised deep facial expression
recognition. All unlabeled samples are partitioned into two subsets by
comparing their confidence scores with the adaptively learned confidence margin
at each training epoch: (1) subset I including samples whose confidence scores
are no lower than the margin; (2) subset II including samples whose confidence
scores are lower than the margin. For samples in subset I, we constrain their
predictions to match pseudo labels. Meanwhile, samples in subset II participate
in the feature-level contrastive objective to learn effective facial expression
features. We extensively evaluate Ada-CM on four challenging datasets, showing
that our method achieves state-of-the-art performance, especially surpassing
fully-supervised baselines in a semi-supervised manner. Ablation study further
proves the effectiveness of our method. The source code is available at
https://github.com/hangyu94/Ada-CM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Hybrid Mesh-neural Representation for 3D Transparent Object Reconstruction. (arXiv:2203.12613v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12613">
<div class="article-summary-box-inner">
<span><p>We propose a novel method to reconstruct the 3D shapes of transparent objects
using hand-held captured images under natural light conditions. It combines the
advantage of explicit mesh and multi-layer perceptron (MLP) network, a hybrid
representation, to simplify the capture setting used in recent contributions.
After obtaining an initial shape through the multi-view silhouettes, we
introduce surface-based local MLPs to encode the vertex displacement field
(VDF) for the reconstruction of surface details. The design of local MLPs
allows to represent the VDF in a piece-wise manner using two layer MLP
networks, which is beneficial to the optimization algorithm. Defining local
MLPs on the surface instead of the volume also reduces the searching space.
Such a hybrid representation enables us to relax the ray-pixel correspondences
that represent the light path constraint to our designed ray-cell
correspondences, which significantly simplifies the implementation of
single-image based environment matting algorithm. We evaluate our
representation and reconstruction algorithm on several transparent objects with
ground truth models. Our experiments show that our method can produce
high-quality reconstruction results superior to state-of-the-art methods using
a simplified data acquisition setup.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-03-25 23:07:53.757871390 UTC">2022-03-25 23:07:53 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>