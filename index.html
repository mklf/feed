<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-06-15T01:30:00Z">06-15</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Don't "research fast and break things": On the ethics of Computational Social Science. (arXiv:2206.06370v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06370">
<div class="article-summary-box-inner">
<span><p>This article is concerned with setting up practical guardrails within the
research activities and environments of CSS. It aims to provide CSS scholars,
as well as policymakers and other stakeholders who apply CSS methods, with the
critical and constructive means needed to ensure that their practices are
ethical, trustworthy, and responsible. It begins by providing a taxonomy of the
ethical challenges faced by researchers in the field of CSS. These are
challenges related to (1) the treatment of research subjects, (2) the impacts
of CSS research on affected individuals and communities, (3) the quality of CSS
research and to its epistemological status, (4) research integrity, and (5)
research equity. Taking these challenges as a motivation for cultural
transformation, it then argues for the end-to-end incorporation of habits of
responsible research and innovation (RRI) into CSS practices, focusing on the
role that contextual considerations, anticipatory reflection, impact
assessment, public engagement, and justifiable and well-documented action
should play across the research lifecycle. In proposing the inclusion of habits
of RRI in CSS practices, the chapter lays out several practical steps needed
for ethical, trustworthy, and responsible CSS research activities. These
include stakeholder engagement processes, research impact assessments, data
lifecycle documentation, bias self-assessments, and transparent research
reporting protocols.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Exploration of Post-Editing Effectiveness in Text Summarization. (arXiv:2206.06383v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06383">
<div class="article-summary-box-inner">
<span><p>Automatic summarization methods are efficient but can suffer from low
quality. In comparison, manual summarization is expensive but produces higher
quality. Can humans and AI collaborate to improve summarization performance? In
similar text generation tasks (e.g., machine translation), human-AI
collaboration in the form of "post-editing" AI-generated text reduces human
workload and improves the quality of AI output. Therefore, we explored whether
post-editing offers advantages in text summarization. Specifically, we
conducted an experiment with 72 participants, comparing post-editing provided
summaries with manual summarization for summary quality, human efficiency, and
user experience on formal (XSum news) and informal (Reddit posts) text. This
study sheds valuable insights on when post-editing is useful for text
summarization: it helped in some cases (e.g., when participants lacked domain
knowledge) but not in others (e.g., when provided summaries include inaccurate
information). Participants' different editing strategies and needs for
assistance offer implications for future human-AI summarization systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hate Speech and Counter Speech Detection: Conversational Context Does Matter. (arXiv:2206.06423v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06423">
<div class="article-summary-box-inner">
<span><p>Hate speech is plaguing the cyberspace along with user-generated content.
This paper investigates the role of conversational context in the annotation
and detection of online hate and counter speech, where context is defined as
the preceding comment in a conversation thread. We created a context-aware
dataset for a 3-way classification task on Reddit comments: hate speech,
counter speech, or neutral. Our analyses indicate that context is critical to
identify hate and counter speech: human judgments change for most comments
depending on whether we show annotators the context. A linguistic analysis
draws insights into the language people use to express hate and counter speech.
Experimental results show that neural networks obtain significantly better
results if context is taken into account. We also present qualitative error
analyses shedding light into (a) when and why context is beneficial and (b) the
remaining errors made by our best model when context is taken into account.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Memory-Based Model Editing at Scale. (arXiv:2206.06520v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06520">
<div class="article-summary-box-inner">
<span><p>Even the largest neural networks make errors, and once-correct predictions
can become invalid as the world changes. Model editors make local updates to
the behavior of base (pre-trained) models to inject updated knowledge or
correct undesirable behaviors. Existing model editors have shown promise, but
also suffer from insufficient expressiveness: they struggle to accurately model
an edit's intended scope (examples affected by the edit), leading to inaccurate
predictions for test inputs loosely related to the edit, and they often fail
altogether after many edits. As a higher-capacity alternative, we propose
Semi-Parametric Editing with a Retrieval-Augmented Counterfactual Model
(SERAC), which stores edits in an explicit memory and learns to reason over
them to modulate the base model's predictions as needed. To enable more
rigorous evaluation of model editors, we introduce three challenging language
model editing problems based on question answering, fact-checking, and dialogue
generation. We find that only SERAC achieves high performance on all three
problems, consistently outperforming existing approaches to model editing by a
significant margin. Code, data, and additional project information will be made
available at https://sites.google.com/view/serac-editing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning. (arXiv:2206.06522v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06522">
<div class="article-summary-box-inner">
<span><p>Fine-tuning large pre-trained models on downstream tasks has been adopted in
a variety of domains recently. However, it is costly to update the entire
parameter set of large pre-trained models. Although recently proposed
parameter-efficient transfer learning (PETL) techniques allow updating a small
subset of parameters (e.g. only using 2% of parameters) inside a pre-trained
backbone network for a new task, they only reduce the training memory
requirement by up to 30%. This is because the gradient computation for the
trainable parameters still requires backpropagation through the large
pre-trained backbone model. To address this, we propose Ladder Side-Tuning
(LST), a new PETL technique that reduces training memory requirements by more
substantial amounts. Unlike existing parameter-efficient methods that insert
additional parameters inside backbone networks, we train a ladder side network,
a small and separate network that takes intermediate activations as input via
shortcut connections (ladders) from backbone networks and makes predictions.
LST has significantly lower memory requirements than previous methods, because
it does not require backpropagation through the backbone network, but instead
only through the side network and ladder connections. We evaluate our method
with various models (T5, CLIP-T5) on both NLP (GLUE) and vision-language (VQA,
GQA, NLVR2, MSCOCO) tasks. LST saves 69% of the memory costs to fine-tune the
whole network, while other methods only save 26% of that in similar parameter
usages (hence, 2.7x more memory savings). Moreover, LST achieves higher
accuracy than Adapter and LoRA in a low-memory regime. To further show the
advantage of this better memory efficiency, we also apply LST to larger T5
models (T5-large, T5-3B), attaining better GLUE performance than full
fine-tuning and other PETL methods. The exact same trend also holds in our
experiments on VL tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LIFT: Language-Interfaced Fine-Tuning for Non-Language Machine Learning Tasks. (arXiv:2206.06565v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06565">
<div class="article-summary-box-inner">
<span><p>Fine-tuning pretrained language models (LMs) without making any architectural
changes has become a norm for learning various language downstream tasks.
However, for non-language downstream tasks, a common practice is to employ
task-specific designs for input, output layers, and loss functions. For
instance, it is possible to fine-tune an LM into an MNIST classifier by
replacing the word embedding layer with an image patch embedding layer, the
word token output layer with a 10-way output layer, and the word prediction
loss with a 10-way classification loss, respectively. A natural question
arises: can LM fine-tuning solve non-language downstream tasks without changing
the model architecture or loss function? To answer this, we propose
Language-Interfaced Fine-Tuning (LIFT) and study its efficacy and limitations
by conducting an extensive empirical study on a suite of non-language
classification and regression tasks. LIFT does not make any changes to the
model architecture or loss function, and it solely relies on the natural
language interface, enabling "no-code machine learning with LMs." We find that
LIFT performs relatively well across a wide range of low-dimensional
classification and regression tasks, matching the performances of the best
baselines in many cases, especially for the classification tasks. We report the
experimental results on the fundamental properties of LIFT, including its
inductive bias, sample efficiency, ability to extrapolate, robustness to
outliers and label noise, and generalization. We also analyze a few
properties/techniques specific to LIFT, e.g., context-aware learning via
appropriate prompting, quantification of predictive uncertainty, and two-stage
fine-tuning. Our code is available at
https://github.com/UW-Madison-Lee-Lab/LanguageInterfacedFineTuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CHQ-Summ: A Dataset for Consumer Healthcare Question Summarization. (arXiv:2206.06581v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06581">
<div class="article-summary-box-inner">
<span><p>The quest for seeking health information has swamped the web with consumers'
health-related questions. Generally, consumers use overly descriptive and
peripheral information to express their medical condition or other healthcare
needs, contributing to the challenges of natural language understanding. One
way to address this challenge is to summarize the questions and distill the key
information of the original question. To address this issue, we introduce a new
dataset, CHQ-Summ that contains 1507 domain-expert annotated consumer health
questions and corresponding summaries. The dataset is derived from the
community question-answering forum and therefore provides a valuable resource
for understanding consumer health-related posts on social media. We benchmark
the dataset on multiple state-of-the-art summarization models to show the
effectiveness of the dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FreeTransfer-X: Safe and Label-Free Cross-Lingual Transfer from Off-the-Shelf Models. (arXiv:2206.06586v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06586">
<div class="article-summary-box-inner">
<span><p>Cross-lingual transfer (CLT) is of various applications. However, labeled
cross-lingual corpus is expensive or even inaccessible, especially in the
fields where labels are private, such as diagnostic results of symptoms in
medicine and user profiles in business. Nevertheless, there are off-the-shelf
models in these sensitive fields. Instead of pursuing the original labels, a
workaround for CLT is to transfer knowledge from the off-the-shelf models
without labels. To this end, we define a novel CLT problem named FreeTransfer-X
that aims to achieve knowledge transfer from the off-the-shelf models in
rich-resource languages. To address the problem, we propose a 2-step knowledge
distillation (KD, Hinton et al., 2015) framework based on multilingual
pre-trained language models (mPLM). The significant improvement over strong
neural machine translation (NMT) baselines demonstrates the effectiveness of
the proposed method. In addition to reducing annotation cost and protecting
private labels, the proposed method is compatible with different networks and
easy to be deployed. Finally, a range of analyses indicate the great potential
of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Astock: A New Dataset and Automated Stock Trading based on Stock-specific News Analyzing Model. (arXiv:2206.06606v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06606">
<div class="article-summary-box-inner">
<span><p>Natural Language Processing(NLP) demonstrates a great potential to support
financial decision-making by analyzing the text from social media or news
outlets. In this work, we build a platform to study the NLP-aided stock
auto-trading algorithms systematically. In contrast to the previous work, our
platform is characterized by three features: (1) We provide financial news for
each specific stock. (2) We provide various stock factors for each stock. (3)
We evaluate performance from more financial-relevant metrics. Such a design
allows us to develop and evaluate NLP-aided stock auto-trading algorithms in a
more realistic setting. In addition to designing an evaluation platform and
dataset collection, we also made a technical contribution by proposing a system
to automatically learn a good feature representation from various input
information. The key to our algorithm is a method called semantic role labeling
Pooling (SRLP), which leverages Semantic Role Labeling (SRL) to create a
compact representation of each news paragraph. Based on SRLP, we further
incorporate other stock factors to make the final prediction. In addition, we
propose a self-supervised learning strategy based on SRLP to enhance the
out-of-distribution generalization performance of our system. Through our
experimental study, we show that the proposed method achieves better
performance and outperforms all the baselines' annualized rate of return as
well as the maximum drawdown of the CSI300 index and XIN9 index on real
trading. Our Astock dataset and code are available at
https://github.com/JinanZou/Astock.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task Transfer and Domain Adaptation for Zero-Shot Question Answering. (arXiv:2206.06705v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06705">
<div class="article-summary-box-inner">
<span><p>Pretrained language models have shown success in various areas of natural
language processing, including reading comprehension tasks. However, when
applying machine learning methods to new domains, labeled data may not always
be available. To address this, we use supervised pretraining on source-domain
data to reduce sample complexity on domain-specific downstream tasks. We
evaluate zero-shot performance on domain-specific reading comprehension tasks
by combining task transfer with domain adaptation to fine-tune a pretrained
model with no labelled data from the target task. Our approach outperforms
Domain-Adaptive Pretraining on downstream domain-specific reading comprehension
tasks in 3 out of 4 domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Causal Structure of Semantic Ambiguities. (arXiv:2206.06807v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06807">
<div class="article-summary-box-inner">
<span><p>Ambiguity is a natural language phenomenon occurring at different levels of
syntax, semantics, and pragmatics. It is widely studied; in Psycholinguistics,
for instance, we have a variety of competing studies for the human
disambiguation processes. These studies are empirical and based on eyetracking
measurements. Here we take first steps towards formalizing these processes for
semantic ambiguities where we identified the presence of two features: (1)
joint plausibility degrees of different possible interpretations, (2) causal
structures according to which certain words play a more substantial role in the
processes. The novel sheaf-theoretic model of definite causality developed by
Gogioso and Pinzani in QPL 2021 offers tools to model and reason about these
features. We applied this theory to a dataset of ambiguous phrases extracted
from Psycholinguistics literature and their human plausibility judgements
collected by us using the Amazon Mechanical Turk engine. We measured the causal
fractions of different disambiguation orders within the phrases and discovered
two prominent orders: from subject to verb in the subject-verb and from object
to verb in the verb object phrases. We also found evidence for delay in the
disambiguation of polysemous vs homonymous verbs, again compatible with
Psycholinguistic findings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"hasSignification()": une nouvelle fonction de distance pour soutenir la d\'etection de donn\'ees personnelles. (arXiv:2206.06836v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06836">
<div class="article-summary-box-inner">
<span><p>Today with Big Data and data lakes, we are faced of a mass of data that is
very difficult to manage it manually. The protection of personal data in this
context requires an automatic analysis for data discovery. Storing the names of
attributes already analyzed in a knowledge base could optimize this automatic
discovery. To have a better knowledge base, we should not store any attributes
whose name does not make sense. In this article, to check if the name of an
attribute has a meaning, we propose a solution that calculate the distances
between this name and the words in a dictionary. Our studies on the distance
functions like N-Gram, Jaro-Winkler and Levenshtein show limits to set an
acceptance threshold for an attribute in the knowledge base. In order to
overcome these limitations, our solution aims to strengthen the score
calculation by using an exponential function based on the longest sequence. In
addition, a double scan in dictionary is also proposed in order to process the
attributes which have a compound name.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Natural Language Sentence Generation from API Specifications. (arXiv:2206.06868v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06868">
<div class="article-summary-box-inner">
<span><p>APIs are everywhere; they provide access to automation solutions that could
help businesses automate some of their tasks. Unfortunately, they may not be
accessible to the business users who need them but are not equipped with the
necessary technical skills to leverage them. Wrapping these APIs with chatbot
capabilities is one solution to make these automation solutions interactive. In
this work, we propose a system to generate sentences to train intent
recognition models, a crucial component within chatbots to understand natural
language utterances from users. Evaluation of our approach based on deep
learning models showed promising and inspiring results, and the
human-in-the-loop interaction will provide further improvement on the system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CERT: Continual Pre-Training on Sketches for Library-Oriented Code Generation. (arXiv:2206.06888v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06888">
<div class="article-summary-box-inner">
<span><p>Code generation is a longstanding challenge, aiming to generate a code
snippet based on a natural language description. Usually, expensive text-code
paired data is essential for training a code generation model. Recently, thanks
to the success of pre-training techniques, large language models are trained on
large-scale unlabelled code corpora and perform well in code generation. In
this paper, we investigate how to leverage an unlabelled code corpus to train a
model for library-oriented code generation. Since it is a common practice for
programmers to reuse third-party libraries, in which case the text-code paired
data are harder to obtain due to the huge number of libraries. We observe that
library-oriented code snippets are more likely to share similar code sketches.
Hence, we present CERT with two steps: a sketcher generates the sketch, then a
generator fills the details in the sketch. Both the sketcher and the generator
are continually pre-trained upon a base model using unlabelled data.
Furthermore, we craft two benchmarks named PandasEval and NumpyEval to evaluate
library-oriented code generation. Experimental results demonstrate the
impressive performance of CERT. For example, it surpasses the base model by an
absolute 15.67% improvement in terms of pass@1 on PandasEval. Our work is
available at https://github.com/microsoft/PyCodeGPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RDU: A Region-based Approach to Form-style Document Understanding. (arXiv:2206.06890v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06890">
<div class="article-summary-box-inner">
<span><p>Key Information Extraction (KIE) is aimed at extracting structured
information (e.g. key-value pairs) from form-style documents (e.g. invoices),
which makes an important step towards intelligent document understanding.
Previous approaches generally tackle KIE by sequence tagging, which faces
difficulty to process non-flatten sequences, especially for table-text mixed
documents. These approaches also suffer from the trouble of pre-defining a
fixed set of labels for each type of documents, as well as the label imbalance
issue. In this work, we assume Optical Character Recognition (OCR) has been
applied to input documents, and reformulate the KIE task as a region prediction
problem in the two-dimensional (2D) space given a target field. Following this
new setup, we develop a new KIE model named Region-based Document Understanding
(RDU) that takes as input the text content and corresponding coordinates of a
document, and tries to predict the result by localizing a bounding-box-like
region. Our RDU first applies a layout-aware BERT equipped with a soft layout
attention masking and bias mechanism to incorporate layout information into the
representations. Then, a list of candidate regions is generated from the
representations via a Region Proposal Module inspired by computer vision models
widely applied for object detection. Finally, a Region Categorization Module
and a Region Selection Module are adopted to judge whether a proposed region is
valid and select the one with the largest probability from all proposed regions
respectively. Experiments on four types of form-style documents show that our
proposed method can achieve impressive results. In addition, our RDU model can
be trained with different document types seamlessly, which is especially
helpful over low-resource documents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Maximum Linear Arrangement for trees under projectivity and planarity. (arXiv:2206.06924v1 [cs.DS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06924">
<div class="article-summary-box-inner">
<span><p>The Maximum Linear Arrangement problem (MaxLA) consists of finding a mapping
$\pi$ from the $n$ vertices of a graph $G$ to distinct consecutive integers
that maximizes $D_{\pi}(G)=\sum_{uv\in E(G)}|\pi(u) - \pi(v)|$. In this
setting, vertices are considered to lie on a horizontal line and edges are
drawn as semicircles above the line. There exist variants of MaxLA in which the
arrangements are constrained. In the planar variant edge crossings are
forbidden. In the projective variant for rooted trees arrangements are planar
and the root cannot be covered by any edge. Here we present $O(n)$-time and
$O(n)$-space algorithms that solve Planar and Projective MaxLA for trees. We
also prove several properties of maximum projective and planar arrangements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comprehending and Ordering Semantics for Image Captioning. (arXiv:2206.06930v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06930">
<div class="article-summary-box-inner">
<span><p>Comprehending the rich semantics in an image and ordering them in linguistic
order are essential to compose a visually-grounded and linguistically coherent
description for image captioning. Modern techniques commonly capitalize on a
pre-trained object detector/classifier to mine the semantics in an image, while
leaving the inherent linguistic ordering of semantics under-exploited. In this
paper, we propose a new recipe of Transformer-style structure, namely
Comprehending and Ordering Semantics Networks (COS-Net), that novelly unifies
an enriched semantic comprehending and a learnable semantic ordering processes
into a single architecture. Technically, we initially utilize a cross-modal
retrieval model to search the relevant sentences of each image, and all words
in the searched sentences are taken as primary semantic cues. Next, a novel
semantic comprehender is devised to filter out the irrelevant semantic words in
primary semantic cues, and meanwhile infer the missing relevant semantic words
visually grounded in the image. After that, we feed all the screened and
enriched semantic words into a semantic ranker, which learns to allocate all
semantic words in linguistic order as humans. Such sequence of ordered semantic
words are further integrated with visual tokens of images to trigger sentence
generation. Empirical evidences show that COS-Net clearly surpasses the
state-of-the-art approaches on COCO and achieves to-date the best CIDEr score
of 141.1% on Karpathy test split. Source code is available at
\url{https://github.com/YehLi/xmodaler/tree/master/configs/image_caption/cosnet}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OSN Dashboard Tool For Sentiment Analysis. (arXiv:2206.06935v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06935">
<div class="article-summary-box-inner">
<span><p>The amount of opinionated data on the internet is rapidly increasing. More
and more people are sharing their ideas and opinions in reviews, discussion
forums, microblogs and general social media. As opinions are central in all
human activities, sentiment analysis has been applied to gain insights in this
type of data. There are proposed several approaches for sentiment
classification. The major drawback is the lack of standardized solutions for
classification and high-level visualization. In this study, a sentiment
analyzer dashboard for online social networking analysis is proposed. This, to
enable people gaining insights in topics interesting to them. The tool allows
users to run the desired sentiment analysis algorithm in the dashboard. In
addition to providing several visualization types, the dashboard facilitates
raw data results from the sentiment classification which can be downloaded for
further analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FETILDA: An Effective Framework For Fin-tuned Embeddings For Long Financial Text Documents. (arXiv:2206.06952v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06952">
<div class="article-summary-box-inner">
<span><p>Unstructured data, especially text, continues to grow rapidly in various
domains. In particular, in the financial sphere, there is a wealth of
accumulated unstructured financial data, such as the textual disclosure
documents that companies submit on a regular basis to regulatory agencies, such
as the Securities and Exchange Commission (SEC). These documents are typically
very long and tend to contain valuable soft information about a company's
performance. It is therefore of great interest to learn predictive models from
these long textual documents, especially for forecasting numerical key
performance indicators (KPIs). Whereas there has been a great progress in
pre-trained language models (LMs) that learn from tremendously large corpora of
textual data, they still struggle in terms of effective representations for
long documents. Our work fills this critical need, namely how to develop better
models to extract useful information from long textual documents and learn
effective features that can leverage the soft financial and risk information
for text regression (prediction) tasks. In this paper, we propose and implement
a deep learning framework that splits long documents into chunks and utilizes
pre-trained LMs to process and aggregate the chunks into vector
representations, followed by self-attention to extract valuable document-level
features. We evaluate our model on a collection of 10-K public disclosure
reports from US banks, and another dataset of reports submitted by US
companies. Overall, our framework outperforms strong baseline methods for
textual modeling as well as a baseline regression model using only numerical
data. Our work provides better insights into how utilizing pre-trained
domain-specific and fine-tuned long-input LMs in representing long documents
can improve the quality of representation of textual data, and therefore, help
in improving predictive analyses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Experimental Investigation of Part-Of-Speech Taggers for Vietnamese. (arXiv:2206.06992v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06992">
<div class="article-summary-box-inner">
<span><p>Part-of-speech (POS) tagging plays an important role in Natural Language
Processing (NLP). Its applications can be found in many NLP tasks such as named
entity recognition, syntactic parsing, dependency parsing and text chunking. In
the investigation conducted in this paper, we utilize the technologies of two
widely-used toolkits, ClearNLP and Stanford POS Tagger, as well as develop two
new POS taggers for Vietnamese, then compare them to three well-known
Vietnamese taggers, namely JVnTagger, vnTagger and RDRPOSTagger. We make a
systematic comparison to find out the tagger having the best performance. We
also design a new feature set to measure the performance of the statistical
taggers. Our new taggers built from Stanford Tagger and ClearNLP with the new
feature set can outperform all other current Vietnamese taggers in term of
tagging accuracy. Moreover, we also analyze the affection of some features to
the performance of statistical taggers. Lastly, the experimental results also
reveal that the transformation-based tagger, RDRPOSTagger, can run
significantly faster than any other statistical tagger.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SBERT studies Meaning Representations: Decomposing Sentence Embeddings into Explainable AMR Meaning Features. (arXiv:2206.07023v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07023">
<div class="article-summary-box-inner">
<span><p>Metrics for graph-based meaning representations (e.g., Abstract Meaning
Representation, AMR) can help us uncover key semantic aspects in which two
sentences are similar to each other. However, such metrics tend to be slow,
rely on parsers, and do not reach state-of-the-art performance when rating
sentence similarity. On the other hand, models based on large-pretrained
language models, such as S(entence)BERT, show high correlation to human
similarity ratings, but lack interpretability.
</p>
<p>In this paper, we aim at the best of these two worlds, by creating similarity
metrics that are highly effective, while also providing an interpretable
rationale for their rating. Our approach works in two steps: We first select
AMR graph metrics that measure meaning similarity of sentences with respect to
key semantic facets, such as, i.a., semantic roles, negation, or
quantification. Second, we employ these metrics to induce Semantically
Structured Sentence BERT embeddings (S$^3$BERT), which are composed of
different meaning aspects captured in different sub-spaces. In our experimental
studies, we show that our approach offers a valuable balance between
performance and interpretability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Computational linguistics and Natural Language Processing. (arXiv:2206.07026v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07026">
<div class="article-summary-box-inner">
<span><p>This chapter provides an introduction to computational linguistics methods,
with focus on their applications to the practice and study of translation. It
covers computational models, methods and tools for collection, storage,
indexing and analysis of linguistic data in the context of translation, and
discusses the main methodological issues and challenges in this field. While an
exhaustive review of existing computational linguistics methods and tools is
beyond the scope of this chapter, we describe the most representative
approaches, and illustrate them with descriptions of typical applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text Generation with Text-Editing Models. (arXiv:2206.07043v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07043">
<div class="article-summary-box-inner">
<span><p>Text-editing models have recently become a prominent alternative to seq2seq
models for monolingual text-generation tasks such as grammatical error
correction, simplification, and style transfer. These tasks share a common
trait - they exhibit a large amount of textual overlap between the source and
target texts. Text-editing models take advantage of this observation and learn
to generate the output by predicting edit operations applied to the source
sequence. In contrast, seq2seq models generate outputs word-by-word from
scratch thus making them slow at inference time. Text-editing models provide
several benefits over seq2seq models including faster inference speed, higher
sample efficiency, and better control and interpretability of the outputs. This
tutorial provides a comprehensive overview of text-editing models and current
state-of-the-art approaches, and analyzes their pros and cons. We discuss
challenges related to productionization and how these models can be used to
mitigate hallucination and bias, both pressing challenges in the field of text
generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">fairseq S2T: Fast Speech-to-Text Modeling with fairseq. (arXiv:2010.05171v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.05171">
<div class="article-summary-box-inner">
<span><p>We introduce fairseq S2T, a fairseq extension for speech-to-text (S2T)
modeling tasks such as end-to-end speech recognition and speech-to-text
translation. It follows fairseq's careful design for scalability and
extensibility. We provide end-to-end workflows from data pre-processing, model
training to offline (online) inference. We implement state-of-the-art
RNN-based, Transformer-based as well as Conformer-based models and open-source
detailed training recipes. Fairseq's machine translation models and language
models can be seamlessly integrated into S2T workflows for multi-task learning
or transfer learning. Fairseq S2T documentation and examples are available at
https://github.com/pytorch/fairseq/tree/master/examples/speech_to_text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast Model Editing at Scale. (arXiv:2110.11309v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11309">
<div class="article-summary-box-inner">
<span><p>While large pre-trained models have enabled impressive results on a variety
of downstream tasks, the largest existing models still make errors, and even
accurate predictions may become outdated over time. Because detecting all such
failures at training time is impossible, enabling both developers and end users
of such models to correct inaccurate outputs while leaving the model otherwise
intact is desirable. However, the distributed, black-box nature of the
representations learned by large neural networks makes producing such targeted
edits difficult. If presented with only a single problematic input and new
desired output, fine-tuning approaches tend to overfit; other editing
algorithms are either computationally infeasible or simply ineffective when
applied to very large models. To enable easy post-hoc editing at scale, we
propose Model Editor Networks using Gradient Decomposition (MEND), a collection
of small auxiliary editing networks that use a single desired input-output pair
to make fast, local edits to a pre-trained model's behavior. MEND learns to
transform the gradient obtained by standard fine-tuning, using a low-rank
decomposition of the gradient to make the parameterization of this
transformation tractable. MEND can be trained on a single GPU in less than a
day even for 10 billion+ parameter models; once trained MEND enables rapid
application of new edits to the pre-trained model. Our experiments with T5,
GPT, BERT, and BART models show that MEND is the only approach to model editing
that effectively edits the behavior of models with more than 10 billion
parameters. Code and data available at
https://sites.google.com/view/mend-editing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CUGE: A Chinese Language Understanding and Generation Evaluation Benchmark. (arXiv:2112.13610v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13610">
<div class="article-summary-box-inner">
<span><p>Realizing general-purpose language intelligence has been a longstanding goal
for natural language processing, where standard evaluation benchmarks play a
fundamental and guiding role. We argue that for general-purpose language
intelligence evaluation, the benchmark itself needs to be comprehensive and
systematic. To this end, we propose CUGE, a Chinese Language Understanding and
Generation Evaluation benchmark with the following features: (1) Hierarchical
benchmark framework, where datasets are principally selected and organized with
a language capability-task-dataset hierarchy. (2) Multi-level scoring strategy,
where different levels of model performance are provided based on the
hierarchical framework. To facilitate CUGE, we provide a public leaderboard
that can be customized to support flexible model judging criteria. Evaluation
results on representative pre-trained language models indicate ample room for
improvement towards general-purpose language intelligence. CUGE is publicly
available at cuge.baai.ac.cn.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KIND: an Italian Multi-Domain Dataset for Named Entity Recognition. (arXiv:2112.15099v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15099">
<div class="article-summary-box-inner">
<span><p>In this paper we present KIND, an Italian dataset for Named-entity
recognition. It contains more than one million tokens with annotation covering
three classes: person, location, and organization. The dataset (around 600K
tokens) mostly contains manual gold annotations in three different domains
(news, literature, and political discourses) and a semi-automatically annotated
part. The multi-domain feature is the main strength of the present work,
offering a resource which covers different styles and language uses, as well as
the largest Italian NER dataset with manual gold annotations. It represents an
important resource for the training of NER systems in Italian. Texts and
annotations are freely downloadable from the Github repository.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chain of Thought Prompting Elicits Reasoning in Large Language Models. (arXiv:2201.11903v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11903">
<div class="article-summary-box-inner">
<span><p>We explore how generating a chain of thought -- a series of intermediate
reasoning steps -- significantly improves the ability of large language models
to perform complex reasoning. In particular, we show how such reasoning
abilities emerge naturally in sufficiently large language models via a simple
method called chain of thought prompting, where a few chain of thought
demonstrations are provided as exemplars in prompting. Experiments on three
large language models show that chain of thought prompting improves performance
on a range of arithmetic, commonsense, and symbolic reasoning tasks. The
empirical gains can be striking. For instance, prompting a 540B-parameter
language model with just eight chain of thought exemplars achieves state of the
art accuracy on the GSM8K benchmark of math word problems, surpassing even
finetuned GPT-3 with a verifier.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLMLF:A Contrastive Learning and Multi-Layer Fusion Method for Multimodal Sentiment Detection. (arXiv:2204.05515v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05515">
<div class="article-summary-box-inner">
<span><p>Compared with unimodal data, multimodal data can provide more features to
help the model analyze the sentiment of data. Previous research works rarely
consider token-level feature fusion, and few works explore learning the common
features related to sentiment in multimodal data to help the model fuse
multimodal features. In this paper, we propose a Contrastive Learning and
Multi-Layer Fusion (CLMLF) method for multimodal sentiment detection.
Specifically, we first encode text and image to obtain hidden representations,
and then use a multi-layer fusion module to align and fuse the token-level
features of text and image. In addition to the sentiment analysis task, we also
designed two contrastive learning tasks, label based contrastive learning and
data based contrastive learning tasks, which will help the model learn common
features related to sentiment in multimodal data. Extensive experiments
conducted on three publicly available multimodal datasets demonstrate the
effectiveness of our approach for multimodal sentiment detection compared with
existing methods. The codes are available for use at
https://github.com/Link-Li/CLMLF
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Duplex Conversation: Towards Human-like Interaction in Spoken Dialogue Systems. (arXiv:2205.15060v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15060">
<div class="article-summary-box-inner">
<span><p>In this paper, we present Duplex Conversation, a multi-turn, multimodal
spoken dialogue system that enables telephone-based agents to interact with
customers like a human. We use the concept of full-duplex in telecommunication
to demonstrate what a human-like interactive experience should be and how to
achieve smooth turn-taking through three subtasks: user state detection,
backchannel selection, and barge-in detection. Besides, we propose
semi-supervised learning with multimodal data augmentation to leverage
unlabeled data to increase model generalization. Experimental results on three
sub-tasks show that the proposed method achieves consistent improvements
compared with baselines. We deploy the Duplex Conversation to Alibaba
intelligent customer service and share lessons learned in production. Online
A/B experiments show that the proposed system can significantly reduce response
latency by 50%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ask to Know More: Generating Counterfactual Explanations for Fake Claims. (arXiv:2206.04869v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04869">
<div class="article-summary-box-inner">
<span><p>Automated fact checking systems have been proposed that quickly provide
veracity prediction at scale to mitigate the negative influence of fake news on
people and on public opinion. However, most studies focus on veracity
classifiers of those systems, which merely predict the truthfulness of news
articles. We posit that effective fact checking also relies on people's
understanding of the predictions. In this paper, we propose elucidating fact
checking predictions using counterfactual explanations to help people
understand why a specific piece of news was identified as fake. In this work,
generating counterfactual explanations for fake news involves three steps:
asking good questions, finding contradictions, and reasoning appropriately. We
frame this research question as contradicted entailment reasoning through
question answering (QA). We first ask questions towards the false claim and
retrieve potential answers from the relevant evidence documents. Then, we
identify the most contradictory answer to the false claim by use of an
entailment classifier. Finally, a counterfactual explanation is created using a
matched QA pair with three different counterfactual explanation forms.
Experiments are conducted on the FEVER dataset for both system and human
evaluations. Results suggest that the proposed approach generates the most
helpful explanations compared to state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The YiTrans End-to-End Speech Translation System for IWSLT 2022 Offline Shared Task. (arXiv:2206.05777v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05777">
<div class="article-summary-box-inner">
<span><p>This paper describes the submission of our end-to-end YiTrans speech
translation system for the IWSLT 2022 offline task, which translates from
English audio to German, Chinese, and Japanese. The YiTrans system is built on
large-scale pre-trained encoder-decoder models. More specifically, we first
design a multi-stage pre-training strategy to build a multi-modality model with
a large amount of labeled and unlabeled data. We then fine-tune the
corresponding components of the model for the downstream speech translation
tasks. Moreover, we make various efforts to improve performance, such as data
filtering, data augmentation, speech segmentation, model ensemble, and so on.
Experimental results show that our YiTrans system obtains a significant
improvement than the strong baseline on three translation directions, and it
achieves +5.2 BLEU improvements over last year's optimal end-to-end system on
tst2021 English-German. Our final submissions rank first on English-German and
English-Chinese end-to-end systems in terms of the automatic evaluation metric.
We make our code and models publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-critiquing models for assisting human evaluators. (arXiv:2206.05802v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05802">
<div class="article-summary-box-inner">
<span><p>We fine-tune large language models to write natural language critiques
(natural language critical comments) using behavioral cloning. On a topic-based
summarization task, critiques written by our models help humans find flaws in
summaries that they would have otherwise missed. Our models help find naturally
occurring flaws in both model and human written summaries, and intentional
flaws in summaries written by humans to be deliberately misleading. We study
scaling properties of critiquing with both topic-based summarization and
synthetic tasks. Larger models write more helpful critiques, and on most tasks,
are better at self-critiquing, despite having harder-to-critique outputs.
Larger models can also integrate their own self-critiques as feedback, refining
their own summaries into better ones. Finally, we motivate and introduce a
framework for comparing critiquing ability to generation and discrimination
ability. Our measurements suggest that even large models may still have
relevant knowledge they cannot or do not articulate as critiques. These results
are a proof of concept for using AI-assisted human feedback to scale the
supervision of machine learning systems to tasks that are difficult for humans
to evaluate directly. We release our training datasets, as well as samples from
our critique assistance experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Latent Diffusion Energy-Based Model for Interpretable Text Modeling. (arXiv:2206.05895v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05895">
<div class="article-summary-box-inner">
<span><p>Latent space Energy-Based Models (EBMs), also known as energy-based priors,
have drawn growing interests in generative modeling. Fueled by its flexibility
in the formulation and strong modeling power of the latent space, recent works
built upon it have made interesting attempts aiming at the interpretability of
text modeling. However, latent space EBMs also inherit some flaws from EBMs in
data space; the degenerate MCMC sampling quality in practice can lead to poor
generation quality and instability in training, especially on data with complex
latent structures. Inspired by the recent efforts that leverage diffusion
recovery likelihood learning as a cure for the sampling issue, we introduce a
novel symbiosis between the diffusion models and latent space EBMs in a
variational learning framework, coined as the latent diffusion energy-based
model. We develop a geometric clustering-based regularization jointly with the
information bottleneck to further improve the quality of the learned latent
space. Experiments on several challenging tasks demonstrate the superior
performance of our model on interpretable text modeling over strong
counterparts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Graph Construction and Its Application in Automatic Radiology Report Generation from Radiologist's Dictation. (arXiv:2206.06308v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06308">
<div class="article-summary-box-inner">
<span><p>Conventionally, the radiologist prepares the diagnosis notes and shares them
with the transcriptionist. Then the transcriptionist prepares a preliminary
formatted report referring to the notes, and finally, the radiologist reviews
the report, corrects the errors, and signs off. This workflow causes
significant delays and errors in the report. In current research work, we focus
on applications of NLP techniques like Information Extraction (IE) and
domain-specific Knowledge Graph (KG) to automatically generate radiology
reports from radiologist's dictation. This paper focuses on KG construction for
each organ by extracting information from an existing large corpus of free-text
radiology reports. We develop an information extraction pipeline that combines
rule-based, pattern-based, and dictionary-based techniques with
lexical-semantic features to extract entities and relations. Missing
information in short dictation can be accessed from the KGs to generate
pathological descriptions and hence the radiology report. Generated
pathological descriptions evaluated using semantic similarity metrics, which
shows 97% similarity with gold standard pathological descriptions. Also, our
analysis shows that our IE module is performing better than the OpenIE tool for
the radiology domain. Furthermore, we include a manual qualitative analysis
from radiologists, which shows that 80-85% of the generated reports are
correctly written, and the remaining are partially correct.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Compositional Mixture Representations for Vision and Text. (arXiv:2206.06404v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06404">
<div class="article-summary-box-inner">
<span><p>Learning a common representation space between vision and language allows
deep networks to relate objects in the image to the corresponding semantic
meaning. We present a model that learns a shared Gaussian mixture
representation imposing the compositionality of the text onto the visual domain
without having explicit location supervision. By combining the spatial
transformer with a representation learning approach we learn to split images
into separately encoded patches to associate visual and textual representations
in an interpretable manner. On variations of MNIST and CIFAR10, our model is
able to perform weakly supervised object detection and demonstrates its ability
to extrapolate to unseen combination of objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GraphMLP: A Graph MLP-Like Architecture for 3D Human Pose Estimation. (arXiv:2206.06420v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06420">
<div class="article-summary-box-inner">
<span><p>Modern multi-layer perceptron (MLP) models have shown competitive results in
learning visual representations without self-attention. However, existing MLP
models are not good at capturing local details and lack prior knowledge of
human configurations, which limits their modeling power for skeletal
representation learning. To address these issues, we propose a simple yet
effective graph-reinforced MLP-Like architecture, named GraphMLP, that combines
MLPs and graph convolutional networks (GCNs) in a global-local-graphical
unified architecture for 3D human pose estimation. GraphMLP incorporates the
graph structure of human bodies into an MLP model to meet the domain-specific
demand while also allowing for both local and global spatial interactions.
Extensive experiments show that the proposed GraphMLP achieves state-of-the-art
performance on two datasets, i.e., Human3.6M and MPI-INF-3DHP. Our source code
and pretrained models will be publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multi-purpose Real Haze Benchmark with Quantifiable Haze Levels and Ground Truth. (arXiv:2206.06427v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06427">
<div class="article-summary-box-inner">
<span><p>Imagery collected from outdoor visual environments is often degraded due to
the presence of dense smoke or haze. A key challenge for research in scene
understanding in these degraded visual environments (DVE) is the lack of
representative benchmark datasets. These datasets are required to evaluate
state-of-the-art object recognition and other computer vision algorithms in
degraded settings. In this paper, we address some of these limitations by
introducing the first paired real image benchmark dataset with hazy and
haze-free images, and in-situ haze density measurements. This dataset was
produced in a controlled environment with professional smoke generating
machines that covered the entire scene, and consists of images captured from
the perspective of both an unmanned aerial vehicle (UAV) and an unmanned ground
vehicle (UGV). We also evaluate a set of representative state-of-the-art
dehazing approaches as well as object detectors on the dataset. The full
dataset presented in this paper, including the ground truth object
classification bounding boxes and haze density measurements, is provided for
the community to evaluate their algorithms at: https://a2i2-archangel.vision. A
subset of this dataset has been used for the Object Detection in Haze Track of
CVPR UG2 2022 challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Training Method For VideoPose3D With Ideology of Action Recognition. (arXiv:2206.06430v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06430">
<div class="article-summary-box-inner">
<span><p>Action recognition and pose estimation from videos are closely related to
understand human motions, but more literature focuses on how to solve pose
estimation tasks alone from action recognition. This research shows a faster
and more flexible training method for VideoPose3D which is based on action
recognition. This model is fed with the same type of action as the type that
will be estimated, and different types of actions can be trained separately.
Evidence has shown that, for common pose-estimation tasks, this model requires
a relatively small amount of data to carry out similar results with the
original research, and for action-oriented tasks, it outperforms the original
research by 4.5% with a limited receptive field size and training epoch on
Velocity Error of MPJPE. This model can handle both action-oriented and common
pose-estimation problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ICP Algorithm: Theory, Practice And Its SLAM-oriented Taxonomy. (arXiv:2206.06435v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06435">
<div class="article-summary-box-inner">
<span><p>The Iterative Closest Point (ICP) algorithm is one of the most important
algorithms for geometric alignment of three-dimensional surface registration,
which is frequently used in computer vision tasks, including the Simultaneous
Localization And Mapping (SLAM) tasks. In this paper, we illustrate the
theoretical principles of the ICP algorithm, how it can be used in surface
registration tasks, and the traditional taxonomy of the variants of the ICP
algorithm. As SLAM is becoming a popular topic, we also introduce a
SLAM-oriented taxonomy of the ICP algorithm, based on the characteristics of
each type of SLAM task, including whether the SLAM task is online or not and
whether the landmarks are present as features in the SLAM task. We make a
synthesis of each type of SLAM task by comparing several up-to-date research
papers and analyzing their implementation details.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fitting Segmentation Networks on Varying Image Resolutions using Splatting. (arXiv:2206.06445v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06445">
<div class="article-summary-box-inner">
<span><p>Data used in image segmentation are not always defined on the same grid. This
is particularly true for medical images, where the resolution, field-of-view
and orientation can differ across channels and subjects. Images and labels are
therefore commonly resampled onto the same grid, as a pre-processing step.
However, the resampling operation introduces partial volume effects and
blurring, thereby changing the effective resolution and reducing the contrast
between structures. In this paper we propose a splat layer, which automatically
handles resolution mismatches in the input data. This layer pushes each image
onto a mean space where the forward pass is performed. As the splat operator is
the adjoint to the resampling operator, the mean-space prediction can be pulled
back to the native label space, where the loss function is computed. Thus, the
need for explicit resolution adjustment using interpolation is removed. We show
on two publicly available datasets, with simulated and real multi-modal
magnetic resonance images, that this model improves segmentation results
compared to resampling as a pre-processing step.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assessing Privacy Leakage in Synthetic 3-D PET Imaging using Transversal GAN. (arXiv:2206.06448v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06448">
<div class="article-summary-box-inner">
<span><p>Training computer-vision related algorithms on medical images for disease
diagnosis or image segmentation is difficult in large part due to privacy
concerns. For this reason, generative image models are highly sought after to
facilitate data sharing. However, 3-D generative models are understudied, and
investigation of their privacy leakage is needed. We introduce our 3-D
generative model, Transversal GAN (TrGAN), using head &amp; neck PET images which
are conditioned on tumour masks as a case study. We define quantitative
measures of image fidelity, utility and privacy for our model. These metrics
are evaluated in the course of training to identify ideal fidelity, utility and
privacy trade-offs and establish the relationships between these parameters. We
show that the discriminator of the TrGAN is vulnerable to attack, and that an
attacker can identify which samples were used in training with almost perfect
accuracy (AUC = 0.99). We also show that an attacker with access to only the
generator cannot reliably classify whether a sample had been used for training
(AUC = 0.51). This suggests that TrGAN generators, but not discriminators, may
be used for sharing synthetic 3-D PET data with minimal privacy risk while
maintaining good utility and fidelity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Representation Learning With MUlti-Segmental Informational Coding (MUSIC). (arXiv:2206.06461v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06461">
<div class="article-summary-box-inner">
<span><p>Self-supervised representation learning maps high-dimensional data into a
meaningful embedding space, where samples of similar semantic contents are
close to each other. Most of the recent representation learning methods
maximize cosine similarity or minimize the distance between the embedding
features of different views from the same sample usually on the $l2$ normalized
unit hypersphere. To prevent the trivial solutions that all samples have the
same embedding feature, various techniques have been developed, such as
contrastive learning, stop gradient, variance and covariance regularization,
etc. In this study, we propose MUlti-Segmental Informational Coding (MUSIC) for
self-supervised representation learning. MUSIC divides the embedding feature
into multiple segments that discriminatively partition samples into different
semantic clusters and different segments focus on different partition
principles. Information theory measurements are directly used to optimize MUSIC
and theoretically guarantee trivial solutions are avoided. MUSIC does not
depend on commonly used techniques, such as memory bank or large batches,
asymmetry networks, gradient stopping, momentum weight updating, etc, making
the training framework flexible. Our experiments demonstrate that MUSIC
achieves better results than most related Barlow Twins and VICReg methods on
ImageNet classification with linear probing, and requires neither deep
projectors nor large feature dimensions. Code will be made available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting the Shape-Bias of Deep Learning for Dermoscopic Skin Lesion Classification. (arXiv:2206.06466v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06466">
<div class="article-summary-box-inner">
<span><p>It is generally believed that the human visual system is biased towards the
recognition of shapes rather than textures. This assumption has led to a
growing body of work aiming to align deep models' decision-making processes
with the fundamental properties of human vision. The reliance on shape features
is primarily expected to improve the robustness of these models under covariate
shift. In this paper, we revisit the significance of shape-biases for the
classification of skin lesion images. Our analysis shows that different skin
lesion datasets exhibit varying biases towards individual image features.
Interestingly, despite deep feature extractors being inclined towards learning
entangled features for skin lesion classification, individual features can
still be decoded from this entangled representation. This indicates that these
features are still represented in the learnt embedding spaces of the models,
but not used for classification. In addition, the spectral analysis of
different datasets shows that in contrast to common visual recognition,
dermoscopic skin lesion classification, by nature, is reliant on complex
feature combinations beyond shape-bias. As a natural consequence, shifting away
from the prevalent desire of shape-biasing models can even improve skin lesion
classifiers in some cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RigNeRF: Fully Controllable Neural 3D Portraits. (arXiv:2206.06481v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06481">
<div class="article-summary-box-inner">
<span><p>Volumetric neural rendering methods, such as neural radiance fields (NeRFs),
have enabled photo-realistic novel view synthesis. However, in their standard
form, NeRFs do not support the editing of objects, such as a human head, within
a scene. In this work, we propose RigNeRF, a system that goes beyond just novel
view synthesis and enables full control of head pose and facial expressions
learned from a single portrait video. We model changes in head pose and facial
expressions using a deformation field that is guided by a 3D morphable face
model (3DMM). The 3DMM effectively acts as a prior for RigNeRF that learns to
predict only residuals to the 3DMM deformations and allows us to render novel
(rigid) poses and (non-rigid) expressions that were not present in the input
sequence. Using only a smartphone-captured short video of a subject for
training, we demonstrate the effectiveness of our method on free view synthesis
of a portrait scene with explicit head pose and expression controls. The
project page can be found here:
<a href="http://shahrukhathar.github.io/2022/06/06/RigNeRF.html">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Image Segmentation With Noisy Labels: Characterization and Volume Properties of the Optimal Solutions to Accuracy and Dice. (arXiv:2206.06484v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06484">
<div class="article-summary-box-inner">
<span><p>We study two of the most popular performance metrics in medical image
segmentation, Accuracy and Dice, when the target labels are noisy. For both
metrics, several statements related to characterization and volume properties
of the set of optimal segmentations are proved, and associated experiments are
provided. Our main insights are: (i) the volume of the solutions to both
metrics may deviate significantly from the expected volume of the target, (ii)
the volume of a solution to Accuracy is always less than or equal to the volume
of a solution to Dice and (iii) the optimal solutions to both of these metrics
coincide when the set of feasible segmentations is constrained to the set of
segmentations with the volume equal to the expected volume of the target.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Modality Focusing Hypothesis: On the Blink of Multimodal Knowledge Distillation. (arXiv:2206.06487v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06487">
<div class="article-summary-box-inner">
<span><p>Multimodal knowledge distillation (KD) extends traditional knowledge
distillation to the area of multimodal learning. One common practice is to
adopt a well-performed multimodal network as the teacher in the hope that it
can transfer its full knowledge to a unimodal student for performance
improvement. In this paper, we investigate the efficacy of multimodal KD. We
begin by providing two failure cases of it and demonstrate that KD is not a
universal cure in multimodal knowledge transfer. We present the modality Venn
diagram to understand modality relationships and the modality focusing
hypothesis revealing the decisive factor in the efficacy of multimodal KD.
Experimental results on 6 multimodal datasets help justify our hypothesis,
diagnose failure cases, and point directions to improve distillation
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Learning with Transformers: A Survey. (arXiv:2206.06488v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06488">
<div class="article-summary-box-inner">
<span><p>Transformer is a promising neural network learner, and has achieved great
success in various machine learning tasks. Thanks to the recent prevalence of
multimodal applications and big data, Transformer-based multimodal learning has
become a hot topic in AI research. This paper presents a comprehensive survey
of Transformer techniques oriented at multimodal data. The main contents of
this survey include: (1) a background of multimodal learning, Transformer
ecosystem, and the multimodal big data era, (2) a theoretical review of Vanilla
Transformer, Vision Transformer, and multimodal Transformers, from a
geometrically topological perspective, (3) a review of multimodal Transformer
applications, via two important paradigms, i.e., for multimodal pretraining and
for specific multimodal tasks, (4) a summary of the common challenges and
designs shared by the multimodal Transformer models and applications, and (5) a
discussion of open problems and potential research directions for the
community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BEHAVIOR in Habitat 2.0: Simulator-Independent Logical Task Description for Benchmarking Embodied AI Agents. (arXiv:2206.06489v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06489">
<div class="article-summary-box-inner">
<span><p>Robots excel in performing repetitive and precision-sensitive tasks in
controlled environments such as warehouses and factories, but have not been yet
extended to embodied AI agents providing assistance in household tasks.
Inspired by the catalyzing effect that benchmarks have played in the AI fields
such as computer vision and natural language processing, the community is
looking for new benchmarks for embodied AI. Prior work in embodied AI benchmark
defines tasks using a different formalism, often specific to one environment,
simulator or domain, making it hard to develop general and comparable
solutions. In this work, we bring a subset of BEHAVIOR activities into Habitat
2.0 to benefit from its fast simulation speed, as a first step towards
demonstrating the ease of adapting activities defined in the logic space into
different simulators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Task-Independent Game State Representations from Unlabeled Images. (arXiv:2206.06490v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06490">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning (SSL) techniques have been widely used to learn
compact and informative representations from high-dimensional complex data. In
many computer vision tasks, such as image classification, such methods achieve
state-of-the-art results that surpass supervised learning approaches. In this
paper, we investigate whether SSL methods can be leveraged for the task of
learning accurate state representations of games, and if so, to what extent.
For this purpose, we collect game footage frames and corresponding sequences of
games' internal state from three different 3D games: VizDoom, the CARLA racing
simulator and the Google Research Football Environment. We train an image
encoder with three widely used SSL algorithms using solely the raw frames, and
then attempt to recover the internal state variables from the learned
representations. Our results across all three games showcase significantly
higher correlation between SSL representations and the game's internal state
compared to pre-trained baseline models such as ImageNet. Such findings suggest
that SSL-based visual encoders can yield general -- not tailored to a specific
task -- yet informative game representations solely from game pixel
information. Such representations can, in turn, form the basis for boosting the
performance of downstream learning tasks in games, including gameplaying,
content generation and player modeling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spiking Neural Networks for Frame-based and Event-based Single Object Localization. (arXiv:2206.06506v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06506">
<div class="article-summary-box-inner">
<span><p>Spiking neural networks have shown much promise as an energy-efficient
alternative to artificial neural networks. However, understanding the impacts
of sensor noises and input encodings on the network activity and performance
remains difficult with common neuromorphic vision baselines like
classification. Therefore, we propose a spiking neural network approach for
single object localization trained using surrogate gradient descent, for frame-
and event-based sensors. We compare our method with similar artificial neural
networks and show that our model has competitive/better performance in
accuracy, robustness against various corruptions, and has lower energy
consumption. Moreover, we study the impact of neural coding schemes for static
images in accuracy, robustness, and energy efficiency. Our observations differ
importantly from previous studies on bio-plausible learning rules, which helps
in the design of surrogate gradient trained architectures, and offers insight
to design priorities in future neuromorphic technologies in terms of noise
characteristics and data encoding methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalizable Method for Face Anti-Spoofing with Semi-Supervised Learning. (arXiv:2206.06510v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06510">
<div class="article-summary-box-inner">
<span><p>Face anti-spoofing has drawn a lot of attention due to the high security
requirements in biometric authentication systems. Bringing face biometric to
commercial hardware became mostly dependent on developing reliable methods for
detecting fake login sessions without specialized sensors. Current CNN-based
method perform well on the domains they were trained for, but often show poor
generalization on previously unseen datasets. In this paper we describe a
method for utilizing unsupervised pretraining for improving performance across
multiple datasets without any adaptation, introduce the Entry Antispoofing
Dataset for supervised fine-tuning, and propose a multi-class auxiliary
classification layer for augmenting the binary classification task of detecting
spoofing attempts with explicit interpretable signals. We demonstrate the
efficiency of our model by achieving state-of-the-art results on cross-dataset
testing on MSU-MFSD, Replay-Attack, and OULU-NPU datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Estimating Pose from Pressure Data for Smart Beds with Deep Image-based Pose Estimators. (arXiv:2206.06518v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06518">
<div class="article-summary-box-inner">
<span><p>In-bed pose estimation has shown value in fields such as hospital patient
monitoring, sleep studies, and smart homes. In this paper, we explore different
strategies for detecting body pose from highly ambiguous pressure data, with
the aid of pre-existing pose estimators. We examine the performance of
pre-trained pose estimators by using them either directly or by re-training
them on two pressure datasets. We also explore other strategies utilizing a
learnable pre-processing domain adaptation step, which transforms the vague
pressure maps to a representation closer to the expected input space of common
purpose pose estimation modules. Accordingly, we used a fully convolutional
network with multiple scales to provide the pose-specific characteristics of
the pressure maps to the pre-trained pose estimation module. Our complete
analysis of different approaches shows that the combination of learnable
pre-processing module along with re-training pre-existing image-based pose
estimators on the pressure data is able to overcome issues such as highly vague
pressure points to achieve very high pose estimation accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning. (arXiv:2206.06522v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06522">
<div class="article-summary-box-inner">
<span><p>Fine-tuning large pre-trained models on downstream tasks has been adopted in
a variety of domains recently. However, it is costly to update the entire
parameter set of large pre-trained models. Although recently proposed
parameter-efficient transfer learning (PETL) techniques allow updating a small
subset of parameters (e.g. only using 2% of parameters) inside a pre-trained
backbone network for a new task, they only reduce the training memory
requirement by up to 30%. This is because the gradient computation for the
trainable parameters still requires backpropagation through the large
pre-trained backbone model. To address this, we propose Ladder Side-Tuning
(LST), a new PETL technique that reduces training memory requirements by more
substantial amounts. Unlike existing parameter-efficient methods that insert
additional parameters inside backbone networks, we train a ladder side network,
a small and separate network that takes intermediate activations as input via
shortcut connections (ladders) from backbone networks and makes predictions.
LST has significantly lower memory requirements than previous methods, because
it does not require backpropagation through the backbone network, but instead
only through the side network and ladder connections. We evaluate our method
with various models (T5, CLIP-T5) on both NLP (GLUE) and vision-language (VQA,
GQA, NLVR2, MSCOCO) tasks. LST saves 69% of the memory costs to fine-tune the
whole network, while other methods only save 26% of that in similar parameter
usages (hence, 2.7x more memory savings). Moreover, LST achieves higher
accuracy than Adapter and LoRA in a low-memory regime. To further show the
advantage of this better memory efficiency, we also apply LST to larger T5
models (T5-large, T5-3B), attaining better GLUE performance than full
fine-tuning and other PETL methods. The exact same trend also holds in our
experiments on VL tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D scene reconstruction from monocular spherical video with motion parallax. (arXiv:2206.06533v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06533">
<div class="article-summary-box-inner">
<span><p>In this paper, we describe a method to capture nearly entirely spherical (360
degree) depth information using two adjacent frames from a single spherical
video with motion parallax. After illustrating a spherical depth information
retrieval using two spherical cameras, we demonstrate monocular spherical
stereo by using stabilized first-person video footage. Experiments demonstrated
that the depth information was retrieved on up to 97% of the entire sphere in
solid angle. At a speed of 30 km/h, we were able to estimate the depth of an
object located over 30 m from the camera. We also reconstructed the 3D
structures (point cloud) using the obtained depth data and confirmed the
structures can be clearly observed. We can apply this method to 3D structure
retrieval of surrounding environments such as 1) previsualization, location
hunting/planning of a film, 2) real scene/computer graphics synthesis and 3)
motion capture. Thanks to its simplicity, this method can be applied to various
videos. As there is no pre-condition other than to be a 360 video with motion
parallax, we can use any 360 videos including those on the Internet to
reconstruct the surrounding environments. The cameras can be lightweight enough
to be mounted on a drone. We also demonstrated such applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pixel-by-pixel Mean Opinion Score (pMOS) for No-Reference Image Quality Assessment. (arXiv:2206.06541v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06541">
<div class="article-summary-box-inner">
<span><p>Deep-learning based techniques have contributed to the remarkable progress in
the field of automatic image quality assessment (IQA). Existing IQA methods are
designed to measure the quality of an image in terms of Mean Opinion Score
(MOS) at the image-level (i.e. the whole image) or at the patch-level (dividing
the image into multiple units and measuring quality of each patch). Some
applications may require assessing the quality at the pixel-level (i.e. MOS
value for each pixel), however, this is not possible in case of existing
techniques as the spatial information is lost owing to their network
structures. This paper proposes an IQA algorithm that can measure the MOS at
the pixel-level, in addition to the image-level MOS. The proposed algorithm
consists of three core parts, namely: i) Local IQA; ii) Region of Interest
(ROI) prediction; iii) High-level feature embedding. The Local IQA part outputs
the MOS at the pixel-level, or pixel-by-pixel MOS - we term it 'pMOS'. The ROI
prediction part outputs weights that characterize the relative importance of
region when calculating the image-level IQA. The high-level feature embedding
part extracts high-level image features which are then embedded into the Local
IQA part. In other words, the proposed algorithm yields three outputs: the pMOS
which represents MOS for each pixel, the weights from the ROI indicating the
relative importance of region, and finally the image-level MOS that is obtained
by the weighted sum of pMOS and ROI values. The image-level MOS thus obtained
by utilizing pMOS and ROI weights shows superior performance compared to the
existing popular IQA techniques. In addition, visualization results indicate
that predicted pMOS and ROI outputs are reasonably aligned with the general
principles of the human visual system (HVS).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Automated Data Augmentation Algorithms for Deep Learning-based Image Classication Tasks. (arXiv:2206.06544v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06544">
<div class="article-summary-box-inner">
<span><p>In recent years, one of the most popular techniques in the computer vision
community has been the deep learning technique. As a data-driven technique,
deep model requires enormous amounts of accurately labelled training data,
which is often inaccessible in many real-world applications. A data-space
solution is Data Augmentation (DA), that can artificially generate new images
out of original samples. Image augmentation strategies can vary by dataset, as
different data types might require different augmentations to facilitate model
training. However, the design of DA policies has been largely decided by the
human experts with domain knowledge, which is considered to be highly
subjective and error-prone. To mitigate such problem, a novel direction is to
automatically learn the image augmentation policies from the given dataset
using Automated Data Augmentation (AutoDA) techniques. The goal of AutoDA
models is to find the optimal DA policies that can maximize the model
performance gains. This survey discusses the underlying reasons of the
emergence of AutoDA technology from the perspective of image classification. We
identify three key components of a standard AutoDA model: a search space, a
search algorithm and an evaluation function. Based on their architecture, we
provide a systematic taxonomy of existing image AutoDA approaches. This paper
presents the major works in AutoDA field, discussing their pros and cons, and
proposing several potential directions for future improvements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Safe Output Feedback Motion Planning from Images via Learned Perception Modules and Contraction Theory. (arXiv:2206.06553v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06553">
<div class="article-summary-box-inner">
<span><p>We present a motion planning algorithm for a class of uncertain
control-affine nonlinear systems which guarantees runtime safety and goal
reachability when using high-dimensional sensor measurements (e.g., RGB-D
images) and a learned perception module in the feedback control loop. First,
given a dataset of states and observations, we train a perception system that
seeks to invert a subset of the state from an observation, and estimate an
upper bound on the perception error which is valid with high probability in a
trusted domain near the data. Next, we use contraction theory to design a
stabilizing state feedback controller and a convergent dynamic state observer
which uses the learned perception system to update its state estimate. We
derive a bound on the trajectory tracking error when this controller is
subjected to errors in the dynamics and incorrect state estimates. Finally, we
integrate this bound into a sampling-based motion planner, guiding it to return
trajectories that can be safely tracked at runtime using sensor data. We
demonstrate our approach in simulation on a 4D car, a 6D planar quadrotor, and
a 17D manipulation task with RGB(-D) sensor measurements, demonstrating that
our method safely and reliably steers the system to the goal, while baselines
that fail to consider the trusted domain or state estimation errors can be
unsafe.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Med-DANet: Dynamic Architecture Network for Efficient Medical Volumetric Segmentation. (arXiv:2206.06575v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06575">
<div class="article-summary-box-inner">
<span><p>For 3D medical image (e.g. CT and MRI) segmentation, the difficulty of
segmenting each slice in a clinical case varies greatly. Previous research on
volumetric medical image segmentation in a slice-by-slice manner conventionally
use the identical 2D deep neural network to segment all the slices of the same
case, ignoring the data heterogeneity among image slices. In this paper, we
focus on multi-modal 3D MRI brain tumor segmentation and propose a dynamic
architecture network named Med-DANet based on adaptive model selection to
achieve effective accuracy and efficiency trade-off. For each slice of the
input 3D MRI volume, our proposed method learns a slice-specific decision by
the Decision Network to dynamically select a suitable model from the predefined
Model Bank for the subsequent 2D segmentation task. Extensive experimental
results on both BraTS 2019 and 2020 datasets show that our proposed method
achieves comparable or better results than previous state-of-the-art methods
for 3D MRI brain tumor segmentation with much less model complexity. Compared
with the state-of-the-art 3D method TransBTS, the proposed framework improves
the model efficiency by up to 3.5x without sacrificing the accuracy. Our code
will be publicly available soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Physics Informed Neural Fields for Smoke Reconstruction with Sparse Data. (arXiv:2206.06577v1 [cs.GR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06577">
<div class="article-summary-box-inner">
<span><p>High-fidelity reconstruction of fluids from sparse multiview RGB videos
remains a formidable challenge due to the complexity of the underlying physics
as well as complex occlusion and lighting in captures. Existing solutions
either assume knowledge of obstacles and lighting, or only focus on simple
fluid scenes without obstacles or complex lighting, and thus are unsuitable for
real-world scenes with unknown lighting or arbitrary obstacles. We present the
first method to reconstruct dynamic fluid by leveraging the governing physics
(ie, Navier -Stokes equations) in an end-to-end optimization from sparse videos
without taking lighting conditions, geometry information, or boundary
conditions as input. We provide a continuous spatio-temporal scene
representation using neural networks as the ansatz of density and velocity
solution functions for fluids as well as the radiance field for static objects.
With a hybrid architecture that separates static and dynamic contents, fluid
interactions with static obstacles are reconstructed for the first time without
additional geometry input or human labeling. By augmenting time-varying neural
radiance fields with physics-informed deep learning, our method benefits from
the supervision of images and physical priors. To achieve robust optimization
from sparse views, we introduced a layer-by-layer growing strategy to
progressively increase the network capacity. Using progressively growing models
with a new regularization term, we manage to disentangle density-color
ambiguity in radiance fields without overfitting. A pretrained
density-to-velocity fluid model is leveraged in addition as the data prior to
avoid suboptimal velocity which underestimates vorticity but trivially fulfills
physical equations. Our method exhibits high-quality results with relaxed
constraints and strong flexibility on a representative set of synthetic and
real flow captures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CorticalFlow$^{++}$: Boosting Cortical Surface Reconstruction Accuracy, Regularity, and Interoperability. (arXiv:2206.06598v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06598">
<div class="article-summary-box-inner">
<span><p>The problem of Cortical Surface Reconstruction from magnetic resonance
imaging has been traditionally addressed using lengthy pipelines of image
processing techniques like FreeSurfer, CAT, or CIVET. These frameworks require
very long runtimes deemed unfeasible for real-time applications and unpractical
for large-scale studies. Recently, supervised deep learning approaches have
been introduced to speed up this task cutting down the reconstruction time from
hours to seconds. Using the state-of-the-art CorticalFlow model as a blueprint,
this paper proposes three modifications to improve its accuracy and
interoperability with existing surface analysis tools, while not sacrificing
its fast inference time and low GPU memory consumption. First, we employ a more
accurate ODE solver to reduce the diffeomorphic mapping approximation error.
Second, we devise a routine to produce smoother template meshes avoiding mesh
artifacts caused by sharp edges in CorticalFlow's convex-hull based template.
Last, we recast pial surface prediction as the deformation of the predicted
white surface leading to a one-to-one mapping between white and pial surface
vertices. This mapping is essential to many existing surface analysis tools for
cortical morphometry. We name the resulting method CorticalFlow$^{++}$. Using
large-scale datasets, we demonstrate the proposed changes provide more
geometric accuracy and surface regularity while keeping the reconstruction time
and GPU memory requirements almost unchanged.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Plug-and-Play Pseudo Label Correction Network for Unsupervised Person Re-identification. (arXiv:2206.06607v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06607">
<div class="article-summary-box-inner">
<span><p>Clustering-based methods, which alternate between the generation of pseudo
labels and the optimization of the feature extraction network, play a dominant
role in both unsupervised learning (USL) and unsupervised domain adaptive (UDA)
person re-identification (Re-ID). To alleviate the adverse effect of noisy
pseudo labels, the existing methods either abandon unreliable labels or refine
the pseudo labels via mutual learning or label propagation. However, a great
many erroneous labels are still accumulated because these methods mostly adopt
traditional unsupervised clustering algorithms which rely on certain
assumptions on data distribution and fail to capture the distribution of
complex real-world data. In this paper, we propose the plug-and-play
graph-based pseudo label correction network (GLC) to refine the pseudo labels
in the manner of supervised clustering. GLC is trained to perceive the varying
data distribution at each epoch of the self-training with the supervision of
initial pseudo labels generated by any clustering method. It can learn to
rectify the initial noisy labels by means of the relationship constraints
between samples on the k Nearest Neighbor (kNN) graph and early-stop training
strategy. Specifically, GLC learns to aggregate node features from neighbors
and predict whether the nodes should be linked on the graph. Besides, GLC is
optimized with 'early stop' before the noisy labels are severely memorized to
prevent overfitting to noisy pseudo labels. Consequently, GLC improves the
quality of pseudo labels though the supervision signals contain some noise,
leading to better Re-ID performance. Extensive experiments in USL and UDA
person Re-ID on Market-1501 and MSMT17 show that our method is widely
compatible with various clustering-based methods and promotes the
state-of-the-art performance consistently.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Label Matching Semi-Supervised Object Detection. (arXiv:2206.06608v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06608">
<div class="article-summary-box-inner">
<span><p>Semi-supervised object detection has made significant progress with the
development of mean teacher driven self-training. Despite the promising
results, the label mismatch problem is not yet fully explored in the previous
works, leading to severe confirmation bias during self-training. In this paper,
we delve into this problem and propose a simple yet effective LabelMatch
framework from two different yet complementary perspectives, i.e.,
distribution-level and instance-level. For the former one, it is reasonable to
approximate the class distribution of the unlabeled data from that of the
labeled data according to Monte Carlo Sampling. Guided by this weakly
supervision cue, we introduce a re-distribution mean teacher, which leverages
adaptive label-distribution-aware confidence thresholds to generate unbiased
pseudo labels to drive student learning. For the latter one, there exists an
overlooked label assignment ambiguity problem across teacher-student models. To
remedy this issue, we present a novel label assignment mechanism for
self-training framework, namely proposal self-assignment, which injects the
proposals from student into teacher and generates accurate pseudo labels to
match each proposal in the student model accordingly. Experiments on both
MS-COCO and PASCAL-VOC datasets demonstrate the considerable superiority of our
proposed framework to other state-of-the-arts. Code will be available at
https://github.com/hikvision-research/SSOD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransVG++: End-to-End Visual Grounding with Language Conditioned Vision Transformer. (arXiv:2206.06619v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06619">
<div class="article-summary-box-inner">
<span><p>In this work, we explore neat yet effective Transformer-based frameworks for
visual grounding. The previous methods generally address the core problem of
visual grounding, i.e., multi-modal fusion and reasoning, with
manually-designed mechanisms. Such heuristic designs are not only complicated
but also make models easily overfit specific data distributions. To avoid this,
we first propose TransVG, which establishes multi-modal correspondences by
Transformers and localizes referred regions by directly regressing box
coordinates. We empirically show that complicated fusion modules can be
replaced by a simple stack of Transformer encoder layers with higher
performance. However, the core fusion Transformer in TransVG is stand-alone
against uni-modal encoders, and thus should be trained from scratch on limited
visual grounding data, which makes it hard to be optimized and leads to
sub-optimal performance. To this end, we further introduce TransVG++ to make
two-fold improvements. For one thing, we upgrade our framework to a purely
Transformer-based one by leveraging Vision Transformer (ViT) for vision feature
encoding. For another, we devise Language Conditioned Vision Transformer that
removes external fusion modules and reuses the uni-modal ViT for
vision-language fusion at the intermediate layers. We conduct extensive
experiments on five prevalent datasets, and report a series of state-of-the-art
records.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Slimmable Domain Adaptation. (arXiv:2206.06620v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06620">
<div class="article-summary-box-inner">
<span><p>Vanilla unsupervised domain adaptation methods tend to optimize the model
with fixed neural architecture, which is not very practical in real-world
scenarios since the target data is usually processed by different
resource-limited devices. It is therefore of great necessity to facilitate
architecture adaptation across various devices. In this paper, we introduce a
simple framework, Slimmable Domain Adaptation, to improve cross-domain
generalization with a weight-sharing model bank, from which models of different
capacities can be sampled to accommodate different accuracy-efficiency
trade-offs. The main challenge in this framework lies in simultaneously
boosting the adaptation performance of numerous models in the model bank. To
tackle this problem, we develop a Stochastic EnsEmble Distillation method to
fully exploit the complementary knowledge in the model bank for inter-model
interaction. Nevertheless, considering the optimization conflict between
inter-model interaction and intra-model adaptation, we augment the existing
bi-classifier domain confusion architecture into an Optimization-Separated
Tri-Classifier counterpart. After optimizing the model bank, architecture
adaptation is leveraged via our proposed Unsupervised Performance Evaluation
Metric. Under various resource constraints, our framework surpasses other
competing approaches by a very large margin on multiple benchmarks. It is also
worth emphasizing that our framework can preserve the performance improvement
against the source-only model even when the computing complexity is reduced to
$1/64$. Code will be available at https://github.com/hikvision-research/SlimDA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ULTRA: Uncertainty-aware Label Distribution Learning for Breast Tumor Cellularity Assessment. (arXiv:2206.06623v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06623">
<div class="article-summary-box-inner">
<span><p>Neoadjuvant therapy (NAT) for breast cancer is a common treatment option in
clinical practice. Tumor cellularity (TC), which represents the percentage of
invasive tumors in the tumor bed, has been widely used to quantify the response
of breast cancer to NAT. Therefore, automatic TC estimation is significant in
clinical practice. However, existing state-of-the-art methods usually take it
as a TC score regression problem, which ignores the ambiguity of TC labels
caused by subjective assessment or multiple raters. In this paper, to
efficiently leverage the label ambiguities, we proposed an Uncertainty-aware
Label disTRibution leArning (ULTRA) framework for automatic TC estimation. The
proposed ULTRA first converted the single-value TC labels to discrete label
distributions, which effectively models the ambiguity among all possible TC
labels. Furthermore, the network learned TC label distributions by minimizing
the Kullback-Leibler (KL) divergence between the predicted and ground-truth TC
label distributions, which better supervised the model to leverage the
ambiguity of TC labels. Moreover, the ULTRA mimicked the multi-rater fusion
process in clinical practice with a multi-branch feature fusion module to
further explore the uncertainties of TC labels. We evaluated the ULTRA on the
public BreastPathQ dataset. The experimental results demonstrate that the ULTRA
outperformed the regression-based methods for a large margin and achieved
state-of-the-art results. The code will be available from
https://github.com/PerceptionComputingLab/ULTRA
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RF-Next: Efficient Receptive Field Search for Convolutional Neural Networks. (arXiv:2206.06637v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06637">
<div class="article-summary-box-inner">
<span><p>Temporal/spatial receptive fields of models play an important role in
sequential/spatial tasks. Large receptive fields facilitate long-term
relations, while small receptive fields help to capture the local details.
Existing methods construct models with hand-designed receptive fields in
layers. Can we effectively search for receptive field combinations to replace
hand-designed patterns? To answer this question, we propose to find better
receptive field combinations through a global-to-local search scheme. Our
search scheme exploits both global search to find the coarse combinations and
local search to get the refined receptive field combinations further. The
global search finds possible coarse combinations other than human-designed
patterns. On top of the global search, we propose an expectation-guided
iterative local search scheme to refine combinations effectively. Our RF-Next
models, plugging receptive field search to various models, boost the
performance on many tasks, e.g., temporal action segmentation, object
detection, instance segmentation, and speech synthesis. The source code is
publicly available on <a href="http://mmcheng.net/rfnext.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Confidence Score for Source-Free Unsupervised Domain Adaptation. (arXiv:2206.06640v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06640">
<div class="article-summary-box-inner">
<span><p>Source-free unsupervised domain adaptation (SFUDA) aims to obtain high
performance in the unlabeled target domain using the pre-trained source model,
not the source data. Existing SFUDA methods assign the same importance to all
target samples, which is vulnerable to incorrect pseudo-labels. To
differentiate between sample importance, in this study, we propose a novel
sample-wise confidence score, the Joint Model-Data Structure (JMDS) score for
SFUDA. Unlike existing confidence scores that use only one of the source or
target domain knowledge, the JMDS score uses both knowledge. We then propose a
Confidence score Weighting Adaptation using the JMDS (CoWA-JMDS) framework for
SFUDA. CoWA-JMDS consists of the JMDS scores as sample weights and weight Mixup
that is our proposed variant of Mixup. Weight Mixup promotes the model make
more use of the target domain knowledge. The experimental results show that the
JMDS score outperforms the existing confidence scores. Moreover, CoWA-JMDS
achieves state-of-the-art performance on various SFUDA scenarios: closed, open,
and partial-set scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Kidneys Are Not All Normal: Investigating the Speckle Distributions of Transplanted Kidneys. (arXiv:2206.06654v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06654">
<div class="article-summary-box-inner">
<span><p>Modelling ultrasound speckle has generated considerable interest for its
ability to characterize tissue properties. As speckle is dependent on the
underlying tissue architecture, modelling it may aid in tasks like segmentation
or disease detection. However, for the transplanted kidney where ultrasound is
commonly used to investigate dysfunction, it is currently unknown which
statistical distribution best characterises such speckle. This is especially
true for the regions of the transplanted kidney: the cortex, the medulla and
the central echogenic complex. Furthermore, it is unclear how these
distributions vary by patient variables such as age, sex, body mass index,
primary disease, or donor type. These traits may influence speckle modelling
given their influence on kidney anatomy. We are the first to investigate these
two aims. N=821 kidney transplant recipient B-mode images were automatically
segmented into the cortex, medulla, and central echogenic complex using a
neural network. Seven distinct probability distributions were fitted to each
region. The Rayleigh and Nakagami distributions had model parameters that
differed significantly between the three regions (p &lt;= 0.05). While both had
excellent goodness of fit, the Nakagami had higher Kullbeck-Leibler divergence.
Recipient age correlated weakly with scale in the cortex (Omega: rho = 0.11, p
= 0.004), while body mass index correlated weakly with shape in the medulla (m:
rho = 0.08, p = 0.04). Neither sex, primary disease, nor donor type
demonstrated any correlation. We propose the Nakagami distribution be used to
characterize transplanted kidneys regionally independent of disease etiology
and most patient characteristics based on our findings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Open Kidney Ultrasound Data Set. (arXiv:2206.06657v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06657">
<div class="article-summary-box-inner">
<span><p>Ultrasound use is because of its low cost, non-ionizing, and non-invasive
characteristics, and has established itself as a cornerstone radiological
examination. Research on ultrasound applications has also expanded, especially
with image analysis with machine learning. However, ultrasound data are
frequently restricted to closed data sets, with only a few openly available.
Despite being a frequently examined organ, the kidney lacks a publicly
available ultrasonography data set. The proposed Open Kidney Ultrasound Data
Set is the first publicly available set of kidney B-mode ultrasound data that
includes annotations for multi-class semantic segmentation. It is based on data
retrospectively collected in a 5-year period from over 500 patients with a mean
age of 53.2 +/- 14.7 years, body mass index of 27.0 +/- 5.4 kg/m2, and most
common primary diseases being diabetes mellitus, IgA nephropathy, and
hypertension. There are labels for the view and fine-grained manual annotations
from two expert sonographers. Notably, this data includes native and
transplanted kidneys. Initial benchmarking measurements are performed,
demonstrating a state-of-the-art algorithm achieving a Dice Sorenson
Coefficient of 0.74 for the kidney capsule. This data set is a high-quality
data set, including two sets of expert annotations, with a larger breadth of
images than previously available. In increasing access to kidney ultrasound
data, future researchers may be able to create novel image analysis techniques
for tissue characterization, disease detection, and prognostication.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Best Combination for Efficient N:M Sparsity. (arXiv:2206.06662v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06662">
<div class="article-summary-box-inner">
<span><p>By forcing at most N out of M consecutive weights to be non-zero, the recent
N:M network sparsity has received increasing attention for its two attractive
advantages: 1) Promising performance at a high sparsity. 2) Significant
speedups on NVIDIA A100 GPUs. Recent studies require an expensive pre-training
phase or a heavy dense-gradient computation. In this paper, we show that the
N:M learning can be naturally characterized as a combinatorial problem which
searches for the best combination candidate within a finite collection.
Motivated by this characteristic, we solve N:M sparsity in an efficient
divide-and-conquer manner. First, we divide the weight vector into
$C_{\text{M}}^{\text{N}}$ combination subsets of a fixed size N. Then, we
conquer the combinatorial problem by assigning each combination a learnable
score that is jointly optimized with its associate weights. We prove that the
introduced scoring mechanism can well model the relative importance between
combination subsets. And by gradually removing low-scored subsets, N:M
fine-grained sparsity can be efficiently optimized during the normal training
phase. Comprehensive experiments demonstrate that our learning best combination
(LBC) performs consistently better than off-the-shelf N:M sparsity methods
across various networks. Our code is released at
\url{https://github.com/zyxxmu/LBC}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantitative Imaging Principles Improves Medical Image Learning. (arXiv:2206.06663v1 [q-bio.QM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06663">
<div class="article-summary-box-inner">
<span><p>Fundamental differences between natural and medical images have recently
favored the use of self-supervised learning (SSL) over ImageNet transfer
learning for medical image applications. Differences between image types are
primarily due to the imaging modality and medical images utilize a wide range
of physics based techniques while natural images are captured using only
visible light. While many have demonstrated that SSL on medical images has
resulted in better downstream task performance, our work suggests that more
performance can be gained. The scientific principles which are used to acquire
medical images are not often considered when constructing learning problems.
For this reason, we propose incorporating quantitative imaging principles
during generative SSL to improve image quality and quantitative biological
accuracy. We show that this training schema results in better starting states
for downstream supervised training on limited data. Our model also generates
images that validate on clinical quantitative analysis software.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Online Easy Example Mining for Weakly-supervised Gland Segmentation from Histology Images. (arXiv:2206.06665v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06665">
<div class="article-summary-box-inner">
<span><p>Developing an AI-assisted gland segmentation method from histology images is
critical for automatic cancer diagnosis and prognosis; however, the high cost
of pixel-level annotations hinders its applications to broader diseases.
Existing weakly-supervised semantic segmentation methods in computer vision
achieve degenerative results for gland segmentation, since the characteristics
and problems of glandular datasets are different from general object datasets.
We observe that, unlike natural images, the key problem with histology images
is the confusion of classes owning to morphological homogeneity and low color
contrast among different tissues. To this end, we propose a novel method Online
Easy Example Mining (OEEM) that encourages the network to focus on credible
supervision signals rather than noisy signals, therefore mitigating the
influence of inevitable false predictions in pseudo-masks. According to the
characteristics of glandular datasets, we design a strong framework for gland
segmentation. Our results exceed many fully-supervised methods and
weakly-supervised methods for gland segmentation over 4.4% and 6.04% at mIoU,
respectively. Code is available at https://github.com/xmed-lab/OEEM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ISLES 2022: A multi-center magnetic resonance imaging stroke lesion segmentation dataset. (arXiv:2206.06694v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06694">
<div class="article-summary-box-inner">
<span><p>Magnetic resonance imaging (MRI) is a central modality for stroke imaging. It
is used upon patient admission to make treatment decisions such as selecting
patients for intravenous thrombolysis or endovascular therapy. MRI is later
used in the duration of hospital stay to predict outcome by visualizing infarct
core size and location. Furthermore, it may be used to characterize stroke
etiology, e.g. differentiation between (cardio)-embolic and non-embolic stroke.
Computer based automated medical image processing is increasingly finding its
way into clinical routine. Previous iterations of the Ischemic Stroke Lesion
Segmentation (ISLES) challenge have aided in the generation of identifying
benchmark methods for acute and sub-acute ischemic stroke lesion segmentation.
Here we introduce an expert-annotated, multicenter MRI dataset for segmentation
of acute to subacute stroke lesions. This dataset comprises 400 multi-vendor
MRI cases with high variability in stroke lesion size, quantity and location.
It is split into a training dataset of n=250 and a test dataset of n=150. All
training data will be made publicly available. The test dataset will be used
for model validation only and will not be released to the public. This dataset
serves as the foundation of the ISLES 2022 challenge with the goal of finding
algorithmic methods to enable the development and benchmarking of robust and
accurate segmentation algorithms for ischemic stroke.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CNN-based Classification Framework for Tissues of Lung with Additional Information. (arXiv:2206.06701v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06701">
<div class="article-summary-box-inner">
<span><p>Interstitial lung diseases are a large group of heterogeneous diseases
characterized by different degrees of alveolitis and pulmonary fibrosis.
Accurately diagnosing these diseases has significant guiding value for
formulating treatment plans. Although previous work has produced impressive
results in classifying interstitial lung diseases, there is still room for
improving the accuracy of these techniques, mainly to enhance automated
decision-making. In order to improve the classification precision, our study
proposes a convolutional neural networks-based framework with additional
information. Firstly, ILD images are added with their medical information by
re-scaling the original image in Hounsfield Units. Secondly, a modified CNN
model is used to produce a vector of classification probability for each
tissue. Thirdly, location information of the input image, consisting of the
occurrence frequencies of different diseases in the CT scans on certain
locations, is used to calculate a location weight vector. Finally, the Hadamard
product between two vectors is used to produce a decision vector for the
prediction. Compared to the state-of-the-art methods, the results using a
publicly available ILD database show the potential of predicting these using
different additional information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Radial Basis Q-Network. (arXiv:2206.06712v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06712">
<div class="article-summary-box-inner">
<span><p>While reinforcement learning (RL) from raw images has been largely
investigated in the last decade, existing approaches still suffer from a number
of constraints. The high input dimension is often handled using either expert
knowledge to extract handcrafted features or environment encoding through
convolutional networks. Both solutions require numerous parameters to be
optimized. In contrast, we propose a generic method to extract sparse features
from raw images with few trainable parameters. We achieved this using a Radial
Basis Function Network (RBFN) directly on raw image. We evaluate the
performance of the proposed approach for visual extraction in Q-learning tasks
in the Vizdoom environment. Then, we compare our results with two Deep
Q-Network, one trained directly on images and another one trained on feature
extracted by a pretrained auto-encoder. We show that the proposed approach
provides similar or, in some cases, even better performances with fewer
trainable parameters while being conceptually simpler.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpretable Gait Recognition by Granger Causality. (arXiv:2206.06714v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06714">
<div class="article-summary-box-inner">
<span><p>Which joint interactions in the human gait cycle can be used as biometric
characteristics? Most current methods on gait recognition suffer from the lack
of interpretability. We propose an interpretable feature representation of gait
sequences by the graphical Granger causal inference. Gait sequence of a person
in the standardized motion capture format, constituting a set of 3D joint
spatial trajectories, is envisaged as a causal system of joints interacting in
time. We apply the graphical Granger model (GGM) to obtain the so-called
Granger causal graph among joints as a discriminative and visually
interpretable representation of a person's gait. We evaluate eleven distance
functions in the GGM feature space by established classification and
class-separability evaluation metrics. Our experiments indicate that, depending
on the metric, the most appropriate distance functions for the GGM are the
total norm distance and the Ky-Fan 1-norm distance. Experiments also show that
the GGM is able to detect the most discriminative joint interactions and that
it outperforms five related interpretable models in correct classification rate
and in Davies-Bouldin index. The proposed GGM model can serve as a
complementary tool for gait analysis in kinesiology or for gait recognition in
video surveillance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-signed neural fitting for surface reconstruction from unoriented point clouds. (arXiv:2206.06715v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06715">
<div class="article-summary-box-inner">
<span><p>Reconstructing 3D geometry from \emph{unoriented} point clouds can benefit
many downstream tasks. Recent methods mostly adopt a neural shape
representation with a neural network to represent a signed distance field and
fit the point cloud with an unsigned supervision. However, we observe that
using unsigned supervision may cause severe ambiguities and often leads to
\emph{unexpected} failures such as generating undesired surfaces in free space
when reconstructing complex structures and struggle with reconstructing
accurate surfaces. To reconstruct a better signed distance field, we propose
semi-signed neural fitting (SSN-Fitting), which consists of a semi-signed
supervision and a loss-based region sampling strategy. Our key insight is that
signed supervision is more informative and regions that are obviously outside
the object can be easily determined. Meanwhile, a novel importance sampling is
proposed to accelerate the optimization and better reconstruct the fine
details. Specifically, we voxelize and partition the object space into
\emph{sign-known} and \emph{sign-uncertain} regions, in which different
supervisions are applied. Also, we adaptively adjust the sampling rate of each
voxel according to the tracked reconstruction loss, so that the network can
focus more on the complex under-fitting regions. We conduct extensive
experiments to demonstrate that SSN-Fitting achieves state-of-the-art
performance under different settings on multiple datasets, including clean,
density-varying, and noisy data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated SSIM Regression for Detection and Quantification of Motion Artefacts in Brain MR Images. (arXiv:2206.06725v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06725">
<div class="article-summary-box-inner">
<span><p>Motion artefacts in magnetic resonance brain images are a crucial issue. The
assessment of MR image quality is fundamental before proceeding with the
clinical diagnosis. If the motion artefacts alter a correct delineation of
structure and substructures of the brain, lesions, tumours and so on, the
patients need to be re-scanned. Otherwise, neuro-radiologists could report an
inaccurate or incorrect diagnosis. The first step right after scanning a
patient is the "\textit{image quality assessment}" in order to decide if the
acquired images are diagnostically acceptable. An automated image quality
assessment based on the structural similarity index (SSIM) regression through a
residual neural network has been proposed here, with the possibility to perform
also the classification in different groups - by subdividing with SSIM ranges.
This method predicts SSIM values of an input image in the absence of a
reference ground truth image. The networks were able to detect motion
artefacts, and the best performance for the regression and classification task
has always been achieved with ResNet-18 with contrast augmentation. Mean and
standard deviation of residuals' distribution were $\mu=-0.0009$ and
$\sigma=0.0139$, respectively. Whilst for the classification task in 3, 5 and
10 classes, the best accuracies were 97, 95 and 89\%, respectively. The
obtained results show that the proposed method could be a tool in supporting
neuro-radiologists and radiographers in evaluating the image quality before the
diagnosis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Precision Localization of Peripherally Inserted Central Catheter Tip through Model-Agnostic Multi-Stage Networks. (arXiv:2206.06730v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06730">
<div class="article-summary-box-inner">
<span><p>Peripherally inserted central catheters (PICCs) have been widely used as one
of the representative central venous lines (CVCs) due to their long-term
intravascular access with low infectivity. However, PICCs have a fatal drawback
of a high frequency of tip mispositions, increasing the risk of puncture,
embolism, and complications such as cardiac arrhythmias. To automatically and
precisely detect it, various attempts have been made by using the latest deep
learning (DL) technologies. However, even with these approaches, it is still
practically difficult to determine the tip location because the multiple
fragments phenomenon (MFP) occurs in the process of predicting and extracting
the PICC line required before predicting the tip. This study aimed to develop a
system generally applied to existing models and to restore the PICC line more
exactly by removing the MFs of the model output, thereby precisely localizing
the actual tip position for detecting its disposition. To achieve this, we
proposed a multi-stage DL-based framework post-processing the PICC line
extraction result of the existing technology. The performance was compared by
each root mean squared error (RMSE) and MFP incidence rate according to whether
or not MFCN is applied to five conventional models. In internal validation,
when MFCN was applied to the existing single model, MFP was improved by an
average of 45%. The RMSE was improved by over 63% from an average of 26.85mm
(17.16 to 35.80mm) to 9.72mm (9.37 to 10.98mm). In external validation, when
MFCN was applied, the MFP incidence rate decreased by an average of 32% and the
RMSE decreased by an average of 65\%. Therefore, by applying the proposed MFCN,
we observed the significant/consistent detection performance improvement of
PICC tip location compared to the existing model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Dense Features for Point Cloud Registration Using Graph Attention Network. (arXiv:2206.06731v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06731">
<div class="article-summary-box-inner">
<span><p>Point cloud registration is a fundamental task in many applications such as
localization, mapping, tracking, and reconstruction. The successful
registration relies on extracting robust and discriminative geometric features.
Existing learning-based methods require high computing capacity for processing
a large number of raw points at the same time. Although these approaches
achieve convincing results, they are difficult to apply in real-world
situations due to high computational costs. In this paper, we introduce a
framework that efficiently and economically extracts dense features using graph
attention network for point cloud matching and registration (DFGAT). The
detector of the DFGAT is responsible for finding highly reliable key points in
large raw data sets. The descriptor of the DFGAT takes these key points
combined with their neighbors to extract invariant density features in
preparation for the matching. The graph attention network uses the attention
mechanism that enriches the relationships between point clouds. Finally, we
consider this as an optimal transport problem and use the Sinkhorn algorithm to
find positive and negative matches. We perform thorough tests on the KITTI
dataset and evaluate the effectiveness of this approach. The results show that
this method with the efficiently compact keypoint selection and description can
achieve the best performance matching metrics and reach highest success ratio
of 99.88% registration in comparison with other state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Vulnerability of Randomized Ensembles. (arXiv:2206.06737v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06737">
<div class="article-summary-box-inner">
<span><p>Despite the tremendous success of deep neural networks across various tasks,
their vulnerability to imperceptible adversarial perturbations has hindered
their deployment in the real world. Recently, works on randomized ensembles
have empirically demonstrated significant improvements in adversarial
robustness over standard adversarially trained (AT) models with minimal
computational overhead, making them a promising solution for safety-critical
resource-constrained applications. However, this impressive performance raises
the question: Are these robustness gains provided by randomized ensembles real?
In this work we address this question both theoretically and empirically. We
first establish theoretically that commonly employed robustness evaluation
methods such as adaptive PGD provide a false sense of security in this setting.
Subsequently, we propose a theoretically-sound and efficient adversarial attack
algorithm (ARC) capable of compromising random ensembles even in cases where
adaptive PGD fails to do so. We conduct comprehensive experiments across a
variety of network architectures, training schemes, datasets, and norms to
support our claims, and empirically establish that randomized ensembles are in
fact more vulnerable to $\ell_p$-bounded adversarial perturbations than even
standard AT models. Our code can be found at https://github.com/hsndbk4/ARC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recurrent Transformer Variational Autoencoders for Multi-Action Motion Synthesis. (arXiv:2206.06741v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06741">
<div class="article-summary-box-inner">
<span><p>We consider the problem of synthesizing multi-action human motion sequences
of arbitrary lengths. Existing approaches have mastered motion sequence
generation in single-action scenarios, but fail to generalize to multi-action
and arbitrary-length sequences. We fill this gap by proposing a novel efficient
approach that leverages the expressiveness of Recurrent Transformers and
generative richness of conditional Variational Autoencoders. The proposed
iterative approach is able to generate smooth and realistic human motion
sequences with an arbitrary number of actions and frames while doing so in
linear space and time. We train and evaluate the proposed approach on PROX
dataset which we augment with ground-truth action labels. Experimental
evaluation shows significant improvements in FID score and semantic consistency
metrics compared to the state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly-Supervised Crack Detection. (arXiv:2206.06743v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06743">
<div class="article-summary-box-inner">
<span><p>Pixel-level crack segmentation is widely studied due to its high impact on
building and road inspections. Recent studies have made significant
improvements in accuracy, but overlooked the annotation cost bottleneck. To
resolve this issue, we reformulate the crack segmentation problem as a
weakly-supervised problem, and propose a two-branched inference framework and
an annotation refinement module that requires no additional data, in order to
counteract the loss in annotation quality. Experimental results confirm the
effectiveness of the proposed method in crack segmentation as well as other
target domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Adversarial Attacks and Defenses in Vision Transformers trained with DINO. (arXiv:2206.06761v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06761">
<div class="article-summary-box-inner">
<span><p>This work conducts the first analysis on the robustness against adversarial
attacks on self-supervised Vision Transformers trained using DINO. First, we
evaluate whether features learned through self-supervision are more robust to
adversarial attacks than those emerging from supervised learning. Then, we
present properties arising for attacks in the latent space. Finally, we
evaluate whether three well-known defense strategies can increase adversarial
robustness in downstream tasks by only fine-tuning the classification head to
provide robustness even in view of limited compute resources. These defense
strategies are: Adversarial Training, Ensemble Adversarial Training and
Ensemble of Specialized Networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Peripheral Vision Transformer. (arXiv:2206.06801v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06801">
<div class="article-summary-box-inner">
<span><p>Human vision possesses a special type of visual processing systems called
peripheral vision. Partitioning the entire visual field into multiple contour
regions based on the distance to the center of our gaze, the peripheral vision
provides us the ability to perceive various visual features at different
regions. In this work, we take a biologically inspired approach and explore to
model peripheral vision in deep neural networks for visual recognition. We
propose to incorporate peripheral position encoding to the multi-head
self-attention layers to let the network learn to partition the visual field
into diverse peripheral regions given training data. We evaluate the proposed
network, dubbed PerViT, on the large-scale ImageNet dataset and systematically
investigate the inner workings of the model for machine perception, showing
that the network learns to perceive visual data similarly to the way that human
vision does. The state-of-the-art performance in image classification task
across various model sizes demonstrates the efficacy of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Asymmetric Dual-Decoder U-Net for Joint Rain and Haze Removal. (arXiv:2206.06803v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06803">
<div class="article-summary-box-inner">
<span><p>This work studies the joint rain and haze removal problem. In real-life
scenarios, rain and haze, two often co-occurring common weather phenomena, can
greatly degrade the clarity and quality of the scene images, leading to a
performance drop in the visual applications, such as autonomous driving.
However, jointly removing the rain and haze in scene images is ill-posed and
challenging, where the existence of haze and rain and the change of atmosphere
light, can both degrade the scene information. Current methods focus on the
contamination removal part, thus ignoring the restoration of the scene
information affected by the change of atmospheric light. We propose a novel
deep neural network, named Asymmetric Dual-decoder U-Net (ADU-Net), to address
the aforementioned challenge. The ADU-Net produces both the contamination
residual and the scene residual to efficiently remove the rain and haze while
preserving the fidelity of the scene information. Extensive experiments show
our work outperforms the existing state-of-the-art methods by a considerable
margin in both synthetic data and real-world data benchmarks, including
RainCityscapes, BID Rain, and SPA-Data. For instance, we improve the
state-of-the-art PSNR value by 2.26/4.57 on the RainCityscapes/SPA-Data,
respectively.
</p>
<p>Codes will be made available freely to the research community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning towards Synchronous Network Memorizability and Generalizability for Continual Segmentation across Multiple Sites. (arXiv:2206.06813v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06813">
<div class="article-summary-box-inner">
<span><p>In clinical practice, a segmentation network is often required to continually
learn on a sequential data stream from multiple sites rather than a
consolidated set, due to the storage cost and privacy restriction. However,
during the continual learning process, existing methods are usually restricted
in either network memorizability on previous sites or generalizability on
unseen sites. This paper aims to tackle the challenging problem of Synchronous
Memorizability and Generalizability (SMG) and to simultaneously improve
performance on both previous and unseen sites, with a novel proposed
SMG-learning framework. First, we propose a Synchronous Gradient Alignment
(SGA) objective, which \emph{not only} promotes the network memorizability by
enforcing coordinated optimization for a small exemplar set from previous sites
(called replay buffer), \emph{but also} enhances the generalizability by
facilitating site-invariance under simulated domain shift. Second, to simplify
the optimization of SGA objective, we design a Dual-Meta algorithm that
approximates the SGA objective as dual meta-objectives for optimization without
expensive computation overhead. Third, for efficient rehearsal, we configure
the replay buffer comprehensively considering additional inter-site diversity
to reduce redundancy. Experiments on prostate MRI data sequentially acquired
from six institutes demonstrate that our method can simultaneously achieve
higher memorizability and generalizability over state-of-the-art methods. Code
is available at https://github.com/jingyzhang/SMG-Learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Decoder-free Object Detection with Transformers. (arXiv:2206.06829v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06829">
<div class="article-summary-box-inner">
<span><p>Vision transformers (ViTs) are changing the landscape of object detection
approaches. A natural usage of ViTs in detection is to replace the CNN-based
backbone with a transformer-based backbone, which is straightforward and
effective, with the price of bringing considerable computation burden for
inference. More subtle usage is the DETR family, which eliminates the need for
many hand-designed components in object detection but introduces a decoder
demanding an extra-long time to converge. As a result, transformer-based object
detection can not prevail in large-scale applications. To overcome these
issues, we propose a novel decoder-free fully transformer-based (DFFT) object
detector, achieving high efficiency in both training and inference stages, for
the first time. We simplify objection detection into an encoder-only
single-level anchor-based dense prediction problem by centering around two
entry points: 1) Eliminate the training-inefficient decoder and leverage two
strong encoders to preserve the accuracy of single-level feature map
prediction; 2) Explore low-level semantic features for the detection task with
limited computational resources. In particular, we design a novel lightweight
detection-oriented transformer backbone that efficiently captures low-level
features with rich semantics based on a well-conceived ablation study.
Extensive experiments on the MS COCO benchmark demonstrate that DFFT_SMALL
outperforms DETR by 2.5% AP with 28% computation cost reduction and more than
$10\times$ fewer training epochs. Compared with the cutting-edge anchor-based
detector RetinaNet, DFFT_SMALL obtains over 5.5% AP gain while cutting down 70%
computation cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When adversarial attacks become interpretable counterfactual explanations. (arXiv:2206.06854v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06854">
<div class="article-summary-box-inner">
<span><p>We argue that, when learning a 1-Lipschitz neural network with the dual loss
of an optimal transportation problem, the gradient of the model is both the
direction of the transportation plan and the direction to the closest
adversarial attack. Traveling along the gradient to the decision boundary is no
more an adversarial attack but becomes a counterfactual explanation, explicitly
transporting from one class to the other. Through extensive experiments on XAI
metrics, we find that the simple saliency map method, applied on such networks,
becomes a reliable explanation, and outperforms the state-of-the-art
explanation approaches on unconstrained models. The proposed networks were
already known to be certifiably robust, and we prove that they are also
explainable with a fast and simple method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating histopathology transfer learning with ChampKit. (arXiv:2206.06862v1 [q-bio.QM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06862">
<div class="article-summary-box-inner">
<span><p>Histopathology remains the gold standard for diagnosis of various cancers.
Recent advances in computer vision, specifically deep learning, have
facilitated the analysis of histopathology images for various tasks, including
immune cell detection and microsatellite instability classification. The
state-of-the-art for each task often employs base architectures that have been
pretrained for image classification on ImageNet. The standard approach to
develop classifiers in histopathology tends to focus narrowly on optimizing
models for a single task, not considering the aspects of modeling innovations
that improve generalization across tasks. Here we present ChampKit
(Comprehensive Histopathology Assessment of Model Predictions toolKit): an
extensible, fully reproducible benchmarking toolkit that consists of a broad
collection of patch-level image classification tasks across different cancers.
ChampKit enables a way to systematically document the performance impact of
proposed improvements in models and methodology. ChampKit source code and data
are freely accessible at https://github.com/kaczmarj/champkit .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Object Scene Representation Transformer. (arXiv:2206.06922v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06922">
<div class="article-summary-box-inner">
<span><p>A compositional understanding of the world in terms of objects and their
geometry in 3D space is considered a cornerstone of human cognition.
Facilitating the learning of such a representation in neural networks holds
promise for substantially improving labeled data efficiency. As a key step in
this direction, we make progress on the problem of learning 3D-consistent
decompositions of complex scenes into individual objects in an unsupervised
fashion. We introduce Object Scene Representation Transformer (OSRT), a
3D-centric model in which individual object representations naturally emerge
through novel view synthesis. OSRT scales to significantly more complex scenes
with larger diversity of objects and backgrounds than existing methods. At the
same time, it is multiple orders of magnitude faster at compositional rendering
thanks to its light field parametrization and the novel Slot Mixer decoder. We
believe this work will not only accelerate future architecture exploration and
scaling efforts, but it will also serve as a useful tool for both
object-centric as well as neural scene representation learning communities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multi-task Framework for Infrared Small Target Detection and Segmentation. (arXiv:2206.06923v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06923">
<div class="article-summary-box-inner">
<span><p>Due to the complicated background and noise of infrared images, infrared
small target detection is one of the most difficult problems in the field of
computer vision. In most existing studies, semantic segmentation methods are
typically used to achieve better results. The centroid of each target is
calculated from the segmentation map as the detection result. In contrast, we
propose a novel end-to-end framework for infrared small target detection and
segmentation in this paper. First, with the use of UNet as the backbone to
maintain resolution and semantic information, our model can achieve a higher
detection accuracy than other state-of-the-art methods by attaching a simple
anchor-free head. Then, a pyramid pool module is used to further extract
features and improve the precision of target segmentation. Next, we use
semantic segmentation tasks that pay more attention to pixel-level features to
assist in the training process of object detection, which increases the average
precision and allows the model to detect some targets that were previously not
detectable. Furthermore, we develop a multi-task framework for infrared small
target detection and segmentation. Our multi-task learning model reduces
complexity by nearly half and speeds up inference by nearly twice compared to
the composite single-task model, while maintaining accuracy. The code and
models are publicly available at https://github.com/Chenastron/MTUNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comprehending and Ordering Semantics for Image Captioning. (arXiv:2206.06930v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06930">
<div class="article-summary-box-inner">
<span><p>Comprehending the rich semantics in an image and ordering them in linguistic
order are essential to compose a visually-grounded and linguistically coherent
description for image captioning. Modern techniques commonly capitalize on a
pre-trained object detector/classifier to mine the semantics in an image, while
leaving the inherent linguistic ordering of semantics under-exploited. In this
paper, we propose a new recipe of Transformer-style structure, namely
Comprehending and Ordering Semantics Networks (COS-Net), that novelly unifies
an enriched semantic comprehending and a learnable semantic ordering processes
into a single architecture. Technically, we initially utilize a cross-modal
retrieval model to search the relevant sentences of each image, and all words
in the searched sentences are taken as primary semantic cues. Next, a novel
semantic comprehender is devised to filter out the irrelevant semantic words in
primary semantic cues, and meanwhile infer the missing relevant semantic words
visually grounded in the image. After that, we feed all the screened and
enriched semantic words into a semantic ranker, which learns to allocate all
semantic words in linguistic order as humans. Such sequence of ordered semantic
words are further integrated with visual tokens of images to trigger sentence
generation. Empirical evidences show that COS-Net clearly surpasses the
state-of-the-art approaches on COCO and achieves to-date the best CIDEr score
of 141.1% on Karpathy test split. Source code is available at
\url{https://github.com/YehLi/xmodaler/tree/master/configs/image_caption/cosnet}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stand-Alone Inter-Frame Attention in Video Models. (arXiv:2206.06931v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06931">
<div class="article-summary-box-inner">
<span><p>Motion, as the uniqueness of a video, has been critical to the development of
video understanding models. Modern deep learning models leverage motion by
either executing spatio-temporal 3D convolutions, factorizing 3D convolutions
into spatial and temporal convolutions separately, or computing self-attention
along temporal dimension. The implicit assumption behind such successes is that
the feature maps across consecutive frames can be nicely aggregated.
Nevertheless, the assumption may not always hold especially for the regions
with large deformation. In this paper, we present a new recipe of inter-frame
attention block, namely Stand-alone Inter-Frame Attention (SIFA), that novelly
delves into the deformation across frames to estimate local self-attention on
each spatial location. Technically, SIFA remoulds the deformable design via
re-scaling the offset predictions by the difference between two frames. Taking
each spatial location in the current frame as the query, the locally deformable
neighbors in the next frame are regarded as the keys/values. Then, SIFA
measures the similarity between query and keys as stand-alone attention to
weighted average the values for temporal aggregation. We further plug SIFA
block into ConvNets and Vision Transformer, respectively, to devise SIFA-Net
and SIFA-Transformer. Extensive experiments conducted on four video datasets
demonstrate the superiority of SIFA-Net and SIFA-Transformer as stronger
backbones. More remarkably, SIFA-Transformer achieves an accuracy of 83.1% on
Kinetics-400 dataset. Source code is available at
\url{https://github.com/FuchenUSTC/SIFA}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">K-Space Transformer for Fast MRIReconstruction with Implicit Representation. (arXiv:2206.06947v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06947">
<div class="article-summary-box-inner">
<span><p>This paper considers the problem of fast MRI reconstruction. We propose a
novel Transformer-based framework for directly processing the sparsely sampled
signals in k-space, going beyond the limitation of regular grids as ConvNets
do. We adopt an implicit representation of spectrogram, treating spatial
coordinates as inputs, and dynamically query the partially observed
measurements to complete the spectrogram, i.e. learning the inductive bias in
k-space. To strive a balance between computational cost and reconstruction
quality, we build an hierarchical structure with low-resolution and
high-resolution decoders respectively. To validate the necessity of our
proposed modules, we have conducted extensive experiments on two public
datasets, and demonstrate superior or comparable performance over
state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Monitoring Urban Forests from Auto-Generated Segmentation Maps. (arXiv:2206.06948v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06948">
<div class="article-summary-box-inner">
<span><p>We present and evaluate a weakly-supervised methodology to quantify the
spatio-temporal distribution of urban forests based on remotely sensed data
with close-to-zero human interaction. Successfully training machine learning
models for semantic segmentation typically depends on the availability of
high-quality labels. We evaluate the benefit of high-resolution,
three-dimensional point cloud data (LiDAR) as source of noisy labels in order
to train models for the localization of trees in orthophotos. As proof of
concept we sense Hurricane Sandy's impact on urban forests in Coney Island, New
York City (NYC) and reference it to less impacted urban space in Brooklyn, NYC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AuxMix: Semi-Supervised Learning with Unconstrained Unlabeled Data. (arXiv:2206.06959v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06959">
<div class="article-summary-box-inner">
<span><p>Semi-supervised learning (SSL) has seen great strides when labeled data is
scarce but unlabeled data is abundant. Critically, most recent work assume that
such unlabeled data is drawn from the same distribution as the labeled data. In
this work, we show that state-of-the-art SSL algorithms suffer a degradation in
performance in the presence of unlabeled auxiliary data that does not
necessarily possess the same class distribution as the labeled set. We term
this problem as Auxiliary-SSL and propose AuxMix, an algorithm that leverages
self-supervised learning tasks to learn generic features in order to mask
auxiliary data that are not semantically similar to the labeled set. We also
propose to regularize learning by maximizing the predicted entropy for
dissimilar auxiliary samples. We show an improvement of 5% over existing
baselines on a ResNet-50 model when trained on CIFAR10 dataset with 4k labeled
samples and all unlabeled data is drawn from the Tiny-ImageNet dataset. We
report competitive results on several datasets and conduct ablation studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ProcTHOR: Large-Scale Embodied AI Using Procedural Generation. (arXiv:2206.06994v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06994">
<div class="article-summary-box-inner">
<span><p>Massive datasets and high-capacity models have driven many recent
advancements in computer vision and natural language understanding. This work
presents a platform to enable similar success stories in Embodied AI. We
propose ProcTHOR, a framework for procedural generation of Embodied AI
environments. ProcTHOR enables us to sample arbitrarily large datasets of
diverse, interactive, customizable, and performant virtual environments to
train and evaluate embodied agents across navigation, interaction, and
manipulation tasks. We demonstrate the power and potential of ProcTHOR via a
sample of 10,000 generated houses and a simple neural model. Models trained
using only RGB images on ProcTHOR, with no explicit mapping and no human task
supervision produce state-of-the-art results across 6 embodied AI benchmarks
for navigation, rearrangement, and arm manipulation, including the presently
running Habitat 2022, AI2-THOR Rearrangement 2022, and RoboTHOR challenges. We
also demonstrate strong 0-shot results on these benchmarks, via pre-training on
ProcTHOR with no fine-tuning on the downstream benchmark, often beating
previous state-of-the-art systems that access the downstream training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Consistent Video Instance Segmentation with Inter-Frame Recurrent Attention. (arXiv:2206.07011v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07011">
<div class="article-summary-box-inner">
<span><p>Video instance segmentation aims at predicting object segmentation masks for
each frame, as well as associating the instances across multiple frames. Recent
end-to-end video instance segmentation methods are capable of performing object
segmentation and instance association together in a direct parallel sequence
decoding/prediction framework. Although these methods generally predict higher
quality object segmentation masks, they can fail to associate instances in
challenging cases because they do not explicitly model the temporal instance
consistency for adjacent frames. We propose a consistent end-to-end video
instance segmentation framework with Inter-Frame Recurrent Attention to model
both the temporal instance consistency for adjacent frames and the global
temporal context. Our extensive experiments demonstrate that the Inter-Frame
Recurrent Attention significantly improves temporal instance consistency while
maintaining the quality of the object segmentation masks. Our model achieves
state-of-the-art accuracy on both YouTubeVIS-2019 (62.1\%) and YouTubeVIS-2021
(54.7\%) datasets. In addition, quantitative and qualitative results show that
the proposed methods predict more temporally consistent instance segmentation
masks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Turning a Curse Into a Blessing: Enabling Clean-Data-Free Defenses by Model Inversion. (arXiv:2206.07018v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07018">
<div class="article-summary-box-inner">
<span><p>It is becoming increasingly common to utilize pre-trained models provided by
third parties due to their convenience. At the same time, however, these models
may be vulnerable to both poisoning and evasion attacks. We introduce an
algorithmic framework that can mitigate potential security vulnerabilities in a
pre-trained model when clean data from its training distribution is unavailable
to the defender. The framework reverse-engineers samples from a given
pre-trained model. The resulting synthetic samples can then be used as a
substitute for clean data to perform various defenses. We consider two
important attack scenarios -- backdoor attacks and evasion attacks -- to
showcase the utility of synthesized samples. For both attacks, we show that
when supplied with our synthetic data, the state-of-the-art defenses perform
comparably or sometimes even better than the case when it's supplied with the
same amount of clean data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning 3D Object Shape and Layout without 3D Supervision. (arXiv:2206.07028v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07028">
<div class="article-summary-box-inner">
<span><p>A 3D scene consists of a set of objects, each with a shape and a layout
giving their position in space. Understanding 3D scenes from 2D images is an
important goal, with applications in robotics and graphics. While there have
been recent advances in predicting 3D shape and layout from a single image,
most approaches rely on 3D ground truth for training which is expensive to
collect at scale. We overcome these limitations and propose a method that
learns to predict 3D shape and layout for objects without any ground truth
shape or layout information: instead we rely on multi-view images with 2D
supervision which can more easily be collected at scale. Through extensive
experiments on 3D Warehouse, Hypersim, and ScanNet we demonstrate that our
approach scales to large datasets of realistic images, and compares favorably
to methods relying on 3D ground truth. On Hypersim and ScanNet where reliable
3D ground truth is not available, our approach outperforms supervised
approaches trained on smaller and less diverse datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accurate 3D Body Shape Regression using Metric and Semantic Attributes. (arXiv:2206.07036v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07036">
<div class="article-summary-box-inner">
<span><p>While methods that regress 3D human meshes from images have progressed
rapidly, the estimated body shapes often do not capture the true human shape.
This is problematic since, for many applications, accurate body shape is as
important as pose. The key reason that body shape accuracy lags pose accuracy
is the lack of data. While humans can label 2D joints, and these constrain 3D
pose, it is not so easy to "label" 3D body shape. Since paired data with images
and 3D body shape are rare, we exploit two sources of information: (1) we
collect internet images of diverse "fashion" models together with a small set
of anthropometric measurements; (2) we collect linguistic shape attributes for
a wide range of 3D body meshes and the model images. Taken together, these
datasets provide sufficient constraints to infer dense 3D shape. We exploit the
anthropometric measurements and linguistic shape attributes in several novel
ways to train a neural network, called SHAPY, that regresses 3D human pose and
shape from an RGB image. We evaluate SHAPY on public benchmarks, but note that
they either lack significant body shape variation, ground-truth shape, or
clothing variation. Thus, we collect a new dataset for evaluating 3D human
shape estimation, called HBW, containing photos of "Human Bodies in the Wild"
for which we have ground-truth 3D body scans. On this new benchmark, SHAPY
significantly outperforms state-of-the-art methods on the task of 3D body shape
estimation. This is the first demonstration that 3D body shape regression from
images can be trained from easy-to-obtain anthropometric measurements and
linguistic shape attributes. Our model and data are available at:
shapy.is.tue.mpg.de
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AnimeSR: Learning Real-World Super-Resolution Models for Animation Videos. (arXiv:2206.07038v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07038">
<div class="article-summary-box-inner">
<span><p>This paper studies the problem of real-world video super-resolution (VSR) for
animation videos, and reveals three key improvements for practical animation
VSR. First, recent real-world super-resolution approaches typically rely on
degradation simulation using basic operators without any learning capability,
such as blur, noise, and compression. In this work, we propose to learn such
basic operators from real low-quality animation videos, and incorporate the
learned ones into the degradation generation pipeline. Such
neural-network-based basic operators could help to better capture the
distribution of real degradations. Second, a large-scale high-quality animation
video dataset, AVC, is built to facilitate comprehensive training and
evaluations for animation VSR. Third, we further investigate an efficient
multi-scale network structure. It takes advantage of the efficiency of
unidirectional recurrent networks and the effectiveness of sliding-window-based
methods. Thanks to the above delicate designs, our method, AnimeSR, is capable
of restoring real-world low-quality animation videos effectively and
efficiently, achieving superior performance to previous state-of-the-art
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReCo: Retrieve and Co-segment for Zero-shot Transfer. (arXiv:2206.07045v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07045">
<div class="article-summary-box-inner">
<span><p>Semantic segmentation has a broad range of applications, but its real-world
impact has been significantly limited by the prohibitive annotation costs
necessary to enable deployment. Segmentation methods that forgo supervision can
side-step these costs, but exhibit the inconvenient requirement to provide
labelled examples from the target distribution to assign concept names to
predictions. An alternative line of work in language-image pre-training has
recently demonstrated the potential to produce models that can both assign
names across large vocabularies of concepts and enable zero-shot transfer for
classification, but do not demonstrate commensurate segmentation abilities. In
this work, we strive to achieve a synthesis of these two approaches that
combines their strengths. We leverage the retrieval abilities of one such
language-image pre-trained model, CLIP, to dynamically curate training sets
from unlabelled images for arbitrary collections of concept names, and leverage
the robust correspondences offered by modern image representations to
co-segment entities among the resulting collections. The synthetic segment
collections are then employed to construct a segmentation model (without
requiring pixel labels) whose knowledge of concepts is inherited from the
scalable pre-training process of CLIP. We demonstrate that our approach, termed
Retrieve and Co-segment (ReCo) performs favourably to unsupervised segmentation
approaches while inheriting the convenience of nameable predictions and
zero-shot transfer. We also demonstrate ReCo's ability to generate specialist
segmenters for extremely rare objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RGB-Multispectral Matching: Dataset, Learning Methodology, Evaluation. (arXiv:2206.07047v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07047">
<div class="article-summary-box-inner">
<span><p>We address the problem of registering synchronized color (RGB) and
multi-spectral (MS) images featuring very different resolution by solving
stereo matching correspondences. Purposely, we introduce a novel RGB-MS dataset
framing 13 different scenes in indoor environments and providing a total of 34
image pairs annotated with semi-dense, high-resolution ground-truth labels in
the form of disparity maps. To tackle the task, we propose a deep learning
architecture trained in a self-supervised manner by exploiting a further RGB
camera, required only during training data acquisition. In this setup, we can
conveniently learn cross-modal matching in the absence of ground-truth labels
by distilling knowledge from an easier RGB-RGB matching task based on a
collection of about 11K unlabeled image triplets. Experiments show that the
proposed pipeline sets a good performance bar (1.16 pixels average registration
error) for future research on this novel, challenging task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Auto-encoders for Track Reconstruction in Drift Chambers for CLAS12. (arXiv:2009.05144v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.05144">
<div class="article-summary-box-inner">
<span><p>In this article we describe the development of machine learning models to
assist the CLAS12 tracking algorithm by identifying tracks through inferring
missing segments in the drift chambers. Auto encoders are used to reconstruct
missing segments from track trajectory. Implemented neural network was able to
reliably reconstruct missing segment positions with accuracy of $\approx 0.35$
wires, and lead to recovery of missing tracks with accuracy of $&gt;99.8\%$.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Unique Is a Face: An Investigative Study. (arXiv:2102.04965v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.04965">
<div class="article-summary-box-inner">
<span><p>Face recognition has been widely accepted as a means of identification in
applications ranging from border control to security in the banking sector.
Surprisingly, while widely accepted, we still lack the understanding of
uniqueness or distinctiveness of faces as biometric modality. In this work, we
study the impact of factors such as image resolution, feature representation,
database size, age and gender on uniqueness denoted by the Kullback-Leibler
divergence between genuine and impostor distributions. Towards understanding
the impact, we present experimental results on the datasets AT&amp;T, LFW,
IMDb-Face, as well as ND-TWINS, with the feature extraction algorithms VGGFace,
VGG16, ResNet50, InceptionV3, MobileNet and DenseNet121, that reveal the
quantitative impact of the named factors. While these are early results, our
findings indicate the need for a better understanding of the concept of
biometric uniqueness and its implication on face recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Balancing Biases and Preserving Privacy on Balanced Faces in the Wild. (arXiv:2103.09118v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.09118">
<div class="article-summary-box-inner">
<span><p>There are demographic biases in current models used for facial recognition
(FR). Our Balanced Faces In the Wild (BFW) dataset serves as a proxy to measure
bias across ethnicity and gender subgroups, allowing one to characterize FR
performances per subgroup. We show results are non-optimal when a single score
threshold determines whether sample pairs are genuine or imposters. Within
subgroups, performance often varies significantly from the global average.
Thus, claims of specific error rates only hold for populations matching the
validation data. We mitigate the imbalanced performances using a novel domain
adaptation learning scheme on the facial features extracted using
state-of-the-art neural networks. This technique balances performance, but it
also boosts the overall performance. A benefit of the proposed is to preserve
identity information in facial features while decreasing the demographic
information they contain. The removal of demographic knowledge prevents
potential future biases from being injected into decision-making. This removal
improves privacy since less information is available or inferred about an
individual. We explore this qualitatively; we also show quantitatively that
subgroup classifiers no longer learn from the features of the proposed domain
adaptation scheme. For source code and data descriptions, see
https://github.com/visionjo/facerec-bias-bfw.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Robustness via Fisher-Rao Regularization. (arXiv:2106.06685v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06685">
<div class="article-summary-box-inner">
<span><p>Adversarial robustness has become a topic of growing interest in machine
learning since it was observed that neural networks tend to be brittle. We
propose an information-geometric formulation of adversarial defense and
introduce FIRE, a new Fisher-Rao regularization for the categorical
cross-entropy loss, which is based on the geodesic distance between the softmax
outputs corresponding to natural and perturbed input features. Based on the
information-geometric properties of the class of softmax distributions, we
derive an explicit characterization of the Fisher-Rao Distance (FRD) for the
binary and multiclass cases, and draw some interesting properties as well as
connections with standard regularization metrics. Furthermore, for a simple
linear and Gaussian model, we show that all Pareto-optimal points in the
accuracy-robustness region can be reached by FIRE while other state-of-the-art
methods fail. Empirically, we evaluate the performance of various classifiers
trained with the proposed loss on standard datasets, showing up to a
simultaneous 1\% of improvement in terms of clean and robust performances while
reducing the training time by 20\% over the best-performing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Steerable 3D Spherical Neurons. (arXiv:2106.13863v7 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.13863">
<div class="article-summary-box-inner">
<span><p>Emerging from low-level vision theory, steerable filters found their
counterpart in prior work on steerable convolutional neural networks
equivariant to rigid transformations. In our work, we propose a steerable
feed-forward learning-based approach that consists of neurons with spherical
decision surfaces and operates on point clouds. Such spherical neurons are
obtained by conformal embedding of Euclidean space and have recently been
revisited in the context of learning representations of point sets. Focusing on
3D geometry, we exploit the isometry property of spherical neurons and derive a
3D steerability constraint. After training spherical neurons to classify point
clouds in a canonical orientation, we use a tetrahedron basis to quadruplicate
the neurons and construct rotation-equivariant spherical filter banks. We then
apply the derived constraint to interpolate the filter bank outputs and, thus,
obtain a rotation-invariant network. Finally, we use a synthetic point set and
real-world 3D skeleton data to verify our theoretical findings. The code is
available at https://github.com/pavlo-melnyk/steerable-3d-neurons.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying High Accuracy Regions in Traffic Camera Images to Enhance the Estimation of Road Traffic Metrics: A Quadtree-Based Method. (arXiv:2106.14049v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.14049">
<div class="article-summary-box-inner">
<span><p>The growing number of real-time camera feeds in urban areas has made it
possible to provide high-quality traffic data for effective transportation
planning, operations, and management. However, deriving reliable traffic
metrics from these camera feeds has been a challenge due to the limitations of
current vehicle detection techniques, as well as the various camera conditions
such as height and resolution. In this work, a quadtree based algorithm is
developed to continuously partition the image extent until only regions with
high detection accuracy are remained. These regions are referred to as the
high-accuracy identification regions (HAIR) in this paper. We demonstrate how
the use of the HAIR can improve the accuracy of traffic density estimates using
images from traffic cameras at different heights and resolutions in Central
Ohio. Our experiments show that the proposed algorithm can be used to derive
robust HAIR where vehicle detection accuracy is 41 percent higher than that in
the original image extent. The use of the HAIR also significantly improves the
traffic density estimation with an overall decrease of 49 percent in root mean
squared error.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CMT: Convolutional Neural Networks Meet Vision Transformers. (arXiv:2107.06263v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.06263">
<div class="article-summary-box-inner">
<span><p>Vision transformers have been successfully applied to image recognition tasks
due to their ability to capture long-range dependencies within an image.
However, there are still gaps in both performance and computational cost
between transformers and existing convolutional neural networks (CNNs). In this
paper, we aim to address this issue and develop a network that can outperform
not only the canonical transformers, but also the high-performance
convolutional models. We propose a new transformer based hybrid network by
taking advantage of transformers to capture long-range dependencies, and of
CNNs to model local features. Furthermore, we scale it to obtain a family of
models, called CMTs, obtaining much better accuracy and efficiency than
previous convolution and transformer based models. In particular, our CMT-S
achieves 83.5% top-1 accuracy on ImageNet, while being 14x and 2x smaller on
FLOPs than the existing DeiT and EfficientNet, respectively. The proposed CMT-S
also generalizes well on CIFAR10 (99.2%), CIFAR100 (91.7%), Flowers (98.7%),
and other challenging vision datasets such as COCO (44.3% mAP), with
considerably less computational cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Relevance Learning for Few-Shot Object Detection. (arXiv:2108.02235v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02235">
<div class="article-summary-box-inner">
<span><p>Expensive bounding-box annotations have limited the development of object
detection task. Thus, it is necessary to focus on more challenging task of
few-shot object detection. It requires the detector to recognize objects of
novel classes with only a few training samples. Nowadays, many existing popular
methods adopting training way similar to meta-learning have achieved promising
performance, such as Meta R-CNN series. However, support data is only used as
the class attention to guide the detecting of query images each time. Their
relevance to each other remains unexploited. Moreover, a lot of recent works
treat the support data and query images as independent branch without
considering the relationship between them. To address this issue, we propose a
dynamic relevance learning model, which utilizes the relationship between all
support images and Region of Interest (RoI) on the query images to construct a
dynamic graph convolutional network (GCN). By adjusting the prediction
distribution of the base detector using the output of this GCN, the proposed
model serves as a hard auxiliary classification task, which guides the detector
to improve the class representation implicitly. Comprehensive experiments have
been conducted on Pascal VOC and MS-COCO dataset. The proposed model achieves
the best overall performance, which shows its effectiveness of learning more
generalized features. Our code is available at
https://github.com/liuweijie19980216/DRL-for-FSOD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Integrated Construction of Multimodal Atlases with Structural Connectomes in the Space of Riemannian Metrics. (arXiv:2109.09808v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09808">
<div class="article-summary-box-inner">
<span><p>The structural network of the brain, or structural connectome, can be
represented by fiber bundles generated by a variety of tractography methods.
While such methods give qualitative insights into brain structure, there is
controversy over whether they can provide quantitative information, especially
at the population level. In order to enable population-level statistical
analysis of the structural connectome, we propose representing a connectome as
a Riemannian metric, which is a point on an infinite-dimensional manifold. We
equip this manifold with the Ebin metric, a natural metric structure for this
space, to get a Riemannian manifold along with its associated geometric
properties. We then use this Riemannian framework to apply object-oriented
statistical analysis to define an atlas as the Fr\'echet mean of a population
of Riemannian metrics. This formulation ties into the existing framework for
diffeomorphic construction of image atlases, allowing us to construct a
multimodal atlas by simultaneously integrating complementary white matter
structure details from DWMRI and cortical details from T1-weighted MRI. We
illustrate our framework with 2D data examples of connectome registration and
atlas formation. Finally, we build an example 3D multimodal atlas using T1
images and connectomes derived from diffusion tensors estimated from a subset
of subjects from the Human Connectome Project.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Skin Deep Unlearning: Artefact and Instrument Debiasing in the Context of Melanoma Classification. (arXiv:2109.09818v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09818">
<div class="article-summary-box-inner">
<span><p>Convolutional Neural Networks have demonstrated dermatologist-level
performance in the classification of melanoma from skin lesion images, but
prediction irregularities due to biases seen within the training data are an
issue that should be addressed before widespread deployment is possible. In
this work, we robustly remove bias and spurious variation from an automated
melanoma classification pipeline using two leading bias unlearning techniques.
We show that the biases introduced by surgical markings and rulers presented in
previous studies can be reasonably mitigated using these bias removal methods.
We also demonstrate the generalisation benefits of unlearning spurious
variation relating to the imaging instrument used to capture lesion images. Our
experimental results provide evidence that the effects of each of the
aforementioned biases are notably reduced, with different debiasing techniques
excelling at different tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Stacking Ensemble Approach for Supervised Video Summarization. (arXiv:2109.12581v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12581">
<div class="article-summary-box-inner">
<span><p>Video summarization methods are usually classified into shot-level or
frame-level methods, which are individually used in a general way. This paper
investigates the underlying complementarity between the frame-level and
shot-level methods, and a stacking ensemble approach is proposed for supervised
video summarization. Firstly, we build up a stacking model to predict both the
key frame probabilities and the temporal interest segments simultaneously. The
two components are then combined via soft decision fusion to obtain the final
scores of each frame in the video. A joint loss function is proposed here to
train the model. The ablation experimental results show that the proposed
method outperforms both the two corresponding individual method. Furthermore,
extensive experiments and analysis on two benchmark datasets demonstrate the
effectiveness of our method and its superior performance in comparison with the
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FLAME: Facial Landmark Heatmap Activated Multimodal Gaze Estimation. (arXiv:2110.04828v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04828">
<div class="article-summary-box-inner">
<span><p>3D gaze estimation is about predicting the line of sight of a person in 3D
space. Person-independent models for the same lack precision due to anatomical
differences of subjects, whereas person-specific calibrated techniques add
strict constraints on scalability. To overcome these issues, we propose a novel
technique, Facial Landmark Heatmap Activated Multimodal Gaze Estimation
(FLAME), as a way of combining eye anatomical information using eye landmark
heatmaps to obtain precise gaze estimation without any person-specific
calibration. Our evaluation demonstrates a competitive performance of about 10%
improvement on benchmark datasets ColumbiaGaze and EYEDIAP. We also conduct an
ablation study to validate our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Normalized Gaussian Wasserstein Distance for Tiny Object Detection. (arXiv:2110.13389v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.13389">
<div class="article-summary-box-inner">
<span><p>Detecting tiny objects is a very challenging problem since a tiny object only
contains a few pixels in size. We demonstrate that state-of-the-art detectors
do not produce satisfactory results on tiny objects due to the lack of
appearance information. Our key observation is that Intersection over Union
(IoU) based metrics such as IoU itself and its extensions are very sensitive to
the location deviation of the tiny objects, and drastically deteriorate the
detection performance when used in anchor-based detectors. To alleviate this,
we propose a new evaluation metric using Wasserstein distance for tiny object
detection. Specifically, we first model the bounding boxes as 2D Gaussian
distributions and then propose a new metric dubbed Normalized Wasserstein
Distance (NWD) to compute the similarity between them by their corresponding
Gaussian distributions. The proposed NWD metric can be easily embedded into the
assignment, non-maximum suppression, and loss function of any anchor-based
detector to replace the commonly used IoU metric. We evaluate our metric on a
new dataset for tiny object detection (AI-TOD) in which the average object size
is much smaller than existing object detection datasets. Extensive experiments
show that, when equipped with NWD metric, our approach yields performance that
is 6.7 AP points higher than a standard fine-tuning baseline, and 6.0 AP points
higher than state-of-the-art competitors. Codes are available at:
https://github.com/jwwangchn/NWD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DocScanner: Robust Document Image Rectification with Progressive Learning. (arXiv:2110.14968v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14968">
<div class="article-summary-box-inner">
<span><p>Compared with flatbed scanners, portable smartphones are much more convenient
for physical documents digitizing. However, such digitized documents are often
distorted due to uncontrolled physical deformations, camera positions, and
illumination variations. To this end, we present DocScanner, a novel framework
for document image rectification. Different from existing methods, DocScanner
addresses this issue by introducing a progressive learning mechanism.
Specifically, DocScanner maintains a single estimate of the rectified image,
which is progressively corrected with a recurrent architecture. The iterative
refinements make DocScanner converge to a robust and superior performance,
while the lightweight recurrent architecture ensures the running efficiency. In
addition, before the above rectification process, observing the corrupted
rectified boundaries existing in prior works, DocScanner exploits a document
localization module to explicitly segment the foreground document from the
cluttered background environments. To further improve the rectification
quality, based on the geometric priori between the distorted and the rectified
images, a geometric regularization is introduced during training to further
improve the performance. Extensive experiments are conducted on the Doc3D
dataset and the DocUNet Benchmark dataset, and the quantitative and qualitative
evaluation results verify the effectiveness of DocScanner, which outperforms
previous methods on OCR accuracy, image similarity, and our proposed distortion
metric by a considerable margin. Furthermore, our DocScanner shows the highest
efficiency in runtime latency and model size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Range-Aware Attention Network for LiDAR-based 3D Object Detection with Auxiliary Point Density Level Estimation. (arXiv:2111.09515v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.09515">
<div class="article-summary-box-inner">
<span><p>3D object detection from LiDAR data for autonomous driving has been making
remarkable strides in recent years. Among the state-of-the-art methodologies,
encoding point clouds into a bird's eye view (BEV) has been demonstrated to be
both effective and efficient. Different from perspective views, BEV preserves
rich spatial and distance information between objects. Yet, while farther
objects of the same type do not appear smaller in the BEV, they contain sparser
point cloud features. This fact weakens BEV feature extraction using
shared-weight convolutional neural networks (CNNs). In order to address this
challenge, we propose Range-Aware Attention Network (RAANet), which extracts
effective BEV features and generates superior 3D object detection outputs. The
range-aware attention (RAA) convolutions significantly improve feature
extraction for near as well as far objects. Moreover, we propose a novel
auxiliary loss for point density estimation to further enhance the detection
accuracy of RAANet for occluded objects. It is worth to note that our proposed
RAA convolution is lightweight and compatible to be integrated into any CNN
architecture used for detection from a BEV. Extensive experiments on the
nuScenes and KITTI datasets demonstrate that our proposed approach outperforms
the state-of-the-art methods for LiDAR-based 3D object detection, with
real-time inference speed of 16 Hz for the full version and 22 Hz for the lite
version tested on nuScenes lidar frames. The code is publicly available at our
Github repository https://github.com/erbloo/RAAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Person Re-identification Method Based on Color Attack and Joint Defence. (arXiv:2111.09571v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.09571">
<div class="article-summary-box-inner">
<span><p>The main challenges of ReID is the intra-class variations caused by color
deviation under different camera conditions. Simultaneously, we find that most
of the existing adversarial metric attacks are realized by interfering with the
color characteristics of the sample. Based on this observation, we first
propose a local transformation attack (LTA) based on color variation. It uses
more obvious color variation to randomly disturb the color of the retrieved
image, rather than adding random noise. Experiments show that the performance
of the proposed LTA method is better than the advanced attack methods.
Furthermore, considering that the contour feature is the main factor of the
robustness of adversarial training, and the color feature will directly affect
the success rate of attack. Therefore, we further propose joint adversarial
defense (JAD) method, which include proactive defense and passive defense.
Proactive defense fuse multi-modality images to enhance the contour feature and
color feature, and considers local homomorphic transformation to solve the
over-fitting problem. Passive defense exploits the invariance of contour
feature during image scaling to mitigate the adversarial disturbance on contour
feature. Finally, a series of experimental results show that the proposed joint
adversarial defense method is more competitive than a state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Encoding Hierarchical Information in Neural Networks helps in Subpopulation Shift. (arXiv:2112.10844v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10844">
<div class="article-summary-box-inner">
<span><p>Over the past decade, deep neural networks have proven to be adept in image
classification tasks, often surpassing humans in terms of accuracy. However,
standard neural networks often fail to understand the concept of hierarchical
structures and dependencies among different classes for vision related tasks.
Humans on the other hand, seem to intuitively learn categories conceptually,
progressively growing from understanding high-level concepts down to granular
levels of categories. One of the issues arising from the inability of neural
networks to encode such dependencies within its learned structure is that of
subpopulation shift -- where models are queried with novel unseen classes taken
from a shifted population of the training set categories. Since the neural
network treats each class as independent from all others, it struggles to
categorize shifting populations that are dependent at higher levels of the
hierarchy. In this work, we study the aforementioned problems through the lens
of a novel conditional supervised training framework. We tackle subpopulation
shift by a structured learning procedure that incorporates hierarchical
information conditionally through labels. Furthermore, we introduce a notion of
graphical distance to model the catastrophic effect of mispredictions. We show
that learning in this structured hierarchical manner results in networks that
are more robust against subpopulation shifts, with an improvement up to 3\% in
terms of accuracy and up to 11\% in terms of graphical distance over standard
models on subpopulation shift benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PETS-SWINF: A regression method that considers images with metadata based Neural Network for pawpularity prediction on 2021 Kaggle Competition "PetFinder.my". (arXiv:2201.06061v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06061">
<div class="article-summary-box-inner">
<span><p>Millions of stray animals suffer on the streets or are euthanized in shelters
every day around the world. In order to better adopt stray animals, scoring the
pawpularity (cuteness) of stray animals is very important, but evaluating the
pawpularity of animals is a very labor-intensive thing. Consequently, there has
been an urgent surge of interest to develop an algorithm that scores
pawpularity of animals. However, the dataset in Kaggle not only has images, but
also metadata describing images. Most methods basically focus on the most
advanced image regression methods in recent years, but there is no good method
to deal with the metadata of images. To address the above challenges, the paper
proposes an image regression model called PETS-SWINF that considers metadata of
the images. Our results based on a dataset of Kaggle competition,
"PetFinder.my", show that PETS-SWINF has an advantage over only based images
models. Our results shows that the RMSE loss of the proposed model on the test
dataset is 17.71876 but 17.76449 without metadata. The advantage of the
proposed method is that PETS-SWINF can consider both low-order and high-order
features of metadata, and adaptively adjust the weights of the image model and
the metadata model. The performance is promising as our leadboard score is
ranked 15 out of 3545 teams (Gold medal) currently for 2021 Kaggle competition
on the challenge "PetFinder.my".
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Consistent and Efficient Evaluation Strategy for Attribution Methods. (arXiv:2202.00449v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00449">
<div class="article-summary-box-inner">
<span><p>With a variety of local feature attribution methods being proposed in recent
years, follow-up work suggested several evaluation strategies. To assess the
attribution quality across different attribution techniques, the most popular
among these evaluation strategies in the image domain use pixel perturbations.
However, recent advances discovered that different evaluation strategies
produce conflicting rankings of attribution methods and can be prohibitively
expensive to compute. In this work, we present an information-theoretic
analysis of evaluation strategies based on pixel perturbations. Our findings
reveal that the results are strongly affected by information leakage through
the shape of the removed pixels as opposed to their actual values. Using our
theoretical insights, we propose a novel evaluation framework termed Remove and
Debias (ROAD) which offers two contributions: First, it mitigates the impact of
the confounders, which entails higher consistency among evaluation strategies.
Second, ROAD does not require the computationally expensive retraining step and
saves up to 99% in computational costs compared to the state-of-the-art. We
release our source code at https://github.com/tleemann/road_evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Benchmarking and Analyzing Point Cloud Classification under Corruptions. (arXiv:2202.03377v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03377">
<div class="article-summary-box-inner">
<span><p>3D perception, especially point cloud classification, has achieved
substantial progress. However, in real-world deployment, point cloud
corruptions are inevitable due to the scene complexity, sensor inaccuracy, and
processing imprecision. In this work, we aim to rigorously benchmark and
analyze point cloud classification under corruptions. To conduct a systematic
investigation, we first provide a taxonomy of common 3D corruptions and
identify the atomic corruptions. Then, we perform a comprehensive evaluation on
a wide range of representative point cloud models to understand their
robustness and generalizability. Our benchmark results show that although point
cloud classification performance improves over time, the state-of-the-art
methods are on the verge of being less robust. Based on the obtained
observations, we propose several effective techniques to enhance point cloud
classifier robustness. We hope our comprehensive benchmark, in-depth analysis,
and proposed techniques could spark future research in robust 3D perception.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Acoustic Matching. (arXiv:2202.06875v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06875">
<div class="article-summary-box-inner">
<span><p>We introduce the visual acoustic matching task, in which an audio clip is
transformed to sound like it was recorded in a target environment. Given an
image of the target environment and a waveform for the source audio, the goal
is to re-synthesize the audio to match the target room acoustics as suggested
by its visible geometry and materials. To address this novel task, we propose a
cross-modal transformer model that uses audio-visual attention to inject visual
properties into the audio and generate realistic audio output. In addition, we
devise a self-supervised training objective that can learn acoustic matching
from in-the-wild Web videos, despite their lack of acoustically mismatched
audio. We demonstrate that our approach successfully translates human speech to
a variety of real-world environments depicted in images, outperforming both
traditional acoustic matching and more heavily supervised baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">General-purpose, long-context autoregressive modeling with Perceiver AR. (arXiv:2202.07765v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07765">
<div class="article-summary-box-inner">
<span><p>Real-world data is high-dimensional: a book, image, or musical performance
can easily contain hundreds of thousands of elements even after compression.
However, the most commonly used autoregressive models, Transformers, are
prohibitively expensive to scale to the number of inputs and layers needed to
capture this long-range structure. We develop Perceiver AR, an autoregressive,
modality-agnostic architecture which uses cross-attention to map long-range
inputs to a small number of latents while also maintaining end-to-end causal
masking. Perceiver AR can directly attend to over a hundred thousand tokens,
enabling practical long-context density estimation without the need for
hand-crafted sparsity patterns or memory mechanisms. When trained on images or
music, Perceiver AR generates outputs with clear long-term coherence and
structure. Our architecture also obtains state-of-the-art likelihood on
long-sequence benchmarks, including 64 x 64 ImageNet images and PG-19 books.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalized Classification of Satellite Image Time Series with Thermal Positional Encoding. (arXiv:2203.09175v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09175">
<div class="article-summary-box-inner">
<span><p>Large-scale crop type classification is a task at the core of remote sensing
efforts with applications of both economic and ecological importance. Current
state-of-the-art deep learning methods are based on self-attention and use
satellite image time series (SITS) to discriminate crop types based on their
unique growth patterns. However, existing methods generalize poorly to regions
not seen during training mainly due to not being robust to temporal shifts of
the growing season caused by variations in climate. To this end, we propose
Thermal Positional Encoding (TPE) for attention-based crop classifiers. Unlike
previous positional encoding based on calendar time (e.g. day-of-year), TPE is
based on thermal time, which is obtained by accumulating daily average
temperatures over the growing season. Since crop growth is directly related to
thermal time, but not calendar time, TPE addresses the temporal shifts between
different regions to improve generalization. We propose multiple TPE
strategies, including learnable methods, to further improve results compared to
the common fixed positional encodings. We demonstrate our approach on a crop
classification task across four different European regions, where we obtain
state-of-the-art generalization results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Vision Transformers for Joint SAR-optical Representation Learning. (arXiv:2204.05381v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.05381">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning (SSL) has attracted much interest in remote sensing
and earth observation due to its ability to learn task-agnostic representations
without human annotation. While most of the existing SSL works in remote
sensing utilize ConvNet backbones and focus on a single modality, we explore
the potential of vision transformers (ViTs) for joint SAR-optical
representation learning. Based on DINO, a state-of-the-art SSL algorithm that
distills knowledge from two augmented views of an input image, we combine SAR
and optical imagery by concatenating all channels to a unified input.
Subsequently, we randomly mask out channels of one modality as a data
augmentation strategy. While training, the model gets fed optical-only,
SAR-only, and SAR-optical image pairs learning both inner- and intra-modality
representations. Experimental results employing the BigEarthNet-MM dataset
demonstrate the benefits of both, the ViT backbones and the proposed multimodal
SSL algorithm DINO-MM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mitigating Bias in Facial Analysis Systems by Incorporating Label Diversity. (arXiv:2204.06364v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06364">
<div class="article-summary-box-inner">
<span><p>Facial analysis models are increasingly applied in real-world applications
that have significant impact on peoples' lives. However, as literature has
shown, models that automatically classify facial attributes might exhibit
algorithmic discrimination behavior with respect to protected groups,
potentially posing negative impacts on individuals and society. It is therefore
critical to develop techniques that can mitigate unintended biases in facial
classifiers. Hence, in this work, we introduce a novel learning method that
combines both subjective human-based labels and objective annotations based on
mathematical definitions of facial traits. Specifically, we generate new
objective annotations from two large-scale human-annotated dataset, each
capturing a different perspective of the analyzed facial trait. We then propose
an ensemble learning method, which combines individual models trained on
different types of annotations. We provide an in-depth analysis of the
annotation procedure as well as the datasets distribution. Moreover, we
empirically demonstrate that, by incorporating label diversity, our method
successfully mitigates unintended biases, while maintaining significant
accuracy on the downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Missingness Bias in Model Debugging. (arXiv:2204.08945v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.08945">
<div class="article-summary-box-inner">
<span><p>Missingness, or the absence of features from an input, is a concept
fundamental to many model debugging tools. However, in computer vision, pixels
cannot simply be removed from an image. One thus tends to resort to heuristics
such as blacking out pixels, which may in turn introduce bias into the
debugging process. We study such biases and, in particular, show how
transformer-based architectures can enable a more natural implementation of
missingness, which side-steps these issues and improves the reliability of
model debugging in practice. Our code is available at
https://github.com/madrylab/missingness
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Keypoint based Sign Language Translation without Glosses. (arXiv:2204.10511v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.10511">
<div class="article-summary-box-inner">
<span><p>Sign Language Translation (SLT) is a task that has not been studied
relatively much compared to the study of Sign Language Recognition (SLR).
However, the SLR is a study that recognizes the unique grammar of sign
language, which is different from the spoken language and has a problem that
non-disabled people cannot easily interpret. So, we're going to solve the
problem of translating directly spoken language in sign language video. To this
end, we propose a new keypoint normalization method for performing translation
based on the skeleton point of the signer and robustly normalizing these points
in sign language translation. It contributed to performance improvement by a
customized normalization method depending on the body parts. In addition, we
propose a stochastic frame selection method that enables frame augmentation and
sampling at the same time. Finally, it is translated into the spoken language
through an Attention-based translation model. Our method can be applied to
various datasets in a way that can be applied to datasets without glosses. In
addition, quantitative experimental evaluation proved the excellence of our
method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoCa: Contrastive Captioners are Image-Text Foundation Models. (arXiv:2205.01917v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01917">
<div class="article-summary-box-inner">
<span><p>Exploring large-scale pretrained foundation models is of significant interest
in computer vision because these models can be quickly transferred to many
downstream tasks. This paper presents Contrastive Captioner (CoCa), a
minimalist design to pretrain an image-text encoder-decoder foundation model
jointly with contrastive loss and captioning loss, thereby subsuming model
capabilities from contrastive approaches like CLIP and generative methods like
SimVLM. In contrast to standard encoder-decoder transformers where all decoder
layers attend to encoder outputs, CoCa omits cross-attention in the first half
of decoder layers to encode unimodal text representations, and cascades the
remaining decoder layers which cross-attend to the image encoder for multimodal
image-text representations. We apply a contrastive loss between unimodal image
and text embeddings, in addition to a captioning loss on the multimodal decoder
outputs which predicts text tokens autoregressively. By sharing the same
computational graph, the two training objectives are computed efficiently with
minimal overhead. CoCa is pretrained end-to-end and from scratch on both
web-scale alt-text data and annotated images by treating all labels simply as
text, seamlessly unifying natural language supervision for representation
learning. Empirically, CoCa achieves state-of-the-art performance with
zero-shot transfer or minimal task-specific adaptation on a broad range of
downstream tasks, spanning visual recognition (ImageNet, Kinetics-400/600/700,
Moments-in-Time), crossmodal retrieval (MSCOCO, Flickr30K, MSR-VTT), multimodal
understanding (VQA, SNLI-VE, NLVR2), and image captioning (MSCOCO, NoCaps).
Notably on ImageNet classification, CoCa obtains 86.3% zero-shot top-1
accuracy, 90.6% with a frozen encoder and learned classification head, and new
state-of-the-art 91.0% top-1 accuracy on ImageNet with a finetuned encoder.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WKGM: Weight-K-space Generative Model for Parallel Imaging Reconstruction. (arXiv:2205.03883v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.03883">
<div class="article-summary-box-inner">
<span><p>Deep learning based parallel Imaging (PI) has made great progresses in recent
years to accelerate magnetic resonance imaging (MRI). Nevertheless, the
performanc-es and robustness of existing methods can still be im-proved. In
this work, we propose to explore the k-space domain learning via robust
generative modeling for flexible PI reconstruction, coined weight-k-space
genera-tive model (WKGM). Specifically, WKGM is a general-ized k-space domain
model, where the k-space weighting technology and high-dimensional space
augmentation design are efficiently incorporated for score-based gen-erative
model training, resulting in good and robust re-constructions. In addition,
WKGM is flexible and thus can be synergistically combined with various
traditional k-space PI models, generating learning-based priors to produce
high-fidelity reconstructions. Experimental re-sults on datasets with varying
sampling patterns and ac-celeration factors demonstrate that WKGM can attain
state-of-the-art reconstruction results with the well-learned k-space
generative prior.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PillarNet: Real-Time and High-Performance Pillar-based 3D Object Detection. (arXiv:2205.07403v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.07403">
<div class="article-summary-box-inner">
<span><p>Real-time and high-performance 3D object detection is of critical importance
for autonomous driving. Recent top-performing 3D object detectors mainly rely
on point-based or 3D voxel-based convolutions, which are both computationally
inefficient for onboard deployment. In contrast, pillar-based methods use
solely 2D convolutions, which consume less computation resources, but they lag
far behind their voxel-based counterparts in detection accuracy. In this paper,
by examining the primary performance gap between pillar- and voxel-based
detectors, we develop a real-time and high-performance pillar-based detector,
dubbed PillarNet. The proposed PillarNet consists of a powerful encoder network
for effective pillar feature learning, a neck network for spatial-semantic
feature fusion and the commonly used detect head. Using only 2D convolutions,
PillarNet is flexible to an optional pillar size and compatible with classical
2D CNN backbones, such as VGGNet and ResNet.Additionally, PillarNet benefits
from our designed orientation-decoupled IoU regression loss along with the
IoU-aware prediction branch. Extensive experimental results on large-scale
nuScenes Dataset and Waymo Open Dataset demonstrate that the proposed PillarNet
performs well over the state-of-the-art 3D detectors in terms of effectiveness
and efficiency. The source code is available at
https://github.com/agent-sgs/PillarNet.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SelfReformer: Self-Refined Network with Transformer for Salient Object Detection. (arXiv:2205.11283v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11283">
<div class="article-summary-box-inner">
<span><p>The global and local contexts significantly contribute to the integrity of
predictions in Salient Object Detection (SOD). Unfortunately, existing methods
still struggle to generate complete predictions with fine details. There are
two major problems in conventional approaches: first, for global context,
high-level CNN-based encoder features cannot effectively catch long-range
dependencies, resulting in incomplete predictions. Second, downsampling the
ground truth to fit the size of predictions will introduce inaccuracy as the
ground truth details are lost during interpolation or pooling. Thus, in this
work, we developed a Transformer-based network and framed a supervised task for
a branch to learn the global context information explicitly. Besides, we adopt
Pixel Shuffle from Super-Resolution (SR) to reshape the predictions back to the
size of ground truth instead of the reverse. Thus details in the ground truth
are untouched. In addition, we developed a two-stage Context Refinement Module
(CRM) to fuse global context and automatically locate and refine the local
details in the predictions. The proposed network can guide and correct itself
based on the global and local context generated, thus is named, Self-Refined
Transformer (SelfReformer). Extensive experiments and evaluation results on
five benchmark datasets demonstrate the outstanding performance of the network,
and we achieved the state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CellCentroidFormer: Combining Self-attention and Convolution for Cell Detection. (arXiv:2206.00338v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00338">
<div class="article-summary-box-inner">
<span><p>Cell detection in microscopy images is important to study how cells move and
interact with their environment. Most recent deep learning-based methods for
cell detection use convolutional neural networks (CNNs). However, inspired by
the success in other computer vision applications, vision transformers (ViTs)
are also used for this purpose. We propose a novel hybrid CNN-ViT model for
cell detection in microscopy images to exploit the advantages of both types of
deep learning models. We employ an efficient CNN, that was pre-trained on the
ImageNet dataset, to extract image features and utilize transfer learning to
reduce the amount of required training data. Extracted image features are
further processed by a combination of convolutional and transformer layers, so
that the convolutional layers can focus on local information and the
transformer layers on global information. Our centroid-based cell detection
method represents cells as ellipses and is end-to-end trainable. Furthermore,
we show that our proposed model can outperform fully convolutional one-stage
detectors on four different 2D microscopy datasets. Code is available at:
https://github.com/roydenwa/cell-centroid-former
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EfficientFormer: Vision Transformers at MobileNet Speed. (arXiv:2206.01191v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01191">
<div class="article-summary-box-inner">
<span><p>Vision Transformers (ViT) have shown rapid progress in computer vision tasks,
achieving promising results on various benchmarks. However, due to the massive
number of parameters and model design, e.g., attention mechanism, ViT-based
models are generally times slower than lightweight convolutional networks.
Therefore, the deployment of ViT for real-time applications is particularly
challenging, especially on resource-constrained hardware such as mobile
devices. Recent efforts try to reduce the computation complexity of ViT through
network architecture search or hybrid design with MobileNet block, yet the
inference speed is still unsatisfactory. This leads to an important question:
can transformers run as fast as MobileNet while obtaining high performance? To
answer this, we first revisit the network architecture and operators used in
ViT-based models and identify inefficient designs. Then we introduce a
dimension-consistent pure transformer (without MobileNet blocks) as a design
paradigm. Finally, we perform latency-driven slimming to get a series of final
models dubbed EfficientFormer. Extensive experiments show the superiority of
EfficientFormer in performance and speed on mobile devices. Our fastest model,
EfficientFormer-L1, achieves $79.2\%$ top-1 accuracy on ImageNet-1K with only
$1.6$ ms inference latency on iPhone 12 (compiled with CoreML), which { runs as
fast as MobileNetV2$\times 1.4$ ($1.6$ ms, $74.7\%$ top-1),} and our largest
model, EfficientFormer-L7, obtains $83.3\%$ accuracy with only $7.0$ ms
latency. Our work proves that properly designed transformers can reach
extremely low latency on mobile devices while maintaining high performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Unbiased Transferability for Domain Adaptation by Uncertainty Modeling. (arXiv:2206.01319v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.01319">
<div class="article-summary-box-inner">
<span><p>Domain adaptation (DA) aims to transfer knowledge learned from a labeled
source domain to an unlabeled or a less labeled but related target domain.
Ideally, the source and target distributions should be aligned to each other
equally to achieve unbiased knowledge transfer. However, due to the significant
imbalance between the amount of annotated data in the source and target
domains, usually only the target distribution is aligned to the source domain,
leading to adapting unnecessary source specific knowledge to the target domain,
i.e., biased domain adaptation. To resolve this problem, in this work, we delve
into the transferability estimation problem in domain adaptation and propose a
non-intrusive Unbiased Transferability Estimation Plug-in (UTEP) by modeling
the uncertainty of a discriminator in adversarial-based DA methods to optimize
unbiased transfer. We theoretically analyze the effectiveness of the proposed
approach to unbiased transferability learning in DA. Furthermore, to alleviate
the impact of imbalanced annotated data, we utilize the estimated uncertainty
for pseudo label selection of unlabeled samples in the target domain, which
helps achieve better marginal and conditional distribution alignments between
domains. Extensive experimental results on a high variety of DA benchmark
datasets show that the proposed approach can be readily incorporated into
various adversarial-based DA methods, achieving state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PP-OCRv3: More Attempts for the Improvement of Ultra Lightweight OCR System. (arXiv:2206.03001v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03001">
<div class="article-summary-box-inner">
<span><p>Optical character recognition (OCR) technology has been widely used in
various scenes, as shown in Figure 1. Designing a practical OCR system is still
a meaningful but challenging task. In previous work, considering the efficiency
and accuracy, we proposed a practical ultra lightweight OCR system (PP-OCR),
and an optimized version PP-OCRv2. In order to further improve the performance
of PP-OCRv2, a more robust OCR system PP-OCRv3 is proposed in this paper.
PP-OCRv3 upgrades the text detection model and text recognition model in 9
aspects based on PP-OCRv2. For text detector, we introduce a PAN module with
large receptive field named LK-PAN, a FPN module with residual attention
mechanism named RSE-FPN, and DML distillation strategy. For text recognizer,
the base model is replaced from CRNN to SVTR, and we introduce lightweight text
recognition network SVTR LCNet, guided training of CTC by attention, data
augmentation strategy TextConAug, better pre-trained model by self-supervised
TextRotNet, UDML, and UIM to accelerate the model and improve the effect.
Experiments on real data show that the hmean of PP-OCRv3 is 5% higher than
PP-OCRv2 under comparable inference speed. All the above mentioned models are
open-sourced and the code is available in the GitHub repository PaddleOCR which
is powered by PaddlePaddle.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MS-RNN: A Flexible Multi-Scale Framework for Spatiotemporal Predictive Learning. (arXiv:2206.03010v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03010">
<div class="article-summary-box-inner">
<span><p>Spatiotemporal predictive learning is to predict future frame changes through
historical prior knowledge. Previous work improves the performance by making
the network wider and deeper, but that also brings huge memory overhead, which
seriously hinders the development and application of the technology. Scale is
another dimension to improve model performance in common computer vision tasks,
which can decrease the computing requirements and better sense context. Such an
important dimension has not been considered and explored by recent RNN models.
In this paper, learning from the benefit of multi-scale, we propose a general
framework named Multi-Scale RNN (MS-RNN) to boost recent RNN models. We verify
the MS-RNN framework by exhaustive experiments with 6 popular RNN models
(ConvLSTM, TrajGRU, PredRNN, PredRNN++, MIM, and MotionRNN) on 4 different
datasets (Moving MNIST, KTH, TaxiBJ, and HKO-7). The results show the
efficiency that the RNN models incorporating our framework have much lower
memory cost but better performance than before. Our code is released at
\url{https://github.com/mazhf/MS-RNN}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wavelet Prior Attention Learning in Axial Inpainting Network. (arXiv:2206.03113v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03113">
<div class="article-summary-box-inner">
<span><p>Image inpainting is the task of filling masked or unknown regions of an image
with visually realistic contents, which has been remarkably improved by Deep
Neural Networks (DNNs) recently. Essentially, as an inverse problem, the
inpainting has the underlying challenges of reconstructing semantically
coherent results without texture artifacts. Many previous efforts have been
made via exploiting attention mechanisms and prior knowledge, such as edges and
semantic segmentation. However, these works are still limited in practice by an
avalanche of learnable prior parameters and prohibitive computational burden.
To this end, we propose a novel model -- Wavelet prior attention learning in
Axial Inpainting Network (WAIN), whose generator contains the encoder, decoder,
as well as two key components of Wavelet image Prior Attention (WPA) and
stacked multi-layer Axial-Transformers (ATs). Particularly, the WPA guides the
high-level feature aggregation in the multi-scale frequency domain, alleviating
the textual artifacts. Stacked ATs employ unmasked clues to help model
reasonable features along with low-level features of horizontal and vertical
axes, improving the semantic coherence. Extensive quantitative and qualitative
experiments on Celeba-HQ and Places2 datasets are conducted to validate that
our WAIN can achieve state-of-the-art performance over the competitors. The
codes and models will be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Understanding Why Mask-Reconstruction Pretraining Helps in Downstream Tasks. (arXiv:2206.03826v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.03826">
<div class="article-summary-box-inner">
<span><p>For unsupervised pretraining, mask-reconstruction pretraining (MRP)
approaches randomly mask input patches and then reconstruct pixels or semantic
features of these masked patches via an auto-encoder. Then for a downstream
task, supervised fine-tuning the pretrained encoder remarkably surpasses the
conventional supervised learning (SL) trained from scratch. However, it is
still unclear 1) how MRP performs semantic learning in the pretraining phase
and 2) why it helps in downstream tasks. To solve these problems, we
theoretically show that on an auto-encoder of a two/one-layered convolution
encoder/decoder, MRP can capture all discriminative semantics in the
pretraining dataset, and accordingly show its provable improvement over SL on
the classification downstream task. Specifically, we assume that pretraining
dataset contains multi-view samples of ratio $1-\mu$ and single-view samples of
ratio $\mu$, where multi/single-view samples has multiple/single discriminative
semantics. Then for pretraining, we prove that 1) the convolution kernels of
the MRP encoder captures all discriminative semantics in the pretraining data;
and 2) a convolution kernel captures at most one semantic. Accordingly, in the
downstream supervised fine-tuning, most semantics would be captured and
different semantics would not be fused together. This helps the downstream
fine-tuned network to easily establish the relation between kernels and
semantic class labels. In this way, the fine-tuned encoder in MRP provably
achieves zero test error with high probability for both multi-view and
single-view test data. In contrast, as proved by~[3], conventional SL can only
obtain a test accuracy between around $0.5\mu$ for single-view test data. These
results together explain the benefits of MRP in downstream tasks. Experimental
results testify to multi-view data assumptions and our theoretical
implications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Prompt Search. (arXiv:2206.04673v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04673">
<div class="article-summary-box-inner">
<span><p>The size of vision models has grown exponentially over the last few years,
especially after the emergence of Vision Transformer. This has motivated the
development of parameter-efficient tuning methods, such as learning adapter
layers or visual prompt tokens, which allow a tiny portion of model parameters
to be trained whereas the vast majority obtained from pre-training are frozen.
However, designing a proper tuning method is non-trivial: one might need to try
out a lengthy list of design choices, not to mention that each downstream
dataset often requires custom designs. In this paper, we view the existing
parameter-efficient tuning methods as "prompt modules" and propose Neural
prOmpt seArcH (NOAH), a novel approach that learns, for large vision models,
the optimal design of prompt modules through a neural architecture search
algorithm, specifically for each downstream dataset. By conducting extensive
experiments on over 20 vision datasets, we demonstrate that NOAH (i) is
superior to individual prompt modules, (ii) has a good few-shot learning
ability, and (iii) is domain-generalizable. The code and models are available
at https://github.com/Davidzhangyuanhan/NOAH.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning self-calibrated optic disc and cup segmentation from multi-rater annotations. (arXiv:2206.05092v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05092">
<div class="article-summary-box-inner">
<span><p>The segmentation of optic disc(OD) and optic cup(OC) from fundus images is an
important fundamental task for glaucoma diagnosis. In the clinical practice, it
is often necessary to collect opinions from multiple experts to obtain the
final OD/OC annotation. This clinical routine helps to mitigate the individual
bias. But when data is multiply annotated, standard deep learning models will
be inapplicable. In this paper, we propose a novel neural network framework to
learn OD/OC segmentation from multi-rater annotations. The segmentation results
are self-calibrated through the iterative optimization of multi-rater
expertness estimation and calibrated OD/OC segmentation. In this way, the
proposed method can realize a mutual improvement of both tasks and finally
obtain a refined segmentation result. Specifically, we propose Diverging
Model(DivM) and Converging Model(ConM) to process the two tasks respectively.
ConM segments the raw image based on the multi-rater expertness map provided by
DivM. DivM generates multi-rater expertness map from the segmentation mask
provided by ConM. The experiment results show that by recurrently running ConM
and DivM, the results can be self-calibrated so as to outperform a range of
state-of-the-art(SOTA) multi-rater segmentation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Object Occlusion of Adding New Categories in Objection Detection. (arXiv:2206.05730v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05730">
<div class="article-summary-box-inner">
<span><p>Building instance detection models that are data efficient and can handle
rare object categories is an important challenge in computer vision. But data
collection methods and metrics are lack of research towards real scenarios
application using neural network. Here, we perform a systematic study of the
Object Occlusion data collection and augmentation methods where we imitate
object occlusion relationship in target scenarios. However, we find that the
simple mechanism of object occlusion is good enough and can provide acceptable
accuracy in real scenarios adding new category. We illustate that only adding
15 images of new category in a half million training dataset with hundreds
categories, can give this new category 95% accuracy in unseen test dataset
including thousands of images of this category.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improve Ranking Correlation of Super-net through Training Scheme from One-shot NAS to Few-shot NAS. (arXiv:2206.05896v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05896">
<div class="article-summary-box-inner">
<span><p>The algorithms of one-shot neural architecture search(NAS) have been widely
used to reduce computation consumption. However, because of the interference
among the subnets in which weights are shared, the subnets inherited from these
super-net trained by those algorithms have poor consistency in precision
ranking. To address this problem, we propose a step-by-step training super-net
scheme from one-shot NAS to few-shot NAS. In the training scheme, we firstly
train super-net in a one-shot way, and then we disentangle the weights of
super-net by splitting them into multi-subnets and training them gradually.
Finally, our method ranks 4th place in the CVPR2022 3rd Lightweight NAS
Challenge Track1. Our code is available at
https://github.com/liujiawei2333/CVPR2022-NAS-competition-Track-1-4th-solution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ATDN vSLAM: An all-through Deep Learning-Based Solution for Visual Simultaneous Localization and Mapping. (arXiv:2206.05963v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05963">
<div class="article-summary-box-inner">
<span><p>In this paper, a novel solution is introduced for visual Simultaneous
Localization and Mapping (vSLAM) that is built up of Deep Learning components.
The proposed architecture is a highly modular framework in which each component
offers state of the art results in their respective fields of vision-based deep
learning solutions. The paper shows that with the synergic integration of these
individual building blocks, a functioning and efficient all-through deep neural
(ATDN) vSLAM system can be created. The Embedding Distance Loss function is
introduced and using it the ATDN architecture is trained. The resulting system
managed to achieve 4.4% translation and 0.0176 deg/m rotational error on a
subset of the KITTI dataset. The proposed architecture can be used for
efficient and low-latency autonomous driving (AD) aiding database creation as
well as a basis for autonomous vehicle (AV) control.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Human-in-the-loop System for Guiding DNNs Attention. (arXiv:2206.05981v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05981">
<div class="article-summary-box-inner">
<span><p>Attention guidance is an approach to addressing dataset bias in deep
learning, where the model relies on incorrect features to make decisions.
Focusing on image classification tasks, we propose an efficient
human-in-the-loop system to interactively direct the attention of classifiers
to the regions specified by users, thereby reducing the influence of
co-occurrence bias and improving the transferability and interpretability of a
DNN. Previous approaches for attention guidance require the preparation of
pixel-level annotations and are not designed as interactive systems. We present
a new interactive method to allow users to annotate images with simple clicks,
and study a novel active learning strategy to significantly reduce the number
of annotations. We conducted both a numerical evaluation and a user study to
evaluate the proposed system on multiple datasets. Compared to the existing
non-active-learning approach which usually relies on huge amounts of
polygon-based segmentation masks to fine-tune or train the DNNs, our system can
save lots of labor and money and obtain a fine-tuned network that works better
even when the dataset is biased. The experiment results indicate that the
proposed system is efficient, reasonable, and reliable.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Better Teacher Better Student: Dynamic Prior Knowledge for Knowledge Distillation. (arXiv:2206.06067v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06067">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation (KD) has shown very promising capabilities in
transferring learning representations from large models (teachers) to small
models (students). However, as the capacity gap between students and teachers
becomes larger, existing KD methods fail to achieve better results. Our work
shows that the 'prior knowledge' is vital to KD, especially when applying large
teachers. Particularly, we propose the dynamic prior knowledge (DPK), which
integrates part of the teacher's features as the prior knowledge before the
feature distillation. This means that our method also takes the teacher's
feature as `input', not just `target'. Besides, we dynamically adjust the ratio
of the prior knowledge during the training phase according to the feature gap,
thus guiding the student in an appropriate difficulty. To evaluate the proposed
method, we conduct extensive experiments on two image classification benchmarks
(i.e. CIFAR100 and ImageNet) and an object detection benchmark (i.e. MS COCO).
The results demonstrate the superiority of our method in performance under
varying settings. More importantly, our DPK makes the performance of the
student model is positively correlated with that of the teacher model, which
means that we can further boost the accuracy of students by applying larger
teachers. Our codes will be publicly available for the reproducibility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Featurized Query R-CNN. (arXiv:2206.06258v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06258">
<div class="article-summary-box-inner">
<span><p>The query mechanism introduced in the DETR method is changing the paradigm of
object detection and recently there are many query-based methods have obtained
strong object detection performance. However, the current query-based detection
pipelines suffer from the following two issues. Firstly, multi-stage decoders
are required to optimize the randomly initialized object queries, incurring a
large computation burden. Secondly, the queries are fixed after training,
leading to unsatisfying generalization capability. To remedy the above issues,
we present featurized object queries predicted by a query generation network in
the well-established Faster R-CNN framework and develop a Featurized Query
R-CNN. Extensive experiments on the COCO dataset show that our Featurized Query
R-CNN obtains the best speed-accuracy trade-off among all R-CNN detectors,
including the recent state-of-the-art Sparse R-CNN detector. The code is
available at \url{https://github.com/hustvl/Featurized-QueryRCNN}.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-06-15 23:08:24.713576676 UTC">2022-06-15 23:08:24 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>