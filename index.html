<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-10-26T01:30:00Z">10-26</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Law Smells: Defining and Detecting Problematic Patterns in Legal Drafting. (arXiv:2110.11984v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11984">
<div class="article-summary-box-inner">
<span><p>Building on the computer science concept of code smells, we initiate the
study of law smells, i.e., patterns in legal texts that pose threats to the
comprehensibility and maintainability of the law. With five intuitive law
smells as running examples - namely, duplicated phrase, long element, large
reference tree, ambiguous syntax, and natural language obsession -, we develop
a comprehensive law smell taxonomy. This taxonomy classifies law smells by when
they can be detected, which aspects of law they relate to, and how they can be
discovered. We introduce text-based and graph-based methods to identify
instances of law smells, confirming their utility in practice using the United
States Code as a test case. Our work demonstrates how ideas from software
engineering can be leveraged to assess and improve the quality of legal code,
thus drawing attention to an understudied area in the intersection of law and
computer science and highlighting the potential of computational legal
drafting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Embracing advanced AI/ML to help investors achieve success: Vanguard Reinforcement Learning for Financial Goal Planning. (arXiv:2110.12003v1 [q-fin.ST])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12003">
<div class="article-summary-box-inner">
<span><p>In the world of advice and financial planning, there is seldom one right
answer. While traditional algorithms have been successful in solving linear
problems, its success often depends on choosing the right features from a
dataset, which can be a challenge for nuanced financial planning scenarios.
Reinforcement learning is a machine learning approach that can be employed with
complex data sets where picking the right features can be nearly impossible. In
this paper, we will explore the use of machine learning for financial
forecasting, predicting economic indicators, and creating a savings strategy.
Vanguard ML algorithm for goals-based financial planning is based on deep
reinforcement learning that identifies optimal savings rates across multiple
goals and sources of income to help clients achieve financial success. Vanguard
learning algorithms are trained to identify market indicators and behaviors too
complex to capture with formulas and rules, instead, it works to model the
financial success trajectory of investors and their investment outcomes as a
Markov decision process. We believe that reinforcement learning can be used to
create value for advisors and end-investors, creating efficiency, more
personalized plans, and data to enable customized solutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ClimateBert: A Pretrained Language Model for Climate-Related Text. (arXiv:2110.12010v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12010">
<div class="article-summary-box-inner">
<span><p>Over the recent years, large pretrained language models (LM) have
revolutionized the field of natural language processing (NLP). However, while
pretraining on general language has been shown to work very well for common
language, it has been observed that niche language poses problems. In
particular, climate-related texts include specific language that common LMs can
not represent accurately. We argue that this shortcoming of today's LMs limits
the applicability of modern NLP to the broad field of text processing of
climate-related texts. As a remedy, we propose ClimateBert, a transformer-based
language model that is further pretrained on over 1.6 million paragraphs of
climate-related texts, crawled from various sources such as common news,
research articles, and climate reporting of companies. We find that
ClimateBertleads to a 46% improvement on a masked language model objective
which, in turn, leads to lowering error rates by 3.57% to 35.71% for various
climate-related downstream tasks like text classification, sentiment analysis,
and fact-checking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PhoMT: A High-Quality and Large-Scale Benchmark Dataset for Vietnamese-English Machine Translation. (arXiv:2110.12199v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12199">
<div class="article-summary-box-inner">
<span><p>We introduce a high-quality and large-scale Vietnamese-English parallel
dataset of 3.02M sentence pairs, which is 2.9M pairs larger than the benchmark
Vietnamese-English machine translation corpus IWSLT15. We conduct experiments
comparing strong neural baselines and well-known automatic translation engines
on our dataset and find that in both automatic and human evaluations: the best
performance is obtained by fine-tuning the pre-trained sequence-to-sequence
denoising auto-encoder mBART. To our best knowledge, this is the first
large-scale Vietnamese-English machine translation study. We hope our publicly
available dataset and study can serve as a starting point for future research
and applications on Vietnamese-English machine translation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hate and Offensive Speech Detection in Hindi and Marathi. (arXiv:2110.12200v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12200">
<div class="article-summary-box-inner">
<span><p>Sentiment analysis is the most basic NLP task to determine the polarity of
text data. There has been a significant amount of work in the area of
multilingual text as well. Still hate and offensive speech detection faces a
challenge due to inadequate availability of data, especially for Indian
languages like Hindi and Marathi. In this work, we consider hate and offensive
speech detection in Hindi and Marathi texts. The problem is formulated as a
text classification task using the state of the art deep learning approaches.
We explore different deep learning architectures like CNN, LSTM, and variations
of BERT like multilingual BERT, IndicBERT, and monolingual RoBERTa. The basic
models based on CNN and LSTM are augmented with fast text word embeddings. We
use the HASOC 2021 Hindi and Marathi hate speech datasets to compare these
algorithms. The Marathi dataset consists of binary labels and the Hindi dataset
consists of binary as well as more-fine grained labels. We show that the
transformer-based models perform the best and even the basic models along with
FastText embeddings give a competitive performance. Moreover, with normal
hyper-parameter tuning, the basic models perform better than BERT-based models
on the fine-grained Hindi dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spanish Legalese Language Model and Corpora. (arXiv:2110.12201v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12201">
<div class="article-summary-box-inner">
<span><p>There are many Language Models for the English language according to its
worldwide relevance. However, for the Spanish language, even if it is a widely
spoken language, there are very few Spanish Language Models which result to be
small and too general. Legal slang could be think of a Spanish variant on its
own as it is very complicated in vocabulary, semantics and phrase
understanding. For this work we gathered legal-domain corpora from different
sources, generated a model and evaluated against Spanish general domain tasks.
The model provides reasonable results in those tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PASTRIE: A Corpus of Prepositions Annotated with Supersense Tags in Reddit International English. (arXiv:2110.12243v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12243">
<div class="article-summary-box-inner">
<span><p>We present the Prepositions Annotated with Supersense Tags in Reddit
International English ("PASTRIE") corpus, a new dataset containing manually
annotated preposition supersenses of English data from presumed speakers of
four L1s: English, French, German, and Spanish. The annotations are
comprehensive, covering all preposition types and tokens in the sample. Along
with the corpus, we provide analysis of distributional patterns across the
included L1s and a discussion of the influence of L1s on L2 preposition choice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoVA: Context-aware Visual Attention for Webpage Information Extraction. (arXiv:2110.12320v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12320">
<div class="article-summary-box-inner">
<span><p>Webpage information extraction (WIE) is an important step to create knowledge
bases. For this, classical WIE methods leverage the Document Object Model (DOM)
tree of a website. However, use of the DOM tree poses significant challenges as
context and appearance are encoded in an abstract manner. To address this
challenge we propose to reformulate WIE as a context-aware Webpage Object
Detection task. Specifically, we develop a Context-aware Visual Attention-based
(CoVA) detection pipeline which combines appearance features with syntactical
structure from the DOM tree. To study the approach we collect a new large-scale
dataset of e-commerce websites for which we manually annotate every web element
with four labels: product price, product title, product image and background.
On this dataset we show that the proposed CoVA approach is a new challenging
baseline which improves upon prior state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chinese Traditional Poetry Generating System Based on Deep Learning. (arXiv:2110.12335v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12335">
<div class="article-summary-box-inner">
<span><p>Chinese traditional poetry is an important intangible cultural heritage of
China and an artistic carrier of thought, culture, spirit and emotion. However,
due to the strict rules of ancient poetry, it is very difficult to write poetry
by machine. This paper proposes an automatic generation method of Chinese
traditional poetry based on deep learning technology, which extracts keywords
from each poem and matches them with the previous text to make the poem conform
to the theme, and when a user inputs a paragraph of text, the machine obtains
the theme and generates poem sentence by sentence. Using the classic word2vec
model as the preprocessing model, the Chinese characters which are not
understood by the computer are transformed into matrix for processing.
Bi-directional Long Short-Term Memory is used as the neural network model to
generate Chinese characters one by one and make the meaning of Chinese
characters as accurate as possible. At the same time, TF-IDF and TextRank are
used to extract keywords. Using the attention mechanism based encoding-decoding
model, we can solve practical problems by transforming the model, and
strengthen the important information of long-distance information, so as to
grasp the key points without losing important information. In the aspect of
emotion judgment, Long Short-Term Memory network is used. The final result
shows that it can get good poetry outputs according to the user input text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scalable knowledge base completion with superposition memories. (arXiv:2110.12341v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12341">
<div class="article-summary-box-inner">
<span><p>We present Harmonic Memory Networks (HMem), a neural architecture for
knowledge base completion that models entities as weighted sums of pairwise
bindings between an entity's neighbors and corresponding relations. Since
entities are modeled as aggregated neighborhoods, representations of unseen
entities can be generated on the fly. We demonstrate this with two new
datasets: WNGen and FBGen. Experiments show that the model is SOTA on
benchmarks, and flexible enough to evolve without retraining as the knowledge
graph grows.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distributed neural encoding of binding to thematic roles. (arXiv:2110.12342v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12342">
<div class="article-summary-box-inner">
<span><p>A framework and method are proposed for the study of constituent composition
in fMRI. The method produces estimates of neural patterns encoding complex
linguistic structures, under the assumption that the contributions of
individual constituents are additive. Like usual techniques for modeling
compositional structure in fMRI, the proposed method employs pattern
superposition to synthesize complex structures from their parts. Unlike these
techniques, superpositions are sensitive to the structural positions of
constituents, making them irreducible to structure-indiscriminate
("bag-of-words") models of composition. Reanalyzing data from a study by
Frankland and Greene (2015), it is shown that comparison of neural predictive
models with differing specifications can illuminate aspects of neural
representational contents that are not apparent when composition is not
modelled. The results indicate that the neural instantiations of the binding of
fillers to thematic roles in a sentence are non-orthogonal, and therefore
spatially overlapping.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Think about it! Improving defeasible reasoning by first modeling the question scenario. (arXiv:2110.12349v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12349">
<div class="article-summary-box-inner">
<span><p>Defeasible reasoning is the mode of reasoning where conclusions can be
overturned by taking into account new evidence. Existing cognitive science
literature on defeasible reasoning suggests that a person forms a mental model
of the problem scenario before answering questions. Our research goal asks
whether neural models can similarly benefit from envisioning the question
scenario before answering a defeasible query. Our approach is, given a
question, to have a model first create a graph of relevant influences, and then
leverage that graph as an additional input when answering the question. Our
system, CURIOUS, achieves a new state-of-the-art on three different defeasible
reasoning datasets. This result is significant as it illustrates that
performance can be improved by guiding a system to "think about" a question and
explicitly model the scenario, rather than answering reflexively. Code, data,
and pre-trained models are located at https://github.com/madaan/thinkaboutit.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Team Enigma at ArgMining-EMNLP 2021: Leveraging Pre-trained Language Models for Key Point Matching. (arXiv:2110.12370v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12370">
<div class="article-summary-box-inner">
<span><p>We present the system description for our submission towards the Key Point
Analysis Shared Task at ArgMining 2021. Track 1 of the shared task requires
participants to develop methods to predict the match score between each pair of
arguments and keypoints, provided they belong to the same topic under the same
stance. We leveraged existing state of the art pre-trained language models
along with incorporating additional data and features extracted from the inputs
(topics, key points, and arguments) to improve performance. We were able to
achieve mAP strict and mAP relaxed score of 0.872 and 0.966 respectively in the
evaluation phase, securing 5th place on the leaderboard. In the post evaluation
phase, we achieved a mAP strict and mAP relaxed score of 0.921 and 0.982
respectively. All the codes to generate reproducible results on our models are
available on Github.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transliterating Kurdish texts in Latin into Persian-Arabic script. (arXiv:2110.12374v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12374">
<div class="article-summary-box-inner">
<span><p>Kurdish is written in different scripts. The two most popular scripts are
Latin and Persian-Arabic. However, not all Kurdish readers are familiar with
both mentioned scripts that could be resolved by automatic transliterators. So
far, the developed tools mostly transliterate Persian-Arabic scripts into
Latin. We present a transliterator to transliterate Kurdish texts in Latin into
Persian-Arabic script. We also discuss the issues that should be considered in
the transliteration process. The tool is a part of Kurdish BLARK, and it is
publicly available for non-commercial use
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Extraction of Sentencing Decisions from Court Cases in the Hebrew Language. (arXiv:2110.12383v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12383">
<div class="article-summary-box-inner">
<span><p>We present the task of Automated Punishment Extraction (APE) in sentencing
decisions from criminal court cases in Hebrew. Addressing APE will enable the
identification of sentencing patterns and constitute an important stepping
stone for many follow up legal NLP applications in Hebrew, including the
prediction of sentencing decisions. We curate a dataset of sexual assault
sentencing decisions and a manually-annotated evaluation dataset, and implement
rule-based and supervised models. We find that while supervised models can
identify the sentence containing the punishment with good accuracy, rule-based
approaches outperform them on the full APE task. We conclude by presenting a
first analysis of sentencing patterns in our dataset and analyze common models'
errors, indicating avenues for future work, such as distinguishing between
probation and actual imprisonment punishment. We will make all our resources
available upon request, including data, annotation, and first benchmark models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved Goal Oriented Dialogue via Utterance Generation and Look Ahead. (arXiv:2110.12412v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12412">
<div class="article-summary-box-inner">
<span><p>Goal oriented dialogue systems have become a prominent customer-care
interaction channel for most businesses. However, not all interactions are
smooth, and customer intent misunderstanding is a major cause of dialogue
failure. We show that intent prediction can be improved by training a deep
text-to-text neural model to generate successive user utterances from unlabeled
dialogue data. For that, we define a multi-task training regime that utilizes
successive user-utterance generation to improve the intent prediction. Our
approach achieves the reported improvement due to two complementary factors:
First, it uses a large amount of unlabeled dialogue data for an auxiliary
generation task. Second, it uses the generated user utterance as an additional
signal for the intent prediction model. Lastly, we present a novel look-ahead
approach that uses user utterance generation to improve intent prediction in
inference time. Specifically, we generate counterfactual successive user
utterances for conversations with ambiguous predicted intents, and disambiguate
the prediction by reassessing the concatenated sequence of available and
generated utterances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentence Punctuation for Collaborative Commentary Generation in Esports Live-Streaming. (arXiv:2110.12416v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12416">
<div class="article-summary-box-inner">
<span><p>To solve the existing sentence punctuation problem for collaborative
commentary generation in Esports live-streaming, this paper presents two
strategies for sentence punctuation for text sequences of game commentary, that
is, punctuating sentences by two or three text sequence(s) originally
punctuated by Youtube to obtain a complete sentence of commentary. We conducted
comparative experiments utilizing and fine-tuning a state-of-the-art
pre-trained generative language model among two strategies and the baseline to
generate collaborative commentary. Both objective evaluations by automatic
metrics and subjective analyses showed that our strategy of punctuating
sentences by two text sequences outperformed the baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Dialog Management: Recent Advances and Challenges. (arXiv:2005.02233v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.02233">
<div class="article-summary-box-inner">
<span><p>Dialog management (DM) is a crucial component in a task-oriented dialog
system. Given the dialog history, DM predicts the dialog state and decides the
next action that the dialog agent should take. Recently, dialog policy learning
has been widely formulated as a Reinforcement Learning (RL) problem, and more
works focus on the applicability of DM. In this paper, we survey recent
advances and challenges within three critical topics for DM: (1) improving
model scalability to facilitate dialog system modeling in new scenarios, (2)
dealing with the data scarcity problem for dialog policy learning, and (3)
enhancing the training efficiency to achieve better task-completion performance
. We believe that this survey can shed a light on future research in dialog
management.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controlled Analyses of Social Biases in Wikipedia Bios. (arXiv:2101.00078v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00078">
<div class="article-summary-box-inner">
<span><p>Social biases on Wikipedia, a widely-read global platform, could greatly
influence public opinion. While prior research has examined man/woman gender
bias in biography articles, possible influences of other demographic attributes
limit conclusions. In this work, we present a methodology for analyzing
Wikipedia pages about people that isolates dimensions of interest (e.g.,
gender), from other attributes (e.g., occupation). Given a target corpus for
analysis (e.g. biographies about women), we present a method for constructing a
comparison corpus that matches the target corpus in as many attributes as
possible, except the target one. We develop evaluation metrics to measure how
well the comparison corpus aligns with the target corpus and then examine how
articles about gender and racial minorities (cis. women, non-binary people,
transgender women, and transgender men; African American, Asian American, and
Hispanic/Latinx American people) differ from other articles. In addition to
identifying suspect social biases, our results show that failing to control for
covariates can result in different conclusions and veil biases. Our
contributions include methodology that facilitates further analyses of bias in
Wikipedia articles, findings that can aid Wikipedia editors in reducing biases,
and a framework and evaluation metrics to guide future work in this area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VeeAlign: Multifaceted Context Representation using Dual Attention for Ontology Alignment. (arXiv:2102.04081v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.04081">
<div class="article-summary-box-inner">
<span><p>Ontology Alignment is an important research problem applied to various fields
such as data integration, data transfer, data preparation, etc.
State-of-the-art (SOTA) Ontology Alignment systems typically use naive
domain-dependent approaches with handcrafted rules or domain-specific
architectures, making them unscalable and inefficient. In this work, we propose
VeeAlign, a Deep Learning based model that uses a novel dual-attention
mechanism to compute the contextualized representation of a concept which, in
turn, is used to discover alignments. By doing this, not only is our approach
able to exploit both syntactic and semantic information encoded in ontologies,
it is also, by design, flexible and scalable to different domains with minimal
effort. We evaluate our model on four different datasets from different domains
and languages, and establish its superiority through these results as well as
detailed ablation studies. The code and datasets used are available at
https://github.com/Remorax/VeeAlign.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">XeroAlign: Zero-Shot Cross-lingual Transformer Alignment. (arXiv:2105.02472v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.02472">
<div class="article-summary-box-inner">
<span><p>The introduction of pretrained cross-lingual language models brought decisive
improvements to multilingual NLP tasks. However, the lack of labelled task data
necessitates a variety of methods aiming to close the gap to high-resource
languages. Zero-shot methods in particular, often use translated task data as a
training signal to bridge the performance gap between the source and target
language(s). We introduce XeroAlign, a simple method for task-specific
alignment of cross-lingual pretrained transformers such as XLM-R. XeroAlign
uses translated task data to encourage the model to generate similar sentence
embeddings for different languages. The XeroAligned XLM-R, called XLM-RA, shows
strong improvements over the baseline models to achieve state-of-the-art
zero-shot results on three multilingual natural language understanding tasks.
XLM-RA's text classification accuracy exceeds that of XLM-R trained with
labelled data and performs on par with state-of-the-art models on a
cross-lingual adversarial paraphrasing task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Modal Generative Augmentation for Visual Question Answering. (arXiv:2105.04780v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.04780">
<div class="article-summary-box-inner">
<span><p>Data augmentation has been shown to effectively improve the performance of
multimodal machine learning models. This paper introduces a generative model
for data augmentation by leveraging the correlations among multiple modalities.
Different from conventional data augmentation approaches that apply low-level
operations with deterministic heuristics, our method learns a generator that
generates samples of the target modality conditioned on observed modalities in
the variational auto-encoder framework. Additionally, the proposed model is
able to quantify the confidence of augmented data by its generative
probability, and can be jointly optimised with a downstream task. Experiments
on Visual Question Answering as downstream task demonstrate the effectiveness
of the proposed generative model, which is able to improve strong UpDn-based
models to achieve state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Yes, BM25 is a Strong Baseline for Legal Case Retrieval. (arXiv:2105.05686v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.05686">
<div class="article-summary-box-inner">
<span><p>We describe our single submission to task 1 of COLIEE 2021. Our vanilla BM25
got second place, well above the median of submissions. Code is available at
https://github.com/neuralmind-ai/coliee.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A cost-benefit analysis of cross-lingual transfer methods. (arXiv:2105.06813v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.06813">
<div class="article-summary-box-inner">
<span><p>An effective method for cross-lingual transfer is to fine-tune a bilingual or
multilingual model on a supervised dataset in one language and evaluating it on
another language in a zero-shot manner. Translating examples at training time
or inference time are also viable alternatives. However, there are costs
associated with these methods that are rarely addressed in the literature. In
this work, we analyze cross-lingual methods in terms of their effectiveness
(e.g., accuracy), development and deployment costs, as well as their latencies
at inference time. Our experiments on three tasks indicate that the best
cross-lingual method is highly task-dependent. Finally, by combining zero-shot
and translation methods, we achieve the state-of-the-art in two of the three
datasets used in this work. Based on these results, we question the need for
manually labeled training data in a target language. Code, models and
translated datasets are available at
https://github.com/unicamp-dl/cross-lingual-analysis
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Specializing Multilingual Language Models: An Empirical Study. (arXiv:2106.09063v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.09063">
<div class="article-summary-box-inner">
<span><p>Pretrained multilingual language models have become a common tool in
transferring NLP capabilities to low-resource languages, often with
adaptations. In this work, we study the performance, extensibility, and
interaction of two such adaptations: vocabulary augmentation and script
transliteration. Our evaluations on part-of-speech tagging, universal
dependency parsing, and named entity recognition in nine diverse low-resource
languages uphold the viability of these approaches while raising new questions
around how to optimally adapt multilingual models to low-resource settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DUKweb: Diachronic word representations from the UK Web Archive corpus. (arXiv:2107.01076v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.01076">
<div class="article-summary-box-inner">
<span><p>Lexical semantic change (detecting shifts in the meaning and usage of words)
is an important task for social and cultural studies as well as for Natural
Language Processing applications. Diachronic word embeddings (time-sensitive
vector representations of words that preserve their meaning) have become the
standard resource for this task. However, given the significant computational
resources needed for their generation, very few resources exist that make
diachronic word embeddings available to the scientific community.
</p>
<p>In this paper we present DUKweb, a set of large-scale resources designed for
the diachronic analysis of contemporary English. DUKweb was created from the
JISC UK Web Domain Dataset (1996-2013), a very large archive which collects
resources from the Internet Archive that were hosted on domains ending in
`.uk'. DUKweb consists of a series word co-occurrence matrices and two types of
word embeddings for each year in the JISC UK Web Domain dataset. We show the
reuse potential of DUKweb and its quality standards via a case study on word
meaning change detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EchoEA: Echo Information between Entities and Relations for Entity Alignment. (arXiv:2107.03054v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.03054">
<div class="article-summary-box-inner">
<span><p>Entity alignment (EA) plays an important role in automatically integrating
knowledge graphs (KGs) from multiple sources. Recent approaches based on Graph
Neural Network (GNN) obtain entity representation from relation information and
have achieved promising results. Besides, more and more methods introduce
semi-supervision to ask for more labeled training data. However, two challenges
still exist in GNN-based EA methods: (1) Deeper GNN Encoder: The GNN encoder of
current methods has limited depth (usually 2-layers). (2) Low-quality
Bootstrapping: The generated semi-supervised data is of low quality. In this
paper, we propose a novel framework, Echo Entity Alignment (EchoEA), which
leverages 4-levels self-attention mechanism to spread entity information to
relations and echo back to entities. Furthermore, we propose attribute-combined
bi-directional global-filtered strategy (ABGS) to improve bootstrapping, reduce
false samples and generate high-quality training data. The experimental results
on three real-world cross-lingual datasets are stable at around 96\% at hits@1
on average, showing that our approach not only significantly outperforms the
state-of-the-art GNN-based methods, but also is universal and transferable for
existing EA methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Dataset for Answering Time-Sensitive Questions. (arXiv:2108.06314v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06314">
<div class="article-summary-box-inner">
<span><p>Time is an important dimension in our physical world. Lots of facts can
evolve with respect to time. For example, the U.S. President might change every
four years. Therefore, it is important to consider the time dimension and
empower the existing QA models to reason over time. However, the existing QA
datasets contain rather few time-sensitive questions, hence not suitable for
diagnosing or benchmarking the model's temporal reasoning capability. In order
to promote research in this direction, we propose to construct a time-sensitive
QA dataset. The dataset is constructed by 1) mining time-evolving facts from
WikiData and aligning them to their corresponding Wikipedia page, 2) employing
crowd workers to verify and calibrate these noisy facts, 3) generating
question-answer pairs based on the annotated time-sensitive facts. Our dataset
poses challenges in the aspect of both temporal understanding and temporal
reasoning. We evaluate different SoTA long-document QA systems like BigBird and
FiD on our dataset. The best-performing model FiD can only achieve 46\%
accuracy, still far behind the human performance of 87\%. We demonstrate that
these models are still lacking the ability to perform consistent temporal
reasoning. Therefore, we believe that our dataset could serve as a benchmark to
develop NLP models more sensitive to temporal shifts. The dataset and code are
released in~\url{https://github.com/wenhuchen/Time-Sensitive-QA}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">mMARCO: A Multilingual Version of the MS MARCO Passage Ranking Dataset. (arXiv:2108.13897v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13897">
<div class="article-summary-box-inner">
<span><p>The MS MARCO ranking dataset has been widely used for training deep learning
models for IR tasks, achieving considerable effectiveness on diverse zero-shot
scenarios. However, this type of resource is scarce in other languages than
English. In this work we present mMARCO, a multilingual version of the MS MARCO
passage ranking dataset comprising 8 languages that was created using machine
translation. We evaluated mMARCO by fine-tuning mono and multilingual
re-ranking models on it. Experimental results demonstrate that multilingual
models fine-tuned on our translated dataset achieve superior effectiveness than
models fine-tuned on the original English version alone. Also, our distilled
multilingual re-ranker is competitive with non-distilled models while having
5.4 times fewer parameters. The translated datasets as well as fine-tuned
models are available at https://github.com/unicamp-dl/mMARCO.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the ability of monolingual models to learn language-agnostic representations. (arXiv:2109.01942v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01942">
<div class="article-summary-box-inner">
<span><p>Pretrained multilingual models have become a de facto default approach for
zero-shot cross-lingual transfer. Previous work has shown that these models are
able to achieve cross-lingual representations when pretrained on two or more
languages with shared parameters. In this work, we provide evidence that a
model can achieve language-agnostic representations even when pretrained on a
single language. That is, we find that monolingual models pretrained and
finetuned on different languages achieve competitive performance compared to
the ones that use the same target language. Surprisingly, the models show a
similar performance on a same task regardless of the pretraining language. For
example, models pretrained on distant languages such as German and Portuguese
perform similarly on English tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exposing Length Divergence Bias of Textual Matching Models. (arXiv:2109.02431v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02431">
<div class="article-summary-box-inner">
<span><p>Despite the remarkable success deep models have achieved in Textual Matching
(TM), their robustness issue is still a topic of concern. In this work, we
propose a new perspective to study this issue -- via the length divergence bias
of TM models. We conclude that this bias stems from two parts: the label bias
of existing TM datasets and the sensitivity of TM models to superficial
information. We critically examine widely used TM datasets, and find that all
of them follow specific length divergence distributions by labels, providing
direct cues for predictions. As for the TM models, we conduct adversarial
evaluation and show that all models' performances drop on the
out-of-distribution adversarial test sets we construct, which demonstrates that
they are all misled by biased training sets. This is also confirmed by the
\textit{SentLen} probing task that all models capture rich length information
during training to facilitate their performances. Finally, to alleviate the
length divergence bias in TM models, we propose a practical adversarial
training method using bias-free training data. Our experiments indicate that we
successfully improve the robustness and generalization ability of models at the
same time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Filling the Gaps in Ancient Akkadian Texts: A Masked Language Modelling Approach. (arXiv:2109.04513v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04513">
<div class="article-summary-box-inner">
<span><p>We present models which complete missing text given transliterations of
ancient Mesopotamian documents, originally written on cuneiform clay tablets
(2500 BCE - 100 CE). Due to the tablets' deterioration, scholars often rely on
contextual cues to manually fill in missing parts in the text in a subjective
and time-consuming process. We identify that this challenge can be formulated
as a masked language modelling task, used mostly as a pretraining objective for
contextualized language models. Following, we develop several architectures
focusing on the Akkadian language, the lingua franca of the time. We find that
despite data scarcity (1M tokens) we can achieve state of the art performance
on missing tokens prediction (89% hit@5) using a greedy decoding scheme and
pretraining on data from other languages and different time periods. Finally,
we conduct human evaluations showing the applicability of our models in
assisting experts to transcribe texts in extinct languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Task Difficulty for Few-Shot Relation Extraction. (arXiv:2109.05473v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05473">
<div class="article-summary-box-inner">
<span><p>Few-shot relation extraction (FSRE) focuses on recognizing novel relations by
learning with merely a handful of annotated instances. Meta-learning has been
widely adopted for such a task, which trains on randomly generated few-shot
tasks to learn generic data representations. Despite impressive results
achieved, existing models still perform suboptimally when handling hard FSRE
tasks, where the relations are fine-grained and similar to each other. We argue
this is largely because existing models do not distinguish hard tasks from easy
ones in the learning process. In this paper, we introduce a novel approach
based on contrastive learning that learns better representations by exploiting
relation label information. We further design a method that allows the model to
adaptively learn how to focus on hard tasks. Experiments on two standard
datasets demonstrate the effectiveness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models as a Knowledge Source for Cognitive Agents. (arXiv:2109.08270v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08270">
<div class="article-summary-box-inner">
<span><p>Language models (LMs) are sentence-completion engines trained on massive
corpora. LMs have emerged as a significant breakthrough in natural-language
processing, providing capabilities that go far beyond sentence completion
including question answering, summarization, and natural-language inference.
While many of these capabilities have potential application to cognitive
systems, exploiting language models as a source of task knowledge, especially
for task learning, offers significant, near-term benefits. We introduce
language models and the various tasks to which they have been applied and then
review methods of knowledge extraction from language models. The resulting
analysis outlines both the challenges and opportunities for using language
models as a new knowledge source for cognitive systems. It also identifies
possible ways to improve knowledge extraction from language models using the
capabilities provided by cognitive systems. Central to success will be the
ability of a cognitive agent to itself learn an abstract model of the knowledge
implicit in the LM as well as methods to extract high-quality knowledge
effectively and efficiently. To illustrate, we introduce a hypothetical robot
agent and describe how language models could extend its task knowledge and
improve its performance and the kinds of knowledge and methods the agent can
use to exploit the knowledge within a language model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AraT5: Text-to-Text Transformers for Arabic Language Understanding and Generation. (arXiv:2109.12068v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12068">
<div class="article-summary-box-inner">
<span><p>Transfer learning with a unified Transformer framework (T5) that converts all
language problems into a text-to-text format has recently been proposed as a
simple, yet effective, transfer learning approach. Although a multilingual
version of the T5 model (mT5) has been introduced, it is not clear how well it
can fare on non-English tasks involving diverse data. To investigate this
question, we apply mT5 on a language with a wide variety of dialects--Arabic.
For evaluation, we use an existing benchmark for Arabic language understanding
and introduce a new benchmark for Arabic language generation (ARGEN). We also
pre-train three powerful Arabic-specific text-to-text Transformer based models
and evaluate them on the two benchmarks. Our new models perform significantly
better than mT5 and exceed MARBERT, the current state-of-the-art Arabic
BERT-based model, on Arabic language understanding. The models also set new
SOTA on the generation benchmark. Our new models and are publicly released at
https://github.com/UBC-NLP/araT5 and ARGEN will be released through the same
repository.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LawSum: A weakly supervised approach for Indian Legal Document Summarization. (arXiv:2110.01188v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01188">
<div class="article-summary-box-inner">
<span><p>Unlike the courts in western countries, public records of Indian judiciary
are completely unstructured and noisy. No large scale publicly available
annotated datasets of Indian legal documents exist till date. This limits the
scope for legal analytics research. In this work, we propose a new dataset
consisting of over 10,000 judgements delivered by the supreme court of India
and their corresponding hand written summaries. The proposed dataset is
pre-processed by normalising common legal abbreviations, handling spelling
variations in named entities, handling bad punctuations and accurate sentence
tokenization. Each sentence is tagged with their rhetorical roles. We also
annotate each judgement with several attributes like date, names of the
plaintiffs, defendants and the people representing them, judges who delivered
the judgement, acts/statutes that are cited and the most common citations used
to refer the judgement. Further, we propose an automatic labelling technique
for identifying sentences which have summary worthy information. We demonstrate
that this auto labeled data can be used effectively to train a weakly
supervised sentence extractor with high accuracy. Some possible applications of
this dataset besides legal document summarization can be in retrieval, citation
analysis and prediction of decisions by a particular judge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining. (arXiv:2110.03888v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03888">
<div class="article-summary-box-inner">
<span><p>Recent expeditious developments in deep learning algorithms, distributed
training, and even hardware design for large models have enabled training
extreme-scale models, say GPT-3 and Switch Transformer possessing hundreds of
billions or even trillions of parameters. However, under limited resources,
extreme-scale model training that requires enormous amounts of computes and
memory footprint suffers from frustratingly low efficiency in model
convergence. In this paper, we propose a simple training strategy called
"Pseudo-to-Real" for high-memory-footprint-required large models.
Pseudo-to-Real is compatible with large models with architecture of sequential
layers. We demonstrate a practice of pretraining unprecedented
10-trillion-parameter model, an order of magnitude larger than the
state-of-the-art, on solely 512 GPUs within 10 days. Besides demonstrating the
application of Pseudo-to-Real, we also provide a technique, Granular CPU
offloading, to manage CPU memory for training large model and maintain high GPU
utilities. Fast training of extreme-scale models on a decent amount of
resources can bring much smaller carbon footprint and contribute to greener AI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Inductive Bias of In-Context Learning: Rethinking Pretraining Example Design. (arXiv:2110.04541v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04541">
<div class="article-summary-box-inner">
<span><p>Pretraining Neural Language Models (NLMs) over a large corpus involves
chunking the text into training examples, which are contiguous text segments of
sizes processable by the neural architecture. We highlight a bias introduced by
this common practice: we prove that the pretrained NLM can model much stronger
dependencies between text segments that appeared in the same training example,
than it can between text segments that appeared in different training examples.
This intuitive result has a twofold role. First, it formalizes the motivation
behind a broad line of recent successful NLM training heuristics, proposed for
the pretraining and fine-tuning stages, which do not necessarily appear related
at first glance. Second, our result clearly indicates further improvements to
be made in NLM pretraining for the benefit of Natural Language Understanding
tasks. As an example, we propose "kNN-Pretraining": we show that including
semantically related non-neighboring sentences in the same pretraining example
yields improved sentence representations and open domain question answering
abilities. This theoretically motivated degree of freedom for "pretraining
example design" indicates new training schemes for self-improving
representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Time Masking for Temporal Language Models. (arXiv:2110.06366v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06366">
<div class="article-summary-box-inner">
<span><p>Our world is constantly evolving, and so is the content on the web.
Consequently, our languages, often said to mirror the world, are dynamic in
nature. However, most current contextual language models are static and cannot
adapt to changes over time. In this work, we propose a temporal contextual
language model called TempoBERT, which uses time as an additional context of
texts. Our technique is based on modifying texts with temporal information and
performing time masking - specific masking for the supplementary time
information. We leverage our approach for the tasks of semantic change
detection and sentence time prediction, experimenting on diverse datasets in
terms of time, size, genre, and language. Our extensive evaluation shows that
both tasks benefit from exploiting time masking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt-tuning in ASR systems for efficient domain-adaptation. (arXiv:2110.06502v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06502">
<div class="article-summary-box-inner">
<span><p>Automatic Speech Recognition (ASR) systems have found their use in numerous
industrial applications in very diverse domains. Since domain-specific systems
perform better than their generic counterparts on in-domain evaluation, the
need for memory and compute-efficient domain adaptation is obvious.
Particularly, adapting parameter-heavy transformer-based language models used
for rescoring ASR hypothesis is challenging. In this work, we overcome the
problem using prompt-tuning, a methodology that trains a small number of domain
token embedding parameters to prime a transformer-based LM to a particular
domain. With just a handful of extra parameters per domain, we achieve much
better perplexity scores over the baseline of using an unadapted LM. Despite
being parameter-efficient, these improvements are comparable to those of
fully-fine-tuned models with hundreds of millions of parameters. We replicate
our findings in perplexity numbers to Word Error Rate in a domain-specific ASR
system for one such domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Transfer Learning & Beyond: Transformer Language Models in Information Systems Research. (arXiv:2110.08975v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08975">
<div class="article-summary-box-inner">
<span><p>AI is widely thought to be poised to transform business, yet current
perceptions of the scope of this transformation may be myopic. Recent progress
in natural language processing involving transformer language models (TLMs)
offers a potential avenue for AI-driven business and societal transformation
that is beyond the scope of what most currently foresee. We review this recent
progress as well as recent literature utilizing text mining in top IS journals
to develop an outline for how future IS research can benefit from these new
techniques. Our review of existing IS literature reveals that suboptimal text
mining techniques are prevalent and that the more advanced TLMs could be
applied to enhance and increase IS research involving text data, and to enable
new IS research topics, thus creating more value for the research community.
This is possible because these techniques make it easier to develop very
powerful custom systems and their performance is superior to existing methods
for a wide range of tasks and applications. Further, multilingual language
models make possible higher quality text analytics for research in multiple
languages. We also identify new avenues for IS research, like language user
interfaces, that may offer even greater potential for future IS research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Aspect-guided Explanation Generation for Explainable Recommendation. (arXiv:2110.10358v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10358">
<div class="article-summary-box-inner">
<span><p>Explainable recommendation systems provide explanations for recommendation
results to improve their transparency and persuasiveness. The existing
explainable recommendation methods generate textual explanations without
explicitly considering the user's preferences on different aspects of the item.
In this paper, we propose a novel explanation generation framework, named
Hierarchical Aspect-guided explanation Generation (HAG), for explainable
recommendation. Specifically, HAG employs a review-based syntax graph to
provide a unified view of the user/item details. An aspect-guided graph pooling
operator is proposed to extract the aspect-relevant information from the
review-based syntax graphs to model the user's preferences on an item at the
aspect level. Then, a hierarchical explanation decoder is developed to generate
aspects and aspect-relevant explanations based on the attention mechanism. The
experimental results on three real datasets indicate that HAG outperforms
state-of-the-art explanation generation methods in both single-aspect and
multi-aspect explanation generation tasks, and also achieves comparable or even
better preference prediction accuracy than strong baseline methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpreting Deep Learning Models in Natural Language Processing: A Review. (arXiv:2110.10470v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10470">
<div class="article-summary-box-inner">
<span><p>Neural network models have achieved state-of-the-art performances in a wide
range of natural language processing (NLP) tasks. However, a long-standing
criticism against neural network models is the lack of interpretability, which
not only reduces the reliability of neural NLP systems but also limits the
scope of their applications in areas where interpretability is essential (e.g.,
health care applications). In response, the increasing interest in interpreting
neural NLP models has spurred a diverse array of interpretation methods over
recent years. In this survey, we provide a comprehensive review of various
interpretation methods for neural models in NLP. We first stretch out a
high-level taxonomy for interpretation methods in NLP, i.e., training-based
approaches, test-based approaches, and hybrid approaches. Next, we describe
sub-categories in each category in detail, e.g., influence-function based
methods, KNN-based methods, attention-based models, saliency-based methods,
perturbation-based methods, etc. We point out deficiencies of current methods
and suggest some avenues for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SciCap: Generating Captions for Scientific Figures. (arXiv:2110.11624v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11624">
<div class="article-summary-box-inner">
<span><p>Researchers use figures to communicate rich, complex information in
scientific papers. The captions of these figures are critical to conveying
effective messages. However, low-quality figure captions commonly occur in
scientific articles and may decrease understanding. In this paper, we propose
an end-to-end neural framework to automatically generate informative,
high-quality captions for scientific figures. To this end, we introduce SCICAP,
a large-scale figure-caption dataset based on computer science arXiv papers
published between 2010 and 2020. After pre-processing - including figure-type
classification, sub-figure identification, text normalization, and caption text
selection - SCICAP contained more than two million figures extracted from over
290,000 papers. We then established baseline models that caption graph plots,
the dominant (19.2%) figure type. The experimental results showed both
opportunities and steep challenges of generating captions for scientific
figures.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">PhotoWCT$^2$: Compact Autoencoder for Photorealistic Style Transfer Resulting from Blockwise Training and Skip Connections of High-Frequency Residuals. (arXiv:2110.11995v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11995">
<div class="article-summary-box-inner">
<span><p>Photorealistic style transfer is an image editing task with the goal to
modify an image to match the style of another image while ensuring the result
looks like a real photograph. A limitation of existing models is that they have
many parameters, which in turn prevents their use for larger image resolutions
and leads to slower run-times. We introduce two mechanisms that enable our
design of a more compact model that we call PhotoWCT$^2$, which preserves
state-of-art stylization strength and photorealism. First, we introduce
blockwise training to perform coarse-to-fine feature transformations that
enable state-of-art stylization strength in a single autoencoder in place of
the inefficient cascade of four autoencoders used in PhotoWCT. Second, we
introduce skip connections of high-frequency residuals in order to preserve
image quality when applying the sequential coarse-to-fine feature
transformations. Our PhotoWCT$^2$ model requires fewer parameters (e.g., 30.3\%
fewer) while supporting higher resolution images (e.g., 4K) and achieving
faster stylization than existing models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Supervised Semantic Segmentation of Vessel Images using Leaking Perturbations. (arXiv:2110.11998v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11998">
<div class="article-summary-box-inner">
<span><p>Semantic segmentation based on deep learning methods can attain appealing
accuracy provided large amounts of annotated samples. However, it remains a
challenging task when only limited labelled data are available, which is
especially common in medical imaging. In this paper, we propose to use Leaking
GAN, a GAN-based semi-supervised architecture for retina vessel semantic
segmentation. Our key idea is to pollute the discriminator by leaking
information from the generator. This leads to more moderate generations that
benefit the training of GAN. As a result, the unlabelled examples can be better
utilized to boost the learning of the discriminator, which eventually leads to
stronger classification performance. In addition, to overcome the variations in
medical images, the mean-teacher mechanism is utilized as an auxiliary
regularization of the discriminator. Further, we modify the focal loss to fit
it as the consistency objective for mean-teacher regularizer. Extensive
experiments demonstrate that the Leaking GAN framework achieves competitive
performance compared to the state-of-the-art methods when evaluated on
benchmark datasets including DRIVE, STARE and CHASE\_DB1, using as few as 8
labelled images in the semi-supervised setting. It also outperforms existing
algorithms on cross-domain segmentation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When to Prune? A Policy towards Early Structural Pruning. (arXiv:2110.12007v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12007">
<div class="article-summary-box-inner">
<span><p>Pruning enables appealing reductions in network memory footprint and time
complexity. Conventional post-training pruning techniques lean towards
efficient inference while overlooking the heavy computation for training.
Recent exploration of pre-training pruning at initialization hints on training
cost reduction via pruning, but suffers noticeable performance degradation. We
attempt to combine the benefits of both directions and propose a policy that
prunes as early as possible during training without hurting performance.
Instead of pruning at initialization, our method exploits initial dense
training for few epochs to quickly guide the architecture, while constantly
evaluating dominant sub-networks via neuron importance ranking. This unveils
dominant sub-networks whose structures turn stable, allowing conventional
pruning to be pushed earlier into the training. To do this early, we further
introduce an Early Pruning Indicator (EPI) that relies on sub-network
architectural similarity and quickly triggers pruning when the sub-network's
architecture stabilizes. Through extensive experiments on ImageNet, we show
that EPI empowers a quick tracking of early training epochs suitable for
pruning, offering same efficacy as an otherwise ``oracle'' grid-search that
scans through epochs and requires orders of magnitude more compute. Our method
yields $1.4\%$ top-1 accuracy boost over state-of-the-art pruning counterparts,
cuts down training cost on GPU by $2.4\times$, hence offers a new
efficiency-accuracy boundary for network pruning during training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Local-Global Associative Frame Assemble in Video Re-ID. (arXiv:2110.12018v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12018">
<div class="article-summary-box-inner">
<span><p>Noisy and unrepresentative frames in automatically generated object bounding
boxes from video sequences cause significant challenges in learning
discriminative representations in video re-identification (Re-ID). Most
existing methods tackle this problem by assessing the importance of video
frames according to either their local part alignments or global appearance
correlations separately. However, given the diverse and unknown sources of
noise which usually co-exist in captured video data, existing methods have not
been effective satisfactorily. In this work, we explore jointly both local
alignments and global correlations with further consideration of their mutual
promotion/reinforcement so to better assemble complementary discriminative
Re-ID information within all the relevant frames in video tracklets.
Specifically, we concurrently optimise a local aligned quality (LAQ) module
that distinguishes the quality of each frame based on local alignments, and a
global correlated quality (GCQ) module that estimates global appearance
correlations. With the help of a local-assembled global appearance prototype,
we associate LAQ and GCQ to exploit their mutual complement. Extensive
experiments demonstrate the superiority of the proposed model against
state-of-the-art methods on five Re-ID benchmarks, including MARS, Duke-Video,
Duke-SI, iLIDS-VID, and PRID2011.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Prototype-Oriented Framework for Unsupervised Domain Adaptation. (arXiv:2110.12024v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12024">
<div class="article-summary-box-inner">
<span><p>Existing methods for unsupervised domain adaptation often rely on minimizing
some statistical distance between the source and target samples in the latent
space. To avoid the sampling variability, class imbalance, and data-privacy
concerns that often plague these methods, we instead provide a memory and
computation-efficient probabilistic framework to extract class prototypes and
align the target features with them. We demonstrate the general applicability
of our method on a wide range of scenarios, including single-source,
multi-source, class-imbalance, and source-private domain adaptation. Requiring
no additional model parameters and having a moderate increase in computation
over the source model alone, the proposed method achieves competitive
performance with state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simple Baseline for Low-Budget Active Learning. (arXiv:2110.12033v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12033">
<div class="article-summary-box-inner">
<span><p>Active learning focuses on choosing a subset of unlabeled data to be labeled.
However, most such methods assume that a large subset of the data can be
annotated. We are interested in low-budget active learning where only a small
subset (e.g., 0.2% of ImageNet) can be annotated. Instead of proposing a new
query strategy to iteratively sample batches of unlabeled data given an initial
pool, we learn rich features by an off-the-shelf self-supervised learning
method only once and then study the effectiveness of different sampling
strategies given a low budget on a variety of datasets as well as ImageNet
dataset. We show that although the state-of-the-art active learning methods
work well given a large budget of data labeling, a simple k-means clustering
algorithm can outperform them on low budgets. We believe this method can be
used as a simple baseline for low-budget active learning on image
classification. Code is available at:
https://github.com/UCDvision/low-budget-al
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Adversarial Networks for Non-Raytraced Global Illumination on Older GPU Hardware. (arXiv:2110.12039v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12039">
<div class="article-summary-box-inner">
<span><p>We give an overview of the different rendering methods and we demonstrate
that the use of a Generative Adversarial Networks (GAN) for Global Illumination
(GI) gives a superior quality rendered image to that of a rasterisations image.
We utilise the Pix2Pix architecture and specify the hyper-parameters and
methodology used to mimic ray-traced images from a set of input features. We
also demonstrate that the GANs quality is comparable to the quality of the
ray-traced images, but is able to produce the image, at a fraction of the time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Development of Semantic Web-based Imaging Database for Biological Morphome. (arXiv:2110.12058v1 [q-bio.QM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12058">
<div class="article-summary-box-inner">
<span><p>We introduce the RIKEN Microstructural Imaging Metadatabase, a semantic
web-based imaging database in which image metadata are described using the
Resource Description Framework (RDF) and detailed biological properties
observed in the images can be represented as Linked Open Data. The metadata are
used to develop a large-scale imaging viewer that provides a straightforward
graphical user interface to visualise a large microstructural tiling image at
the gigabyte level. We applied the database to accumulate comprehensive
microstructural imaging data produced by automated scanning electron
microscopy. As a result, we have successfully managed vast numbers of images
and their metadata, including the interpretation of morphological phenotypes
occurring in sub-cellular components and biosamples captured in the images. We
also discuss advanced utilisation of morphological imaging data that can be
promoted by this database.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CD&S Dataset: Handheld Imagery Dataset Acquired Under Field Conditions for Corn Disease Identification and Severity Estimation. (arXiv:2110.12084v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12084">
<div class="article-summary-box-inner">
<span><p>Accurate disease identification and its severity estimation is an important
consideration for disease management. Deep learning-based solutions for disease
management using imagery datasets are being increasingly explored by the
research community. However, most reported studies have relied on imagery
datasets that were acquired under controlled lab conditions. As a result, such
models lacked the ability to identify diseases in the field. Therefore, to
train a robust deep learning model for field use, an imagery dataset was
created using raw images acquired under field conditions using a handheld
sensor and augmented images with varying backgrounds. The Corn Disease and
Severity (CD&amp;S) dataset consisted of 511, 524, and 562, field acquired raw
images, corresponding to three common foliar corn diseases, namely Northern
Leaf Blight (NLB), Gray Leaf Spot (GLS), and Northern Leaf Spot (NLS),
respectively. For training disease identification models, half of the imagery
data for each disease was annotated using bounding boxes and also used to
generate 2343 additional images through augmentation using three different
backgrounds. For severity estimation, an additional 515 raw images for NLS were
acquired and categorized into severity classes ranging from 1 (resistant) to 5
(susceptible). Overall, the CD&amp;S dataset consisted of 4455 total images
comprising of 2112 field images and 2343 augmented images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Circle Representation for Medical Object Detection. (arXiv:2110.12093v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12093">
<div class="article-summary-box-inner">
<span><p>Box representation has been extensively used for object detection in computer
vision. Such representation is efficacious but not necessarily optimized for
biomedical objects (e.g., glomeruli), which play an essential role in renal
pathology. In this paper, we propose a simple circle representation for medical
object detection and introduce CircleNet, an anchor-free detection framework.
Compared with the conventional bounding box representation, the proposed
bounding circle representation innovates in three-fold: (1) it is optimized for
ball-shaped biomedical objects; (2) The circle representation reduced the
degree of freedom compared with box representation; (3) It is naturally more
rotation invariant. When detecting glomeruli and nuclei on pathological images,
the proposed circle representation achieved superior detection performance and
be more rotation-invariant, compared with the bounding box. The code has been
made publicly available: https://github.com/hrlblab/CircleNet
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MTGLS: Multi-Task Gaze Estimation with Limited Supervision. (arXiv:2110.12100v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12100">
<div class="article-summary-box-inner">
<span><p>Robust gaze estimation is a challenging task, even for deep CNNs, due to the
non-availability of large-scale labeled data. Moreover, gaze annotation is a
time-consuming process and requires specialized hardware setups. We propose
MTGLS: a Multi-Task Gaze estimation framework with Limited Supervision, which
leverages abundantly available non-annotated facial image data. MTGLS distills
knowledge from off-the-shelf facial image analysis models, and learns strong
feature representations of human eyes, guided by three complementary auxiliary
signals: (a) the line of sight of the pupil (i.e. pseudo-gaze) defined by the
localized facial landmarks, (b) the head-pose given by Euler angles, and (c)
the orientation of the eye patch (left/right eye). To overcome inherent noise
in the supervisory signals, MTGLS further incorporates a noise distribution
modelling approach. Our experimental results show that MTGLS learns highly
generalized representations which consistently perform well on a range of
datasets. Our proposed framework outperforms the unsupervised state-of-the-art
on CAVE (by 6.43%) and even supervised state-of-the-art methods on Gaze360 (by
6.59%) datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ConformalLayers: A non-linear sequential neural network with associative layers. (arXiv:2110.12108v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12108">
<div class="article-summary-box-inner">
<span><p>Convolutional Neural Networks (CNNs) have been widely applied. But as the
CNNs grow, the number of arithmetic operations and memory footprint also
increase. Furthermore, typical non-linear activation functions do not allow
associativity of the operations encoded by consecutive layers, preventing the
simplification of intermediate steps by combining them. We present a new
activation function that allows associativity between sequential layers of
CNNs. Even though our activation function is non-linear, it can be represented
by a sequence of linear operations in the conformal model for Euclidean
geometry. In this domain, operations like, but not limited to, convolution,
average pooling, and dropout remain linear. We take advantage of associativity
to combine all the "conformal layers" and make the cost of inference constant
regardless of the depth of the network.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dense Dual-Attention Network for Light Field Image Super-Resolution. (arXiv:2110.12114v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12114">
<div class="article-summary-box-inner">
<span><p>Light field (LF) images can be used to improve the performance of image
super-resolution (SR) because both angular and spatial information is
available. It is challenging to incorporate distinctive information from
different views for LF image SR. Moreover, the long-term information from the
previous layers can be weakened as the depth of network increases. In this
paper, we propose a dense dual-attention network for LF image SR. Specifically,
we design a view attention module to adaptively capture discriminative features
across different views and a channel attention module to selectively focus on
informative information across all channels. These two modules are fed to two
branches and stacked separately in a chain structure for adaptive fusion of
hierarchical features and distillation of valid information. Meanwhile, a dense
connection is used to fully exploit multi-level information. Extensive
experiments demonstrate that our dense dual-attention mechanism can capture
informative information across views and channels to improve SR performance.
Comparative results show the advantage of our method over state-of-the-art
methods on public datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RCNet: Reverse Feature Pyramid and Cross-scale Shift Network for Object Detection. (arXiv:2110.12130v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12130">
<div class="article-summary-box-inner">
<span><p>Feature pyramid networks (FPN) are widely exploited for multi-scale feature
fusion in existing advanced object detection frameworks. Numerous previous
works have developed various structures for bidirectional feature fusion, all
of which are shown to improve the detection performance effectively. We observe
that these complicated network structures require feature pyramids to be
stacked in a fixed order, which introduces longer pipelines and reduces the
inference speed. Moreover, semantics from non-adjacent levels are diluted in
the feature pyramid since only features at adjacent pyramid levels are merged
by the local fusion operation in a sequence manner. To address these issues, we
propose a novel architecture named RCNet, which consists of Reverse Feature
Pyramid (RevFP) and Cross-scale Shift Network (CSN). RevFP utilizes local
bidirectional feature fusion to simplify the bidirectional pyramid inference
pipeline. CSN directly propagates representations to both adjacent and
non-adjacent levels to enable multi-scale features more correlative. Extensive
experiments on the MS COCO dataset demonstrate RCNet can consistently bring
significant improvements over both one-stage and two-stage detectors with
subtle extra computational overhead. In particular, RetinaNet is boosted to
40.2 AP, which is 3.7 points higher than baseline, by replacing FPN with our
proposed model. On COCO test-dev, RCNet can achieve very competitive
performance with a single-model single-scale 50.5 AP. Codes will be made
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Study of Multimodal Person Verification Using Audio-Visual-Thermal Data. (arXiv:2110.12136v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12136">
<div class="article-summary-box-inner">
<span><p>In this paper, we study an approach to multimodal person verification using
audio, visual, and thermal modalities. The combination of audio and visual
modalities has already been shown to be effective for robust person
verification. From this perspective, we investigate the impact of further
increasing the number of modalities by supplementing thermal images. In
particular, we implemented unimodal, bimodal, and trimodal verification systems
using the state-of-the-art deep learning architectures and compared their
performance under clean and noisy conditions. We also compared two popular
fusion approaches based on simple score averaging and soft attention mechanism.
The experiment conducted on the SpeakingFaces dataset demonstrates the
superiority of the trimodal verification system over both unimodal and bimodal
systems. To enable the reproducibility of the experiment and facilitate
research into multimodal person verification, we make our code, pretrained
models and preprocessed dataset freely available in our GitHub repository.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatio-Temporal Graph Complementary Scattering Networks. (arXiv:2110.12150v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12150">
<div class="article-summary-box-inner">
<span><p>Spatio-temporal graph signal analysis has a significant impact on a wide
range of applications, including hand/body pose action recognition. To achieve
effective analysis, spatio-temporal graph convolutional networks (ST-GCN)
leverage the powerful learning ability to achieve great empirical successes;
however, those methods need a huge amount of high-quality training data and
lack theoretical interpretation. To address this issue, the spatio-temporal
graph scattering transform (ST-GST) was proposed to put forth a theoretically
interpretable framework; however, the empirical performance of this approach is
constrainted by the fully mathematical design. To benefit from both sides, this
work proposes a novel complementary mechanism to organically combine the
spatio-temporal graph scattering transform and neural networks, resulting in
the proposed spatio-temporal graph complementary scattering networks (ST-GCSN).
The essence is to leverage the mathematically designed graph wavelets with
pruning techniques to cover major information and use trainable networks to
capture complementary information. The empirical experiments on hand pose
action recognition show that the proposed ST-GCSN outperforms both ST-GCN and
ST-GST.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spectrum-to-Kernel Translation for Accurate Blind Image Super-Resolution. (arXiv:2110.12151v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12151">
<div class="article-summary-box-inner">
<span><p>Deep-learning based Super-Resolution (SR) methods have exhibited promising
performance under non-blind setting where blur kernel is known. However, blur
kernels of Low-Resolution (LR) images in different practical applications are
usually unknown. It may lead to significant performance drop when degradation
process of training images deviates from that of real images. In this paper, we
propose a novel blind SR framework to super-resolve LR images degraded by
arbitrary blur kernel with accurate kernel estimation in frequency domain. To
our best knowledge, this is the first deep learning method which conducts blur
kernel estimation in frequency domain. Specifically, we first demonstrate that
feature representation in frequency domain is more conducive for blur kernel
reconstruction than in spatial domain. Next, we present a Spectrum-to-Kernel
(S$2$K) network to estimate general blur kernels in diverse forms. We use a
Conditional GAN (CGAN) combined with SR-oriented optimization target to learn
the end-to-end translation from degraded images' spectra to unknown kernels.
Extensive experiments on both synthetic and real-world images demonstrate that
our proposed method sufficiently reduces blur kernel estimation error, thus
enables the off-the-shelf non-blind SR methods to work under blind setting
effectively, and achieves superior performance over state-of-the-art blind SR
methods, averagely by 1.39dB, 0.48dB on commom blind SR setting (with Gaussian
kernels) for scales $2\times$ and $4\times$, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vertebrae segmentation, identification and localization using a graph optimization and a synergistic cycle. (arXiv:2110.12177v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12177">
<div class="article-summary-box-inner">
<span><p>This paper considers the segmentation, identification and localization of
vertebrae in CT images. Although these three tasks are related, they face
specific problems that add up when they are addressed together. For example
neighboring vertebrae with similar shapes perturb the identification and
vertebrae with complex or even pathological morphologies impact the
segmentation. Consequently, the three tasks tend to be approached
independently, e.g. labelling (localization and identification) or segmenting
only, or, when treated globally, a sequential strategy is used. Sequential
methods however are prone to accumulate errors as they are not able to recover
from mistakes of the previous module. In this work, we propose to combine all
three tasks and leverage their interdependence: locations ease the
segmentation, the segmentations in turn improve the locations and they all
contribute and benefit from the identification task. To this purpose we propose
a virtuous cycle to enforce coherence between the three tasks. Within such a
cycle, the tasks interoperate and are iterated until a global consistency
criterion is satisfied. Our experiments validate this strategy with
anatomically coherent results that outperform the state of the art on the
VerSe20 challenge benchmark. Our code and model are openly available for
research purposes at https://gitlab.inria.fr/spine/vertebrae_segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An attention-driven hierarchical multi-scale representation for visual recognition. (arXiv:2110.12178v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12178">
<div class="article-summary-box-inner">
<span><p>Convolutional Neural Networks (CNNs) have revolutionized the understanding of
visual content. This is mainly due to their ability to break down an image into
smaller pieces, extract multi-scale localized features and compose them to
construct highly expressive representations for decision making. However, the
convolution operation is unable to capture long-range dependencies such as
arbitrary relations between pixels since it operates on a fixed-size window.
Therefore, it may not be suitable for discriminating subtle changes (e.g.
fine-grained visual recognition). To this end, our proposed method captures the
high-level long-range dependencies by exploring Graph Convolutional Networks
(GCNs), which aggregate information by establishing relationships among
multi-scale hierarchical regions. These regions consist of smaller (closer
look) to larger (far look), and the dependency between regions is modeled by an
innovative attention-driven message propagation, guided by the graph structure
to emphasize the neighborhoods of a given region. Our approach is simple yet
extremely effective in solving both the fine-grained and generic visual
classification problems. It outperforms the state-of-the-arts with a
significant margin on three and is very competitive on other two datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MisMatch: Learning to Change Predictive Confidences with Attention for Consistency-Based, Semi-Supervised Medical Image Segmentation. (arXiv:2110.12179v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12179">
<div class="article-summary-box-inner">
<span><p>The lack of labels is one of the fundamental constraints in deep learning
based methods for image classification and segmentation, especially in
applications such as medical imaging. Semi-supervised learning (SSL) is a
promising method to address the challenge of labels carcity. The
state-of-the-art SSL methods utilise consistency regularisation to learn
unlabelled predictions which are invariant to perturbations on the prediction
confidence. However, such SSL approaches rely on hand-crafted augmentation
techniques which could be sub-optimal. In this paper, we propose MisMatch, a
novel consistency based semi-supervised segmentation method. MisMatch
automatically learns to produce paired predictions with increasedand decreased
confidences. MisMatch consists of an encoder and two decoders. One decoder
learns positive attention for regions of interest (RoI) on unlabelled data
thereby generating higher confidence predictions of RoI. The other decoder
learns negative attention for RoI on the same unlabelled data thereby
generating lower confidence predictions. We then apply a consistency
regularisation between the paired predictions of the decoders. For evaluation,
we first perform extensive cross-validation on a CT-based pulmonary vessel
segmentation task and show that MisMatch statistically outperforms
state-of-the-art semi-supervised methods when only 6.25% of the total labels
are used. Furthermore MisMatch performance using 6.25% ofthe total labels is
comparable to state-of-the-art methodsthat utilise all available labels. In a
second experiment, MisMatch outperforms state-of-the-art methods on an
MRI-based brain tumour segmentation task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attend and Guide (AG-Net): A Keypoints-driven Attention-based Deep Network for Image Recognition. (arXiv:2110.12183v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12183">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel keypoints-based attention mechanism for visual
recognition in still images. Deep Convolutional Neural Networks (CNNs) for
recognizing images with distinctive classes have shown great success, but their
performance in discriminating fine-grained changes is not at the same level. We
address this by proposing an end-to-end CNN model, which learns meaningful
features linking fine-grained changes using our novel attention mechanism. It
captures the spatial structures in images by identifying semantic regions (SRs)
and their spatial distributions, and is proved to be the key to modelling
subtle changes in images. We automatically identify these SRs by grouping the
detected keypoints in a given image. The ``usefulness'' of these SRs for image
recognition is measured using our innovative attentional mechanism focusing on
parts of the image that are most relevant to a given task. This framework
applies to traditional and fine-grained image recognition tasks and does not
require manually annotated regions (e.g. bounding-box of body parts, objects,
etc.) for learning and prediction. Moreover, the proposed keypoints-driven
attention mechanism can be easily integrated into the existing CNN models. The
framework is evaluated on six diverse benchmark datasets. The model outperforms
the state-of-the-art approaches by a considerable margin using Distracted
Driver V1 (Acc: 3.39%), Distracted Driver V2 (Acc: 6.58%), Stanford-40 Actions
(mAP: 2.15%), People Playing Musical Instruments (mAP: 16.05%), Food-101 (Acc:
6.30%) and Caltech-256 (Acc: 2.59%) datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Group-disentangled Representation Learning with Weakly-Supervised Regularization. (arXiv:2110.12185v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12185">
<div class="article-summary-box-inner">
<span><p>Learning interpretable and human-controllable representations that uncover
factors of variation in data remains an ongoing key challenge in representation
learning. We investigate learning group-disentangled representations for groups
of factors with weak supervision. Existing techniques to address this challenge
merely constrain the approximate posterior by averaging over observations of a
shared group. As a result, observations with a common set of variations are
encoded to distinct latent representations, reducing their capacity to
disentangle and generalize to downstream tasks. In contrast to previous works,
we propose GroupVAE, a simple yet effective Kullback-Leibler (KL)
divergence-based regularization across shared latent representations to enforce
consistent and disentangled representations. We conduct a thorough evaluation
and demonstrate that our GroupVAE significantly improves group disentanglement.
Further, we demonstrate that learning group-disentangled representations
improve upon downstream tasks, including fair classification and 3D
shape-related tasks such as reconstruction, classification, and transfer
learning, and is competitive to supervised methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual Shape Guided Segmentation Network for Organs-at-Risk in Head and Neck CT Images. (arXiv:2110.12192v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12192">
<div class="article-summary-box-inner">
<span><p>The accurate segmentation of organs-at-risk (OARs) in head and neck CT images
is a critical step for radiation therapy of head and neck cancer patients.
However, manual delineation for numerous OARs is time-consuming and laborious,
even for expert oncologists. Moreover, manual delineation results are
susceptible to high intra- and inter-variability. To this end, we propose a
novel dual shape guided network (DSGnet) to automatically delineate nine
important OARs in head and neck CT images. To deal with the large shape
variation and unclear boundary of OARs in CT images, we represent the organ
shape using an organ-specific unilateral inverse-distance map (UIDM) and guide
the segmentation task from two different perspectives: direct shape guidance by
following the segmentation prediction and across shape guidance by sharing the
segmentation feature. In the direct shape guidance, the segmentation prediction
is not only supervised by the true label mask, but also by the true UIDM, which
is implemented through a simple yet effective encoder-decoder mapping from the
label space to the distance space. In the across shape guidance, UIDM is used
to facilitate the segmentation by optimizing the shared feature maps. For the
experiments, we build a large head and neck CT dataset with a total of 699
images from different volunteers, and conduct comprehensive experiments and
comparisons with other state-of-the-art methods to justify the effectiveness
and efficiency of our proposed method. The overall Dice Similarity Coefficient
(DSC) value of 0.842 across the nine important OARs demonstrates great
potential applications in improving the delineation quality and reducing the
time cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RPT++: Customized Feature Representation for Siamese Visual Tracking. (arXiv:2110.12194v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12194">
<div class="article-summary-box-inner">
<span><p>While recent years have witnessed remarkable progress in the feature
representation of visual tracking, the problem of feature misalignment between
the classification and regression tasks is largely overlooked. The approaches
of feature extraction make no difference for these two tasks in most of
advanced trackers. We argue that the performance gain of visual tracking is
limited since features extracted from the salient area provide more
recognizable visual patterns for classification, while these around the
boundaries contribute to accurately estimating the target state.
</p>
<p>We address this problem by proposing two customized feature extractors, named
polar pooling and extreme pooling to capture task-specific visual patterns.
Polar pooling plays the role of enriching information collected from the
semantic keypoints for stronger classification, while extreme pooling
facilitates explicit visual patterns of the object boundary for accurate target
state estimation. We demonstrate the effectiveness of the task-specific feature
representation by integrating it into the recent and advanced tracker RPT.
Extensive experiments on several benchmarks show that our Customized Features
based RPT (RPT++) achieves new state-of-the-art performances on OTB-100,
VOT2018, VOT2019, GOT-10k, TrackingNet and LaSOT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards a Robust Differentiable Architecture Search under Label Noise. (arXiv:2110.12197v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12197">
<div class="article-summary-box-inner">
<span><p>Neural Architecture Search (NAS) is the game changer in designing robust
neural architectures. Architectures designed by NAS outperform or compete with
the best manual network designs in terms of accuracy, size, memory footprint
and FLOPs. That said, previous studies focus on developing NAS algorithms for
clean high quality data, a restrictive and somewhat unrealistic assumption. In
this paper, focusing on the differentiable NAS algorithms, we show that vanilla
NAS algorithms suffer from a performance loss if class labels are noisy. To
combat this issue, we make use of the principle of information bottleneck as a
regularizer. This leads us to develop a noise injecting operation that is
included during the learning process, preventing the network from learning from
noisy samples. Our empirical evaluations show that the noise injecting
operation does not degrade the performance of the NAS algorithm if the data is
indeed clean. In contrast, if the data is noisy, the architecture learned by
our algorithm comfortably outperforms algorithms specifically equipped with
sophisticated mechanisms to learn in the presence of label noise. In contrast
to many algorithms designed to work in the presence of noisy labels, prior
knowledge about the properties of the noise and its characteristics are not
required for our algorithm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cascading Feature Extraction for Fast Point Cloud Registration. (arXiv:2110.12204v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12204">
<div class="article-summary-box-inner">
<span><p>We propose a method for speeding up a 3D point cloud registration through a
cascading feature extraction. The current approach with the highest accuracy is
realized by iteratively executing feature extraction and registration using
deep features. However, iterative feature extraction takes time. Our proposed
method significantly reduces the computational cost using cascading shallow
layers. Our idea is to omit redundant computations that do not always
contribute to the final accuracy. The proposed approach is approximately three
times faster than the existing methods without a loss of accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Domain Incremental Learning for Semantic Segmentation. (arXiv:2110.12205v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12205">
<div class="article-summary-box-inner">
<span><p>Recent efforts in multi-domain learning for semantic segmentation attempt to
learn multiple geographical datasets in a universal, joint model. A simple
fine-tuning experiment performed sequentially on three popular road scene
segmentation datasets demonstrates that existing segmentation frameworks fail
at incrementally learning on a series of visually disparate geographical
domains. When learning a new domain, the model catastrophically forgets
previously learned knowledge. In this work, we pose the problem of multi-domain
incremental learning for semantic segmentation. Given a model trained on a
particular geographical domain, the goal is to (i) incrementally learn a new
geographical domain, (ii) while retaining performance on the old domain, (iii)
given that the previous domain's dataset is not accessible. We propose a
dynamic architecture that assigns universally shared, domain-invariant
parameters to capture homogeneous semantic features present in all domains,
while dedicated domain-specific parameters learn the statistics of each domain.
Our novel optimization strategy helps achieve a good balance between retention
of old knowledge (stability) and acquiring new knowledge (plasticity). We
demonstrate the effectiveness of our proposed solution on domain incremental
settings pertaining to real-world driving scenes from roads of Germany
(Cityscapes), the United States (BDD100k), and India (IDD).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MaskSplit: Self-supervised Meta-learning for Few-shot Semantic Segmentation. (arXiv:2110.12207v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12207">
<div class="article-summary-box-inner">
<span><p>Just like other few-shot learning problems, few-shot segmentation aims to
minimize the need for manual annotation, which is particularly costly in
segmentation tasks. Even though the few-shot setting reduces this cost for
novel test classes, there is still a need to annotate the training data. To
alleviate this need, we propose a self-supervised training approach for
learning few-shot segmentation models. We first use unsupervised saliency
estimation to obtain pseudo-masks on images. We then train a simple prototype
based model over different splits of pseudo masks and augmentations of images.
Our extensive experiments show that the proposed approach achieves promising
results, highlighting the potential of self-supervised training. To the best of
our knowledge this is the first work that addresses unsupervised few-shot
segmentation problem on natural images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ES-ImageNet: A Million Event-Stream Classification Dataset for Spiking Neural Networks. (arXiv:2110.12211v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12211">
<div class="article-summary-box-inner">
<span><p>With event-driven algorithms, especially the spiking neural networks (SNNs),
achieving continuous improvement in neuromorphic vision processing, a more
challenging event-stream-dataset is urgently needed. However, it is well known
that creating an ES-dataset is a time-consuming and costly task with
neuromorphic cameras like dynamic vision sensors (DVS). In this work, we
propose a fast and effective algorithm termed Omnidirectional Discrete Gradient
(ODG) to convert the popular computer vision dataset ILSVRC2012 into its
event-stream (ES) version, generating about 1,300,000 frame-based images into
ES-samples in 1000 categories. In this way, we propose an ES-dataset called
ES-ImageNet, which is dozens of times larger than other neuromorphic
classification datasets at present and completely generated by the software.
The ODG algorithm implements an image motion to generate local value changes
with discrete gradient information in different directions, providing a
low-cost and high-speed way for converting frame-based images into event
streams, along with Edge-Integral to reconstruct the high-quality images from
event streams. Furthermore, we analyze the statistics of the ES-ImageNet in
multiple ways, and a performance benchmark of the dataset is also provided
using both famous deep neural network algorithms and spiking neural network
algorithms. We believe that this work shall provide a new large-scale benchmark
dataset for SNNs and neuromorphic vision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Adaptation for Rare Classes Augmented with Synthetic Samples. (arXiv:2110.12216v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12216">
<div class="article-summary-box-inner">
<span><p>To alleviate lower classification performance on rare classes in imbalanced
datasets, a possible solution is to augment the underrepresented classes with
synthetic samples. Domain adaptation can be incorporated in a classifier to
decrease the domain discrepancy between real and synthetic samples. While
domain adaptation is generally applied on completely synthetic source domains
and real target domains, we explore how domain adaptation can be applied when
only a single rare class is augmented with simulated samples. As a testbed, we
use a camera trap animal dataset with a rare deer class, which is augmented
with synthetic deer samples. We adapt existing domain adaptation methods to two
new methods for the single rare class setting: DeerDANN, based on the
Domain-Adversarial Neural Network (DANN), and DeerCORAL, based on deep
correlation alignment (Deep CORAL) architectures. Experiments show that
DeerDANN has the highest improvement in deer classification accuracy of 24.0%
versus 22.4% improvement of DeerCORAL when compared to the baseline. Further,
both methods require fewer than 10k synthetic samples, as used by the baseline,
to achieve these higher accuracies. DeerCORAL requires the least number of
synthetic samples (2k deer), followed by DeerDANN (8k deer).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parametric Variational Linear Units (PVLUs) in Deep Convolutional Networks. (arXiv:2110.12246v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12246">
<div class="article-summary-box-inner">
<span><p>The Rectified Linear Unit is currently a state-of-the-art activation function
in deep convolutional neural networks. To combat ReLU's dying neuron problem,
we propose the Parametric Variational Linear Unit (PVLU), which adds a
sinusoidal function with trainable coefficients to ReLU. Along with introducing
nonlinearity and non-zero gradients across the entire real domain, PVLU allows
for increased model generalization and robustness when implemented in the
context of transfer learning. On a simple, non-transfer sequential CNN, PVLU
led to relative error decrease of 16.3% and 11.3% without and with data
augmentation, relative to ReLU. PVLU is also tested on transfer learning
problems. The VGG-16 and VGG-19 models experience relative error reductions of
9.5% and 10.7% on CIFAR-10, respectively, after the substitution of ReLU with
PVLU. When training on Gaussian-filtered CIFAR-10 images, similar improvements
are noted for the VGG models. Most notably, PVLU fine tuning allows for
relative error reductions up to and exceeding 10% on near state-of-the-art
ResNet models for both CIFAR-10 and CIFAR-100.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Confidence-Aware Active Feedback for Efficient Instance Search. (arXiv:2110.12255v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12255">
<div class="article-summary-box-inner">
<span><p>Relevance feedback is widely used in instance search (INS) tasks to further
refine imperfect ranking results, but it often comes with low interaction
efficiency. Active learning (AL) technique has achieved great success in
improving annotation efficiency in classification tasks. However, considering
irrelevant samples' diversity and class imbalance in INS tasks, existing AL
methods cannot always select the most suitable feedback candidates for INS
problems. In addition, they are often too computationally complex to be applied
in interactive INS scenario. To address the above problems, we propose a
confidence-aware active feedback (CAAF) method that can efficiently select the
most valuable feedback candidates to improve the re-ranking performance.
Specifically, inspired by the explicit sample difficulty modeling in self-paced
learning, we utilize a pairwise manifold ranking loss to evaluate the ranking
confidence of each unlabeled sample, and formulate the INS process as a
confidence-weighted manifold ranking problem. Furthermore, we introduce an
approximate optimization scheme to simplify the solution from QP problems with
constraints to closed-form expressions, and selects only the top-K samples in
the initial ranking list for INS, so that CAAF is able to handle large-scale
INS tasks in a short period of time. Extensive experiments on both image and
video INS tasks demonstrate the effectiveness of the proposed CAAF method. In
particular, CAAF outperforms the first-place record in the public large-scale
video INS evaluation of TRECVID 2021.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">espiownage: Tracking Transients in Steelpan Drum Strikes Using Surveillance Technology. (arXiv:2110.12261v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12261">
<div class="article-summary-box-inner">
<span><p>We present an improvement in the ability to meaningfully track features in
high speed videos of Caribbean steelpan drums illuminated by Electronic Speckle
Pattern Interferometry (ESPI). This is achieved through the use of up-to-date
computer vision libraries for object detection and image segmentation as well
as a significant effort toward cleaning the dataset previously used to train
systems for this application. Besides improvements on previous metric scores by
10% or more, noteworthy in this project are the introduction of a
segmentation-regression map for the entire drum surface yielding interference
fringe counts comparable to those obtained via object detection, as well as the
accelerated workflow for coordinating the data-cleaning-and-model-training
feedback loop for rapid iteration allowing this project to be conducted on a
timescale of only 18 days.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Benchmarking of Lightweight Deep Learning Architectures for Skin Cancer Classification using ISIC 2017 Dataset. (arXiv:2110.12270v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12270">
<div class="article-summary-box-inner">
<span><p>Skin cancer is one of the deadly types of cancer and is common in the world.
Recently, there has been a huge jump in the rate of people getting skin cancer.
For this reason, the number of studies on skin cancer classification with deep
learning are increasing day by day. For the growth of work in this area, the
International Skin Imaging Collaboration (ISIC) organization was established
and they created an open dataset archive. In this study, images were taken from
ISIC 2017 Challenge. The skin cancer images taken were preprocessed and data
augmented. Later, these images were trained with transfer learning and
fine-tuning approach and deep learning models were created in this way. 3
different mobile deep learning models and 3 different batch size values were
determined for each, and a total of 9 models were created. Among these models,
the NASNetMobile model with 16 batch size got the best result. The accuracy
value of this model is 82.00%, the precision value is 81.77% and the F1 score
value is 0.8038. Our method is to benchmark mobile deep learning models which
have few parameters and compare the results of the models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Validation: Early Stopping for Single-Instance Deep Generative Priors. (arXiv:2110.12271v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12271">
<div class="article-summary-box-inner">
<span><p>Recent works have shown the surprising effectiveness of deep generative
models in solving numerous image reconstruction (IR) tasks, even without
training data. We call these models, such as deep image prior and deep decoder,
collectively as single-instance deep generative priors (SIDGPs). The successes,
however, often hinge on appropriate early stopping (ES), which by far has
largely been handled in an ad-hoc manner. In this paper, we propose the first
principled method for ES when applying SIDGPs to IR, taking advantage of the
typical bell trend of the reconstruction quality. In particular, our method is
based on collaborative training and self-validation: the primal reconstruction
process is monitored by a deep autoencoder, which is trained online with the
historic reconstructed images and used to validate the reconstruction quality
constantly. Experimentally, on several IR problems and different SIDGPs, our
self-validation method is able to reliably detect near-peak performance and
signal good ES points. Our code is available at
https://sun-umn.github.io/Self-Validation/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"One-Shot" Reduction of Additive Artifacts in Medical Images. (arXiv:2110.12274v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12274">
<div class="article-summary-box-inner">
<span><p>Medical images may contain various types of artifacts with different patterns
and mixtures, which depend on many factors such as scan setting, machine
condition, patients' characteristics, surrounding environment, etc. However,
existing deep-learning-based artifact reduction methods are restricted by their
training set with specific predetermined artifact types and patterns. As such,
they have limited clinical adoption. In this paper, we introduce One-Shot
medical image Artifact Reduction (OSAR), which exploits the power of deep
learning but without using pre-trained general networks. Specifically, we train
a light-weight image-specific artifact reduction network using data synthesized
from the input image at test-time. Without requiring any prior large training
data set, OSAR can work with almost any medical images that contain varying
additive artifacts which are not in any existing data sets. In addition,
Computed Tomography (CT) and Magnetic Resonance Imaging (MRI) are used as
vehicles and show that the proposed method can reduce artifacts better than
state-of-the-art both qualitatively and quantitatively using shorter test time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Signal to Noise Ratio Loss Function. (arXiv:2110.12275v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12275">
<div class="article-summary-box-inner">
<span><p>This work proposes a new loss function targeting classification problems,
utilizing a source of information overlooked by cross entropy loss. First, we
derive a series of the tightest upper and lower bounds for the probability of a
random variable in a given interval. Second, a lower bound is proposed for the
probability of a true positive for a parametric classification problem, where
the form of probability density function (pdf) of data is given. A closed form
for finding the optimal function of unknowns is derived to maximize the
probability of true positives. Finally, for the case that the pdf of data is
unknown, we apply the proposed boundaries to find the lower bound of the
probability of true positives and upper bound of the probability of false
positives and optimize them using a loss function which is given by combining
the boundaries. We demonstrate that the resultant loss function is a function
of the signal to noise ratio both within and across logits. We empirically
evaluate our proposals to show their benefit for classification problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perineural Invasion Detection in Multiple Organ Cancer Based on Deep Convolutional Neural Network. (arXiv:2110.12283v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12283">
<div class="article-summary-box-inner">
<span><p>Perineural invasion (PNI) by malignant tumor cells has been reported as an
independent indicator of poor prognosis in various cancers. Assessment of PNI
in small nerves on glass slides is a labor-intensive task. In this study, we
propose an algorithm to detect the perineural invasions in colon, prostate, and
pancreas cancers based on a convolutional neural network (CNN).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Face sketch to photo translation using generative adversarial networks. (arXiv:2110.12290v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12290">
<div class="article-summary-box-inner">
<span><p>Translating face sketches to photo-realistic faces is an interesting and
essential task in many applications like law enforcement and the digital
entertainment industry. One of the most important challenges of this task is
the inherent differences between the sketch and the real image such as the lack
of color and details of the skin tissue in the sketch. With the advent of
adversarial generative models, an increasing number of methods have been
proposed for sketch-to-image synthesis. However, these models still suffer from
limitations such as the large number of paired data required for training, the
low resolution of the produced images, or the unrealistic appearance of the
generated images. In this paper, we propose a method for converting an input
facial sketch to a colorful photo without the need for any paired dataset. To
do so, we use a pre-trained face photo generating model to synthesize
high-quality natural face photos and employ an optimization procedure to keep
high-fidelity to the input sketch. We train a network to map the facial
features extracted from the input sketch to a vector in the latent space of the
face generating model. Also, we study different optimization criteria and
compare the results of the proposed model with those of the state-of-the-art
models quantitatively and qualitatively. The proposed model achieved 0.655 in
the SSIM index and 97.59% rank-1 face recognition rate with higher quality of
the produced images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Layer-wise Adversarial-aware Quantization Optimization for Improving Robustness. (arXiv:2110.12308v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12308">
<div class="article-summary-box-inner">
<span><p>Neural networks are getting better accuracy with higher energy and
computational cost. After quantization, the cost can be greatly saved, and the
quantized models are more hardware friendly with acceptable accuracy loss. On
the other hand, recent research has found that neural networks are vulnerable
to adversarial attacks, and the robustness of a neural network model can only
be improved with defense methods, such as adversarial training. In this work,
we find that adversarially-trained neural networks are more vulnerable to
quantization loss than plain models. To minimize both the adversarial and the
quantization losses simultaneously and to make the quantized model robust, we
propose a layer-wise adversarial-aware quantization method, using the Lipschitz
constant to choose the best quantization parameter settings for a neural
network. We theoretically derive the losses and prove the consistency of our
metric selection. The experiment results show that our method can effectively
and efficiently improve the robustness of quantized adversarially-trained
neural networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoVA: Context-aware Visual Attention for Webpage Information Extraction. (arXiv:2110.12320v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12320">
<div class="article-summary-box-inner">
<span><p>Webpage information extraction (WIE) is an important step to create knowledge
bases. For this, classical WIE methods leverage the Document Object Model (DOM)
tree of a website. However, use of the DOM tree poses significant challenges as
context and appearance are encoded in an abstract manner. To address this
challenge we propose to reformulate WIE as a context-aware Webpage Object
Detection task. Specifically, we develop a Context-aware Visual Attention-based
(CoVA) detection pipeline which combines appearance features with syntactical
structure from the DOM tree. To study the approach we collect a new large-scale
dataset of e-commerce websites for which we manually annotate every web element
with four labels: product price, product title, product image and background.
On this dataset we show that the proposed CoVA approach is a new challenging
baseline which improves upon prior state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ADC: Adversarial attacks against object Detection that evade Context consistency checks. (arXiv:2110.12321v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12321">
<div class="article-summary-box-inner">
<span><p>Deep Neural Networks (DNNs) have been shown to be vulnerable to adversarial
examples, which are slightly perturbed input images which lead DNNs to make
wrong predictions. To protect from such examples, various defense strategies
have been proposed. A very recent defense strategy for detecting adversarial
examples, that has been shown to be robust to current attacks, is to check for
intrinsic context consistencies in the input data, where context refers to
various relationships (e.g., object-to-object co-occurrence relationships) in
images. In this paper, we show that even context consistency checks can be
brittle to properly crafted adversarial examples and to the best of our
knowledge, we are the first to do so. Specifically, we propose an adaptive
framework to generate examples that subvert such defenses, namely, Adversarial
attacks against object Detection that evade Context consistency checks (ADC).
In ADC, we formulate a joint optimization problem which has two attack goals,
viz., (i) fooling the object detector and (ii) evading the context consistency
check system, at the same time. Experiments on both PASCAL VOC and MS COCO
datasets show that examples generated with ADC fool the object detector with a
success rate of over 85% in most cases, and at the same time evade the recently
proposed context consistency checks, with a bypassing rate of over 80% in most
cases. Our results suggest that how to robustly model context and check its
consistency, is still an open problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A methodology for detection and localization of fruits in apples orchards from aerial images. (arXiv:2110.12331v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12331">
<div class="article-summary-box-inner">
<span><p>Computer vision methods based on convolutional neural networks (CNNs) have
presented promising results on image-based fruit detection at ground-level for
different crops. However, the integration of the detections found in different
images, allowing accurate fruit counting and yield prediction, have received
less attention. This work presents a methodology for automated fruit counting
employing aerial-images. It includes algorithms based on multiple view geometry
to perform fruits tracking, not just avoiding double counting but also locating
the fruits in the 3-D space. Preliminary assessments show correlations above
0.8 between fruit counting and true yield for apples. The annotated dataset
employed on CNN training is publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SOLVER: Scene-Object Interrelated Visual Emotion Reasoning Network. (arXiv:2110.12334v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12334">
<div class="article-summary-box-inner">
<span><p>Visual Emotion Analysis (VEA) aims at finding out how people feel emotionally
towards different visual stimuli, which has attracted great attention recently
with the prevalence of sharing images on social networks. Since human emotion
involves a highly complex and abstract cognitive process, it is difficult to
infer visual emotions directly from holistic or regional features in affective
images. It has been demonstrated in psychology that visual emotions are evoked
by the interactions between objects as well as the interactions between objects
and scenes within an image. Inspired by this, we propose a novel Scene-Object
interreLated Visual Emotion Reasoning network (SOLVER) to predict emotions from
images. To mine the emotional relationships between distinct objects, we first
build up an Emotion Graph based on semantic concepts and visual features. Then,
we conduct reasoning on the Emotion Graph using Graph Convolutional Network
(GCN), yielding emotion-enhanced object features. We also design a Scene-Object
Fusion Module to integrate scenes and objects, which exploits scene features to
guide the fusion process of object features with the proposed scene-based
attention mechanism. Extensive experiments and comparisons are conducted on
eight public visual emotion datasets, and the results demonstrate that the
proposed SOLVER consistently outperforms the state-of-the-art methods by a
large margin. Ablation studies verify the effectiveness of our method and
visualizations prove its interpretability, which also bring new insight to
explore the mysteries in VEA. Notably, we further discuss SOLVER on three other
potential datasets with extended experiments, where we validate the robustness
of our method and notice some limitations of it.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quality Map Fusion for Adversarial Learning. (arXiv:2110.12338v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12338">
<div class="article-summary-box-inner">
<span><p>Generative adversarial models that capture salient low-level features which
convey visual information in correlation with the human visual system (HVS)
still suffer from perceptible image degradations. The inability to convey such
highly informative features can be attributed to mode collapse, convergence
failure and vanishing gradients. In this paper, we improve image quality
adversarially by introducing a novel quality map fusion technique that
harnesses image features similar to the HVS and the perceptual properties of a
deep convolutional neural network (DCNN). We extend the widely adopted l2
Wasserstein distance metric to other preferable quality norms derived from
Banach spaces that capture richer image properties like structure, luminance,
contrast and the naturalness of images. We also show that incorporating a
perceptual attention mechanism (PAM) that extracts global feature embeddings
from the network bottleneck with aggregated perceptual maps derived from
standard image quality metrics translate to a better image quality. We also
demonstrate impressive performance over other methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Closer Look at Few-Shot Video Classification: A New Baseline and Benchmark. (arXiv:2110.12358v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12358">
<div class="article-summary-box-inner">
<span><p>The existing few-shot video classification methods often employ a
meta-learning paradigm by designing customized temporal alignment module for
similarity calculation. While significant progress has been made, these methods
fail to focus on learning effective representations, and heavily rely on the
ImageNet pre-training, which might be unreasonable for the few-shot recognition
setting due to semantics overlap. In this paper, we aim to present an in-depth
study on few-shot video classification by making three contributions. First, we
perform a consistent comparative study on the existing metric-based methods to
figure out their limitations in representation learning. Accordingly, we
propose a simple classifier-based baseline without any temporal alignment that
surprisingly outperforms the state-of-the-art meta-learning based methods.
Second, we discover that there is a high correlation between the novel action
class and the ImageNet object class, which is problematic in the few-shot
recognition setting. Our results show that the performance of training from
scratch drops significantly, which implies that the existing benchmarks cannot
provide enough base data. Finally, we present a new benchmark with more base
data to facilitate future few-shot video classification without pre-training.
The code will be made available at https://github.com/MCG-NJU/FSL-Video.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CvT-ASSD: Convolutional vision-Transformer Based Attentive Single Shot MultiBox Detector. (arXiv:2110.12364v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12364">
<div class="article-summary-box-inner">
<span><p>Due to the success of Bidirectional Encoder Representations from Transformers
(BERT) in natural language process (NLP), the multi-head attention transformer
has been more and more prevalent in computer-vision researches (CV). However,
it still remains a challenge for researchers to put forward complex tasks such
as vision detection and semantic segmentation. Although multiple
Transformer-Based architectures like DETR and ViT-FRCNN have been proposed to
complete object detection task, they inevitably decreases discrimination
accuracy and brings down computational efficiency caused by the enormous
learning parameters and heavy computational complexity incurred by the
traditional self-attention operation. In order to alleviate these issues, we
present a novel object detection architecture, named Convolutional vision
Transformer Based Attentive Single Shot MultiBox Detector (CvT-ASSD), that
built on the top of Convolutional vision Transormer (CvT) with the efficient
Attentive Single Shot MultiBox Detector (ASSD). We provide comprehensive
empirical evidence showing that our model CvT-ASSD can leads to good system
efficiency and performance while being pretrained on large-scale detection
datasets such as PASCAL VOC and MS COCO. Code has been released on public
github repository at https://github.com/albert-jin/CvT-ASSD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AuxAdapt: Stable and Efficient Test-Time Adaptation for Temporally Consistent Video Semantic Segmentation. (arXiv:2110.12369v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12369">
<div class="article-summary-box-inner">
<span><p>In video segmentation, generating temporally consistent results across frames
is as important as achieving frame-wise accuracy. Existing methods rely either
on optical flow regularization or fine-tuning with test data to attain temporal
consistency. However, optical flow is not always avail-able and reliable.
Besides, it is expensive to compute. Fine-tuning the original model in test
time is cost sensitive.
</p>
<p>This paper presents an efficient, intuitive, and unsupervised online
adaptation method, AuxAdapt, for improving the temporal consistency of most
neural network models. It does not require optical flow and only takes one pass
of the video. Since inconsistency mainly arises from the model's uncertainty in
its output, we propose an adaptation scheme where the model learns from its own
segmentation decisions as it streams a video, which allows producing more
confident and temporally consistent labeling for similarly-looking pixels
across frames. For stability and efficiency, we leverage a small auxiliary
segmentation network (AuxNet) to assist with this adaptation. More
specifically, AuxNet readjusts the decision of the original segmentation
network (Main-Net) by adding its own estimations to that of MainNet. At every
frame, only AuxNet is updated via back-propagation while keeping MainNet fixed.
We extensively evaluate our test-time adaptation approach on standard video
benchmarks, including Cityscapes, CamVid, and KITTI. The results demonstrate
that our approach provides label-wise accurate, temporally consistent, and
computationally efficient adaptation (5+ folds overhead reduction comparing to
state-of-the-art test-time adaptation methods).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty-Aware Lung Nodule Segmentation with Multiple Annotations. (arXiv:2110.12372v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12372">
<div class="article-summary-box-inner">
<span><p>Since radiologists have different training and clinical experience, they may
provide various segmentation maps for a lung nodule. As a result, for a
specific lung nodule, some regions have a higher chance of causing segmentation
uncertainty, which brings difficulty for lung nodule segmentation with multiple
annotations. To address this problem, this paper proposes an Uncertainty-Aware
Segmentation Network (UAS-Net) based on multi-branch U-Net, which can learn the
valuable visual features from the regions that may cause segmentation
uncertainty and contribute to a better segmentation result. Meanwhile, this
network can provide a Multi-Confidence Mask (MCM) simultaneously, pointing out
regions with different segmentation uncertainty levels. We introduce a
Feature-Aware Concatenation structure for different learning targets and let
each branch have a specific learning preference. Moreover, a joint adversarial
learning process is also adopted to help learn discriminative features of
complex structures. Experimental results show that our method can predict the
reasonable regions with higher uncertainty and improve lung nodule segmentation
performance in LIDC-IDRI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perceptual Consistency in Video Segmentation. (arXiv:2110.12385v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12385">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a novel perceptual consistency perspective on video
semantic segmentation, which can capture both temporal consistency and
pixel-wise correctness. Given two nearby video frames, perceptual consistency
measures how much the segmentation decisions agree with the pixel
correspondences obtained via matching general perceptual features. More
specifically, for each pixel in one frame, we find the most perceptually
correlated pixel in the other frame. Our intuition is that such a pair of
pixels are highly likely to belong to the same class. Next, we assess how much
the segmentation agrees with such perceptual correspondences, based on which we
derive the perceptual consistency of the segmentation maps across these two
frames. Utilizing perceptual consistency, we can evaluate the temporal
consistency of video segmentation by measuring the perceptual consistency over
consecutive pairs of segmentation maps in a video. Furthermore, given a
sparsely labeled test video, perceptual consistency can be utilized to aid with
predicting the pixel-wise correctness of the segmentation on an unlabeled
frame. More specifically, by measuring the perceptual consistency between the
predicted segmentation and the available ground truth on a nearby frame and
combining it with the segmentation confidence, we can accurately assess the
classification correctness on each pixel. Our experiments show that the
proposed perceptual consistency can more accurately evaluate the temporal
consistency of video segmentation as compared to flow-based measures.
Furthermore, it can help more confidently predict segmentation accuracy on
unlabeled test frames, as compared to using classification confidence alone.
Finally, our proposed measure can be used as a regularizer during the training
of segmentation models, which leads to more temporally consistent video
segmentation while maintaining accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Motion History Images with 3D Convolutional Networks in Isolated Sign Language Recognition. (arXiv:2110.12396v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12396">
<div class="article-summary-box-inner">
<span><p>Sign language recognition using computational models is a challenging problem
that requires simultaneous spatio-temporal modeling of the multiple sources,
i.e. faces, hands, body etc. In this paper, we propose an isolated sign
language recognition model based on a model trained using Motion History Images
(MHI) that are generated from RGB video frames. RGB-MHI images represent
spatio-temporal summary of each sign video effectively in a single RGB image.
We propose two different approaches using this model. In the first approach, we
use RGB-MHI model as a motion-based spatial attention module integrated in a
3D-CNN architecture. In the second approach, we use RGB-MHI model features
directly with a late fusion technique with the features of a 3D-CNN model. We
perform extensive experiments on two recently released large-scale isolated
sign language datasets, namely AUTSL and BosphorusSign22k datasets. Our
experiments show that our models, which use only RGB data, can compete with the
state-of-the-art models in the literature that use multi-modal data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IQNAS: Interpretable Integer Quadratic Programming Neural Architecture Search. (arXiv:2110.12399v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12399">
<div class="article-summary-box-inner">
<span><p>Realistic use of neural networks often requires adhering to multiple
constraints on latency, energy and memory among others. A popular approach to
find fitting networks is through constrained Neural Architecture Search (NAS).
However, previous methods use complicated predictors for the accuracy of the
network. Those predictors are hard to interpret and sensitive to many
hyperparameters to be tuned, hence, the resulting accuracy of the generated
models is often harmed. In this work we resolve this by introducing
Interpretable Integer Quadratic programming Neural Architecture Search (IQNAS),
that is based on an accurate and simple quadratic formulation of both the
accuracy predictor and the expected resource requirement, together with a
scalable search method with theoretical guarantees. The simplicity of our
proposed predictor together with the intuitive way it is constructed bring
interpretability through many insights about the contribution of different
design choices. For example, we find that in the examined search space, adding
depth and width is more effective at deeper stages of the network and at the
beginning of each resolution stage. Our experiments show that IQNAS generates
comparable to or better architectures than other state-of-the-art NAS methods
within a reduced search cost for each additional generated network, while
strictly satisfying the resource constraints.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Dynamic Keypoints Selection Network for 6DoF Pose Estimation. (arXiv:2110.12401v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12401">
<div class="article-summary-box-inner">
<span><p>6 DoF poses estimation problem aims to estimate the rotation and translation
parameters between two coordinates, such as object world coordinate and camera
world coordinate. Although some advances are made with the help of deep
learning, how to full use scene information is still a problem. Prior works
tackle the problem by pixel-wise feature fusion but need to randomly selecte
numerous points from images, which can not satisfy the demands of fast
inference simultaneously and accurate pose estimation. In this work, we present
a novel deep neural network based on dynamic keypoints selection designed for
6DoF pose estimation from a single RGBD image. Our network includes three
parts, instance semantic segmentation, edge points detection and 6DoF pose
estimation. Given an RGBD image, our network is trained to predict pixel
category and the translation to edge points and center points. Then, a
least-square fitting manner is applied to estimate the 6DoF pose parameters.
Specifically, we propose a dynamic keypoints selection algorithm to choose
keypoints from the foreground feature map. It allows us to leverage geometric
and appearance information. During 6DoF pose estimation, we utilize the
instance semantic segmentation result to filter out background points and only
use foreground points to finish edge points detection and 6DoF pose estimation.
Experiments on two commonly used 6DoF estimation benchmark datasets, YCB-Video
and LineMoD, demonstrate that our method outperforms the state-of-the-art
methods and achieves significant improvements over other same category methods
time efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NAS-FCOS: Efficient Search for Object Detection Architectures. (arXiv:2110.12423v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12423">
<div class="article-summary-box-inner">
<span><p>Neural Architecture Search (NAS) has shown great potential in effectively
reducing manual effort in network design by automatically discovering optimal
architectures. What is noteworthy is that as of now, object detection is less
touched by NAS algorithms despite its significant importance in computer
vision. To the best of our knowledge, most of the recent NAS studies on object
detection tasks fail to satisfactorily strike a balance between performance and
efficiency of the resulting models, let alone the excessive amount of
computational resources cost by those algorithms. Here we propose an efficient
method to obtain better object detectors by searching for the feature pyramid
network (FPN) as well as the prediction head of a simple anchor-free object
detector, namely, FCOS [36], using a tailored reinforcement learning paradigm.
With carefully designed search space, search algorithms, and strategies for
evaluating network quality, we are able to find top-performing detection
architectures within 4 days using 8 V100 GPUs. The discovered architectures
surpass state-of-the-art object detection models (such as Faster R-CNN,
Retina-Net and, FCOS) by 1.0% to 5.4% points in AP on the COCO dataset, with
comparable computation complexity and memory footprint, demonstrating the
efficacy of the proposed NAS method for object detection. Code is available at
https://github.com/Lausannen/NAS-FCOS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image-Based CLIP-Guided Essence Transfer. (arXiv:2110.12427v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12427">
<div class="article-summary-box-inner">
<span><p>CLIP is trained on a large corpus of matched images and text captions and is,
therefore, much richer semantically than networks that perform multiclass
classification for a limited number of classes only. It has been shown to be
extremely suitable for zero-shot computer vision tasks; here, we demonstrate
its ability to support semantic blending. While the StyleGAN space already
performs reasonable blending for images of, e.g., two children, it struggles
when blending images with different attributes. On the other hand, CLIP by
itself struggles to maintain identity when blending. The combination of the two
seems to provide a powerful blending technique, which enjoys the benefits of
both representations. This is enabled through a novel method, which assumes
additivity in the first latent space and ensures additivity in the second
through optimization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WARPd: A linearly convergent first-order method for inverse problems with approximate sharpness conditions. (arXiv:2110.12437v1 [math.NA])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12437">
<div class="article-summary-box-inner">
<span><p>Reconstruction of signals from undersampled and noisy measurements is a topic
of considerable interest. Sharpness conditions directly control the recovery
performance of restart schemes for first-order methods without the need for
restrictive assumptions such as strong convexity. However, they are challenging
to apply in the presence of noise or approximate model classes (e.g.,
approximate sparsity). We provide a first-order method: Weighted, Accelerated
and Restarted Primal-dual (WARPd), based on primal-dual iterations and a novel
restart-reweight scheme. Under a generic approximate sharpness condition, WARPd
achieves stable linear convergence to the desired vector. Many problems of
interest fit into this framework. For example, we analyze sparse recovery in
compressed sensing, low-rank matrix recovery, matrix completion, TV
regularization, minimization of $\|Bx\|_{l^1}$ under constraints
($l^1$-analysis problems for general $B$), and mixed regularization problems.
We show how several quantities controlling recovery performance also provide
explicit approximate sharpness constants. Numerical experiments show that WARPd
compares favorably with specialized state-of-the-art methods and is ideally
suited for solving large-scale problems. We also present a noise-blind variant
based on the Square-Root LASSO decoder. Finally, we show how to unroll WARPd as
neural networks. This approximation theory result provides lower bounds for
stable and accurate neural networks for inverse problems and sheds light on
architecture choices. Code and a gallery of examples are made available online
as a MATLAB package.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bangla Image Caption Generation through CNN-Transformer based Encoder-Decoder Network. (arXiv:2110.12442v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12442">
<div class="article-summary-box-inner">
<span><p>Automatic Image Captioning is the never-ending effort of creating
syntactically and validating the accuracy of textual descriptions of an image
in natural language with context. The encoder-decoder structure used throughout
existing Bengali Image Captioning (BIC) research utilized abstract image
feature vectors as the encoder's input. We propose a novel transformer-based
architecture with an attention mechanism with a pre-trained ResNet-101 model
image encoder for feature extraction from images. Experiments demonstrate that
the language decoder in our technique captures fine-grained information in the
caption and, then paired with image features, produces accurate and diverse
captions on the BanglaLekhaImageCaptions dataset. Our approach outperforms all
existing Bengali Image Captioning work and sets a new benchmark by scoring
0.694 on BLEU-1, 0.630 on BLEU-2, 0.582 on BLEU-3, and 0.337 on METEOR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Edge Detection with Diverse Deep Supervision. (arXiv:1804.02864v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1804.02864">
<div class="article-summary-box-inner">
<span><p>Semantic edge detection (SED), which aims at jointly extracting edges as well
as their category information, has far-reaching applications in domains such as
semantic segmentation, object proposal generation, and object recognition. SED
naturally requires achieving two distinct supervision targets: locating fine
detailed edges and identifying high-level semantics. Our motivation comes from
the hypothesis that such distinct targets prevent state-of-the-art SED methods
from effectively using deep supervision to improve results. To this end, we
propose a novel fully convolutional neural network using diverse deep
supervision (DDS) within a multi-task framework where bottom layers aim at
generating category-agnostic edges, while top layers are responsible for the
detection of category-aware semantic edges. To overcome the hypothesized
supervision challenge, a novel information converter unit is introduced, whose
effectiveness has been extensively evaluated on SBD and Cityscapes datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">You Only Recognize Once: Towards Fast Video Text Spotting. (arXiv:1903.03299v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1903.03299">
<div class="article-summary-box-inner">
<span><p>Video text spotting is still an important research topic due to its various
real-applications. Previous approaches usually fall into the four-staged
pipeline: text detection in individual images, framewisely recognizing
localized text regions, tracking text streams and generating final results with
complicated post-processing skills, which might suffer from the huge
computational cost as well as the interferences of low-quality text. In this
paper, we propose a fast and robust video text spotting framework by only
recognizing the localized text one-time instead of frame-wisely recognition.
Specifically, we first obtain text regions in videos with a well-designed
spatial-temporal detector. Then we concentrate on developing a novel text
recommender for selecting the highest-quality text from text streams and only
recognizing the selected ones. Here, the recommender assembles text tracking,
quality scoring and recognition into an end-to-end trainable module, which not
only avoids the interferences from low-quality text but also dramatically
speeds up the video text spotting process. In addition, we collect a larger
scale video text dataset (LSVTD) for promoting the video text spotting
community, which contains 100 text videos from 22 different real-life
scenarios. Extensive experiments on two public benchmarks show that our method
greatly speeds up the recognition process averagely by 71 times compared with
the frame-wise manner, and also achieves the remarkable state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Image Classification Using Coupled Dictionary Embedding. (arXiv:1906.10509v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1906.10509">
<div class="article-summary-box-inner">
<span><p>Zero-shot learning (ZSL) is a framework to classify images belonging to
unseen classes based on solely semantic information about these unseen classes.
In this paper, we propose a new ZSL algorithm using coupled dictionary
learning. The core idea is that the visual features and the semantic attributes
of an image can share the same sparse representation in an intermediate space.
We use images from seen classes and semantic attributes from seen and unseen
classes to learn two dictionaries that can represent sparsely the visual and
semantic feature vectors of an image. In the ZSL testing stage and in the
absence of labeled data, images from unseen classes can be mapped into the
attribute space by finding the joint sparse representation using solely the
visual data. The image is then classified in the attribute space given semantic
descriptions of unseen classes. We also provide an attribute-aware formulation
to tackle domain shift and hubness problems in ZSL. Extensive experiments are
provided to demonstrate the superior performance of our approach against the
state of the art ZSL algorithms on benchmark ZSL datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PhIT-Net: Photo-consistent Image Transform for Robust Illumination Invariant Matching. (arXiv:1911.12641v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1911.12641">
<div class="article-summary-box-inner">
<span><p>We propose a new and completely data-driven approach for generating a
photo-consistent image transform. We show that simple classical algorithms
which operate in the transform domain become extremely resilient to
illumination changes. This considerably improves matching accuracy,
outperforming the use of state-of-the-art invariant representations as well as
new matching methods based on deep features. The transform is obtained by
training a neural network with a specialized triplet loss, designed to
emphasize actual scene changes while attenuating illumination changes. The
transform yields an illumination invariant representation, structured as an
image map, which is highly flexible and can be easily used for various tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Malware Makeover: Breaking ML-based Static Analysis by Modifying Executable Bytes. (arXiv:1912.09064v2 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1912.09064">
<div class="article-summary-box-inner">
<span><p>Motivated by the transformative impact of deep neural networks (DNNs) in
various domains, researchers and anti-virus vendors have proposed DNNs for
malware detection from raw bytes that do not require manual feature
engineering. In this work, we propose an attack that interweaves
binary-diversification techniques and optimization frameworks to mislead such
DNNs while preserving the functionality of binaries. Unlike prior attacks, ours
manipulates instructions that are a functional part of the binary, which makes
it particularly challenging to defend against. We evaluated our attack against
three DNNs in white- and black-box settings, and found that it often achieved
success rates near 100%. Moreover, we found that our attack can fool some
commercial anti-viruses, in certain cases with a success rate of 85%. We
explored several defenses, both new and old, and identified some that can foil
over 80% of our evasion attempts. However, these defenses may still be
susceptible to evasion by attacks, and so we advocate for augmenting
malware-detection systems with methods that do not rely on machine learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learn to Predict Sets Using Feed-Forward Neural Networks. (arXiv:2001.11845v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2001.11845">
<div class="article-summary-box-inner">
<span><p>This paper addresses the task of set prediction using deep feed-forward
neural networks. A set is a collection of elements which is invariant under
permutation and the size of a set is not fixed in advance. Many real-world
problems, such as image tagging and object detection, have outputs that are
naturally expressed as sets of entities. This creates a challenge for
traditional deep neural networks which naturally deal with structured outputs
such as vectors, matrices or tensors. We present a novel approach for learning
to predict sets with unknown permutation and cardinality using deep neural
networks. In our formulation we define a likelihood for a set distribution
represented by a) two discrete distributions defining the set cardinally and
permutation variables, and b) a joint distribution over set elements with a
fixed cardinality. Depending on the problem under consideration, we define
different training models for set prediction using deep neural networks. We
demonstrate the validity of our set formulations on relevant vision problems
such as: 1) multi-label image classification where we outperform the other
competing methods on the PASCAL VOC and MS COCO datasets, 2) object detection,
for which our formulation outperforms popular state-of-the-art detectors, and
3) a complex CAPTCHA test, where we observe that, surprisingly, our set-based
network acquired the ability of mimicking arithmetics without any rules being
coded.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text Perceptron: Towards End-to-End Arbitrary-Shaped Text Spotting. (arXiv:2002.06820v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.06820">
<div class="article-summary-box-inner">
<span><p>Many approaches have recently been proposed to detect irregular scene text
and achieved promising results. However, their localization results may not
well satisfy the following text recognition part mainly because of two reasons:
1) recognizing arbitrary shaped text is still a challenging task, and 2)
prevalent non-trainable pipeline strategies between text detection and text
recognition will lead to suboptimal performances. To handle this
incompatibility problem, in this paper we propose an end-to-end trainable text
spotting approach named Text Perceptron. Concretely, Text Perceptron first
employs an efficient segmentation-based text detector that learns the latent
text reading order and boundary information. Then a novel Shape Transform
Module (abbr. STM) is designed to transform the detected feature regions into
regular morphologies without extra parameters. It unites text detection and the
following recognition part into a whole framework, and helps the whole network
achieve global optimization. Experiments show that our method achieves
competitive performance on two standard text benchmarks, i.e., ICDAR 2013 and
ICDAR 2015, and also obviously outperforms existing methods on irregular text
benchmarks SCUT-CTW1500 and Total-Text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Causality-Aware Inferring: A Sequential Discriminative Approach for Medical Automatic Diagnosis. (arXiv:2003.06534v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.06534">
<div class="article-summary-box-inner">
<span><p>Through learning from the patient simulator built on the collected
patient-doctor dialogues records, medical automatic diagnosis (MAD) aims to
build an interactive diagnostic agent to sequentially inquire about symptoms
for discriminating diseases. However, due to some task-unrelated and non-causal
associations in these collected data, e.g., the preference of the collectors,
the simulator is probably biased against the disease-symptom causality and the
diagnostic agent might be hindered from capturing the transportable knowledge.
This work attempts to address these critical issues in MAD by taking advantage
of the structural causal model (SCM) to identify and resolve two representative
non-causal biases, i.e., (i) default-answer bias and (ii) distributional
inquiry bias, from the aspects of the data usage and the agent design,
respectively. Specifically, Bias (i) originates from that the patient simulator
tries to answer unrecorded inquiries with default answers, which cannot be
resolved by feeding more data [1]. Suffering from the biased simulator,
previous MAD methods cannot fully demonstrate their advantages. To eliminate
this bias and inspired by the propensity score matching technique with SCM, we
propose a propensity-based patient simulator to effectively answer unrecorded
inquiry by drawing knowledge from the other records; Bias (ii) inherently comes
along with the passive manner of collecting MAD data. To this end, we propose a
progressive assurance agent, which includes the dual processes accounting for
symptom inquiry and disease diagnosis. The inquiry process is driven by the
diagnosis process in a top-down manner to inquire about symptoms for enhancing
diagnostic confidence. The diagnosis process can reason within that mental
representation by intervening with imaginary questions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Night-time Scene Parsing with a Large Real Dataset. (arXiv:2003.06883v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.06883">
<div class="article-summary-box-inner">
<span><p>Although huge progress has been made on scene analysis in recent years, most
existing works assume the input images to be in day-time with good lighting
conditions. In this work, we aim to address the night-time scene parsing (NTSP)
problem, which has two main challenges: 1) labeled night-time data are scarce,
and 2) over- and under-exposures may co-occur in the input night-time images
and are not explicitly modeled in existing pipelines. To tackle the scarcity of
night-time data, we collect a novel labeled dataset, named {\it NightCity}, of
4,297 real night-time images with ground truth pixel-level semantic
annotations. To our knowledge, NightCity is the largest dataset for NTSP. In
addition, we also propose an exposure-aware framework to address the NTSP
problem through augmenting the segmentation process with explicitly learned
exposure features. Extensive experiments show that training on NightCity can
significantly improve NTSP performances and that our exposure-aware model
outperforms the state-of-the-art methods, yielding top performances on our
dataset as well as existing datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Convolution-Weight-Distribution Assumption: Rethinking the Criteria of Channel Pruning. (arXiv:2004.11627v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.11627">
<div class="article-summary-box-inner">
<span><p>Channel pruning is a popular technique for compressing convolutional neural
networks (CNNs), where various pruning criteria have been proposed to remove
the redundant filters. From our comprehensive experiments, we found two blind
spots in the study of pruning criteria: (1) Similarity: There are some strong
similarities among several primary pruning criteria that are widely cited and
compared. According to these criteria, the ranks of filters'Importance Score
are almost identical, resulting in similar pruned structures. (2)
Applicability: The filters'Importance Score measured by some pruning criteria
are too close to distinguish the network redundancy well. In this paper, we
analyze these two blind spots on different types of pruning criteria with
layer-wise pruning or global pruning. The analyses are based on the empirical
experiments and our assumption (Convolutional Weight Distribution Assumption)
that the well-trained convolutional filters each layer approximately follow a
Gaussian-alike distribution. This assumption has been verified through
systematic and extensive statistical tests.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPIN: Structure-Preserving Inner Offset Network for Scene Text Recognition. (arXiv:2005.13117v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.13117">
<div class="article-summary-box-inner">
<span><p>Arbitrary text appearance poses a great challenge in scene text recognition
tasks. Existing works mostly handle with the problem in consideration of the
shape distortion, including perspective distortions, line curvature or other
style variations. Therefore, methods based on spatial transformers are
extensively studied. However, chromatic difficulties in complex scenes have not
been paid much attention on. In this work, we introduce a new learnable
geometric-unrelated module, the Structure-Preserving Inner Offset Network
(SPIN), which allows the color manipulation of source data within the network.
This differentiable module can be inserted before any recognition architecture
to ease the downstream tasks, giving neural networks the ability to actively
transform input intensity rather than the existing spatial rectification. It
can also serve as a complementary module to known spatial transformations and
work in both independent and collaborative ways with them. Extensive
experiments show that the use of SPIN results in a significant improvement on
multiple text recognition benchmarks compared to the state-of-the-arts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TRIE: End-to-End Text Reading and Information Extraction for Document Understanding. (arXiv:2005.13118v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.13118">
<div class="article-summary-box-inner">
<span><p>Since real-world ubiquitous documents (e.g., invoices, tickets, resumes and
leaflets) contain rich information, automatic document image understanding has
become a hot topic. Most existing works decouple the problem into two separate
tasks, (1) text reading for detecting and recognizing texts in images and (2)
information extraction for analyzing and extracting key elements from
previously extracted plain text. However, they mainly focus on improving
information extraction task, while neglecting the fact that text reading and
information extraction are mutually correlated. In this paper, we propose a
unified end-to-end text reading and information extraction network, where the
two tasks can reinforce each other. Specifically, the multimodal visual and
textual features of text reading are fused for information extraction and in
turn, the semantics in information extraction contribute to the optimization of
text reading. On three real-world datasets with diverse document images (from
fixed layout to variable layout, from structured text to semi-structured text),
our proposed method significantly outperforms the state-of-the-art methods in
both efficiency and accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Spatio-temporal Latent Feature Clustering for Multiple-object Tracking and Segmentation. (arXiv:2007.07175v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.07175">
<div class="article-summary-box-inner">
<span><p>Assigning consistent temporal identifiers to multiple moving objects in a
video sequence is a challenging problem. A solution to that problem would have
immediate ramifications in multiple object tracking and segmentation problems.
We propose a strategy that treats the temporal identification task as a
spatio-temporal clustering problem. We propose an unsupervised learning
approach using a convolutional and fully connected autoencoder, which we call
deep heterogeneous autoencoder, to learn discriminative features from
segmentation masks and detection bounding boxes. We extract masks and their
corresponding bounding boxes from a pretrained instance segmentation network
and train the autoencoders jointly using task-dependent uncertainty weights to
generate common latent features. We then construct constraints graphs that
encourage associations among objects that satisfy a set of known temporal
conditions. The feature vectors and the constraints graphs are then provided to
the kmeans clustering algorithm to separate the corresponding data points in
the latent space. We evaluate the performance of our method using challenging
synthetic and real-world multiple-object video datasets. Our results show that
our technique outperforms several state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CenterNet3D: An Anchor Free Object Detector for Point Cloud. (arXiv:2007.07214v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.07214">
<div class="article-summary-box-inner">
<span><p>Accurate and fast 3D object detection from point clouds is a key task in
autonomous driving. Existing one-stage 3D object detection methods can achieve
real-time performance, however, they are dominated by anchor-based detectors
which are inefficient and require additional post-processing. In this paper, we
eliminate anchors and model an object as a single point--the center point of
its bounding box. Based on the center point, we propose an anchor-free
CenterNet3D network that performs 3D object detection without anchors. Our
CenterNet3D uses keypoint estimation to find center points and directly
regresses 3D bounding boxes. However, because inherent sparsity of point
clouds, 3D object center points are likely to be in empty space which makes it
difficult to estimate accurate boundaries. To solve this issue, we propose an
extra corner attention module to enforce the CNN backbone to pay more attention
to object boundaries. Besides, considering that one-stage detectors suffer from
the discordance between the predicted bounding boxes and corresponding
classification confidences, we develop an efficient keypoint-sensitive warping
operation to align the confidences to the predicted bounding boxes. Our
proposed CenterNet3D is non-maximum suppression free which makes it more
efficient and simpler. We evaluate CenterNet3D on the widely used KITTI dataset
and more challenging nuScenes dataset. Our method outperforms all
state-of-the-art anchor-based one-stage methods and has comparable performance
to two-stage methods as well. It has an inference speed of 20 FPS and achieves
the best speed and accuracy trade-off. Our source code will be released at
https://github.com/wangguojun2018/CenterNet3d.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Face Image Quality Assessment: A Literature Survey. (arXiv:2009.01103v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.01103">
<div class="article-summary-box-inner">
<span><p>The performance of face analysis and recognition systems depends on the
quality of the acquired face data, which is influenced by numerous factors.
Automatically assessing the quality of face data in terms of biometric utility
can thus be useful to detect low-quality data and make decisions accordingly.
This survey provides an overview of the face image quality assessment
literature, which predominantly focuses on visible wavelength face image input.
A trend towards deep learning based methods is observed, including notable
conceptual differences among the recent approaches, such as the integration of
quality assessment into face recognition models. Besides image selection, face
image quality assessment can also be used in a variety of other application
scenarios, which are discussed herein. Open issues and challenges are pointed
out, i.a. highlighting the importance of comparability for algorithm
evaluations, and the challenge for future work to create deep learning
approaches that are interpretable in addition to providing accurate utility
predictions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">360-Degree Gaze Estimation in the Wild Using Multiple Zoom Scales. (arXiv:2009.06924v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.06924">
<div class="article-summary-box-inner">
<span><p>Gaze estimation involves predicting where the person is looking at within an
image or video. Technically, the gaze information can be inferred from two
different magnification levels: face orientation and eye orientation. The
inference is not always feasible for gaze estimation in the wild, given the
lack of clear eye patches in conditions like extreme left/right gazes or
occlusions. In this work, we design a model that mimics humans' ability to
estimate the gaze by aggregating from focused looks, each at a different
magnification level of the face area. The model avoids the need to extract
clear eye patches and at the same time addresses another important issue of
face-scale variation for gaze estimation in the wild. We further extend the
model to handle the challenging task of 360-degree gaze estimation by encoding
the backward gazes in the polar representation along with a robust averaging
scheme. Experiment results on the ETH-XGaze dataset, which does not contain
scale-varying faces, demonstrate the model's effectiveness to assimilate
information from multiple scales. For other benchmark datasets with many
scale-varying faces (Gaze360 and RT-GENE), the proposed model achieves
state-of-the-art performance for gaze estimation when using either images or
videos. Our code and pretrained models can be accessed at
https://github.com/ashesh-0/MultiZoomGaze.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weak-shot Fine-grained Classification via Similarity Transfer. (arXiv:2009.09197v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.09197">
<div class="article-summary-box-inner">
<span><p>Recognizing fine-grained categories remains a challenging task, due to the
subtle distinctions among different subordinate categories, which results in
the need of abundant annotated samples. To alleviate the data-hungry problem,
we consider the problem of learning novel categories from web data with the
support of a clean set of base categories, which is referred to as weak-shot
learning. In this setting, we propose a method called SimTrans to transfer
pairwise semantic similarity from base categories to novel categories.
Specifically, we firstly train a similarity net on clean data, and then
leverage the transferred similarity to denoise web training data using two
simple yet effective strategies. In addition, we apply adversarial loss on
similarity net to enhance the transferability of similarity. Comprehensive
experiments demonstrate the effectiveness of our weak-shot setting and our
SimTrans method. Datasets and codes are available at
https://github.com/bcmi/SimTrans-Weak-Shot-Classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty Sets for Image Classifiers using Conformal Prediction. (arXiv:2009.14193v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.14193">
<div class="article-summary-box-inner">
<span><p>Convolutional image classifiers can achieve high predictive accuracy, but
quantifying their uncertainty remains an unresolved challenge, hindering their
deployment in consequential settings. Existing uncertainty quantification
techniques, such as Platt scaling, attempt to calibrate the network's
probability estimates, but they do not have formal guarantees. We present an
algorithm that modifies any classifier to output a predictive set containing
the true label with a user-specified probability, such as 90%. The algorithm is
simple and fast like Platt scaling, but provides a formal finite-sample
coverage guarantee for every model and dataset. Our method modifies an existing
conformal prediction algorithm to give more stable predictive sets by
regularizing the small scores of unlikely classes after Platt scaling. In
experiments on both Imagenet and Imagenet-V2 with ResNet-152 and other
classifiers, our scheme outperforms existing approaches, achieving coverage
with sets that are often factors of 5 to 10 smaller than a stand-alone Platt
scaling baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Back to the Future: Cycle Encoding Prediction for Self-supervised Contrastive Video Representation Learning. (arXiv:2010.07217v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.07217">
<div class="article-summary-box-inner">
<span><p>In this paper we show that learning video feature spaces in which temporal
cycles are maximally predictable benefits action classification. In particular,
we propose a novel learning approach termed Cycle Encoding Prediction (CEP)
that is able to effectively represent high-level spatio-temporal structure of
unlabelled video content. CEP builds a latent space wherein the concept of
closed forward-backward as well as backward-forward temporal loops is
approximately preserved. As a self-supervision signal, CEP leverages the
bi-directional temporal coherence of the video stream and applies loss
functions that encourage both temporal cycle closure as well as contrastive
feature separation. Architecturally, the underpinning network structure
utilises a single feature encoder for all video snippets, adding two predictive
modules that learn temporal forward and backward transitions. We apply our
framework for pretext training of networks for action recognition tasks. We
report significantly improved results for the standard datasets UCF101 and
HMDB51. Detailed ablation studies support the effectiveness of the proposed
components. We publish source code for the CEP components in full with this
paper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Intriguing Properties of Contrastive Losses. (arXiv:2011.02803v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.02803">
<div class="article-summary-box-inner">
<span><p>We study three intriguing properties of contrastive learning. First, we
generalize the standard contrastive loss to a broader family of losses, and we
find that various instantiations of the generalized loss perform similarly
under the presence of a multi-layer non-linear projection head. Second, we
study if instance-based contrastive learning (with a global image
representation) can learn well on images with multiple objects present. We find
that meaningful hierarchical local features can be learned despite the fact
that these objectives operate on global instance-level features. Finally, we
study the phenomenon of feature suppression among competing features shared
across augmented views, such as "color distribution" vs "object class". We
construct datasets with explicit and controllable competing features, and show
that, for contrastive learning, a few bits of easy-to-learn shared features can
suppress, and even fully prevent, the learning of other sets of competing
features. In scenarios where there are multiple objects in an image, the
dominant object would suppress the learning of smaller objects. Existing
contrastive learning methods critically rely on data augmentation to favor
certain sets of features over others, and could suffer from learning saturation
for scenarios where existing augmentations cannot fully address the feature
suppression. This poses open challenges to existing contrastive learning
techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Scale 2D Temporal Adjacent Networks for Moment Localization with Natural Language. (arXiv:2012.02646v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.02646">
<div class="article-summary-box-inner">
<span><p>We address the problem of retrieving a specific moment from an untrimmed
video by natural language. It is a challenging problem because a target moment
may take place in the context of other temporal moments in the untrimmed video.
Existing methods cannot tackle this challenge well since they do not fully
consider the temporal contexts between temporal moments. In this paper, we
model the temporal context between video moments by a set of predefined
two-dimensional maps under different temporal scales. For each map, one
dimension indicates the starting time of a moment and the other indicates the
duration. These 2D temporal maps can cover diverse video moments with different
lengths, while representing their adjacent contexts at different temporal
scales. Based on the 2D temporal maps, we propose a Multi-Scale Temporal
Adjacent Network (MS-2D-TAN), a single-shot framework for moment localization.
It is capable of encoding the adjacent temporal contexts at each scale, while
learning discriminative features for matching video moments with referring
expressions. We evaluate the proposed MS-2D-TAN on three challenging
benchmarks, i.e., Charades-STA, ActivityNet Captions, and TACoS, where our
MS-2D-TAN outperforms the state of the art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MANGO: A Mask Attention Guided One-Stage Scene Text Spotter. (arXiv:2012.04350v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.04350">
<div class="article-summary-box-inner">
<span><p>Recently end-to-end scene text spotting has become a popular research topic
due to its advantages of global optimization and high maintainability in real
applications. Most methods attempt to develop various region of interest (RoI)
operations to concatenate the detection part and the sequence recognition part
into a two-stage text spotting framework. However, in such framework, the
recognition part is highly sensitive to the detected results (e.g.), the
compactness of text contours). To address this problem, in this paper, we
propose a novel Mask AttentioN Guided One-stage text spotting framework named
MANGO, in which character sequences can be directly recognized without RoI
operation. Concretely, a position-aware mask attention module is developed to
generate attention weights on each text instance and its characters. It allows
different text instances in an image to be allocated on different feature map
channels which are further grouped as a batch of instance features. Finally, a
lightweight sequence decoder is applied to generate the character sequences. It
is worth noting that MANGO inherently adapts to arbitrary-shaped text spotting
and can be trained end-to-end with only coarse position information (e.g.),
rectangular bounding box) and text annotations. Experimental results show that
the proposed method achieves competitive and even new state-of-the-art
performance on both regular and irregular text spotting benchmarks, i.e., ICDAR
2013, ICDAR 2015, Total-Text, and SCUT-CTW1500.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spectral Analysis for Semantic Segmentation with Applications on Feature Truncation and Weak Annotation. (arXiv:2012.14123v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.14123">
<div class="article-summary-box-inner">
<span><p>We propose spectral analysis to investigate the correlation between the
accuracy and the resolution of segmentation maps for semantic segmentation. The
current networks predict segmentation maps on the down-sampled grid of images
to alleviate the computational cost. Moreover, these networks can be trained by
weak annotations that utilize only the coarse contour of segmentation maps.
Despite the successful achievement of these works utilizing the low-frequency
information of segmentation maps, however, the accuracy of resultant
segmentation maps may also be degraded in the regions near object boundaries.
It is yet unclear for a theoretical guideline to determine an optimal
down-sampled grid to strike the balance between the cost and the accuracy of
segmentation. We analyze the objective function (cross-entropy) and network
back-propagation process in frequency domain. We discover that cross-entropy
and key features of CNN are mainly contributed by the low-frequency components
of segmentation maps. This further provides us quantitative results to
determine the efficacy of down-sampled grid of segmentation maps. The analysis
is then validated on the two applications: the feature truncation method and
the block-wise annotation that limit the high-frequency components of the CNN
features and annotation, respectively. The results agree with our analysis.
Thus the success of the existing work utilizing low-frequency information of
segmentation maps now has theoretical foundation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Surprisingly Simple Semi-Supervised Domain Adaptation with Pretraining and Consistency. (arXiv:2101.12727v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.12727">
<div class="article-summary-box-inner">
<span><p>Most modern unsupervised domain adaptation (UDA) approaches are rooted in
domain alignment, i.e., learning to align source and target features to learn a
target domain classifier using source labels. In semi-supervised domain
adaptation (SSDA), when the learner can access few target domain labels, prior
approaches have followed UDA theory to use domain alignment for learning. We
show that the case of SSDA is different and a good target classifier can be
learned without needing alignment. We use self-supervised pretraining (via
rotation prediction) and consistency regularization to achieve well separated
target clusters, aiding in learning a low error target classifier. With our
Pretraining and Consistency (PAC) approach, we achieve state of the art target
accuracy on this semi-supervised domain adaptation task, surpassing multiple
adversarial domain alignment methods, across multiple datasets. PAC, while
using simple techniques, performs remarkably well on large and challenging SSDA
benchmarks like DomainNet and Visda-17, often outperforming recent state of the
art by sizeable margins. Code for our experiments can be found at
https://github.com/venkatesh-saligrama/PAC
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-tuning deep learning model parameters for improved super-resolution of dynamic MRI with prior-knowledge. (arXiv:2102.02711v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.02711">
<div class="article-summary-box-inner">
<span><p>Dynamic imaging is a beneficial tool for interventions to assess
physiological changes. Nonetheless during dynamic MRI, while achieving a high
temporal resolution, the spatial resolution is compromised. To overcome this
spatio-temporal trade-off, this research presents a super-resolution (SR) MRI
reconstruction with prior knowledge based fine-tuning to maximise spatial
information while reducing the required scan-time for dynamic MRIs. An U-Net
based network with perceptual loss is trained on a benchmark dataset and
fine-tuned using one subject-specific static high resolution MRI as prior
knowledge to obtain high resolution dynamic images during the inference stage.
3D dynamic data for three subjects were acquired with different parameters to
test the generalisation capabilities of the network. The method was tested for
different levels of in-plane undersampling for dynamic MRI. The reconstructed
dynamic SR results after fine-tuning showed higher similarity with the high
resolution ground-truth, while quantitatively achieving statistically
significant improvement. The average SSIM of the lowest resolution experimented
during this research (6.25~\% of the k-space) before and after fine-tuning were
0.939 $\pm$ 0.008 and 0.957 $\pm$ 0.006 respectively. This could theoretically
result in an acceleration factor of 16, which can potentially be acquired in
less than half a second. The proposed approach shows that the super-resolution
MRI reconstruction with prior-information can alleviate the spatio-temporal
trade-off in dynamic MRI, even for high acceleration factors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data-Efficient GAN Training Beyond (Just) Augmentations: A Lottery Ticket Perspective. (arXiv:2103.00397v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.00397">
<div class="article-summary-box-inner">
<span><p>Training generative adversarial networks (GANs) with limited real image data
generally results in deteriorated performance and collapsed models. To conquer
this challenge, we are inspired by the latest observation, that one can
discover independently trainable and highly sparse subnetworks (a.k.a., lottery
tickets) from GANs. Treating this as an inductive prior, we suggest a brand-new
angle towards data-efficient GAN training: by first identifying the lottery
ticket from the original GAN using the small training set of real images; and
then focusing on training that sparse subnetwork by re-using the same set. We
find our coordinated framework to offer orthogonal gains to existing real image
data augmentation methods, and we additionally present a new feature-level
augmentation that can be applied together with them. Comprehensive experiments
endorse the effectiveness of our proposed framework, across various GAN
architectures (SNGAN, BigGAN, and StyleGAN-V2) and diverse datasets (CIFAR-10,
CIFAR-100, Tiny-ImageNet, ImageNet, and multiple few-shot generation datasets).
Codes are available at:
https://github.com/VITA-Group/Ultra-Data-Efficient-GAN-Training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPICE: Semantic Pseudo-labeling for Image Clustering. (arXiv:2103.09382v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.09382">
<div class="article-summary-box-inner">
<span><p>The similarity among samples and the discrepancy between clusters are two
crucial aspects of image clustering. However, current deep clustering methods
suffer from the inaccurate estimation of either feature similarity or semantic
discrepancy. In this paper, we present a Semantic Pseudo-labeling-based Image
ClustEring (SPICE) framework, which divides the clustering network into a
feature model for measuring the instance-level similarity and a clustering head
for identifying the cluster-level discrepancy. We design two semantics-aware
pseudo-labeling algorithms, prototype pseudo-labeling, and reliable
pseudo-labeling, which enable accurate and reliable self-supervision over
clustering. Without using any ground-truth label, we optimize the clustering
network in three stages: 1) train the feature model through contrastive
learning to measure the instance similarity, 2) train the clustering head with
the prototype pseudo-labeling algorithm to identify cluster semantics, and 3)
jointly train the feature model and clustering head with the reliable
pseudo-labeling algorithm to improve the clustering performance. Extensive
experimental results demonstrate that SPICE achieves significant improvements
(~10%) over existing methods and establishes the new state-of-the-art
clustering results on six image benchmark datasets in terms of three popular
metrics. Importantly, SPICE significantly reduces the gap between unsupervised
and fully-supervised classification; e.g., there is only a 2% (91.8% vs 93.8%)
accuracy difference on CIFAR-10. Our code has been made publically available at
https://github.com/niuchuangnn/SPICE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Harmonic Beltrami Signature: A Novel 2D Shape Representation for Object Classification. (arXiv:2103.16411v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.16411">
<div class="article-summary-box-inner">
<span><p>There is a growing interest in shape analysis in recent years. We present a
novel shape signature for 2D bounded simply-connected domains, named the
Harmonic Beltrami signature (HBS). The proposed signature is based on the
harmonic extension of the conformal welding map of a unit circle and its
Beltrami coefficient. We show that there is a one-to-one correspondence between
the quotient space of HBS and the space of 2D simply-connected shapes up to a
translation, rotation and scaling. With a suitable normalization, each
equivalence class in the quotient space of HBS is associated to a unique
representative. It gets rid of the conformal ambiguity. As such, each shape is
associated to a unique HBS. Conversely, the associated shape of a HBS can be
reconstructed based on quasiconformal Teichmuller theories, which is uniquely
determined up to a translation, rotation and scaling. The HBS is thus an
effective fingerprint to represent a 2D shape. The robustness of HBS is studied
both theoretically and experimentally. With the HBS, simple metric, such as L2,
can be used to measure geometric dissimilarity between shapes. Experiments have
been carried out to classify shapes in different classes using HBS. Results
show good classification performance, which demonstrate the efficacy of our
proposed shape signature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Investigation of Critical Issues in Bias Mitigation Techniques. (arXiv:2104.00170v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.00170">
<div class="article-summary-box-inner">
<span><p>A critical problem in deep learning is that systems learn inappropriate
biases, resulting in their inability to perform well on minority groups. This
has led to the creation of multiple algorithms that endeavor to mitigate bias.
However, it is not clear how effective these methods are. This is because study
protocols differ among papers, systems are tested on datasets that fail to test
many forms of bias, and systems have access to hidden knowledge or are tuned
specifically to the test set. To address this, we introduce an improved
evaluation protocol, sensible metrics, and a new dataset, which enables us to
ask and answer critical questions about bias mitigation algorithms. We evaluate
seven state-of-the-art algorithms using the same network architecture and
hyperparameter selection policy across three benchmark datasets. We introduce a
new dataset called Biased MNIST that enables assessment of robustness to
multiple bias sources. We use Biased MNIST and a visual question answering
(VQA) benchmark to assess robustness to hidden biases. Rather than only tuning
to the test set distribution, we study robustness across different tuning
distributions, which is critical because for many applications the test
distribution may not be known during development. We find that algorithms
exploit hidden biases, are unable to scale to multiple forms of bias, and are
highly sensitive to the choice of tuning set. Based on our findings, we implore
the community to adopt more rigorous assessment of future bias mitigation
methods. All data, code, and results are publicly available at:
https://github.com/erobic/bias-mitigators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training Deep Neural Networks via Branch-and-Bound. (arXiv:2104.01730v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01730">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose BPGrad, a novel approximate algorithm for deep
nueral network training, based on adaptive estimates of feasible region via
branch-and-bound. The method is based on the assumption of Lipschitz continuity
in objective function, and as a result, it can adaptively determine the step
size for the current gradient given the history of previous updates. We prove
that, by repeating such a branch-and-pruning procedure, it can achieve the
optimal solution within finite iterations. A computationally efficient solver
based on BPGrad has been proposed to train the deep neural networks. Empirical
results demonstrate that BPGrad solver works well in practice and compares
favorably to other stochastic optimization methods in the tasks of object
recognition, detection, and segmentation. The code is available at
\url{https://github.com/RyanCV/BPGrad}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Permutation Equivariant Structure from Motion. (arXiv:2104.06703v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06703">
<div class="article-summary-box-inner">
<span><p>Existing deep methods produce highly accurate 3D reconstructions in stereo
and multiview stereo settings, i.e., when cameras are both internally and
externally calibrated. Nevertheless, the challenge of simultaneous recovery of
camera poses and 3D scene structure in multiview settings with deep networks is
still outstanding. Inspired by projective factorization for Structure from
Motion (SFM) and by deep matrix completion techniques, we propose a neural
network architecture that, given a set of point tracks in multiple images of a
static scene, recovers both the camera parameters and a (sparse) scene
structure by minimizing an unsupervised reprojection loss. Our network
architecture is designed to respect the structure of the problem: the sought
output is equivariant to permutations of both cameras and scene points.
Notably, our method does not require initialization of camera parameters or 3D
point locations. We test our architecture in two setups: (1) single scene
reconstruction and (2) learning from multiple scenes. Our experiments,
conducted on a variety of datasets in both internally calibrated and
uncalibrated settings, indicate that our method accurately recovers pose and
structure, on par with classical state of the art methods. Additionally, we
show that a pre-trained network can be used to reconstruct novel scenes using
inexpensive fine-tuning with no loss of accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">M3DeTR: Multi-representation, Multi-scale, Mutual-relation 3D Object Detection with Transformers. (arXiv:2104.11896v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.11896">
<div class="article-summary-box-inner">
<span><p>We present a novel architecture for 3D object detection, M3DeTR, which
combines different point cloud representations (raw, voxels, bird-eye view)
with different feature scales based on multi-scale feature pyramids. M3DeTR is
the first approach that unifies multiple point cloud representations, feature
scales, as well as models mutual relationships between point clouds
simultaneously using transformers. We perform extensive ablation experiments
that highlight the benefits of fusing representation and scale, and modeling
the relationships. Our method achieves state-of-the-art performance on the
KITTI 3D object detection dataset and Waymo Open Dataset. Results show that
M3DeTR improves the baseline significantly by 1.48% mAP for all classes on
Waymo Open Dataset. In particular, our approach ranks 1st on the well-known
KITTI 3D Detection Benchmark for both car and cyclist classes, and ranks 1st on
Waymo Open Dataset with single frame point cloud input. Our code is available
at: https://github.com/rayguan97/M3DETR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Synergistic Attention for Light Field Salient Object Detection. (arXiv:2104.13916v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.13916">
<div class="article-summary-box-inner">
<span><p>We propose a novel Synergistic Attention Network (SA-Net) to address the
light field salient object detection by establishing a synergistic effect
between multi-modal features with advanced attention mechanisms. Our SA-Net
exploits the rich information of focal stacks via 3D convolutional neural
networks, decodes the high-level features of multi-modal light field data with
two cascaded synergistic attention modules, and predicts the saliency map using
an effective feature fusion module in a progressive manner. Extensive
experiments on three widely-used benchmark datasets show that our SA-Net
outperforms 28 state-of-the-art models, sufficiently demonstrating its
effectiveness and superiority. Our code is available at
https://github.com/PanoAsh/SA-Net.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Modal Generative Augmentation for Visual Question Answering. (arXiv:2105.04780v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.04780">
<div class="article-summary-box-inner">
<span><p>Data augmentation has been shown to effectively improve the performance of
multimodal machine learning models. This paper introduces a generative model
for data augmentation by leveraging the correlations among multiple modalities.
Different from conventional data augmentation approaches that apply low-level
operations with deterministic heuristics, our method learns a generator that
generates samples of the target modality conditioned on observed modalities in
the variational auto-encoder framework. Additionally, the proposed model is
able to quantify the confidence of augmented data by its generative
probability, and can be jointly optimised with a downstream task. Experiments
on Visual Question Answering as downstream task demonstrate the effectiveness
of the proposed generative model, which is able to improve strong UpDn-based
models to achieve state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LGPMA: Complicated Table Structure Recognition with Local and Global Pyramid Mask Alignment. (arXiv:2105.06224v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.06224">
<div class="article-summary-box-inner">
<span><p>Table structure recognition is a challenging task due to the various
structures and complicated cell spanning relations. Previous methods handled
the problem starting from elements in different granularities (rows/columns,
text regions), which somehow fell into the issues like lossy heuristic rules or
neglect of empty cell division. Based on table structure characteristics, we
find that obtaining the aligned bounding boxes of text region can effectively
maintain the entire relevant range of different cells. However, the aligned
bounding boxes are hard to be accurately predicted due to the visual
ambiguities. In this paper, we aim to obtain more reliable aligned bounding
boxes by fully utilizing the visual information from both text regions in
proposed local features and cell relations in global features. Specifically, we
propose the framework of Local and Global Pyramid Mask Alignment, which adopts
the soft pyramid mask learning mechanism in both the local and global feature
maps. It allows the predicted boundaries of bounding boxes to break through the
limitation of original proposals. A pyramid mask re-scoring module is then
integrated to compromise the local and global information and refine the
predicted boundaries. Finally, we propose a robust table structure recovery
pipeline to obtain the final structure, in which we also effectively solve the
problems of empty cells locating and division. Experimental results show that
the proposed method achieves competitive and even new state-of-the-art
performance on several public benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reciprocal Feature Learning via Explicit and Implicit Tasks in Scene Text Recognition. (arXiv:2105.06229v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.06229">
<div class="article-summary-box-inner">
<span><p>Text recognition is a popular topic for its broad applications. In this work,
we excavate the implicit task, character counting within the traditional text
recognition, without additional labor annotation cost. The implicit task plays
as an auxiliary branch for complementing the sequential recognition. We design
a two-branch reciprocal feature learning framework in order to adequately
utilize the features from both the tasks. Through exploiting the complementary
effect between explicit and implicit tasks, the feature is reliably enhanced.
Extensive experiments on 7 benchmarks show the advantages of the proposed
methods in both text recognition and the new-built character counting tasks. In
addition, it is convenient yet effective to equip with variable networks and
tasks. We offer abundant ablation studies, generalizing experiments with deeper
understanding on the tasks. Code is available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Boombox: Visual Reconstruction from Acoustic Vibrations. (arXiv:2105.08052v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.08052">
<div class="article-summary-box-inner">
<span><p>Interacting with bins and containers is a fundamental task in robotics,
making state estimation of the objects inside the bin critical. While robots
often use cameras for state estimation, the visual modality is not always ideal
due to occlusions and poor illumination. We introduce The Boombox, a container
that uses sound to estimate the state of the contents inside a box. Based on
the observation that the collision between objects and its containers will
cause an acoustic vibration, we present a convolutional network for learning to
reconstruct visual scenes. Although we use low-cost and low-power contact
microphones to detect the vibrations, our results show that learning from
multimodal data enables state estimation from affordable audio sensors. Due to
the many ways that robots use containers, we believe the box will have a number
of applications in robotics. Our project website is at: boombox.cs.columbia.edu
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SSCAP: Self-supervised Co-occurrence Action Parsing for Unsupervised Temporal Action Segmentation. (arXiv:2105.14158v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14158">
<div class="article-summary-box-inner">
<span><p>Temporal action segmentation is a task to classify each frame in the video
with an action label. However, it is quite expensive to annotate every frame in
a large corpus of videos to construct a comprehensive supervised training
dataset. Thus in this work we propose an unsupervised method, namely SSCAP,
that operates on a corpus of unlabeled videos and predicts a likely set of
temporal segments across the videos. SSCAP leverages Self-Supervised learning
to extract distinguishable features and then applies a novel Co-occurrence
Action Parsing algorithm to not only capture the correlation among sub-actions
underlying the structure of activities, but also estimate the temporal path of
the sub-actions in an accurate and general way. We evaluate on both classic
datasets (Breakfast, 50Salads) and the emerging fine-grained action dataset
(FineGym) with more complex activity structures and similar sub-actions.
Results show that SSCAP achieves state-of-the-art performance on all datasets
and can even outperform some weakly-supervised approaches, demonstrating its
effectiveness and generalizability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Less is More: Pay Less Attention in Vision Transformers. (arXiv:2105.14217v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14217">
<div class="article-summary-box-inner">
<span><p>Transformers have become one of the dominant architectures in deep learning,
particularly as a powerful alternative to convolutional neural networks (CNNs)
in computer vision. However, Transformer training and inference in previous
works can be prohibitively expensive due to the quadratic complexity of
self-attention over a long sequence of representations, especially for
high-resolution dense prediction tasks. To this end, we present a novel Less
attention vIsion Transformer (LIT), building upon the fact that the early
self-attention layers in Transformers still focus on local patterns and bring
minor benefits in recent hierarchical vision Transformers. Specifically, we
propose a hierarchical Transformer where we use pure multi-layer perceptrons
(MLPs) to encode rich local patterns in the early stages while applying
self-attention modules to capture longer dependencies in deeper layers.
Moreover, we further propose a learned deformable token merging module to
adaptively fuse informative patches in a non-uniform manner. The proposed LIT
achieves promising performance on image recognition tasks, including image
classification, object detection and instance segmentation, serving as a strong
backbone for many vision tasks. Code is available at:
https://github.com/MonashAI/LIT
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhanced Isotropy Maximization Loss: Seamless and High-Performance Out-of-Distribution Detection Simply Replacing the SoftMax Loss. (arXiv:2105.14399v8 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14399">
<div class="article-summary-box-inner">
<span><p>Current out-of-distribution detection approaches usually present special
requirements (e.g., collecting outlier data and hyperparameter validation) and
produce side effects (e.g., classification accuracy drop and slow/inefficient
inferences). Recently, entropic out-of-distribution detection has been proposed
as a seamless approach (i.e., a solution that avoids all previously mentioned
drawbacks). The entropic out-of-distribution detection solution uses the IsoMax
loss for training and the entropic score for out-of-distribution detection. The
IsoMax loss works as a drop-in replacement of the SoftMax loss (i.e., the
combination of the output linear layer, the SoftMax activation, and the
cross-entropy loss) because swapping the SoftMax loss with the IsoMax loss
requires no changes in the model's architecture or training hyperparameters. In
this paper, we perform what we call an isometrization of the distances used in
the IsoMax loss. Additionally, we propose replacing the entropic score with the
minimum distance score. Experiments showed that these simple modifications
significantly increase out-of-distribution detection performance while keeping
the solution seamless. Besides being competitive with or outperforming all
major current approaches, the proposed solution avoids all their current
limitations in addition to being much easier to use because only a simple loss
replacement for training the neural network is required. The code to replace
the SoftMax loss with the IsoMax+ loss and reproduce the results is available
https://github.com/dlmacedo/entropic-out-of-distribution-detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Aligning Pretraining for Detection via Object-Level Contrastive Learning. (arXiv:2106.02637v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02637">
<div class="article-summary-box-inner">
<span><p>Image-level contrastive representation learning has proven to be highly
effective as a generic model for transfer learning. Such generality for
transfer learning, however, sacrifices specificity if we are interested in a
certain downstream task. We argue that this could be sub-optimal and thus
advocate a design principle which encourages alignment between the
self-supervised pretext task and the downstream task. In this paper, we follow
this principle with a pretraining method specifically designed for the task of
object detection. We attain alignment in the following three aspects: 1)
object-level representations are introduced via selective search bounding boxes
as object proposals; 2) the pretraining network architecture incorporates the
same dedicated modules used in the detection pipeline (e.g. FPN); 3) the
pretraining is equipped with object detection properties such as object-level
translation invariance and scale invariance. Our method, called Selective
Object COntrastive learning (SoCo), achieves state-of-the-art results for
transfer performance on COCO detection using a Mask R-CNN framework. Code is
available at https://github.com/hologerry/SoCo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Combinatorial Optimization for Panoptic Segmentation: A Fully Differentiable Approach. (arXiv:2106.03188v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03188">
<div class="article-summary-box-inner">
<span><p>We propose a fully differentiable architecture for simultaneous semantic and
instance segmentation (a.k.a. panoptic segmentation) consisting of a
convolutional neural network and an asymmetric multiway cut problem solver. The
latter solves a combinatorial optimization problem that elegantly incorporates
semantic and boundary predictions to produce a panoptic labeling. Our
formulation allows to directly maximize a smooth surrogate of the panoptic
quality metric by backpropagating the gradient through the optimization
problem. Experimental evaluation shows improvement by backpropagating through
the optimization problem w.r.t. comparable approaches on Cityscapes and COCO
datasets. Overall, our approach shows the utility of using combinatorial
optimization in tandem with deep learning in a challenging large scale
real-world problem and showcases benefits and insights into training such an
architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Semantic Hallucination for Domain Generalized Semantic Segmentation. (arXiv:2106.04144v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04144">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks may perform poorly when the test and train data
are from different domains. While this problem can be mitigated by using the
target domain data to align the source and target domain feature
representations, the target domain data may be unavailable due to privacy
concerns. Consequently, there is a need for methods that generalize well
without access to target domain data during training. In this work, we propose
an adversarial hallucination approach, which combines a class-wise
hallucination module and a semantic segmentation module. Since the segmentation
performance varies across different classes, we design a semantic-conditioned
style hallucination layer to adaptively stylize each class. The classwise
stylization parameters are generated from the semantic knowledge in the
segmentation probability maps of the source domain image. Both modules compete
adversarially, with the hallucination module generating increasingly
'difficult' style images to challenge the segmentation module. In response, the
segmentation module improves its performance as it is trained with generated
samples at an appropriate class-wise difficulty level. Experiments on state of
the art domain adaptation work demonstrate the efficacy of our proposed method
when no target domain data are available for training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chasing Sparsity in Vision Transformers: An End-to-End Exploration. (arXiv:2106.04533v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04533">
<div class="article-summary-box-inner">
<span><p>Vision transformers (ViTs) have recently received explosive popularity, but
their enormous model sizes and training costs remain daunting. Conventional
post-training pruning often incurs higher training budgets. In contrast, this
paper aims to trim down both the training memory overhead and the inference
complexity, without sacrificing the achievable accuracy. We carry out the
first-of-its-kind comprehensive exploration, on taking a unified approach of
integrating sparsity in ViTs "from end to end". Specifically, instead of
training full ViTs, we dynamically extract and train sparse subnetworks, while
sticking to a fixed small parameter budget. Our approach jointly optimizes
model parameters and explores connectivity throughout training, ending up with
one sparse network as the final output. The approach is seamlessly extended
from unstructured to structured sparsity, the latter by considering to guide
the prune-and-grow of self-attention heads inside ViTs. We further co-explore
data and architecture sparsity for additional efficiency gains by plugging in a
novel learnable token selector to adaptively determine the currently most vital
patches. Extensive results on ImageNet with diverse ViT backbones validate the
effectiveness of our proposals which obtain significantly reduced computational
cost and almost unimpaired generalization. Perhaps most surprisingly, we find
that the proposed sparse (co-)training can sometimes improve the ViT accuracy
rather than compromising it, making sparsity a tantalizing "free lunch". For
example, our sparsified DeiT-Small at (5%, 50%) sparsity for (data,
architecture), improves 0.28% top-1 accuracy, and meanwhile enjoys 49.32% FLOPs
and 4.40% running time savings. Our codes are available at
https://github.com/VITA-Group/SViTE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Keeping Your Eye on the Ball: Trajectory Attention in Video Transformers. (arXiv:2106.05392v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05392">
<div class="article-summary-box-inner">
<span><p>In video transformers, the time dimension is often treated in the same way as
the two spatial dimensions. However, in a scene where objects or the camera may
move, a physical point imaged at one location in frame $t$ may be entirely
unrelated to what is found at that location in frame $t+k$. These temporal
correspondences should be modeled to facilitate learning about dynamic scenes.
To this end, we propose a new drop-in block for video transformers --
trajectory attention -- that aggregates information along implicitly determined
motion paths. We additionally propose a new method to address the quadratic
dependence of computation and memory on the input size, which is particularly
important for high resolution or long videos. While these ideas are useful in a
range of settings, we apply them to the specific task of video action
recognition with a transformer model and obtain state-of-the-art results on the
Kinetics, Something--Something V2, and Epic-Kitchens datasets. Code and models
are available at: https://github.com/facebookresearch/Motionformer
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MST: Masked Self-Supervised Transformer for Visual Representation. (arXiv:2106.05656v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05656">
<div class="article-summary-box-inner">
<span><p>Transformer has been widely used for self-supervised pre-training in Natural
Language Processing (NLP) and achieved great success. However, it has not been
fully explored in visual self-supervised learning. Meanwhile, previous methods
only consider the high-level feature and learning representation from a global
perspective, which may fail to transfer to the downstream dense prediction
tasks focusing on local features. In this paper, we present a novel Masked
Self-supervised Transformer approach named MST, which can explicitly capture
the local context of an image while preserving the global semantic information.
Specifically, inspired by the Masked Language Modeling (MLM) in NLP, we propose
a masked token strategy based on the multi-head self-attention map, which
dynamically masks some tokens of local patches without damaging the crucial
structure for self-supervised learning. More importantly, the masked tokens
together with the remaining tokens are further recovered by a global image
decoder, which preserves the spatial information of the image and is more
friendly to the downstream dense prediction tasks. The experiments on multiple
datasets demonstrate the effectiveness and generality of the proposed method.
For instance, MST achieves Top-1 accuracy of 76.9% with DeiT-S only using
300-epoch pre-training by linear evaluation, which outperforms supervised
methods with the same epoch by 0.4% and its comparable variant DINO by 1.0\%.
For dense prediction tasks, MST also achieves 42.7% mAP on MS COCO object
detection and 74.04% mIoU on Cityscapes segmentation only with 100-epoch
pre-training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Partial success in closing the gap between human and machine vision. (arXiv:2106.07411v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07411">
<div class="article-summary-box-inner">
<span><p>A few years ago, the first CNN surpassed human performance on ImageNet.
However, it soon became clear that machines lack robustness on more challenging
test cases, a major obstacle towards deploying machines "in the wild" and
towards obtaining better computational models of human visual perception. Here
we ask: Are we making progress in closing the gap between human and machine
vision? To answer this question, we tested human observers on a broad range of
out-of-distribution (OOD) datasets, recording 85,120 psychophysical trials
across 90 participants. We then investigated a range of promising machine
learning developments that crucially deviate from standard supervised CNNs
along three axes: objective function (self-supervised, adversarially trained,
CLIP language-image training), architecture (e.g. vision transformers), and
dataset size (ranging from 1M to 1B).
</p>
<p>Our findings are threefold. (1.) The longstanding distortion robustness gap
between humans and CNNs is closing, with the best models now exceeding human
feedforward performance on most of the investigated OOD datasets. (2.) There is
still a substantial image-level consistency gap, meaning that humans make
different errors than models. In contrast, most models systematically agree in
their categorisation errors, even substantially different ones like contrastive
self-supervised vs. standard supervised models. (3.) In many cases,
human-to-model consistency improves when training dataset size is increased by
one to three orders of magnitude. Our results give reason for cautious
optimism: While there is still much room for improvement, the behavioural
difference between human and machine vision is narrowing. In order to measure
future progress, 17 OOD datasets with image-level human behavioural data and
evaluation code are provided as a toolbox and benchmark at:
https://github.com/bethgelab/model-vs-human/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FastAno: Fast Anomaly Detection via Spatio-temporal Patch Transformation. (arXiv:2106.08613v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.08613">
<div class="article-summary-box-inner">
<span><p>Video anomaly detection has gained significant attention due to the
increasing requirements of automatic monitoring for surveillance videos.
Especially, the prediction based approach is one of the most studied methods to
detect anomalies by predicting frames that include abnormal events in the test
set after learning with the normal frames of the training set. However, a lot
of prediction networks are computationally expensive owing to the use of
pre-trained optical flow networks, or fail to detect abnormal situations
because of their strong generative ability to predict even the anomalies. To
address these shortcomings, we propose spatial rotation transformation (SRT)
and temporal mixing transformation (TMT) to generate irregular patch cuboids
within normal frame cuboids in order to enhance the learning of normal
features. Additionally, the proposed patch transformation is used only during
the training phase, allowing our model to detect abnormal frames at fast speed
during inference. Our model is evaluated on three anomaly detection benchmarks,
achieving competitive accuracy and surpassing all the previous works in terms
of speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Anytime Learning at Macroscale. (arXiv:2106.09563v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.09563">
<div class="article-summary-box-inner">
<span><p>Classical machine learning frameworks assume access to a possibly large
dataset in order to train a predictive model. In many practical applications
however, data does not arrive all at once, but in batches over time. This
creates a natural trade-off between accuracy of a model and time to obtain such
a model. A greedy predictor could produce non-trivial predictions by
immediately training on batches as soon as these become available but, it may
also make suboptimal use of future data. On the other hand, a tardy predictor
could wait for a long time to aggregate several batches into a larger dataset,
but ultimately deliver a much better performance. In this work, we consider
such a streaming learning setting, which we dub anytime learning at macroscale}
(ALMA). It is an instance of anytime learning applied not at the level of a
single chunk of data, but at the level of the entire sequence of large batches.
We first formalize this learning setting, we then introduce metrics to assess
how well learners perform on the given task for a given memory and compute
budget, and finally we test about thirty baseline approaches on three standard
benchmarks repurposed for anytime learning at macroscale. Our findings indicate
that no model strikes the best trade-off across the board. While replay-based
methods attain the lowest error rate, they also incur in a 5 to 10 times
increase of compute. Approaches that grow capacity over time do offer better
scaling in terms of training flops, but they also underperform simpler
ensembling methods in terms of error rate. Overall, ALMA offers both a good
abstraction of the typical learning setting faced everyday by practitioners,
and a set of unsolved modeling problems for those interested in efficient
learning of dynamic models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse Training via Boosting Pruning Plasticity with Neuroregeneration. (arXiv:2106.10404v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.10404">
<div class="article-summary-box-inner">
<span><p>Works on lottery ticket hypothesis (LTH) and single-shot network pruning
(SNIP) have raised a lot of attention currently on post-training pruning
(iterative magnitude pruning), and before-training pruning (pruning at
initialization). The former method suffers from an extremely large computation
cost and the latter usually struggles with insufficient performance. In
comparison, during-training pruning, a class of pruning methods that
simultaneously enjoys the training/inference efficiency and the comparable
performance, temporarily, has been less explored. To better understand
during-training pruning, we quantitatively study the effect of pruning
throughout training from the perspective of pruning plasticity (the ability of
the pruned networks to recover the original performance). Pruning plasticity
can help explain several other empirical observations about neural network
pruning in literature. We further find that pruning plasticity can be
substantially improved by injecting a brain-inspired mechanism called
neuroregeneration, i.e., to regenerate the same number of connections as
pruned. We design a novel gradual magnitude pruning (GMP) method, named gradual
pruning with zero-cost neuroregeneration (\textbf{GraNet}), that advances state
of the art. Perhaps most impressively, its sparse-to-sparse version for the
first time boosts the sparse-to-sparse training performance over various
dense-to-sparse methods with ResNet-50 on ImageNet without extending the
training time. We release all codes in
https://github.com/Shiweiliuiiiiiii/GraNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Million Scenes for Autonomous Driving: ONCE Dataset. (arXiv:2106.11037v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.11037">
<div class="article-summary-box-inner">
<span><p>Current perception models in autonomous driving have become notorious for
greatly relying on a mass of annotated data to cover unseen cases and address
the long-tail problem. On the other hand, learning from unlabeled large-scale
collected data and incrementally self-training powerful recognition models have
received increasing attention and may become the solutions of next-generation
industry-level powerful and robust perception models in autonomous driving.
However, the research community generally suffered from data inadequacy of
those essential real-world scene data, which hampers the future exploration of
fully/semi/self-supervised methods for 3D perception. In this paper, we
introduce the ONCE (One millioN sCenEs) dataset for 3D object detection in the
autonomous driving scenario. The ONCE dataset consists of 1 million LiDAR
scenes and 7 million corresponding camera images. The data is selected from 144
driving hours, which is 20x longer than the largest 3D autonomous driving
dataset available (e.g. nuScenes and Waymo), and it is collected across a range
of different areas, periods and weather conditions. To facilitate future
research on exploiting unlabeled data for 3D detection, we additionally provide
a benchmark in which we reproduce and evaluate a variety of self-supervised and
semi-supervised methods on the ONCE dataset. We conduct extensive analyses on
those methods and provide valuable observations on their performance related to
the scale of used data. Data, code, and more information are available at
https://once-for-auto-driving.github.io/index.html.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Image is Worth More Than a Thousand Words: Towards Disentanglement in the Wild. (arXiv:2106.15610v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.15610">
<div class="article-summary-box-inner">
<span><p>Unsupervised disentanglement has been shown to be theoretically impossible
without inductive biases on the models and the data. As an alternative
approach, recent methods rely on limited supervision to disentangle the factors
of variation and allow their identifiability. While annotating the true
generative factors is only required for a limited number of observations, we
argue that it is infeasible to enumerate all the factors of variation that
describe a real-world image distribution. To this end, we propose a method for
disentangling a set of factors which are only partially labeled, as well as
separating the complementary set of residual factors that are never explicitly
specified. Our success in this challenging setting, demonstrated on synthetic
benchmarks, gives rise to leveraging off-the-shelf image descriptors to
partially annotate a subset of attributes in real image domains (e.g. of human
faces) with minimal manual effort. Specifically, we use a recent language-image
embedding model (CLIP) to annotate a set of attributes of interest in a
zero-shot manner and demonstrate state-of-the-art disentangled image
manipulation results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synthetic Data Are as Good as the Real for Association Knowledge Learning in Multi-object Tracking. (arXiv:2106.16100v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.16100">
<div class="article-summary-box-inner">
<span><p>Association, aiming to link bounding boxes of the same identity in a video
sequence, is a central component in multi-object tracking (MOT). To train
association modules, e.g., parametric networks, real video data are usually
used. However, annotating person tracks in consecutive video frames is
expensive, and such real data, due to its inflexibility, offer us limited
opportunities to evaluate the system performance w.r.t changing tracking
scenarios. In this paper, we study whether 3D synthetic data can replace
real-world videos for association training. Specifically, we introduce a
large-scale synthetic data engine named MOTX, where the motion characteristics
of cameras and objects are manually configured to be similar to those in
real-world datasets. We show that compared with real data, association
knowledge obtained from synthetic data can achieve very similar performance on
real-world test sets without domain adaption techniques. Our intriguing
observation is credited to two factors. First and foremost, 3D engines can well
simulate motion factors such as camera movement, camera view and object
movement, so that the simulated videos can provide association modules with
effective motion features. Second, experimental results show that the
appearance domain gap hardly harms the learning of association knowledge. In
addition, the strong customization ability of MOTX allows us to quantitatively
assess the impact of motion factors on MOT, which brings new insights to the
community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sanity Checks for Lottery Tickets: Does Your Winning Ticket Really Win the Jackpot?. (arXiv:2107.00166v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.00166">
<div class="article-summary-box-inner">
<span><p>There have been long-standing controversies and inconsistencies over the
experiment setup and criteria for identifying the "winning ticket" in
literature. To reconcile such, we revisit the definition of lottery ticket
hypothesis, with comprehensive and more rigorous conditions. Under our new
definition, we show concrete evidence to clarify whether the winning ticket
exists across the major DNN architectures and/or applications. Through
extensive experiments, we perform quantitative analysis on the correlations
between winning tickets and various experimental factors, and empirically study
the patterns of our observations. We find that the key training
hyperparameters, such as learning rate and training epochs, as well as the
architecture characteristics such as capacities and residual connections, are
all highly correlated with whether and when the winning tickets can be
identified. Based on our analysis, we summarize a guideline for parameter
settings in regards of specific architecture characteristics, which we hope to
catalyze the research progress on the topic of lottery ticket hypothesis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hybrid Supervision Learning for Pathology Whole Slide Image Classification. (arXiv:2107.00934v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.00934">
<div class="article-summary-box-inner">
<span><p>Weak supervision learning on classification labels has demonstrated high
performance in various tasks, while a few pixel-level fine annotations are also
affordable. Naturally a question comes to us that whether the combination of
pixel-level (e.g., segmentation) and image level (e.g., classification)
annotation can introduce further improvement. However in computational
pathology this is a difficult task for this reason: High resolution of whole
slide images makes it difficult to do end-to-end classification model training,
which is challenging to research of weak or hybrid supervision learning in the
past. To handle this problem, we propose a hybrid supervision learning
framework for this kind of high resolution images with sufficient image-level
coarse annotations and a few pixel-level fine labels. This framework, when
applied in training patch model, can carefully make use of coarse image-level
labels to refine generated pixel-level pseudo labels. Complete strategy is
proposed to suppress pixel-level false positives and false negatives. A large
hybrid annotated dataset is used to evaluate the effectiveness of hybrid
supervision learning. By extracting pixel-level pseudo labels in initially
image-level labeled samples, we achieve 5.2% higher specificity than purely
training on existing labels while retaining 100% sensitivity, in the task of
image-level classification to be positive or negative.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inter-intra Variant Dual Representations forSelf-supervised Video Recognition. (arXiv:2107.01194v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.01194">
<div class="article-summary-box-inner">
<span><p>Contrastive learning applied to self-supervised representation learning has
seen a resurgence in deep models. In this paper, we find that existing
contrastive learning based solutions for self-supervised video recognition
focus on inter-variance encoding but ignore the intra-variance existing in
clips within the same video. We thus propose to learn dual representations for
each clip which (\romannumeral 1) encode intra-variance through a shuffle-rank
pretext task; (\romannumeral 2) encode inter-variance through a temporal
coherent contrastive loss. Experiment results show that our method plays an
essential role in balancing inter and intra variances and brings consistent
performance gains on multiple backbones and contrastive learning frameworks.
Integrated with SimCLR and pretrained on Kinetics-400, our method achieves
$\textbf{82.0\%}$ and $\textbf{51.2\%}$ downstream classification accuracy on
UCF101 and HMDB51 test sets respectively and $\textbf{46.1\%}$ video retrieval
accuracy on UCF101, outperforming both pretext-task based and contrastive
learning based counterparts. Our code is available at
\href{https://github.com/lzhangbj/DualVar}{https://github.com/lzhangbj/DualVar}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Object Behavioral Feature Extraction for Potential Risk Analysis based on Video Sensor. (arXiv:2107.03554v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.03554">
<div class="article-summary-box-inner">
<span><p>Pedestrians are exposed to risk of death or serious injuries on roads,
especially unsignalized crosswalks, for a variety of reasons. To date, an
extensive variety of studies have reported on vision based traffic safety
system. However, many studies required manual inspection of the volumes of
traffic video to reliably obtain traffic related objects behavioral factors. In
this paper, we propose an automated and simpler system for effectively
extracting object behavioral features from video sensors deployed on the road.
We conduct basic statistical analysis on these features, and show how they can
be useful for monitoring the traffic behavior on the road. We confirm the
feasibility of the proposed system by applying our prototype to two
unsignalized crosswalks in Osan city, South Korea. To conclude, we compare
behaviors of vehicles and pedestrians in those two areas by simple statistical
analysis. This study demonstrates the potential for a network of connected
video sensors to provide actionable data for smart cities to improve pedestrian
safety in dangerous road environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HDMapNet: A Local Semantic Map Learning and Evaluation Framework. (arXiv:2107.06307v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.06307">
<div class="article-summary-box-inner">
<span><p>Estimating local semantics from sensory inputs is a central component for
high-definition map constructions in autonomous driving. However, traditional
pipelines require a vast amount of human efforts and resources in annotating
and maintaining the semantics in the map, which limits its scalability. In this
paper, we introduce the problem of local semantic map learning, which
dynamically constructs the vectorized semantics based on onboard sensor
observations. Meanwhile, we introduce a local semantic map learning method,
dubbed HDMapNet. HDMapNet encodes image features from surrounding cameras
and/or point clouds from LiDAR, and predicts vectorized map elements in the
bird's-eye view. We benchmark HDMapNet on nuScenes dataset and show that in all
settings, it performs better than baseline methods. Of note, our fusion-based
HDMapNet outperforms existing methods by more than 50% in all metrics. In
addition, we develop semantic-level and instance-level metrics to evaluate the
map learning performance. Finally, we showcase our method is capable of
predicting a locally consistent map. By introducing the method and metrics, we
invite the community to study this novel map learning problem. Code and
evaluation kit will be released to facilitate future development.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FetalNet: Multi-task deep learning framework for fetal ultrasound biometric measurements. (arXiv:2107.06943v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.06943">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose an end-to-end multi-task neural network called
FetalNet with an attention mechanism and stacked module for spatio-temporal
fetal ultrasound scan video analysis. Fetal biometric measurement is a standard
examination during pregnancy used for the fetus growth monitoring and
estimation of gestational age and fetal weight. The main goal in fetal
ultrasound scan video analysis is to find proper standard planes to measure the
fetal head, abdomen and femur. Due to natural high speckle noise and shadows in
ultrasound data, medical expertise and sonographic experience are required to
find the appropriate acquisition plane and perform accurate measurements of the
fetus. In addition, existing computer-aided methods for fetal US biometric
measurement address only one single image frame without considering temporal
features. To address these shortcomings, we propose an end-to-end multi-task
neural network for spatio-temporal ultrasound scan video analysis to
simultaneously localize, classify and measure the fetal body parts. We propose
a new encoder-decoder segmentation architecture that incorporates a
classification branch. Additionally, we employ an attention mechanism with a
stacked module to learn salient maps to suppress irrelevant US regions and
efficient scan plane localization. We trained on the fetal ultrasound video
comes from routine examinations of 700 different patients. Our method called
FetalNet outperforms existing state-of-the-art methods in both classification
and segmentation in fetal ultrasound video recordings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Flood Segmentation on Sentinel-1 SAR Imagery with Semi-Supervised Learning. (arXiv:2107.08369v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.08369">
<div class="article-summary-box-inner">
<span><p>Floods wreak havoc throughout the world, causing billions of dollars in
damages, and uprooting communities, ecosystems and economies. The NASA Impact
Flood Detection competition tasked participants with predicting flooded pixels
after training with synthetic aperture radar (SAR) images in a supervised
setting. We propose a semi-supervised learning pseudo-labeling scheme that
derives confidence estimates from U-Net ensembles, progressively improving
accuracy. Concretely, we use a cyclical approach involving multiple stages (1)
training an ensemble model of multiple U-Net architectures with the provided
high confidence hand-labeled data and, generated pseudo labels or low
confidence labels on the entire unlabeled test dataset, and then, (2) filter
out quality generated labels and, (3) combine the generated labels with the
previously available high confidence hand-labeled dataset. This assimilated
dataset is used for the next round of training ensemble models and the cyclical
process is repeated until the performance improvement plateaus. We post process
our results with Conditional Random Fields. Our approach sets a new
state-of-the-art on the Sentinel-1 dataset with 0.7654 IoU, an impressive
improvement over the 0.60 IoU baseline. Our method, which we release with all
the code and models, can also be used as an open science benchmark for the
Sentinel-1 dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ANFIC: Image Compression Using Augmented Normalizing Flows. (arXiv:2107.08470v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.08470">
<div class="article-summary-box-inner">
<span><p>This paper introduces an end-to-end learned image compression system, termed
ANFIC, based on Augmented Normalizing Flows (ANF). ANF is a new type of flow
model, which stacks multiple variational autoencoders (VAE) for greater model
expressiveness. The VAE-based image compression has gone mainstream, showing
promising compression performance. Our work presents the first attempt to
leverage VAE-based compression in a flow-based framework. ANFIC advances
further compression efficiency by stacking and extending hierarchically
multiple VAE's. The invertibility of ANF, together with our training
strategies, enables ANFIC to support a wide range of quality levels without
changing the encoding and decoding networks. Extensive experimental results
show that in terms of PSNR-RGB, ANFIC performs comparably to or better than the
state-of-the-art learned image compression. Moreover, it performs close to VVC
intra coding, from low-rate compression up to nearly-lossless compression. In
particular, ANFIC achieves the state-of-the-art performance, when extended with
conditional convolution for variable rate compression with a single model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Proximal Unrolling Network for Compressive Imaging. (arXiv:2107.11007v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.11007">
<div class="article-summary-box-inner">
<span><p>Compressive imaging aims to recover a latent image from under-sampled
measurements, suffering from a serious ill-posed inverse problem. Recently,
deep neural networks have been applied to this problem with superior results,
owing to the learned advanced image priors. These approaches, however, require
training separate models for different imaging modalities and sampling ratios,
leading to overfitting to specific settings. In this paper, a dynamic proximal
unrolling network (dubbed DPUNet) was proposed, which can handle a variety of
measurement matrices via one single model without retraining. Specifically,
DPUNet can exploit both the embedded observation model via gradient descent and
imposed image priors by learned dynamic proximal operators, achieving joint
reconstruction. A key component of DPUNet is a dynamic proximal mapping module,
whose parameters can be dynamically adjusted at the inference stage and make it
adapt to different imaging settings. Experimental results demonstrate that the
proposed DPUNet can effectively handle multiple compressive imaging modalities
under varying sampling ratios and noise levels via only one trained model, and
outperform the state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridging Gap between Image Pixels and Semantics via Supervision: A Survey. (arXiv:2107.13757v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.13757">
<div class="article-summary-box-inner">
<span><p>The fact that there exists a gap between low-level features and semantic
meanings of images, called the semantic gap, is known for decades. Resolution
of the semantic gap is a long standing problem. The semantic gap problem is
reviewed and a survey on recent efforts in bridging the gap is made in this
work. Most importantly, we claim that the semantic gap is primarily bridged
through supervised learning today. Experiences are drawn from two application
domains to illustrate this point: 1) object detection and 2) metric learning
for content-based image retrieval (CBIR). To begin with, this paper offers a
historical retrospective on supervision, makes a gradual transition to the
modern data-driven methodology and introduces commonly used datasets. Then, it
summarizes various supervision methods to bridge the semantic gap in the
context of object detection and metric learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Road Scenes Segmentation Across Different Domains by Disentangling Latent Representations. (arXiv:2108.03021v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03021">
<div class="article-summary-box-inner">
<span><p>Deep learning models obtain impressive accuracy in road scenes understanding,
however they need a large quantity of labeled samples for their training.
Additionally, such models do not generalize well to environments where the
statistical properties of data do not perfectly match those of training scenes,
and this can be a significant problem for intelligent vehicles. Hence, domain
adaptation approaches have been introduced to transfer knowledge acquired on a
label-abundant source domain to a related label-scarce target domain. In this
work, we design and carefully analyze multiple latent space-shaping
regularization strategies that work together to reduce the domain shift. More
in detail, we devise a feature clustering strategy to increase domain
alignment, a feature perpendicularity constraint to space apart features
belonging to different semantic classes, including those not present in the
current batch, and a feature norm alignment strategy to separate active and
inactive channels. In addition, we propose a novel evaluation metric to capture
the relative performance of an adapted model with respect to supervised
training. We validate our framework in driving scenarios, considering both
synthetic-to-real and real-to-real adaptation, outperforming previous
feature-level state-of-the-art methods on multiple road scenes benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distributional Depth-Based Estimation of Object Articulation Models. (arXiv:2108.05875v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.05875">
<div class="article-summary-box-inner">
<span><p>We propose a method that efficiently learns distributions over articulation
model parameters directly from depth images without the need to know
articulation model categories a priori. By contrast, existing methods that
learn articulation models from raw observations typically only predict point
estimates of the model parameters, which are insufficient to guarantee the safe
manipulation of articulated objects. Our core contributions include a novel
representation for distributions over rigid body transformations and
articulation model parameters based on screw theory, von Mises-Fisher
distributions, and Stiefel manifolds. Combining these concepts allows for an
efficient, mathematically sound representation that implicitly satisfies the
constraints that rigid body transformations and articulations must adhere to.
Leveraging this representation, we introduce a novel deep learning based
approach, DUST-net, that performs category-independent articulation model
estimation while also providing model uncertainties. We evaluate our approach
on several benchmarking datasets and real-world objects and compare its
performance with two current state-of-the-art methods. Our results demonstrate
that DUST-net can successfully learn distributions over articulation models for
novel objects across articulation model categories, which generate point
estimates with better accuracy than state-of-the-art methods and effectively
capture the uncertainty over predicted model parameters due to noisy inputs.
Project webpage: https://pearl-utexas.github.io/DUST-net/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FedPara: Low-Rank Hadamard Product for Communication-Efficient Federated Learning. (arXiv:2108.06098v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06098">
<div class="article-summary-box-inner">
<span><p>In this work, we propose a communication-efficient parameterization, FedPara,
for federated learning (FL) to overcome the burdens on frequent model uploads
and downloads. Our method re-parameterizes weight parameters of layers using
low-rank weights followed by the Hadamard product. Compared to the conventional
low-rank parameterization, our FedPara method is not restricted to low-rank
constraints, and thereby it has a far larger capacity. This property enables to
achieve comparable performance while requiring 3 to 10 times lower
communication costs than the model with the original layers, which is not
achievable by the traditional low-rank methods. The efficiency of our method
can be further improved by combining with other efficient FL optimizers. In
addition, we extend our method to a personalized FL application, pFedPara,
which separates parameters into global and local ones. We show that pFedPara
outperforms competing personalized FL methods with more than three times fewer
parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Anchored Unsigned Distance Functions with Gradient Direction Alignment for Single-view Garment Reconstruction. (arXiv:2108.08478v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08478">
<div class="article-summary-box-inner">
<span><p>While single-view 3D reconstruction has made significant progress benefiting
from deep shape representations in recent years, garment reconstruction is
still not solved well due to open surfaces, diverse topologies and complex
geometric details. In this paper, we propose a novel learnable Anchored
Unsigned Distance Function (AnchorUDF) representation for 3D garment
reconstruction from a single image. AnchorUDF represents 3D shapes by
predicting unsigned distance fields (UDFs) to enable open garment surface
modeling at arbitrary resolution. To capture diverse garment topologies,
AnchorUDF not only computes pixel-aligned local image features of query points,
but also leverages a set of anchor points located around the surface to enrich
3D position features for query points, which provides stronger 3D space context
for the distance function. Furthermore, in order to obtain more accurate point
projection direction at inference, we explicitly align the spatial gradient
direction of AnchorUDF with the ground-truth direction to the surface during
training. Extensive experiments on two public 3D garment datasets, i.e., MGN
and Deep Fashion3D, demonstrate that AnchorUDF achieves the state-of-the-art
performance on single-view garment reconstruction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Synthesis-Based Approach for Thermal-to-Visible Face Verification. (arXiv:2108.09558v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09558">
<div class="article-summary-box-inner">
<span><p>In recent years, visible-spectrum face verification systems have been shown
to match the performance of experienced forensic examiners. However, such
systems are ineffective in low-light and nighttime conditions. Thermal face
imagery, which captures body heat emissions, effectively augments the visible
spectrum, capturing discriminative facial features in scenes with limited
illumination. Due to the increased cost and difficulty of obtaining diverse,
paired thermal and visible spectrum datasets, not many algorithms and
large-scale benchmarks for low-light recognition are available. This paper
presents an algorithm that achieves state-of-the-art performance on both the
ARL-VTF and TUFTS multi-spectral face datasets. Importantly, we study the
impact of face alignment, pixel-level correspondence, and identity
classification with label smoothing for multi-spectral face synthesis and
verification. We show that our proposed method is widely applicable, robust,
and highly effective. In addition, we show that the proposed method
significantly outperforms face frontalization methods on profile-to-frontal
verification. Finally, we present MILAB-VTF(B), a challenging multi-spectral
face dataset that is composed of paired thermal and visible videos. To the best
of our knowledge, with face data from 400 subjects, this dataset represents the
most extensive collection of publicly available indoor and long-range outdoor
thermal-visible face imagery. Lastly, we show that our end-to-end
thermal-to-visible face verification system provides strong performance on the
MILAB-VTF(B) dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Superpixel-guided Discriminative Low-rank Representation of Hyperspectral Images for Classification. (arXiv:2108.11172v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11172">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel classification scheme for the remotely
sensed hyperspectral image (HSI), namely SP-DLRR, by comprehensively exploring
its unique characteristics, including the local spatial information and
low-rankness. SP-DLRR is mainly composed of two modules, i.e., the
classification-guided superpixel segmentation and the discriminative low-rank
representation, which are iteratively conducted. Specifically, by utilizing the
local spatial information and incorporating the predictions from a typical
classifier, the first module segments pixels of an input HSI (or its
restoration generated by the second module) into superpixels. According to the
resulting superpixels, the pixels of the input HSI are then grouped into
clusters and fed into our novel discriminative low-rank representation model
with an effective numerical solution. Such a model is capable of increasing the
intra-class similarity by suppressing the spectral variations locally while
promoting the inter-class discriminability globally, leading to a restored HSI
with more discriminative pixels. Experimental results on three benchmark
datasets demonstrate the significant superiority of SP-DLRR over
state-of-the-art methods, especially for the case with an extremely limited
number of training pixels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Riemannian Framework for Analysis of Human Body Surface. (arXiv:2108.11449v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11449">
<div class="article-summary-box-inner">
<span><p>We propose a novel framework for comparing 3D human shapes under the change
of shape and pose. This problem is challenging since 3D human shapes vary
significantly across subjects and body postures. We solve this problem by using
a Riemannian approach. Our core contribution is the mapping of the human body
surface to the space of metrics and normals. We equip this space with a family
of Riemannian metrics, called Ebin (or DeWitt) metrics. We treat a human body
surface as a point in a "shape space" equipped with a family of Riemannian
metrics. The family of metrics is invariant under rigid motions and
reparametrizations; hence it induces a metric on the "shape space" of surfaces.
Using the alignment of human bodies with a given template, we show that this
family of metrics allows us to distinguish the changes in shape and pose. The
proposed framework has several advantages. First, we define a family of metrics
with desired invariance properties for the comparison of human shape. Second,
we present an efficient framework to compute geodesic paths between human shape
given the chosen metric. Third, this framework provides some basic tools for
statistical shape analysis of human body surfaces. Finally, we demonstrate the
utility of the proposed framework in pose and shape retrieval of human body.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PoissonSeg: Semi-Supervised Few-Shot Medical Image Segmentation via Poisson Learning. (arXiv:2108.11694v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11694">
<div class="article-summary-box-inner">
<span><p>The application of deep learning to medical image segmentation has been
hampered due to the lack of abundant pixel-level annotated data. Few-shot
Semantic Segmentation (FSS) is a promising strategy for breaking the deadlock.
However, a high-performing FSS model still requires sufficient pixel-level
annotated classes for training to avoid overfitting, which leads to its
performance bottleneck in medical image segmentation due to the unmet need for
annotations. Thus, semi-supervised FSS for medical images is accordingly
proposed to utilize unlabeled data for further performance improvement.
Nevertheless, existing semi-supervised FSS methods has two obvious defects: (1)
neglecting the relationship between the labeled and unlabeled data; (2) using
unlabeled data directly for end-to-end training leads to degenerated
representation learning. To address these problems, we propose a novel
semi-supervised FSS framework for medical image segmentation. The proposed
framework employs Poisson learning for modeling data relationship and
propagating supervision signals, and Spatial Consistency Calibration for
encouraging the model to learn more coherent representations. In this process,
unlabeled samples do not involve in end-to-end training, but provide
supervisory information for query image segmentation through graph-based
learning. We conduct extensive experiments on three medical image segmentation
datasets (i.e. ISIC skin lesion segmentation, abdominal organs segmentation for
MRI and abdominal organs segmentation for CT) to demonstrate the
state-of-the-art performance and broad applicability of the proposed framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PAENet: A Progressive Attention-Enhanced Network for 3D to 2D Retinal Vessel Segmentation. (arXiv:2108.11695v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11695">
<div class="article-summary-box-inner">
<span><p>3D to 2D retinal vessel segmentation is a challenging problem in Optical
Coherence Tomography Angiography (OCTA) images. Accurate retinal vessel
segmentation is important for the diagnosis and prevention of ophthalmic
diseases. However, making full use of the 3D data of OCTA volumes is a vital
factor for obtaining satisfactory segmentation results. In this paper, we
propose a Progressive Attention-Enhanced Network (PAENet) based on attention
mechanisms to extract rich feature representation. Specifically, the framework
consists of two main parts, the three-dimensional feature learning path and the
two-dimensional segmentation path. In the three-dimensional feature learning
path, we design a novel Adaptive Pooling Module (APM) and propose a new
Quadruple Attention Module (QAM). The APM captures dependencies along the
projection direction of volumes and learns a series of pooling coefficients for
feature fusion, which efficiently reduces feature dimension. In addition, the
QAM reweights the features by capturing four-group cross-dimension
dependencies, which makes maximum use of 4D feature tensors. In the
two-dimensional segmentation path, to acquire more detailed information, we
propose a Feature Fusion Module (FFM) to inject 3D information into the 2D
path. Meanwhile, we adopt the Polarized Self-Attention (PSA) block to model the
semantic interdependencies in spatial and channel dimensions respectively.
Experimentally, our extensive experiments on the OCTA-500 dataset show that our
proposed algorithm achieves state-of-the-art performance compared with previous
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A realistic approach to generate masked faces applied on two novel masked face recognition data sets. (arXiv:2109.01745v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01745">
<div class="article-summary-box-inner">
<span><p>The COVID-19 pandemic raises the problem of adapting face recognition systems
to the new reality, where people may wear surgical masks to cover their noses
and mouths. Traditional data sets (e.g., CelebA, CASIA-WebFace) used for
training these systems were released before the pandemic, so they now seem
unsuited due to the lack of examples of people wearing masks. We propose a
method for enhancing data sets containing faces without masks by creating
synthetic masks and overlaying them on faces in the original images. Our method
relies on SparkAR Studio, a developer program made by Facebook that is used to
create Instagram face filters. In our approach, we use 9 masks of different
colors, shapes and fabrics. We employ our method to generate a number of
445,446 (90%) samples of masks for the CASIA-WebFace data set and 196,254
(96.8%) masks for the CelebA data set, releasing the mask images at
https://github.com/securifai/masked_faces. We show that our method produces
significantly more realistic training examples of masks overlaid on faces by
asking volunteers to qualitatively compare it to other methods or data sets
designed for the same task. We also demonstrate the usefulness of our method by
evaluating state-of-the-art face recognition systems (FaceNet, VGG-face,
ArcFace) trained on our enhanced data sets and showing that they outperform
equivalent systems trained on original data sets (containing faces without
masks) or competing data sets (containing masks generated by related methods),
when the test benchmarks contain masked faces.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Out-of-distribution Generalization of Probabilistic Image Modelling. (arXiv:2109.02639v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02639">
<div class="article-summary-box-inner">
<span><p>Out-of-distribution (OOD) detection and lossless compression constitute two
problems that can be solved by the training of probabilistic models on a first
dataset with subsequent likelihood evaluation on a second dataset, where data
distributions differ. By defining the generalization of probabilistic models in
terms of likelihood we show that, in the case of image models, the OOD
generalization ability is dominated by local features. This motivates our
proposal of a Local Autoregressive model that exclusively models local image
features towards improving OOD performance. We apply the proposed model to OOD
detection tasks and achieve state-of-the-art unsupervised OOD detection
performance without the introduction of additional data. Additionally, we
employ our model to build a new lossless image compressor: NeLLoC (Neural Local
Lossless Compressor) and report state-of-the-art compression rates and model
size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying Autism Spectrum Disorder Based on Individual-Aware Down-Sampling and Multi-Modal Learning. (arXiv:2109.09129v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09129">
<div class="article-summary-box-inner">
<span><p>Autism Spectrum Disorder(ASD) is a set of neurodevelopmental conditions that
affect patients' social abilities. In recent years, many studies have employed
deep learning to diagnose this brain dysfunction through functional MRI (fMRI).
However, existing approaches solely focused on the abnormal brain functional
connections but ignored the impact of regional activities. Due to this biased
prior knowledge, previous diagnosis models suffered from inter-site measurement
heterogeneity and inter-individual phenotypic differences. To address this
issue, we propose a novel feature extraction method for fMRI that can learn a
personalized lower-resolution representation of the entire brain networking
regarding both the functional connections and regional activities.
Specifically, we abstract the brain imaging as a graph structure and
straightforwardly downsample it to substructures by hierarchical graph pooling.
To further recalibrate the distribution of the extracted features under
phenotypic information, we subsequently embed the sparse feature vectors into a
population graph, where the hidden inter-subject heterogeneity and homogeneity
are explicitly expressed as inter- and intra-community connectivity
differences, and utilize Graph Convolutional Networks to learn the node
embeddings. By these means, our framework can extract features directly and
efficiently from the entire fMRI and be aware of implicit inter-individual
variance. We have evaluated our framework on the ABIDE-I dataset with 10-fold
cross-validation. The present model has achieved a mean classification accuracy
of 87.62\% and a mean AUC of 0.92, better than the state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Skin Deep Unlearning: Artefact and Instrument Debiasing in the Context of Melanoma Classification. (arXiv:2109.09818v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09818">
<div class="article-summary-box-inner">
<span><p>Convolutional Neural Networks have demonstrated dermatologist-level
performance in the classification of melanoma and other skin lesions, but
prediction irregularities due to biases seen within the training data are an
issue that should be addressed before widespread deployment is possible. In
this work, we robustly remove bias and spurious variation from an automated
melanoma classification pipeline using two leading bias unlearning techniques.
We show that the biases introduced by surgical markings and rulers presented in
previous studies can be reasonably mitigated using these bias removal methods.
We also demonstrate the generalisation benefits of unlearning spurious
variation relating to the imaging instrument used to capture lesion images.
Contributions of this work include the application of different debiasing
techniques for artefact bias removal and the concept of instrument bias
unlearning for domain generalisation in melanoma detection. Our experimental
results provide evidence that the effects of each of the aforementioned biases
are notably reduced, with different debiasing techniques excelling at different
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Data-Free Domain Generalization. (arXiv:2110.04545v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04545">
<div class="article-summary-box-inner">
<span><p>In this work, we investigate the unexplored intersection of domain
generalization and data-free learning. In particular, we address the question:
How can knowledge contained in models trained on different source data domains
can be merged into a single model that generalizes well to unseen target
domains, in the absence of source and target domain data? Machine learning
models that can cope with domain shift are essential for for real-world
scenarios with often changing data distributions. Prior domain generalization
methods typically rely on using source domain data, making them unsuitable for
private decentralized data. We define the novel problem of Data-Free Domain
Generalization (DFDG), a practical setting where models trained on the source
domains separately are available instead of the original datasets, and
investigate how to effectively solve the domain generalization problem in that
case. We propose DEKAN, an approach that extracts and fuses domain-specific
knowledge from the available teacher models into a student model robust to
domain shift. Our empirical evaluation demonstrates the effectiveness of our
method which achieves first state-of-the-art results in DFDG by significantly
outperforming ensemble and data-free knowledge distillation baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modality-Guided Subnetwork for Salient Object Detection. (arXiv:2110.04904v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04904">
<div class="article-summary-box-inner">
<span><p>Recent RGBD-based models for saliency detection have attracted research
attention. The depth clues such as boundary clues, surface normal, shape
attribute, etc., contribute to the identification of salient objects with
complicated scenarios. However, most RGBD networks require multi-modalities
from the input side and feed them separately through a two-stream design, which
inevitably results in extra costs on depth sensors and computation. To tackle
these inconveniences, we present in this paper a novel fusion design named
modality-guided subnetwork (MGSnet). It has the following superior designs: 1)
Our model works for both RGB and RGBD data, and dynamically estimating depth if
not available. Taking the inner workings of depth-prediction networks into
account, we propose to estimate the pseudo-geometry maps from RGB input -
essentially mimicking the multi-modality input. 2) Our MGSnet for RGB SOD
results in real-time inference but achieves state-of-the-art performance
compared to other RGB models. 3) The flexible and lightweight design of MGS
facilitates the integration into RGBD two-streaming models. The introduced
fusion design enables a cross-modality interaction to enable further progress
but with a minimal cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accurate Fine-grained Layout Analysis for the Historical Tibetan Document Based on the Instance Segmentation. (arXiv:2110.08164v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08164">
<div class="article-summary-box-inner">
<span><p>Accurate layout analysis without subsequent text-line segmentation remains an
ongoing challenge, especially when facing the Kangyur, a kind of historical
Tibetan document featuring considerable touching components and mottled
background. Aiming at identifying different regions in document images, layout
analysis is indispensable for subsequent procedures such as character
recognition. However, there was only a little research being carried out to
perform line-level layout analysis which failed to deal with the Kangyur. To
obtain the optimal results, a fine-grained sub-line level layout analysis
approach is presented. Firstly, we introduced an accelerated method to build
the dataset which is dynamic and reliable. Secondly, enhancement had been made
to the SOLOv2 according to the characteristics of the Kangyur. Then, we fed the
enhanced SOLOv2 with the prepared annotation file during the training phase.
Once the network is trained, instances of the text line, sentence, and titles
can be segmented and identified during the inference stage. The experimental
results show that the proposed method delivers a decent 72.7% AP on our
dataset. In general, this preliminary research provides insights into the
fine-grained sub-line level layout analysis and testifies the SOLOv2-based
approaches. We also believe that the proposed methods can be adopted on other
language documents with various layouts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparing Human and Machine Bias in Face Recognition. (arXiv:2110.08396v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08396">
<div class="article-summary-box-inner">
<span><p>Much recent research has uncovered and discussed serious concerns of bias in
facial analysis technologies, finding performance disparities between groups of
people based on perceived gender, skin type, lighting condition, etc. These
audits are immensely important and successful at measuring algorithmic bias but
have two major challenges: the audits (1) use facial recognition datasets which
lack quality metadata, like LFW and CelebA, and (2) do not compare their
observed algorithmic bias to the biases of their human alternatives. In this
paper, we release improvements to the LFW and CelebA datasets which will enable
future researchers to obtain measurements of algorithmic bias that are not
tainted by major flaws in the dataset (e.g. identical images appearing in both
the gallery and test set). We also use these new data to develop a series of
challenging facial identification and verification questions that we
administered to various algorithms and a large, balanced sample of human
reviewers. We find that both computer models and human survey participants
perform significantly better at the verification task, generally obtain lower
accuracy rates on dark-skinned or female subjects for both tasks, and obtain
higher accuracy rates when their demographics match that of the question.
Computer models are observed to achieve a higher level of accuracy than the
survey participants on both tasks and exhibit bias to similar degrees as the
human survey participants.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LoveDA: A Remote Sensing Land-Cover Dataset for Domain Adaptive Semantic Segmentation. (arXiv:2110.08733v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08733">
<div class="article-summary-box-inner">
<span><p>Deep learning approaches have shown promising results in remote sensing high
spatial resolution (HSR) land-cover mapping. However, urban and rural scenes
can show completely different geographical landscapes, and the inadequate
generalizability of these algorithms hinders city-level or national-level
mapping. Most of the existing HSR land-cover datasets mainly promote the
research of learning semantic representation, thereby ignoring the model
transferability. In this paper, we introduce the Land-cOVEr Domain Adaptive
semantic segmentation (LoveDA) dataset to advance semantic and transferable
learning. The LoveDA dataset contains 5987 HSR images with 166768 annotated
objects from three different cities. Compared to the existing datasets, the
LoveDA dataset encompasses two domains (urban and rural), which brings
considerable challenges due to the: 1) multi-scale objects; 2) complex
background samples; and 3) inconsistent class distributions. The LoveDA dataset
is suitable for both land-cover semantic segmentation and unsupervised domain
adaptation (UDA) tasks. Accordingly, we benchmarked the LoveDA dataset on
eleven semantic segmentation methods and eight UDA methods. Some exploratory
studies including multi-scale architectures and strategies, additional
background supervision, and pseudo-label analysis were also carried out to
address these challenges. The code and data are available at
https://github.com/Junjue-Wang/LoveDA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Image Fusion Using Deep Image Priors. (arXiv:2110.09490v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09490">
<div class="article-summary-box-inner">
<span><p>A significant number of researchers have recently applied deep learning
methods to image fusion. However, most of these works either require a large
amount of training data or depend on pre-trained models or frameworks. This
inevitably encounters a shortage of training data or a mismatch between the
framework and the actual problem. Recently, the publication of Deep Image Prior
(DIP) method made it possible to do image restoration totally
training-data-free. However, the original design of DIP is hard to be
generalized to multi-image processing problems. This paper introduces a novel
loss calculation structure, in the framework of DIP, while formulating image
fusion as an inverse problem. This enables the extension of DIP to general
multisensor/multifocus image fusion problems. Secondly, we propose a
multi-channel approach to improve the effect of DIP. Finally, an evaluation is
conducted using several commonly used image fusion assessment metrics. The
results are compared with state-of-the-art traditional and deep learning image
fusion methods. Our method outperforms previous techniques for a range of
metrics. In particular, it is shown to provide the best objective results for
most metrics when applied to medical images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Distillation: Aggregating Knowledge from Multiple Paths for Efficient Distillation. (arXiv:2110.09674v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09674">
<div class="article-summary-box-inner">
<span><p>Knowledge Distillation is becoming one of the primary trends among neural
network compression algorithms to improve the generalization performance of a
smaller student model with guidance from a larger teacher model. This momentous
rise in applications of knowledge distillation is accompanied by the
introduction of numerous algorithms for distilling the knowledge such as soft
targets and hint layers. Despite this advancement in different techniques for
distilling the knowledge, the aggregation of different paths for distillation
has not been studied comprehensively. This is of particular significance, not
only because different paths have different importance, but also due to the
fact that some paths might have negative effects on the generalization
performance of the student model. Hence, we need to adaptively adjust the
importance of each path to maximize the impact of distillation on the student
model. In this paper, we explore different approaches for aggregating these
different paths and introduce our proposed adaptive approach based on multitask
learning methods. We empirically demonstrate the effectiveness of the proposed
approach over other baselines on the applications of knowledge distillation in
classification, semantic segmentation, and object detection tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Not to Reconstruct Anomalies. (arXiv:2110.09742v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09742">
<div class="article-summary-box-inner">
<span><p>Video anomaly detection is often seen as one-class classification (OCC)
problem due to the limited availability of anomaly examples. Typically, to
tackle this problem, an autoencoder (AE) is trained to reconstruct the input
with training set consisting only of normal data. At test time, the AE is then
expected to well reconstruct the normal data while poorly reconstructing the
anomalous data. However, several studies have shown that, even with only normal
data training, AEs can often start reconstructing anomalies as well which
depletes the anomaly detection performance. To mitigate this problem, we
propose a novel methodology to train AEs with the objective of reconstructing
only normal data, regardless of the input (i.e., normal or abnormal). Since no
real anomalies are available in the OCC settings, the training is assisted by
pseudo anomalies that are generated by manipulating normal data to simulate the
out-of-normal-data distribution. We additionally propose two ways to generate
pseudo anomalies: patch and skip frame based. Extensive experiments on three
challenging video anomaly datasets demonstrate the effectiveness of our method
in improving conventional AEs, achieving state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Memory-Augmented Deep Unfolding Network for Compressive Sensing. (arXiv:2110.09766v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09766">
<div class="article-summary-box-inner">
<span><p>Mapping a truncated optimization method into a deep neural network, deep
unfolding network (DUN) has attracted growing attention in compressive sensing
(CS) due to its good interpretability and high performance. Each stage in DUNs
corresponds to one iteration in optimization. By understanding DUNs from the
perspective of the human brain's memory processing, we find there exists two
issues in existing DUNs. One is the information between every two adjacent
stages, which can be regarded as short-term memory, is usually lost seriously.
The other is no explicit mechanism to ensure that the previous stages affect
the current stage, which means memory is easily forgotten. To solve these
issues, in this paper, a novel DUN with persistent memory for CS is proposed,
dubbed Memory-Augmented Deep Unfolding Network (MADUN). We design a
memory-augmented proximal mapping module (MAPMM) by combining two types of
memory augmentation mechanisms, namely High-throughput Short-term Memory (HSM)
and Cross-stage Long-term Memory (CLM). HSM is exploited to allow DUNs to
transmit multi-channel short-term memory, which greatly reduces information
loss between adjacent stages. CLM is utilized to develop the dependency of deep
information across cascading stages, which greatly enhances network
representation capability. Extensive CS experiments on natural and MR images
show that with the strong ability to maintain and balance information our MADUN
outperforms existing state-of-the-art methods by a large margin. The source
code is available at https://github.com/jianzhangcs/MADUN/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-Grained Control of Artistic Styles in Image Generation. (arXiv:2110.10278v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10278">
<div class="article-summary-box-inner">
<span><p>Recent advances in generative models and adversarial training have enabled
artificially generating artworks in various artistic styles. It is highly
desirable to gain more control over the generated style in practice. However,
artistic styles are unlike object categories -- there are a continuous spectrum
of styles distinguished by subtle differences. Few works have been explored to
capture the continuous spectrum of styles and apply it to a style generation
task. In this paper, we propose to achieve this by embedding original artwork
examples into a continuous style space. The style vectors are fed to the
generator and discriminator to achieve fine-grained control. Our method can be
used with common generative adversarial networks (such as StyleGAN).
Experiments show that our method not only precisely controls the fine-grained
artistic style but also improves image quality over vanilla StyleGAN as
measured by FID.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ASSANet: An Anisotropic Separable Set Abstraction for Efficient Point Cloud Representation Learning. (arXiv:2110.10538v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10538">
<div class="article-summary-box-inner">
<span><p>Access to 3D point cloud representations has been widely facilitated by LiDAR
sensors embedded in various mobile devices. This has led to an emerging need
for fast and accurate point cloud processing techniques. In this paper, we
revisit and dive deeper into PointNet++, one of the most influential yet
under-explored networks, and develop faster and more accurate variants of the
model. We first present a novel Separable Set Abstraction (SA) module that
disentangles the vanilla SA module used in PointNet++ into two separate
learning stages: (1) learning channel correlation and (2) learning spatial
correlation. The Separable SA module is significantly faster than the vanilla
version, yet it achieves comparable performance. We then introduce a new
Anisotropic Reduction function into our Separable SA module and propose an
Anisotropic Separable SA (ASSA) module that substantially increases the
network's accuracy. We later replace the vanilla SA modules in PointNet++ with
the proposed ASSA module, and denote the modified network as ASSANet. Extensive
experiments on point cloud classification, semantic segmentation, and part
segmentation show that ASSANet outperforms PointNet++ and other methods,
achieving much higher accuracy and faster speeds. In particular, ASSANet
outperforms PointNet++ by $7.4$ mIoU on S3DIS Area 5, while maintaining $1.6
\times $ faster inference speed on a single NVIDIA 2080Ti GPU. Our scaled
ASSANet variant achieves $66.8$ mIoU and outperforms KPConv, while being more
than $54 \times$ faster.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatial Location Constraint Prototype Loss for Open Set Recognition. (arXiv:2110.11013v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11013">
<div class="article-summary-box-inner">
<span><p>One of the challenges in pattern recognition is open set recognition.
Compared with closed set recognition, open set recognition needs to reduce not
only the empirical risk, but also the open space risk, and the reduction of
these two risks corresponds to classifying the known classes and identifying
the unknown classes respectively. How to reduce the open space risk is the key
of open set recognition. This paper explores the origin of the open space risk
by analyzing the distribution of known and unknown classes features. On this
basis, the spatial location constraint prototype loss function is proposed to
reduce the two risks simultaneously. Extensive experiments on multiple
benchmark datasets and many visualization results indicate that our methods is
superior to most existing approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Adversarial Graph Convolutional Networks for Human Action Synthesis. (arXiv:2110.11191v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11191">
<div class="article-summary-box-inner">
<span><p>Synthesising the spatial and temporal dynamics of the human body skeleton
remains a challenging task, not only in terms of the quality of the generated
shapes, but also of their diversity, particularly to synthesise realistic body
movements of a specific action (action conditioning). In this paper, we propose
Kinetic-GAN, a novel architecture that leverages the benefits of Generative
Adversarial Networks and Graph Convolutional Networks to synthesise the
kinetics of the human body. The proposed adversarial architecture can condition
up to 120 different actions over local and global body movements while
improving sample quality and diversity through latent space disentanglement and
stochastic variations. Our experiments were carried out in three well-known
datasets, where Kinetic-GAN notably surpasses the state-of-the-art methods in
terms of distribution quality metrics while having the ability to synthesise
more than one order of magnitude regarding the number of different actions. Our
code and models are publicly available at
https://github.com/DegardinBruno/Kinetic-GAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast Graph Sampling for Short Video Summarization using Gershgorin Disc Alignment. (arXiv:2110.11420v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11420">
<div class="article-summary-box-inner">
<span><p>We study the problem of efficiently summarizing a short video into several
keyframes, leveraging recent progress in fast graph sampling. Specifically, we
first construct a similarity path graph (SPG) $\mathcal{G}$, represented by
graph Laplacian matrix $\mathbf{L}$, where the similarities between adjacent
frames are encoded as positive edge weights. We show that maximizing the
smallest eigenvalue $\lambda_{\min}(\mathbf{B})$ of a coefficient matrix
$\mathbf{B} = \text{diag}(\mathbf{a}) + \mu \mathbf{L}$, where $\mathbf{a}$ is
the binary keyframe selection vector, is equivalent to minimizing a worst-case
signal reconstruction error. We prove that, after partitioning $\mathcal{G}$
into $Q$ sub-graphs $\{\mathcal{G}^q\}^Q_{q=1}$, the smallest Gershgorin circle
theorem (GCT) lower bound of $Q$ corresponding coefficient matrices -- $\min_q
\lambda^-_{\min}(\mathbf{B}^q)$ -- is a lower bound for
$\lambda_{\min}(\mathbf{B})$. This inspires a fast graph sampling algorithm to
iteratively partition $\mathcal{G}$ into $Q$ sub-graphs using $Q$ samples
(keyframes), while maximizing $\lambda^-_{\min}(\mathbf{B}^q)$ for each
sub-graph $\mathcal{G}^q$. Experimental results show that our algorithm
achieves comparable video summarization performance as state-of-the-art
methods, at a substantially reduced complexity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AEI: Actors-Environment Interaction with Adaptive Attention for Temporal Action Proposals Generation. (arXiv:2110.11474v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11474">
<div class="article-summary-box-inner">
<span><p>Humans typically perceive the establishment of an action in a video through
the interaction between an actor and the surrounding environment. An action
only starts when the main actor in the video begins to interact with the
environment, while it ends when the main actor stops the interaction. Despite
the great progress in temporal action proposal generation, most existing works
ignore the aforementioned fact and leave their model learning to propose
actions as a black-box. In this paper, we make an attempt to simulate that
ability of a human by proposing Actor Environment Interaction (AEI) network to
improve the video representation for temporal action proposals generation. AEI
contains two modules, i.e., perception-based visual representation (PVR) and
boundary-matching module (BMM). PVR represents each video snippet by taking
human-human relations and humans-environment relations into consideration using
the proposed adaptive attention mechanism. Then, the video representation is
taken by BMM to generate action proposals. AEI is comprehensively evaluated in
ActivityNet-1.3 and THUMOS-14 datasets, on temporal action proposal and
detection tasks, with two boundary-matching architectures (i.e., CNN-based and
GCN-based) and two classifiers (i.e., Unet and P-GCN). Our AEI robustly
outperforms the state-of-the-art methods with remarkable performance and
generalization for both temporal action proposal generation and temporal action
detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Semi-Supervised Learning for 3D Objects. (arXiv:2110.11601v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11601">
<div class="article-summary-box-inner">
<span><p>In recent years, semi-supervised learning has been widely explored and shows
excellent data efficiency for 2D data. There is an emerging need to improve
data efficiency for 3D tasks due to the scarcity of labeled 3D data. This paper
explores how the coherence of different modelities of 3D data (e.g. point
cloud, image, and mesh) can be used to improve data efficiency for both 3D
classification and retrieval tasks. We propose a novel multimodal
semi-supervised learning framework by introducing instance-level consistency
constraint and a novel multimodal contrastive prototype (M2CP) loss. The
instance-level consistency enforces the network to generate consistent
representations for multimodal data of the same object regardless of its
modality. The M2CP maintains a multimodal prototype for each class and learns
features with small intra-class variations by minimizing the feature distance
of each object to its prototype while maximizing the distance to the others.
Our proposed framework significantly outperforms all the state-of-the-art
counterparts for both classification and retrieval tasks by a large margin on
the modelNet10 and ModelNet40 datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SciCap: Generating Captions for Scientific Figures. (arXiv:2110.11624v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11624">
<div class="article-summary-box-inner">
<span><p>Researchers use figures to communicate rich, complex information in
scientific papers. The captions of these figures are critical to conveying
effective messages. However, low-quality figure captions commonly occur in
scientific articles and may decrease understanding. In this paper, we propose
an end-to-end neural framework to automatically generate informative,
high-quality captions for scientific figures. To this end, we introduce SCICAP,
a large-scale figure-caption dataset based on computer science arXiv papers
published between 2010 and 2020. After pre-processing - including figure-type
classification, sub-figure identification, text normalization, and caption text
selection - SCICAP contained more than two million figures extracted from over
290,000 papers. We then established baseline models that caption graph plots,
the dominant (19.2%) figure type. The experimental results showed both
opportunities and steep challenges of generating captions for scientific
figures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conditional Variational Autoencoder for Learned Image Reconstruction. (arXiv:2110.11681v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11681">
<div class="article-summary-box-inner">
<span><p>Learned image reconstruction techniques using deep neural networks have
recently gained popularity, and have delivered promising empirical results.
However, most approaches focus on one single recovery for each observation, and
thus neglect the uncertainty information. In this work, we develop a novel
computational framework that approximates the posterior distribution of the
unknown image at each query observation. The proposed framework is very
flexible: It handles implicit noise models and priors, it incorporates the data
formation process (i.e., the forward operator), and the learned reconstructive
properties are transferable between different datasets. Once the network is
trained using the conditional variational autoencoder loss, it provides a
computationally efficient sampler for the approximate posterior distribution
via feed-forward propagation, and the summarizing statistics of the generated
samples are used for both point-estimation and uncertainty quantification. We
illustrate the proposed framework with extensive numerical experiments on
positron emission tomography (with both moderate and low count levels) showing
that the framework generates high-quality samples when compared with
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Using Clothes Style Transfer for Scenario-aware Person Video Generation. (arXiv:2110.11894v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11894">
<div class="article-summary-box-inner">
<span><p>Clothes style transfer for person video generation is a challenging task, due
to drastic variations of intra-person appearance and video scenarios. To tackle
this problem, most recent AdaIN-based architectures are proposed to extract
clothes and scenario features for generation. However, these approaches suffer
from being short of fine-grained details and are prone to distort the origin
person. To further improve the generation performance, we propose a novel
framework with disentangled multi-branch encoders and a shared decoder.
Moreover, to pursue the strong video spatio-temporal consistency, an
inner-frame discriminator is delicately designed with input being cross-frame
difference. Besides, the proposed framework possesses the property of scenario
adaptation. Extensive experiments on the TEDXPeople benchmark demonstrate the
superiority of our method over state-of-the-art approaches in terms of image
quality and video coherence.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-10-26 23:02:15.441396597 UTC">2021-10-26 23:02:15 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.6</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>