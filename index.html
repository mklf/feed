<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-01-10T01:30:00Z">01-10</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Large-scale protein-protein post-translational modification extraction with distant supervision and confidence calibrated BioBERT. (arXiv:2201.02229v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02229">
<div class="article-summary-box-inner">
<span><p>Protein-protein interactions (PPIs) are critical to normal cellular function
and are related to many disease pathways. However, only 4% of PPIs are
annotated with PTMs in biological knowledge databases such as IntAct, mainly
performed through manual curation, which is neither time nor cost-effective. We
use the IntAct PPI database to create a distant supervised dataset annotated
with interacting protein pairs, their corresponding PTM type, and associated
abstracts from the PubMed database. We train an ensemble of BioBERT models -
dubbed PPI-BioBERT-x10 to improve confidence calibration. We extend the use of
ensemble average confidence approach with confidence variation to counteract
the effects of class imbalance to extract high confidence predictions. The
PPI-BioBERT-x10 model evaluated on the test set resulted in a modest F1-micro
41.3 (P =5 8.1, R = 32.1). However, by combining high confidence and low
variation to identify high quality predictions, tuning the predictions for
precision, we retained 19% of the test predictions with 100% precision. We
evaluated PPI-BioBERT-x10 on 18 million PubMed abstracts and extracted 1.6
million (546507 unique PTM-PPI triplets) PTM-PPI predictions, and filter ~ 5700
(4584 unique) high confidence predictions. Of the 5700, human evaluation on a
small randomly sampled subset shows that the precision drops to 33.7% despite
confidence calibration and highlights the challenges of generalisability beyond
the test set even with confidence calibration. We circumvent the problem by
only including predictions associated with multiple papers, improving the
precision to 58.8%. In this work, we highlight the benefits and challenges of
deep learning-based text mining in practice, and the need for increased
emphasis on confidence calibration to facilitate human curation efforts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Applying Word Embeddings to Measure Valence in Information Operations Targeting Journalists in Brazil. (arXiv:2201.02257v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02257">
<div class="article-summary-box-inner">
<span><p>Among the goals of information operations are to change the overall
information environment vis-\'a-vis specific actors. For example, "trolling
campaigns" seek to undermine the credibility of specific public figures,
leading others to distrust them and intimidating these figures into silence. To
accomplish these aims, information operations frequently make use of "trolls"
-- malicious online actors who target verbal abuse at these figures. In Brazil,
in particular, allies of Brazil's current president have been accused of
operating a "hate cabinet" -- a trolling operation that targets journalists who
have alleged corruption by this politician and other members of his regime.
Leading approaches to detecting harmful speech, such as Google's Perspective
API, seek to identify specific messages with harmful content. While this
approach is helpful in identifying content to downrank, flag, or remove, it is
known to be brittle, and may miss attempts to introduce more subtle biases into
the discourse. Here, we aim to develop a measure that might be used to assess
how targeted information operations seek to change the overall valence, or
appraisal, of specific actors. Preliminary results suggest known campaigns
target female journalists more so than male journalists, and that these
campaigns may leave detectable traces in overall Twitter discourse.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Repurposing Existing Deep Networks for Caption and Aesthetic-Guided Image Cropping. (arXiv:2201.02280v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02280">
<div class="article-summary-box-inner">
<span><p>We propose a novel optimization framework that crops a given image based on
user description and aesthetics. Unlike existing image cropping methods, where
one typically trains a deep network to regress to crop parameters or cropping
actions, we propose to directly optimize for the cropping parameters by
repurposing pre-trained networks on image captioning and aesthetic tasks,
without any fine-tuning, thereby avoiding training a separate network.
Specifically, we search for the best crop parameters that minimize a combined
loss of the initial objectives of these networks. To make the optimization
table, we propose three strategies: (i) multi-scale bilinear sampling, (ii)
annealing the scale of the crop region, therefore effectively reducing the
parameter space, (iii) aggregation of multiple optimization results. Through
various quantitative and qualitative evaluations, we show that our framework
can produce crops that are well-aligned to intended user descriptions and
aesthetically pleasing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Transfer Learning Pipeline for Educational Resource Discovery with Application in Leading Paragraph Generation. (arXiv:2201.02312v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02312">
<div class="article-summary-box-inner">
<span><p>Effective human learning depends on a wide selection of educational materials
that align with the learner's current understanding of the topic. While the
Internet has revolutionized human learning or education, a substantial resource
accessibility barrier still exists. Namely, the excess of online information
can make it challenging to navigate and discover high-quality learning
materials. In this paper, we propose the educational resource discovery (ERD)
pipeline that automates web resource discovery for novel domains. The pipeline
consists of three main steps: data collection, feature extraction, and resource
classification. We start with a known source domain and conduct resource
discovery on two unseen target domains via transfer learning. We first collect
frequent queries from a set of seed documents and search on the web to obtain
candidate resources, such as lecture slides and introductory blog posts. Then
we introduce a novel pretrained information retrieval deep neural network
model, query-document masked language modeling (QD-MLM), to extract deep
features of these candidate resources. We apply a tree-based classifier to
decide whether the candidate is a positive learning resource. The pipeline
achieves F1 scores of 0.94 and 0.82 when evaluated on two similar but novel
target domains. Finally, we demonstrate how this pipeline can benefit an
application: leading paragraph generation for surveys. This is the first study
that considers various web resources for survey generation, to the best of our
knowledge. We also release a corpus of 39,728 manually labeled web resources
and 659 queries from NLP, Computer Vision (CV), and Statistics (STATS).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Unsupervised Masking Objective for Abstractive Multi-Document News Summarization. (arXiv:2201.02321v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02321">
<div class="article-summary-box-inner">
<span><p>We show that a simple unsupervised masking objective can approach near
supervised performance on abstractive multi-document news summarization. Our
method trains a state-of-the-art neural summarization model to predict the
masked out source document with highest lexical centrality relative to the
multi-document group. In experiments on the Multi-News dataset, our masked
training objective yields a system that outperforms past unsupervised methods
and, in human evaluation, surpasses the best supervised method without
requiring access to any ground-truth summaries. Further, we evaluate how
different measures of lexical centrality, inspired by past work on extractive
summarization, affect final performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Defeat of the Winograd Schema Challenge. (arXiv:2201.02387v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02387">
<div class="article-summary-box-inner">
<span><p>The Winograd Schema Challenge -- a set of twin sentences involving pronoun
reference disambiguation that seem to require the use of commonsense knowledge
-- was proposed by Hector Levesque in 2011. By 2019, a number of AI systems,
based on large pre-trained transformer-based language models and fine-tuned on
these kinds of problems, achieved better than 90% accuracy. In this paper, we
review the history of the Winograd Schema Challenge and assess its
significance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Speech Recognition Datasets in Cantonese Language: A Survey and a New Dataset. (arXiv:2201.02419v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02419">
<div class="article-summary-box-inner">
<span><p>Automatic speech recognition (ASR) on low resource languages improves access
of linguistic minorities to technological advantages provided by Artificial
Intelligence (AI). In this paper, we address a problem of data scarcity of Hong
Kong Cantonese language by creating a new Cantonese dataset. Our dataset,
Multi-Domain Cantonese Corpus (MDCC), consists of 73.6 hours of clean read
speech paired with transcripts, collected from Cantonese audiobooks from Hong
Kong. It combines philosophy, politics, education, culture, lifestyle and
family domains, covering a wide range of topics. We also review all existing
Cantonese datasets and perform experiments on the two biggest datasets (MDCC
and Common Voice zh-HK). We analyze the existing datasets according to their
speech type, data source, total size and availability. The results of
experiments conducted with Fairseq S2T Transformer, a state-of-the-art ASR
model, show the effectiveness of our dataset. In addition, we create a powerful
and robust Cantonese ASR model by applying multi-dataset learning on MDCC and
Common Voice zh-HK.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic-based Data Augmentation for Math Word Problems. (arXiv:2201.02489v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02489">
<div class="article-summary-box-inner">
<span><p>It's hard for neural MWP solvers to deal with tiny local variances. In MWP
task, some local changes conserve the original semantic while the others may
totally change the underlying logic. Currently, existing datasets for MWP task
contain limited samples which are key for neural models to learn to
disambiguate different kinds of local variances in questions and solve the
questions correctly. In this paper, we propose a set of novel data augmentation
approaches to supplement existing datasets with such data that are augmented
with different kinds of local variances, and help to improve the generalization
ability of current neural models. New samples are generated by knowledge guided
entity replacement, and logic guided problem reorganization. The augmentation
approaches are ensured to keep the consistency between the new data and their
labels. Experimental results have shown the necessity and the effectiveness of
our methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Summarization Based on Video-text Representation. (arXiv:2201.02494v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02494">
<div class="article-summary-box-inner">
<span><p>Modern video summarization methods are based on deep neural networks which
require a large amount of annotated data for training. However, existing
datasets for video summarization are small-scale, easily leading to
over-fitting of the deep models. Considering that the annotation of large-scale
datasets is time-consuming, we propose a multimodal self-supervised learning
framework to obtain semantic representations of videos, which benefits the
video summarization task. Specifically, we explore the semantic consistency
between the visual information and text information of videos, for the
self-supervised pretraining of a multimodal encoder on a newly-collected
dataset of video-text pairs. Additionally, we introduce a progressive video
summarization method, where the important content in a video is pinpointed
progressively to generate better summaries. Finally, an objective evaluation
framework is proposed to measure the quality of video summaries based on video
classification. Extensive experiments have proved the effectiveness and
superiority of our method in rank correlation coefficients, F-score, and the
proposed objective evaluation compared to the state of the art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sign Language Video Retrieval with Free-Form Textual Queries. (arXiv:2201.02495v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02495">
<div class="article-summary-box-inner">
<span><p>Systems that can efficiently search collections of sign language videos have
been highlighted as a useful application of sign language technology. However,
the problem of searching videos beyond individual keywords has received limited
attention in the literature. To address this gap, in this work we introduce the
task of sign language retrieval with free-form textual queries: given a written
query (e.g., a sentence) and a large collection of sign language videos, the
objective is to find the signing video in the collection that best matches the
written query. We propose to tackle this task by learning cross-modal
embeddings on the recently introduced large-scale How2Sign dataset of American
Sign Language (ASL). We identify that a key bottleneck in the performance of
the system is the quality of the sign video embedding which suffers from a
scarcity of labeled training data. We, therefore, propose SPOT-ALIGN, a
framework for interleaving iterative rounds of sign spotting and feature
alignment to expand the scope and scale of available training data. We validate
the effectiveness of SPOT-ALIGN for learning a robust sign video embedding
through improvements in both sign recognition and the proposed video retrieval
task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Repairing Adversarial Texts through Perturbation. (arXiv:2201.02504v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02504">
<div class="article-summary-box-inner">
<span><p>It is known that neural networks are subject to attacks through adversarial
perturbations, i.e., inputs which are maliciously crafted through perturbations
to induce wrong predictions. Furthermore, such attacks are impossible to
eliminate, i.e., the adversarial perturbation is still possible after applying
mitigation methods such as adversarial training. Multiple approaches have been
developed to detect and reject such adversarial inputs, mostly in the image
domain. Rejecting suspicious inputs however may not be always feasible or
ideal. First, normal inputs may be rejected due to false alarms generated by
the detection algorithm. Second, denial-of-service attacks may be conducted by
feeding such systems with adversarial inputs. To address the gap, in this work,
we propose an approach to automatically repair adversarial texts at runtime.
Given a text which is suspected to be adversarial, we novelly apply multiple
adversarial perturbation methods in a positive way to identify a repair, i.e.,
a slightly mutated but semantically equivalent text that the neural network
correctly classifies. Our approach has been experimented with multiple models
trained for natural language processing tasks and the results show that our
approach is effective, i.e., it successfully repairs about 80\% of the
adversarial texts. Furthermore, depending on the applied perturbation method,
an adversarial text could be repaired in as short as one second on average.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting Patient Readmission Risk from Medical Text via Knowledge Graph Enhanced Multiview Graph Convolution. (arXiv:2201.02510v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02510">
<div class="article-summary-box-inner">
<span><p>Unplanned intensive care unit (ICU) readmission rate is an important metric
for evaluating the quality of hospital care. Efficient and accurate prediction
of ICU readmission risk can not only help prevent patients from inappropriate
discharge and potential dangers, but also reduce associated costs of
healthcare. In this paper, we propose a new method that uses medical text of
Electronic Health Records (EHRs) for prediction, which provides an alternative
perspective to previous studies that heavily depend on numerical and
time-series features of patients. More specifically, we extract discharge
summaries of patients from their EHRs, and represent them with multiview graphs
enhanced by an external knowledge graph. Graph convolutional networks are then
used for representation learning. Experimental results prove the effectiveness
of our method, yielding state-of-the-art performance for this task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RxWhyQA: a clinical question-answering dataset with the challenge of multi-answer questions. (arXiv:2201.02517v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02517">
<div class="article-summary-box-inner">
<span><p>Objectives Create a dataset for the development and evaluation of clinical
question-answering (QA) systems that can handle multi-answer questions.
Materials and Methods We leveraged the annotated relations from the 2018
National NLP Clinical Challenges (n2c2) corpus to generate a QA dataset. The
1-to-0 and 1-to-N drug-reason relations formed the unanswerable and
multi-answer entries, which represent challenging scenarios lacking in the
existing clinical QA datasets. Results The result RxWhyQA dataset contains
91,440 QA entries, of which half are unanswerable, and 21% (n=19,269) of the
answerable ones require multiple answers. The dataset conforms to the
community-vetted Stanford Question Answering Dataset (SQuAD) format. Discussion
The RxWhyQA is useful for comparing different systems that need to handle the
zero- and multi-answer challenges, demanding dual mitigation of both false
positive and false negative answers. Conclusion We created and shared a
clinical QA dataset with a focus on multi-answer questions to represent
real-world scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Code-Switching Text Augmentation for Multilingual Speech Processing. (arXiv:2201.02550v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02550">
<div class="article-summary-box-inner">
<span><p>The pervasiveness of intra-utterance Code-switching (CS) in spoken content
has enforced ASR systems to handle mixed input. Yet, designing a CS-ASR has
many challenges, mainly due to the data scarcity, grammatical structure
complexity, and mismatch along with unbalanced language usage distribution.
Recent ASR studies showed the predominance of E2E-ASR using multilingual data
to handle CS phenomena with little CS data. However, the dependency on the CS
data still remains. In this work, we propose a methodology to augment the
monolingual data for artificially generating spoken CS text to improve
different speech modules. We based our approach on Equivalence Constraint
theory while exploiting aligned translation pairs, to generate grammatically
valid CS content. Our empirical results show a relative gain of 29-34 % in
perplexity and around 2% in WER for two ecological and noisy CS test sets.
Finally, the human evaluation suggests that 83.8% of the generated data is
acceptable to humans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assigning function to protein-protein interactions: a weakly supervised BioBERT based approach using PubMed abstracts. (arXiv:2008.08727v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.08727">
<div class="article-summary-box-inner">
<span><p>Motivation: Protein-protein interactions (PPI) are critical to the function
of proteins in both normal and diseased cells, and many critical protein
functions are mediated by interactions.Knowledge of the nature of these
interactions is important for the construction of networks to analyse
biological data. However, only a small percentage of PPIs captured in protein
interaction databases have annotations of function available, e.g. only 4% of
PPI are functionally annotated in the IntAct database. Here, we aim to label
the function type of PPIs by extracting relationships described in PubMed
abstracts.
</p>
<p>Method: We create a weakly supervised dataset from the IntAct PPI database
containing interacting protein pairs with annotated function and associated
abstracts from the PubMed database. We apply a state-of-the-art deep learning
technique for biomedical natural language processing tasks, BioBERT, to build a
model - dubbed PPI-BioBERT - for identifying the function of PPIs. In order to
extract high quality PPI functions at large scale, we use an ensemble of
PPI-BioBERT models to improve uncertainty estimation and apply an interaction
type-specific threshold to counteract the effects of variations in the number
of training samples per interaction type.
</p>
<p>Results: We scan 18 million PubMed abstracts to automatically identify 3253
new typed PPIs, including phosphorylation and acetylation interactions, with an
overall precision of 46% (87% for acetylation) based on a human-reviewed
sample. This work demonstrates that analysis of biomedical abstracts for PPI
function extraction is a feasible approach to substantially increasing the
number of interactions annotated with function captured in online databases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Monitoring Covid-19 on social media using a novel triage and diagnosis approach. (arXiv:2103.11850v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.11850">
<div class="article-summary-box-inner">
<span><p>Objective: This study aims to develop an end-to-end natural language
processing pipeline for triage and diagnosis of COVID-19 from patient-authored
social media posts, in order to provide researchers and public health
practitioners with additional information on the symptoms, severity and
prevalence of the disease rather than to provide an actionable decision at the
individual level. Materials and Methods: The text processing pipeline first
extracts COVID-19 symptoms and related concepts such as severity, duration,
negations, and body parts from patients' posts using conditional random fields.
An unsupervised rule-based algorithm is then applied to establish relations
between concepts in the next step of the pipeline. The extracted concepts and
relations are subsequently used to construct two different vector
representations of each post. These vectors are applied separately to build
support vector machine learning models to triage patients into three categories
and diagnose them for COVID-19. Results: We report that macro- and
micro-averaged F1 scores in the range of 71-96% and 61-87%, respectively, for
the triage and diagnosis of COVID-19, when the models are trained on human
labelled data. Our experimental results indicate that similar performance can
be achieved when the models are trained using predicted labels from concept
extraction and rule-based classifiers, thus yielding end-to-end machine
learning. Also, we highlight important features uncovered by our diagnostic
machine learning models and compare them with the most frequent symptoms
revealed in another COVID-19 dataset. In particular, we found that the most
important features are not always the most frequent ones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Toolbox for Construction and Analysis of Speech Datasets. (arXiv:2104.04896v3 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.04896">
<div class="article-summary-box-inner">
<span><p>Automatic Speech Recognition and Text-to-Speech systems are primarily trained
in a supervised fashion and require high-quality, accurately labeled speech
datasets. In this work, we examine common problems with speech data and
introduce a toolbox for the construction and interactive error analysis of
speech datasets. The construction tool is based on K\"urzinger et al. work,
and, to the best of our knowledge, the dataset exploration tool is the world's
first open-source tool of this kind. We demonstrate how to apply these tools to
create a Russian speech dataset and analyze existing speech datasets
(Multilingual LibriSpeech, Mozilla Common Voice). The tools are open sourced as
a part of the NeMo framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Audiomer: A Convolutional Transformer For Keyword Spotting. (arXiv:2109.10252v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10252">
<div class="article-summary-box-inner">
<span><p>Transformers have seen an unprecedented rise in Natural Language Processing
and Computer Vision tasks. However, in audio tasks, they are either infeasible
to train due to extremely large sequence length of audio waveforms or incur a
performance penalty when trained on Fourier-based features. In this work, we
introduce an architecture, Audiomer, where we combine 1D Residual Networks with
Performer Attention to achieve state-of-the-art performance in keyword spotting
with raw audio waveforms, outperforming all previous methods while being
computationally cheaper and parameter-efficient. Additionally, our model has
practical advantages for speech processing, such as inference on arbitrarily
long audio clips owing to the absence of positional encoding. The code is
available at https://github.com/The-Learning-Machines/Audiomer-PyTorch.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Multimodal Language Representations using Convolutional Autoencoders. (arXiv:2110.03007v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03007">
<div class="article-summary-box-inner">
<span><p>Multimodal Language Analysis is a demanding area of research, since it is
associated with two requirements: combining different modalities and capturing
temporal information. During the last years, several works have been proposed
in the area, mostly centered around supervised learning in downstream tasks. In
this paper we propose extracting unsupervised Multimodal Language
representations that are universal and can be applied to different tasks.
Towards this end, we map the word-level aligned multimodal sequences to 2-D
matrices and then use Convolutional Autoencoders to learn embeddings by
combining multiple datasets. Extensive experimentation on Sentiment Analysis
(MOSEI) and Emotion Recognition (IEMOCAP) indicate that the learned
representations can achieve near-state-of-the-art performance with just the use
of a Logistic Regression algorithm for downstream classification. It is also
shown that our method is extremely lightweight and can be easily generalized to
other tasks and unseen data with small performance drop and almost the same
number of parameters. The proposed multimodal representation models are
open-sourced and will help grow the applicability of Multimodal Language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ASCEND: A Spontaneous Chinese-English Dataset for Code-switching in Multi-turn Conversation. (arXiv:2112.06223v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06223">
<div class="article-summary-box-inner">
<span><p>Code-switching is a speech phenomenon when a speaker switches language during
a conversation. Despite the spontaneous nature of code-switching in
conversational spoken language, most existing works collect code-switching data
through read speech instead of spontaneous speech. ASCEND (A Spontaneous
Chinese-English Dataset) introduces a high-quality resource of spontaneous
multi-turn conversational dialogue Chinese-English code-switching corpus
collected in Hong Kong. We report ASCEND's design and procedure of collecting
the speech data, including the annotations in this work. ASCEND includes 23
bilinguals that are fluent in both Chinese and English and consists of 10.62
hours clean speech corpus. We also conduct a baseline experiment using
pre-trained wav2vec 2.0 models, achieving the best performance of 22.69%
character error rate and 27.05% mixed error rate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Consistency and Coherence from Points of Contextual Similarity. (arXiv:2112.11638v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11638">
<div class="article-summary-box-inner">
<span><p>Factual consistency is one of important summary evaluation dimensions,
especially as summary generation becomes more fluent and coherent. The ESTIME
measure, recently proposed specifically for factual consistency, achieves high
correlations with human expert scores both for consistency and fluency, while
in principle being restricted to evaluating such text-summary pairs that have
high dictionary overlap. This is not a problem for current styles of
summarization, but it may become an obstacle for future summarization systems,
or for evaluating arbitrary claims against the text. In this work we generalize
the method, and make a variant of the measure applicable to any text-summary
pairs. As ESTIME uses points of contextual similarity, it provides insights
into usefulness of information taken from different BERT layers. We observe
that useful information exists in almost all of the layers except the several
lowest ones. For consistency and fluency - qualities focused on local text
details - the most useful layers are close to the top (but not at the top); for
coherence and relevance we found a more complicated and interesting picture.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Intracranial Aneurysm Classification and Segmentation via Unsupervised Dual-branch Learning. (arXiv:2201.02198v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02198">
<div class="article-summary-box-inner">
<span><p>Intracranial aneurysms are common nowadays and how to detect them
intelligently is of great significance in digital health. While most existing
deep learning research focused on medical images in a supervised way, we
introduce an unsupervised method for the detection of intracranial aneurysms
based on 3D point cloud data. In particular, our method consists of two stages:
unsupervised pre-training and downstream tasks. As for the former, the main
idea is to pair each point cloud with its jittered counterpart and maximise
their correspondence. Then we design a dual-branch contrastive network with an
encoder for each branch and a subsequent common projection head. As for the
latter, we design simple networks for supervised classification and
segmentation training. Experiments on the public dataset (IntrA) show that our
unsupervised method achieves comparable or even better performance than some
state-of-the-art supervised techniques, and it is most prominent in the
detection of aneurysmal vessels. Experiments on the ModelNet40 also show that
our method achieves the accuracy of 90.79\% which outperforms existing
state-of-the-art unsupervised models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Consistent Style Transfer. (arXiv:2201.02233v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02233">
<div class="article-summary-box-inner">
<span><p>Recently, attentional arbitrary style transfer methods have been proposed to
achieve fine-grained results, which manipulates the point-wise similarity
between content and style features for stylization. However, the attention
mechanism based on feature points ignores the feature multi-manifold
distribution, where each feature manifold corresponds to a semantic region in
the image. Consequently, a uniform content semantic region is rendered by
highly different patterns from various style semantic regions, producing
inconsistent stylization results with visual artifacts. We proposed the
progressive attentional manifold alignment (PAMA) to alleviate this problem,
which repeatedly applies attention operations and space-aware interpolations.
The attention operation rearranges style features dynamically according to the
spatial distribution of content features. This makes the content and style
manifolds correspond on the feature map. Then the space-aware interpolation
adaptively interpolates between the corresponding content and style manifolds
to increase their similarity. By gradually aligning the content manifolds to
style manifolds, the proposed PAMA achieves state-of-the-art performance while
avoiding the inconsistency of semantic regions. Codes are available at
https://github.com/computer-vision2022/PAMA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Keypoint Detection and Description Network Based on the Vessel Structure for Multi-Modal Retinal Image Registration. (arXiv:2201.02242v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02242">
<div class="article-summary-box-inner">
<span><p>Ophthalmological imaging utilizes different imaging systems, such as color
fundus, infrared, fluorescein angiography, optical coherence tomography (OCT)
or OCT angiography. Multiple images with different modalities or acquisition
times are often analyzed for the diagnosis of retinal diseases. Automatically
aligning the vessel structures in the images by means of multi-modal
registration can support the ophthalmologists in their work. Our method uses a
convolutional neural network to extract features of the vessel structure in
multi-modal retinal images. We jointly train a keypoint detection and
description network on small patches using a classification and a cross-modal
descriptor loss function and apply the network to the full image size in the
test phase. Our method demonstrates the best registration performance on our
and a public multi-modal dataset in comparison to competing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CitySurfaces: City-Scale Semantic Segmentation of Sidewalk Materials. (arXiv:2201.02260v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02260">
<div class="article-summary-box-inner">
<span><p>While designing sustainable and resilient urban built environment is
increasingly promoted around the world, significant data gaps have made
research on pressing sustainability issues challenging to carry out. Pavements
are known to have strong economic and environmental impacts; however, most
cities lack a spatial catalog of their surfaces due to the cost-prohibitive and
time-consuming nature of data collection. Recent advancements in computer
vision, together with the availability of street-level images, provide new
opportunities for cities to extract large-scale built environment data with
lower implementation costs and higher accuracy. In this paper, we propose
CitySurfaces, an active learning-based framework that leverages computer vision
techniques for classifying sidewalk materials using widely available
street-level images. We trained the framework on images from New York City and
Boston and the evaluation results show a 90.5% mIoU score. Furthermore, we
evaluated the framework using images from six different cities, demonstrating
that it can be applied to regions with distinct urban fabrics, even outside the
domain of the training data. CitySurfaces can provide researchers and city
agencies with a low-cost, accurate, and extensible method to collect sidewalk
material data which plays a critical role in addressing major sustainability
issues, including climate change and surface water management.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ITSA: An Information-Theoretic Approach to Automatic Shortcut Avoidance and Domain Generalization in Stereo Matching Networks. (arXiv:2201.02263v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02263">
<div class="article-summary-box-inner">
<span><p>State-of-the-art stereo matching networks trained only on synthetic data
often fail to generalize to more challenging real data domains. In this paper,
we attempt to unfold an important factor that hinders the networks from
generalizing across domains: through the lens of shortcut learning. We
demonstrate that the learning of feature representations in stereo matching
networks is heavily influenced by synthetic data artefacts (shortcut
attributes). To mitigate this issue, we propose an Information-Theoretic
Shortcut Avoidance~(ITSA) approach to automatically restrict shortcut-related
information from being encoded into the feature representations. As a result,
our proposed method learns robust and shortcut-invariant features by minimizing
the sensitivity of latent features to input variations. To avoid the
prohibitive computational cost of direct input sensitivity optimization, we
propose an effective yet feasible algorithm to achieve robustness. We show that
using this method, state-of-the-art stereo matching networks that are trained
purely on synthetic data can effectively generalize to challenging and
previously unseen real data scenarios. Importantly, the proposed method
enhances the robustness of the synthetic trained networks to the point that
they outperform their fine-tuned counterparts (on real data) for challenging
out-of-domain stereo datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">De-rendering 3D Objects in the Wild. (arXiv:2201.02279v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02279">
<div class="article-summary-box-inner">
<span><p>With increasing focus on augmented and virtual reality applications (XR)
comes the demand for algorithms that can lift objects from images and videos
into representations that are suitable for a wide variety of related 3D tasks.
Large-scale deployment of XR devices and applications means that we cannot
solely rely on supervised learning, as collecting and annotating data for the
unlimited variety of objects in the real world is infeasible. We present a
weakly supervised method that is able to decompose a single image of an object
into shape (depth and normals), material (albedo, reflectivity and shininess)
and global lighting parameters. For training, the method only relies on a rough
initial shape estimate of the training objects to bootstrap the learning
process. This shape supervision can come for example from a pretrained depth
network or - more generically - from a traditional structure-from-motion
pipeline. In our experiments, we show that the method can successfully
de-render 2D images into a decomposed 3D representation and generalizes to
unseen object categories. Since in-the-wild evaluation is difficult due to the
lack of ground truth data, we also introduce a photo-realistic synthetic test
set that allows for quantitative evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Repurposing Existing Deep Networks for Caption and Aesthetic-Guided Image Cropping. (arXiv:2201.02280v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02280">
<div class="article-summary-box-inner">
<span><p>We propose a novel optimization framework that crops a given image based on
user description and aesthetics. Unlike existing image cropping methods, where
one typically trains a deep network to regress to crop parameters or cropping
actions, we propose to directly optimize for the cropping parameters by
repurposing pre-trained networks on image captioning and aesthetic tasks,
without any fine-tuning, thereby avoiding training a separate network.
Specifically, we search for the best crop parameters that minimize a combined
loss of the initial objectives of these networks. To make the optimization
table, we propose three strategies: (i) multi-scale bilinear sampling, (ii)
annealing the scale of the crop region, therefore effectively reducing the
parameter space, (iii) aggregation of multiple optimization results. Through
various quantitative and qualitative evaluations, we show that our framework
can produce crops that are well-aligned to intended user descriptions and
aesthetically pleasing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Persistent Homology for Breast Tumor Classification using Mammogram Scans. (arXiv:2201.02295v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02295">
<div class="article-summary-box-inner">
<span><p>An Important tool in the field topological data analysis is known as
persistent Homology (PH) which is used to encode abstract representation of the
homology of data at different resolutions in the form of persistence diagram
(PD). In this work we build more than one PD representation of a single image
based on a landmark selection method, known as local binary patterns, that
encode different types of local textures from images. We employed different PD
vectorizations using persistence landscapes, persistence images, persistence
binning (Betti Curve) and statistics. We tested the effectiveness of proposed
landmark based PH on two publicly available breast abnormality detection
datasets using mammogram scans. Sensitivity of landmark based PH obtained is
over 90% in both datasets for the detection of abnormal breast scans. Finally,
experimental results give new insights on using different types of PD
vectorizations which help in utilising PH in conjunction with machine learning
classifiers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extending One-Stage Detection with Open-World Proposals. (arXiv:2201.02302v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02302">
<div class="article-summary-box-inner">
<span><p>In many applications, such as autonomous driving, hand manipulation, or robot
navigation, object detection methods must be able to detect objects unseen in
the training set. Open World Detection(OWD) seeks to tackle this problem by
generalizing detection performance to seen and unseen class categories. Recent
works have seen success in the generation of class-agnostic proposals, which we
call Open-World Proposals(OWP), but this comes at the cost of a big drop on the
classification task when both tasks are considered in the detection model.
These works have investigated two-stage Region Proposal Networks (RPN) by
taking advantage of objectness scoring cues; however, for its simplicity,
run-time, and decoupling of localization and classification, we investigate OWP
through the lens of fully convolutional one-stage detection network, such as
FCOS. We show that our architectural and sampling optimizations on FCOS can
increase OWP performance by as much as 6% in recall on novel classes, marking
the first proposal-free one-stage detection network to achieve comparable
performance to RPN-based two-stage networks. Furthermore, we show that the
inherent, decoupled architecture of FCOS has benefits to retaining
classification performance. While two-stage methods worsen by 6% in recall on
novel classes, we show that FCOS only drops 2% when jointly optimizing for OWP
and classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Budget-aware Few-shot Learning via Graph Convolutional Network. (arXiv:2201.02304v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02304">
<div class="article-summary-box-inner">
<span><p>This paper tackles the problem of few-shot learning, which aims to learn new
visual concepts from a few examples. A common problem setting in few-shot
classification assumes random sampling strategy in acquiring data labels, which
is inefficient in practical applications. In this work, we introduce a new
budget-aware few-shot learning problem that not only aims to learn novel object
categories, but also needs to select informative examples to annotate in order
to achieve data efficiency.
</p>
<p>We develop a meta-learning strategy for our budget-aware few-shot learning
task, which jointly learns a novel data selection policy based on a Graph
Convolutional Network (GCN) and an example-based few-shot classifier. Our
selection policy computes a context-sensitive representation for each unlabeled
data by graph message passing, which is then used to predict an informativeness
score for sequential selection. We validate our method by extensive experiments
on the mini-ImageNet, tiered-ImageNet and Omniglot datasets. The results show
our few-shot learning strategy outperforms baselines by a sizable margin, which
demonstrates the efficacy of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A three-dimensional dual-domain deep network for high-pitch and sparse helical CT reconstruction. (arXiv:2201.02309v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02309">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a new GPU implementation of the Katsevich algorithm
for helical CT reconstruction. Our implementation divides the sinograms and
reconstructs the CT images pitch by pitch. By utilizing the periodic properties
of the parameters of the Katsevich algorithm, our method only needs to
calculate these parameters once for all the pitches and so has lower GPU-memory
burdens and is very suitable for deep learning. By embedding our implementation
into the network, we propose an end-to-end deep network for the high pitch
helical CT reconstruction with sparse detectors. Since our network utilizes the
features extracted from both sinograms and CT images, it can simultaneously
reduce the streak artifacts caused by the sparsity of sinograms and preserve
fine details in the CT images. Experiments show that our network outperforms
the related methods both in subjective and objective evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RestoreDet: Degradation Equivariant Representation for Object Detection in Low Resolution Images. (arXiv:2201.02314v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02314">
<div class="article-summary-box-inner">
<span><p>Image restoration algorithms such as super resolution (SR) are indispensable
pre-processing modules for object detection in degraded images. However, most
of these algorithms assume the degradation is fixed and known a priori. When
the real degradation is unknown or differs from assumption, both the
pre-processing module and the consequent high-level task such as object
detection would fail. Here, we propose a novel framework, RestoreDet, to detect
objects in degraded low resolution images. RestoreDet utilizes the downsampling
degradation as a kind of transformation for self-supervised signals to explore
the equivariant representation against various resolutions and other
degradation conditions. Specifically, we learn this intrinsic visual structure
by encoding and decoding the degradation transformation from a pair of original
and randomly degraded images. The framework could further take the advantage of
advanced SR architectures with an arbitrary resolution restoring decoder to
reconstruct the original correspondence from the degraded input image. Both the
representation learning and object detection are optimized jointly in an
end-to-end training fashion. RestoreDet is a generic framework that could be
implemented on any mainstream object detection architectures. The extensive
experiment shows that our framework based on CenterNet has achieved superior
performance compared with existing methods when facing variant degradation
situations. Our code would be released soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiresolution Fully Convolutional Networks to detect Clouds and Snow through Optical Satellite Images. (arXiv:2201.02350v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02350">
<div class="article-summary-box-inner">
<span><p>Clouds and snow have similar spectral features in the visible and
near-infrared (VNIR) range and are thus difficult to distinguish from each
other in high resolution VNIR images. We address this issue by introducing a
shortwave-infrared (SWIR) band where clouds are highly reflective, and snow is
absorptive. As SWIR is typically of a lower resolution compared to VNIR, this
study proposes a multiresolution fully convolutional neural network (FCN) that
can effectively detect clouds and snow in VNIR images. We fuse the
multiresolution bands within a deep FCN and perform semantic segmentation at
the higher, VNIR resolution. Such a fusion-based classifier, trained in an
end-to-end manner, achieved 94.31% overall accuracy and an F1 score of 97.67%
for clouds on Resourcesat-2 data captured over the state of Uttarakhand, India.
These scores were found to be 30% higher than a Random Forest classifier, and
10% higher than a standalone single-resolution FCN. Apart from being useful for
cloud detection purposes, the study also highlights the potential of
convolutional neural networks for multi-sensor fusion problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Modality Deep Feature Learning for Brain Tumor Segmentation. (arXiv:2201.02356v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02356">
<div class="article-summary-box-inner">
<span><p>Recent advances in machine learning and prevalence of digital medical images
have opened up an opportunity to address the challenging brain tumor
segmentation (BTS) task by using deep convolutional neural networks. However,
different from the RGB image data that are very widespread, the medical image
data used in brain tumor segmentation are relatively scarce in terms of the
data scale but contain the richer information in terms of the modality
property. To this end, this paper proposes a novel cross-modality deep feature
learning framework to segment brain tumors from the multi-modality MRI data.
The core idea is to mine rich patterns across the multi-modality data to make
up for the insufficient data scale. The proposed cross-modality deep feature
learning framework consists of two learning processes: the cross-modality
feature transition (CMFT) process and the cross-modality feature fusion (CMFF)
process, which aims at learning rich feature representations by transiting
knowledge across different modality data and fusing knowledge from different
modality data, respectively. Comprehensive experiments are conducted on the
BraTS benchmarks, which show that the proposed cross-modality deep feature
learning framework can effectively improve the brain tumor segmentation
performance when compared with the baseline methods and state-of-the-art
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Motion Prediction via Joint Dependency Modeling in Phase Space. (arXiv:2201.02365v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02365">
<div class="article-summary-box-inner">
<span><p>Motion prediction is a classic problem in computer vision, which aims at
forecasting future motion given the observed pose sequence. Various deep
learning models have been proposed, achieving state-of-the-art performance on
motion prediction. However, existing methods typically focus on modeling
temporal dynamics in the pose space. Unfortunately, the complicated and high
dimensionality nature of human motion brings inherent challenges for dynamic
context capturing. Therefore, we move away from the conventional pose based
representation and present a novel approach employing a phase space trajectory
representation of individual joints. Moreover, current methods tend to only
consider the dependencies between physically connected joints. In this paper,
we introduce a novel convolutional neural model to effectively leverage
explicit prior knowledge of motion anatomy, and simultaneously capture both
spatial and temporal information of joint trajectory dynamics. We then propose
a global optimization module that learns the implicit relationships between
individual joint features.
</p>
<p>Empirically, our method is evaluated on large-scale 3D human motion benchmark
datasets (i.e., Human3.6M, CMU MoCap). These results demonstrate that our
method sets the new state-of-the-art on the benchmark datasets. Our code will
be available at https://github.com/Pose-Group/TEID.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty-Aware Cascaded Dilation Filtering for High-Efficiency Deraining. (arXiv:2201.02366v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02366">
<div class="article-summary-box-inner">
<span><p>Deraining is a significant and fundamental computer vision task, aiming to
remove the rain streaks and accumulations in an image or video captured under a
rainy day. Existing deraining methods usually make heuristic assumptions of the
rain model, which compels them to employ complex optimization or iterative
refinement for high recovery quality. This, however, leads to time-consuming
methods and affects the effectiveness for addressing rain patterns deviated
from from the assumptions. In this paper, we propose a simple yet efficient
deraining method by formulating deraining as a predictive filtering problem
without complex rain model assumptions. Specifically, we identify
spatially-variant predictive filtering (SPFilt) that adaptively predicts proper
kernels via a deep network to filter different individual pixels. Since the
filtering can be implemented via well-accelerated convolution, our method can
be significantly efficient. We further propose the EfDeRain+ that contains
three main contributions to address residual rain traces, multi-scale, and
diverse rain patterns without harming the efficiency. First, we propose the
uncertainty-aware cascaded predictive filtering (UC-PFilt) that can identify
the difficulties of reconstructing clean pixels via predicted kernels and
remove the residual rain traces effectively. Second, we design the
weight-sharing multi-scale dilated filtering (WS-MS-DFilt) to handle
multi-scale rain streaks without harming the efficiency. Third, to eliminate
the gap across diverse rain patterns, we propose a novel data augmentation
method (i.e., RainMix) to train our deep models. By combining all contributions
with sophisticated analysis on different variants, our final method outperforms
baseline methods on four single-image deraining datasets and one video
deraining dataset in terms of both recovery quality and speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Generative Framework for Interactive 3D Terrain Authoring and Manipulation. (arXiv:2201.02369v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02369">
<div class="article-summary-box-inner">
<span><p>Automated generation and (user) authoring of the realistic virtual terrain is
most sought for by the multimedia applications like VR models and gaming. The
most common representation adopted for terrain is Digital Elevation Model
(DEM). Existing terrain authoring and modeling techniques have addressed some
of these and can be broadly categorized as: procedural modeling, simulation
method, and example-based methods. In this paper, we propose a novel realistic
terrain authoring framework powered by a combination of VAE and generative
conditional GAN model. Our framework is an example-based method that attempts
to overcome the limitations of existing methods by learning a latent space from
a real-world terrain dataset. This latent space allows us to generate multiple
variants of terrain from a single input as well as interpolate between terrains
while keeping the generated terrains close to real-world data distribution. We
also developed an interactive tool, that lets the user generate diverse
terrains with minimalist inputs. We perform thorough qualitative and
quantitative analysis and provide comparisons with other SOTA methods. We
intend to release our code/tool to the academic community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Human-to-Human-or-Object (H2O) Interactions with DIABOLO. (arXiv:2201.02396v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02396">
<div class="article-summary-box-inner">
<span><p>Detecting human interactions is crucial for human behavior analysis. Many
methods have been proposed to deal with Human-to-Object Interaction (HOI)
detection, i.e., detecting in an image which person and object interact
together and classifying the type of interaction. However, Human-to-Human
Interactions, such as social and violent interactions, are generally not
considered in available HOI training datasets. As we think these types of
interactions cannot be ignored and decorrelated from HOI when analyzing human
behavior, we propose a new interaction dataset to deal with both types of human
interactions: Human-to-Human-or-Object (H2O). In addition, we introduce a novel
taxonomy of verbs, intended to be closer to a description of human body
attitude in relation to the surrounding targets of interaction, and more
independent of the environment. Unlike some existing datasets, we strive to
avoid defining synonymous verbs when their use highly depends on the target
type or requires a high level of semantic interpretation. As H2O dataset
includes V-COCO images annotated with this new taxonomy, images obviously
contain more interactions. This can be an issue for HOI detection methods whose
complexity depends on the number of people, targets or interactions. Thus, we
propose DIABOLO (Detecting InterActions By Only Looking Once), an efficient
subject-centric single-shot method to detect all interactions in one forward
pass, with constant inference time independent of image content. In addition,
this multi-task network simultaneously detects all people and objects. We show
how sharing a network for these tasks does not only save computation resource
but also improves performance collaboratively. Finally, DIABOLO is a strong
baseline for the new proposed challenge of H2O Interaction detection, as it
outperforms all state-of-the-art methods when trained and evaluated on HOI
dataset V-COCO.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Amplitude SAR Imagery Splicing Localization. (arXiv:2201.02409v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02409">
<div class="article-summary-box-inner">
<span><p>Synthetic Aperture Radar (SAR) images are a valuable asset for a wide variety
of tasks. In the last few years, many websites have been offering them for free
in the form of easy to manage products, favoring their widespread diffusion and
research work in the SAR field. The drawback of these opportunities is that
such images might be exposed to forgeries and manipulations by malicious users,
raising new concerns about their integrity and trustworthiness. Up to now, the
multimedia forensics literature has proposed various techniques to localize
manipulations in natural photographs, but the integrity assessment of SAR
images was never investigated. This task poses new challenges, since SAR images
are generated with a processing chain completely different from that of natural
photographs. This implies that many forensics methods developed for natural
images are not guaranteed to succeed. In this paper, we investigate the problem
of amplitude SAR imagery splicing localization. Our goal is to localize regions
of an amplitude SAR image that have been copied and pasted from another image,
possibly undergoing some kind of editing in the process. To do so, we leverage
a Convolutional Neural Network (CNN) to extract a fingerprint highlighting
inconsistencies in the processing traces of the analyzed input. Then, we
examine this fingerprint to produce a binary tampering mask indicating the
pixel region under splicing attack. Results show that our proposed method,
tailored to the nature of SAR signals, provides better performances than
state-of-the-art forensic tools developed for natural images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Auto-Weighted Layer Representation Based View Synthesis Distortion Estimation for 3-D Video Coding. (arXiv:2201.02420v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02420">
<div class="article-summary-box-inner">
<span><p>Recently, various view synthesis distortion estimation models have been
studied to better serve for 3-D video coding. However, they can hardly model
the relationship quantitatively among different levels of depth changes,
texture degeneration, and the view synthesis distortion (VSD), which is crucial
for rate-distortion optimization and rate allocation. In this paper, an
auto-weighted layer representation based view synthesis distortion estimation
model is developed. Firstly, the sub-VSD (S-VSD) is defined according to the
level of depth changes and their associated texture degeneration. After that, a
set of theoretical derivations demonstrate that the VSD can be approximately
decomposed into the S-VSDs multiplied by their associated weights. To obtain
the S-VSDs, a layer-based representation of S-VSD is developed, where all the
pixels with the same level of depth changes are represented with a layer to
enable efficient S-VSD calculation at the layer level. Meanwhile, a nonlinear
mapping function is learnt to accurately represent the relationship between the
VSD and S-VSDs, automatically providing weights for S-VSDs during the VSD
estimation. To learn such function, a dataset of VSD and its associated S-VSDs
are built. Experimental results show that the VSD can be accurately estimated
with the weights learnt by the nonlinear mapping function once its associated
S-VSDs are available. The proposed method outperforms the relevant
state-of-the-art methods in both accuracy and efficiency. The dataset and
source code of the proposed method will be available at
https://github.com/jianjin008/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effect of Prior-based Losses on Segmentation Performance: A Benchmark. (arXiv:2201.02428v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02428">
<div class="article-summary-box-inner">
<span><p>Today, deep convolutional neural networks (CNNs) have demonstrated
state-of-the-art performance for medical image segmentation, on various imaging
modalities and tasks. Despite early success, segmentation networks may still
generate anatomically aberrant segmentations, with holes or inaccuracies near
the object boundaries. To enforce anatomical plausibility, recent research
studies have focused on incorporating prior knowledge such as object shape or
boundary, as constraints in the loss function. Prior integrated could be
low-level referring to reformulated representations extracted from the
ground-truth segmentations, or high-level representing external medical
information such as the organ's shape or size. Over the past few years,
prior-based losses exhibited a rising interest in the research field since they
allow integration of expert knowledge while still being architecture-agnostic.
However, given the diversity of prior-based losses on different medical imaging
challenges and tasks, it has become hard to identify what loss works best for
which dataset. In this paper, we establish a benchmark of recent prior-based
losses for medical image segmentation. The main objective is to provide
intuition onto which losses to choose given a particular task or dataset. To
this end, four low-level and high-level prior-based losses are selected. The
considered losses are validated on 8 different datasets from a variety of
medical image segmentation challenges including the Decathlon, the ISLES and
the WMH challenge. Results show that whereas low-level prior-based losses can
guarantee an increase in performance over the Dice loss baseline regardless of
the dataset characteristics, high-level prior-based losses can increase
anatomical plausibility as per data characteristics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Negative Evidence Matters in Interpretable Histology Image Classification. (arXiv:2201.02445v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02445">
<div class="article-summary-box-inner">
<span><p>Using only global annotations such as the image class labels,
weakly-supervised learning methods allow CNN classifiers to jointly classify an
image, and yield the regions of interest associated with the predicted class.
However, without any guidance at the pixel level, such methods may yield
inaccurate regions. This problem is known to be more challenging with histology
images than with natural ones, since objects are less salient, structures have
more variations, and foreground and background regions have stronger
similarities. Therefore, methods in computer vision literature for visual
interpretation of CNNs may not directly apply. In this work, we propose a
simple yet efficient method based on a composite loss function that leverages
information from the fully negative samples. Our new loss function contains two
complementary terms: the first exploits positive evidence collected from the
CNN classifier, while the second leverages the fully negative samples from the
training dataset. In particular, we equip a pre-trained classifier with a
decoder that allows refining the regions of interest. The same classifier is
exploited to collect both the positive and negative evidence at the pixel level
to train the decoder. This enables to take advantages of the fully negative
samples that occurs naturally in the data, without any additional supervision
signals and using only the image class as supervision. Compared to several
recent related methods, over the public benchmark GlaS for colon cancer and a
Camelyon16 patch-based benchmark for breast cancer using three different
backbones, we show the substantial improvements introduced by our method. Our
results shows the benefits of using both negative and positive evidence, ie,
the one obtained from a classifier and the one naturally available in datasets.
We provide an ablation study of both terms. Our code is publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Domain Adversarial Adaptation for Photon-efficient Imaging Based on Spatiotemporal Inception Network. (arXiv:2201.02475v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02475">
<div class="article-summary-box-inner">
<span><p>In single-photon LiDAR, photon-efficient imaging captures the 3D structure of
a scene by only several detected signal photons per pixel. The existing deep
learning models for this task are trained on simulated datasets, which poses
the domain shift challenge when applied to realistic scenarios. In this paper,
we propose a spatiotemporal inception network (STIN) for photon-efficient
imaging, which is able to precisely predict the depth from a sparse and
high-noise photon counting histogram by fully exploiting spatial and temporal
information. Then the domain adversarial adaptation frameworks, including
domain-adversarial neural network and adversarial discriminative domain
adaptation, are effectively applied to STIN to alleviate the domain shift
problem for realistic applications. Comprehensive experiments on the simulated
data generated from the NYU~v2 and the Middlebury datasets demonstrate that
STIN outperforms the state-of-the-art models at low signal-to-background ratios
from 2:10 to 2:100. Moreover, experimental results on the real-world dataset
captured by the single-photon imaging prototype show that the STIN with domain
adversarial training achieves better generalization performance compared with
the state-of-the-arts as well as the baseline STIN trained by simulated data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bayesian Neural Networks for Reversible Steganography. (arXiv:2201.02478v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02478">
<div class="article-summary-box-inner">
<span><p>Recent advances in deep learning have led to a paradigm shift in reversible
steganography. A fundamental pillar of reversible steganography is predictive
modelling which can be realised via deep neural networks. However, non-trivial
errors exist in inferences about some out-of-distribution and noisy data. In
view of this issue, we propose to consider uncertainty in predictive models
based upon a theoretical framework of Bayesian deep learning. Bayesian neural
networks can be regarded as self-aware machinery; that is, a machine that knows
its own limitations. To quantify uncertainty, we approximate the posterior
predictive distribution through Monte Carlo sampling with stochastic forward
passes. We further show that predictive uncertainty can be disentangled into
aleatoric and epistemic uncertainties and these quantities can be learnt in an
unsupervised manner. Experimental results demonstrate an improvement delivered
by Bayesian uncertainty analysis upon steganographic capacity-distortion
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Summarization Based on Video-text Representation. (arXiv:2201.02494v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02494">
<div class="article-summary-box-inner">
<span><p>Modern video summarization methods are based on deep neural networks which
require a large amount of annotated data for training. However, existing
datasets for video summarization are small-scale, easily leading to
over-fitting of the deep models. Considering that the annotation of large-scale
datasets is time-consuming, we propose a multimodal self-supervised learning
framework to obtain semantic representations of videos, which benefits the
video summarization task. Specifically, we explore the semantic consistency
between the visual information and text information of videos, for the
self-supervised pretraining of a multimodal encoder on a newly-collected
dataset of video-text pairs. Additionally, we introduce a progressive video
summarization method, where the important content in a video is pinpointed
progressively to generate better summaries. Finally, an objective evaluation
framework is proposed to measure the quality of video summaries based on video
classification. Extensive experiments have proved the effectiveness and
superiority of our method in rank correlation coefficients, F-score, and the
proposed objective evaluation compared to the state of the art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sign Language Video Retrieval with Free-Form Textual Queries. (arXiv:2201.02495v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02495">
<div class="article-summary-box-inner">
<span><p>Systems that can efficiently search collections of sign language videos have
been highlighted as a useful application of sign language technology. However,
the problem of searching videos beyond individual keywords has received limited
attention in the literature. To address this gap, in this work we introduce the
task of sign language retrieval with free-form textual queries: given a written
query (e.g., a sentence) and a large collection of sign language videos, the
objective is to find the signing video in the collection that best matches the
written query. We propose to tackle this task by learning cross-modal
embeddings on the recently introduced large-scale How2Sign dataset of American
Sign Language (ASL). We identify that a key bottleneck in the performance of
the system is the quality of the sign video embedding which suffers from a
scarcity of labeled training data. We, therefore, propose SPOT-ALIGN, a
framework for interleaving iterative rounds of sign spotting and feature
alignment to expand the scope and scale of available training data. We validate
the effectiveness of SPOT-ALIGN for learning a robust sign video embedding
through improvements in both sign recognition and the proposed video retrieval
task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Review of Deep Learning Techniques for Markerless Human Motion on Synthetic Datasets. (arXiv:2201.02503v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02503">
<div class="article-summary-box-inner">
<span><p>Markerless motion capture has become an active field of research in computer
vision in recent years. Its extensive applications are known in a great variety
of fields, including computer animation, human motion analysis, biomedical
research, virtual reality, and sports science. Estimating human posture has
recently gained increasing attention in the computer vision community, but due
to the depth of uncertainty and the lack of the synthetic datasets, it is a
challenging task. Various approaches have recently been proposed to solve this
problem, many of which are based on deep learning. They are primarily focused
on improving the performance of existing benchmarks with significant advances,
especially 2D images. Based on powerful deep learning techniques and recently
collected real-world datasets, we explored a model that can predict the
skeleton of an animation based solely on 2D images. Frames generated from
different real-world datasets with synthesized poses using different body
shapes from simple to complex. The implementation process uses DeepLabCut on
its own dataset to perform many necessary steps, then use the input frames to
train the model. The output is an animated skeleton for human movement. The
composite dataset and other results are the "ground truth" of the deep model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Target-aware Representation for Visual Tracking via Informative Interactions. (arXiv:2201.02526v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02526">
<div class="article-summary-box-inner">
<span><p>We introduce a novel backbone architecture to improve target-perception
ability of feature representation for tracking. Specifically, having observed
that de facto frameworks perform feature matching simply using the outputs from
backbone for target localization, there is no direct feedback from the matching
module to the backbone network, especially the shallow layers. More concretely,
only the matching module can directly access the target information (in the
reference frame), while the representation learning of candidate frame is blind
to the reference target. As a consequence, the accumulation effect of
target-irrelevant interference in the shallow stages may degrade the feature
quality of deeper layers. In this paper, we approach the problem from a
different angle by conducting multiple branch-wise interactions inside the
Siamese-like backbone networks (InBN). At the core of InBN is a general
interaction modeler (GIM) that injects the prior knowledge of reference image
to different stages of the backbone network, leading to better
target-perception and robust distractor-resistance of candidate feature
representation with negligible computation cost. The proposed GIM module and
InBN mechanism are general and applicable to different backbone types including
CNN and Transformer for improvements, as evidenced by our extensive experiments
on multiple benchmarks. In particular, the CNN version (based on SiamCAR)
improves the baseline with 3.2/6.9 absolute gains of SUC on LaSOT/TNL2K,
respectively. The Transformer version obtains SUC scores of 65.7/52.0 on
LaSOT/TNL2K, which are on par with recent state of the arts. Code and models
will be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeROIC: Neural Rendering of Objects from Online Image Collections. (arXiv:2201.02533v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02533">
<div class="article-summary-box-inner">
<span><p>We present a novel method to acquire object representations from online image
collections, capturing high-quality geometry and material properties of
arbitrary objects from photographs with varying cameras, illumination, and
backgrounds. This enables various object-centric rendering applications such as
novel-view synthesis, relighting, and harmonized background composition from
challenging in-the-wild input. Using a multi-stage approach extending neural
radiance fields, we first infer the surface geometry and refine the coarsely
estimated initial camera parameters, while leveraging coarse foreground object
masks to improve the training efficiency and geometry quality. We also
introduce a robust normal estimation technique which eliminates the effect of
geometric noise while retaining crucial details. Lastly, we extract surface
material properties and ambient illumination, represented in spherical
harmonics with extensions that handle transient elements, e.g. sharp shadows.
The union of these components results in a highly modular and efficient object
acquisition framework. Extensive evaluations and comparisons demonstrate the
advantages of our approach in capturing high-quality geometry and appearance
properties useful for rendering applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Incremental Learning Driven Instance Segmentation Framework to Recognize Highly Cluttered Instances of the Contraband Items. (arXiv:2201.02560v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02560">
<div class="article-summary-box-inner">
<span><p>Screening cluttered and occluded contraband items from baggage X-ray scans is
a cumbersome task even for the expert security staff. This paper presents a
novel strategy that extends a conventional encoder-decoder architecture to
perform instance-aware segmentation and extract merged instances of contraband
items without using any additional sub-network or an object detector. The
encoder-decoder network first performs conventional semantic segmentation and
retrieves cluttered baggage items. The model then incrementally evolves during
training to recognize individual instances using significantly reduced training
batches. To avoid catastrophic forgetting, a novel objective function minimizes
the network loss in each iteration by retaining the previously acquired
knowledge while learning new class representations and resolving their complex
structural inter-dependencies through Bayesian inference. A thorough evaluation
of our framework on two publicly available X-ray datasets shows that it
outperforms state-of-the-art methods, especially within the challenging
cluttered scenarios, while achieving an optimal trade-off between detection
accuracy and efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Incremental Learning Approach to Automatically Recognize Pulmonary Diseases from the Multi-vendor Chest Radiographs. (arXiv:2201.02574v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02574">
<div class="article-summary-box-inner">
<span><p>Pulmonary diseases can cause severe respiratory problems, leading to sudden
death if not treated timely. Many researchers have utilized deep learning
systems to diagnose pulmonary disorders using chest X-rays (CXRs). However,
such systems require exhaustive training efforts on large-scale data to
effectively diagnose chest abnormalities. Furthermore, procuring such
large-scale data is often infeasible and impractical, especially for rare
diseases. With the recent advances in incremental learning, researchers have
periodically tuned deep neural networks to learn different classification tasks
with few training examples. Although, such systems can resist catastrophic
forgetting, they treat the knowledge representations independently of each
other, and this limits their classification performance. Also, to the best of
our knowledge, there is no incremental learning-driven image diagnostic
framework that is specifically designed to screen pulmonary disorders from the
CXRs. To address this, we present a novel framework that can learn to screen
different chest abnormalities incrementally. In addition to this, the proposed
framework is penalized through an incremental learning loss function that
infers Bayesian theory to recognize structural and semantic inter-dependencies
between incrementally learned knowledge representations to diagnose the
pulmonary diseases effectively, regardless of the scanner specifications. We
tested the proposed framework on five public CXR datasets containing different
chest abnormalities, where it outperformed various state-of-the-art system
through various metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Scale-Invariance and Uncertainity with Self-Supervised Domain Adaptation for Semantic Segmentation of Foggy Scenes. (arXiv:2201.02588v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02588">
<div class="article-summary-box-inner">
<span><p>This paper presents FogAdapt, a novel approach for domain adaptation of
semantic segmentation for dense foggy scenes. Although significant research has
been directed to reduce the domain shift in semantic segmentation, adaptation
to scenes with adverse weather conditions remains an open question. Large
variations in the visibility of the scene due to weather conditions, such as
fog, smog, and haze, exacerbate the domain shift, thus making unsupervised
adaptation in such scenarios challenging. We propose a self-entropy and
multi-scale information augmented self-supervised domain adaptation method
(FogAdapt) to minimize the domain shift in foggy scenes segmentation. Supported
by the empirical evidence that an increase in fog density results in high
self-entropy for segmentation probabilities, we introduce a self-entropy based
loss function to guide the adaptation method. Furthermore, inferences obtained
at different image scales are combined and weighted by the uncertainty to
generate scale-invariant pseudo-labels for the target domain. These
scale-invariant pseudo-labels are robust to visibility and scale variations. We
evaluate the proposed model on real clear-weather scenes to real foggy scenes
adaptation and synthetic non-foggy images to real foggy scenes adaptation
scenarios. Our experiments demonstrate that FogAdapt significantly outperforms
the current state-of-the-art in semantic segmentation of foggy images.
Specifically, by considering the standard settings compared to state-of-the-art
(SOTA) methods, FogAdapt gains 3.8% on Foggy Zurich, 6.0% on Foggy
Driving-dense, and 3.6% on Foggy Driving in mIoU when adapted from Cityscapes
to Foggy Zurich.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Equalized Focal Loss for Dense Long-Tailed Object Detection. (arXiv:2201.02593v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02593">
<div class="article-summary-box-inner">
<span><p>Despite the recent success of long-tailed object detection, almost all
long-tailed object detectors are developed based on the two-stage paradigm. In
practice, one-stage detectors are more prevalent in the industry because they
have a simple and fast pipeline that is easy to deploy. However, in the
long-tailed scenario, this line of work has not been explored so far. In this
paper, we investigate whether one-stage detectors can perform well in this
case. We discover the primary obstacle that prevents one-stage detectors from
achieving excellent performance is: categories suffer from different degrees of
positive-negative imbalance problems under the long-tailed data distribution.
The conventional focal loss balances the training process with the same
modulating factor for all categories, thus failing to handle the long-tailed
problem. To address this issue, we propose the Equalized Focal Loss (EFL) that
rebalances the loss contribution of positive and negative samples of different
categories independently according to their imbalance degrees. Specifically,
EFL adopts a category-relevant modulating factor which can be adjusted
dynamically by the training status of different categories. Extensive
experiments conducted on the challenging LVIS v1 benchmark demonstrate the
effectiveness of our proposed method. With an end-to-end training pipeline, EFL
achieves 29.2% in terms of overall AP and obtains significant performance
improvements on rare categories, surpassing all existing state-of-the-art
methods. The code is available at https://github.com/ModelTC/EOD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Twenty-thousand Classes using Image-level Supervision. (arXiv:2201.02605v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02605">
<div class="article-summary-box-inner">
<span><p>Current object detectors are limited in vocabulary size due to the small
scale of detection datasets. Image classifiers, on the other hand, reason about
much larger vocabularies, as their datasets are larger and easier to collect.
We propose Detic, which simply trains the classifiers of a detector on image
classification data and thus expands the vocabulary of detectors to tens of
thousands of concepts. Unlike prior work, Detic does not assign image labels to
boxes based on model predictions, making it much easier to implement and
compatible with a range of detection architectures and backbones. Our results
show that Detic yields excellent detectors even for classes without box
annotations. It outperforms prior work on both open-vocabulary and long-tail
detection benchmarks. Detic provides a gain of 2.4 mAP for all classes and 8.3
mAP for novel classes on the open-vocabulary LVIS benchmark. On the standard
LVIS benchmark, Detic reaches 41.7 mAP for all classes and 41.7 mAP for rare
classes. For the first time, we train a detector with all the
twenty-one-thousand classes of the ImageNet dataset and show that it
generalizes to new datasets without fine-tuning. Code is available at
https://github.com/facebookresearch/Detic.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalized Category Discovery. (arXiv:2201.02609v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02609">
<div class="article-summary-box-inner">
<span><p>In this paper, we consider a highly general image recognition setting
wherein, given a labelled and unlabelled set of images, the task is to
categorize all images in the unlabelled set. Here, the unlabelled images may
come from labelled classes or from novel ones. Existing recognition methods are
not able to deal with this setting, because they make several restrictive
assumptions, such as the unlabelled instances only coming from known - or
unknown - classes and the number of unknown classes being known a-priori. We
address the more unconstrained setting, naming it 'Generalized Category
Discovery', and challenge all these assumptions. We first establish strong
baselines by taking state-of-the-art algorithms from novel category discovery
and adapting them for this task. Next, we propose the use of vision
transformers with contrastive representation learning for this open world
setting. We then introduce a simple yet effective semi-supervised $k$-means
method to cluster the unlabelled data into seen and unseen classes
automatically, substantially outperforming the baselines. Finally, we also
propose a new approach to estimate the number of classes in the unlabelled
data. We thoroughly evaluate our approach on public datasets for generic object
classification including CIFAR10, CIFAR100 and ImageNet-100, and for
fine-grained visual recognition including CUB, Stanford Cars and Herbarium19,
benchmarking on this new setting to foster future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Embodied Hands: Modeling and Capturing Hands and Bodies Together. (arXiv:2201.02610v1 [cs.GR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02610">
<div class="article-summary-box-inner">
<span><p>Humans move their hands and bodies together to communicate and solve tasks.
Capturing and replicating such coordinated activity is critical for virtual
characters that behave realistically. Surprisingly, most methods treat the 3D
modeling and tracking of bodies and hands separately. Here we formulate a model
of hands and bodies interacting together and fit it to full-body 4D sequences.
When scanning or capturing the full body in 3D, hands are small and often
partially occluded, making their shape and pose hard to recover. To cope with
low-resolution, occlusion, and noise, we develop a new model called MANO (hand
Model with Articulated and Non-rigid defOrmations). MANO is learned from around
1000 high-resolution 3D scans of hands of 31 subjects in a wide variety of hand
poses. The model is realistic, low-dimensional, captures non-rigid shape
changes with pose, is compatible with standard graphics packages, and can fit
any human hand. MANO provides a compact mapping from hand poses to pose blend
shape corrections and a linear manifold of pose synergies. We attach MANO to a
standard parameterized 3D body shape model (SMPL), resulting in a fully
articulated body and hand model (SMPL+H). We illustrate SMPL+H by fitting
complex, natural, activities of subjects captured with a 4D scanner. The
fitting is fully automatic and results in full body models that move naturally
with detailed hand motions and a realism not seen before in full body
performance capture. The models and data are freely available for research
purposes in our website (<a href="http://mano.is.tue.mpg.de">this http URL</a>).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Compare Relation: Semantic Alignment for Few-Shot Learning. (arXiv:2003.00210v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.00210">
<div class="article-summary-box-inner">
<span><p>Few-shot learning is a fundamental and challenging problem since it requires
recognizing novel categories from only a few examples. The objects for
recognition have multiple variants and can locate anywhere in images. Directly
comparing query images with example images can not handle content misalignment.
The representation and metric for comparison are critical but challenging to
learn due to the scarcity and wide variation of the samples in few-shot
learning. In this paper, we present a novel semantic alignment model to compare
relations, which is robust to content misalignment. We propose to add two key
ingredients to existing few-shot learning frameworks for better feature and
metric learning ability. First, we introduce a semantic alignment loss to align
the relation statistics of the features from samples that belong to the same
category. And second, local and global mutual information maximization is
introduced, allowing for representations that contain locally-consistent and
intra-class shared information across structural locations in an image.
Thirdly, we introduce a principled approach to weigh multiple loss functions by
considering the homoscedastic uncertainty of each stream. We conduct extensive
experiments on several few-shot learning datasets. Experimental results show
that the proposed method is capable of comparing relations with semantic
alignment strategies, and achieves state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SpinalNet: Deep Neural Network with Gradual Input. (arXiv:2007.03347v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.03347">
<div class="article-summary-box-inner">
<span><p>Deep neural networks (DNNs) have achieved the state of the art performance in
numerous fields. However, DNNs need high computation times, and people always
expect better performance in a lower computation. Therefore, we study the human
somatosensory system and design a neural network (SpinalNet) to achieve higher
accuracy with fewer computations. Hidden layers in traditional NNs receive
inputs in the previous layer, apply activation function, and then transfer the
outcomes to the next layer. In the proposed SpinalNet, each layer is split into
three splits: 1) input split, 2) intermediate split, and 3) output split. Input
split of each layer receives a part of the inputs. The intermediate split of
each layer receives outputs of the intermediate split of the previous layer and
outputs of the input split of the current layer. The number of incoming weights
becomes significantly lower than traditional DNNs. The SpinalNet can also be
used as the fully connected or classification layer of DNN and supports both
traditional learning and transfer learning. We observe significant error
reductions with lower computational costs in most of the DNNs. Traditional
learning on the VGG-5 network with SpinalNet classification layers provided the
state-of-the-art (SOTA) performance on QMNIST, Kuzushiji-MNIST, EMNIST
(Letters, Digits, and Balanced) datasets. Traditional learning with ImageNet
pre-trained initial weights and SpinalNet classification layers provided the
SOTA performance on STL-10, Fruits 360, Bird225, and Caltech-101 datasets. The
scripts of the proposed SpinalNet are available at the following link:
https://github.com/dipuk0506/SpinalNet
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Image Retrieval-based Visual Localization using Kapture. (arXiv:2007.13867v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.13867">
<div class="article-summary-box-inner">
<span><p>Visual localization tackles the challenge of estimating the camera pose from
images by using correspondence analysis between query images and a map. This
task is computation and data intensive which poses challenges on thorough
evaluation of methods on various datasets. However, in order to further advance
in the field, we claim that robust visual localization algorithms should be
evaluated on multiple datasets covering a broad domain variety. To facilitate
this, we introduce kapture, a new, flexible, unified data format and toolbox
for visual localization and structure-from-motion (SFM). It enables easy usage
of different datasets as well as efficient and reusable data processing. To
demonstrate this, we present a versatile pipeline for visual localization that
facilitates the use of different local and global features, 3D data (e.g. depth
maps), non-vision sensor data (e.g. IMU, GPS, WiFi), and various processing
algorithms. Using multiple configurations of the pipeline, we show the great
versatility of kapture in our experiments. Furthermore, we evaluate our methods
on eight public datasets where they rank top on all and first on many of them.
To foster future research, we release code, models, and all datasets used in
this paper in the kapture format open source under a permissive BSD license.
github.com/naver/kapture, github.com/naver/kapture-localization
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiffusionNet: Discretization Agnostic Learning on Surfaces. (arXiv:2012.00888v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.00888">
<div class="article-summary-box-inner">
<span><p>We introduce a new general-purpose approach to deep learning on 3D surfaces,
based on the insight that a simple diffusion layer is highly effective for
spatial communication. The resulting networks are automatically robust to
changes in resolution and sampling of a surface -- a basic property which is
crucial for practical applications. Our networks can be discretized on various
geometric representations such as triangle meshes or point clouds, and can even
be trained on one representation then applied to another. We optimize the
spatial support of diffusion as a continuous network parameter ranging from
purely local to totally global, removing the burden of manually choosing
neighborhood sizes. The only other ingredients in the method are a multi-layer
perceptron applied independently at each point, and spatial gradient features
to support directional filters. The resulting networks are simple, robust, and
efficient. Here, we focus primarily on triangle mesh surfaces, and demonstrate
state-of-the-art results for a variety of tasks including surface
classification, segmentation, and non-rigid correspondence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Adversarial Robustness of Multi-Sensor Perception Systems in Self Driving. (arXiv:2101.06784v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.06784">
<div class="article-summary-box-inner">
<span><p>Modern self-driving perception systems have been shown to improve upon
processing complementary inputs such as LiDAR with images. In isolation, 2D
images have been found to be extremely vulnerable to adversarial attacks. Yet,
there have been limited studies on the adversarial robustness of multi-modal
models that fuse LiDAR features with image features. Furthermore, existing
works do not consider physically realizable perturbations that are consistent
across the input modalities. In this paper, we showcase practical
susceptibilities of multi-sensor detection by placing an adversarial object on
top of a host vehicle. We focus on physically realizable and input-agnostic
attacks as they are feasible to execute in practice, and show that a single
universal adversary can hide different host vehicles from state-of-the-art
multi-modal detectors. Our experiments demonstrate that successful attacks are
primarily caused by easily corrupted image features. Furthermore, we find that
in modern sensor fusion methods which project image features into 3D,
adversarial attacks can exploit the projection process to generate false
positives across distant regions in 3D. Towards more robust multi-modal
perception systems, we show that adversarial training with feature denoising
can boost robustness to such attacks significantly. However, we find that
standard adversarial defenses still struggle to prevent false positives which
are also caused by inaccurate associations between 3D LiDAR points and 2D
pixels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MIN2Net: End-to-End Multi-Task Learning for Subject-Independent Motor Imagery EEG Classification. (arXiv:2102.03814v4 [eess.SP] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.03814">
<div class="article-summary-box-inner">
<span><p>Advances in the motor imagery (MI)-based brain-computer interfaces (BCIs)
allow control of several applications by decoding neurophysiological phenomena,
which are usually recorded by electroencephalography (EEG) using a non-invasive
technique. Despite great advances in MI-based BCI, EEG rhythms are specific to
a subject and various changes over time. These issues point to significant
challenges to enhance the classification performance, especially in a
subject-independent manner. To overcome these challenges, we propose MIN2Net, a
novel end-to-end multi-task learning to tackle this task. We integrate deep
metric learning into a multi-task autoencoder to learn a compact and
discriminative latent representation from EEG and perform classification
simultaneously. This approach reduces the complexity in pre-processing, results
in significant performance improvement on EEG classification. Experimental
results in a subject-independent manner show that MIN2Net outperforms the
state-of-the-art techniques, achieving an F1-score improvement of 6.72%, and
2.23% on the SMR-BCI, and OpenBMI datasets, respectively. We demonstrate that
MIN2Net improves discriminative information in the latent representation. This
study indicates the possibility and practicality of using this model to develop
MI-based BCI applications for new users without the need for calibration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Just Noticeable Difference for Deep Machine Vision. (arXiv:2102.08168v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.08168">
<div class="article-summary-box-inner">
<span><p>As an important perceptual characteristic of the Human Visual System (HVS),
the Just Noticeable Difference (JND) has been studied for decades with image
and video processing (e.g., perceptual visual signal compression). However,
there is little exploration on the existence of JND for the Deep Machine Vision
(DMV), although the DMV has made great strides in many machine vision tasks. In
this paper, we take an initial attempt, and demonstrate that the DMV has the
JND, termed as the DMV-JND. We then propose a JND model for the image
classification task in the DMV. It has been discovered that the DMV can
tolerate distorted images with average PSNR of only 9.56dB (the lower the
better), by generating JND via unsupervised learning with the proposed
DMV-JND-NET. In particular, a semantic-guided redundancy assessment strategy is
designed to restrain the magnitude and spatial distribution of the DMV-JND.
Experimental results on image classification demonstrate that we successfully
find the JND for deep machine vision. Our DMV-JND facilitates a possible
direction for DMV-oriented image and video compression, watermarking, quality
assessment, deep neural network security, and so on.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantitative Performance Assessment of CNN Units via Topological Entropy Calculation. (arXiv:2103.09716v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.09716">
<div class="article-summary-box-inner">
<span><p>Identifying the status of individual network units is critical for
understanding the mechanism of convolutional neural networks (CNNs). However,
it is still challenging to reliably give a general indication of unit status,
especially for units in different network models. To this end, we propose a
novel method for quantitatively clarifying the status of single unit in CNN
using algebraic topological tools. Unit status is indicated via the calculation
of a defined topological-based entropy, called feature entropy, which measures
the degree of chaos of the global spatial pattern hidden in the unit for a
category. In this way, feature entropy could provide an accurate indication of
status for units in different networks with diverse situations like
weight-rescaling operation. Further, we show that feature entropy decreases as
the layer goes deeper and shares almost simultaneous trend with loss during
training. We show that by investigating the feature entropy of units on only
training data, it could give discrimination between networks with different
generalization ability from the view of the effectiveness of feature
representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comprehensive Survey of Scene Graphs: Generation and Application. (arXiv:2104.01111v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01111">
<div class="article-summary-box-inner">
<span><p>Scene graph is a structured representation of a scene that can clearly
express the objects, attributes, and relationships between objects in the
scene. As computer vision technology continues to develop, people are no longer
satisfied with simply detecting and recognizing objects in images; instead,
people look forward to a higher level of understanding and reasoning about
visual scenes. For example, given an image, we want to not only detect and
recognize objects in the image, but also know the relationship between objects
(visual relationship detection), and generate a text description (image
captioning) based on the image content. Alternatively, we might want the
machine to tell us what the little girl in the image is doing (Visual Question
Answering (VQA)), or even remove the dog from the image and find similar images
(image editing and retrieval), etc. These tasks require a higher level of
understanding and reasoning for image vision tasks. The scene graph is just
such a powerful tool for scene understanding. Therefore, scene graphs have
attracted the attention of a large number of researchers, and related research
is often cross-modal, complex, and rapidly developing. However, no relatively
systematic survey of scene graphs exists at present. To this end, this survey
conducts a comprehensive investigation of the current scene graph research.
More specifically, we first summarized the general definition of the scene
graph, then conducted a comprehensive and systematic discussion on the
generation method of the scene graph (SGG) and the SGG with the aid of prior
knowledge. We then investigated the main applications of scene graphs and
summarized the most commonly used datasets. Finally, we provide some insights
into the future development of scene graphs. We believe this will be a very
helpful foundation for future research on scene graphs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantically Stealthy Adversarial Attacks against Segmentation Models. (arXiv:2104.01732v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01732">
<div class="article-summary-box-inner">
<span><p>Segmentation models have been found to be vulnerable to targeted and
non-targeted adversarial attacks. However, the resulting segmentation outputs
are often so damaged that it is easy to spot an attack. In this paper, we
propose semantically stealthy adversarial attacks which can manipulate targeted
labels while preserving non-targeted labels at the same time. One challenge is
making semantically meaningful manipulations across datasets and models.
Another challenge is avoiding damaging non-targeted labels. To solve these
challenges, we consider each input image as prior knowledge to generate
perturbations. We also design a special regularizer to help extract features.
To evaluate our model's performance, we design three basic attack types, namely
`vanishing into the context,' `embedding fake labels,' and `displacing target
objects.' Our experiments show that our stealthy adversarial model can attack
segmentation models with a relatively high success rate on Cityscapes,
Mapillary, and BDD100K. Our framework shows good empirical generalization
across datasets and models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Example Detection for DNN Models: A Review and Experimental Comparison. (arXiv:2105.00203v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.00203">
<div class="article-summary-box-inner">
<span><p>Deep learning (DL) has shown great success in many human-related tasks, which
has led to its adoption in many computer vision based applications, such as
security surveillance systems, autonomous vehicles and healthcare. Such
safety-critical applications have to draw their path to success deployment once
they have the capability to overcome safety-critical challenges. Among these
challenges are the defense against or/and the detection of the adversarial
examples (AEs). Adversaries can carefully craft small, often imperceptible,
noise called perturbations to be added to the clean image to generate the AE.
The aim of AE is to fool the DL model which makes it a potential risk for DL
applications. Many test-time evasion attacks and countermeasures,i.e., defense
or detection methods, are proposed in the literature. Moreover, few reviews and
surveys were published and theoretically showed the taxonomy of the threats and
the countermeasure methods with little focus in AE detection methods. In this
paper, we focus on image classification task and attempt to provide a survey
for detection methods of test-time evasion attacks on neural network
classifiers. A detailed discussion for such methods is provided with
experimental results for eight state-of-the-art detectors under different
scenarios on four datasets. We also provide potential challenges and future
perspectives for this research direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sharing Pain: Using Pain Domain Transfer for Video Recognition of Low Grade Orthopedic Pain in Horses. (arXiv:2105.10313v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.10313">
<div class="article-summary-box-inner">
<span><p>Orthopedic disorders are common among horses, often leading to euthanasia,
which often could have been avoided with earlier detection. These conditions
often create varying degrees of subtle long-term pain. It is challenging to
train a visual pain recognition method with video data depicting such pain,
since the resulting pain behavior also is subtle, sparsely appearing, and
varying, making it challenging for even an expert human labeller to provide
accurate ground-truth for the data. We show that a model trained solely on a
dataset of horses with acute experimental pain (where labeling is less
ambiguous) can aid recognition of the more subtle displays of orthopedic pain.
Moreover, we present a human expert baseline for the problem, as well as an
extensive empirical study of various domain transfer methods and of what is
detected by the pain recognition method trained on clean experimental pain in
the orthopedic dataset. Finally, this is accompanied with a discussion around
the challenges posed by real-world animal behavior datasets and how best
practices can be established for similar fine-grained action recognition tasks.
Our code is available at https://github.com/sofiabroome/painface-recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning for Predictive Analytics in Reversible Steganography. (arXiv:2106.06924v2 [cs.MM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06924">
<div class="article-summary-box-inner">
<span><p>Deep learning is regarded as a promising solution for reversible
steganography. The recent development of end-to-end learning has made it
possible to bypass multiple intermediate stages of steganographic operations
with a pair of encoder and decoder neural networks. This framework is, however,
incapable of guaranteeing perfect reversibility since it is difficult for this
kind of monolithic machinery, in the form of a black box, to learn the
intricate logics of reversible computing. A more reliable way to develop a
learning-based reversible steganographic scheme is through a divide-and-conquer
paradigm. Prediction-error modulation is a well-established modular framework
that consists of an analytics module and a coding module. The former serves the
purpose of analysing pixel correlations and predicting pixel intensities, while
the latter specialises in reversible coding mechanisms. Given that
reversibility is governed independently by the coding module, we narrow our
focus to the incorporation of neural networks into the analytics module. The
objective of this study is to evaluate the impacts of different training
configurations on predictive neural networks and to provide practical insights.
Context-aware pixel intensity prediction has a central role in reversible
steganography and can be perceived as a low-level computer vision task.
Therefore, instead of reinventing the wheel, we can adopt neural network models
originally designed for such computer vision tasks to perform intensity
prediction. Furthermore, we rigorously investigate the effect of intensity
initialisation upon predictive performance and the influence of distributional
shift in dual-layer prediction. Experimental results show that state-of-the-art
steganographic performance can be achieved with advanced neural network models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-view Image-based Hand Geometry Refinement using Differentiable Monte Carlo Ray Tracing. (arXiv:2107.05509v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.05509">
<div class="article-summary-box-inner">
<span><p>The amount and quality of datasets and tools available in the research field
of hand pose and shape estimation act as evidence to the significant progress
that has been made.However, even the datasets of the highest quality, reported
to date, have shortcomings in annotation. We propose a refinement approach,
based on differentiable ray tracing,and demonstrate how a high-quality publicly
available, multi-camera dataset of hands(InterHand2.6M) can become an even
better dataset, with respect to annotation quality. Differentiable ray tracing
has not been employed so far to relevant problems and is hereby shown to be
superior to the approximative alternatives that have been employed in the past.
To tackle the lack of reliable ground truth, as far as quantitative evaluation
is concerned, we resort to realistic synthetic data, to show that the
improvement we induce is indeed significant. The same becomes evident in real
data through visual evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">XCI-Sketch: Extraction of Color Information from Images for Generation of Colored Outlines and Sketches. (arXiv:2108.11554v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11554">
<div class="article-summary-box-inner">
<span><p>Sketches are a medium to convey a visual scene from an individual's creative
perspective. The addition of color substantially enhances the overall
expressivity of a sketch. This paper proposes two methods to mimic human-drawn
colored sketches by utilizing the Contour Drawing Dataset. Our first approach
renders colored outline sketches by applying image processing techniques aided
by k-means color clustering. The second method uses a generative adversarial
network to develop a model that can generate colored sketches from previously
unobserved images. We assess the results obtained through quantitative and
qualitative evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PnP-DETR: Towards Efficient Visual Analysis with Transformers. (arXiv:2109.07036v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07036">
<div class="article-summary-box-inner">
<span><p>Recently, DETR pioneered the solution of vision tasks with transformers, it
directly translates the image feature map into the object detection result.
Though effective, translating the full feature map can be costly due to
redundant computation on some area like the background. In this work, we
encapsulate the idea of reducing spatial redundancy into a novel poll and pool
(PnP) sampling module, with which we build an end-to-end PnP-DETR architecture
that adaptively allocates its computation spatially to be more efficient.
Concretely, the PnP module abstracts the image feature map into fine foreground
object feature vectors and a small number of coarse background contextual
feature vectors. The transformer models information interaction within the
fine-coarse feature space and translates the features into the detection
result. Moreover, the PnP-augmented model can instantly achieve various desired
trade-offs between performance and computation with a single model by varying
the sampled feature length, without requiring to train multiple models as
existing methods. Thus it offers greater flexibility for deployment in diverse
scenarios with varying computation constraint. We further validate the
generalizability of the PnP module on panoptic segmentation and the recent
transformer-based image recognition model ViT and show consistent efficiency
gain. We believe our method makes a step for efficient visual analysis with
transformers, wherein spatial redundancy is commonly observed. Code will be
available at \url{https://github.com/twangnh/pnp-detr}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep DIC: Deep Learning-Based Digital Image Correlation for End-to-End Displacement and Strain Measurement. (arXiv:2110.13720v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.13720">
<div class="article-summary-box-inner">
<span><p>Digital image correlation (DIC) has become an industry standard to retrieve
accurate displacement and strain measurement in tensile testing and other
material characterization. Though traditional DIC offers a high precision
estimation of deformation for general tensile testing cases, the prediction
becomes unstable at large deformation or when the speckle patterns start to
tear. In addition, traditional DIC requires a long computation time and often
produces a low spatial resolution output affected by filtering and speckle
pattern quality. To address these challenges, we propose a new deep
learning-based DIC approach--Deep DIC, in which two convolutional neural
networks, DisplacementNet and StrainNet, are designed to work together for
end-to-end prediction of displacements and strains. DisplacementNet predicts
the displacement field and adaptively tracks a region of interest. StrainNet
predicts the strain field directly from the image input without relying on the
displacement prediction, which significantly improves the strain prediction
accuracy. A new dataset generation method is developed to synthesize a
realistic and comprehensive dataset, including the generation of speckle
patterns and the deformation of the speckle image with synthetic displacement
fields. Though trained on synthetic datasets only, Deep DIC gives highly
consistent and comparable predictions of displacement and strain with those
obtained from commercial DIC software for real experiments, while it
outperforms commercial software with very robust strain prediction even at
large and localized deformation and varied pattern qualities. In addition, Deep
DIC is capable of real-time prediction of deformation with a calculation time
down to milliseconds.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stereoscopic Universal Perturbations across Different Architectures and Datasets. (arXiv:2112.06116v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06116">
<div class="article-summary-box-inner">
<span><p>We study the effect of adversarial perturbations of images on deep stereo
matching networks for the disparity estimation task. We present a method to
craft a single set of perturbations that, when added to any stereo image pair
in a dataset, can fool a stereo network to significantly alter the perceived
scene geometry. Our perturbation images are "universal" in that they not only
corrupt estimates of the network on the dataset they are optimized for, but
also generalize to stereo networks with different architectures across
different datasets. We evaluate our approach on multiple public benchmark
datasets and show that our perturbations can increase D1-error (akin to fooling
rate) of state-of-the-art stereo networks from 1% to as much as 87%. We
investigate the effect of perturbations on the estimated scene geometry and
identify object classes that are most vulnerable. Our analysis on the
activations of registered points between left and right images led us to find
that certain architectural components, i.e. deformable convolution and explicit
matching, can increase robustness against adversaries. We demonstrate that by
simply designing networks with such components, one can reduce the effect of
adversaries by up to 60.5%, which rivals the robustness of networks fine-tuned
with costly adversarial data augmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Facial Synthesis: A New Challenge. (arXiv:2112.15439v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15439">
<div class="article-summary-box-inner">
<span><p>The goal of this paper is to conduct a comprehensive study on the facial
sketch synthesis (FSS) problem. However, due to the high costs in obtaining
hand-drawn sketch datasets, there lacks a complete benchmark for assessing the
development of FSS algorithms over the last decade. As such, we first introduce
a high-quality dataset for FSS, named FS2K, which consists of 2,104
image-sketch pairs spanning three types of sketch styles, image backgrounds,
lighting conditions, skin colors, and facial attributes. FS2K differs from
previous FSS datasets in difficulty, diversity, and scalability, and should
thus facilitate the progress of FSS research. Second, we present the
largest-scale FSS study by investigating 139 classical methods, including 24
handcrafted feature based facial sketch synthesis approaches, 37 general
neural-style transfer methods, 43 deep image-to-image translation methods, and
35 image-to-sketch approaches. Besides, we elaborate comprehensive experiments
for existing 19 cutting-edge models. Third, we present a simple baseline for
FSS, named FSGAN. With only two straightforward components, i.e., facial-aware
masking and style-vector expansion, FSGAN surpasses the performance of all
previous state-of-the-art models on the proposed FS2K dataset, by a large
margin. Finally, we conclude with lessons learned over the past years, and
point out several unsolved challenges. Our open-source code is available at
https://github.com/DengPingFan/FSGAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Understanding and Harnessing the Effect of Image Transformation in Adversarial Detection. (arXiv:2201.01080v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01080">
<div class="article-summary-box-inner">
<span><p>Deep neural networks (DNNs) are threatened by adversarial examples.
Adversarial detection, which distinguishes adversarial images from benign
images, is fundamental for robust DNN-based services. Image transformation is
one of the most effective approaches to detect adversarial examples. During the
last few years, a variety of image transformations have been studied and
discussed to design reliable adversarial detectors. In this paper, we
systematically synthesize the recent progress on adversarial detection via
image transformations with a novel classification method. Then, we conduct
extensive experiments to test the detection performance of image
transformations against state-of-the-art adversarial attacks. Furthermore, we
reveal that each individual transformation is not capable of detecting
adversarial examples in a robust way, and propose a DNN-based approach referred
to as AdvJudge, which combines scores of 9 image transformations. Without
knowing which individual scores are misleading or not misleading, AdvJudge can
make the right judgment, and achieve a significant improvement in detection
accuracy. We claim that AdvJudge is a more effective adversarial detector than
those based on an individual image transformation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-SRN: Structure-Preserving Super-Resolution Network with Cross Convolution. (arXiv:2201.01458v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01458">
<div class="article-summary-box-inner">
<span><p>It is challenging to restore low-resolution (LR) images to super-resolution
(SR) images with correct and clear details. Existing deep learning works almost
neglect the inherent structural information of images, which acts as an
important role for visual perception of SR results. In this paper, we design a
hierarchical feature exploitation network to probe and preserve structural
information in a multi-scale feature fusion manner. First, we propose a cross
convolution upon traditional edge detectors to localize and represent edge
features. Then, cross convolution blocks (CCBs) are designed with feature
normalization and channel attention to consider the inherent correlations of
features. Finally, we leverage multi-scale feature fusion group (MFFG) to embed
the cross convolution blocks and develop the relations of structural features
in different scales hierarchically, invoking a lightweight structure-preserving
network named as Cross-SRN. Experimental results demonstrate the Cross-SRN
achieves competitive or superior restoration performances against the
state-of-the-art methods with accurate and clear structural details. Moreover,
we set a criterion to select images with rich structural textures. The proposed
Cross-SRN outperforms the state-of-the-art methods on the selected benchmark,
which demonstrates that our network has a significant advantage in preserving
edges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Investigation of "Benford's" Law Divergence and Machine Learning Techniques for "Intra-Class" Separability of Fingerprint Images. (arXiv:2201.01699v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01699">
<div class="article-summary-box-inner">
<span><p>Protecting a fingerprint database against attackers is very vital in order to
protect against false acceptance rate or false rejection rate. A key property
in distinguishing fingerprint images is by exploiting the characteristics of
these different types of fingerprint images. The aim of this paper is to
perform the classification of fingerprint images using the Ben-ford's law
divergence values and machine learning techniques. The usage of these
Ben-ford's law divergence values as features fed into the machine learning
techniques has proved to be very effective and efficient in the classification
of fingerprint images. The effectiveness of our proposed methodology was
demonstrated on five datasets, achieving very high classification "accuracies"
of 100% for the Decision Tree and CNN. However, the "Naive" Bayes, and Logistic
Regression achieved "accuracies" of 95.95%, and 90.54%, respectively. These
results showed that Ben-ford's law features and machine learning techniques
especially Decision Tree and CNN can be effectively applied for the
classification of fingerprint images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Egocentric 3D Pose Estimation with Third Person Views. (arXiv:2201.02017v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02017">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel approach to enhance the 3D body pose
estimation of a person computed from videos captured from a single wearable
camera. The key idea is to leverage high-level features linking first- and
third-views in a joint embedding space. To learn such embedding space we
introduce First2Third-Pose, a new paired synchronized dataset of nearly 2,000
videos depicting human activities captured from both first- and third-view
perspectives. We explicitly consider spatial- and motion-domain features,
combined using a semi-Siamese architecture trained in a self-supervised
fashion. Experimental results demonstrate that the joint multi-view embedded
space learned with our dataset is useful to extract discriminatory features
from arbitrary single-view egocentric videos, without needing domain adaptation
nor knowledge of camera parameters. We achieve significant improvement of
egocentric 3D body pose estimation performance on two unconstrained datasets,
over three supervised state-of-the-art approaches. Our dataset and code will be
available for research purposes.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-01-10 23:07:33.852287484 UTC">2022-01-10 23:07:33 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>