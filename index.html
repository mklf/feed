<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-09T01:30:00Z">09-09</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Text-Free Prosody-Aware Generative Spoken Language Modeling. (arXiv:2109.03264v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03264">
<div class="article-summary-box-inner">
<span><p>Speech pre-training has primarily demonstrated efficacy on classification
tasks, while its capability of generating novel speech, similar to how GPT-2
can generate coherent paragraphs, has barely been explored. Generative Spoken
Language Modeling (GSLM) (Lakhotia et al., 2021) is the only prior work
addressing the generative aspects of speech pre-training, which replaces text
with discovered phone-like units for language modeling and shows the ability to
generate meaningful novel sentences. Unfortunately, despite eliminating the
need of text, the units used in GSLM discard most of the prosodic information.
Hence, GSLM fails to leverage prosody for better comprehension, and does not
generate expressive speech. In this work, we present a prosody-aware generative
spoken language model (pGSLM). It is composed of a multi-stream transformer
language model (MS-TLM) of speech, represented as discovered unit and prosodic
feature streams, and an adapted HiFi-GAN model converting MS-TLM outputs to
waveforms. We devise a series of metrics for prosody modeling and generation,
and re-use metrics from GSLM for content modeling. Experimental results show
that the pGSLM can utilize prosody to improve both prosody and content
modeling, and also generate natural, meaningful, and coherent speech given a
spoken prompt. Audio samples can be found at https://speechbot.github.io/pgslm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Dual-Decoder Conformer for Multilingual Speech Recognition. (arXiv:2109.03277v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03277">
<div class="article-summary-box-inner">
<span><p>Transformer-based models have recently become very popular for
sequence-to-sequence applications such as machine translation and speech
recognition. This work proposes a dual-decoder transformer model for
low-resource multilingual speech recognition for Indian languages. Our proposed
model consists of a Conformer [1] encoder, two parallel transformer decoders,
and a language classifier. We use a phoneme decoder (PHN-DEC) for the phoneme
recognition task and a grapheme decoder (GRP-DEC) to predict grapheme sequence
along with language information. We consider phoneme recognition and language
identification as auxiliary tasks in the multi-task learning framework. We
jointly optimize the network for phoneme recognition, grapheme recognition, and
language identification tasks with Joint CTC-Attention [2] training. Our
experiments show that we can obtain a significant reduction in WER over the
baseline approaches. We also show that our dual-decoder approach obtains
significant improvement over the single decoder approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hi, my name is Martha: Using names to measure and mitigate bias in generative dialogue models. (arXiv:2109.03300v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03300">
<div class="article-summary-box-inner">
<span><p>All AI models are susceptible to learning biases in data that they are
trained on. For generative dialogue models, being trained on real human
conversations containing unbalanced gender and race/ethnicity references can
lead to models that display learned biases, which we define here broadly as any
measurable differences in the distributions of words or semantic content of
conversations based on demographic groups. We measure the strength of such
biases by producing artificial conversations between two copies of a dialogue
model, conditioning one conversational partner to state a name commonly
associated with a certain gender and/or race/ethnicity. We find that larger
capacity models tend to exhibit more gender bias and greater stereotyping of
occupations by gender. We show that several methods of tuning these dialogue
models, specifically name scrambling, controlled generation, and unlikelihood
training, are effective in reducing bias in conversation, including on a
downstream conversational task. Name scrambling is also effective in lowering
differences in token usage across conversations where partners have names
associated with different genders or races/ethnicities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Corpus-based Open-Domain Event Type Induction. (arXiv:2109.03322v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03322">
<div class="article-summary-box-inner">
<span><p>Traditional event extraction methods require predefined event types and their
corresponding annotations to learn event extractors. These prerequisites are
often hard to be satisfied in real-world applications. This work presents a
corpus-based open-domain event type induction method that automatically
discovers a set of event types from a given corpus. As events of the same type
could be expressed in multiple ways, we propose to represent each event type as
a cluster of &lt;predicate sense, object head&gt; pairs. Specifically, our method (1)
selects salient predicates and object heads, (2) disambiguates predicate senses
using only a verb sense dictionary, and (3) obtains event types by jointly
embedding and clustering &lt;predicate sense, object head&gt; pairs in a latent
spherical space. Our experiments, on three datasets from different domains,
show our method can discover salient and high-quality event types, according to
both automatic and human evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Challenges of Evaluating Compositional Explanations in Multi-Hop Inference: Relevance, Completeness, and Expert Ratings. (arXiv:2109.03334v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03334">
<div class="article-summary-box-inner">
<span><p>Building compositional explanations requires models to combine two or more
facts that, together, describe why the answer to a question is correct.
Typically, these "multi-hop" explanations are evaluated relative to one (or a
small number of) gold explanations. In this work, we show these evaluations
substantially underestimate model performance, both in terms of the relevance
of included facts, as well as the completeness of model-generated explanations,
because models regularly discover and produce valid explanations that are
different than gold explanations. To address this, we construct a large corpus
of 126k domain-expert (science teacher) relevance ratings that augment a corpus
of explanations to standardized science exam questions, discovering 80k
additional relevant facts not rated as gold. We build three strong models based
on different methodologies (generation, ranking, and schemas), and empirically
show that while expert-augmented ratings provide better estimates of
explanation quality, both original (gold) and expert-augmented automatic
evaluations still substantially underestimate performance by up to 36% when
compared with full manual expert judgements, with different models being
disproportionately affected. This poses a significant methodological challenge
to accurately evaluating explanations produced by compositional reasoning
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Contrastive Cross-Modality Representation Learning for Spoken Question Answering. (arXiv:2109.03381v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03381">
<div class="article-summary-box-inner">
<span><p>Spoken question answering (SQA) requires fine-grained understanding of both
spoken documents and questions for the optimal answer prediction. In this
paper, we propose novel training schemes for spoken question answering with a
self-supervised training stage and a contrastive representation learning stage.
In the self-supervised stage, we propose three auxiliary self-supervised tasks,
including utterance restoration, utterance insertion, and question
discrimination, and jointly train the model to capture consistency and
coherence among speech documents without any additional data or annotations. We
then propose to learn noise-invariant utterance representations in a
contrastive objective by adopting multiple augmentation strategies, including
span deletion and span substitution. Besides, we design a Temporal-Alignment
attention to semantically align the speech-text clues in the learned common
space and benefit the SQA tasks. By this means, the training schemes can more
effectively guide the generation model to predict more proper answers.
Experimental results show that our model achieves state-of-the-art results on
three SQA benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepZensols: Deep Natural Language Processing Framework. (arXiv:2109.03383v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03383">
<div class="article-summary-box-inner">
<span><p>Reproducing results in publications by distributing publicly available source
code is becoming ever more popular. Given the difficulty of reproducing machine
learning (ML) experiments, there have been significant efforts in reducing the
variance of these results. As in any science, the ability to consistently
reproduce results effectively strengthens the underlying hypothesis of the
work, and thus, should be regarded as important as the novel aspect of the
research itself. The contribution of this work is a framework that is able to
reproduce consistent results and provides a means of easily creating, training,
and evaluating natural language processing (NLP) deep learning (DL) models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mixup Decoding for Diverse Machine Translation. (arXiv:2109.03402v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03402">
<div class="article-summary-box-inner">
<span><p>Diverse machine translation aims at generating various target language
translations for a given source language sentence. Leveraging the linear
relationship in the sentence latent space introduced by the mixup training, we
propose a novel method, MixDiversity, to generate different translations for
the input sentence by linearly interpolating it with different sentence pairs
sampled from the training corpus when decoding. To further improve the
faithfulness and diversity of the translations, we propose two simple but
effective approaches to select diverse sentence pairs in the training corpus
and adjust the interpolation weight for each pair correspondingly. Moreover, by
controlling the interpolation weight, our method can achieve the trade-off
between faithfulness and diversity without any additional training, which is
required in most of the previous methods. Experiments on WMT'16 en-ro, WMT'14
en-de, and WMT'17 zh-en are conducted to show that our method substantially
outperforms all previous diverse machine translation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision Matters When It Should: Sanity Checking Multimodal Machine Translation Models. (arXiv:2109.03415v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03415">
<div class="article-summary-box-inner">
<span><p>Multimodal machine translation (MMT) systems have been shown to outperform
their text-only neural machine translation (NMT) counterparts when visual
context is available. However, recent studies have also shown that the
performance of MMT models is only marginally impacted when the associated image
is replaced with an unrelated image or noise, which suggests that the visual
context might not be exploited by the model at all. We hypothesize that this
might be caused by the nature of the commonly used evaluation benchmark, also
known as Multi30K, where the translations of image captions were prepared
without actually showing the images to human translators. In this paper, we
present a qualitative study that examines the role of datasets in stimulating
the leverage of visual modality and we propose methods to highlight the
importance of visual signals in the datasets which demonstrate improvements in
reliance of models on the source images. Our findings suggest the research on
effective MMT architectures is currently impaired by the lack of suitable
datasets and careful consideration must be taken in creation of future MMT
datasets, for which we also provide useful insights.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">It is AI's Turn to Ask Human a Question: Question and Answer Pair Generation for Children Storybooks in FairytaleQA Dataset. (arXiv:2109.03423v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03423">
<div class="article-summary-box-inner">
<span><p>Existing question answering (QA) datasets are created mainly for the
application of having AI to be able to answer questions asked by humans. But in
educational applications, teachers and parents sometimes may not know what
questions they should ask a child that can maximize their language learning
results. With a newly released book QA dataset (FairytaleQA), which educational
experts labeled on 46 fairytale storybooks for early childhood readers, we
developed an automated QA generation model architecture for this novel
application. Our model (1) extracts candidate answers from a given storybook
passage through carefully designed heuristics based on a pedagogical framework;
(2) generates appropriate questions corresponding to each extracted answer
using a language model; and, (3) uses another QA model to rank top QA-pairs.
Automatic and human evaluations show that our model outperforms baselines. We
also demonstrate that our method can help with the scarcity issue of the
children's book QA dataset via data augmentation on 200 unlabeled storybooks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ArchivalQA: A Large-scale Benchmark Dataset for Open Domain Question Answering over Archival News Collections. (arXiv:2109.03438v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03438">
<div class="article-summary-box-inner">
<span><p>In the last few years, open-domain question answering (ODQA) has advanced
rapidly due to the development of deep learning techniques and the availability
of large-scale QA datasets. However, the current datasets are essentially
designed for synchronic document collections (e.g., Wikipedia). Temporal news
collections such as long-term news archives spanning several decades, are
rarely used in training the models despite they are quite valuable for our
society. In order to foster the research in the field of ODQA on such
historical collections, we present ArchivalQA, a large question answering
dataset consisting of 1,067,056 question-answer pairs which is designed for
temporal news QA. In addition, we create four subparts of our dataset based on
the question difficulty levels and the containment of temporal expressions,
which we believe could be useful for training or testing ODQA systems
characterized by different strengths and abilities. The novel QA
dataset-constructing framework that we introduce can be also applied to create
datasets over other types of collections.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Referee: Towards reference-free cross-speaker style transfer with low-quality data for expressive speech synthesis. (arXiv:2109.03439v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03439">
<div class="article-summary-box-inner">
<span><p>Cross-speaker style transfer (CSST) in text-to-speech (TTS) synthesis aims at
transferring a speaking style to the synthesised speech in a target speaker's
voice. Most previous CSST approaches rely on expensive high-quality data
carrying desired speaking style during training and require a reference
utterance to obtain speaking style descriptors as conditioning on the
generation of a new sentence. This work presents Referee, a robust
reference-free CSST approach for expressive TTS, which fully leverages
low-quality data to learn speaking styles from text. Referee is built by
cascading a text-to-style (T2S) model with a style-to-wave (S2W) model.
Phonetic PosteriorGram (PPG), phoneme-level pitch and energy contours are
adopted as fine-grained speaking style descriptors, which are predicted from
text using the T2S model. A novel pretrain-refinement method is adopted to
learn a robust T2S model by only using readily accessible low-quality data. The
S2W model is trained with high-quality target data, which is adopted to
effectively aggregate style descriptors and generate high-fidelity speech in
the target speaker's voice. Experimental results are presented, showing that
Referee outperforms a global-style-token (GST)-based baseline approach in CSST.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequence Level Contrastive Learning for Text Summarization. (arXiv:2109.03481v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03481">
<div class="article-summary-box-inner">
<span><p>Contrastive learning models have achieved great success in unsupervised
visual representation learning, which maximize the similarities between feature
representations of different views of the same image, while minimize the
similarities between feature representations of views of different images. In
text summarization, the output summary is a shorter form of the input document
and they have similar meanings. In this paper, we propose a contrastive
learning model for supervised abstractive text summarization, where we view a
document, its gold summary and its model generated summaries as different views
of the same mean representation and maximize the similarities between them
during training. We improve over a strong sequence-to-sequence text generation
model (i.e., BART) on three different summarization datasets. Human evaluation
also shows that our model achieves better faithfulness ratings compared to its
counterpart without contrastive objectives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Social Analysis of Young Basque Speaking Communities in Twitter. (arXiv:2109.03487v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03487">
<div class="article-summary-box-inner">
<span><p>In this paper we take into account both social and linguistic aspects to
perform demographic analysis by processing a large amount of tweets in Basque
language. The study of demographic characteristics and social relationships are
approached by applying machine learning and modern deep-learning Natural
Language Processing (NLP) techniques, combining social sciences with automatic
text processing. More specifically, our main objective is to combine
demographic inference and social analysis in order to detect young Basque
Twitter users and to identify the communities that arise from their
relationships or shared content. This social and demographic analysis will be
entirely based on the~automatically collected tweets using NLP to convert
unstructured textual information into interpretable knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spelling provides a precise (but sometimes misplaced) phonological target. Orthography and acoustic variability in second language word learning. (arXiv:2109.03490v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03490">
<div class="article-summary-box-inner">
<span><p>L1 French participants learned novel L2 English words over two days of
learning sessions, with half of the words presented with their orthographic
forms (Audio-Ortho) and half without (Audio only). One group heard the words
pronounced by a single talker, while another group heard them pronounced by
multiple talkers. On the third day, they completed a variety of tasks to
evaluate their learning. Our results show a robust influence of orthography,
with faster response times in both production (picture naming) and recognition
(picture mapping) tasks for words learned in the Audio-Ortho condition.
Moreover, formant analyses of the picture naming responses show that
orthographic input pulls pronunciations of English novel words towards a
non-native (French) phonological target. Words learned with their orthographic
forms were pronounced more precisely (with smaller Dispersion Scores), but were
misplaced in the vowel space (as reflected by smaller Euclidian distances with
respect to French vowels). Finally, we found only limited evidence of an effect
of talker-based acoustic variability: novel words learned with multiple talkers
showed faster responses times in the picture naming task, but only in the
Audio-only condition, which suggests that orthographic information may have
overwhelmed any advantage of talker-based acoustic variability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">R2-D2: A Modular Baseline for Open-Domain Question Answering. (arXiv:2109.03502v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03502">
<div class="article-summary-box-inner">
<span><p>This work presents a novel four-stage open-domain QA pipeline R2-D2 (Rank
twice, reaD twice). The pipeline is composed of a retriever, passage reranker,
extractive reader, generative reader and a mechanism that aggregates the final
prediction from all system's components. We demonstrate its strength across
three open-domain QA datasets: NaturalQuestions, TriviaQA and EfficientQA,
surpassing state-of-the-art on the first two. Our analysis demonstrates that:
(i) combining extractive and generative reader yields absolute improvements up
to 5 exact match and it is at least twice as effective as the posterior
averaging ensemble of the same models with different parameters, (ii) the
extractive reader with fewer parameters can match the performance of the
generative reader on extractive QA datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RefineCap: Concept-Aware Refinement for Image Captioning. (arXiv:2109.03529v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03529">
<div class="article-summary-box-inner">
<span><p>Automatically translating images to texts involves image scene understanding
and language modeling. In this paper, we propose a novel model, termed
RefineCap, that refines the output vocabulary of the language decoder using
decoder-guided visual semantics, and implicitly learns the mapping between
visual tag words and images. The proposed Visual-Concept Refinement method can
allow the generator to attend to semantic details in the image, thereby
generating more semantically descriptive captions. Our model achieves superior
performance on the MS-COCO dataset in comparison with previous visual-concept
based models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Transferability of Pre-trained Language Models: A Study from Artificial Datasets. (arXiv:2109.03537v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03537">
<div class="article-summary-box-inner">
<span><p>Pre-training language models (LMs) on large-scale unlabeled text data makes
the model much easier to achieve exceptional downstream performance than their
counterparts directly trained on the downstream tasks. In this work, we study
what specific traits in the pre-training data, other than the semantics, make a
pre-trained LM superior to their counterparts trained from scratch on
downstream tasks. We propose to use artificially constructed datasets as the
pre-training data to exclude the effect of semantics, and further control what
characteristics the pre-training corpora have. By fine-tuning the pre-trained
models on GLUE benchmark, we can learn how beneficial it is to transfer the
knowledge from the model trained on the dataset possessing that specific trait.
We define and discuss three different characteristics in the artificial
dataset: 1) matching the token's uni-gram or bi-gram distribution between
pre-training and downstream fine-tuning, 2) the presence of the explicit
dependencies among the tokens in a sequence, 3) the length of the implicit
dependencies among the tokens in a sequence. Our experiments show that the
explicit dependencies in the sequences of the pre-training data are critical to
the downstream performance. Our results also reveal that models achieve better
downstream performance when pre-trained on a dataset with a longer range of
implicit dependencies. Based on our analysis, we find that models pre-trained
with artificial datasets are prone to learn spurious correlation in downstream
tasks. Our work reveals that even if the LMs are not pre-trained on natural
language, they still gain transferability on certain human language downstream
tasks once the LMs learn to model the token dependencies in the sequences. This
result helps us understand the exceptional transferability of pre-trained LMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Time Alignment using Lip Images for Frame-based Electrolaryngeal Voice Conversion. (arXiv:2109.03551v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03551">
<div class="article-summary-box-inner">
<span><p>Voice conversion (VC) is an effective approach to electrolaryngeal (EL)
speech enhancement, a task that aims to improve the quality of the artificial
voice from an electrolarynx device. In frame-based VC methods, time alignment
needs to be performed prior to model training, and the dynamic time warping
(DTW) algorithm is widely adopted to compute the best time alignment between
each utterance pair. The validity is based on the assumption that the same
phonemes of the speakers have similar features and can be mapped by measuring a
pre-defined distance between speech frames of the source and the target.
However, the special characteristics of the EL speech can break the assumption,
resulting in a sub-optimal DTW alignment. In this work, we propose to use lip
images for time alignment, as we assume that the lip movements of laryngectomee
remain normal compared to healthy people. We investigate two naive lip
representations and distance metrics, and experimental results demonstrate that
the proposed method can significantly outperform the audio-only alignment in
terms of objective and subjective evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-lingual Offensive Language Identification for Low Resource Languages: The Case of Marathi. (arXiv:2109.03552v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03552">
<div class="article-summary-box-inner">
<span><p>The widespread presence of offensive language on social media motivated the
development of systems capable of recognizing such content automatically. Apart
from a few notable exceptions, most research on automatic offensive language
identification has dealt with English. To address this shortcoming, we
introduce MOLD, the Marathi Offensive Language Dataset. MOLD is the first
dataset of its kind compiled for Marathi, thus opening a new domain for
research in low-resource Indo-Aryan languages. We present results from several
machine learning experiments on this dataset, including zero-short and other
transfer learning experiments on state-of-the-art cross-lingual transformers
from existing data in Bengali, English, and Hindi.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NSP-BERT: A Prompt-based Zero-Shot Learner Through an Original Pre-training Task--Next Sentence Prediction. (arXiv:2109.03564v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03564">
<div class="article-summary-box-inner">
<span><p>Using prompts to utilize language models to perform various downstream tasks,
also known as prompt-based learning or prompt-learning, has lately gained
significant success in comparison to the pre-train and fine-tune paradigm.
Nonetheless, virtually all prompt-based methods are token-level, meaning they
all utilize GPT's left-to-right language model or BERT's masked language model
to perform cloze-style tasks. In this paper, we attempt to accomplish several
NLP tasks in the zero-shot scenario using a BERT original pre-training task
abandoned by RoBERTa and other models--Next Sentence Prediction (NSP). Unlike
token-level techniques, our sentence-level prompt-based method NSP-BERT does
not need to fix the length of the prompt or the position to be predicted,
allowing it to handle tasks such as entity linking with ease. Based on the
characteristics of NSP-BERT, we offer several quick building templates for
various downstream tasks. We suggest a two-stage prompt method for word sense
disambiguation tasks in particular. Our strategies for mapping the labels
significantly enhance the model's performance on sentence pair tasks. On the
FewCLUE benchmark, our NSP-BERT outperforms other zero-shot methods on most of
these tasks and comes close to the few-shot methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Biomedical and Clinical Language Models for Spanish: On the Benefits of Domain-Specific Pretraining in a Mid-Resource Scenario. (arXiv:2109.03570v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03570">
<div class="article-summary-box-inner">
<span><p>This work presents biomedical and clinical language models for Spanish by
experimenting with different pretraining choices, such as masking at word and
subword level, varying the vocabulary size and testing with domain data,
looking for better language representations. Interestingly, in the absence of
enough clinical data to train a model from scratch, we applied mixed-domain
pretraining and cross-domain transfer approaches to generate a performant
bio-clinical model suitable for real-world clinical data. We evaluated our
models on Named Entity Recognition (NER) tasks for biomedical documents and
challenging hospital discharge reports. When compared against the competitive
mBERT and BETO models, we outperform them in all NER tasks by a significant
margin. Finally, we studied the impact of the model's vocabulary on the NER
performances by offering an interesting vocabulary-centric analysis. The
results confirm that domain-specific pretraining is fundamental to achieving
higher performances in downstream NER tasks, even within a mid-resource
scenario. To the best of our knowledge, we provide the first biomedical and
clinical transformer-based pretrained language models for Spanish, intending to
boost native Spanish NLP applications in biomedicine. Our models will be made
freely available after publication.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TrollsWithOpinion: A Dataset for Predicting Domain-specific Opinion Manipulation in Troll Memes. (arXiv:2109.03571v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03571">
<div class="article-summary-box-inner">
<span><p>Research into the classification of Image with Text (IWT) troll memes has
recently become popular. Since the online community utilizes the refuge of
memes to express themselves, there is an abundance of data in the form of
memes. These memes have the potential to demean, harras, or bully targeted
individuals. Moreover, the targeted individual could fall prey to opinion
manipulation. To comprehend the use of memes in opinion manipulation, we define
three specific domains (product, political or others) which we classify into
troll or not-troll, with or without opinion manipulation. To enable this
analysis, we enhanced an existing dataset by annotating the data with our
defined classes, resulting in a dataset of 8,881 IWT or multimodal memes in the
English language (TrollsWithOpinion dataset). We perform baseline experiments
on the annotated dataset, and our result shows that existing state-of-the-art
techniques could only reach a weighted-average F1-score of 0.37. This shows the
need for a development of a specific technique to deal with multimodal troll
memes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Dual-Channel Framework for Sarcasm Recognition by Detecting Sentiment Conflict. (arXiv:2109.03587v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03587">
<div class="article-summary-box-inner">
<span><p>Sarcasm employs ambivalence, where one says something positive but actually
means negative, and vice versa. Due to the sophisticated and obscure sentiment,
sarcasm brings in great challenges to sentiment analysis. In this paper, we
show up the essence of sarcastic text is that the literal sentiment (expressed
by the surface form of the text) is opposite to the deep sentiment (expressed
by the actual meaning of the text). To this end, we propose a Dual-Channel
Framework by modeling both literal and deep sentiments to recognize the
sentiment conflict. Specifically, the proposed framework is capable of
detecting the sentiment conflict between the literal and deep meanings of the
input text. Experiments on the political debates and the Twitter datasets show
that our framework achieves the best performance on sarcasm recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Formal Query Building with Query Structure Prediction for Complex Question Answering over Knowledge Base. (arXiv:2109.03614v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03614">
<div class="article-summary-box-inner">
<span><p>Formal query building is an important part of complex question answering over
knowledge bases. It aims to build correct executable queries for questions.
Recent methods try to rank candidate queries generated by a state-transition
strategy. However, this candidate generation strategy ignores the structure of
queries, resulting in a considerable number of noisy queries. In this paper, we
propose a new formal query building approach that consists of two stages. In
the first stage, we predict the query structure of the question and leverage
the structure to constrain the generation of the candidate queries. We propose
a novel graph generation framework to handle the structure prediction task and
design an encoder-decoder model to predict the argument of the predetermined
operation in each generative step. In the second stage, we follow the previous
methods to rank the candidate queries. The experimental results show that our
formal query building approach outperforms existing methods on complex
questions while staying competitive on simple questions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discrete and Soft Prompting for Multilingual Models. (arXiv:2109.03630v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03630">
<div class="article-summary-box-inner">
<span><p>It has been shown for English that discrete and soft prompting perform
strongly in few-shot learning with pretrained language models (PLMs). In this
paper, we show that discrete and soft prompting perform better than finetuning
in multilingual cases: Crosslingual transfer and in-language training of
multilingual natural language inference. For example, with 48 English training
examples, finetuning obtains 33.74% accuracy in crosslingual transfer, barely
surpassing the majority baseline (33.33%). In contrast, discrete and soft
prompting outperform finetuning, achieving 36.43% and 38.79%. We also
demonstrate good performance of prompting with training data in multiple
languages other than English.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Data Augmentation for Low-Resource Neural Machine Translation: A Multi-Task Learning Approach. (arXiv:2109.03645v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03645">
<div class="article-summary-box-inner">
<span><p>In the context of neural machine translation, data augmentation (DA)
techniques may be used for generating additional training samples when the
available parallel data are scarce. Many DA approaches aim at expanding the
support of the empirical data distribution by generating new sentence pairs
that contain infrequent words, thus making it closer to the true data
distribution of parallel sentences. In this paper, we propose to follow a
completely different approach and present a multi-task DA approach in which we
generate new sentence pairs with transformations, such as reversing the order
of the target sentence, which produce unfluent target sentences. During
training, these augmented sentences are used as auxiliary tasks in a multi-task
framework with the aim of providing new contexts where the target prefix is not
informative enough to predict the next word. This strengthens the encoder and
forces the decoder to pay more attention to the source representations of the
encoder. Experiments carried out on six low-resource translation tasks show
consistent improvements over the baseline and over DA methods aiming at
extending the support of the empirical data distribution. The systems trained
with our approach rely more on the source tokens, are more robust against
domain shift and suffer less hallucinations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sustainable Modular Debiasing of Language Models. (arXiv:2109.03646v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03646">
<div class="article-summary-box-inner">
<span><p>Unfair stereotypical biases (e.g., gender, racial, or religious biases)
encoded in modern pretrained language models (PLMs) have negative ethical
implications for widespread adoption of state-of-the-art language technology.
To remedy for this, a wide range of debiasing techniques have recently been
introduced to remove such stereotypical biases from PLMs. Existing debiasing
methods, however, directly modify all of the PLMs parameters, which -- besides
being computationally expensive -- comes with the inherent risk of
(catastrophic) forgetting of useful language knowledge acquired in pretraining.
In this work, we propose a more sustainable modular debiasing approach based on
dedicated debiasing adapters, dubbed ADELE. Concretely, we (1) inject adapter
modules into the original PLM layers and (2) update only the adapters (i.e., we
keep the original PLM parameters frozen) via language modeling training on a
counterfactually augmented corpus. We showcase ADELE, in gender debiasing of
BERT: our extensive evaluation, encompassing three intrinsic and two extrinsic
bias measures, renders ADELE, very effective in bias mitigation. We further
show that -- due to its modular nature -- ADELE, coupled with task adapters,
retains fairness even after large-scale downstream training. Finally, by means
of multilingual BERT, we successfully transfer ADELE, to six target languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Label Verbalization and Entailment for Effective Zero- and Few-Shot Relation Extraction. (arXiv:2109.03659v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03659">
<div class="article-summary-box-inner">
<span><p>Relation extraction systems require large amounts of labeled examples which
are costly to annotate. In this work we reformulate relation extraction as an
entailment task, with simple, hand-made, verbalizations of relations produced
in less than 15 min per relation. The system relies on a pretrained textual
entailment engine which is run as-is (no training examples, zero-shot) or
further fine-tuned on labeled examples (few-shot or fully trained). In our
experiments on TACRED we attain 63% F1 zero-shot, 69% with 16 examples per
relation (17% points better than the best supervised system on the same
conditions), and only 4 points short to the state-of-the-art (which uses 20
times more training data). We also show that the performance can be improved
significantly with larger entailment models, up to 12 points in zero-shot,
allowing to report the best results to date on TACRED when fully trained. The
analysis shows that our few-shot systems are specially effective when
discriminating between relations, and that the performance difference in low
data regimes comes mainly from identifying no-relation cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open Aspect Target Sentiment Classification with Natural Language Prompts. (arXiv:2109.03685v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03685">
<div class="article-summary-box-inner">
<span><p>For many business applications, we often seek to analyze sentiments
associated with any arbitrary aspects of commercial products, despite having a
very limited amount of labels or even without any labels at all. However,
existing aspect target sentiment classification (ATSC) models are not trainable
if annotated datasets are not available. Even with labeled data, they fall
short of reaching satisfactory performance. To address this, we propose simple
approaches that better solve ATSC with natural language prompts, enabling the
task under zero-shot cases and enhancing supervised settings, especially for
few-shot cases. Under the few-shot setting for SemEval 2014 Task 4 laptop
domain, our method of reformulating ATSC as an NLI task outperforms supervised
SOTA approaches by up to 24.13 accuracy points and 33.14 macro F1 points.
Moreover, we demonstrate that our prompts could handle implicitly stated
aspects as well: our models reach about 77% accuracy on detecting sentiments
for aspect categories (e.g., food), which do not necessarily appear within the
text, even though we trained the models only with explicitly mentioned aspect
terms (e.g., fajitas) from just 16 reviews - while the accuracy of the
no-prompt baseline is only around 65%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continuous Entailment Patterns for Lexical Inference in Context. (arXiv:2109.03695v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03695">
<div class="article-summary-box-inner">
<span><p>Combining a pretrained language model (PLM) with textual patterns has been
shown to help in both zero- and few-shot settings. For zero-shot performance,
it makes sense to design patterns that closely resemble the text seen during
self-supervised pretraining because the model has never seen anything else.
Supervised training allows for more flexibility. If we allow for tokens outside
the PLM's vocabulary, patterns can be adapted more flexibly to a PLM's
idiosyncrasies. Contrasting patterns where a "token" can be any continuous
vector vs. those where a discrete choice between vocabulary elements has to be
made, we call our method CONtinuous pAtterNs (CONAN). We evaluate CONAN on two
established benchmarks for lexical inference in context (LIiC) a.k.a. predicate
entailment, a challenging natural language understanding task with relatively
small training sets. In a direct comparison with discrete patterns, CONAN
consistently leads to improved performance, setting a new state of the art. Our
experiments give valuable insights into the kind of pattern that enhances a
PLM's performance on LIiC and raise important questions regarding our
understanding of PLMs using text patterns.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Policy Compliance Detection via Question Answering. (arXiv:2109.03731v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03731">
<div class="article-summary-box-inner">
<span><p>Policy compliance detection is the task of ensuring that a scenario conforms
to a policy (e.g. a claim is valid according to government rules or a post in
an online platform conforms to community guidelines). This task has been
previously instantiated as a form of textual entailment, which results in poor
accuracy due to the complexity of the policies. In this paper we propose to
address policy compliance detection via decomposing it into question answering,
where questions check whether the conditions stated in the policy apply to the
scenario, and an expression tree combines the answers to obtain the label.
Despite the initial upfront annotation cost, we demonstrate that this approach
results in better accuracy, especially in the cross-policy setup where the
policies during testing are unseen in training. In addition, it allows us to
use existing question answering models pre-trained on existing large datasets.
Finally, it explicitly identifies the information missing from a scenario in
case policy compliance cannot be determined. We conduct our experiments using a
recent dataset consisting of government policies, which we augment with expert
annotations and find that the cost of annotating question answering
decomposition is largely offset by improved inter-annotator agreement and
speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Memory and Knowledge Augmented Language Models for Inferring Salience in Long-Form Stories. (arXiv:2109.03754v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03754">
<div class="article-summary-box-inner">
<span><p>Measuring event salience is essential in the understanding of stories. This
paper takes a recent unsupervised method for salience detection derived from
Barthes Cardinal Functions and theories of surprise and applies it to longer
narrative forms. We improve the standard transformer language model by
incorporating an external knowledgebase (derived from Retrieval Augmented
Generation) and adding a memory mechanism to enhance performance on longer
works. We use a novel approach to derive salience annotation using
chapter-aligned summaries from the Shmoop corpus for classic literary works.
Our evaluation against this data demonstrates that our salience detection model
improves performance over and above a non-knowledgebase and memory augmented
language model, both of which are crucial to this improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active Learning by Acquiring Contrastive Examples. (arXiv:2109.03764v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03764">
<div class="article-summary-box-inner">
<span><p>Common acquisition functions for active learning use either uncertainty or
diversity sampling, aiming to select difficult and diverse data points from the
pool of unlabeled data, respectively. In this work, leveraging the best of both
worlds, we propose an acquisition function that opts for selecting
\textit{contrastive examples}, i.e. data points that are similar in the model
feature space and yet the model outputs maximally different predictive
likelihoods. We compare our approach, CAL (Contrastive Active Learning), with a
diverse set of acquisition functions in four natural language understanding
tasks and seven datasets. Our experiments show that CAL performs consistently
better or equal than the best performing baseline across all tasks, on both
in-domain and out-of-domain data. We also conduct an extensive ablation study
of our method and we further analyze all actively acquired datasets showing
that CAL achieves a better trade-off between uncertainty and diversity compared
to other strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self- and Pseudo-self-supervised Prediction of Speaker and Key-utterance for Multi-party Dialogue Reading Comprehension. (arXiv:2109.03772v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03772">
<div class="article-summary-box-inner">
<span><p>Multi-party dialogue machine reading comprehension (MRC) brings tremendous
challenge since it involves multiple speakers at one dialogue, resulting in
intricate speaker information flows and noisy dialogue contexts. To alleviate
such difficulties, previous models focus on how to incorporate these
information using complex graph-based modules and additional manually labeled
data, which is usually rare in real scenarios. In this paper, we design two
labour-free self- and pseudo-self-supervised prediction tasks on speaker and
key-utterance to implicitly model the speaker information flows, and capture
salient clues in a long dialogue. Experimental results on two benchmark
datasets have justified the effectiveness of our method over competitive
baselines and current state-of-the-art models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Forget me not: A Gentle Reminder to Mind the Simple Multi-Layer Perceptron Baseline for Text Classification. (arXiv:2109.03777v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03777">
<div class="article-summary-box-inner">
<span><p>Graph neural networks have triggered a resurgence of graph-based text
classification. We show that already a simple MLP baseline achieves comparable
performance on benchmark datasets, questioning the importance of synthetic
graph structures. When considering an inductive scenario, i. e., when adding
new documents to a corpus, a simple MLP even outperforms most graph-based
models. We further fine-tune DistilBERT for comparison and find that it
outperforms all state-of-the-art models. We suggest that future studies use at
least an MLP baseline to contextualize the results. We provide recommendations
for the design and training of such a baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Highly Parallel Autoregressive Entity Linking with Discriminative Correction. (arXiv:2109.03792v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03792">
<div class="article-summary-box-inner">
<span><p>Generative approaches have been recently shown to be effective for both
Entity Disambiguation and Entity Linking (i.e., joint mention detection and
disambiguation). However, the previously proposed autoregressive formulation
for EL suffers from i) high computational cost due to a complex (deep) decoder,
ii) non-parallelizable decoding that scales with the source sequence length,
and iii) the need for training on a large amount of data. In this work, we
propose a very efficient approach that parallelizes autoregressive linking
across all potential mentions and relies on a shallow and efficient decoder.
Moreover, we augment the generative objective with an extra discriminative
component, i.e., a correction term which lets us directly optimize the
generator's ranking. When taken together, these techniques tackle all the above
issues: our model is &gt;70 times faster and more accurate than the previous
generative method, outperforming state-of-the-art approaches on the standard
English dataset AIDA-CoNLL. Source code available at
https://github.com/nicola-decao/efficient-autoregressive-EL
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Smelting Gold and Silver for Improved Multilingual AMR-to-Text Generation. (arXiv:2109.03808v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03808">
<div class="article-summary-box-inner">
<span><p>Recent work on multilingual AMR-to-text generation has exclusively focused on
data augmentation strategies that utilize silver AMR. However, this assumes a
high quality of generated AMRs, potentially limiting the transferability to the
target task. In this paper, we investigate different techniques for
automatically generating AMR annotations, where we aim to study which source of
information yields better multilingual results. Our models trained on gold AMR
with silver (machine translated) sentences outperform approaches which leverage
generated silver AMR. We find that combining both complementary sources of
information further improves multilingual AMR-to-text generation. Our models
surpass the previous state of the art for German, Italian, Spanish, and Chinese
by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Common Semantic Space for Monolingual and Cross-Lingual Meta-Embeddings. (arXiv:2001.06381v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2001.06381">
<div class="article-summary-box-inner">
<span><p>This paper presents a new technique for creating monolingual and
cross-lingual meta-embeddings. Our method integrates multiple word embeddings
created from complementary techniques, textual sources, knowledge bases and
languages. Existing word vectors are projected to a common semantic space using
linear transformations and averaging. With our method the resulting
meta-embeddings maintain the dimensionality of the original embeddings without
losing information while dealing with the out-of-vocabulary problem. An
extensive empirical evaluation demonstrates the effectiveness of our technique
with respect to previous work on various intrinsic and extrinsic multilingual
evaluations, obtaining competitive results for Semantic Textual Similarity and
state-of-the-art performance for word similarity and POS tagging (English and
Spanish). The resulting cross-lingual meta-embeddings also exhibit excellent
cross-lingual transfer learning capabilities. In other words, we can leverage
pre-trained source embeddings from a resource-rich language in order to improve
the word representations for under-resourced languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Chart-based Constituency Parse Extraction from Pre-trained Language Models. (arXiv:2004.13805v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.13805">
<div class="article-summary-box-inner">
<span><p>As it has been unveiled that pre-trained language models (PLMs) are to some
extent capable of recognizing syntactic concepts in natural language, much
effort has been made to develop a method for extracting complete (binary)
parses from PLMs without training separate parsers. We improve upon this
paradigm by proposing a novel chart-based method and an effective top-K
ensemble technique. Moreover, we demonstrate that we can broaden the scope of
application of the approach into multilingual settings. Specifically, we show
that by applying our method on multilingual PLMs, it becomes possible to induce
non-trivial parses for sentences from nine languages in an integrated and
language-agnostic manner, attaining performance superior or comparable to that
of unsupervised PCFGs. We also verify that our approach is robust to
cross-lingual transfer. Finally, we provide analyses on the inner workings of
our method. For instance, we discover universal attention heads which are
consistently sensitive to syntactic information irrespective of the input
language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Word Equations: Inherently Interpretable Sparse Word Embeddingsthrough Sparse Coding. (arXiv:2004.13847v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.13847">
<div class="article-summary-box-inner">
<span><p>Word embeddings are a powerful natural lan-guage processing technique, but
they are ex-tremely difficult to interpret. To enable inter-pretable NLP
models, we create vectors whereeach dimension isinherently interpretable.
Byinherently interpretable, we mean a systemwhere each dimension is associated
with somehuman-understandablehintthat can describethe meaning of that
dimension. In order tocreate more interpretable word embeddings,we transform
pretrained dense word embed-dings into sparse embeddings. These new em-beddings
are inherently interpretable: each oftheir dimensions is created from and
repre-sents a natural language word or specific gram-matical concept. We
construct these embed-dings through sparse coding, where each vec-tor in the
basis set is itself a word embedding.Therefore, each dimension of our sparse
vec-tors corresponds to a natural language word.We also show that models
trained using thesesparse embeddings can achieve good perfor-mance and are more
interpretable in practice,including through human evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERTnesia: Investigating the capture and forgetting of knowledge in BERT. (arXiv:2010.09313v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.09313">
<div class="article-summary-box-inner">
<span><p>Probing complex language models has recently revealed several insights into
linguistic and semantic patterns found in the learned representations. In this
paper, we probe BERT specifically to understand and measure the relational
knowledge it captures. We utilize knowledge base completion tasks to probe
every layer of pre-trained as well as fine-tuned BERT (ranking, question
answering, NER). Our findings show that knowledge is not just contained in
BERT's final layers. Intermediate layers contribute a significant amount
(17-60%) to the total knowledge found. Probing intermediate layers also reveals
how different types of knowledge emerge at varying rates. When BERT is
fine-tuned, relational knowledge is forgotten but the extent of forgetting is
impacted by the fine-tuning objective but not the size of the dataset. We found
that ranking models forget the least and retain more knowledge in their final
layer. We release our code on github to repeat the experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers. (arXiv:2101.00234v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00234">
<div class="article-summary-box-inner">
<span><p>Transformers have shown improved performance when compared to previous
architectures for sequence processing such as RNNs. Despite their sizeable
performance gains, as recently suggested, the model is computationally
expensive to train and with a high parameter budget. In light of this, we
explore parameter-sharing methods in Transformers with a specific focus on
generative models. We perform an analysis of different parameter
sharing/reduction methods and develop the Subformer. Our model combines
sandwich-style parameter sharing, which overcomes naive cross-layer parameter
sharing in generative models, and self-attentive embedding factorization
(SAFE). Experiments on machine translation, abstractive summarization and
language modeling show that the Subformer can outperform the Transformer even
when using significantly fewer parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Minimum projective linearizations of trees in linear time. (arXiv:2102.03277v4 [cs.DS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.03277">
<div class="article-summary-box-inner">
<span><p>The Minimum Linear Arrangement problem (MLA) consists of finding a mapping
$\pi$ from vertices of a graph to distinct integers that minimizes
$\sum_{\{u,v\}\in E}|\pi(u) - \pi(v)|$. In that setting, vertices are often
assumed to lie on a horizontal line and edges are drawn as semicircles above
said line. For trees, various algorithms are available to solve the problem in
polynomial time in $n=|V|$. There exist variants of the MLA in which the
arrangements are constrained. Iordanskii, and later Hochberg and Stallmann
(HS), put forward $O(n)$-time algorithms that solve the problem when
arrangements are constrained to be planar (also known as one-page book
embeddings). We also consider linear arrangements of rooted trees that are
constrained to be projective (planar embeddings where the root is not covered
by any edge). Gildea and Temperley (GT) sketched an algorithm for projective
arrangements which they claimed runs in $O(n)$ but did not provide any
justification of its cost. In contrast, Park and Levy claimed that GT's
algorithm runs in $O(n \log d_{max})$ where $d_{max}$ is the maximum degree but
did not provide sufficient detail. Here we correct an error in HS's algorithm
for the planar case, show its relationship with the projective case, and derive
simple algorithms for the projective and planar cases that run without a doubt
in $O(n)$ time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structural Adapters in Pretrained Language Models for AMR-to-text Generation. (arXiv:2103.09120v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.09120">
<div class="article-summary-box-inner">
<span><p>Pretrained language models (PLM) have recently advanced graph-to-text
generation, where the input graph is linearized into a sequence and fed into
the PLM to obtain its representation. However, efficiently encoding the graph
structure in PLMs is challenging because such models were pretrained on natural
language, and modeling structured data may lead to catastrophic forgetting of
distributional knowledge. In this paper, we propose StructAdapt, an adapter
method to encode graph structure into PLMs. Contrary to prior work, StructAdapt
effectively models interactions among the nodes based on the graph
connectivity, only training graph structure-aware adapter parameters. In this
way, we incorporate task-specific knowledge while maintaining the topological
structure of the graph. We empirically show the benefits of explicitly encoding
graph structure into PLMs using StructAdapt, outperforming the state of the art
on two AMR-to-text datasets, training only 5.1% of the PLM parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SelfExplain: A Self-Explaining Architecture for Neural Text Classifiers. (arXiv:2103.12279v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.12279">
<div class="article-summary-box-inner">
<span><p>We introduce SelfExplain, a novel self-explaining model that explains a text
classifier's predictions using phrase-based concepts. SelfExplain augments
existing neural classifiers by adding (1) a globally interpretable layer that
identifies the most influential concepts in the training set for a given sample
and (2) a locally interpretable layer that quantifies the contribution of each
local input concept by computing a relevance score relative to the predicted
label. Experiments across five text-classification datasets show that
SelfExplain facilitates interpretability without sacrificing performance. Most
importantly, explanations from SelfExplain show sufficiency for model
predictions and are perceived as adequate, trustworthy and understandable by
human judges compared to existing widely-used baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training. (arXiv:2104.01027v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01027">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning of speech representations has been a very active
research area but most work is focused on a single domain such as read audio
books for which there exist large quantities of labeled and unlabeled data. In
this paper, we explore more general setups where the domain of the unlabeled
data for pre-training data differs from the domain of the labeled data for
fine-tuning, which in turn may differ from the test data domain. Our
experiments show that using target domain data during pre-training leads to
large performance improvements across a variety of setups. On a large-scale
competitive setup, we show that pre-training on unlabeled in-domain data
reduces the gap between models trained on in-domain and out-of-domain labeled
data by 66%-73%. This has obvious practical implications since it is much
easier to obtain unlabeled target domain data than labeled data. Moreover, we
find that pre-training on multiple domains improves generalization performance
on domains not seen during training. Code and models will be made available at
https://github.com/pytorch/fairseq.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections. (arXiv:2104.04670v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.04670">
<div class="article-summary-box-inner">
<span><p>Large pre-trained language models (LMs) such as GPT-3 have acquired a
surprising ability to perform zero-shot learning. For example, to classify
sentiment without any training examples, we can "prompt" the LM with the review
and the label description "Does the user like this movie?", and ask whether the
next word is "yes" or "no". However, the next word prediction training
objective is still misaligned with the target zero-shot learning objective. To
address this weakness, we propose meta-tuning, which directly optimizes the
zero-shot learning objective by fine-tuning pre-trained language models on a
collection of datasets. We focus on classification tasks, and construct the
meta-dataset by aggregating 43 existing datasets and annotating 441 label
descriptions in a question-answering (QA) format. When evaluated on unseen
tasks, meta-tuned models outperform a same-sized QA model and the previous SOTA
zero-shot learning system based on natural language inference. Additionally,
increasing parameter count from 220M to 770M improves AUC-ROC scores by 6.3%,
and we forecast that even larger models would perform better. Therefore,
measuring zero-shot learning performance on language models out-of-the-box
might underestimate their true potential, and community-wide efforts on
aggregating datasets and unifying their formats can help build models that
answer prompts better.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Temporal Adaptation of BERT and Performance on Downstream Document Classification: Insights from Social Media. (arXiv:2104.08116v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08116">
<div class="article-summary-box-inner">
<span><p>Language use differs between domains and even within a domain, language use
changes over time. For pre-trained language models like BERT, domain adaptation
through continued pre-training has been shown to improve performance on
in-domain downstream tasks. In this article, we investigate whether temporal
adaptation can bring additional benefits. For this purpose, we introduce a
corpus of social media comments sampled over three years. It contains
unlabelled data for adaptation and evaluation on an upstream masked language
modelling task as well as labelled data for fine-tuning and evaluation on a
downstream document classification task. We find that temporality matters for
both tasks: temporal adaptation improves upstream and temporal fine-tuning
downstream task performance. Time-specific models generally perform better on
past than on future test sets, which matches evidence on the bursty usage of
topical words. However, adapting BERT to time and domain does not improve
performance on the downstream task over only adapting to domain. Token-level
analysis shows that temporal adaptation captures event-driven changes in
language use in the downstream task, but not those changes that are actually
relevant to task performance. Based on our findings, we discuss when temporal
adaptation may be more effective.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning from Noisy Labels for Entity-Centric Information Extraction. (arXiv:2104.08656v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08656">
<div class="article-summary-box-inner">
<span><p>Recent information extraction approaches have relied on training deep neural
models. However, such models can easily overfit noisy labels and suffer from
performance degradation. While it is very costly to filter noisy labels in
large learning resources, recent studies show that such labels take more
training steps to be memorized and are more frequently forgotten than clean
labels, therefore are identifiable in training. Motivated by such properties,
we propose a simple co-regularization framework for entity-centric information
extraction, which consists of several neural models with identical structures
but different parameter initialization. These models are jointly optimized with
the task-specific losses and are regularized to generate similar predictions
based on an agreement loss, which prevents overfitting on noisy labels.
Extensive experiments on two widely used but noisy benchmarks for information
extraction, TACRED and CoNLL03, demonstrate the effectiveness of our framework.
We release our code to the community for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models. (arXiv:2104.08663v3 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08663">
<div class="article-summary-box-inner">
<span><p>Existing neural information retrieval (IR) models have often been studied in
homogeneous and narrow settings, which has considerably limited insights into
their out-of-distribution (OOD) generalization capabilities. To address this,
and to facilitate researchers to broadly evaluate the effectiveness of their
models, we introduce Benchmarking-IR (BEIR), a robust and heterogeneous
evaluation benchmark for information retrieval. We leverage a careful selection
of 18 publicly available datasets from diverse text retrieval tasks and domains
and evaluate 10 state-of-the-art retrieval systems including lexical, sparse,
dense, late-interaction and re-ranking architectures on the BEIR benchmark. Our
results show BM25 is a robust baseline and re-ranking and
late-interaction-based models on average achieve the best zero-shot
performances, however, at high computational costs. In contrast, dense and
sparse-retrieval models are computationally more efficient but often
underperform other approaches, highlighting the considerable room for
improvement in their generalization capabilities. We hope this framework allows
us to better evaluate and understand existing retrieval systems, and
contributes to accelerating progress towards better robust and generalizable
systems in the future. BEIR is publicly available at
https://github.com/UKPLab/beir.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Out-of-Distribution Detection for Pretrained Transformers. (arXiv:2104.08812v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08812">
<div class="article-summary-box-inner">
<span><p>Pretrained Transformers achieve remarkable performance when training and test
data are from the same distribution. However, in real-world scenarios, the
model often faces out-of-distribution (OOD) instances that can cause severe
semantic shift problems at inference time. Therefore, in practice, a reliable
model should identify such instances, and then either reject them during
inference or pass them over to models that handle another distribution. In this
paper, we develop an unsupervised OOD detection method, in which only the
in-distribution (ID) data are used in training. We propose to fine-tune the
Transformers with a contrastive loss, which improves the compactness of
representations, such that OOD instances can be better differentiated from ID
ones. These OOD instances can then be accurately detected using the Mahalanobis
distance in the model's penultimate layer. We experiment with comprehensive
settings and achieve near-perfect OOD detection performance, outperforming
baselines drastically. We further investigate the rationales behind the
improvement, finding that more compact representations through margin-based
contrastive learning bring the improvement. We release our code to the
community for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stream-level Latency Evaluation for Simultaneous Machine Translation. (arXiv:2104.08817v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08817">
<div class="article-summary-box-inner">
<span><p>Simultaneous machine translation has recently gained traction thanks to
significant quality improvements and the advent of streaming applications.
Simultaneous translation systems need to find a trade-off between translation
quality and response time, and with this purpose multiple latency measures have
been proposed. However, latency evaluations for simultaneous translation are
estimated at the sentence level, not taking into account the sequential nature
of a streaming scenario. Indeed, these sentence-level latency measures are not
well suited for continuous stream translation resulting in figures that are not
coherent with the simultaneous translation policy of the system being assessed.
This work proposes a stream-level adaptation of the current latency measures
based on a re-segmentation approach applied to the output translation, that is
successfully evaluated on streaming conditions for a reference IWSLT task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERTnesia: Investigating the capture and forgetting of knowledge in BERT. (arXiv:2106.02902v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02902">
<div class="article-summary-box-inner">
<span><p>Probing complex language models has recently revealed several insights into
linguistic and semantic patterns found in the learned representations. In this
article, we probe BERT specifically to understand and measure the relational
knowledge it captures in its parametric memory. While probing for linguistic
understanding is commonly applied to all layers of BERT as well as fine-tuned
models, this has not been done for factual knowledge. We utilize existing
knowledge base completion tasks (LAMA) to probe every layer of pre-trained as
well as fine-tuned BERT models(ranking, question answering, NER). Our findings
show that knowledge is not just contained in BERT's final layers. Intermediate
layers contribute a significant amount (17-60%) to the total knowledge found.
Probing intermediate layers also reveals how different types of knowledge
emerge at varying rates. When BERT is fine-tuned, relational knowledge is
forgotten. The extent of forgetting is impacted by the fine-tuning objective
and the training data. We found that ranking models forget the least and retain
more knowledge in their final layer compared to masked language modeling and
question-answering. However, masked language modeling performed the best at
acquiring new knowledge from the training data. When it comes to learning
facts, we found that capacity and fact density are key factors. We hope this
initial work will spur further research into understanding the parametric
memory of language models and the effect of training objectives on factual
knowledge. The code to repeat the experiments is publicly available on GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">JNLP Team: Deep Learning Approaches for Legal Processing Tasks in COLIEE 2021. (arXiv:2106.13405v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.13405">
<div class="article-summary-box-inner">
<span><p>COLIEE is an annual competition in automatic computerized legal text
processing. Automatic legal document processing is an ambitious goal, and the
structure and semantics of the law are often far more complex than everyday
language. In this article, we survey and report our methods and experimental
results in using deep learning in legal document processing. The results show
the difficulties as well as potentials in this family of approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BROS: A Layout-Aware Pre-trained Language Model for Understanding Documents. (arXiv:2108.04539v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04539">
<div class="article-summary-box-inner">
<span><p>Understanding documents from their visual snapshots is an emerging problem
that requires both advanced computer vision and NLP methods. The recent advance
in OCR enables the accurate recognition of text blocks, yet it is still
challenging to extract key information from documents due to the diversity of
their layouts. Although recent studies on pre-trained language models show the
importance of incorporating layout information on this task, the conjugation of
texts and their layouts still follows the style of BERT optimized for
understanding the 1D text. This implies there is room for further improvement
considering the 2D nature of text layouts. This paper introduces a pre-trained
language model, BERT Relying On Spatiality (BROS), which effectively utilizes
the information included in individual text blocks and their layouts.
Specifically, BROS encodes spatial information by utilizing relative positions
and learns spatial dependencies between OCR blocks with a novel area-masking
strategy. These two novel approaches lead to an efficient encoding of spatial
layout information highlighted by the robust performance of BROS under
low-resource environments. We also introduce a general-purpose parser that can
be combined with BROS to extract key information even when there is no order
information between text blocks. BROS shows its superiority on four public
benchmarks -- FUNSD, SROIE*, CORD, and SciTSR -- and its robustness in
practical cases where order information of text blocks is not available.
Further experiments with a varying number of training examples demonstrate the
high training efficiency of our approach. Our code will be open to the public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Partition Filter Network for Joint Entity and Relation Extraction. (arXiv:2108.12202v6 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12202">
<div class="article-summary-box-inner">
<span><p>In joint entity and relation extraction, existing work either sequentially
encode task-specific features, leading to an imbalance in inter-task feature
interaction where features extracted later have no direct contact with those
that come first. Or they encode entity features and relation features in a
parallel manner, meaning that feature representation learning for each task is
largely independent of each other except for input sharing. We propose a
partition filter network to model two-way interaction between tasks properly,
where feature encoding is decomposed into two steps: partition and filter. In
our encoder, we leverage two gates: entity and relation gate, to segment
neurons into two task partitions and one shared partition. The shared partition
represents inter-task information valuable to both tasks and is evenly shared
across two tasks to ensure proper two-way interaction. The task partitions
represent intra-task information and are formed through concerted efforts of
both gates, making sure that encoding of task-specific features is dependent
upon each other. Experiment results on six public datasets show that our model
performs significantly better than previous approaches. In addition, contrary
to what previous work has claimed, our auxiliary experiments suggest that
relation prediction is contributory to named entity prediction in a
non-negligible way. The source code can be found at
https://github.com/Coopercoppers/PFN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ProtoInfoMax: Prototypical Networks with Mutual Information Maximization for Out-of-Domain Detection. (arXiv:2108.12229v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12229">
<div class="article-summary-box-inner">
<span><p>The ability to detect Out-of-Domain (OOD) inputs has been a critical
requirement in many real-world NLP applications since the inclusion of
unsupported OOD inputs may lead to catastrophic failure of systems. However, it
remains an empirical question whether current algorithms can tackle such
problem reliably in a realistic scenario where zero OOD training data is
available. In this study, we propose ProtoInfoMax, a new architecture that
extends Prototypical Networks to simultaneously process In-Domain (ID) and OOD
sentences via Mutual Information Maximization (InfoMax) objective. Experimental
results show that our proposed method can substantially improve performance up
to 20% for OOD detection in low resource settings of text classification. We
also show that ProtoInfoMax is less prone to typical over-confidence Error of
Neural Networks, leading to more reliable ID and OOD prediction outcomes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Factual Consistency Evaluation for Text Summarization via Counterfactual Estimation. (arXiv:2108.13134v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13134">
<div class="article-summary-box-inner">
<span><p>Despite significant progress has been achieved in text summarization, factual
inconsistency in generated summaries still severely limits its practical
applications. Among the key factors to ensure factual consistency, a reliable
automatic evaluation metric is the first and the most crucial one. However,
existing metrics either neglect the intrinsic cause of the factual
inconsistency or rely on auxiliary tasks, leading to an unsatisfied correlation
with human judgments or increasing the inconvenience of usage in practice. In
light of these challenges, we propose a novel metric to evaluate the factual
consistency in text summarization via counterfactual estimation, which
formulates the causal relationship among the source document, the generated
summary, and the language prior. We remove the effect of language prior, which
can cause factual inconsistency, from the total causal effect on the generated
summary, and provides a simple yet effective way to evaluate consistency
without relying on other auxiliary tasks. We conduct a series of experiments on
three public abstractive text summarization datasets, and demonstrate the
advantages of the proposed metric in both improving the correlation with human
judgments and the convenience of usage. The source code is available at
https://github.com/xieyxclack/factual_coco.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effective Sequence-to-Sequence Dialogue State Tracking. (arXiv:2108.13990v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13990">
<div class="article-summary-box-inner">
<span><p>Sequence-to-sequence models have been applied to a wide variety of NLP tasks,
but how to properly use them for dialogue state tracking has not been
systematically investigated. In this paper, we study this problem from the
perspectives of pre-training objectives as well as the formats of context
representations. We demonstrate that the choice of pre-training objective makes
a significant difference to the state tracking quality. In particular, we find
that masked span prediction is more effective than auto-regressive language
modeling. We also explore using Pegasus, a span prediction-based pre-training
objective for text summarization, for the state tracking model. We found that
pre-training for the seemingly distant summarization task works surprisingly
well for dialogue state tracking. In addition, we found that while recurrent
state context representation works also reasonably well, the model may have a
hard time recovering from earlier mistakes. We conducted experiments on the
MultiWOZ 2.1-2.4, WOZ 2.0, and DSTC2 datasets with consistent observations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MergeBERT: Program Merge Conflict Resolution via Neural Transformers. (arXiv:2109.00084v2 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00084">
<div class="article-summary-box-inner">
<span><p>Collaborative software development is an integral part of the modern software
development life cycle, essential to the success of large-scale software
projects. When multiple developers make concurrent changes around the same
lines of code, a merge conflict may occur. Such conflicts stall pull requests
and continuous integration pipelines for hours to several days, seriously
hurting developer productivity.
</p>
<p>In this paper, we introduce MergeBERT, a novel neural program merge framework
based on the token-level three-way differencing and a transformer encoder
model. Exploiting restricted nature of merge conflict resolutions, we
reformulate the task of generating the resolution sequence as a classification
task over a set of primitive merge patterns extracted from real-world merge
commit data.
</p>
<p>Our model achieves 64--69% precision of merge resolution synthesis, yielding
nearly a 2x performance improvement over existing structured and neural program
merge tools. Finally, we demonstrate versatility of our model, which is able to
perform program merge in a multilingual setting with Java, JavaScript,
TypeScript, and C# programming languages, generalizing zero-shot to unseen
languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">M^2-MedDialog: A Dataset and Benchmarks for Multi-domain Multi-service Medical Dialogues. (arXiv:2109.00430v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00430">
<div class="article-summary-box-inner">
<span><p>Medical dialogue systems (MDSs) aim to assist doctors and patients with a
range of professional medical services, i.e., diagnosis, consultation, and
treatment. However, one-stop MDS is still unexplored because: (1) no dataset
has so large-scale dialogues contains both multiple medical services and
fine-grained medical labels (i.e., intents, slots, values); (2) no model has
addressed a MDS based on multiple-service conversations in a unified framework.
In this work, we first build a Multiple-domain Multiple-service medical
dialogue (M^2-MedDialog)dataset, which contains 1,557 conversations between
doctors and patients, covering 276 types of diseases, 2,468 medical entities,
and 3 specialties of medical services. To the best of our knowledge, it is the
only medical dialogue dataset that includes both multiple medical services and
fine-grained medical labels. Then, we formulate a one-stop MDS as a
sequence-to-sequence generation problem. We unify a MDS with causal language
modeling and conditional causal language modeling, respectively. Specifically,
we employ several pretrained models (i.e., BERT-WWM, BERT-MED, GPT2, and MT5)
and their variants to get benchmarks on M^2-MedDialog dataset. We also propose
pseudo labeling and natural perturbation methods to expand M2-MedDialog dataset
and enhance the state-of-the-art pretrained models. We demonstrate the results
achieved by the benchmarks so far through extensive experiments on
M2-MedDialog. We release the dataset, the code, as well as the evaluation
scripts to facilitate future research in this important research direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PermuteFormer: Efficient Relative Position Encoding for Long Sequences. (arXiv:2109.02377v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02377">
<div class="article-summary-box-inner">
<span><p>A recent variation of Transformer, Performer, scales Transformer to longer
sequences with a linear attention mechanism. However, it is not compatible with
relative position encoding, which has advantages over absolute position
encoding. In this paper, we discuss possible ways to add relative position
encoding to Performer. Based on the analysis, we propose PermuteFormer, a
Performer-based model with relative position encoding that scales linearly on
long sequences. PermuteFormer applies position-dependent transformation on
queries and keys to encode positional information into the attention module.
This transformation is carefully crafted so that the final output of
self-attention is not affected by absolute positions of tokens. PermuteFormer
introduces negligible computational overhead by design that it runs as fast as
Performer. We evaluate PermuteFormer on Long-Range Arena, a dataset for long
sequences, as well as WikiText-103, a language modeling dataset. The
experiments show that PermuteFormer uniformly improves the performance of
Performer with almost no computational overhead and outperforms vanilla
Transformer on most of the tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision Guided Generative Pre-trained Language Models for Multimodal Abstractive Summarization. (arXiv:2109.02401v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02401">
<div class="article-summary-box-inner">
<span><p>Multimodal abstractive summarization (MAS) models that summarize videos
(vision modality) and their corresponding transcripts (text modality) are able
to extract the essential information from massive multimodal data on the
Internet. Recently, large-scale generative pre-trained language models (GPLMs)
have been shown to be effective in text generation tasks. However, existing MAS
models cannot leverage GPLMs' powerful generation ability. To fill this
research gap, we aim to study two research questions: 1) how to inject visual
information into GPLMs without hurting their generation ability; and 2) where
is the optimal place in GPLMs to inject the visual information? In this paper,
we present a simple yet effective method to construct vision guided (VG) GPLMs
for the MAS task using attention-based add-on layers to incorporate visual
information while maintaining their original text generation ability. Results
show that our best model significantly surpasses the prior state-of-the-art
model by 5.7 ROUGE-1, 5.3 ROUGE-2, and 5.1 ROUGE-L scores on the How2 dataset,
and our visual guidance method contributes 83.6% of the overall improvement.
Furthermore, we conduct thorough ablation studies to analyze the effectiveness
of various modality fusion methods and fusion locations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient conformer: Progressive downsampling and grouped attention for automatic speech recognition. (arXiv:2109.01163v2 [eess.AS] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01163">
<div class="article-summary-box-inner">
<span><p>The recently proposed Conformer architecture has shown state-of-the-art
performances in Automatic Speech Recognition by combining convolution with
attention to model both local and global dependencies. In this paper, we study
how to reduce the Conformer architecture complexity with a limited computing
budget, leading to a more efficient architecture design that we call Efficient
Conformer. We introduce progressive downsampling to the Conformer encoder and
propose a novel attention mechanism named grouped attention, allowing us to
reduce attention complexity from $O(n^{2}d)$ to $O(n^{2}d / g)$ for sequence
length $n$, hidden dimension $d$ and group size parameter $g$. We also
experiment the use of strided multi-head self-attention as a global
downsampling operation. Our experiments are performed on the LibriSpeech
dataset with CTC and RNN-Transducer losses. We show that within the same
computing budget, the proposed architecture achieves better performances with
faster training and decoding compared to the Conformer. Our 13M parameters CTC
model achieves competitive WERs of 3.6%/9.0% without using a language model and
2.7%/6.7% with an external n-gram language model on the test-clean/test-other
sets while being 29% faster than our CTC Conformer baseline at inference and
36% faster to train.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">MRI Reconstruction Using Deep Energy-Based Model. (arXiv:2109.03237v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03237">
<div class="article-summary-box-inner">
<span><p>Purpose: Although recent deep energy-based generative models (EBMs) have
shown encouraging results in many image generation tasks, how to take advantage
of the self-adversarial cogitation in deep EBMs to boost the performance of
Magnetic Resonance Imaging (MRI) reconstruction is still desired.
</p>
<p>Methods: With the successful application of deep learning in a wide range of
MRI reconstruction, a line of emerging research involves formulating an
optimization-based reconstruction method in the space of a generative model.
Leveraging this, a novel regularization strategy is introduced in this article
which takes advantage of self-adversarial cogitation of the deep energy-based
model. More precisely, we advocate for alternative learning a more powerful
energy-based model with maximum likelihood estimation to obtain the deep
energy-based information, represented as image prior. Simultaneously, implicit
inference with Langevin dynamics is a unique property of re-construction. In
contrast to other generative models for reconstruction, the proposed method
utilizes deep energy-based information as the image prior in reconstruction to
improve the quality of image.
</p>
<p>Results: Experiment results that imply the proposed technique can obtain
remarkable performance in terms of high reconstruction accuracy that is
competitive with state-of-the-art methods, and does not suffer from mode
collapse.
</p>
<p>Conclusion: Algorithmically, an iterative approach was presented to
strengthen EBM training with the gradient of energy network. The robustness and
the reproducibility of the algorithm were also experimentally validated. More
importantly, the proposed reconstruction framework can be generalized for most
MRI reconstruction scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simple Video Generation using Neural ODEs. (arXiv:2109.03292v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03292">
<div class="article-summary-box-inner">
<span><p>Despite having been studied to a great extent, the task of conditional
generation of sequences of frames, or videos, remains extremely challenging. It
is a common belief that a key step towards solving this task resides in
modelling accurately both spatial and temporal information in video signals. A
promising direction to do so has been to learn latent variable models that
predict the future in latent space and project back to pixels, as suggested in
recent literature. Following this line of work and building on top of a family
of models introduced in prior work, Neural ODE, we investigate an approach that
models time-continuous dynamics over a continuous latent space with a
differential equation with respect to time. The intuition behind this approach
is that these trajectories in latent space could then be extrapolated to
generate video frames beyond the time steps for which the model is trained. We
show that our approach yields promising results in the task of future frame
prediction on the Moving MNIST dataset with 1 and 2 digits.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Representation Learning using Visual Field Expansion on Digital Pathology. (arXiv:2109.03299v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03299">
<div class="article-summary-box-inner">
<span><p>The examination of histopathology images is considered to be the gold
standard for the diagnosis and stratification of cancer patients. A key
challenge in the analysis of such images is their size, which can run into the
gigapixels and can require tedious screening by clinicians. With the recent
advances in computational medicine, automatic tools have been proposed to
assist clinicians in their everyday practice. Such tools typically process
these large images by slicing them into tiles that can then be encoded and
utilized for different clinical models. In this study, we propose a novel
generative framework that can learn powerful representations for such tiles by
learning to plausibly expand their visual field. In particular, we developed a
progressively grown generative model with the objective of visual field
expansion. Thus trained, our model learns to generate different tissue types
with fine details, while simultaneously learning powerful representations that
can be used for different clinical endpoints, all in a self-supervised way. To
evaluate the performance of our model, we conducted classification experiments
on CAMELYON17 and CRC benchmark datasets, comparing favorably to other
self-supervised and pre-trained strategies that are commonly used in digital
pathology. Our code is available at https://github.com/jcboyd/cdpath21-gan.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Melatect: A Machine Learning Model Approach For Identifying Malignant Melanoma in Skin Growths. (arXiv:2109.03310v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03310">
<div class="article-summary-box-inner">
<span><p>Malignant melanoma is a common skin cancer that is mostly curable before
metastasis, where melanoma growths spawn in organs away from the original site.
Melanoma is the most dangerous type of skin cancer if left untreated due to the
high chance of metastasis. This paper presents Melatect, a machine learning
model that identifies potential malignant melanoma. A recursive computer image
analysis algorithm was used to create a machine learning model which is capable
of detecting likely melanoma. The comparison is performed using 20,000 raw
images of benign and malignant lesions from the International Skin Imaging
Collaboration (ISIC) archive that were augmented to 60,000 images. Tests of the
algorithm using subsets of the ISIC images suggest it accurately classifies
lesions as malignant or benign over 95% of the time with no apparent bias or
overfitting. The Melatect iOS app was later created (unpublished), in which the
machine learning model was embedded. With the app, users have the ability to
take pictures of skin lesions (moles) using the app, which are then processed
through the machine learning model, and users are notified whether their lesion
could be abnormal or not. Melatect provides a convenient way to get free advice
on lesions and track these lesions over time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reconstructing High-resolution Turbulent Flows Using Physics-Guided Neural Networks. (arXiv:2109.03327v1 [physics.flu-dyn])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03327">
<div class="article-summary-box-inner">
<span><p>Direct numerical simulation (DNS) of turbulent flows is computationally
expensive and cannot be applied to flows with large Reynolds numbers. Large
eddy simulation (LES) is an alternative that is computationally less demanding,
but is unable to capture all of the scales of turbulent transport accurately.
Our goal in this work is to build a new data-driven methodology based on
super-resolution techniques to reconstruct DNS data from LES predictions. We
leverage the underlying physical relationships to regularize the relationships
amongst different physical variables. We also introduce a hierarchical
generative process and a reverse degradation process to fully explore the
correspondence between DNS and LES data. We demonstrate the effectiveness of
our method through a single-snapshot experiment and a cross-time experiment.
The results confirm that our method can better reconstruct high-resolution DNS
data over space and over time in terms of pixel-wise reconstruction error and
structural similarity. Visual comparisons show that our method performs much
better in capturing fine-level flow dynamics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-World Adversarial Examples involving Makeup Application. (arXiv:2109.03329v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03329">
<div class="article-summary-box-inner">
<span><p>Deep neural networks have developed rapidly and have achieved outstanding
performance in several tasks, such as image classification and natural language
processing. However, recent studies have indicated that both digital and
physical adversarial examples can fool neural networks. Face-recognition
systems are used in various applications that involve security threats from
physical adversarial examples. Herein, we propose a physical adversarial attack
with the use of full-face makeup. The presence of makeup on the human face is a
reasonable possibility, which possibly increases the imperceptibility of
attacks. In our attack framework, we combine the cycle-adversarial generative
network (cycle-GAN) and a victimized classifier. The Cycle-GAN is used to
generate adversarial makeup, and the architecture of the victimized classifier
is VGG 16. Our experimental results show that our attack can effectively
overcome manual errors in makeup application, such as color and
position-related errors. We also demonstrate that the approaches used to train
the models can influence physical attacks; the adversarial perturbations
crafted from the pre-trained model are affected by the corresponding training
data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Branch Deep Radial Basis Function Networks for Facial Emotion Recognition. (arXiv:2109.03336v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03336">
<div class="article-summary-box-inner">
<span><p>Emotion recognition (ER) from facial images is one of the landmark tasks in
affective computing with major developments in the last decade. Initial efforts
on ER relied on handcrafted features that were used to characterize facial
images and then feed to standard predictive models. Recent methodologies
comprise end-to-end trainable deep learning methods that simultaneously learn
both, features and predictive model. Perhaps the most successful models are
based on convolutional neural networks (CNNs). While these models have excelled
at this task, they still fail at capturing local patterns that could emerge in
the learning process. We hypothesize these patterns could be captured by
variants based on locally weighted learning. Specifically, in this paper we
propose a CNN based architecture enhanced with multiple branches formed by
radial basis function (RBF) units that aims at exploiting local information at
the final stage of the learning process. Intuitively, these RBF units capture
local patterns shared by similar instances using an intermediate
representation, then the outputs of the RBFs are feed to a softmax layer that
exploits this information to improve the predictive performance of the model.
This feature could be particularly advantageous in ER as cultural / ethnicity
differences may be identified by the local units. We evaluate the proposed
method in several ER datasets and show the proposed methodology achieves
state-of-the-art in some of them, even when we adopt a pre-trained VGG-Face
model as backbone. We show it is the incorporation of local information what
makes the proposed model competitive.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Certifiable Outlier-Robust Geometric Perception: Exact Semidefinite Relaxations and Scalable Global Optimization. (arXiv:2109.03349v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03349">
<div class="article-summary-box-inner">
<span><p>We propose the first general and scalable framework to design certifiable
algorithms for robust geometric perception in the presence of outliers. Our
first contribution is to show that estimation using common robust costs, such
as truncated least squares (TLS), maximum consensus, Geman-McClure, Tukey's
biweight, among others, can be reformulated as polynomial optimization problems
(POPs). By focusing on the TLS cost, our second contribution is to exploit
sparsity in the POP and propose a sparse semidefinite programming (SDP)
relaxation that is much smaller than the standard Lasserre's hierarchy while
preserving exactness, i.e., the SDP recovers the optimizer of the nonconvex POP
with an optimality certificate. Our third contribution is to solve the SDP
relaxations at an unprecedented scale and accuracy by presenting STRIDE, a
solver that blends global descent on the convex SDP with fast local search on
the nonconvex POP. Our fourth contribution is an evaluation of the proposed
framework on six geometric perception problems including single and multiple
rotation averaging, point cloud and mesh registration, absolute pose
estimation, and category-level object pose and shape estimation. Our
experiments demonstrate that (i) our sparse SDP relaxation is exact with up to
60%-90% outliers across applications; (ii) while still being far from
real-time, STRIDE is up to 100 times faster than existing SDP solvers on
medium-scale problems, and is the only solver that can solve large-scale SDPs
with hundreds of thousands of constraints to high accuracy; (iii) STRIDE
provides a safeguard to existing fast heuristics for robust estimation (e.g.,
RANSAC or Graduated Non-Convexity), i.e., it certifies global optimality if the
heuristic estimates are optimal, or detects and allows escaping local optima
when the heuristic estimates are suboptimal.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Capturing the objects of vision with neural networks. (arXiv:2109.03351v1 [q-bio.NC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03351">
<div class="article-summary-box-inner">
<span><p>Human visual perception carves a scene at its physical joints, decomposing
the world into objects, which are selectively attended, tracked, and predicted
as we engage our surroundings. Object representations emancipate perception
from the sensory input, enabling us to keep in mind that which is out of sight
and to use perceptual content as a basis for action and symbolic cognition.
Human behavioral studies have documented how object representations emerge
through grouping, amodal completion, proto-objects, and object files. Deep
neural network (DNN) models of visual object recognition, by contrast, remain
largely tethered to the sensory input, despite achieving human-level
performance at labeling objects. Here, we review related work in both fields
and examine how these fields can help each other. The cognitive literature
provides a starting point for the development of new experimental tasks that
reveal mechanisms of human object perception and serve as benchmarks driving
development of deep neural network models that will put the object into object
recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RoadAtlas: Intelligent Platform for Automated Road Defect Detection and Asset Management. (arXiv:2109.03385v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03385">
<div class="article-summary-box-inner">
<span><p>With the rapid development of intelligent detection algorithms based on deep
learning, much progress has been made in automatic road defect recognition and
road marking parsing. This can effectively address the issue of an expensive
and time-consuming process for professional inspectors to review the street
manually. Towards this goal, we present RoadAtlas, a novel end-to-end
integrated system that can support 1) road defect detection, 2) road marking
parsing, 3) a web-based dashboard for presenting and inputting data by users,
and 4) a backend containing a well-structured database and developed APIs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Discriminate Information for Online Action Detection: Analysis and Application. (arXiv:2109.03393v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03393">
<div class="article-summary-box-inner">
<span><p>Online action detection, which aims to identify an ongoing action from a
streaming video, is an important subject in real-world applications. For this
task, previous methods use recurrent neural networks for modeling temporal
relations in an input sequence. However, these methods overlook the fact that
the input image sequence includes not only the action of interest but
background and irrelevant actions. This would induce recurrent units to
accumulate unnecessary information for encoding features on the action of
interest. To overcome this problem, we propose a novel recurrent unit, named
Information Discrimination Unit (IDU), which explicitly discriminates the
information relevancy between an ongoing action and others to decide whether to
accumulate the input information. This enables learning more discriminative
representations for identifying an ongoing action. In this paper, we further
present a new recurrent unit, called Information Integration Unit (IIU), for
action anticipation. Our IIU exploits the outputs from IDU as pseudo action
labels as well as RGB frames to learn enriched features of observed actions
effectively. In experiments on TVSeries and THUMOS-14, the proposed methods
outperform state-of-the-art methods by a significant margin in online action
detection and action anticipation. Moreover, we demonstrate the effectiveness
of the proposed units by conducting comprehensive ablation studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Master Face Attacks on Face Recognition Systems. (arXiv:2109.03398v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03398">
<div class="article-summary-box-inner">
<span><p>Face authentication is now widely used, especially on mobile devices, rather
than authentication using a personal identification number or an unlock
pattern, due to its convenience. It has thus become a tempting target for
attackers using a presentation attack. Traditional presentation attacks use
facial images or videos of the victim. Previous work has proven the existence
of master faces, i.e., faces that match multiple enrolled templates in face
recognition systems, and their existence extends the ability of presentation
attacks. In this paper, we perform an extensive study on latent variable
evolution (LVE), a method commonly used to generate master faces. We run an LVE
algorithm for various scenarios and with more than one database and/or face
recognition system to study the properties of the master faces and to
understand in which conditions strong master faces could be generated.
Moreover, through analysis, we hypothesize that master faces come from some
dense areas in the embedding spaces of the face recognition systems. Last but
not least, simulated presentation attacks using generated master faces
generally preserve the false-matching ability of their original digital forms,
thus demonstrating that the existence of master faces poses an actual threat.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GTT-Net: Learned Generalized Trajectory Triangulation. (arXiv:2109.03408v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03408">
<div class="article-summary-box-inner">
<span><p>We present GTT-Net, a supervised learning framework for the reconstruction of
sparse dynamic 3D geometry. We build on a graph-theoretic formulation of the
generalized trajectory triangulation problem, where non-concurrent multi-view
imaging geometry is known but global image sequencing is not provided. GTT-Net
learns pairwise affinities modeling the spatio-temporal relationships among our
input observations and leverages them to determine 3D geometry estimates.
Experiments reconstructing 3D motion-capture sequences show GTT-Net outperforms
the state of the art in terms of accuracy and robustness. Within the context of
articulated motion reconstruction, our proposed architecture is 1) able to
learn and enforce semantic 3D motion priors for shared training and test
domains, while being 2) able to generalize its performance across different
training and test domains. Moreover, GTT-Net provides a computationally
streamlined framework for trajectory triangulation with applications to
multi-instance reconstruction and event segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">YouRefIt: Embodied Reference Understanding with Language and Gesture. (arXiv:2109.03413v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03413">
<div class="article-summary-box-inner">
<span><p>We study the understanding of embodied reference: One agent uses both
language and gesture to refer to an object to another agent in a shared
physical environment. Of note, this new visual task requires understanding
multimodal cues with perspective-taking to identify which object is being
referred to. To tackle this problem, we introduce YouRefIt, a new crowd-sourced
dataset of embodied reference collected in various physical scenes; the dataset
contains 4,195 unique reference clips in 432 indoor scenes. To the best of our
knowledge, this is the first embodied reference dataset that allows us to study
referring expressions in daily physical scenes to understand referential
behavior, human communication, and human-robot interaction. We further devise
two benchmarks for image-based and video-based embodied reference
understanding. Comprehensive baselines and extensive experiments provide the
very first result of machine perception on how the referring expressions and
gestures affect the embodied reference understanding. Our results provide
essential evidence that gestural cues are as critical as language cues in
understanding the embodied reference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RGB-D Salient Object Detection with Ubiquitous Target Awareness. (arXiv:2109.03425v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03425">
<div class="article-summary-box-inner">
<span><p>Conventional RGB-D salient object detection methods aim to leverage depth as
complementary information to find the salient regions in both modalities.
However, the salient object detection results heavily rely on the quality of
captured depth data which sometimes are unavailable. In this work, we make the
first attempt to solve the RGB-D salient object detection problem with a novel
depth-awareness framework. This framework only relies on RGB data in the
testing phase, utilizing captured depth data as supervision for representation
learning. To construct our framework as well as achieving accurate salient
detection results, we propose a Ubiquitous Target Awareness (UTA) network to
solve three important challenges in RGB-D SOD task: 1) a depth awareness module
to excavate depth information and to mine ambiguous regions via adaptive
depth-error weights, 2) a spatial-aware cross-modal interaction and a
channel-aware cross-level interaction, exploiting the low-level boundary cues
and amplifying high-level salient channels, and 3) a gated multi-scale
predictor module to perceive the object saliency in different contextual
scales. Besides its high performance, our proposed UTA network is depth-free
for inference and runs in real-time with 43 FPS. Experimental evidence
demonstrates that our proposed network not only surpasses the state-of-the-art
methods on five public RGB-D SOD benchmarks by a large margin, but also
verifies its extensibility on five public RGB SOD benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mask is All You Need: Rethinking Mask R-CNN for Dense and Arbitrary-Shaped Scene Text Detection. (arXiv:2109.03426v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03426">
<div class="article-summary-box-inner">
<span><p>Due to the large success in object detection and instance segmentation, Mask
R-CNN attracts great attention and is widely adopted as a strong baseline for
arbitrary-shaped scene text detection and spotting. However, two issues remain
to be settled. The first is dense text case, which is easy to be neglected but
quite practical. There may exist multiple instances in one proposal, which
makes it difficult for the mask head to distinguish different instances and
degrades the performance. In this work, we argue that the performance
degradation results from the learning confusion issue in the mask head. We
propose to use an MLP decoder instead of the "deconv-conv" decoder in the mask
head, which alleviates the issue and promotes robustness significantly. And we
propose instance-aware mask learning in which the mask head learns to predict
the shape of the whole instance rather than classify each pixel to text or
non-text. With instance-aware mask learning, the mask branch can learn
separated and compact masks. The second is that due to large variations in
scale and aspect ratio, RPN needs complicated anchor settings, making it hard
to maintain and transfer across different datasets. To settle this issue, we
propose an adaptive label assignment in which all instances especially those
with extreme aspect ratios are guaranteed to be associated with enough anchors.
Equipped with these components, the proposed method named MAYOR achieves
state-of-the-art performance on five benchmarks including DAST1500, MSRA-TD500,
ICDAR2015, CTW1500, and Total-Text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SSEGEP: Small SEGment Emphasized Performance evaluation metric for medical image segmentation. (arXiv:2109.03435v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03435">
<div class="article-summary-box-inner">
<span><p>Automatic image segmentation is a critical component of medical image
analysis, and hence quantifying segmentation performance is crucial. Challenges
in medical image segmentation are mainly due to spatial variations of regions
to be segmented and imbalance in distribution of classes. Commonly used metrics
treat all detected pixels, indiscriminately. However, pixels in smaller
segments must be treated differently from pixels in larger segments, as
detection of smaller ones aid in early treatment of associated disease and are
also easier to miss. To address this, we propose a novel evaluation metric for
segmentation performance, emphasizing smaller segments, by assigning higher
weightage to smaller segment pixels. Weighted false positives are also
considered in deriving the new metric named, "SSEGEP"(Small SEGment Emphasized
Performance evaluation metric), (range : 0(Bad) to 1(Good)). The experiments
were performed on diverse anatomies(eye, liver, pancreas and breast) from
publicly available datasets to show applicability of the proposed metric across
different imaging techniques. Mean opinion score (MOS) and statistical
significance testing is used to quantify the relevance of proposed approach.
Across 33 fundus images, where the largest exudate is 1.41%, and the smallest
is 0.0002% of the image, the proposed metric is 30% closer to MOS, as compared
to Dice Similarity Coefficient (DSC). Statistical significance testing resulted
in promising p-value of order 10^{-18} with SSEGEP for hepatic tumor compared
to DSC. The proposed metric is found to perform better for the images having
multiple segments for a single label.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unfolding Taylor's Approximations for Image Restoration. (arXiv:2109.03442v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03442">
<div class="article-summary-box-inner">
<span><p>Deep learning provides a new avenue for image restoration, which demands a
delicate balance between fine-grained details and high-level contextualized
information during recovering the latent clear image. In practice, however,
existing methods empirically construct encapsulated end-to-end mapping networks
without deepening into the rationality, and neglect the intrinsic prior
knowledge of restoration task. To solve the above problems, inspired by
Taylor's Approximations, we unfold Taylor's Formula to construct a novel
framework for image restoration. We find the main part and the derivative part
of Taylor's Approximations take the same effect as the two competing goals of
high-level contextualized information and spatial details of image restoration
respectively. Specifically, our framework consists of two steps,
correspondingly responsible for the mapping and derivative functions. The
former first learns the high-level contextualized information and the later
combines it with the degraded input to progressively recover local high-order
spatial details. Our proposed framework is orthogonal to existing methods and
thus can be easily integrated with them for further improvement, and extensive
experiments demonstrate the effectiveness and scalability of our proposed
framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward Real-World Super-Resolution via Adaptive Downsampling Models. (arXiv:2109.03444v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03444">
<div class="article-summary-box-inner">
<span><p>Most image super-resolution (SR) methods are developed on synthetic
low-resolution (LR) and high-resolution (HR) image pairs that are constructed
by a predetermined operation, e.g., bicubic downsampling. As existing methods
typically learn an inverse mapping of the specific function, they produce
blurry results when applied to real-world images whose exact formulation is
different and unknown. Therefore, several methods attempt to synthesize much
more diverse LR samples or learn a realistic downsampling model. However, due
to restrictive assumptions on the downsampling process, they are still biased
and less generalizable. This study proposes a novel method to simulate an
unknown downsampling process without imposing restrictive prior knowledge. We
propose a generalizable low-frequency loss (LFL) in the adversarial training
framework to imitate the distribution of target LR images without using any
paired examples. Furthermore, we design an adaptive data loss (ADL) for the
downsampler, which can be adaptively learned and updated from the data during
the training loops. Extensive experiments validate that our downsampling model
can facilitate existing SR methods to perform more accurate reconstructions on
various synthetic and real-world examples than the conventional approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Which and Where to Focus: A Simple yet Accurate Framework for Arbitrary-Shaped Nearby Text Detection in Scene Images. (arXiv:2109.03451v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03451">
<div class="article-summary-box-inner">
<span><p>Scene text detection has drawn the close attention of researchers. Though
many methods have been proposed for horizontal and oriented texts, previous
methods may not perform well when dealing with arbitrary-shaped texts such as
curved texts. In particular, confusion problem arises in the case of nearby
text instances. In this paper, we propose a simple yet effective method for
accurate arbitrary-shaped nearby scene text detection. Firstly, a One-to-Many
Training Scheme (OMTS) is designed to eliminate confusion and enable the
proposals to learn more appropriate groundtruths in the case of nearby text
instances. Secondly, we propose a Proposal Feature Attention Module (PFAM) to
exploit more effective features for each proposal, which can better adapt to
arbitrary-shaped text instances. Finally, we propose a baseline that is based
on Faster R-CNN and outputs the curve representation directly. Equipped with
PFAM and OMTS, the detector can achieve state-of-the-art or competitive
performance on several challenging benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recalibrating the KITTI Dataset Camera Setup for Improved Odometry Accuracy. (arXiv:2109.03462v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03462">
<div class="article-summary-box-inner">
<span><p>Over the last decade, one of the most relevant public datasets for evaluating
odometry accuracy is the KITTI dataset. Beside the quality and rich sensor
setup, its success is also due to the online evaluation tool, which enables
researchers to benchmark and compare algorithms. The results are evaluated on
the test subset solely, without any knowledge about the ground truth, yielding
unbiased, overfit free and therefore relevant validation for robot localization
based on cameras, 3D laser or combination of both. However, as any sensor
setup, it requires prior calibration and rectified stereo images are provided,
introducing dependence on the default calibration parameters. Given that, a
natural question arises if a better set of calibration parameters can be found
that would yield higher odometry accuracy. In this paper, we propose a new
approach for one shot calibration of the KITTI dataset multiple camera setup.
The approach yields better calibration parameters, both in the sense of lower
calibration reprojection errors and lower visual odometry error. We conducted
experiments where we show for three different odometry algorithms, namely
SOFT2, ORB-SLAM2 and VISO2, that odometry accuracy is significantly improved
with the proposed calibration parameters. Moreover, our odometry, SOFT2, in
conjunction with the proposed calibration method achieved the highest accuracy
on the official KITTI scoreboard with 0.53% translational and 0.0009 deg/m
rotational error, outperforming even 3D laser-based methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Level Set Binocular Stereo with Occlusions. (arXiv:2109.03464v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03464">
<div class="article-summary-box-inner">
<span><p>Localizing stereo boundaries and predicting nearby disparities are difficult
because stereo boundaries induce occluded regions where matching cues are
absent. Most modern computer vision algorithms treat occlusions secondarily
(e.g., via left-right consistency checks after matching) or rely on high-level
cues to improve nearby disparities (e.g., via deep networks and large training
sets). They ignore the geometry of stereo occlusions, which dictates that the
spatial extent of occlusion must equal the amplitude of the disparity jump that
causes it. This paper introduces an energy and level-set optimizer that
improves boundaries by encoding occlusion geometry. Our model applies to
two-layer, figure-ground scenes, and it can be implemented cooperatively using
messages that pass predominantly between parents and children in an undecimated
hierarchy of multi-scale image patches. In a small collection of figure-ground
scenes curated from Middlebury and Falling Things stereo datasets, our model
provides more accurate boundaries than previous occlusion-handling stereo
techniques. This suggests new directions for creating cooperative stereo
systems that incorporate occlusion cues in a human-like manner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Site Severity Assessment of COVID-19 from CT Images via Domain Adaptation. (arXiv:2109.03478v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03478">
<div class="article-summary-box-inner">
<span><p>Early and accurate severity assessment of Coronavirus disease 2019 (COVID-19)
based on computed tomography (CT) images offers a great help to the estimation
of intensive care unit event and the clinical decision of treatment planning.
To augment the labeled data and improve the generalization ability of the
classification model, it is necessary to aggregate data from multiple sites.
This task faces several challenges including class imbalance between mild and
severe infections, domain distribution discrepancy between sites, and presence
of heterogeneous features. In this paper, we propose a novel domain adaptation
(DA) method with two components to address these problems. The first component
is a stochastic class-balanced boosting sampling strategy that overcomes the
imbalanced learning problem and improves the classification performance on
poorly-predicted classes. The second component is a representation learning
that guarantees three properties: 1) domain-transferability by prototype
triplet loss, 2) discriminant by conditional maximum mean discrepancy loss, and
3) completeness by multi-view reconstruction loss. Particularly, we propose a
domain translator and align the heterogeneous data to the estimated class
prototypes (i.e., class centers) in a hyper-sphere manifold. Experiments on
cross-site severity assessment of COVID-19 from CT images show that the
proposed method can effectively tackle the imbalanced learning problem and
outperform recent DA approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pose-guided Inter- and Intra-part Relational Transformer for Occluded Person Re-Identification. (arXiv:2109.03483v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03483">
<div class="article-summary-box-inner">
<span><p>Person Re-Identification (Re-Id) in occlusion scenarios is a challenging
problem because a pedestrian can be partially occluded. The use of local
information for feature extraction and matching is still necessary. Therefore,
we propose a Pose-guided inter-and intra-part relational transformer (Pirt) for
occluded person Re-Id, which builds part-aware long-term correlations by
introducing transformers. In our framework, we firstly develop a pose-guided
feature extraction module with regional grouping and mask construction for
robust feature representations. The positions of a pedestrian in the image
under surveillance scenarios are relatively fixed, hence we propose an
intra-part and inter-part relational transformer. The intra-part module creates
local relations with mask-guided features, while the inter-part relationship
builds correlations with transformers, to develop cross relationships between
part nodes. With the collaborative learning inter- and intra-part
relationships, experiments reveal that our proposed Pirt model achieves a new
state of the art on the public occluded dataset, and further extensions on
standard non-occluded person Re-Id datasets also reveal our comparable
performances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shuffled Patch-Wise Supervision for Presentation Attack Detection. (arXiv:2109.03484v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03484">
<div class="article-summary-box-inner">
<span><p>Face anti-spoofing is essential to prevent false facial verification by using
a photo, video, mask, or a different substitute for an authorized person's
face. Most of the state-of-the-art presentation attack detection (PAD) systems
suffer from overfitting, where they achieve near-perfect scores on a single
dataset but fail on a different dataset with more realistic data. This problem
drives researchers to develop models that perform well under real-world
conditions. This is an especially challenging problem for frame-based
presentation attack detection systems that use convolutional neural networks
(CNN). To this end, we propose a new PAD approach, which combines pixel-wise
binary supervision with patch-based CNN. We believe that training a CNN with
face patches allows the model to distinguish spoofs without learning background
or dataset-specific traces. We tested the proposed method both on the standard
benchmark datasets -- Replay-Mobile, OULU-NPU -- and on a real-world dataset.
The proposed approach shows its superiority on challenging experimental setups.
Namely, it achieves higher performance on OULU-NPU protocol 3, 4 and on
inter-dataset real-world experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FaceCook: Face Generation Based on Linear Scaling Factors. (arXiv:2109.03492v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03492">
<div class="article-summary-box-inner">
<span><p>With the excellent disentanglement properties of state-of-the-art generative
models, image editing has been the dominant approach to control the attributes
of synthesised face images. However, these edited results often suffer from
artifacts or incorrect feature rendering, especially when there is a large
discrepancy between the image to be edited and the desired feature set.
Therefore, we propose a new approach to mapping the latent vectors of the
generative model to the scaling factors through solving a set of multivariate
linear equations. The coefficients of the equations are the eigenvectors of the
weight parameters of the pre-trained model, which form the basis of a hyper
coordinate system. The qualitative and quantitative results both show that the
proposed method outperforms the baseline in terms of image diversity. In
addition, the method is much more time-efficient because you can obtain
synthesised images with desirable features directly from the latent vectors,
rather than the former process of editing randomly generated images requiring
many processing steps.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Temporal RoI Align for Video Object Recognition. (arXiv:2109.03495v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03495">
<div class="article-summary-box-inner">
<span><p>Video object detection is challenging in the presence of appearance
deterioration in certain video frames. Therefore, it is a natural choice to
aggregate temporal information from other frames of the same video into the
current frame. However, RoI Align, as one of the most core procedures of video
detectors, still remains extracting features from a single-frame feature map
for proposals, making the extracted RoI features lack temporal information from
videos. In this work, considering the features of the same object instance are
highly similar among frames in a video, a novel Temporal RoI Align operator is
proposed to extract features from other frames feature maps for current frame
proposals by utilizing feature similarity. The proposed Temporal RoI Align
operator can extract temporal information from the entire video for proposals.
We integrate it into single-frame video detectors and other state-of-the-art
video detectors, and conduct quantitative experiments to demonstrate that the
proposed Temporal RoI Align operator can consistently and significantly boost
the performance. Besides, the proposed Temporal RoI Align can also be applied
into video instance segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Elastic Significant Bit Quantization and Acceleration for Deep Neural Networks. (arXiv:2109.03513v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03513">
<div class="article-summary-box-inner">
<span><p>Quantization has been proven to be a vital method for improving the inference
efficiency of deep neural networks (DNNs). However, it is still challenging to
strike a good balance between accuracy and efficiency while quantizing DNN
weights or activation values from high-precision formats to their quantized
counterparts. We propose a new method called elastic significant bit
quantization (ESB) that controls the number of significant bits of quantized
values to obtain better inference accuracy with fewer resources. We design a
unified mathematical formula to constrain the quantized values of the ESB with
a flexible number of significant bits. We also introduce a distribution
difference aligner (DDA) to quantitatively align the distributions between the
full-precision weight or activation values and quantized values. Consequently,
ESB is suitable for various bell-shaped distributions of weights and activation
of DNNs, thus maintaining a high inference accuracy. Benefitting from fewer
significant bits of quantized values, ESB can reduce the multiplication
complexity. We implement ESB as an accelerator and quantitatively evaluate its
efficiency on FPGAs. Extensive experimental results illustrate that ESB
quantization consistently outperforms state-of-the-art methods and achieves
average accuracy improvements of 4.78%, 1.92%, and 3.56% over AlexNet,
ResNet18, and MobileNetV2, respectively. Furthermore, ESB as an accelerator can
achieve 10.95 GOPS peak performance of 1k LUTs without DSPs on the Xilinx
ZCU102 FPGA platform. Compared with CPU, GPU, and state-of-the-art accelerators
on FPGAs, the ESB accelerator can improve the energy efficiency by up to 65x,
11x, and 26x, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Time Alignment using Lip Images for Frame-based Electrolaryngeal Voice Conversion. (arXiv:2109.03551v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03551">
<div class="article-summary-box-inner">
<span><p>Voice conversion (VC) is an effective approach to electrolaryngeal (EL)
speech enhancement, a task that aims to improve the quality of the artificial
voice from an electrolarynx device. In frame-based VC methods, time alignment
needs to be performed prior to model training, and the dynamic time warping
(DTW) algorithm is widely adopted to compute the best time alignment between
each utterance pair. The validity is based on the assumption that the same
phonemes of the speakers have similar features and can be mapped by measuring a
pre-defined distance between speech frames of the source and the target.
However, the special characteristics of the EL speech can break the assumption,
resulting in a sub-optimal DTW alignment. In this work, we propose to use lip
images for time alignment, as we assume that the lip movements of laryngectomee
remain normal compared to healthy people. We investigate two naive lip
representations and distance metrics, and experimental results demonstrate that
the proposed method can significantly outperform the audio-only alignment in
terms of objective and subjective evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LiDARTouch: Monocular metric depth estimation with a few-beam LiDAR. (arXiv:2109.03569v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03569">
<div class="article-summary-box-inner">
<span><p>Vision-based depth estimation is a key feature in autonomous systems, which
often relies on a single camera or several independent ones. In such a
monocular setup, dense depth is obtained with either additional input from one
or several expensive LiDARs, e.g., with 64 beams, or camera-only methods, which
suffer from scale-ambiguity and infinite-depth problems. In this paper, we
propose a new alternative of densely estimating metric depth by combining a
monocular camera with a light-weight LiDAR, e.g., with 4 beams, typical of
today's automotive-grade mass-produced laser scanners. Inspired by recent
self-supervised methods, we introduce a novel framework, called LiDARTouch, to
estimate dense depth maps from monocular images with the help of ``touches'' of
LiDAR, i.e., without the need for dense ground-truth depth. In our setup, the
minimal LiDAR input contributes on three different levels: as an additional
model's input, in a self-supervised LiDAR reconstruction objective function,
and to estimate changes of pose (a key component of self-supervised depth
estimation architectures). Our LiDARTouch framework achieves new state of the
art in self-supervised depth estimation on the KITTI dataset, thus supporting
our choices of integrating the very sparse LiDAR signal with other visual
features. Moreover, we show that the use of a few-beam LiDAR alleviates scale
ambiguity and infinite-depth issues that camera-only methods suffer from. We
also demonstrate that methods from the fully-supervised depth-completion
literature can be adapted to a self-supervised regime with a minimal LiDAR
signal.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deriving Explanation of Deep Visual Saliency Models. (arXiv:2109.03575v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03575">
<div class="article-summary-box-inner">
<span><p>Deep neural networks have shown their profound impact on achieving human
level performance in visual saliency prediction. However, it is still unclear
how they learn the task and what it means in terms of understanding human
visual system. In this work, we develop a technique to derive explainable
saliency models from their corresponding deep neural architecture based
saliency models by applying human perception theories and the conventional
concepts of saliency. This technique helps us understand the learning pattern
of the deep network at its intermediate layers through their activation maps.
Initially, we consider two state-of-the-art deep saliency models, namely UNISAL
and MSI-Net for our interpretation. We use a set of biologically plausible
log-gabor filters for identifying and reconstructing the activation maps of
them using our explainable saliency model. The final saliency map is generated
using these reconstructed activation maps. We also build our own deep saliency
model named cross-concatenated multi-scale residual block based network
(CMRNet) for saliency prediction. Then, we evaluate and compare the performance
of the explainable models derived from UNISAL, MSI-Net and CMRNet on three
benchmark datasets with other state-of-the-art methods. Hence, we propose that
this approach of explainability can be applied to any deep visual saliency
model for interpretation which makes it a generic one.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Matching in the Dark: A Dataset for Matching Image Pairs of Low-light Scenes. (arXiv:2109.03585v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03585">
<div class="article-summary-box-inner">
<span><p>This paper considers matching images of low-light scenes, aiming to widen the
frontier of SfM and visual SLAM applications. Recent image sensors can record
the brightness of scenes with more than eight-bit precision, available in their
RAW-format image. We are interested in making full use of such high-precision
information to match extremely low-light scene images that conventional methods
cannot handle. For extreme low-light scenes, even if some of their brightness
information exists in the RAW format images' low bits, the standard raw image
processing on cameras fails to utilize them properly. As was recently shown by
Chen et al., CNNs can learn to produce images with a natural appearance from
such RAW-format images. To consider if and how well we can utilize such
information stored in RAW-format images for image matching, we have created a
new dataset named MID (matching in the dark). Using it, we experimentally
evaluated combinations of eight image-enhancing methods and eleven image
matching methods consisting of classical/neural local descriptors and
classical/neural initial point-matching methods. The results show the advantage
of using the RAW-format images and the strengths and weaknesses of the above
component methods. They also imply there is room for further research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identification of Social-Media Platform of Videos through the Use of Shared Features. (arXiv:2109.03598v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03598">
<div class="article-summary-box-inner">
<span><p>Videos have become a powerful tool for spreading illegal content such as
military propaganda, revenge porn, or bullying through social networks. To
counter these illegal activities, it has become essential to try new methods to
verify the origin of videos from these platforms. However, collecting datasets
large enough to train neural networks for this task has become difficult
because of the privacy regulations that have been enacted in recent years. To
mitigate this limitation, in this work we propose two different solutions based
on transfer learning and multitask learning to determine whether a video has
been uploaded from or downloaded to a specific social platform through the use
of shared features with images trained on the same task. By transferring
features from the shallowest to the deepest levels of the network from the
image task to videos, we measure the amount of information shared between these
two tasks. Then, we introduce a model based on multitask learning, which learns
from both tasks simultaneously. The promising experimental results show, in
particular, the effectiveness of the multitask approach. According to our
knowledge, this is the first work that addresses the problem of social media
platform identification of videos through the use of shared features.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tactile Image-to-Image Disentanglement of Contact Geometry from Motion-Induced Shear. (arXiv:2109.03615v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03615">
<div class="article-summary-box-inner">
<span><p>Robotic touch, particularly when using soft optical tactile sensors, suffers
from distortion caused by motion-dependent shear. The manner in which the
sensor contacts a stimulus is entangled with the tactile information about the
geometry of the stimulus. In this work, we propose a supervised convolutional
deep neural network model that learns to disentangle, in the latent space, the
components of sensor deformations caused by contact geometry from those due to
sliding-induced shear. The approach is validated by reconstructing unsheared
tactile images from sheared images and showing they match unsheared tactile
images collected with no sliding motion. In addition, the unsheared tactile
images give a faithful reconstruction of the contact geometry that is not
possible from the sheared data, and robust estimation of the contact pose that
can be used for servo control sliding around various 2D shapes. Finally, the
contact geometry reconstruction in conjunction with servo control sliding were
used for faithful full object reconstruction of various 2D shapes. The methods
have broad applicability to deep learning models for robots with a
shear-sensitive sense of touch.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Local-Global Contextual Adaptation for Fully End-to-End Bottom-Up Human Pose Estimation. (arXiv:2109.03622v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03622">
<div class="article-summary-box-inner">
<span><p>This paper presents a method of learning Local-GlObal Contextual Adaptation
for fully end-to-end and fast bottom-up human Pose estimation, dubbed as
LOGO-CAP. It is built on the conceptually simple center-offset formulation that
lacks inaccuracy for pose estimation. When revisiting the bottom-up human pose
estimation with the thought of "thinking, fast and slow" by D. Kahneman, we
introduce a "slow keypointer" to remedy the lack of sufficient accuracy of the
"fast keypointer". In learning the "slow keypointer", the proposed LOGO-CAP
lifts the initial "fast" keypoints by offset predictions to keypoint expansion
maps (KEMs) to counter their uncertainty in two modules. Firstly, the local
KEMs (e.g., 11x11) are extracted from a low-dimensional feature map. A proposed
convolutional message passing module learns to "re-focus" the local KEMs to the
keypoint attraction maps (KAMs) by accounting for the structured output
prediction nature of human pose estimation, which is directly supervised by the
object keypoint similarity (OKS) loss in training. Secondly, the global KEMs
are extracted, with a sufficiently large region-of-interest (e.g., 97x97), from
the keypoint heatmaps that are computed by a direct map-to-map regression.
Then, a local-global contextual adaptation module is proposed to convolve the
global KEMs using the learned KAMs as the kernels. This convolution can be
understood as the learnable offsets guided deformable and dynamic convolution
in a pose-sensitive way. The proposed method is end-to-end trainable with near
real-time inference speed, obtaining state-of-the-art performance on the COCO
keypoint benchmark for bottom-up human pose estimation. With the COCO trained
model, our LOGO-CAP also outperforms prior arts by a large margin on the
challenging OCHuman dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Recognizing Occluded Faces in the Wild. (arXiv:2109.03672v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03672">
<div class="article-summary-box-inner">
<span><p>Facial appearance variations due to occlusion has been one of the main
challenges for face recognition systems. To facilitate further research in this
area, it is necessary and important to have occluded face datasets collected
from real-world, as synthetically generated occluded faces cannot represent the
nature of the problem. In this paper, we present the Real World Occluded Faces
(ROF) dataset, that contains faces with both upper face occlusion, due to
sunglasses, and lower face occlusion, due to masks. We propose two evaluation
protocols for this dataset. Benchmark experiments on the dataset have shown
that no matter how powerful the deep face representation models are, their
performance degrades significantly when they are tested on real-world occluded
faces. It is observed that the performance drop is far less when the models are
tested on synthetically generated occluded faces. The ROF dataset and the
associated evaluation protocols are publicly available at the following link
https://github.com/ekremerakin/RealWorldOccludedFaces.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised clothing change adaptive person ReID. (arXiv:2109.03702v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03702">
<div class="article-summary-box-inner">
<span><p>Clothing changes and lack of data labels are both crucial challenges in
person ReID. For the former challenge, people may occur multiple times at
different locations wearing different clothing. However, most of the current
person ReID research works focus on the benchmarks in which a person's clothing
is kept the same all the time. For the last challenge, some researchers try to
make model learn information from a labeled dataset as a source to an unlabeled
dataset. Whereas purely unsupervised training is less used. In this paper, we
aim to solve both problems at the same time. We design a novel unsupervised
model, Sync-Person-Cloud ReID, to solve the unsupervised clothing change person
ReID problem. We developer a purely unsupervised clothing change person ReID
pipeline with person sync augmentation operation and same person feature
restriction. The person sync augmentation is to supply additional same person
resources. These same person's resources can be used as part supervised input
by same person feature restriction. The extensive experiments on clothing
change ReID datasets show the out-performance of our methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Disentangling Alzheimer's disease neurodegeneration from typical brain aging using machine learning. (arXiv:2109.03723v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03723">
<div class="article-summary-box-inner">
<span><p>Neuroimaging biomarkers that distinguish between typical brain aging and
Alzheimer's disease (AD) are valuable for determining how much each contributes
to cognitive decline. Machine learning models can derive multi-variate brain
change patterns related to the two processes, including the SPARE-AD (Spatial
Patterns of Atrophy for Recognition of Alzheimer's Disease) and SPARE-BA (of
Brain Aging) investigated herein. However, substantial overlap between brain
regions affected in the two processes confounds measuring them independently.
We present a methodology toward disentangling the two. T1-weighted MRI images
of 4,054 participants (48-95 years) with AD, mild cognitive impairment (MCI),
or cognitively normal (CN) diagnoses from the iSTAGING (Imaging-based
coordinate SysTem for AGIng and NeurodeGenerative diseases) consortium were
analyzed. First, a subset of AD patients and CN adults were selected based
purely on clinical diagnoses to train SPARE-BA1 (regression of age using CN
individuals) and SPARE-AD1 (classification of CN versus AD). Second, analogous
groups were selected based on clinical and molecular markers to train SPARE-BA2
and SPARE-AD2: amyloid-positive (A+) AD continuum group (consisting of A+AD,
A+MCI, and A+ and tau-positive CN individuals) and amyloid-negative (A-) CN
group. Finally, the combined group of the AD continuum and A-/CN individuals
was used to train SPARE-BA3, with the intention to estimate brain age
regardless of AD-related brain changes. Disentangled SPARE models derived brain
patterns that were more specific to the two types of the brain changes.
Correlation between the SPARE-BA and SPARE-AD was significantly reduced.
Correlation of disentangled SPARE-AD was non-inferior to the molecular
measurements and to the number of APOE4 alleles, but was less to AD-related
psychometric test scores, suggesting contribution of advanced brain aging to
these scores.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Axial multi-layer perceptron architecture for automatic segmentation of choroid plexus in multiple sclerosis. (arXiv:2109.03778v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03778">
<div class="article-summary-box-inner">
<span><p>Choroid plexuses (CP) are structures of the ventricles of the brain which
produce most of the cerebrospinal fluid (CSF). Several postmortem and in vivo
studies have pointed towards their role in the inflammatory process in multiple
sclerosis (MS). Automatic segmentation of CP from MRI thus has high value for
studying their characteristics in large cohorts of patients. To the best of our
knowledge, the only freely available tool for CP segmentation is FreeSurfer but
its accuracy for this specific structure is poor. In this paper, we propose to
automatically segment CP from non-contrast enhanced T1-weighted MRI. To that
end, we introduce a new model called "Axial-MLP" based on an assembly of Axial
multi-layer perceptrons (MLPs). This is inspired by recent works which showed
that the self-attention layers of Transformers can be replaced with MLPs. This
approach is systematically compared with a standard 3D U-Net, nnU-Net,
Freesurfer and FastSurfer. For our experiments, we make use of a dataset of 141
subjects (44 controls and 97 patients with MS). We show that all the tested
deep learning (DL) methods outperform FreeSurfer (Dice around 0.7 for DL vs
0.33 for FreeSurfer). Axial-MLP is competitive with U-Nets even though it is
slightly less accurate. The conclusions of our paper are two-fold: 1) the
studied deep learning methods could be useful tools to study CP in large
cohorts of MS patients; 2)~Axial-MLP is a potentially viable alternative to
convolutional neural networks for such tasks, although it could benefit from
further improvements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Egocentric View Hand Action Recognition by Leveraging Hand Surface and Hand Grasp Type. (arXiv:2109.03783v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03783">
<div class="article-summary-box-inner">
<span><p>We introduce a multi-stage framework that uses mean curvature on a hand
surface and focuses on learning interaction between hand and object by
analyzing hand grasp type for hand action recognition in egocentric videos. The
proposed method does not require 3D information of objects including 6D object
poses which are difficult to annotate for learning an object's behavior while
it interacts with hands. Instead, the framework synthesizes the mean curvature
of the hand mesh model to encode the hand surface geometry in 3D space.
Additionally, our method learns the hand grasp type which is highly correlated
with the hand action. From our experiment, we notice that using hand grasp type
and mean curvature of hand increases the performance of the hand action
recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FIDNet: LiDAR Point Cloud Semantic Segmentation with Fully Interpolation Decoding. (arXiv:2109.03787v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03787">
<div class="article-summary-box-inner">
<span><p>Projecting the point cloud on the 2D spherical range image transforms the
LiDAR semantic segmentation to a 2D segmentation task on the range image.
However, the LiDAR range image is still naturally different from the regular 2D
RGB image; for example, each position on the range image encodes the unique
geometry information. In this paper, we propose a new projection-based LiDAR
semantic segmentation pipeline that consists of a novel network structure and
an efficient post-processing step. In our network structure, we design a FID
(fully interpolation decoding) module that directly upsamples the
multi-resolution feature maps using bilinear interpolation. Inspired by the 3D
distance interpolation used in PointNet++, we argue this FID module is a 2D
version distance interpolation on $(\theta, \phi)$ space. As a parameter-free
decoding module, the FID largely reduces the model complexity by maintaining
good performance. Besides the network structure, we empirically find that our
model predictions have clear boundaries between different semantic classes.
This makes us rethink whether the widely used K-nearest-neighbor
post-processing is still necessary for our pipeline. Then, we realize the
many-to-one mapping causes the blurring effect that some points are mapped into
the same pixel and share the same label. Therefore, we propose to process those
occluded points by assigning the nearest predicted label to them. This NLA
(nearest label assignment) post-processing step shows a better performance than
KNN with faster inference speed in the ablation study. On the SemanticKITTI
dataset, our pipeline achieves the best performance among all projection-based
methods with $64 \times 2048$ resolution and all point-wise solutions. With a
ResNet-34 as the backbone, both the training and testing of our model can be
finished on a single RTX 2080 Ti with 11G memory. The code is released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Few-Shot Learning PoC Ultrasound COVID-19 Diagnostic System. (arXiv:2109.03793v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03793">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel ultrasound imaging point-of-care (PoC) COVID-19
diagnostic system. The adaptive visual diagnostics utilize few-shot learning
(FSL) to generate encoded disease state models that are stored and classified
using a dictionary of knowns. The novel vocabulary based feature processing of
the pipeline adapts the knowledge of a pretrained deep neural network to
compress the ultrasound images into discrimative descriptions. The
computational efficiency of the FSL approach enables high diagnostic deep
learning performance in PoC settings, where training data is limited and the
annotation process is not strictly controlled. The algorithm performance is
evaluated on the open source COVID-19 POCUS Dataset to validate the system's
ability to distinguish COVID-19, pneumonia, and healthy disease states. The
results of the empirical analyses demonstrate the appropriate efficiency and
accuracy for scalable PoC use. The code for this work will be made publicly
available on GitHub upon acceptance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Digitize-PID: Automatic Digitization of Piping and Instrumentation Diagrams. (arXiv:2109.03794v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03794">
<div class="article-summary-box-inner">
<span><p>Digitization of scanned Piping and Instrumentation diagrams(P&amp;ID), widely
used in manufacturing or mechanical industries such as oil and gas over several
decades, has become a critical bottleneck in dynamic inventory management and
creation of smart P&amp;IDs that are compatible with the latest CAD tools.
Historically, P&amp;ID sheets have been manually generated at the design stage,
before being scanned and stored as PDFs. Current digitization initiatives
involve manual processing and are consequently very time consuming, labour
intensive and error-prone.Thanks to advances in image processing, machine and
deep learning techniques there are emerging works on P&amp;ID digitization.
However, existing solutions face several challenges owing to the variation in
the scale, size and noise in the P&amp;IDs, sheer complexity and crowdedness within
drawings, domain knowledge required to interpret the drawings. This motivates
our current solution called Digitize-PID which comprises of an end-to-end
pipeline for detection of core components from P&amp;IDs like pipes, symbols and
textual information, followed by their association with each other and
eventually, the validation and correction of output data based on inherent
domain knowledge. A novel and efficient kernel-based line detection and a
two-step method for detection of complex symbols based on a fine-grained deep
recognition technique is presented in the paper. In addition, we have created
an annotated synthetic dataset, Dataset-P&amp;ID, of 500 P&amp;IDs by incorporating
different types of noise and complex symbols which is made available for public
use (currently there exists no public P&amp;ID dataset). We evaluate our proposed
method on this synthetic dataset and a real-world anonymized private dataset of
12 P&amp;ID sheets. Results show that Digitize-PID outperforms the existing
state-of-the-art for P&amp;ID digitization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Panoptic nuScenes: A Large-Scale Benchmark for LiDAR Panoptic Segmentation and Tracking. (arXiv:2109.03805v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03805">
<div class="article-summary-box-inner">
<span><p>Panoptic scene understanding and tracking of dynamic agents are essential for
robots and automated vehicles to navigate in urban environments. As LiDARs
provide accurate illumination-independent geometric depictions of the scene,
performing these tasks using LiDAR point clouds provides reliable predictions.
However, existing datasets lack diversity in the type of urban scenes and have
a limited number of dynamic object instances which hinders both learning of
these tasks as well as credible benchmarking of the developed methods. In this
paper, we introduce the large-scale Panoptic nuScenes benchmark dataset that
extends our popular nuScenes dataset with point-wise groundtruth annotations
for semantic segmentation, panoptic segmentation, and panoptic tracking tasks.
To facilitate comparison, we provide several strong baselines for each of these
tasks on our proposed dataset. Moreover, we analyze the drawbacks of the
existing metrics for the panoptic tracking problem and propose a novel
instance-centric metric that addresses the concerns. We present extensive
experiments that demonstrate the utility of Panoptic nuScenes compared to
existing datasets and make the online evaluation server available at
\url{nuScenes.org}. We believe that this extension will accelerate the research
of novel methods for scene understanding of dynamic urban environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaled ReLU Matters for Training Vision Transformers. (arXiv:2109.03810v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03810">
<div class="article-summary-box-inner">
<span><p>Vision transformers (ViTs) have been an alternative design paradigm to
convolutional neural networks (CNNs). However, the training of ViTs is much
harder than CNNs, as it is sensitive to the training parameters, such as
learning rate, optimizer and warmup epoch. The reasons for training difficulty
are empirically analysed in ~\cite{xiao2021early}, and the authors conjecture
that the issue lies with the \textit{patchify-stem} of ViT models and propose
that early convolutions help transformers see better. In this paper, we further
investigate this problem and extend the above conclusion: only early
convolutions do not help for stable training, but the scaled ReLU operation in
the \textit{convolutional stem} (\textit{conv-stem}) matters. We verify, both
theoretically and empirically, that scaled ReLU in \textit{conv-stem} not only
improves training stabilization, but also increases the diversity of patch
tokens, thus boosting peak performance with a large margin via adding few
parameters and flops. In addition, extensive experiments are conducted to
demonstrate that previous ViTs are far from being well trained, further showing
that ViTs have great potential to be a better substitute of CNNs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">fastMRI+: Clinical Pathology Annotations for Knee and Brain Fully Sampled Multi-Coil MRI Data. (arXiv:2109.03812v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03812">
<div class="article-summary-box-inner">
<span><p>Improving speed and image quality of Magnetic Resonance Imaging (MRI) via
novel reconstruction approaches remains one of the highest impact applications
for deep learning in medical imaging. The fastMRI dataset, unique in that it
contains large volumes of raw MRI data, has enabled significant advances in
accelerating MRI using deep learning-based reconstruction methods. While the
impact of the fastMRI dataset on the field of medical imaging is unquestioned,
the dataset currently lacks clinical expert pathology annotations, critical to
addressing clinically relevant reconstruction frameworks and exploring
important questions regarding rendering of specific pathology using such novel
approaches. This work introduces fastMRI+, which consists of 16154
subspecialist expert bounding box annotations and 13 study-level labels for 22
different pathology categories on the fastMRI knee dataset, and 7570
subspecialist expert bounding box annotations and 643 study-level labels for 30
different pathology categories for the fastMRI brain dataset. The fastMRI+
dataset is open access and aims to support further research and advancement of
medical imaging in MRI reconstruction and beyond.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Panoptic SegFormer. (arXiv:2109.03814v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03814">
<div class="article-summary-box-inner">
<span><p>We present Panoptic SegFormer, a general framework for end-to-end panoptic
segmentation with Transformers. The proposed method extends Deformable DETR
with a unified mask prediction workflow for both things and stuff, making the
panoptic segmentation pipeline concise and effective. With a ResNet-50
backbone, our method achieves 50.0\% PQ on the COCO test-dev split, surpassing
previous state-of-the-art methods by significant margins without bells and
whistles. Using a more powerful PVTv2-B5 backbone, Panoptic-SegFormer achieves
a new record of 54.1\%PQ and 54.4\% PQ on the COCO val and test-dev splits with
single scale input.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Adaptive Ensemble Learning. (arXiv:2003.07325v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.07325">
<div class="article-summary-box-inner">
<span><p>The problem of generalizing deep neural networks from multiple source domains
to a target one is studied under two settings: When unlabeled target data is
available, it is a multi-source unsupervised domain adaptation (UDA) problem,
otherwise a domain generalization (DG) problem. We propose a unified framework
termed domain adaptive ensemble learning (DAEL) to address both problems. A
DAEL model is composed of a CNN feature extractor shared across domains and
multiple classifier heads each trained to specialize in a particular source
domain. Each such classifier is an expert to its own domain and a non-expert to
others. DAEL aims to learn these experts collaboratively so that when forming
an ensemble, they can leverage complementary information from each other to be
more effective for an unseen target domain. To this end, each source domain is
used in turn as a pseudo-target-domain with its own expert providing
supervisory signal to the ensemble of non-experts learned from the other
sources. For unlabeled target data under the UDA setting where real expert does
not exist, DAEL uses pseudo-label to supervise the ensemble learning. Extensive
experiments on three multi-source UDA datasets and two DG datasets show that
DAEL improves the state of the art on both problems, often by significant
margins. The code is released at
\url{https://github.com/KaiyangZhou/Dassl.pytorch}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quasi-Dense Similarity Learning for Multiple Object Tracking. (arXiv:2006.06664v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.06664">
<div class="article-summary-box-inner">
<span><p>Similarity learning has been recognized as a crucial step for object
tracking. However, existing multiple object tracking methods only use sparse
ground truth matching as the training objective, while ignoring the majority of
the informative regions on the images. In this paper, we present Quasi-Dense
Similarity Learning, which densely samples hundreds of region proposals on a
pair of images for contrastive learning. We can directly combine this
similarity learning with existing detection methods to build Quasi-Dense
Tracking (QDTrack) without turning to displacement regression or motion priors.
We also find that the resulting distinctive feature space admits a simple
nearest neighbor search at the inference time. Despite its simplicity, QDTrack
outperforms all existing methods on MOT, BDD100K, Waymo, and TAO tracking
benchmarks. It achieves 68.7 MOTA at 20.3 FPS on MOT17 without using external
training data. Compared to methods with similar detectors, it boosts almost 10
points of MOTA and significantly decreases the number of ID switches on BDD100K
and Waymo datasets. Our code and trained models are available at
<a href="http://vis.xyz/pub/qdtrack.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self supervised contrastive learning for digital histopathology. (arXiv:2011.13971v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.13971">
<div class="article-summary-box-inner">
<span><p>Unsupervised learning has been a long-standing goal of machine learning and
is especially important for medical image analysis, where the learning can
compensate for the scarcity of labeled datasets. A promising subclass of
unsupervised learning is self-supervised learning, which aims to learn salient
features using the raw input as the learning signal. In this paper, we use a
contrastive self-supervised learning method called SimCLR that achieved
state-of-the-art results on natural-scene images and apply this method to
digital histopathology by collecting and pretraining on 57 histopathology
datasets without any labels. We find that combining multiple multi-organ
datasets with different types of staining and resolution properties improves
the quality of the learned features. Furthermore, we find using more images for
pretraining leads to a better performance in multiple downstream tasks. Linear
classifiers trained on top of the learned features show that networks
pretrained on digital histopathology datasets perform better than ImageNet
pretrained networks, boosting task performances by more than 28% in F1 scores
on average. These findings may also be useful when applying newer contrastive
techniques to histopathology data. Pretrained PyTorch models are made publicly
available at https://github.com/ozanciga/self-supervised-histopathology.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image-based Plant Disease Diagnosis with Unsupervised Anomaly Detection Based on Reconstructability of Colors. (arXiv:2011.14306v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.14306">
<div class="article-summary-box-inner">
<span><p>This paper proposes an unsupervised anomaly detection technique for
image-based plant disease diagnosis. The construction of large and publicly
available datasets containing labeled images of healthy and diseased crop
plants led to growing interest in computer vision techniques for automatic
plant disease diagnosis. Although supervised image classifiers based on deep
learning can be a powerful tool for plant disease diagnosis, they require a
huge amount of labeled data. The data mining technique of anomaly detection
includes unsupervised approaches that do not require rare samples for training
classifiers. We propose an unsupervised anomaly detection technique for
image-based plant disease diagnosis that is based on the reconstructability of
colors; a deep encoder-decoder network trained to reconstruct the colors of
\textit{healthy} plant images should fail to reconstruct colors of symptomatic
regions. Our proposed method includes a new image-based framework for plant
disease detection that utilizes a conditional adversarial network called
pix2pix and a new anomaly score based on CIEDE2000 color difference.
Experiments with PlantVillage dataset demonstrated the superiority of our
proposed method compared to an existing anomaly detector at identifying
diseased crop images in terms of accuracy, interpretability and computational
efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Improved Iterative Neural Network for High-Quality Image-Domain Material Decomposition in Dual-Energy CT. (arXiv:2012.01986v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.01986">
<div class="article-summary-box-inner">
<span><p>Dual-energy computed tomography (DECT) has been widely used in many
applications that need material decomposition. Image-domain methods directly
decompose material images from high- and low-energy attenuation images, and
thus, are susceptible to noise and artifacts on attenuation images. The purpose
of this study is to develop an improved iterative neural network (INN) for
high-quality image-domain material decomposition in DECT, and to study its
properties. We propose a new INN architecture for DECT material decomposition.
The proposed INN architecture uses distinct cross-material convolutional neural
network (CNN) in image refining modules, and uses image decomposition physics
in image reconstruction modules. The distinct cross-material CNN refiners
incorporate distinct encoding-decoding filters and cross-material model that
captures correlations between different materials. We study the distinct
cross-material CNN refiner with patch-based reformulation and tight-frame
condition. Numerical experiments with extended cardiactorso (XCAT) phantom and
clinical data show that the proposed INN significantly improves the image
quality over several image-domain material decomposition methods, including a
conventional model-based image decomposition (MBID) method using an
edge-preserving regularizer, a recent MBID method using pre-learned
material-wise sparsifying transforms, and a noniterative deep CNN method. Our
study with patch-based reformulations reveals that learned filters of distinct
cross-material CNN refiners can approximately satisfy the tight-frame
condition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large-scale Robust Deep AUC Maximization: A New Surrogate Loss and Empirical Studies on Medical Image Classification. (arXiv:2012.03173v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.03173">
<div class="article-summary-box-inner">
<span><p>Deep AUC Maximization (DAM) is a new paradigm for learning a deep neural
network by maximizing the AUC score of the model on a dataset. Most previous
works of AUC maximization focus on the perspective of optimization by designing
efficient stochastic algorithms, and studies on generalization performance of
large-scale DAM on difficult tasks are missing. In this work, we aim to make
DAM more practical for interesting real-world applications (e.g., medical image
classification). First, we propose a new margin-based min-max surrogate loss
function for the AUC score (named as AUC min-max-margin loss or simply AUC
margin loss for short). It is more robust than the commonly used AUC square
loss, while enjoying the same advantage in terms of large-scale stochastic
optimization. Second, we conduct extensive empirical studies of our DAM method
on four difficult medical image classification tasks, namely (i) classification
of chest x-ray images for identifying many threatening diseases, (ii)
classification of images of skin lesions for identifying melanoma, (iii)
classification of mammogram for breast cancer screening, and (iv)
classification of microscopic images for identifying tumor tissue. Our studies
demonstrate that the proposed DAM method improves the performance of optimizing
cross-entropy loss by a large margin, and also achieves better performance than
optimizing the existing AUC square loss on these medical image classification
tasks. Specifically, our DAM method has achieved the 1st place on Stanford
CheXpert competition on Aug. 31, 2020. To the best of our knowledge, this is
the first work that makes DAM succeed on large-scale medical image datasets. We
also conduct extensive ablation studies to demonstrate the advantages of the
new AUC margin loss over the AUC square loss on benchmark datasets. The
proposed method is implemented in our open-sourced library LibAUC
(www.libauc.org).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformers in Vision: A Survey. (arXiv:2101.01169v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.01169">
<div class="article-summary-box-inner">
<span><p>Astounding results from Transformer models on natural language tasks have
intrigued the vision community to study their application to computer vision
problems. Among their salient benefits, Transformers enable modeling long
dependencies between input sequence elements and support parallel processing of
sequence as compared to recurrent networks e.g., Long short-term memory (LSTM).
Different from convolutional networks, Transformers require minimal inductive
biases for their design and are naturally suited as set-functions. Furthermore,
the straightforward design of Transformers allows processing multiple
modalities (e.g., images, videos, text and speech) using similar processing
blocks and demonstrates excellent scalability to very large capacity networks
and huge datasets. These strengths have led to exciting progress on a number of
vision tasks using Transformer networks. This survey aims to provide a
comprehensive overview of the Transformer models in the computer vision
discipline. We start with an introduction to fundamental concepts behind the
success of Transformers i.e., self-attention, large-scale pre-training, and
bidirectional encoding. We then cover extensive applications of transformers in
vision including popular recognition tasks (e.g., image classification, object
detection, action recognition, and segmentation), generative modeling,
multi-modal tasks (e.g., visual-question answering, visual reasoning, and
visual grounding), video processing (e.g., activity recognition, video
forecasting), low-level vision (e.g., image super-resolution, image
enhancement, and colorization) and 3D analysis (e.g., point cloud
classification and segmentation). We compare the respective advantages and
limitations of popular techniques both in terms of architectural design and
their experimental value. Finally, we provide an analysis on open research
directions and possible future works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Salient Object Detection via Integrity Learning. (arXiv:2101.07663v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.07663">
<div class="article-summary-box-inner">
<span><p>Albeit current salient object detection (SOD) works have achieved fantastic
progress, they are cast into the shade when it comes to the integrity of the
predicted salient regions. We define the concept of integrity at both the micro
and macro level. Specifically, at the micro level, the model should highlight
all parts that belong to a certain salient object, while at the macro level,
the model needs to discover all salient objects from the given image scene. To
facilitate integrity learning for salient object detection, we design a novel
Integrity Cognition Network (ICON), which explores three important components
to learn strong integrity features. 1) Unlike the existing models that focus
more on feature discriminability, we introduce a diverse feature aggregation
(DFA) component to aggregate features with various receptive fields (i.e.,,
kernel shape and context) and increase the feature diversity. Such diversity is
the foundation for mining the integral salient objects. 2) Based on the DFA
features, we introduce the integrity channel enhancement (ICE) component with
the goal of enhancing feature channels that highlight the integral salient
objects at the macro level, while suppressing the other distracting ones. 3)
After extracting the enhanced features, the part-whole verification (PWV)
method is employed to determine whether the part and whole object features have
strong agreement. Such part-whole agreements can further improve the
micro-level integrity for each salient object. To demonstrate the effectiveness
of ICON, comprehensive experiments are conducted on seven challenging
benchmarks, where promising results are achieved.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DMN4: Few-shot Learning via Discriminative Mutual Nearest Neighbor Neural Network. (arXiv:2103.08160v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.08160">
<div class="article-summary-box-inner">
<span><p>Few-shot learning (FSL) aims to classify images under low-data regimes, where
the conventional pooled global feature is likely to lose useful local
characteristics. Recent work has achieved promising performances by using deep
descriptors. They generally take all deep descriptors from neural networks into
consideration while ignoring that some of them are useless in classification
due to their limited receptive field, e.g., task-irrelevant descriptors could
be misleading and multiple aggregative descriptors from background clutter
could even overwhelm the object's presence. In this paper, we argue that a
Mutual Nearest Neighbor (MNN) relation should be established to explicitly
select the query descriptors that are most relevant to each task and discard
less relevant ones from aggregative clutters in FSL. Specifically, we propose
Discriminative Mutual Nearest Neighbor Neural Network (DMN4) for FSL. Extensive
experiments demonstrate that our method outperforms the existing
state-of-the-arts on both fine-grained and generalized datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Suppress-and-Refine Framework for End-to-End 3D Object Detection. (arXiv:2103.10042v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.10042">
<div class="article-summary-box-inner">
<span><p>3D object detector based on Hough voting achieves great success and derives
many follow-up works. Despite constantly refreshing the detection accuracy,
these works suffer from handcrafted components used to eliminate redundant
boxes, and thus are non-end-to-end and time-consuming. In this work, we propose
a suppress-and-refine framework to remove these handcrafted components. To
fully utilize full-resolution information and achieve real-time speed, it
directly consumes feature points and redundant 3D proposals. Specifically, it
first suppresses noisy 3D feature points and then feeds them to 3D proposals
for the following RoI-aware refinement. With the gating mechanism to build fine
proposal features and the self-attention mechanism to model relationships, our
method can produce high-quality predictions with a small computation budget in
an end-to-end manner. To this end, we present the first fully end-to-end 3D
detector, SRDet, on the basis of VoteNet. It achieves state-of-the-art
performance on the challenging ScanNetV2 and SUN RGB-D datasets with the
fastest speed ever. Our code will be available at
https://github.com/ZJULearning/SRDet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling-up Disentanglement for Image Translation. (arXiv:2103.14017v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.14017">
<div class="article-summary-box-inner">
<span><p>Image translation methods typically aim to manipulate a set of labeled
attributes (given as supervision at training time e.g. domain label) while
leaving the unlabeled attributes intact. Current methods achieve either: (i)
disentanglement, which exhibits low visual fidelity and can only be satisfied
where the attributes are perfectly uncorrelated. (ii) visually-plausible
translations, which are clearly not disentangled. In this work, we propose
OverLORD, a single framework for disentangling labeled and unlabeled attributes
as well as synthesizing high-fidelity images, which is composed of two stages;
(i) Disentanglement: Learning disentangled representations with latent
optimization. Differently from previous approaches, we do not rely on
adversarial training or any architectural biases. (ii) Synthesis: Training
feed-forward encoders for inferring the learned attributes and tuning the
generator in an adversarial manner to increase the perceptual quality. When the
labeled and unlabeled attributes are correlated, we model an additional
representation that accounts for the correlated attributes and improves
disentanglement. We highlight that our flexible framework covers multiple
settings as disentangling labeled attributes, pose and appearance, localized
concepts, and shape and texture. We present significantly better
disentanglement with higher translation quality and greater output diversity
than state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lidar Point Cloud Guided Monocular 3D Object Detection. (arXiv:2104.09035v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.09035">
<div class="article-summary-box-inner">
<span><p>Monocular 3D detection currently struggles with extremely lower detection
rates compared to LiDAR-based methods. The poor accuracy is mainly caused by
the absence of accurate location cues due to the ill-posed nature of monocular
imagery. LiDAR point clouds, which provide precise spatial measurement, can
offer beneficial information for the training of monocular methods. To make use
of LiDAR point clouds, prior works project them to form depth map labels,
subsequently training a dense depth estimator to extract explicit location
features. This indirect and complicated way introduces intermediate products,
i.e., depth map predictions, taking much computation costs as well as leading
to suboptimal performances. In this paper, we propose LPCG (LiDAR point cloud
guided monocular 3D object detection), which is a general framework for guiding
the training of monocular 3D detectors with LiDAR point clouds. Specifically,
we use LiDAR point clouds to generate pseudo labels, allowing monocular 3D
detectors to benefit from easy-collected massive unlabeled data. LPCG works
well under both supervised and unsupervised setups. Thanks to a general design,
LPCG can be plugged into any monocular 3D detector, significantly boosting the
performance. As a result, we take the first place on KITTI monocular 3D/BEV
(bird's-eye-view) detection benchmark with a considerable margin. The code will
be made publicly available soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CAGAN: Text-To-Image Generation with Combined Attention GANs. (arXiv:2104.12663v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12663">
<div class="article-summary-box-inner">
<span><p>Generating images according to natural language descriptions is a challenging
task. Prior research has mainly focused to enhance the quality of generation by
investigating the use of spatial attention and/or textual attention thereby
neglecting the relationship between channels. In this work, we propose the
Combined Attention Generative Adversarial Network (CAGAN) to generate
photo-realistic images according to textual descriptions. The proposed CAGAN
utilises two attention models: word attention to draw different sub-regions
conditioned on related words; and squeeze-and-excitation attention to capture
non-linear interaction among channels. With spectral normalisation to stabilise
training, our proposed CAGAN improves the state of the art on the IS and FID on
the CUB dataset and the FID on the more challenging COCO dataset. Furthermore,
we demonstrate that judging a model by a single evaluation metric can be
misleading by developing an additional model adding local self-attention which
scores a higher IS, outperforming the state of the art on the CUB dataset, but
generates unrealistic images through feature repetition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Information Extraction by Character-Level Embedding and Multi-Stage Attentional U-Net. (arXiv:2106.00952v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.00952">
<div class="article-summary-box-inner">
<span><p>Information extraction from document images has received a lot of attention
recently, due to the need for digitizing a large volume of unstructured
documents such as invoices, receipts, bank transfers, etc. In this paper, we
propose a novel deep learning architecture for end-to-end information
extraction on the 2D character-grid embedding of the document, namely the
\textit{Multi-Stage Attentional U-Net}. To effectively capture the textual and
spatial relations between 2D elements, our model leverages a specialized
multi-stage encoder-decoders design, in conjunction with efficient uses of the
self-attention mechanism and the box convolution. Experimental results on
different datasets show that our model outperforms the baseline U-Net
architecture by a large margin while using 40\% fewer parameters. Moreover, it
also significantly improved the baseline in erroneous OCR and limited training
data scenario, thus becomes practical for real-world applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TSI: Temporal Saliency Integration for Video Action Recognition. (arXiv:2106.01088v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01088">
<div class="article-summary-box-inner">
<span><p>Efficient spatiotemporal modeling is an important yet challenging problem for
video action recognition. Existing state-of-the-art methods exploit motion
clues to assist in short-term temporal modeling through temporal difference
over consecutive frames. However, insignificant noises will be inevitably
introduced due to the camera movement. Besides, movements of different actions
can vary greatly. In this paper, we propose a Temporal Saliency Integration
(TSI) block, which mainly contains a Salient Motion Excitation (SME) module and
a Cross-scale Temporal Integration (CTI) module. Specifically, SME aims to
highlight the motion-sensitive area through local-global motion modeling, where
the saliency alignment and pyramidal feature difference are conducted
successively between neighboring frames to capture motion dynamics with less
noises caused by misaligned background. CTI is designed to perform multi-scale
temporal modeling through a group of separate 1D convolutions respectively.
Meanwhile, temporal interactions across different scales are integrated with
attention mechanism. Through these two modules, long short-term temporal
relationships can be encoded efficiently by introducing limited additional
parameters. Extensive experiments are conducted on several popular benchmarks
(i.e., Something-Something V1 &amp; V2, Kinetics-400, UCF-101, and HMDB-51), which
demonstrate the effectiveness and superiority of our proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised 3D Hand Pose Estimation from monocular RGB via Contrastive Learning. (arXiv:2106.05953v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05953">
<div class="article-summary-box-inner">
<span><p>Encouraged by the success of contrastive learning on image classification
tasks, we propose a new self-supervised method for the structured regression
task of 3D hand pose estimation. Contrastive learning makes use of unlabeled
data for the purpose of representation learning via a loss formulation that
encourages the learned feature representations to be invariant under any image
transformation. For 3D hand pose estimation, it too is desirable to have
invariance to appearance transformation such as color jitter. However, the task
requires equivariance under affine transformations, such as rotation and
translation. To address this issue, we propose an equivariant contrastive
objective and demonstrate its effectiveness in the context of 3D hand pose
estimation. We experimentally investigate the impact of invariant and
equivariant contrastive objectives and show that learning equivariant features
leads to better representations for the task of 3D hand pose estimation.
Furthermore, we show that standard ResNets with sufficient depth, trained on
additional unlabeled data, attain improvements of up to 14.5% in PA-EPE on
FreiHAND and thus achieves state-of-the-art performance without any task
specific, specialized architectures. Code and models are available at
https://ait.ethz.ch/projects/2021/PeCLR/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Plant Disease Detection Using Image Processing and Machine Learning. (arXiv:2106.10698v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.10698">
<div class="article-summary-box-inner">
<span><p>One of the important and tedious task in agricultural practices is the
detection of the disease on crops. It requires huge time as well as skilled
labor. This paper proposes a smart and efficient technique for detection of
crop disease which uses computer vision and machine learning techniques. The
proposed system is able to detect 20 different diseases of 5 common plants with
93% accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Built-in Elastic Transformations for Improved Robustness. (arXiv:2107.09391v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.09391">
<div class="article-summary-box-inner">
<span><p>We focus on building robustness in the convolutions of neural visual
classifiers, especially against natural perturbations like elastic
deformations, occlusions and Gaussian noise. Existing CNNs show outstanding
performance on clean images, but fail to tackle naturally occurring
perturbations. In this paper, we start from elastic perturbations, which
approximate (local) view-point changes of the object. We present
elastically-augmented convolutions (EAConv) by parameterizing filters as a
combination of fixed elastically-perturbed bases functions and trainable
weights for the purpose of integrating unseen viewpoints in the CNN. We show on
CIFAR-10 and STL-10 datasets that the general robustness of our method on
unseen occlusion, zoom, rotation, image cut and Gaussian perturbations
improves, while significantly improving the performance on clean images without
any data augmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ManiSkill: Generalizable Manipulation Skill Benchmark with Large-Scale Demonstrations. (arXiv:2107.14483v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.14483">
<div class="article-summary-box-inner">
<span><p>Object manipulation from 3D visual inputs poses many challenges on building
generalizable perception and policy models. However, 3D assets in existing
benchmarks mostly lack the diversity of 3D shapes that align with real-world
intra-class complexity in topology and geometry. Here we propose SAPIEN
Manipulation Skill Benchmark (ManiSkill) to benchmark manipulation skills over
diverse objects in a full-physics simulator. 3D assets in ManiSkill include
large intra-class topological and geometric variations. Tasks are carefully
chosen to cover distinct types of manipulation challenges. Latest progress in
3D vision also makes us believe that we should customize the benchmark so that
the challenge is inviting to researchers working on 3D deep learning. To this
end, we simulate a moving panoramic camera that returns ego-centric point
clouds or RGB-D images. In addition, we would like ManiSkill to serve a broad
set of researchers interested in manipulation research. Besides supporting the
learning of policies from interactions, we also support
learning-from-demonstrations (LfD) methods, by providing a large number of
high-quality demonstrations (~36,000 successful trajectories, ~1.5M point
cloud/RGB-D frames in total). We provide baselines using 3D deep learning and
LfD algorithms. All code of our benchmark (simulator, environment, SDK, and
baselines) is open-sourced, and a challenge facing interdisciplinary
researchers will be held based on the benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ParamCrop: Parametric Cubic Cropping for Video Contrastive Learning. (arXiv:2108.10501v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10501">
<div class="article-summary-box-inner">
<span><p>The central idea of contrastive learning is to discriminate between different
instances and force different views of the same instance to share the same
representation. To avoid trivial solutions, augmentation plays an important
role in generating different views, among which random cropping is shown to be
effective for the model to learn a strong and generalized representation.
Commonly used random crop operation keeps the difference between two views
statistically consistent along the training process. In this work, we challenge
this convention by showing that adaptively controlling the disparity between
two augmented views along the training process enhances the quality of the
learnt representation. Specifically, we present a parametric cubic cropping
operation, ParamCrop, for video contrastive learning, which automatically crops
a 3D cubic from the video by differentiable 3D affine transformations.
ParamCrop is trained simultaneously with the video backbone using an
adversarial objective and learns an optimal cropping strategy from the data.
The visualizations show that the center distance and the IoU between two
augmented views are adaptively controlled by ParamCrop and the learned change
in the disparity along the training process is beneficial to learning a strong
representation. Extensive ablation studies demonstrate the effectiveness of the
proposed ParamCrop on multiple contrastive learning frameworks and video
backbones. With ParamCrop, we improve the state-of-the-art performance on both
HMDB51 and UCF101 datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wanderlust: Online Continual Object Detection in the Real World. (arXiv:2108.11005v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11005">
<div class="article-summary-box-inner">
<span><p>Online continual learning from data streams in dynamic environments is a
critical direction in the computer vision field. However, realistic benchmarks
and fundamental studies in this line are still missing. To bridge the gap, we
present a new online continual object detection benchmark with an egocentric
video dataset, Objects Around Krishna (OAK). OAK adopts the KrishnaCAM videos,
an ego-centric video stream collected over nine months by a graduate student.
OAK provides exhaustive bounding box annotations of 80 video snippets (~17.5
hours) for 105 object categories in outdoor scenes. The emergence of new object
categories in our benchmark follows a pattern similar to what a single person
might see in their day-to-day life. The dataset also captures the natural
distribution shifts as the person travels to different places. These egocentric
long-running videos provide a realistic playground for continual learning
algorithms, especially in online embodied settings. We also introduce new
evaluation metrics to evaluate the model performance and catastrophic
forgetting and provide baseline studies for online continual object detection.
We believe this benchmark will pose new exciting challenges for learning from
non-stationary data in continual learning. The OAK dataset and the associated
benchmark are released at https://oakdata.github.io/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking the Aligned and Misaligned Features in One-stage Object Detection. (arXiv:2108.12176v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12176">
<div class="article-summary-box-inner">
<span><p>One-stage object detectors rely on a point feature to predict the detection
results. However, the point feature often lacks the information of the whole
object, thereby leading to a misalignment between the object and the point
feature. Meanwhile, the classification and regression tasks are sensitive to
different object regions, but their features are spatially aligned. Both of
these two problems hinder the detection performance. In order to solve these
two problems, we propose a simple and plug-in operator that can generate
aligned and disentangled features for each task, respectively, without breaking
the fully convolutional manner. By predicting two task-aware point sets that
are located in each sensitive region, the proposed operator can align the point
feature with the object and disentangle the two tasks from the spatial
dimension. We also reveal an interesting finding of the opposite effect of the
long-range skip connection for classification and regression. On the basis of
the Object-Aligned and Task-disentangled operator (OAT), we propose OAT-Net,
which explicitly exploits point-set features for accurate detection results.
Extensive experiments on the MS-COCO dataset show that OAT can consistently
boost different state-of-the-art one-stage detectors by $\sim$2 AP. Notably,
OAT-Net with Res2Net-101-DCN backbone achieves 53.7 AP on the COCO test-dev.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Digging into Uncertainty in Self-supervised Multi-view Stereo. (arXiv:2108.12966v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12966">
<div class="article-summary-box-inner">
<span><p>Self-supervised Multi-view stereo (MVS) with a pretext task of image
reconstruction has achieved significant progress recently. However, previous
methods are built upon intuitions, lacking comprehensive explanations about the
effectiveness of the pretext task in self-supervised MVS. To this end, we
propose to estimate epistemic uncertainty in self-supervised MVS, accounting
for what the model ignores. Specially, the limitations can be categorized into
two types: ambiguious supervision in foreground and invalid supervision in
background. To address these issues, we propose a novel Uncertainty reduction
Multi-view Stereo (UMVS) framework for self-supervised learning. To alleviate
ambiguous supervision in foreground, we involve extra correspondence prior with
a flow-depth consistency loss. The dense 2D correspondence of optical flows is
used to regularize the 3D stereo correspondence in MVS. To handle the invalid
supervision in background, we use Monte-Carlo Dropout to acquire the
uncertainty map and further filter the unreliable supervision signals on
invalid regions. Extensive experiments on DTU and Tank&amp;Temples benchmark show
that our U-MVS framework achieves the best performance among unsupervised MVS
methods, with competitive performance with its supervised opponents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anatomical-Guided Attention Enhances Unsupervised PET Image Denoising Performance. (arXiv:2109.00802v2 [physics.med-ph] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00802">
<div class="article-summary-box-inner">
<span><p>Although supervised convolutional neural networks (CNNs) often outperform
conventional alternatives for denoising positron emission tomography (PET)
images, they require many low- and high-quality reference PET image pairs.
Herein, we propose an unsupervised 3D PET image denoising method based on an
anatomical information-guided attention mechanism. The proposed magnetic
resonance-guided deep decoder (MR-GDD) utilizes the spatial details and
semantic features of MR-guidance image more effectively by introducing
encoder-decoder and deep decoder subnetworks. Moreover, the specific shapes and
patterns of the guidance image do not affect the denoised PET image, because
the guidance image is input to the network through an attention gate. In a
Monte Carlo simulation of [$^{18}$F]fluoro-2-deoxy-D-glucose (FDG), the
proposed method achieved the highest peak signal-to-noise ratio and structural
similarity (27.92 $\pm$ 0.44 dB/0.886 $\pm$ 0.007), as compared with Gaussian
filtering (26.68 $\pm$ 0.10 dB/0.807 $\pm$ 0.004), image guided filtering
(27.40 $\pm$ 0.11 dB/0.849 $\pm$ 0.003), deep image prior (DIP) (24.22 $\pm$
0.43 dB/0.737 $\pm$ 0.017), and MR-DIP (27.65 $\pm$ 0.42 dB/0.879 $\pm$ 0.007).
Furthermore, we experimentally visualized the behavior of the optimization
process, which is often unknown in unsupervised CNN-based restoration problems.
For preclinical (using [$^{18}$F]FDG and [$^{11}$C]raclopride) and clinical
(using [$^{18}$F]florbetapir) studies, the proposed method demonstrates
state-of-the-art denoising performance while retaining spatial resolution and
quantitative accuracy, despite using a common network architecture for various
noisy PET images with 1/10th of the full counts. These results suggest that the
proposed MR-GDD can reduce PET scan times and PET tracer doses considerably
without impacting patients.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GDP: Stabilized Neural Network Pruning via Gates with Differentiable Polarization. (arXiv:2109.02220v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02220">
<div class="article-summary-box-inner">
<span><p>Model compression techniques are recently gaining explosive attention for
obtaining efficient AI models for various real-time applications. Channel
pruning is one important compression strategy and is widely used in slimming
various DNNs. Previous gate-based or importance-based pruning methods aim to
remove channels whose importance is smallest. However, it remains unclear what
criteria the channel importance should be measured on, leading to various
channel selection heuristics. Some other sampling-based pruning methods deploy
sampling strategies to train sub-nets, which often causes the training
instability and the compressed model's degraded performance. In view of the
research gaps, we present a new module named Gates with Differentiable
Polarization (GDP), inspired by principled optimization ideas. GDP can be
plugged before convolutional layers without bells and whistles, to control the
on-and-off of each channel or whole layer block. During the training process,
the polarization effect will drive a subset of gates to smoothly decrease to
exact zero, while other gates gradually stay away from zero by a large margin.
When training terminates, those zero-gated channels can be painlessly removed,
while other non-zero gates can be absorbed into the succeeding convolution
kernel, causing completely no interruption to training nor damage to the
trained model. Experiments conducted over CIFAR-10 and ImageNet datasets show
that the proposed GDP algorithm achieves the state-of-the-art performance on
various benchmark DNNs at a broad range of pruning ratios. We also apply GDP to
DeepLabV3Plus-ResNet50 on the challenging Pascal VOC segmentation task, whose
test performance sees no drop (even slightly improved) with over 60% FLOPs
saving.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Point-Based Neural Rendering with Per-View Optimization. (arXiv:2109.02369v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02369">
<div class="article-summary-box-inner">
<span><p>There has recently been great interest in neural rendering methods. Some
approaches use 3D geometry reconstructed with Multi-View Stereo (MVS) but
cannot recover from the errors of this process, while others directly learn a
volumetric neural representation, but suffer from expensive training and
inference. We introduce a general approach that is initialized with MVS, but
allows further optimization of scene properties in the space of input views,
including depth and reprojected features, resulting in improved novel-view
synthesis. A key element of our approach is our new differentiable point-based
pipeline, based on bi-directional Elliptical Weighted Average splatting, a
probabilistic depth test and effective camera selection. We use these elements
together in our neural renderer, that outperforms all previous methods both in
quality and speed in almost all scenes we tested. Our pipeline can be applied
to multi-view harmonization and stylization in addition to novel-view
synthesis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Animation Transformer: Visual Correspondence via Segment Matching. (arXiv:2109.02614v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02614">
<div class="article-summary-box-inner">
<span><p>Visual correspondence is a fundamental building block on the way to building
assistive tools for hand-drawn animation. However, while a large body of work
has focused on learning visual correspondences at the pixel-level, few
approaches have emerged to learn correspondence at the level of line enclosures
(segments) that naturally occur in hand-drawn animation. Exploiting this
structure in animation has numerous benefits: it avoids the intractable memory
complexity of attending to individual pixels in high resolution images and
enables the use of real-world animation datasets that contain correspondence
information at the level of per-segment colors. To that end, we propose the
Animation Transformer (AnT) which uses a transformer-based architecture to
learn the spatial and visual relationships between segments across a sequence
of images. AnT enables practical ML-assisted colorization for professional
animation workflows and is publicly accessible as a creative tool in Cadmium.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GCsT: Graph Convolutional Skeleton Transformer for Action Recognition. (arXiv:2109.02860v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02860">
<div class="article-summary-box-inner">
<span><p>Graph convolutional networks (GCNs) achieve promising performance for
skeleton-based action recognition. However, in most GCN-based methods, the
spatial-temporal graph convolution is strictly restricted by the graph topology
while only captures the short-term temporal context, thus lacking the
flexibility of feature extraction. In this work, we present a novel
architecture, named Graph Convolutional skeleton Transformer (GCsT), which
addresses limitations in GCNs by introducing Transformer. Our GCsT employs all
the benefits of Transformer (i.e. dynamical attention and global context) while
keeps the advantages of GCNs (i.e. hierarchy and local topology structure). In
GCsT, the spatial-temporal GCN forces the capture of local dependencies while
Transformer dynamically extracts global spatial-temporal relationships.
Furthermore, the proposed GCsT shows stronger expressive capability by adding
additional information present in skeleton sequences. Incorporating the
Transformer allows that information to be introduced into the model almost
effortlessly. We validate the proposed GCsT by conducting extensive
experiments, which achieves the state-of-the-art performance on NTU RGB+D, NTU
RGB+D 120 and Northwestern-UCLA datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unpaired Adversarial Learning for Single Image Deraining with Rain-Space Contrastive Constraints. (arXiv:2109.02973v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02973">
<div class="article-summary-box-inner">
<span><p>Deep learning-based single image deraining (SID) with unpaired information is
of immense importance, as relying on paired synthetic data often limits their
generality and scalability in real-world applications. However, we noticed that
direct employ of unpaired adversarial learning and cycle-consistency
constraints in the SID task is insufficient to learn the underlying
relationship from rainy input to clean outputs, since the domain knowledge
between rainy and rain-free images is asymmetrical. To address such limitation,
we develop an effective unpaired SID method which explores mutual properties of
the unpaired exemplars by a contrastive learning manner in a GAN framework,
named as CDR-GAN. The proposed method mainly consists of two cooperative
branches: Bidirectional Translation Branch (BTB) and Contrastive Guidance
Branch (CGB). Specifically, BTB takes full advantage of the circulatory
architecture of adversarial consistency to exploit latent feature distributions
and guide transfer ability between two domains by equipping it with
bidirectional mapping. Simultaneously, CGB implicitly constrains the embeddings
of different exemplars in rain space by encouraging the similar feature
distributions closer while pushing the dissimilar further away, in order to
better help rain removal and image restoration. During training, we explore
several loss functions to further constrain the proposed CDR-GAN. Extensive
experiments show that our method performs favorably against existing unpaired
deraining approaches on both synthetic and real-world datasets, even
outperforms several fully-supervised or semi-supervised models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fair Comparison: Quantifying Variance in Resultsfor Fine-grained Visual Categorization. (arXiv:2109.03156v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03156">
<div class="article-summary-box-inner">
<span><p>For the task of image classification, researchers work arduously to develop
the next state-of-the-art (SOTA) model, each bench-marking their own
performance against that of their predecessors and of their peers.
Unfortunately, the metric used most frequently to describe a model's
performance, average categorization accuracy, is often used in isolation. As
the number of classes increases, such as in fine-grained visual categorization
(FGVC), the amount of information conveyed by average accuracy alone dwindles.
While its most glaring weakness is its failure to describe the model's
performance on a class-by-class basis, average accuracy also fails to describe
how performance may vary from one trained model of the same architecture, on
the same dataset, to another (both averaged across all categories and at the
per-class level). We first demonstrate the magnitude of these variations across
models and across class distributions based on attributes of the data,
comparing results on different visual domains and different per-class image
distributions, including long-tailed distributions and few-shot subsets. We
then analyze the impact various FGVC methods have on overall and per-class
variance. From this analysis, we both highlight the importance of reporting and
comparing methods based on information beyond overall accuracy, as well as
point out techniques that mitigate variance in FGVC results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NnFormer: Interleaved Transformer for Volumetric Segmentation. (arXiv:2109.03201v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03201">
<div class="article-summary-box-inner">
<span><p>Transformers, the default model of choices in natural language processing,
have drawn scant attention from the medical imaging community. Given the
ability to exploit long-term dependencies, transformers are promising to help
atypical convolutional neural networks (convnets) to overcome its inherent
shortcomings of spatial inductive bias. However, most of recently proposed
transformer-based segmentation approaches simply treated transformers as
assisted modules to help encode global context into convolutional
representations without investigating how to optimally combine self-attention
(i.e., the core of transformers) with convolution. To address this issue, in
this paper, we introduce nnFormer (i.e., Not-aNother transFormer), a powerful
segmentation model with an interleaved architecture based on empirical
combination of self-attention and convolution. In practice, nnFormer learns
volumetric representations from 3D local volumes. Compared to the naive
voxel-level self-attention implementation, such volume-based operations help to
reduce the computational complexity by approximate 98% and 99.5% on Synapse and
ACDC datasets, respectively. In comparison to prior-art network configurations,
nnFormer achieves tremendous improvements over previous transformer-based
methods on two commonly used datasets Synapse and ACDC. For instance, nnFormer
outperforms Swin-UNet by over 7 percents on Synapse. Even when compared to
nnUNet, currently the best performing fully-convolutional medical segmentation
network, nnFormer still provides slightly better performance on Synapse and
ACDC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Common Assumptions to Mitigate Racial Bias in Face Recognition Datasets. (arXiv:2109.03229v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03229">
<div class="article-summary-box-inner">
<span><p>Many existing works have made great strides towards reducing racial bias in
face recognition. However, most of these methods attempt to rectify bias that
manifests in models during training instead of directly addressing a major
source of the bias, the dataset itself. Exceptions to this are
BUPT-Balancedface/RFW and Fairface, but these works assume that primarily
training on a single race or not racially balancing the dataset are inherently
disadvantageous. We demonstrate that these assumptions are not necessarily
valid. In our experiments, training on only African faces induced less bias
than training on a balanced distribution of faces and distributions skewed to
include more African faces produced more equitable models. We additionally
notice that adding more images of existing identities to a dataset in place of
adding new identities can lead to accuracy boosts across racial categories. Our
code is available at
https://github.com/j-alex-hanson/rethinking-race-face-datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly supervised semantic segmentation of tomographic images in the diagnosis of stroke. (arXiv:2109.01887v1 [eess.IV] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01887">
<div class="article-summary-box-inner">
<span><p>This paper presents an automatic algorithm for the segmentation of areas
affected by an acute stroke on the non-contrast computed tomography brain
images. The proposed algorithm is designed for learning in a weakly supervised
scenario when some images are labeled accurately, and some images are labeled
inaccurately. Wrong labels appear as a result of inaccuracy made by a
radiologist in the process of manual annotation of computed tomography images.
We propose methods for solving the segmentation problem in the case of
inaccurately labeled training data. We use the U-Net neural network
architecture with several modifications. Experiments on real computed
tomography scans show that the proposed methods increase the segmentation
accuracy.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-09-09 23:02:08.158331950 UTC">2021-09-09 23:02:08 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.3</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>