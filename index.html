<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-03-01T01:30:00Z">03-01</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning English with Peppa Pig. (arXiv:2202.12917v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12917">
<div class="article-summary-box-inner">
<span><p>Attempts to computationally simulate the acquisition of spoken language via
grounding in perception have a long tradition but have gained momentum in the
past few years. Current neural approaches exploit associations between the
spoken and visual modality and learn to represent speech and visual data in a
joint vector space. A major unresolved issue from the point of ecological
validity is the training data, typically consisting of images or videos paired
with spoken descriptions of what is depicted. Such a setup guarantees an
unrealistically strong correlation between speech and the visual world. In the
real world the coupling between the linguistic and the visual is loose, and
often contains confounds in the form of correlations with non-semantic aspects
of the speech signal. The current study is a first step towards simulating a
naturalistic grounding scenario by using a dataset based on the children's
cartoon Peppa Pig. We train a simple bi-modal architecture on the portion of
the data consisting of naturalistic dialog between characters, and evaluate on
segments containing descriptive narrations. Despite the weak and confounded
signal in this training data our model succeeds at learning aspects of the
visual semantics of spoken language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ASSIST: Towards Label Noise-Robust Dialogue State Tracking. (arXiv:2202.13024v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13024">
<div class="article-summary-box-inner">
<span><p>The MultiWOZ 2.0 dataset has greatly boosted the research on dialogue state
tracking (DST). However, substantial noise has been discovered in its state
annotations. Such noise brings about huge challenges for training DST models
robustly. Although several refined versions, including MultiWOZ 2.1-2.4, have
been published recently, there are still lots of noisy labels, especially in
the training set. Besides, it is costly to rectify all the problematic
annotations. In this paper, instead of improving the annotation quality
further, we propose a general framework, named ASSIST (lAbel noiSe-robuSt
dIalogue State Tracking), to train DST models robustly from noisy labels.
ASSIST first generates pseudo labels for each sample in the training set by
using an auxiliary model trained on a small clean dataset, then puts the
generated pseudo labels and vanilla noisy labels together to train the primary
model. We show the validity of ASSIST theoretically. Experimental results also
demonstrate that ASSIST improves the joint goal accuracy of DST by up to
$28.16\%$ on the initial version MultiWOZ 2.0 and $8.41\%$ on the latest
version MultiWOZ 2.4, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AugESC: Large-scale Data Augmentation for Emotional Support Conversation with Pre-trained Language Models. (arXiv:2202.13047v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13047">
<div class="article-summary-box-inner">
<span><p>Crowd-sourcing is commonly adopted for dialog data collection. However, it is
highly costly and time-consuming, and the collected data is limited in scale
and topic coverage. In this paper, aiming to generate emotional support
conversations, we propose exploiting large-scale pre-trained language models
for data augmentation, and provide key findings in our pilot exploration. Our
adopted approach leverages the 6B-parameter GPT-J model and utilizes publicly
available dialog posts to trigger conversations on various topics. Then we
construct AugESC, a machine-augmented dataset for emotional support
conversation. It is two orders of magnitude larger than the original ESConv
dataset in scale, covers more diverse topics, and is shown to be of high
quality by human evaluation. Lastly, we demonstrate with interactive evaluation
that AugESC can further enhance dialog models tuned on ESConv to handle various
conversation topics and to provide significantly more effective emotional
support.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Identification of Toxic Code Reviews: How Far Can We Go?. (arXiv:2202.13056v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13056">
<div class="article-summary-box-inner">
<span><p>Toxic conversations during software development interactions may have serious
repercussions on a Free and Open Source Software (FOSS) development project.
For example, victims of toxic conversations may become afraid to express
themselves, therefore get demotivated, and may eventually leave the project.
Automated filtering of toxic conversations may help a FOSS community to
maintain healthy interactions among its members. However, off-the-shelf
toxicity detectors perform poorly on Software Engineering (SE) dataset, such as
one curated from code review comments. To encounter this challenge, we present
ToxiCR, a supervised learning-based toxicity identification tool for code
review interactions. ToxiCR includes a choice to select one of the ten
supervised learning algorithms, an option to select text vectorization
techniques, five mandatory and three optional SE domain specific processing
steps, and a large scale labeled dataset of 19,571 code review comments. With
our rigorous evaluation of the models with various combinations of
preprocessing steps and vectorization techniques, we have identified the best
combination for our dataset that boosts 95.8% accuracy and 88.9% F1 score.
ToxiCR significantly outperforms existing toxicity detectors on our dataset. We
have released our dataset, pretrained models, evaluation results, and source
code publicly available at: https://github.com/WSU-SEAL/ToxiCR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bi-directional Joint Neural Networks for Intent Classification and Slot Filling. (arXiv:2202.13079v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13079">
<div class="article-summary-box-inner">
<span><p>Intent classification and slot filling are two critical tasks for natural
language understanding. Traditionally the two tasks proceeded independently.
However, more recently joint models for intent classification and slot filling
have achieved state-of-the-art performance, and have proved that there exists a
strong relationship between the two tasks. In this paper, we propose a
bi-directional joint model for intent classification and slot filling, which
includes a multi-stage hierarchical process via BERT and bi-directional joint
natural language understanding mechanisms, including intent2slot and
slot2intent, to obtain mutual performance enhancement between intent
classification and slot filling. The evaluations show that our model achieves
state-of-the-art results on intent classification accuracy, slot filling F1,
and significantly improves sentence-level semantic frame accuracy when applied
to publicly available benchmark datasets, ATIS (88.6%) and SNIPS (92.8%).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Level Contrastive Learning for Cross-Lingual Alignment. (arXiv:2202.13083v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13083">
<div class="article-summary-box-inner">
<span><p>Cross-language pre-trained models such as multilingual BERT (mBERT) have
achieved significant performance in various cross-lingual downstream NLP tasks.
This paper proposes a multi-level contrastive learning (ML-CTL) framework to
further improve the cross-lingual ability of pre-trained models. The proposed
method uses translated parallel data to encourage the model to generate similar
semantic embeddings for different languages. However, unlike the sentence-level
alignment used in most previous studies, in this paper, we explicitly integrate
the word-level information of each pair of parallel sentences into contrastive
learning. Moreover, cross-zero noise contrastive estimation (CZ-NCE) loss is
proposed to alleviate the impact of the floating-point error in the training
process with a small batch size. The proposed method significantly improves the
cross-lingual transfer ability of our basic model (mBERT) and outperforms on
multiple zero-shot cross-lingual downstream tasks compared to the same-size
models in the Xtreme benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Impact of Negative Samples of Contrastive Learning: A Case Study of Sentence Embeddin. (arXiv:2202.13093v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13093">
<div class="article-summary-box-inner">
<span><p>Contrastive learning is emerging as a powerful technique for extracting
knowledge from unlabeled data. This technique requires a balanced mixture of
two ingredients: positive (similar) and negative (dissimilar) samples. This is
typically achieved by maintaining a queue of negative samples during training.
Prior works in the area typically uses a fixed-length negative sample queue,
but how the negative sample size affects the model performance remains unclear.
The opaque impact of the number of negative samples on performance when
employing contrastive learning aroused our in-depth exploration. This paper
presents a momentum contrastive learning model with negative sample queue for
sentence embedding, namely MoCoSE. We add the prediction layer to the online
branch to make the model asymmetric and together with EMA update mechanism of
the target branch to prevent model from collapsing. We define a maximum
traceable distance metric, through which we learn to what extent the text
contrastive learning benefits from the historical information of negative
samples. Our experiments find that the best results are obtained when the
maximum traceable distance is at a certain range, demonstrating that there is
an optimal range of historical information for a negative sample queue. We
evaluate the proposed unsupervised MoCoSE on the semantic text similarity (STS)
task and obtain an average Spearman's correlation of $77.27\%$. Source code is
available at https://github.com/xbdxwyh/mocose
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Supervision: Enabling Generalization over Output Spaces. (arXiv:2202.13100v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13100">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose Semantic Supervision (SemSup) - a unified paradigm
for training classifiers that generalize over output spaces. In contrast to
standard classification, which treats classes as discrete symbols, SemSup
represents them as dense vector features obtained from descriptions of classes
(e.g., "The cat is a small carnivorous mammal"). This allows the output space
to be unbounded (in the space of descriptions) and enables models to generalize
both over unseen inputs and unseen outputs (e.g. "The aardvark is a nocturnal
burrowing mammal with long ears"). Specifically, SemSup enables four types of
generalization, to -- (1) unseen class descriptions, (2) unseen classes, (3)
unseen super-classes, and (4) unseen tasks. Through experiments on four
classification datasets across two variants (multi-class and multi-label), two
input modalities (text and images), and two output description modalities (text
and JSON), we show that our SemSup models significantly outperform standard
supervised models and existing models that leverage word embeddings over class
names. For instance, our model outperforms baselines by 40% and 20% precision
points on unseen descriptions and classes, respectively, on a news
categorization dataset (RCV1). SemSup can serve as a pathway for scaling neural
models to large unbounded output spaces and enabling better generalization and
model reuse for unseen tasks and domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QuoteR: A Benchmark of Quote Recommendation for Writing. (arXiv:2202.13145v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13145">
<div class="article-summary-box-inner">
<span><p>It is very common to use quotations (quotes) to make our writings more
elegant or convincing. To help people find appropriate quotes more efficiently,
the task of quote recommendation is presented, aiming to recommend quotes that
fit the current context of writing. There have been various quote
recommendation approaches, but they are evaluated on different unpublished
datasets. To facilitate the research on this task, we build a large and fully
open quote recommendation dataset called QuoteR, which comprises three parts
including English, standard Chinese and classical Chinese. Any part of it is
larger than previous unpublished counterparts. We conduct an extensive
evaluation of existing quote recommendation methods on QuoteR. Furthermore, we
propose a new quote recommendation model that significantly outperforms
previous methods on all three parts of QuoteR. All the code and data of this
paper are available at https://github.com/thunlp/QuoteR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COMPASS: a Creative Support System that Alerts Novelists to the Unnoticed Missing Contents. (arXiv:2202.13151v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13151">
<div class="article-summary-box-inner">
<span><p>When humans write, they may unintentionally omit some information.
Complementing the omitted information using a computer is helpful in providing
writing support. Recently, in the field of story understanding and generation,
story completion (SC) was proposed to generate the missing parts of an
incomplete story. Although its applicability is limited because it requires
that the user have prior knowledge of the missing part of a story, missing
position prediction (MPP) can be used to compensate for this problem. MPP aims
to predict the position of the missing part, but the prerequisite knowledge
that "one sentence is missing" is still required. In this study, we propose
Variable Number MPP (VN-MPP), a new MPP task that removes this restriction;
that is, the task to predict multiple missing sentences or to judge whether
there are no missing sentences in the first place. We also propose two methods
for this new MPP task. Furthermore, based on the novel task and methods, we
developed a creative writing support system, COMPASS. The results of a user
experiment involving professional creators who write texts in Japanese confirm
the efficacy and utility of the developed system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Integrating Text Inputs For Training and Adapting RNN Transducer ASR Models. (arXiv:2202.13155v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13155">
<div class="article-summary-box-inner">
<span><p>Compared to hybrid automatic speech recognition (ASR) systems that use a
modular architecture in which each component can be independently adapted to a
new domain, recent end-to-end (E2E) ASR system are harder to customize due to
their all-neural monolithic construction. In this paper, we propose a novel
text representation and training framework for E2E ASR models. With this
approach, we show that a trained RNN Transducer (RNN-T) model's internal LM
component can be effectively adapted with text-only data. An RNN-T model
trained using both speech and text inputs improves over a baseline model
trained on just speech with close to 13% word error rate (WER) reduction on the
Switchboard and CallHome test sets of the NIST Hub5 2000 evaluation. The
usefulness of the proposed approach is further demonstrated by customizing this
general purpose RNN-T model to three separate datasets. We observe 20-45%
relative word error rate (WER) reduction in these settings with this novel LM
style customization technique using only unpaired text data from the new
domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Systematic Evaluation of Large Language Models of Code. (arXiv:2202.13169v1 [cs.PL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13169">
<div class="article-summary-box-inner">
<span><p>Large language models (LMs) of code have recently shown tremendous promise in
completing code and synthesizing code from natural language descriptions.
However, the current state-of-the-art code LMs (e.g., Codex (Chen et al.,
2021)) are not publicly available, leaving many questions about their model and
data design decisions. We aim to fill in some of these blanks through a
systematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo,
GPT-NeoX-20B, and CodeParrot, across various programming languages. Although
Codex itself is not open-source, we find that existing open-source models do
achieve close results in some programming languages, although targeted mainly
for natural language modeling. We further identify an important missing piece
in the form of a large open-source model trained exclusively on a multi-lingual
corpus of code. We release a new model, PolyCoder, with 2.7B parameters based
on the GPT-2 architecture, which was trained on 249GB of code across 12
programming languages on a single machine. In the C programming language,
PolyCoder outperforms all models including Codex. Our trained models are
open-source and publicly available at https://github.com/VHellendoorn/Code-LMs,
which enables future research and application in this area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BioADAPT-MRC: Adversarial Learning-based Domain Adaptation Improves Biomedical Machine Reading Comprehension Task. (arXiv:2202.13174v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13174">
<div class="article-summary-box-inner">
<span><p>Motivation: Biomedical machine reading comprehension (biomedical-MRC) aims to
comprehend complex biomedical narratives and assist healthcare professionals in
retrieving information from them. The high performance of modern neural
network-based MRC systems depends on high-quality, large-scale, human-annotated
training datasets. In the biomedical domain, a crucial challenge in creating
such datasets is the requirement for domain knowledge, inducing the scarcity of
labeled data and the need for transfer learning from the labeled
general-purpose (source) domain to the biomedical (target) domain. However,
there is a discrepancy in marginal distributions between the general-purpose
and biomedical domains due to the variances in topics. Therefore,
direct-transferring of learned representations from a model trained on a
general-purpose domain to the biomedical domain can hurt the model's
performance.
</p>
<p>Results: We present an adversarial learning-based domain adaptation framework
for the biomedical machine reading comprehension task (BioADAPT-MRC), a neural
network-based method to address the discrepancies in the marginal distributions
between the general and biomedical domain datasets. BioADAPT-MRC relaxes the
need for generating pseudo labels for training a well-performing biomedical-MRC
model. We extensively evaluate the performance of BioADAPT-MRC by comparing it
with the best existing methods on three widely used benchmark biomedical-MRC
datasets -- BioASQ-7b, BioASQ-8b, and BioASQ-9b. Our results suggest that
without using any synthetic or human-annotated data from the biomedical domain,
BioADAPT-MRC can achieve state-of-the-art performance on these datasets.
</p>
<p>Availability: BioADAPT-MRC is freely available as an open-source project
at\\https://github.com/mmahbub/BioADAPT-MRC
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Generative Model for Relation Extraction and Classification. (arXiv:2202.13229v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13229">
<div class="article-summary-box-inner">
<span><p>Relation extraction (RE) is an important information extraction task which
provides essential information to many NLP applications such as knowledge base
population and question answering. In this paper, we present a novel generative
model for relation extraction and classification (which we call GREC), where RE
is modeled as a sequence-to-sequence generation task. We explore various
encoding representations for the source and target sequences, and design
effective schemes that enable GREC to achieve state-of-the-art performance on
three benchmark RE datasets. In addition, we introduce negative sampling and
decoding scaling techniques which provide a flexible tool to tune the precision
and recall performance of the model. Our approach can be extended to extract
all relation triples from a sentence in one pass. Although the one-pass
approach incurs certain performance loss, it is much more computationally
efficient.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controllable Natural Language Generation with Contrastive Prefixes. (arXiv:2202.13257v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13257">
<div class="article-summary-box-inner">
<span><p>To guide the generation of large pretrained language models (LM), previous
work has focused on directly fine-tuning the language model or utilizing an
attribute discriminator. In this work, we propose a novel lightweight framework
for controllable GPT2 generation, which utilizes a set of small
attribute-specific vectors, called prefixes, to steer natural language
generation. Different from prefix-tuning, where each prefix is trained
independently, we take the relationship among prefixes into consideration and
train multiple prefixes simultaneously. We propose a novel supervised method
and also an unsupervised method to train the prefixes for single-aspect control
while the combination of these two methods can achieve multi-aspect control.
Experimental results on both single-aspect and multi-aspect control show that
our methods can guide generation towards the desired attributes while keeping
high linguistic quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OCR Improves Machine Translation for Low-Resource Languages. (arXiv:2202.13274v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13274">
<div class="article-summary-box-inner">
<span><p>We aim to investigate the performance of current OCR systems on low resource
languages and low resource scripts. We introduce and make publicly available a
novel benchmark, \textsc{OCR4MT}, consisting of real and synthetic data,
enriched with noise, for 60 low-resource languages in low resource scripts. We
evaluate state-of-the-art OCR systems on our benchmark and analyse most common
errors. We show that OCR monolingual data is a valuable resource that can
increase performance of Machine Translation models, when used in
backtranslation. We then perform an ablation study to investigate how OCR
errors impact Machine Translation performance and determine what is the minimum
level of OCR quality needed for the monolingual data to be useful for Machine
Translation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning the Beauty in Songs: Neural Singing Voice Beautifier. (arXiv:2202.13277v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13277">
<div class="article-summary-box-inner">
<span><p>We are interested in a novel task, singing voice beautifying (SVB). Given the
singing voice of an amateur singer, SVB aims to improve the intonation and
vocal tone of the voice, while keeping the content and vocal timbre. Current
automatic pitch correction techniques are immature, and most of them are
restricted to intonation but ignore the overall aesthetic quality. Hence, we
introduce Neural Singing Voice Beautifier (NSVB), the first generative model to
solve the SVB task, which adopts a conditional variational autoencoder as the
backbone and learns the latent representations of vocal tone. In NSVB, we
propose a novel time-warping approach for pitch correction: Shape-Aware Dynamic
Time Warping (SADTW), which ameliorates the robustness of existing time-warping
approaches, to synchronize the amateur recording with the template pitch curve.
Furthermore, we propose a latent-mapping algorithm in the latent space to
convert the amateur vocal tone to the professional one. To achieve this, we
also propose a new dataset containing parallel singing recordings of both
amateur and professional versions. Extensive experiments on both Chinese and
English songs demonstrate the effectiveness of our methods in terms of both
objective and subjective metrics. Audio samples are available
at~\url{https://neuralsvb.github.io}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Subgraph Retrieval Enhanced Model for Multi-hop Knowledge Base Question Answering. (arXiv:2202.13296v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13296">
<div class="article-summary-box-inner">
<span><p>Recent works on knowledge base question answering (KBQA) retrieve subgraphs
for easier reasoning. A desired subgraph is crucial as a small one may exclude
the answer but a large one might introduce more noises. However, the existing
retrieval is either heuristic or interwoven with the reasoning, causing
reasoning on the partial subgraphs, which increases the reasoning bias when the
intermediate supervision is missing. This paper proposes a trainable subgraph
retriever (SR) decoupled from the subsequent reasoning process, which enables a
plug-and-play framework to enhance any subgraph-oriented KBQA model. Extensive
experiments demonstrate SR achieves significantly better retrieval and QA
performance than existing retrieval methods. Via weakly supervised pre-training
as well as the end-to-end fine-tuning, SRl achieves new state-of-the-art
performance when combined with NSM, a subgraph-oriented reasoner, for
embedding-based KBQA methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HiCLRE: A Hierarchical Contrastive Learning Framework for Distantly Supervised Relation Extraction. (arXiv:2202.13352v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13352">
<div class="article-summary-box-inner">
<span><p>Distant supervision assumes that any sentence containing the same entity
pairs reflects identical relationships. Previous works of distantly supervised
relation extraction (DSRE) task generally focus on sentence-level or bag-level
de-noising techniques independently, neglecting the explicit interaction with
cross levels. In this paper, we propose a hierarchical contrastive learning
Framework for Distantly Supervised relation extraction (HiCLRE) to reduce noisy
sentences, which integrate the global structural information and local
fine-grained interaction. Specifically, we propose a three-level hierarchical
learning framework to interact with cross levels, generating the de-noising
context-aware representations via adapting the existing multi-head
self-attention, named Multi-Granularity Recontextualization. Meanwhile, pseudo
positive samples are also provided in the specific level for contrastive
learning via a dynamic gradient-based data augmentation strategy, named Dynamic
Gradient Adversarial Perturbation. Experiments demonstrate that HiCLRE
significantly outperforms strong baselines in various mainstream DSRE datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Variational Autoencoder with Disentanglement Priors for Low-Resource Task-Specific Natural Language Generation. (arXiv:2202.13363v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13363">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a variational autoencoder with disentanglement
priors, VAE-DPRIOR, for conditional natural language generation with none or a
handful of task-specific labeled examples. In order to improve compositional
generalization, our model performs disentangled representation learning by
introducing a prior for the latent content space and another prior for the
latent label space. We show both empirically and theoretically that the
conditional priors can already disentangle representations even without
specific regularizations as in the prior work. We can also sample diverse
content representations from the content space without accessing data of the
seen tasks, and fuse them with the representations of novel tasks for
generating diverse texts in the low-resource settings. Our extensive
experiments demonstrate the superior performance of our model over competitive
baselines in terms of i) data augmentation in continuous zero/few-shot
learning, and ii) text style transfer in both zero/few-shot settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simple but Effective Pluggable Entity Lookup Table for Pre-trained Language Models. (arXiv:2202.13392v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13392">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (PLMs) cannot well recall rich factual knowledge
of entities exhibited in large-scale corpora, especially those rare entities.
In this paper, we propose to build a simple but effective Pluggable Entity
Lookup Table (PELT) on demand by aggregating the entity's output
representations of multiple occurrences in the corpora. PELT can be compatibly
plugged as inputs to infuse supplemental entity knowledge into PLMs. Compared
to previous knowledge-enhanced PLMs, PELT only requires 0.2%-5% pre-computation
with capability of acquiring knowledge from out-of-domain corpora for domain
adaptation scenario. The experiments on knowledge-related tasks demonstrate
that our method, PELT, can flexibly and effectively transfer entity knowledge
from related corpora into PLMs with different architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multimodal German Dataset for Automatic Lip Reading Systems and Transfer Learning. (arXiv:2202.13403v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13403">
<div class="article-summary-box-inner">
<span><p>Large datasets as required for deep learning of lip reading do not exist in
many languages. In this paper we present the dataset GLips (German Lips)
consisting of 250,000 publicly available videos of the faces of speakers of the
Hessian Parliament, which was processed for word-level lip reading using an
automatic pipeline. The format is similar to that of the English language LRW
(Lip Reading in the Wild) dataset, with each video encoding one word of
interest in a context of 1.16 seconds duration, which yields compatibility for
studying transfer learning between both datasets. By training a deep neural
network, we investigate whether lip reading has language-independent features,
so that datasets of different languages can be used to improve lip reading
models. We demonstrate learning from scratch and show that transfer learning
from LRW to GLips and vice versa improves learning speed and performance, in
particular for the validation set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Candidate Retrieval with Entity Profile Generation for Wikidata Entity Linking. (arXiv:2202.13404v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13404">
<div class="article-summary-box-inner">
<span><p>Entity linking (EL) is the task of linking entity mentions in a document to
referent entities in a knowledge base (KB). Many previous studies focus on
Wikipedia-derived KBs. There is little work on EL over Wikidata, even though it
is the most extensive crowdsourced KB. The scale of Wikidata can open up many
new real-world applications, but its massive number of entities also makes EL
challenging. To effectively narrow down the search space, we propose a novel
candidate retrieval paradigm based on entity profiling. Wikidata entities and
their textual fields are first indexed into a text search engine (e.g.,
Elasticsearch). During inference, given a mention and its context, we use a
sequence-to-sequence (seq2seq) model to generate the profile of the target
entity, which consists of its title and description. We use the profile to
query the indexed search engine to retrieve candidate entities. Our approach
complements the traditional approach of using a Wikipedia anchor-text
dictionary, enabling us to further design a highly effective hybrid method for
candidate retrieval. Combined with a simple cross-attention reranker, our
complete EL framework achieves state-of-the-art results on three Wikidata-based
datasets and strong performance on TACKBP-2010.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Legal Argument Mining with Domain Pre-training and Neural Networks. (arXiv:2202.13457v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13457">
<div class="article-summary-box-inner">
<span><p>The contextual word embedding model, BERT, has proved its ability on
downstream tasks with limited quantities of annotated data. BERT and its
variants help to reduce the burden of complex annotation work in many
interdisciplinary research areas, for example, legal argument mining in digital
humanities. Argument mining aims to develop text analysis tools that can
automatically retrieve arguments and identify relationships between
argumentation clauses. Since argumentation is one of the key aspects of case
law, argument mining tools for legal texts are applicable to both academic and
non-academic legal research. Domain-specific BERT variants (pre-trained with
corpora from a particular background) have also achieved strong performance in
many tasks. To our knowledge, previous machine learning studies of argument
mining on judicial case law still heavily rely on statistical models. In this
paper, we provide a broad study of both classic and contextual embedding models
and their performance on practical case law from the European Court of Human
Rights (ECHR). During our study, we also explore a number of neural networks
when being combined with different embeddings. Our experiments provide a
comprehensive overview of a variety of approaches to the legal argument mining
task. We conclude that domain pre-trained transformer models have great
potential in this area, although traditional embeddings can also achieve strong
performance when combined with additional neural network layers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UCTopic: Unsupervised Contrastive Learning for Phrase Representations and Topic Mining. (arXiv:2202.13469v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13469">
<div class="article-summary-box-inner">
<span><p>High-quality phrase representations are essential to finding topics and
related terms in documents (a.k.a. topic mining). Existing phrase
representation learning methods either simply combine unigram representations
in a context-free manner or rely on extensive annotations to learn
context-aware knowledge. In this paper, we propose UCTopic, a novel
unsupervised contrastive learning framework for context-aware phrase
representations and topic mining. UCTopic is pretrained in a large scale to
distinguish if the contexts of two phrase mentions have the same semantics. The
key to pretraining is positive pair construction from our phrase-oriented
assumptions. However, we find traditional in-batch negatives cause performance
decay when finetuning on a dataset with small topic numbers. Hence, we propose
cluster-assisted contrastive learning(CCL) which largely reduces noisy
negatives by selecting negatives from clusters and further improves phrase
representations for topics accordingly. UCTopic outperforms the
state-of-the-art phrase representation model by 38.2% NMI in average on four
entity cluster-ing tasks. Comprehensive evaluation on topic mining shows that
UCTopic can extract coherent and diverse topical phrases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does Order Matter? An Empirical Study on Generating Multiple Keyphrases as a Sequence. (arXiv:1909.03590v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1909.03590">
<div class="article-summary-box-inner">
<span><p>Recently, concatenating multiple keyphrases as a target sequence has been
proposed as a new learning paradigm for keyphrase generation. Existing studies
concatenate target keyphrases in different orders but no study has examined the
effects of ordering on models' behavior. In this paper, we propose several
orderings for concatenation and inspect the important factors for training a
successful keyphrase generation model. By running comprehensive comparisons, we
observe one preferable ordering and summarize a number of empirical findings
and challenges, which can shed light on future research on this line of work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Study on Text-Independent Speaker Verification based on the GE2E Method. (arXiv:2011.04896v4 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.04896">
<div class="article-summary-box-inner">
<span><p>While many researchers in the speaker recognition area have started to
replace the former classical state-of-the-art methods with deep learning
techniques, some of the traditional i-vector-based methods are still
state-of-the-art in the context of text-independent speaker verification.
Google's Generalized End-to-End Loss for Speaker Verification (GE2E), a deep
learning-based technique using long short-term memory units, has recently
gained a lot of attention due to its speed in convergence and generalization.
In this study, we aim at further studying the GE2E method and comparing
different scenarios in order to investigate all of its aspects. Various
experiments including the effects of a random sampling of test and enrollment
utterances, test utterance duration, and the number of enrollment utterances
are discussed in this article. Furthermore, we compare the GE2E method with the
baseline state-of-the-art i-vector-based methods for text-independent speaker
verification and show that it outperforms them by resulting in lower error
rates while being end-to-end and requiring less training time for convergence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FFCI: A Framework for Interpretable Automatic Evaluation of Summarization. (arXiv:2011.13662v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.13662">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose FFCI, a framework for fine-grained summarization
evaluation that comprises four elements: faithfulness (degree of factual
consistency with the source), focus (precision of summary content relative to
the reference), coverage (recall of summary content relative to the reference),
and inter-sentential coherence (document fluency between adjacent sentences).
We construct a novel dataset for focus, coverage, and inter-sentential
coherence, and develop automatic methods for evaluating each of the four
dimensions of FFCI based on cross-comparison of evaluation metrics and
model-based evaluation methods, including question answering (QA) approaches,
semantic textual similarity (STS), next-sentence prediction (NSP), and scores
derived from 19 pre-trained language models. We then apply the developed
metrics in evaluating a broad range of summarization models across two
datasets, with some surprising findings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attention-based Clinical Note Summarization. (arXiv:2104.08942v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08942">
<div class="article-summary-box-inner">
<span><p>In recent years, the trend of deploying digital systems in numerous
industries has hiked. The health sector has observed an extensive adoption of
digital systems and services that generate significant medical records.
Electronic health records contain valuable information for prospective and
retrospective analysis that is often not entirely exploited because of the
complicated dense information storage. The crude purpose of condensing health
records is to select the information that holds most characteristics of the
original documents based on a reported disease. These summaries may boost
diagnosis and save a doctor's time during a saturated workload situation like
the COVID-19 pandemic. In this paper, we are applying a multi-head
attention-based mechanism to perform extractive summarization of meaningful
phrases on clinical notes. Our method finds major sentences for a summary by
correlating tokens, segments, and positional embeddings of sentences in a
clinical note. The model outputs attention scores that are statistically
transformed to extract critical phrases for visualization on the heat-mapping
tool and for human use.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When FastText Pays Attention: Efficient Estimation of Word Representations using Constrained Positional Weighting. (arXiv:2104.09691v6 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.09691">
<div class="article-summary-box-inner">
<span><p>In 2018, Mikolov et al. introduced the positional language model, which has
characteristics of attention-based neural machine translation models and which
achieved state-of-the-art performance on the intrinsic word analogy task.
However, the positional model is not practically fast and it has never been
evaluated on qualitative criteria or extrinsic tasks. We propose a constrained
positional model, which adapts the sparse attention mechanism from neural
machine translation to improve the speed of the positional model. We evaluate
the positional and constrained positional models on three novel qualitative
criteria and on language modeling. We show that the positional and constrained
positional models contain interpretable information about the grammatical
properties of words and outperform other shallow models on language modeling.
We also show that our constrained model outperforms the positional model on
language modeling and trains twice as fast.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NAAQA: A Neural Architecture for Acoustic Question Answering. (arXiv:2106.06147v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06147">
<div class="article-summary-box-inner">
<span><p>The goal of the Acoustic Question Answering (AQA) task is to answer a
free-form text question about the content of an acoustic scene. It was inspired
by the Visual Question Answering (VQA) task. In this paper, based on the
previously introduced CLEAR dataset, we propose a new benchmark for AQA, namely
CLEAR2, that emphasizes the specific challenges of acoustic inputs. These
include handling of variable duration scenes, and scenes built with elementary
sounds that differ between training and test set. We also introduce NAAQA, a
neural architecture that leverages specific properties of acoustic inputs. The
use of 1D convolutions in time and frequency to process 2D spectro-temporal
representations of acoustic content shows promising results and enables
reductions in model complexity. We show that time coordinate maps augment
temporal localization capabilities which enhance performance of the network by
~17 percentage points. On the other hand, frequency coordinate maps have little
influence on this task. NAAQA achieves 79.5% of accuracy on the AQA task with
~4 times fewer parameters than the previously explored VQA model. We evaluate
the perfomance of NAAQA on an independent data set reconstructed from DAQA. We
also test the addition of a MALiMo module in our model on both CLEAR2 and DAQA.
We provide a detailed analysis of the results for the different question types.
We release the code to produce CLEAR2 as well as NAAQA to foster research in
this newly emerging machine learning task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Packed Levitated Marker for Entity and Relation Extraction. (arXiv:2109.06067v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06067">
<div class="article-summary-box-inner">
<span><p>Recent entity and relation extraction works focus on investigating how to
obtain a better span representation from the pre-trained encoder. However, a
major limitation of existing works is that they ignore the interrelation
between spans (pairs). In this work, we propose a novel span representation
approach, named Packed Levitated Markers (PL-Marker), to consider the
interrelation between the spans (pairs) by strategically packing the markers in
the encoder. In particular, we propose a neighborhood-oriented packing
strategy, which considers the neighbor spans integrally to better model the
entity boundary information. Furthermore, for those more complicated span pair
classification tasks, we design a subject-oriented packing strategy, which
packs each subject and all its objects to model the interrelation between the
same-subject span pairs. The experimental results show that, with the enhanced
marker feature, our model advances baselines on six NER benchmarks, and obtains
a 4.1%-4.3% strict relation F1 improvement with higher speed over previous
state-of-the-art models on ACE04 and ACE05.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequential Reptile: Inter-Task Gradient Alignment for Multilingual Learning. (arXiv:2110.02600v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02600">
<div class="article-summary-box-inner">
<span><p>Multilingual models jointly pretrained on multiple languages have achieved
remarkable performance on various multilingual downstream tasks. Moreover,
models finetuned on a single monolingual downstream task have shown to
generalize to unseen languages. In this paper, we first show that it is crucial
for those tasks to align gradients between them in order to maximize knowledge
transfer while minimizing negative transfer. Despite its importance, the
existing methods for gradient alignment either have a completely different
purpose, ignore inter-task alignment, or aim to solve continual learning
problems in rather inefficient ways. As a result of the misaligned gradients
between tasks, the model suffers from severe negative transfer in the form of
catastrophic forgetting of the knowledge acquired from the pretraining. To
overcome the limitations, we propose a simple yet effective method that can
efficiently align gradients between tasks. Specifically, we perform each
inner-optimization by sequentially sampling batches from all the tasks,
followed by a Reptile outer update. Thanks to the gradients aligned between
tasks by our method, the model becomes less vulnerable to negative transfer and
catastrophic forgetting. We extensively validate our method on various
multi-task learning and zero-shot cross-lingual transfer tasks, where our
method largely outperforms all the relevant baselines we consider.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Linguistic Cues of Deception in a Multilingual April Fools' Day Context. (arXiv:2111.03913v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.03913">
<div class="article-summary-box-inner">
<span><p>In this work we consider the collection of deceptive April Fools' Day(AFD)
news articles as a useful addition in existing datasets for deception detection
tasks. Such collections have an established ground truth and are relatively
easy to construct across languages. As a result, we introduce a corpus that
includes diachronic AFD and normal articles from Greek newspapers and news
websites. On top of that, we build a rich linguistic feature set, and analyze
and compare its deception cues with the only AFD collection currently
available, which is in English. Following a current research thread, we also
discuss the individualism/collectivism dimension in deception with respect to
these two datasets. Lastly, we build classifiers by testing various monolingual
and crosslingual settings. The results showcase that AFD datasets can be
helpful in deception detection studies, and are in alignment with the
observations of other deception detection works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards More Robust Natural Language Understanding. (arXiv:2112.02992v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02992">
<div class="article-summary-box-inner">
<span><p>Natural Language Understanding (NLU) is a branch of Natural Language
Processing (NLP) that uses intelligent computer software to understand texts
that encode human knowledge. Recent years have witnessed notable progress
across various NLU tasks with deep learning techniques, especially with
pretrained language models. Besides proposing more advanced model
architectures, constructing more reliable and trustworthy datasets also plays a
huge role in improving NLU systems, without which it would be impossible to
train a decent NLU model. It's worth noting that the human ability of
understanding natural language is flexible and robust. On the contrary, most of
existing NLU systems fail to achieve desirable performance on out-of-domain
data or struggle on handling challenging items (e.g., inherently ambiguous
items, adversarial items) in the real world. Therefore, in order to have NLU
models understand human language more effectively, it is expected to prioritize
the study on robust natural language understanding. In this thesis, we deem
that NLU systems are consisting of two components: NLU models and NLU datasets.
As such, we argue that, to achieve robust NLU, the model architecture/training
and the dataset are equally important. Specifically, we will focus on three NLU
tasks to illustrate the robustness problem in different NLU tasks and our
contributions (i.e., novel models and new datasets) to help achieve more robust
natural language understanding. Moving forward, the ultimate goal for robust
natural language understanding is to build NLU models which can behave humanly.
That is, it's expected that robust NLU systems are capable to transfer the
knowledge from training corpus to unseen documents more reliably and survive
when encountering challenging items even if the system doesn't know a priori of
users' inputs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Pretrained Language Models Based Text Generation. (arXiv:2201.05273v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05273">
<div class="article-summary-box-inner">
<span><p>Text Generation aims to produce plausible and readable text in human language
from input data. The resurgence of deep learning has greatly advanced this
field by neural generation models, especially the paradigm of pretrained
language models (PLMs). Text generation based on PLMs is viewed as a promising
area in both academics and industry. In this survey, we begin with introducing
three key aspects of applying PLMs to text generation: 1) how to encode the
input as representations preserving input semantics which can be fused into
PLMs; 2) how to design an effective and performant PLM served as the generation
model; and 3) how to effectively optimize PLMs given the reference text and
ensure the generated text satisfying special text properties. Then, we figure
out some major challenges and solutions corresponding to the three key views.
Next, we present a summary of various useful resources and typical text
generation applications to work with PLMs. Finally, we highlight some of the
future research directions which will further improve these PLMs for text
generation. We strongly believe that this comprehensive survey paper will serve
as a valuable resource to learn the core concepts as well as stay up to date on
the latest developments in PLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Streaming Multi-Talker ASR with Token-Level Serialized Output Training. (arXiv:2202.00842v3 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00842">
<div class="article-summary-box-inner">
<span><p>This paper proposes a token-level serialized output training (t-SOT), a novel
framework for streaming multi-talker automatic speech recognition (ASR). Unlike
existing streaming multi-talker ASR models using multiple output layers, the
t-SOT model has only a single output layer that generates recognition tokens
(e.g., words, subwords) of multiple speakers in chronological order based on
their emission times. A special token that indicates the change of "virtual"
output channels is introduced to keep track of the overlapping utterances.
Compared to the prior streaming multi-talker ASR models, the t-SOT model has
the advantages of less inference cost and a simpler model architecture.
Moreover, in our experiments with LibriSpeechMix and LibriCSS datasets, the
t-SOT-based transformer transducer model achieves the state-of-the-art word
error rates by a significant margin to the prior results. For non-overlapping
speech, the t-SOT model is on par with a single-talker ASR model in terms of
both accuracy and computational cost, opening the door for deploying one model
for both single- and multi-talker scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BLUE at Memotion 2.0 2022: You have my Image, my Text and my Transformer. (arXiv:2202.07543v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07543">
<div class="article-summary-box-inner">
<span><p>Memes are prevalent on the internet and continue to grow and evolve alongside
our culture. An automatic understanding of memes propagating on the internet
can shed light on the general sentiment and cultural attitudes of people. In
this work, we present team BLUE's solution for the second edition of the
MEMOTION competition. We showcase two approaches for meme classification (i.e.
sentiment, humour, offensive, sarcasm and motivation levels) using a text-only
method using BERT, and a Multi-Modal-Multi-Task transformer network that
operates on both the meme image and its caption to output the final scores. In
both approaches, we leverage state-of-the-art pretrained models for text (BERT,
Sentence Transformer) and image processing (EfficientNetV4, CLIP). Through our
efforts, we obtain first place in task A, second place in task B and third
place in task C. In addition, our team obtained the highest average score for
all three tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reward Modeling for Mitigating Toxicity in Transformer-based Language Models. (arXiv:2202.09662v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09662">
<div class="article-summary-box-inner">
<span><p>Transformer-based language models are able to generate fluent text and be
efficiently adapted across various natural language generation tasks. However,
language models that are pretrained on large unlabeled web text corpora have
been shown to suffer from degenerating toxic content and social bias behaviors,
consequently hindering their safe deployment. Various detoxification methods
were proposed to mitigate the language model's toxicity; however, these methods
struggled to detoxify language models when conditioned on prompts that contain
specific social identities related to gender, race, or religion. In this study,
we propose Reinforce-Detoxify; A reinforcement learning-based method for
mitigating toxicity in language models. We address the challenge of safety in
language models and propose a new reward model that is able to detect toxic
content and mitigate unintended bias towards social identities in toxicity
prediction. The experiments demonstrate that the Reinforce-Detoxify method for
language model detoxification outperforms existing detoxification approaches in
automatic evaluation metrics, indicating the ability of our approach in
language model detoxification and less prone to unintended bias toward social
identities in generated content.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatically Generating Counterfactuals for Relation Exaction. (arXiv:2202.10668v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.10668">
<div class="article-summary-box-inner">
<span><p>The goal of relation extraction (RE) is to extract the semantic relations
between/among entities in the text. As a fundamental task in natural language
processing, it is crucial to ensure the robustness of RE models. Despite the
high accuracy current deep neural models have achieved in RE tasks, they are
easily affected by spurious correlations. One solution to this problem is to
train the model with counterfactually augmented data (CAD) such that it can
learn the causation rather than the confounding. However, no attempt has been
made on generating counterfactuals for RE tasks. In this paper, we formulate
the problem of automatically generating CAD for RE tasks from an entity-centric
viewpoint, and develop a novel approach to derive contextual counterfactuals
for entities. Specifically, we exploit two elementary topological properties,
i.e., the centrality and the shortest path, in syntactic and semantic
dependency graphs, to first identify and then intervene on the contextual
causal features for entities. We conduct a comprehensive evaluation on four RE
datasets by combining our proposed approach with a variety of backbone RE
models. The results demonstrate that our approach not only improves the
performance of the backbones, but also makes them more robust in the
out-of-domain test.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NU HLT at CMCL 2022 Shared Task: Multilingual and Crosslingual Prediction of Human Reading Behavior in Universal Language Space. (arXiv:2202.10855v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.10855">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a unified model that works for both multilingual
and crosslingual prediction of reading times of words in various languages. The
secret behind the success of this model is in the preprocessing step where all
words are transformed to their universal language representation via the
International Phonetic Alphabet (IPA). To the best of our knowledge, this is
the first study to favorable exploit this phonological property of language for
the two tasks. Various feature types were extracted covering basic frequencies,
n-grams, information theoretic, and psycholinguistically-motivated predictors
for model training. A finetuned Random Forest model obtained best performance
for both tasks with 3.8031 and 3.9065 MAE scores for mean first fixation
duration (FFDAvg) and mean total reading time (TRTAvg) respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Attention for Incomplete Utterance Rewriting. (arXiv:2202.12160v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12160">
<div class="article-summary-box-inner">
<span><p>Incomplete utterance rewriting (IUR) has recently become an essential task in
NLP, aiming to complement the incomplete utterance with sufficient context
information for comprehension. In this paper, we propose a novel method by
directly extracting the coreference and omission relationship from the
self-attention weight matrix of the transformer instead of word embeddings and
edit the original text accordingly to generate the complete utterance.
Benefiting from the rich information in the self-attention weight matrix, our
method achieved competitive results on public IUR datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">JParaCrawl v3.0: A Large-scale English-Japanese Parallel Corpus. (arXiv:2202.12607v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12607">
<div class="article-summary-box-inner">
<span><p>Most current machine translation models are mainly trained with parallel
corpora, and their translation accuracy largely depends on the quality and
quantity of the corpora. Although there are billions of parallel sentences for
a few language pairs, effectively dealing with most language pairs is difficult
due to a lack of publicly available parallel corpora. This paper creates a
large parallel corpus for English-Japanese, a language pair for which only
limited resources are available, compared to such resource-rich languages as
English-German. It introduces a new web-based English-Japanese parallel corpus
named JParaCrawl v3.0. Our new corpus contains more than 21 million unique
parallel sentence pairs, which is more than twice as many as the previous
JParaCrawl v2.0 corpus. Through experiments, we empirically show how our new
corpus boosts the accuracy of machine translation models on various domains.
The JParaCrawl v3.0 corpus will eventually be publicly available online for
research purposes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Robust Cybersecurity Topic Classification Tool. (arXiv:2109.02473v2 [cs.IR] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02473">
<div class="article-summary-box-inner">
<span><p>In this research, we use user defined labels from three internet text sources
(Reddit, Stackexchange, Arxiv) to train 21 different machine learning models
for the topic classification task of detecting cybersecurity discussions in
natural text. We analyze the false positive and false negative rates of each of
the 21 model's in a cross validation experiment. Then we present a
Cybersecurity Topic Classification (CTC) tool, which takes the majority vote of
the 21 trained machine learning models as the decision mechanism for detecting
cybersecurity related text. We also show that the majority vote mechanism of
the CTC tool provides lower false negative and false positive rates on average
than any of the 21 individual models. We show that the CTC tool is scalable to
the hundreds of thousands of documents with a wall clock time on the order of
hours.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Federated Distillation of Natural Language Understanding with Confident Sinkhorns. (arXiv:2110.02432v1 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02432">
<div class="article-summary-box-inner">
<span><p>Enhancing the user experience is an essential task for application service
providers. For instance, two users living wide apart may have different tastes
of food. A food recommender mobile application installed on an edge device
might want to learn from user feedback (reviews) to satisfy the client's needs
pertaining to distinct domains. Retrieving user data comes at the cost of
privacy while asking for model parameters trained on a user device becomes
space inefficient at a large scale. In this work, we propose an approach to
learn a central (global) model from the federation of (local) models which are
trained on user-devices, without disclosing the local data or model parameters
to the server. We propose a federation mechanism for the problems with natural
similarity metric between the labels which commonly appear in natural language
understanding (NLU) tasks. To learn the global model, the objective is to
minimize the optimal transport cost of the global model's predictions from the
confident sum of soft-targets assigned by local models. The confidence (a model
weighting scheme) score of a model is defined as the L2 distance of a model's
prediction from its probability bias. The method improves the global model's
performance over the baseline designed on three NLU tasks with intrinsic label
space semantics, i.e., fine-grained sentiment analysis, emotion recognition in
conversation, and natural language inference. We make our codes public at
https://github.com/declare-lab/sinkhorn-loss.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Refining Self-Supervised Learning in Imaging: Beyond Linear Metric. (arXiv:2202.12921v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12921">
<div class="article-summary-box-inner">
<span><p>We introduce in this paper a new statistical perspective, exploiting the
Jaccard similarity metric, as a measure-based metric to effectively invoke
non-linear features in the loss of self-supervised contrastive learning.
Specifically, our proposed metric may be interpreted as a dependence measure
between two adapted projections learned from the so-called latent
representations. This is in contrast to the cosine similarity measure in the
conventional contrastive learning model, which accounts for correlation
information. To the best of our knowledge, this effectively non-linearly fused
information embedded in the Jaccard similarity, is novel to self-supervision
learning with promising results. The proposed approach is compared to two
state-of-the-art self-supervised contrastive learning methods on three image
datasets. We not only demonstrate its amenable applicability in current ML
problems, but also its improved performance and training efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OptGAN: Optimizing and Interpreting the Latent Space of the Conditional Text-to-Image GANs. (arXiv:2202.12929v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12929">
<div class="article-summary-box-inner">
<span><p>Text-to-image generation intends to automatically produce a photo-realistic
image, conditioned on a textual description. It can be potentially employed in
the field of art creation, data augmentation, photo-editing, etc. Although many
efforts have been dedicated to this task, it remains particularly challenging
to generate believable, natural scenes. To facilitate the real-world
applications of text-to-image synthesis, we focus on studying the following
three issues: 1) How to ensure that generated samples are believable, realistic
or natural? 2) How to exploit the latent space of the generator to edit a
synthesized image? 3) How to improve the explainability of a text-to-image
generation framework? In this work, we constructed two novel data sets (i.e.,
the Good &amp; Bad bird and face data sets) consisting of successful as well as
unsuccessful generated samples, according to strict criteria. To effectively
and efficiently acquire high-quality images by increasing the probability of
generating Good latent codes, we use a dedicated Good/Bad classifier for
generated images. It is based on a pre-trained front end and fine-tuned on the
basis of the proposed Good &amp; Bad data set. After that, we present a novel
algorithm which identifies semantically-understandable directions in the latent
space of a conditional text-to-image GAN architecture by performing independent
component analysis on the pre-trained weight values of the generator.
Furthermore, we develop a background-flattening loss (BFL), to improve the
background appearance in the edited image. Subsequently, we introduce linear
interpolation analysis between pairs of keywords. This is extended into a
similar triangular `linguistic' interpolation in order to take a deep look into
what a text-to-image synthesis model has learned within the linguistic
embeddings. Our data set is available at
https://zenodo.org/record/6283798#.YhkN_ujMI2w.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-View Fusion Transformer for Sensor-Based Human Activity Recognition. (arXiv:2202.12949v1 [eess.SP])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12949">
<div class="article-summary-box-inner">
<span><p>As a fundamental problem in ubiquitous computing and machine learning,
sensor-based human activity recognition (HAR) has drawn extensive attention and
made great progress in recent years. HAR aims to recognize human activities
based on the availability of rich time-series data collected from multi-modal
sensors such as accelerometers and gyroscopes. However, recent deep learning
methods are focusing on one view of the data, i.e., the temporal view, while
shallow methods tend to utilize the hand-craft features for recognition, e.g.,
the statistics view. In this paper, to extract a better feature for advancing
the performance, we propose a novel method, namely multi-view fusion
transformer (MVFT) along with a novel attention mechanism. First, MVFT encodes
three views of information, i.e., the temporal, frequent, and statistical views
to generate multi-view features. Second, the novel attention mechanism uncovers
inner- and cross-view clues to catalyze mutual interactions between three views
for detailed relation modeling. Moreover, extensive experiments on two datasets
illustrate the superiority of our methods over several state-of-the-art
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Model Attribution of Face-swap Deepfake Videos. (arXiv:2202.12951v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12951">
<div class="article-summary-box-inner">
<span><p>AI-created face-swap videos, commonly known as Deepfakes, have attracted wide
attention as powerful impersonation attacks. Existing research on Deepfakes
mostly focuses on binary detection to distinguish between real and fake videos.
However, it is also important to determine the specific generation model for a
fake video, which can help attribute it to the source for forensic
investigation. In this paper, we fill this gap by studying the model
attribution problem of Deepfake videos. We first introduce a new dataset with
DeepFakes from Different Models (DFDM) based on several Autoencoder models.
Specifically, five generation models with variations in encoder, decoder,
intermediate layer, input resolution, and compression ratio have been used to
generate a total of 6,450 Deepfake videos based on the same input. Then we take
Deepfakes model attribution as a multiclass classification task and propose a
spatial and temporal attention based method to explore the differences among
Deepfakes in the new dataset. Experimental evaluation shows that most existing
Deepfakes detection methods failed in Deepfakes model attribution, while the
proposed method achieved over 70% accuracy on the high-quality DFDM dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image reconstruction algorithms in radio interferometry: from handcrafted to learned denoisers. (arXiv:2202.12959v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12959">
<div class="article-summary-box-inner">
<span><p>We introduce a new class of iterative image reconstruction algorithms for
radio interferometry, at the interface of convex optimization and deep
learning, inspired by plug-and-play methods. The approach consists in learning
a prior image model by training a deep neural network (DNN) as a denoiser, and
substituting it for the handcrafted proximal regularization operator of an
optimization algorithm. The proposed AIRI ("AI for Regularization in
Radio-Interferometric Imaging") framework, for imaging complex intensity
structure with diffuse and faint emission, inherits the robustness and
interpretability of optimization, and the learning power and speed of networks.
Our approach relies on three steps. Firstly, we design a low dynamic range
database for supervised training from optical intensity images. Secondly, we
train a DNN denoiser with basic architecture ensuring positivity of the output
image, at a noise level inferred from the signal-to-noise ratio of the data. We
use either $\ell_2$ or $\ell_1$ training losses, enhanced with a
nonexpansiveness term ensuring algorithm convergence, and including on-the-fly
database dynamic range enhancement via exponentiation. Thirdly, we plug the
learned denoiser into the forward-backward optimization algorithm, resulting in
a simple iterative structure alternating a denoising step with a
gradient-descent data-fidelity step. The resulting AIRI-$\ell_2$ and
AIRI-$\ell_1$ were validated against CLEAN and optimization algorithms of the
SARA family, propelled by the "average sparsity" proximal regularization
operator. Simulation results show that these first AIRI incarnations are
competitive in imaging quality with SARA and its unconstrained
forward-backward-based version uSARA, while providing significant acceleration.
CLEAN remains faster but offers lower reconstruction quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FSGANv2: Improved Subject Agnostic Face Swapping and Reenactment. (arXiv:2202.12972v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12972">
<div class="article-summary-box-inner">
<span><p>We present Face Swapping GAN (FSGAN) for face swapping and reenactment.
Unlike previous work, we offer a subject agnostic swapping scheme that can be
applied to pairs of faces without requiring training on those faces. We derive
a novel iterative deep learning--based approach for face reenactment which
adjusts significant pose and expression variations that can be applied to a
single image or a video sequence. For video sequences, we introduce a
continuous interpolation of the face views based on reenactment, Delaunay
Triangulation, and barycentric coordinates. Occluded face regions are handled
by a face completion network. Finally, we use a face blending network for
seamless blending of the two faces while preserving the target skin color and
lighting conditions. This network uses a novel Poisson blending loss combining
Poisson optimization with a perceptual loss. We compare our approach to
existing state-of-the-art systems and show our results to be both qualitatively
and quantitatively superior. This work describes extensions of the FSGAN
method, proposed in an earlier conference version of our work, as well as
additional experiments and results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OCR-IDL: OCR Annotations for Industry Document Library Dataset. (arXiv:2202.12985v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12985">
<div class="article-summary-box-inner">
<span><p>Pretraining has proven successful in Document Intelligence tasks where deluge
of documents are used to pretrain the models only later to be finetuned on
downstream tasks. One of the problems of the pretraining approaches is the
inconsistent usage of pretraining data with different OCR engines leading to
incomparable results between models. In other words, it is not obvious whether
the performance gain is coming from diverse usage of amount of data and
distinct OCR engines or from the proposed models. To remedy the problem, we
make public the OCR annotations for IDL documents using commercial OCR engine
given their superior performance over open source OCR models. The contributed
dataset (OCR-IDL) has an estimated monetary value over 20K US$. It is our hope
that OCR-IDL can be a starting point for future works on Document Intelligence.
All of our data and its collection process with the annotations can be found in
https://github.com/furkanbiten/idl_data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extracting Effective Subnetworks with Gumebel-Softmax. (arXiv:2202.12986v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12986">
<div class="article-summary-box-inner">
<span><p>Large and performant neural networks are often overparameterized and can be
drastically reduced in size and complexity thanks to pruning. Pruning is a
group of methods, which seeks to remove redundant or unnecessary weights or
groups of weights in a network. These techniques allow the creation of
lightweight networks, which are particularly critical in embedded or mobile
applications. In this paper, we devise an alternative pruning method that
allows extracting effective subnetworks from larger untrained ones. Our method
is stochastic and extracts subnetworks by exploring different topologies which
are sampled using Gumbel Softmax. The latter is also used to train probability
distributions which measure the relevance of weights in the sampled topologies.
The resulting subnetworks are further enhanced using a highly efficient
rescaling mechanism that reduces training time and improves performances.
Extensive experiments conducted on CIFAR10 show the outperformance of our
subnetwork extraction method against the related work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Brief Survey on Adaptive Video Streaming Quality Assessment. (arXiv:2202.12987v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12987">
<div class="article-summary-box-inner">
<span><p>Quality of experience (QoE) assessment for adaptive video streaming plays a
significant role in advanced network management systems. It is especially
challenging in case of dynamic adaptive streaming schemes over HTTP (DASH)
which has increasingly complex characteristics including additional playback
issues. In this paper, we provide a brief overview of adaptive video streaming
quality assessment. Upon our review of related works, we analyze and compare
different variations of objective QoE assessment models with or without using
machine learning techniques for adaptive video streaming. Through the
performance analysis, we observe that hybrid models perform better than both
quality-of-service (QoS) driven QoE approaches and signal fidelity measurement.
Moreover, the machine learning-based model slightly outperforms the model
without using machine learning for the same setting. In addition, we find that
existing video streaming QoE assessment models still have limited performance,
which makes it difficult to be applied in practical communication systems.
Therefore, based on the success of deep learned feature representations for
traditional video quality prediction, we also apply the off-the-shelf deep
convolutional neural network (DCNN) to evaluate the perceptual quality of
streaming videos, where the spatio-temporal properties of streaming videos are
taken into consideration. Experiments demonstrate its superiority, which sheds
light on the future development of specifically designed deep learning
frameworks for adaptive video streaming quality assessment. We believe this
survey can serve as a guideline for QoE assessment of adaptive video streaming.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Supervised Instance Segmentation using Motion Information via Optical Flow. (arXiv:2202.13006v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13006">
<div class="article-summary-box-inner">
<span><p>Weakly supervised instance segmentation has gained popularity because it
reduces high annotation cost of pixel-level masks required for model training.
Recent approaches for weakly supervised instance segmentation detect and
segment objects using appearance information obtained from a static image.
However, it poses the challenge of identifying objects with a
non-discriminatory appearance. In this study, we address this problem by using
motion information from image sequences. We propose a two-stream encoder that
leverages appearance and motion features extracted from images and optical
flows. Additionally, we propose a novel pairwise loss that considers both
appearance and motion information to supervise segmentation. We conducted
extensive evaluations on the YouTube-VIS 2019 benchmark dataset. Our results
demonstrate that the proposed method improves the Average Precision of the
state-of-the-art method by 3.1.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-view Gradient Consistency for SVBRDF Estimation of Complex Scenes under Natural Illumination. (arXiv:2202.13017v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13017">
<div class="article-summary-box-inner">
<span><p>This paper presents a process for estimating the spatially varying surface
reflectance of complex scenes observed under natural illumination. In contrast
to previous methods, our process is not limited to scenes viewed under
controlled lighting conditions but can handle complex indoor and outdoor scenes
viewed under arbitrary illumination conditions. An end-to-end process uses a
model of the scene's geometry and several images capturing the scene's surfaces
from arbitrary viewpoints and under various natural illumination conditions. We
develop a differentiable path tracer that leverages least-square conformal
mapping for handling multiple disjoint objects appearing in the scene. We
follow a two-step optimization process and introduce a multi-view gradient
consistency loss which results in up to 30-50% improvement in the image
reconstruction loss and can further achieve better disentanglement of the
diffuse and specular BRDFs compared to other state-of-the-art. We demonstrate
the process in real-world indoor and outdoor scenes from images in the wild and
show that we can produce realistic renders consistent with actual images using
the estimated reflectance properties. Experiments show that our technique
produces realistic results for arbitrary outdoor scenes with complex geometry.
The source code is publicly available at:
https://gitlab.com/alen.joy/multi-view-gradient-consistency-for-svbrdf-estimation-of-complex-scenes-under-natural-illumination
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HCIL: Hierarchical Class Incremental Learning for Longline Fishing Visual Monitoring. (arXiv:2202.13018v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13018">
<div class="article-summary-box-inner">
<span><p>The goal of electronic monitoring of longline fishing is to visually monitor
the fish catching activities on fishing vessels based on cameras, either for
regulatory compliance or catch counting. The previous hierarchical
classification method demonstrates efficient fish species identification of
catches from longline fishing, where fishes are under severe deformation and
self-occlusion during the catching process. Although the hierarchical
classification mitigates the laborious efforts of human reviews by providing
confidence scores in different hierarchical levels, its performance drops
dramatically under the class incremental learning (CIL) scenario. A CIL system
should be able to learn about more and more classes over time from a stream of
data, i.e., only the training data for a small number of classes have to be
present at the beginning and new classes can be added progressively. In this
work, we introduce a Hierarchical Class Incremental Learning (HCIL) model,
which significantly improves the state-of-the-art hierarchical classification
methods under the CIL scenario.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Building a visual semantics aware object hierarchy. (arXiv:2202.13021v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13021">
<div class="article-summary-box-inner">
<span><p>The semantic gap is defined as the difference between the linguistic
representations of the same concept, which usually leads to misunderstanding
between individuals with different knowledge backgrounds. Since linguistically
annotated images are extensively used for training machine learning models,
semantic gap problem (SGP) also results in inevitable bias on image annotations
and further leads to poor performance on current computer vision tasks. To
address this problem, we propose a novel unsupervised method to build visual
semantics aware object hierarchy, aiming to get a classification model by
learning from pure-visual information and to dissipate the bias of linguistic
representations caused by SGP. Our intuition in this paper comes from
real-world knowledge representation where concepts are hierarchically
organized, and each concept can be described by a set of features rather than a
linguistic annotation, namely visual semantic. The evaluation consists of two
parts, firstly we apply the constructed hierarchy on the object recognition
task and then we compare our visual hierarchy and existing lexical hierarchies
to show the validity of our method. The preliminary results reveal the
efficiency and potential of our proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalized Label Shift Correction via Minimum Uncertainty Principle: Theory and Algorithm. (arXiv:2202.13043v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13043">
<div class="article-summary-box-inner">
<span><p>As a fundamental problem in machine learning, dataset shift induces a
paradigm to learn and transfer knowledge under changing environment. Previous
methods assume the changes are induced by covariate, which is less practical
for complex real-world data. We consider the Generalized Label Shift (GLS),
which provides an interpretable insight into the learning and transfer of
desirable knowledge. Current GLS methods: 1) are not well-connected with the
statistical learning theory; 2) usually assume the shifting conditional
distributions will be matched with an implicit transformation, but its explicit
modeling is unexplored. In this paper, we propose a conditional adaptation
framework to deal with these challenges. From the perspective of learning
theory, we prove that the generalization error of conditional adaptation is
lower than previous covariate adaptation. Following the theoretical results, we
propose the minimum uncertainty principle to learn conditional invariant
transformation via discrepancy optimization. Specifically, we propose the
\textit{conditional metric operator} on Hilbert space to characterize the
distinctness of conditional distributions. For finite observations, we prove
that the empirical estimation is always well-defined and will converge to
underlying truth as sample size increases. The results of extensive experiments
demonstrate that the proposed model achieves competitive performance under
different GLS scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optical flow-based branch segmentation for complex orchard environments. (arXiv:2202.13050v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13050">
<div class="article-summary-box-inner">
<span><p>Machine vision is a critical subsystem for enabling robots to be able to
perform a variety of tasks in orchard environments. However, orchards are
highly visually complex environments, and computer vision algorithms operating
in them must be able to contend with variable lighting conditions and
background noise. Past work on enabling deep learning algorithms to operate in
these environments has typically required large amounts of hand-labeled data to
train a deep neural network or physically controlling the conditions under
which the environment is perceived. In this paper, we train a neural network
system in simulation only using simulated RGB data and optical flow. This
resulting neural network is able to perform foreground segmentation of branches
in a busy orchard environment without additional real-world training or using
any special setup or equipment beyond a standard camera. Our results show that
our system is highly accurate and, when compared to a network using manually
labeled RGBD data, achieves significantly more consistent and robust
performance across environments that differ from the training set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Depth from Focal Stack with Defocus Model for Camera-Setting Invariance. (arXiv:2202.13055v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13055">
<div class="article-summary-box-inner">
<span><p>We propose a learning-based depth from focus/defocus (DFF), which takes a
focal stack as input for estimating scene depth. Defocus blur is a useful cue
for depth estimation. However, the size of the blur depends on not only scene
depth but also camera settings such as focus distance, focal length, and
f-number. Current learning-based methods without any defocus models cannot
estimate a correct depth map if camera settings are different at training and
test times. Our method takes a plane sweep volume as input for the constraint
between scene depth, defocus images, and camera settings, and this intermediate
representation enables depth estimation with different camera settings at
training and test times. This camera-setting invariance can enhance the
applicability of learning-based DFF methods. The experimental results also
indicate that our method is robust against a synthetic-to-real domain gap, and
exhibits state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An End-to-End Transformer Model for Crowd Localization. (arXiv:2202.13065v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13065">
<div class="article-summary-box-inner">
<span><p>Crowd localization, predicting head positions, is a more practical and
high-level task than simply counting. Existing methods employ pseudo-bounding
boxes or pre-designed localization maps, relying on complex post-processing to
obtain the head positions. In this paper, we propose an elegant, end-to-end
Crowd Localization TRansformer named CLTR that solves the task in the
regression-based paradigm. The proposed method views the crowd localization as
a direct set prediction problem, taking extracted features and trainable
embeddings as input of the transformer-decoder. To achieve good matching
results, we introduce a KMO-based Hungarian, which innovatively revisits the
label assignment from a context view instead of an independent instance view.
Extensive experiments conducted on five datasets in various data settings show
the effectiveness of our method. In particular, the proposed method achieves
the best localization performance on the NWPU-Crowd, UCF-QNRF, and ShanghaiTech
Part A datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Robust Document Image Watermarking Scheme using Deep Neural Network. (arXiv:2202.13067v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13067">
<div class="article-summary-box-inner">
<span><p>Watermarking is an important copyright protection technology which generally
embeds the identity information into the carrier imperceptibly. Then the
identity can be extracted to prove the copyright from the watermarked carrier
even after suffering various attacks. Most of the existing watermarking
technologies take the nature images as carriers. Different from the natural
images, document images are not so rich in color and texture, and thus have
less redundant information to carry watermarks. This paper proposes an
end-to-end document image watermarking scheme using the deep neural network.
Specifically, an encoder and a decoder are designed to embed and extract the
watermark. A noise layer is added to simulate the various attacks that could be
encountered in reality, such as the Cropout, Dropout, Gaussian blur, Gaussian
noise, Resize, and JPEG Compression. A text-sensitive loss function is designed
to limit the embedding modification on characters. An embedding strength
adjustment strategy is proposed to improve the quality of watermarked image
with little loss of extraction accuracy. Experimental results show that the
proposed document image watermarking technology outperforms three
state-of-the-arts in terms of the robustness and image quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty-Aware Deep Multi-View Photometric Stereo. (arXiv:2202.13071v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13071">
<div class="article-summary-box-inner">
<span><p>This paper presents a simple and effective solution to the problem of
multi-view photometric stereo (MVPS). It is well-known that photometric stereo
(PS) is excellent at recovering high-frequency surface details, whereas
multi-view stereo (MVS) can help remove the low-frequency distortion due to PS
and retain the global geometry of the shape. This paper proposes an approach
that can effectively utilize such complementary strengths of PS and MVS. Our
key idea is to suitably combine them while taking into account the per-pixel
uncertainty of their estimates. To this end, we estimate per-pixel surface
normals and depth using an uncertainty-aware deep-PS network and deep-MVS
network, respectively. Uncertainty modeling helps select reliable surface
normal and depth estimates at each pixel which then act as a true
representative of the dense surface geometry. At each pixel, our approach
either selects or discards deep-PS and deep-MVS network prediction depending on
the prediction uncertainty measure. For dense, detailed, and precise inference
of the object's surface profile, we propose to learn the implicit neural shape
representation via a multilayer perceptron (MLP). Our approach encourages the
MLP to converge to a natural zero-level set surface using the confident
prediction from deep-PS and deep-MVS networks, providing superior dense surface
reconstruction. Extensive experiments on the DiLiGenT-MV benchmark dataset show
that our method outperforms most of the existing approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Contrastive Self-Supervised Learning. (arXiv:2202.13072v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13072">
<div class="article-summary-box-inner">
<span><p>Recently, learning from vast unlabeled data, especially self-supervised
learning, has been emerging and attracted widespread attention. Self-supervised
learning followed by the supervised fine-tuning on a few labeled examples can
significantly improve label efficiency and outperform standard supervised
training using fully annotated data. In this work, we present a novel
self-supervised deep learning paradigm based on online hard negative pair
mining. Specifically, we design a student-teacher network to generate
multi-view of the data for self-supervised learning and integrate hard negative
pair mining into the training. Then we derive a new triplet-like loss
considering both positive sample pairs and mined hard negative sample pairs.
Extensive experiments demonstrate the effectiveness of the proposed method and
its components on ILSVRC-2012.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Global Instance Tracking: Locating Target More Like Humans. (arXiv:2202.13073v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13073">
<div class="article-summary-box-inner">
<span><p>Target tracking, the essential ability of the human visual system, has been
simulated by computer vision tasks. However, existing trackers perform well in
austere experimental environments but fail in challenges like occlusion and
fast motion. The massive gap indicates that researches only measure tracking
performance rather than intelligence. How to scientifically judge the
intelligence level of trackers? Distinct from decision-making problems, lacking
three requirements (a challenging task, a fair environment, and a scientific
evaluation procedure) makes it strenuous to answer the question. In this
article, we first propose the global instance tracking (GIT) task, which is
supposed to search an arbitrary user-specified instance in a video without any
assumptions about camera or motion consistency, to model the human visual
tracking ability. Whereafter, we construct a high-quality and large-scale
benchmark VideoCube to create a challenging environment. Finally, we design a
scientific evaluation procedure using human capabilities as the baseline to
judge tracking intelligence. Additionally, we provide an online platform with
toolkit and an updated leaderboard. Although the experimental results indicate
a definite gap between trackers and humans, we expect to take a step forward to
generate authentic human-like trackers. The database, toolkit, evaluation
server, and baseline results are available at <a href="http://videocube.aitestunion.com.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Utility and Feasibility of a Center Surround Event Camera. (arXiv:2202.13076v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13076">
<div class="article-summary-box-inner">
<span><p>Standard dynamic vision sensor (DVS) event cameras output a stream of
spatially-independent log-intensity brightness change events so they cannot
suppress spatial redundancy. Nearly all biological retinas use an antagonistic
center-surround organization. This paper proposes a practical method of
implementing a compact, energy-efficient Center Surround DVS (CSDVS) with a
surround smoothing network that uses compact polysilicon resistors for lateral
resistance. The paper includes behavioral simulation results for the CSDVS (see
sites.google.com/view/csdvs/home). The CSDVS would significantly reduce events
caused by low spatial frequencies, but amplify the informative high frequency
spatiotemporal events.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SWIS: Self-Supervised Representation Learning For Writer Independent Offline Signature Verification. (arXiv:2202.13078v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13078">
<div class="article-summary-box-inner">
<span><p>Writer independent offline signature verification is one of the most
challenging tasks in pattern recognition as there is often a scarcity of
training data. To handle such data scarcity problem, in this paper, we propose
a novel self-supervised learning (SSL) framework for writer independent offline
signature verification. To our knowledge, this is the first attempt to utilize
self-supervised setting for the signature verification task. The objective of
self-supervised representation learning from the signature images is achieved
by minimizing the cross-covariance between two random variables belonging to
different feature directions and ensuring a positive cross-covariance between
the random variables denoting the same feature direction. This ensures that the
features are decorrelated linearly and the redundant information is discarded.
Through experimental results on different data sets, we obtained encouraging
results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved Hard Example Mining Approach for Single Shot Object Detectors. (arXiv:2202.13080v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13080">
<div class="article-summary-box-inner">
<span><p>Hard example mining methods generally improve the performance of the object
detectors, which suffer from imbalanced training sets. In this work, two
existing hard example mining approaches (LRM and focal loss, FL) are adapted
and combined in a state-of-the-art real-time object detector, YOLOv5. The
effectiveness of the proposed approach for improving the performance on hard
examples is extensively evaluated. The proposed method increases mAP by 3%
compared to using the original loss function and around 1-2% compared to using
the hard-mining methods (LRM or FL) individually on 2021 Anti-UAV Challenge
Dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Improved Deep Learning Approach For Product Recognition on Racks in Retail Stores. (arXiv:2202.13081v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13081">
<div class="article-summary-box-inner">
<span><p>Automated product recognition in retail stores is an important real-world
application in the domain of Computer Vision and Pattern Recognition. In this
paper, we consider the problem of automatically identifying the classes of the
products placed on racks in retail stores from an image of the rack and
information about the query/product images. We improve upon the existing
approaches in terms of effectiveness and memory requirement by developing a
two-stage object detection and recognition pipeline comprising of a
Faster-RCNN-based object localizer that detects the object regions in the rack
image and a ResNet-18-based image encoder that classifies the detected regions
into the appropriate classes. Each of the models is fine-tuned using
appropriate data sets for better prediction and data augmentation is performed
on each query image to prepare an extensive gallery set for fine-tuning the
ResNet-18-based product recognition model. This encoder is trained using a
triplet loss function following the strategy of online-hard-negative-mining for
improved prediction. The proposed models are lightweight and can be connected
in an end-to-end manner during deployment for automatically identifying each
product object placed in a rack image. Extensive experiments using Grozi-32k
and GP-180 data sets verify the effectiveness of the proposed model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Speech Recognition for Multiple Languages in the Wild. (arXiv:2202.13084v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13084">
<div class="article-summary-box-inner">
<span><p>Visual speech recognition (VSR) aims to recognise the content of speech based
on the lip movements without relying on the audio stream. Advances in deep
learning and the availability of large audio-visual datasets have led to the
development of much more accurate and robust VSR models than ever before.
However, these advances are usually due to larger training sets rather than the
model design. In this work, we demonstrate that designing better models is
equally important to using larger training sets. We propose the addition of
prediction-based auxiliary tasks to a VSR model and highlight the importance of
hyper-parameter optimisation and appropriate data augmentations. We show that
such model works for different languages and outperforms all previous methods
trained on publicly available datasets by a large margin. It even outperforms
models that were trained on non-publicly available datasets containing up to to
21 times more data. We show furthermore that using additional training data,
even in other languages or with automatically generated transcriptions, results
in further improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RIConv++: Effective Rotation Invariant Convolutions for 3D Point Clouds Deep Learning. (arXiv:2202.13094v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13094">
<div class="article-summary-box-inner">
<span><p>3D point clouds deep learning is a promising field of research that allows a
neural network to learn features of point clouds directly, making it a robust
tool for solving 3D scene understanding tasks. While recent works show that
point cloud convolutions can be invariant to translation and point permutation,
investigations of the rotation invariance property for point cloud convolution
has been so far scarce. Some existing methods perform point cloud convolutions
with rotation-invariant features, existing methods generally do not perform as
well as translation-invariant only counterpart. In this work, we argue that a
key reason is that compared to point coordinates, rotation-invariant features
consumed by point cloud convolution are not as distinctive. To address this
problem, we propose a simple yet effective convolution operator that enhances
feature distinction by designing powerful rotation invariant features from the
local regions. We consider the relationship between the point of interest and
its neighbors as well as the internal relationship of the neighbors to largely
improve the feature descriptiveness. Our network architecture can capture both
local and global context by simply tuning the neighborhood size in each
convolution layer. We conduct several experiments on synthetic and real-world
point cloud classifications, part segmentation, and shape retrieval to evaluate
our method, which achieves the state-of-the-art accuracy under challenging
rotations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continuous Human Action Recognition for Human-Machine Interaction: A Review. (arXiv:2202.13096v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13096">
<div class="article-summary-box-inner">
<span><p>With advances in data-driven machine learning research, a wide variety of
prediction models have been proposed to capture spatio-temporal features for
the analysis of video streams. Recognising actions and detecting action
transitions within an input video are challenging but necessary tasks for
applications that require real-time human-machine interaction. By reviewing a
large body of recent related work in the literature, we thoroughly analyse,
explain and compare action segmentation methods and provide details on the
feature extraction and learning strategies that are used on most
state-of-the-art methods. We cover the impact of the performance of object
detection and tracking techniques on human action segmentation methodologies.
We investigate the application of such models to real-world scenarios and
discuss several limitations and key research directions towards improving
interpretability, generalisation, optimisation and deployment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Symmetric Convolutional Filters: A Novel Way to Constrain Parameters in CNN. (arXiv:2202.13099v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13099">
<div class="article-summary-box-inner">
<span><p>We propose a novel technique to constrain parameters in CNN based on
symmetric filters. We investigate the impact on SOTA networks when varying the
combinations of symmetricity. We demonstrate that our models offer effective
generalisation and a structured elimination of redundancy in parameters. We
conclude by comparing our method with other pruning techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Supervision: Enabling Generalization over Output Spaces. (arXiv:2202.13100v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13100">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose Semantic Supervision (SemSup) - a unified paradigm
for training classifiers that generalize over output spaces. In contrast to
standard classification, which treats classes as discrete symbols, SemSup
represents them as dense vector features obtained from descriptions of classes
(e.g., "The cat is a small carnivorous mammal"). This allows the output space
to be unbounded (in the space of descriptions) and enables models to generalize
both over unseen inputs and unseen outputs (e.g. "The aardvark is a nocturnal
burrowing mammal with long ears"). Specifically, SemSup enables four types of
generalization, to -- (1) unseen class descriptions, (2) unseen classes, (3)
unseen super-classes, and (4) unseen tasks. Through experiments on four
classification datasets across two variants (multi-class and multi-label), two
input modalities (text and images), and two output description modalities (text
and JSON), we show that our SemSup models significantly outperform standard
supervised models and existing models that leverage word embeddings over class
names. For instance, our model outperforms baselines by 40% and 20% precision
points on unseen descriptions and classes, respectively, on a news
categorization dataset (RCV1). SemSup can serve as a pathway for scaling neural
models to large unbounded output spaces and enabling better generalization and
model reuse for unseen tasks and domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analysis of Visual Reasoning on One-Stage Object Detection. (arXiv:2202.13115v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13115">
<div class="article-summary-box-inner">
<span><p>Current state-of-the-art one-stage object detectors are limited by treating
each image region separately without considering possible relations of the
objects. This causes dependency solely on high-quality convolutional feature
representations for detecting objects successfully. However, this may not be
possible sometimes due to some challenging conditions. In this paper, the usage
of reasoning features on one-stage object detection is analyzed. We attempted
different architectures that reason the relations of the image regions by using
self-attention. YOLOv3-Reasoner2 model spatially and semantically enhances
features in the reasoning layer and fuses them with the original convolutional
features to improve performance. The YOLOv3-Reasoner2 model achieves around
2.5% absolute improvement with respect to baseline YOLOv3 on COCO in terms of
mAP while still running in real-time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Unsupervised Cross-Modal Hashing Method Robust to Noisy Training Image-Text Correspondences in Remote Sensing. (arXiv:2202.13117v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13117">
<div class="article-summary-box-inner">
<span><p>The development of accurate and scalable cross-modal image-text retrieval
methods, where queries from one modality (e.g., text) can be matched to archive
entries from another (e.g., remote sensing image) has attracted great attention
in remote sensing (RS). Most of the existing methods assume that a reliable
multi-modal training set with accurately matched text-image pairs is existing.
However, this assumption may not always hold since the multi-modal training
sets may include noisy pairs (i.e., textual descriptions/captions associated to
training images can be noisy), distorting the learning process of the retrieval
methods. To address this problem, we propose a novel unsupervised cross-modal
hashing method robust to the noisy image-text correspondences (CHNR). CHNR
consists of three modules: 1) feature extraction module, which extracts feature
representations of image-text pairs; 2) noise detection module, which detects
potential noisy correspondences; and 3) hashing module that generates
cross-modal binary hash codes. The proposed CHNR includes two training phases:
i) meta-learning phase that uses a small portion of clean (i.e., reliable) data
to train the noise detection module in an adversarial fashion; and ii) the main
training phase for which the trained noise detection module is used to identify
noisy correspondences while the hashing module is trained on the noisy
multi-modal training set. Experimental results show that the proposed CHNR
outperforms state-of-the-art methods. Our code is publicly available at
https://git.tu-berlin.de/rsim/chnr
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accurate Human Body Reconstruction for Volumetric Video. (arXiv:2202.13118v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13118">
<div class="article-summary-box-inner">
<span><p>In this work, we enhance a professional end-to-end volumetric video
production pipeline to achieve high-fidelity human body reconstruction using
only passive cameras. While current volumetric video approaches estimate depth
maps using traditional stereo matching techniques, we introduce and optimize
deep learning-based multi-view stereo networks for depth map estimation in the
context of professional volumetric video reconstruction. Furthermore, we
propose a novel depth map post-processing approach including filtering and
fusion, by taking into account photometric confidence, cross-view geometric
consistency, foreground masks as well as camera viewing frustums. We show that
our method can generate high levels of geometric detail for reconstructed human
bodies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Person Re-identification: A Retrospective on Domain Specific Open Challenges and Future Trends. (arXiv:2202.13121v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13121">
<div class="article-summary-box-inner">
<span><p>Person re-identification (Re-ID) is one of the primary components of an
automated visual surveillance system. It aims to automatically identify/search
persons in a multi-camera network having non-overlapping field-of-views. Owing
to its potential in various applications and research significance, a plethora
of deep learning based re-Id approaches have been proposed in the recent years.
However, there exist several vision related challenges, e.g., occlusion, pose
scale \&amp; viewpoint variance, background clutter, person misalignment and
cross-domain generalization across camera modalities, which makes the problem
of re-Id still far from being solved. Majority of the proposed approaches
directly or indirectly aim to solve one or multiple of these existing
challenges. In this context, a comprehensive review of current re-ID approaches
in solving theses challenges is needed to analyze and focus on particular
aspects for further advancements. At present, such a focused review does not
exist and henceforth in this paper, we have presented a systematic
challenge-specific literature survey of 230+ papers between the years of
2015-21. For the first time a survey of this type have been presented where the
person re-Id approaches are reviewed in such solution-oriented perspective.
Moreover, we have presented several diversified prominent developing trends in
the respective research domain which will provide a visionary perspective
regarding ongoing person re-Id research and eventually help to develop
practical real world solutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Content-Variant Reference Image Quality Assessment via Knowledge Distillation. (arXiv:2202.13123v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13123">
<div class="article-summary-box-inner">
<span><p>Generally, humans are more skilled at perceiving differences between
high-quality (HQ) and low-quality (LQ) images than directly judging the quality
of a single LQ image. This situation also applies to image quality assessment
(IQA). Although recent no-reference (NR-IQA) methods have made great progress
to predict image quality free from the reference image, they still have the
potential to achieve better performance since HQ image information is not fully
exploited. In contrast, full-reference (FR-IQA) methods tend to provide more
reliable quality evaluation, but its practicability is affected by the
requirement for pixel-level aligned reference images. To address this, we
firstly propose the content-variant reference method via knowledge distillation
(CVRKD-IQA). Specifically, we use non-aligned reference (NAR) images to
introduce various prior distributions of high-quality images. The comparisons
of distribution differences between HQ and LQ images can help our model better
assess the image quality. Further, the knowledge distillation transfers more
HQ-LQ distribution difference information from the FR-teacher to the
NAR-student and stabilizing CVRKD-IQA performance. Moreover, to fully mine the
local-global combined information, while achieving faster inference speed, our
model directly processes multiple image patches from the input with the
MLP-mixer. Cross-dataset experiments verify that our model can outperform all
NAR/NR-IQA SOTAs, even reach comparable performance with FR-IQA methods on some
occasions. Since the content-variant and non-aligned reference HQ images are
easy to obtain, our model can support more IQA applications with its relative
robustness to content variations. Our code and more detailed elaborations of
supplements are available: https://github.com/guanghaoyin/CVRKD-IQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-image Super-resolution via Quality Map Associated Temporal Attention Network. (arXiv:2202.13124v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13124">
<div class="article-summary-box-inner">
<span><p>With the rising interest in deep learning-based methods in remote sensing,
neural networks have made remarkable advancements in multi-image fusion and
super-resolution. To fully exploit the advantages of multi-image
super-resolution, temporal attention is crucial as it allows a model to focus
on reliable features rather than noises. Despite the presence of quality maps
(QMs) that indicate noises in images, most of the methods tested in the PROBA-V
dataset have not been used QMs for temporal attention. We present a quality map
associated temporal attention network (QA-Net), a novel method that
incorporates QMs into both feature representation and fusion processes for the
first time. Low-resolution features are temporally attended by QM features in
repeated multi-head attention modules. The proposed method achieved
state-of-the-art results in the PROBA-V dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nonlinear Discrete Optimisation of Reversible Steganographic Coding. (arXiv:2202.13133v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13133">
<div class="article-summary-box-inner">
<span><p>Authentication mechanisms are at the forefront of defending the world from
various types of cybercrime. Steganography can serve as an authentication
solution by embedding a digital signature into a carrier object to ensure the
integrity of the object and simultaneously lighten the burden of metadata
management. However, steganographic distortion, albeit generally imperceptible
to human sensory systems, might be inadmissible in fidelity-sensitive
situations. This has led to the concept of reversible steganography. A
fundamental element of reversible steganography is predictive analytics, for
which powerful neural network models have been effectively deployed. As another
core aspect, contemporary reversible steganographic coding is based primarily
on heuristics and therefore worth further study. While attempts have been made
to realise automatic coding with neural networks, perfect reversibility is
still unreachable via such an unexplainable intelligent machinery. Instead of
relying on deep learning, we aim to derive an optimal coding by means of
mathematical optimisation. In this study, we formulate reversible
steganographic coding as a nonlinear discrete optimisation problem with a
logarithmic capacity constraint and a quadratic distortion objective.
Linearisation techniques are developed to enable mixed-integer linear
programming. Experimental results validate the near-optimality of the proposed
optimisation algorithm benchmarked against a brute-force method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RONELDv2: A faster, improved lane tracking method. (arXiv:2202.13137v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13137">
<div class="article-summary-box-inner">
<span><p>Lane detection is an integral part of control systems in autonomous vehicles
and lane departure warning systems as lanes are a key component of the
operating environment for road vehicles. In a previous paper, a robust neural
network output enhancement for active lane detection (RONELD) method augmenting
deep learning lane detection models to improve active, or ego, lane accuracy
performance was presented. This paper extends the work by further investigating
the lane tracking methods used to increase robustness of the method to lane
changes and different lane dimensions (e.g. lane marking thickness) and
proposes an improved, lighter weight lane detection method, RONELDv2. It
improves on the previous RONELD method by detecting the lane point variance,
merging lanes to find a more accurate set of lane parameters, and using an
exponential moving average method to calculate more robust lane weights.
Experiments using the proposed improvements show a consistent increase in lane
detection accuracy results across different datasets and deep learning models,
as well as a decrease in computational complexity observed via an up to
two-fold decrease in runtime, which enhances its suitability for real-time use
on autonomous vehicles and lane departure warning systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Blind Image Super Resolution with Semantic-Aware Quantized Texture Prior. (arXiv:2202.13142v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13142">
<div class="article-summary-box-inner">
<span><p>A key challenge of blind image super resolution is to recover realistic
textures for low-resolution images with unknown degradations. Most recent works
completely rely on the generative ability of GANs, which are difficult to
train. Other methods resort to high-resolution image references that are
usually not available. In this work, we propose a novel framework, denoted as
QuanTexSR, to restore realistic textures with the Quantized Texture Priors
encoded in Vector Quantized GAN. The QuanTexSR generates textures by aligning
the textureless content features to the quantized feature vectors, i.e., a
pretrained feature codebook. Specifically, QuanTexSR formulates the texture
generation as a feature matching problem between textureless features and a
pretrained feature codebook. The final textures are then generated by the
quantized features from the codebook. Since features in the codebook have shown
the ability to generate natural textures in the pretrain stage, QuanTexSR can
generate rich and realistic textures with the pretrained codebook as texture
priors. Moreover, we propose a semantic regularization technique that
regularizes the pre-training of the codebook using clusters of features
extracted from the pretrained VGG19 network. This further improves texture
generation with semantic context. Experiments demonstrate that the proposed
QuanTexSR can generate competitive or better textures than previous approaches.
Code will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DGSS : Domain Generalized Semantic Segmentation using Iterative Style Mining and Latent Representation Alignment. (arXiv:2202.13144v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13144">
<div class="article-summary-box-inner">
<span><p>Semantic segmentation algorithms require access to well-annotated datasets
captured under diverse illumination conditions to ensure consistent
performance. However, poor visibility conditions at varying illumination
conditions result in laborious and error-prone labeling. Alternatively, using
synthetic samples to train segmentation algorithms has gained interest with the
drawback of domain gap that results in sub-optimal performance. While current
state-of-the-art (SoTA) have proposed different mechanisms to bridge the domain
gap, they still perform poorly in low illumination conditions with an average
performance drop of - 10.7 mIOU. In this paper, we focus upon single source
domain generalization to overcome the domain gap and propose a two-step
framework wherein we first identify an adversarial style that maximizes the
domain gap between stylized and source images. Subsequently, these stylized
images are used to categorically align features such that features belonging to
the same class are clustered together in latent space, irrespective of domain
gap. Furthermore, to increase intra-class variance while training, we propose a
style mixing mechanism wherein the same objects from different styles are mixed
to construct a new training image. This framework allows us to achieve a domain
generalized semantic segmentation algorithm with consistent performance without
prior information of the target domain while relying on a single source. Based
on extensive experiments, we match SoTA performance on SYNTHIA $\to$
Cityscapes, GTAV $\to$ Cityscapes while setting new SoTA on GTAV $\to$ Dark
Zurich and GTAV $\to$ Night Driving benchmarks without retraining.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pix2NeRF: Unsupervised Conditional $\pi$-GAN for Single Image to Neural Radiance Fields Translation. (arXiv:2202.13162v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13162">
<div class="article-summary-box-inner">
<span><p>We propose a pipeline to generate Neural Radiance Fields~(NeRF) of an object
or a scene of a specific class, conditioned on a single input image. This is a
challenging task, as training NeRF requires multiple views of the same scene,
coupled with corresponding poses, which are hard to obtain. Our method is based
on $\pi$-GAN, a generative model for unconditional 3D-aware image synthesis,
which maps random latent codes to radiance fields of a class of objects. We
jointly optimize (1) the $\pi$-GAN objective to utilize its high-fidelity
3D-aware generation and (2) a carefully designed reconstruction objective. The
latter includes an encoder coupled with $\pi$-GAN generator to form an
auto-encoder. Unlike previous few-shot NeRF approaches, our pipeline is
unsupervised, capable of being trained with independent images without 3D,
multi-view, or pose supervision. Applications of our pipeline include 3d avatar
generation, object-centric novel view synthesis with a single input image, and
3d-aware super-resolution, to name a few.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Edge Augmentation for Large-Scale Sketch Recognition without Sketches. (arXiv:2202.13164v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13164">
<div class="article-summary-box-inner">
<span><p>This work addresses scaling up the sketch classification task into a large
number of categories. Collecting sketches for training is a slow and tedious
process that has so far precluded any attempts to large-scale sketch
recognition. We overcome the lack of training sketch data by exploiting labeled
collections of natural images that are easier to obtain. To bridge the domain
gap we present a novel augmentation technique that is tailored to the task of
learning sketch recognition from a training set of natural images.
Randomization is introduced in the parameters of edge detection and edge
selection. Natural images are translated to a pseudo-novel domain called
"randomized Binary Thin Edges" (rBTE), which is used as a training domain
instead of natural images. The ability to scale up is demonstrated by training
CNN-based sketch recognition of more than 2.5 times larger number of categories
than used previously. For this purpose, a dataset of natural images from 874
categories is constructed by combining a number of popular computer vision
datasets. The categories are selected to be suitable for sketch recognition. To
estimate the performance, a subset of 393 categories with sketches is also
collected.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Domain Adaptive Salient Object Detection Through Uncertainty-Aware Pseudo-Label Learning. (arXiv:2202.13170v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13170">
<div class="article-summary-box-inner">
<span><p>Recent advances in deep learning significantly boost the performance of
salient object detection (SOD) at the expense of labeling larger-scale
per-pixel annotations. To relieve the burden of labor-intensive labeling, deep
unsupervised SOD methods have been proposed to exploit noisy labels generated
by handcrafted saliency methods. However, it is still difficult to learn
accurate saliency details from rough noisy labels. In this paper, we propose to
learn saliency from synthetic but clean labels, which naturally has higher
pixel-labeling quality without the effort of manual annotations. Specifically,
we first construct a novel synthetic SOD dataset by a simple copy-paste
strategy. Considering the large appearance differences between the synthetic
and real-world scenarios, directly training with synthetic data will lead to
performance degradation on real-world scenarios. To mitigate this problem, we
propose a novel unsupervised domain adaptive SOD method to adapt between these
two domains by uncertainty-aware self-training. Experimental results show that
our proposed method outperforms the existing state-of-the-art deep unsupervised
SOD methods on several benchmark datasets, and is even comparable to
fully-supervised ones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relational Surrogate Loss Learning. (arXiv:2202.13197v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13197">
<div class="article-summary-box-inner">
<span><p>Evaluation metrics in machine learning are often hardly taken as loss
functions, as they could be non-differentiable and non-decomposable, e.g.,
average precision and F1 score. This paper aims to address this problem by
revisiting the surrogate loss learning, where a deep neural network is employed
to approximate the evaluation metrics. Instead of pursuing an exact recovery of
the evaluation metric through a deep neural network, we are reminded of the
purpose of the existence of these evaluation metrics, which is to distinguish
whether one model is better or worse than another. In this paper, we show that
directly maintaining the relation of models between surrogate losses and
metrics suffices, and propose a rank correlation-based optimization method to
maximize this relation and learn surrogate losses. Compared to previous works,
our method is much easier to optimize and enjoys significant efficiency and
performance gains. Extensive experiments show that our method achieves
improvements on various tasks including image classification and neural machine
translation, and even outperforms state-of-the-art methods on human pose
estimation and machine reading comprehension tasks. Code is available at:
https://github.com/hunto/ReLoss.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dropout can Simulate Exponential Number of Models for Sample Selection Techniques. (arXiv:2202.13203v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13203">
<div class="article-summary-box-inner">
<span><p>Following Coteaching, generally in the literature, two models are used in
sample selection based approaches for training with noisy labels. Meanwhile, it
is also well known that Dropout when present in a network trains an ensemble of
sub-networks. We show how to leverage this property of Dropout to train an
exponential number of shared models, by training a single model with Dropout.
We show how we can modify existing two model-based sample selection
methodologies to use an exponential number of shared models. Not only is it
more convenient to use a single model with Dropout, but this approach also
combines the natural benefits of Dropout with that of training an exponential
number of models, leading to improved results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How much depth information can radar infer and contribute. (arXiv:2202.13220v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13220">
<div class="article-summary-box-inner">
<span><p>Since the release of radar data in large scale autonomous driving dataset,
many works have been proposed fusing radar data as an additional guidance
signal into monocular depth estimation models. Although positive performances
are reported, it is still hard to tell how much depth information radar can
infer and contribute in depth estimation models. In this paper, we conduct two
experiments to investigate the intrinsic depth capability of radar data using
state-of-the-art depth estimation models. Our experiments demonstrate that the
estimated depth from only sparse radar input can detect the shape of
surroundings to a certain extent. Furthermore, the monocular depth estimation
model supervised by preprocessed radar only during training can achieve 70%
performance in delta_1 score compared to the baseline model trained with sparse
lidar.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Orientation-Discriminative Feature Representation for Decentralized Pedestrian Tracking. (arXiv:2202.13237v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13237">
<div class="article-summary-box-inner">
<span><p>This paper focuses on the problem of decentralized pedestrian tracking using
a sensor network. Traditional works on pedestrian tracking usually use a
centralized framework, which becomes less practical for robotic applications
due to limited communication bandwidth. Our paper proposes a
communication-efficient, orientation-discriminative feature representation to
characterize pedestrian appearance information, that can be shared among
sensors. Building upon that representation, our work develops a cross-sensor
track association approach to achieve decentralized tracking. Extensive
evaluations are conducted on publicly available datasets and results show that
our proposed approach leads to improved performance in multi-sensor tracking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On-chip QNN: Towards Efficient On-Chip Training of Quantum Neural Networks. (arXiv:2202.13239v1 [quant-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13239">
<div class="article-summary-box-inner">
<span><p>Quantum Neural Network (QNN) is drawing increasing research interest thanks
to its potential to achieve quantum advantage on near-term Noisy Intermediate
Scale Quantum (NISQ) hardware. In order to achieve scalable QNN learning, the
training process needs to be offloaded to real quantum machines instead of
using exponential-cost classical simulators. One common approach to obtain QNN
gradients is parameter shift whose cost scales linearly with the number of
qubits. We present On-chip QNN, the first experimental demonstration of
practical on-chip QNN training with parameter shift. Nevertheless, we find that
due to the significant quantum errors (noises) on real machines, gradients
obtained from naive parameter shift have low fidelity and thus degrade the
training accuracy. To this end, we further propose probabilistic gradient
pruning to firstly identify gradients with potentially large errors and then
remove them. Specifically, small gradients have larger relative errors than
large ones, thus having a higher probability to be pruned. We perform extensive
experiments on 5 classification tasks with 5 real quantum machines. The results
demonstrate that our on-chip training achieves over 90% and 60% accuracy for
2-class and 4-class image classification tasks. The probabilistic gradient
pruning brings up to 7% QNN accuracy improvements over no pruning. Overall, we
successfully obtain similar on-chip training accuracy compared with noise-free
simulation but have much better training scalability. The code for parameter
shift on-chip training is available in the TorchQuantum library.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Supervising Remote Sensing Change Detection Models with 3D Surface Semantics. (arXiv:2202.13251v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13251">
<div class="article-summary-box-inner">
<span><p>Remote sensing change detection, identifying changes between scenes of the
same location, is an active area of research with a broad range of
applications. Recent advances in multimodal self-supervised pretraining have
resulted in state-of-the-art methods which surpass vision models trained solely
on optical imagery. In the remote sensing field, there is a wealth of
overlapping 2D and 3D modalities which can be exploited to supervise
representation learning in vision models. In this paper we propose Contrastive
Surface-Image Pretraining (CSIP) for joint learning using optical RGB and above
ground level (AGL) map pairs. We then evaluate these pretrained models on
several building segmentation and change detection datasets to show that our
method does, in fact, extract features relevant to downstream applications
where natural and artificial surface information is relevant.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Next-Best-View Prediction for Active Stereo Cameras and Highly Reflective Objects. (arXiv:2202.13263v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13263">
<div class="article-summary-box-inner">
<span><p>Depth acquisition with the active stereo camera is a challenging task for
highly reflective objects. When setup permits, multi-view fusion can provide
increased levels of depth completion. However, due to the slow acquisition
speed of high-end active stereo cameras, collecting a large number of
viewpoints for a single scene is generally not practical. In this work, we
propose a next-best-view framework to strategically select camera viewpoints
for completing depth data on reflective objects. In particular, we explicitly
model the specular reflection of reflective surfaces based on the Phong
reflection model and a photometric response function. Given the object CAD
model and grayscale image, we employ an RGB-based pose estimator to obtain
current pose predictions from the existing data, which is used to form
predicted surface normal and depth hypotheses, and allows us to then assess the
information gain from a subsequent frame for any candidate viewpoint. Using
this formulation, we implement an active perception pipeline which is evaluated
on a challenging real-world dataset. The evaluation results demonstrate that
our active depth acquisition method outperforms two strong baselines for both
depth completion and object pose estimation performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Texture Characterization of Histopathologic Images Using Ecological Diversity Measures and Discrete Wavelet Transform. (arXiv:2202.13270v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13270">
<div class="article-summary-box-inner">
<span><p>Breast cancer is a health problem that affects mainly the female population.
An early detection increases the chances of effective treatment, improving the
prognosis of the disease. In this regard, computational tools have been
proposed to assist the specialist in interpreting the breast digital image
exam, providing features for detecting and diagnosing tumors and cancerous
cells. Nonetheless, detecting tumors with a high sensitivity rate and reducing
the false positives rate is still challenging. Texture descriptors have been
quite popular in medical image analysis, particularly in histopathologic images
(HI), due to the variability of both the texture found in such images and the
tissue appearance due to irregularity in the staining process. Such variability
may exist depending on differences in staining protocol such as fixation,
inconsistency in the staining condition, and reagents, either between
laboratories or in the same laboratory. Textural feature extraction for
quantifying HI information in a discriminant way is challenging given the
distribution of intrinsic properties of such images forms a non-deterministic
complex system. This paper proposes a method for characterizing texture across
HIs with a considerable success rate. By employing ecological diversity
measures and discrete wavelet transform, it is possible to quantify the
intrinsic properties of such images with promising accuracy on two HI datasets
compared with state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Dual Neighborhood Hypergraph Neural Network for Change Detection in VHR Remote Sensing Images. (arXiv:2202.13275v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13275">
<div class="article-summary-box-inner">
<span><p>The very high spatial resolution (VHR) remote sensing images have been an
extremely valuable source for monitoring changes occurred on the earth surface.
However, precisely detecting relevant changes in VHR images still remains a
challenge, due to the complexity of the relationships among ground objects. To
address this limitation, a dual neighborhood hypergraph neural network is
proposed in this article, which combines the multiscale superpixel segmentation
and hypergraph convolution to model and exploit the complex relationships.
First, the bi-temporal image pairs are segmented under two scales and fed to a
pre-trained U-net to obtain node features by treating each object under the
fine scale as a node. The dual neighborhood is then defined using the
father-child and adjacent relationships of the segmented objects to construct
the hypergraph, which permits models to represent the higher-order structured
information far more complex than just pairwise relationships. The hypergraph
convolutions are conducted on the constructed hypergraph to propagate the label
information from a small amount of labeled nodes to the other unlabeled ones by
the node-edge-node transform. Moreover, to alleviate the problem of imbalanced
sample, the focal loss function is adopted to train the hypergraph neural
network. The experimental results on optical, SAR and heterogeneous optical/SAR
data sets demonstrate that the proposed method comprises better effectiveness
and robustness compared to many state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Computer Vision-assisted Approach to Automated Real-Time Road Infrastructure Management. (arXiv:2202.13285v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13285">
<div class="article-summary-box-inner">
<span><p>Accurate automated detection of road pavement distresses is critical for the
timely identification and repair of potentially accident-inducing road hazards
such as potholes and other surface-level asphalt cracks. Deployment of such a
system would be further advantageous in low-resource environments where lack of
government funding for infrastructure maintenance typically entails heightened
risks of potentially fatal vehicular road accidents as a result of inadequate
and infrequent manual inspection of road systems for road hazards. To remedy
this, a recent research initiative organized by the Institute of Electrical and
Electronics Engineers ("IEEE") as part of their 2020 Global Road Damage
Detection ("GRDC") Challenge published in May 2020 a novel 21,041 annotated
image dataset of various road distresses calling upon academic and other
researchers to submit innovative deep learning-based solutions to these road
hazard detection problems. Making use of this dataset, we propose a supervised
object detection approach leveraging You Only Look Once ("YOLO") and the Faster
R-CNN frameworks to detect and classify road distresses in real-time via a
vehicle dashboard-mounted smartphone camera, producing 0.68 F1-score
experimental results ranking in the top 5 of 121 teams that entered this
challenge as of December 2021.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DXM-TransFuse U-net: Dual Cross-Modal Transformer Fusion U-net for Automated Nerve Identification. (arXiv:2202.13304v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13304">
<div class="article-summary-box-inner">
<span><p>Accurate nerve identification is critical during surgical procedures for
preventing any damages to nerve tissues. Nerve injuries can lead to long-term
detrimental effects for patients as well as financial overburdens. In this
study, we develop a deep-learning network framework using the U-Net
architecture with a Transformer block based fusion module at the bottleneck to
identify nerve tissues from a multi-modal optical imaging system. By leveraging
and extracting the feature maps of each modality independently and using each
modalities information for cross-modal interactions, we aim to provide a
solution that would further increase the effectiveness of the imaging systems
for enabling the noninvasive intraoperative nerve identification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attention-based Cross-Layer Domain Alignment for Unsupervised Domain Adaptation. (arXiv:2202.13310v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13310">
<div class="article-summary-box-inner">
<span><p>Unsupervised domain adaptation (UDA) aims to learn transferable knowledge
from a labeled source domain and adapts a trained model to an unlabeled target
domain. To bridge the gap between source and target domains, one prevailing
strategy is to minimize the distribution discrepancy by aligning their semantic
features extracted by deep models. The existing alignment-based methods mainly
focus on reducing domain divergence in the same model layer. However, the same
level of semantic information could distribute across model layers due to the
domain shifts. To further boost model adaptation performance, we propose a
novel method called Attention-based Cross-layer Domain Alignment (ACDA), which
captures the semantic relationship between the source and target domains across
model layers and calibrates each level of semantic information automatically
through a dynamic attention mechanism. An elaborate attention mechanism is
designed to reweight each cross-layer pair based on their semantic similarity
for precise domain alignment, effectively matching each level of semantic
information during model adaptation. Extensive experiments on multiple
benchmark datasets consistently show that the proposed method ACDA yields
state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Efficient End-to-End 3D Model Reconstructionbased on Neural Architecture Search. (arXiv:2202.13313v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13313">
<div class="article-summary-box-inner">
<span><p>Using neural networks to represent 3D objects has become popular. However,
many previous works employ neural networks with fixed architecture and size to
represent different 3D objects, which lead to excessive network parameters for
simple objects and limited reconstruction accuracy for complex objects. For
each 3D model, it is desirable to have an end-to-end neural network with as few
parameters as possible to achieve high-fidelity reconstruction. In this paper,
we propose an efficient model reconstruction method utilizing neural
architecture search (NAS) and binary classification. Taking the number of
layers, the number of nodes in each layer, and the activation function of each
layer as the search space, a specific network architecture can be obtained
based on reinforcement learning technology. Furthermore, to get rid of the
traditional surface reconstruction algorithms (e.g., marching cube) used after
network inference, we complete the end-to-end network by classifying binary
voxels. Compared to other signed distance field (SDF) prediction or binary
classification networks, our method achieves significantly higher
reconstruction accuracy using fewer network parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topology-Preserving Segmentation Network: A Deep Learning Segmentation Framework for Connected Component. (arXiv:2202.13331v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13331">
<div class="article-summary-box-inner">
<span><p>Medical image segmentation, which aims to automatically extract anatomical or
pathological structures, plays a key role in computer-aided diagnosis and
disease analysis. Despite the problem has been widely studied, existing methods
are prone to topological errors. In medical imaging, the topology of the
structure, such as the kidney or lung, is usually known. Preserving the
topology of the structure in the segmentation process is of utmost importance
for accurate image analysis. In this work, a novel learning-based segmentation
model is proposed. A {\it topology-preserving segmentation network (TPSN)} is
trained to give an accurate segmentation result of an input image that
preserves the prescribed topology. TPSN is a deformation-based model that
yields a deformation map through a UNet, which takes the medical image and a
template mask as inputs. The main idea is to deform a template mask describing
the prescribed topology by a diffeomorphism to segment the object in the image.
The topology of the shape in the template mask is well preserved under the
diffeomorphic map. The diffeomorphic property of the map is controlled by
introducing a regularization term related to the Jacobian in the loss function.
As such, a topology-preserving segmentation result can be guaranteed.
Furthermore, a multi-scale TPSN is developed in this paper that incorporates
multi-level information of images to produce more precise segmentation results.
To evaluate our method, we applied the 2D TPSN on Ham10000 and 3D TPSN on
KiTS21. Experimental results illustrate our method outperforms the baseline
UNet segmentation model with/without connected-component analysis (CCA) by both
the dice score and IoU score. Besides, results show that our method can produce
reliable results even in challenging cases, where pixel-wise segmentation
models by UNet and CCA fail to obtain accurate results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Overlap: A Prerequisite For Disentanglement. (arXiv:2202.13341v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13341">
<div class="article-summary-box-inner">
<span><p>Learning disentangled representations with variational autoencoders (VAEs) is
often attributed to the regularisation component of the loss. In this work, we
highlight the interaction between data and the reconstruction term of the loss
as the main contributor to disentanglement in VAEs. We note that standardised
benchmark datasets are constructed in a way that is conducive to learning what
appear to be disentangled representations. We design an intuitive adversarial
dataset that exploits this mechanism to break existing state-of-the-art
disentanglement frameworks. Finally, we provide solutions in the form of a
modified reconstruction loss suggesting that VAEs are accidental distance
learners.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Robust Multimodal Remote Sensing Image Registration Method and System Using Steerable Filters with First- and Second-order Gradients. (arXiv:2202.13347v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13347">
<div class="article-summary-box-inner">
<span><p>Co-registration of multimodal remote sensing images is still an ongoing
challenge because of nonlinear radiometric differences (NRD) and significant
geometric distortions (e.g., scale and rotation changes) between these images.
In this paper, a robust matching method based on the Steerable filters is
proposed consisting of two critical steps. First, to address severe NRD, a
novel structural descriptor named the Steerable Filters of first- and
second-Order Channels (SFOC) is constructed, which combines the first- and
second-order gradient information by using the steerable filters with a
multi-scale strategy to depict more discriminative structure features of
images. Then, a fast similarity measure is established called Fast Normalized
Cross-Correlation (Fast-NCCSFOC), which employs the Fast Fourier Transform
technique and the integral image to improve the matching efficiency.
Furthermore, to achieve reliable registration performance, a coarse-to-fine
multimodal registration system is designed consisting of two pivotal modules.
The local coarse registration is first conducted by involving both detection of
interest points (IPs) and local geometric correction, which effectively
utilizes the prior georeferencing information of RS images to address global
geometric distortions. In the fine registration stage, the proposed SFOC is
used to resist significant NRD, and to detect control points between multimodal
images by a template matching scheme. The performance of the proposed matching
method has been evaluated with many different kinds of multimodal RS images.
The results show its superior matching performance compared with the
state-of-the-art methods. Moreover, the designed registration system also
outperforms the popular commercial software in both registration accuracy and
computational efficiency. Our system is available at
https://github.com/yeyuanxin110.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Self-Supervised LiDAR Odometry via Representative Structure Discovery and 3D Inherent Error Modeling. (arXiv:2202.13353v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13353">
<div class="article-summary-box-inner">
<span><p>The correct ego-motion estimation basically relies on the understanding of
correspondences between adjacent LiDAR scans. However, given the complex
scenarios and the low-resolution LiDAR, finding reliable structures for
identifying correspondences can be challenging. In this paper, we delve into
structure reliability for accurate self-supervised ego-motion estimation and
aim to alleviate the influence of unreliable structures in training, inference
and mapping phases. We improve the self-supervised LiDAR odometry substantially
from three aspects: 1) A two-stage odometry estimation network is developed,
where we obtain the ego-motion by estimating a set of sub-region
transformations and averaging them with a motion voting mechanism, to encourage
the network focusing on representative structures. 2) The inherent alignment
errors, which cannot be eliminated via ego-motion optimization, are
down-weighted in losses based on the 3D point covariance estimations. 3) The
discovered representative structures and learned point covariances are
incorporated in the mapping module to improve the robustness of map
construction. Our two-frame odometry outperforms the previous state of the arts
by 16%/12% in terms of translational/rotational errors on the KITTI dataset and
performs consistently well on the Apollo-Southbay datasets. We can even rival
the fully supervised counterparts with our mapping module and more unlabeled
training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Supervised Learning for cell recognition in immunohistochemical cytoplasm staining images. (arXiv:2202.13372v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13372">
<div class="article-summary-box-inner">
<span><p>Cell classification and counting in immunohistochemical cytoplasm staining
images play a pivotal role in cancer diagnosis. Weakly supervised learning is a
potential method to deal with labor-intensive labeling. However, the inconstant
cell morphology and subtle differences between classes also bring challenges.
To this end, we present a novel cell recognition framework based on multi-task
learning, which utilizes two additional auxiliary tasks to guide robust
representation learning of the main task. To deal with misclassification, the
tissue prior learning branch is introduced to capture the spatial
representation of tumor cells without additional tissue annotation. Moreover,
dynamic masks and consistency learning are adopted to learn the invariance of
cell scale and shape. We have evaluated our framework on immunohistochemical
cytoplasm staining images, and the results demonstrate that our method
outperforms recent cell recognition approaches. Besides, we have also done some
ablation studies to show significant improvements after adding the auxiliary
branches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta-RangeSeg: LiDAR Sequence Semantic Segmentation Using Multiple Feature Aggregation. (arXiv:2202.13377v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13377">
<div class="article-summary-box-inner">
<span><p>LiDAR sensor is essential to the perception system in autonomous vehicles and
intelligent robots. To fulfill the real-time requirements in real-world
applications, it is necessary to efficiently segment the LiDAR scans. Most of
previous approaches directly project 3D point cloud onto the 2D spherical range
image so that they can make use of the efficient 2D convolutional operations
for image segmentation. Although having achieved the encouraging results, the
neighborhood information is not well-preserved in the spherical projection.
Moreover, the temporal information is not taken into consideration in the
single scan segmentation task. To tackle these problems, we propose a novel
approach to semantic segmentation for LiDAR sequences named Meta-RangeSeg,
where a novel range residual image representation is introduced to capture the
spatial-temporal information. Specifically, Meta-Kernel is employed to extract
the meta features, which reduces the inconsistency between the 2D range image
coordinates input and Cartesian coordinates output. An efficient U-Net backbone
is used to obtain the multi-scale features. Furthermore, Feature Aggregation
Module (FAM) aggregates the meta features and multi-scale features, which tends
to strengthen the role of range channel. We have conducted extensive
experiments for performance evaluation on SemanticKITTI, which is the de-facto
dataset for LiDAR semantic segmentation. The promising results show that our
proposed Meta-RangeSeg method is more efficient and effective than the existing
approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PanoFlow: Learning Optical Flow for Panoramic Images. (arXiv:2202.13388v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13388">
<div class="article-summary-box-inner">
<span><p>Optical flow estimation is a basic task in self-driving and robotics systems,
which enables to temporally interpret the traffic scene. Autonomous vehicles
clearly benefit from the ultra-wide Field of View (FoV) offered by 360-degree
panoramic sensors. However, due to the unique imaging process of panoramic
images, models designed for pinhole images do not directly generalize
satisfactorily to 360-degree panoramic images. In this paper, we put forward a
novel network framework--PanoFlow, to learn optical flow for panoramic images.
To overcome the distortions introduced by equirectangular projection in
panoramic transformation, we design a Flow Distortion Augmentation (FDA)
method. We further propose a Cyclic Flow Estimation (CFE) method by leveraging
the cyclicity of spherical images to infer 360-degree optical flow and
converting large displacement to relatively small displacement. PanoFlow is
applicable to any existing flow estimation method and benefit from the progress
of narrow-FoV flow estimation. In addition, we create and release a synthetic
panoramic dataset Flow360 based on CARLA to facilitate training and
quantitative analysis. PanoFlow achieves state-of-the-art performance. Our
proposed approach reduces the End-Point-Error (EPE) on the established Flow360
dataset by 26%. On the public OmniFlowNet dataset, PanoFlow achieves an EPE of
3.34 pixels, a 53.1% error reduction from the best published result (7.12
pixels). We also validate our method via an outdoor collection vehicle,
indicating strong potential and robustness for real-world navigation
applications. Code and dataset are publicly available at
https://github.com/MasterHow/PanoFlow.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer-based Knowledge Distillation for Efficient Semantic Segmentation of Road-driving Scenes. (arXiv:2202.13393v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13393">
<div class="article-summary-box-inner">
<span><p>For scene understanding in robotics and automated driving, there is a growing
interest in solving semantic segmentation tasks with transformer-based methods.
However, effective transformers are always too cumbersome and computationally
expensive to solve semantic segmentation in real time, which is desired for
robotic systems. Moreover, due to the lack of inductive biases compared to
Convolutional Neural Networks (CNNs), pre-training on a large dataset is
essential but it takes a long time. Knowledge Distillation (KD) speeds up
inference and maintains accuracy while transferring knowledge from a
pre-trained cumbersome teacher model to a compact student model. Most
traditional KD methods for CNNs focus on response-based knowledge and
feature-based knowledge. In contrast, we present a novel KD framework according
to the nature of transformers, i.e., training compact transformers by
transferring the knowledge from feature maps and patch embeddings of large
transformers. To this purpose, two modules are proposed: (1) the Selective
Kernel Fusion (SKF) module, which helps to construct an efficient
relation-based KD framework, Selective Kernel Review (SKR); (2) the Patch
Embedding Alignment (PEA) module, which performs the dimensional transformation
of patch embeddings. The combined KD framework is called SKR+PEA. Through
comprehensive experiments on Cityscapes and ACDC datasets, it indicates that
our proposed approach outperforms recent state-of-the-art KD frameworks and
rivals the time-consuming pre-training method. Code will be made publicly
available at https://github.com/RuipingL/SKR_PEA.git
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Concept Graph Neural Networks for Surgical Video Understanding. (arXiv:2202.13402v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13402">
<div class="article-summary-box-inner">
<span><p>We constantly integrate our knowledge and understanding of the world to
enhance our interpretation of what we see.
</p>
<p>This ability is crucial in application domains which entail reasoning about
multiple entities and concepts, such as AI-augmented surgery. In this paper, we
propose a novel way of integrating conceptual knowledge into temporal analysis
tasks via temporal concept graph networks. In the proposed networks, a global
knowledge graph is incorporated into the temporal analysis of surgical
instances, learning the meaning of concepts and relations as they apply to the
data. We demonstrate our results in surgical video data for tasks such as
verification of critical view of safety, as well as estimation of Parkland
grading scale. The results show that our method improves the recognition and
detection of complex benchmarks as well as enables other analytic applications
of interest.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multimodal German Dataset for Automatic Lip Reading Systems and Transfer Learning. (arXiv:2202.13403v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13403">
<div class="article-summary-box-inner">
<span><p>Large datasets as required for deep learning of lip reading do not exist in
many languages. In this paper we present the dataset GLips (German Lips)
consisting of 250,000 publicly available videos of the faces of speakers of the
Hessian Parliament, which was processed for word-level lip reading using an
automatic pipeline. The format is similar to that of the English language LRW
(Lip Reading in the Wild) dataset, with each video encoding one word of
interest in a context of 1.16 seconds duration, which yields compatibility for
studying transfer learning between both datasets. By training a deep neural
network, we investigate whether lip reading has language-independent features,
so that datasets of different languages can be used to improve lip reading
models. We demonstrate learning from scratch and show that transfer learning
from LRW to GLips and vice versa improves learning speed and performance, in
particular for the validation set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta-path Analysis on Spatio-Temporal Graphs for Pedestrian Trajectory Prediction. (arXiv:2202.13427v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13427">
<div class="article-summary-box-inner">
<span><p>Spatio-temporal graphs (ST-graphs) have been used to model time series tasks
such as traffic forecasting, human motion modeling, and action recognition. The
high-level structure and corresponding features from ST-graphs have led to
improved performance over traditional architectures. However, current methods
tend to be limited by simple features, despite the rich information provided by
the full graph structure, which leads to inefficiencies and suboptimal
performance in downstream tasks. We propose the use of features derived from
meta-paths, walks across different types of edges, in ST-graphs to improve the
performance of Structural Recurrent Neural Network. In this paper, we present
the Meta-path Enhanced Structural Recurrent Neural Network (MESRNN), a generic
framework that can be applied to any spatio-temporal task in a simple and
scalable manner. We employ MESRNN for pedestrian trajectory prediction,
utilizing these meta-path based features to capture the relationships between
the trajectories of pedestrians at different points in time and space. We
compare our MESRNN against state-of-the-art ST-graph methods on standard
datasets to show the performance boost provided by meta-path information. The
proposed model consistently outperforms the baselines in trajectory prediction
over long time horizons by over 32\%, and produces more socially compliant
trajectories in dense crowds. For more information please refer to the project
website at https://sites.google.com/illinois.edu/mesrnn/home.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unified Wasserstein Distributional Robustness Framework for Adversarial Training. (arXiv:2202.13437v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13437">
<div class="article-summary-box-inner">
<span><p>It is well-known that deep neural networks (DNNs) are susceptible to
adversarial attacks, exposing a severe fragility of deep learning systems. As
the result, adversarial training (AT) method, by incorporating adversarial
examples during training, represents a natural and effective approach to
strengthen the robustness of a DNN-based classifier. However, most AT-based
methods, notably PGD-AT and TRADES, typically seek a pointwise adversary that
generates the worst-case adversarial example by independently perturbing each
data sample, as a way to "probe" the vulnerability of the classifier. Arguably,
there are unexplored benefits in considering such adversarial effects from an
entire distribution. To this end, this paper presents a unified framework that
connects Wasserstein distributional robustness with current state-of-the-art AT
methods. We introduce a new Wasserstein cost function and a new series of risk
functions, with which we show that standard AT methods are special cases of
their counterparts in our framework. This connection leads to an intuitive
relaxation and generalization of existing AT methods and facilitates the
development of a new family of distributional robustness AT-based algorithms.
Extensive experiments show that our distributional robustness AT algorithms
robustify further their standard AT counterparts in various settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Application of DatasetGAN in medical imaging: preliminary studies. (arXiv:2202.13463v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13463">
<div class="article-summary-box-inner">
<span><p>Generative adversarial networks (GANs) have been widely investigated for many
potential applications in medical imaging. DatasetGAN is a recently proposed
framework based on modern GANs that can synthesize high-quality segmented
images while requiring only a small set of annotated training images. The
synthesized annotated images could be potentially employed for many medical
imaging applications, where images with segmentation information are required.
However, to the best of our knowledge, there are no published studies focusing
on its applications to medical imaging. In this work, preliminary studies were
conducted to investigate the utility of DatasetGAN in medical imaging. Three
improvements were proposed to the original DatasetGAN framework, considering
the unique characteristics of medical images. The synthesized segmented images
by DatasetGAN were visually evaluated. The trained DatasetGAN was further
analyzed by evaluating the performance of a pre-defined image segmentation
technique, which was trained by the use of the synthesized datasets. The
effectiveness, concerns, and potential usage of DatasetGAN were discussed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synergistic Network Learning and Label Correction for Noise-robust Image Classification. (arXiv:2202.13472v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13472">
<div class="article-summary-box-inner">
<span><p>Large training datasets almost always contain examples with inaccurate or
incorrect labels. Deep Neural Networks (DNNs) tend to overfit training label
noise, resulting in poorer model performance in practice. To address this
problem, we propose a robust label correction framework combining the ideas of
small loss selection and noise correction, which learns network parameters and
reassigns ground truth labels iteratively. Taking the expertise of DNNs to
learn meaningful patterns before fitting noise, our framework first trains two
networks over the current dataset with small loss selection. Based on the
classification loss and agreement loss of two networks, we can measure the
confidence of training data. More and more confident samples are selected for
label correction during the learning process. We demonstrate our method on both
synthetic and real-world datasets with different noise types and rates,
including CIFAR-10, CIFAR-100 and Clothing1M, where our method outperforms the
baseline approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Spectral Bias of Polynomial Neural Networks. (arXiv:2202.13473v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13473">
<div class="article-summary-box-inner">
<span><p>Polynomial neural networks (PNNs) have been recently shown to be particularly
effective at image generation and face recognition, where high-frequency
information is critical. Previous studies have revealed that neural networks
demonstrate a $\textit{spectral bias}$ towards low-frequency functions, which
yields faster learning of low-frequency components during training. Inspired by
such studies, we conduct a spectral analysis of the Neural Tangent Kernel (NTK)
of PNNs. We find that the $\Pi$-Net family, i.e., a recently proposed
parametrization of PNNs, speeds up the learning of the higher frequencies. We
verify the theoretical bias through extensive experiments. We expect our
analysis to provide novel insights into designing architectures and learning
frameworks by incorporating multiplicative interactions via polynomials.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpretable Concept-based Prototypical Networks for Few-Shot Learning. (arXiv:2202.13474v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13474">
<div class="article-summary-box-inner">
<span><p>Few-shot learning aims at recognizing new instances from classes with limited
samples. This challenging task is usually alleviated by performing
meta-learning on similar tasks. However, the resulting models are black-boxes.
There has been growing concerns about deploying black-box machine learning
models and FSL is not an exception in this regard. In this paper, we propose a
method for FSL based on a set of human-interpretable concepts. It constructs a
set of metric spaces associated with the concepts and classifies samples of
novel classes by aggregating concept-specific decisions. The proposed method
does not require concept annotations for query samples. This interpretable
method achieved results on a par with six previously state-of-the-art black-box
FSL methods on the CUB fine-grained bird classification dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Point Label Aware Superpixels for Multi-species Segmentation of Underwater Imagery. (arXiv:2202.13487v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13487">
<div class="article-summary-box-inner">
<span><p>Monitoring coral reefs using underwater vehicles increases the range of
marine surveys and availability of historical ecological data by collecting
significant quantities of images. Analysis of this imagery can be automated
using a model trained to perform semantic segmentation, however it is too
costly and time-consuming to densely label images for training supervised
models. In this letter, we leverage photo-quadrat imagery labeled by ecologists
with sparse point labels. We propose a point label aware method for propagating
labels within superpixel regions to obtain augmented ground truth for training
a semantic segmentation model. Our point label aware superpixel method utilizes
the sparse point labels, and clusters pixels using learned features to
accurately generate single-species segments in cluttered, complex coral images.
Our method outperforms prior methods on the UCSD Mosaics dataset by 3.62% for
pixel accuracy and 8.35% for mean IoU for the label propagation task.
Furthermore, our approach reduces computation time reported by previous
approaches by 76%. We train a DeepLabv3+ architecture and outperform
state-of-the-art for semantic segmentation by 2.91% for pixel accuracy and
9.65% for mean IoU on the UCSD Mosaics dataset and by 4.19% for pixel accuracy
and 14.32% mean IoU for the Eilat dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Globally Optimal Boresight Alignment of UAV-LiDAR Systems. (arXiv:2202.13501v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13501">
<div class="article-summary-box-inner">
<span><p>In airborne light detection and ranging (LiDAR) systems, misalignments
between the LiDAR-scanner and the inertial navigation system (INS) mounted on
an unmanned aerial vehicle (UAV)'s frame can lead to inaccurate 3D point
clouds. Determining the orientation offset, or boresight error is key to many
LiDAR-based applications. In this work, we introduce a mixed-integer
quadratically constrained quadratic program (MIQCQP) that can globally solve
this misalignment problem. We also propose a nested spatial branch and bound
(nsBB) algorithm that improves computational performance. The nsBB relies on
novel preprocessing steps that progressively reduce the problem size. In
addition, an adaptive grid search (aGS) allowing us to obtain quick heuristic
solutions is presented. Our algorithms are open-source, multi-threaded and
multi-machine compatible.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ESW Edge-Weights : Ensemble Stochastic Watershed Edge-Weights for Hyperspectral Image Classification. (arXiv:2202.13502v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13502">
<div class="article-summary-box-inner">
<span><p>Hyperspectral image (HSI) classification is a topic of active research. One
of the main challenges of HSI classification is the lack of reliable labelled
samples. Various semi-supervised and unsupervised classification methods are
proposed to handle the low number of labelled samples. Chief among them are
graph convolution networks (GCN) and their variants. These approaches exploit
the graph structure for semi-supervised and unsupervised classification. While
several of these methods implicitly construct edge-weights, to our knowledge,
not much work has been done to estimate the edge-weights explicitly. In this
article, we estimate the edge-weights explicitly and use them for the
downstream classification tasks - both semi-supervised and unsupervised. The
proposed edge-weights are based on two key insights - (a) Ensembles reduce the
variance and (b) Classes in HSI datasets and feature similarity have only
one-sided implications. That is, while same classes would have similar
features, similar features do not necessarily imply the same classes.
Exploiting these, we estimate the edge-weights using an aggregate of ensembles
of watersheds over subsamples of features. These edge weights are evaluated for
both semi-supervised and unsupervised classification tasks. The evaluation for
semi-supervised tasks uses Random-Walk based approach. For the unsupervised
case, we use a simple filter using a graph convolution network (GCN). In both
these cases, the proposed edge weights outperform the traditional approaches to
compute edge-weights - Euclidean distances and cosine similarities.
Fascinatingly, with the proposed edge-weights, the simplest GCN obtained
results comparable to the recent state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cyber Mobility Mirror: Deep Learning-based Real-time 3D Object Perception and Reconstruction Using Roadside LiDAR. (arXiv:2202.13505v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13505">
<div class="article-summary-box-inner">
<span><p>Enabling Cooperative Driving Automation (CDA) requires high-fidelity and
real-time perception information, which is available from onboard sensors or
vehicle-to-everything (V2X) communications. Nevertheless, the accessibility of
this information may suffer from the range and occlusion of perception or
limited penetration rates in connectivity. In this paper, we introduce the
prototype of Cyber Mobility Mirror (CMM), a next-generation real-time traffic
surveillance system for 3D object detection, classification, tracking, and
reconstruction, to provide CAVs with wide-range high-fidelity perception
information in a mixed traffic environment. The CMM system consists of six main
components: 1) the data pre-processor to retrieve and pre-process raw data from
the roadside LiDAR; 2) the 3D object detector to generate 3D bounding boxes
based on point cloud data; 3) the multi-objects tracker to endow unique IDs to
detected objects and estimate their dynamic states; 4) the global locator to
map positioning information from the LiDAR coordinate to geographic coordinate
using coordinate transformation; 5) the cloud-based communicator to transmit
perception information from roadside sensors to equipped vehicles; and 6) the
onboard advisor to reconstruct and display the real-time traffic conditions via
Graphical User Interface (GUI). In this study, a field-operational prototype
system is deployed at a real-world intersection, University Avenue and Iowa
Avenue in Riverside, California to assess the feasibility and performance of
our CMM system. Results from field tests demonstrate that our CMM prototype
system can provide satisfactory perception performance with 96.99% precision
and 83.62% recall. High-fidelity real-time traffic conditions (at the object
level) can be displayed on the GUI of the equipped vehicle with a frequency of
3-4 Hz.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GRAPHITE: Generating Automatic Physical Examples for Machine-Learning Attacks on Computer Vision Systems. (arXiv:2002.07088v6 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.07088">
<div class="article-summary-box-inner">
<span><p>This paper investigates an adversary's ease of attack in generating
adversarial examples for real-world scenarios. We address three key
requirements for practical attacks for the real-world: 1) automatically
constraining the size and shape of the attack so it can be applied with
stickers, 2) transform-robustness, i.e., robustness of a attack to
environmental physical variations such as viewpoint and lighting changes, and
3) supporting attacks in not only white-box, but also black-box hard-label
scenarios, so that the adversary can attack proprietary models. In this work,
we propose GRAPHITE, an efficient and general framework for generating attacks
that satisfy the above three key requirements. GRAPHITE takes advantage of
transform-robustness, a metric based on expectation over transforms (EoT), to
automatically generate small masks and optimize with gradient-free
optimization. GRAPHITE is also flexible as it can easily trade-off
transform-robustness, perturbation size, and query count in black-box settings.
On a GTSRB model in a hard-label black-box setting, we are able to find attacks
on all possible 1,806 victim-target class pairs with averages of 77.8%
transform-robustness, perturbation size of 16.63% of the victim images, and
126K queries per pair. For digital-only attacks where achieving
transform-robustness is not a requirement, GRAPHITE is able to find successful
small-patch attacks with an average of only 566 queries for 92.2% of
victim-target pairs. GRAPHITE is also able to find successful attacks using
perturbations that modify small areas of the input image against PatchGuard, a
recently proposed defense against patch-based attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Character Labeling in Movie Videos: Data Resources and Self-supervised Feature Adaptation. (arXiv:2008.11289v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.11289">
<div class="article-summary-box-inner">
<span><p>Robust face clustering is a vital step in enabling computational
understanding of visual character portrayal in media. Face clustering for
long-form content is challenging because of variations in appearance and lack
of supporting large-scale labeled data. Our work in this paper focuses on two
key aspects of this problem: the lack of domain-specific training or benchmark
datasets, and adapting face embeddings learned on web images to long-form
content, specifically movies. First, we present a dataset of over 169,000 face
tracks curated from 240 Hollywood movies with weak labels on whether a pair of
face tracks belong to the same or a different character. We propose an offline
algorithm based on nearest-neighbor search in the embedding space to mine
hard-examples from these tracks. We then investigate triplet-loss and multiview
correlation-based methods for adapting face embeddings to hard-examples. Our
experimental results highlight the usefulness of weakly labeled data for
domain-specific feature adaptation. Overall, we find that multiview
correlation-based adaptation yields more discriminative and robust face
embeddings. Its performance on downstream face verification and clustering
tasks is comparable to that of the state-of-the-art results in this domain. We
also present the SAIL-Movie Character Benchmark corpus developed to augment
existing benchmarks. It consists of racially diverse actors and provides
face-quality labels for subsequent error analysis. We hope that the large-scale
datasets developed in this work can further advance automatic character
labeling in videos. All resources are available freely at
https://sail.usc.edu/~ccmi/multiface.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ray-based classification framework for high-dimensional data. (arXiv:2010.00500v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.00500">
<div class="article-summary-box-inner">
<span><p>While classification of arbitrary structures in high dimensions may require
complete quantitative information, for simple geometrical structures,
low-dimensional qualitative information about the boundaries defining the
structures can suffice. Rather than using dense, multi-dimensional data, we
propose a deep neural network (DNN) classification framework that utilizes a
minimal collection of one-dimensional representations, called \emph{rays}, to
construct the "fingerprint" of the structure(s) based on substantially reduced
information. We empirically study this framework using a synthetic dataset of
double and triple quantum dot devices and apply it to the classification
problem of identifying the device state. We show that the performance of the
ray-based classifier is already on par with traditional 2D images for low
dimensional systems, while significantly cutting down the data acquisition
cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Classification of Single-View Object Point Clouds. (arXiv:2012.10042v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.10042">
<div class="article-summary-box-inner">
<span><p>Object point cloud classification has drawn great research attention since
the release of benchmarking datasets, such as the ModelNet and the ShapeNet.
These benchmarks assume point clouds covering complete surfaces of object
instances, for which plenty of high-performing methods have been developed.
However, their settings deviate from those often met in practice, where, due to
(self-)occlusion, a point cloud covering partial surface of an object is
captured from an arbitrary view. We show in this paper that performance of
existing point cloud classifiers drops drastically under the considered
single-view, partial setting; the phenomenon is consistent with the observation
that semantic category of a partial object surface is less ambiguous only when
its distribution on the whole surface is clearly specified. To this end, we
argue for a single-view, partial setting where supervised learning of object
pose estimation should be accompanied with classification. Technically, we
propose a baseline method of Pose-Accompanied Point cloud classification
Network (PAPNet); built upon SE(3)-equivariant convolutions, the PAPNet learns
intermediate pose transformations for equivariant features defined on vector
fields, which makes the subsequent classification easier (ideally) in the
category-level, canonical pose. By adapting existing ModelNet40 and ScanNet
datasets to the single-view, partial setting, experiment results can verify the
necessity of object pose estimation and superiority of our PAPNet to existing
classifiers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Memory-Augmented Reinforcement Learning for Image-Goal Navigation. (arXiv:2101.05181v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.05181">
<div class="article-summary-box-inner">
<span><p>In this work, we present a memory-augmented approach for image-goal
navigation. Earlier attempts, including RL-based and SLAM-based approaches have
either shown poor generalization performance, or are heavily-reliant on
pose/depth sensors. Our method is based on an attention-based end-to-end model
that leverages an episodic memory to learn to navigate. First, we train a
state-embedding network in a self-supervised fashion, and then use it to embed
previously-visited states into the agent's memory. Our navigation policy takes
advantage of this information through an attention mechanism. We validate our
approach with extensive evaluations, and show that our model establishes a new
state of the art on the challenging Gibson dataset. Furthermore, we achieve
this impressive performance from RGB input alone, without access to additional
information such as position or depth, in stark contrast to related work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A DCNN-based Arbitrarily-Oriented Object Detector for Quality Control and Inspection Application. (arXiv:2101.07383v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.07383">
<div class="article-summary-box-inner">
<span><p>Following the success of machine vision systems for on-line automated quality
control and inspection processes, an object recognition solution is presented
in this work for two different specific applications, i.e., the detection of
quality control items in surgery toolboxes prepared for sterilizing in a
hospital, as well as the detection of defects in vessel hulls to prevent
potential structural failures. The solution has two stages. First, a feature
pyramid architecture based on Single Shot MultiBox Detector (SSD) is used to
improve the detection performance, and a statistical analysis based on ground
truth is employed to select parameters of a range of default boxes. Second, a
lightweight neural network is exploited to achieve oriented detection results
using a regression method. The first stage of the proposed method is capable of
detecting the small targets considered in the two scenarios. In the second
stage, despite the simplicity, it is efficient to detect elongated targets
while maintaining high running efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hyperspherical embedding for novel class classification. (arXiv:2102.03243v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.03243">
<div class="article-summary-box-inner">
<span><p>Deep learning models have become increasingly useful in many different
industries. On the domain of image classification, convolutional neural
networks proved the ability to learn robust features for the closed set
problem, as shown in many different datasets, such as MNIST FASHIONMNIST,
CIFAR10, CIFAR100, and IMAGENET. These approaches use deep neural networks with
dense layers with softmax activation functions in order to learn features that
can separate classes in a latent space. However, this traditional approach is
not useful for identifying classes unseen on the training set, known as the
open set problem. A similar problem occurs in scenarios involving learning on
small data. To tackle both problems, few-shot learning has been proposed. In
particular, metric learning learns features that obey constraints of a metric
distance in the latent space in order to perform classification. However, while
this approach proves to be useful for the open set problem, current
implementation requires pair-wise training, where both positive and negative
examples of similar images are presented during the training phase, which
limits the applicability of these approaches in large data or large class
scenarios given the combinatorial nature of the possible inputs.In this paper,
we present a constraint-based approach applied to the representations in the
latent space under the normalized softmax loss, proposed by[18]. We
experimentally validate the proposed approach for the classification of unseen
classes on different datasets using both metric learning and the normalized
softmax loss, on disjoint and joint scenarios. Our results show that not only
our proposed strategy can be efficiently trained on larger set of classes, as
it does not require pairwise learning, but also present better classification
results than the metric learning strategies surpassing its accuracy by a
significant margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Theoretical bounds on data requirements for the ray-based classification. (arXiv:2103.09577v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.09577">
<div class="article-summary-box-inner">
<span><p>The problem of classifying high-dimensional shapes in real-world data grows
in complexity as the dimension of the space increases. For the case of
identifying convex shapes of different geometries, a new classification
framework has recently been proposed in which the intersections of a set of
one-dimensional representations, called rays, with the boundaries of the shape
are used to identify the specific geometry. This ray-based classification (RBC)
has been empirically verified using a synthetic dataset of two- and
three-dimensional shapes (Zwolak et al. in Proceedings of Third Workshop on
Machine Learning and the Physical Sciences (NeurIPS 2020), Vancouver, Canada
[December 11, 2020], <a href="/abs/2010.00500">arXiv:2010.00500</a>, 2020) and, more recently, has also been
validated experimentally (Zwolak et al., PRX Quantum 2:020335, 2021). Here, we
establish a bound on the number of rays necessary for shape classification,
defined by key angular metrics, for arbitrary convex shapes. For two
dimensions, we derive a lower bound on the number of rays in terms of the
shape's length, diameter, and exterior angles. For convex polytopes in
$\mathbb{R}^N$, we generalize this result to a similar bound given as a
function of the dihedral angle and the geometrical parameters of polygonal
faces. This result enables a different approach for estimating high-dimensional
shapes using substantially fewer data elements than volumetric or surface-based
approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conditional Frechet Inception Distance. (arXiv:2103.11521v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.11521">
<div class="article-summary-box-inner">
<span><p>We consider distance functions between conditional distributions. We focus on
the Wasserstein metric and its Gaussian case known as the Frechet Inception
Distance (FID). We develop conditional versions of these metrics, analyze their
relations and provide a closed form solution to the conditional FID (CFID)
metric. We numerically compare the metrics in the context of performance
evaluation of modern conditional generative models. Our results show the
advantages of CFID compared to the classical FID and mean squared error (MSE)
measures. In contrast to FID, CFID is useful in identifying failures where
realistic outputs which are not related to their inputs are generated. On the
other hand, compared to MSE, CFID is useful in identifying failures where a
single realistic output is generated even though there is a diverse set of
equally probable outputs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepBF: Malicious URL detection using Learned Bloom Filter and Evolutionary Deep Learning. (arXiv:2103.12544v2 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.12544">
<div class="article-summary-box-inner">
<span><p>Malicious URL detection is an emerging research area due to continuous
modernization of various systems, for instance, Edge Computing. In this
article, we present a novel malicious URL detection technique, called deepBF
(deep learning and Bloom Filter). deepBF is presented in two-fold. Firstly, we
propose a learned Bloom Filter using 2-dimensional Bloom Filter. We
experimentally decide the best non-cryptography string hash function. Then, we
derive a modified non-cryptography string hash function from the selected hash
function for deepBF by introducing biases in the hashing method and compared
among the string hash functions. The modified string hash function is compared
to other variants of diverse non-cryptography string hash functions. It is also
compared with various filters, particularly, counting Bloom Filter, Kirsch
\textit{et al.}, and Cuckoo Filter using various use cases. The use cases
unearth weakness and strength of the filters. Secondly, we propose a malicious
URL detection mechanism using deepBF. We apply the evolutionary convolutional
neural network to identify the malicious URLs. The evolutionary convolutional
neural network is trained and tested with malicious URL datasets. The output is
tested in deepBF for accuracy. We have achieved many conclusions from our
experimental evaluation and results and are able to reach various conclusive
decisions which are presented in the article.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A State-of-the-art Survey of Artificial Neural Networks for Whole-slide Image Analysis:from Popular Convolutional Neural Networks to Potential Visual Transformers. (arXiv:2104.06243v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06243">
<div class="article-summary-box-inner">
<span><p>To increase the objectivity and accuracy of pathologists' work, artificial
neural network(ANN) methods have been generally needed in the segmentation,
classification, and detection of histopathological WSI. In this paper, WSI
analysis methods based on ANN are reviewed. Firstly, the development status of
WSI and ANN methods is introduced. Secondly, we summarize the common ANN
methods. Next, we discuss publicly available WSI datasets and evaluation
metrics. These ANN architectures for WSI processing are divided into classical
neural networks and deep neural networks(DNNs) and then analyzed. Finally, the
application prospect of the analytical method in this field is discussed. The
important potential method is Visual Transformers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An End-to-End Computer Vision Methodology for Quantitative Metallography. (arXiv:2104.11159v2 [cond-mat.mtrl-sci] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.11159">
<div class="article-summary-box-inner">
<span><p>Metallography is crucial for a proper assessment of material's properties. It
involves mainly the investigation of spatial distribution of grains and the
occurrence and characteristics of inclusions or precipitates. This work
presents an holistic artificial intelligence model for Anomaly Detection that
automatically quantifies the degree of anomaly of impurities in alloys. We
suggest the following examination process: (1) Deep semantic segmentation is
performed on the inclusions (based on a suitable metallographic database of
alloys and corresponding tags of inclusions), producing inclusions masks that
are saved into a separated database. (2) Deep image inpainting is performed to
fill the removed inclusions parts, resulting in 'clean' metallographic images,
which contain the background of grains. (3) Grains' boundaries are marked using
deep semantic segmentation (based on another metallographic database of
alloys), producing boundaries that are ready for further inspection on the
distribution of grains' size. (4) Deep anomaly detection and pattern
recognition is performed on the inclusions masks to determine spatial, shape
and area anomaly detection of the inclusions. Finally, the system recommends to
an expert on areas of interests for further examination. The performance of the
model is presented and analyzed based on few representative cases. Although the
models presented here were developed for metallography analysis, most of them
can be generalized to a wider set of problems in which anomaly detection of
geometrical objects is desired. All models as well as the data-sets that were
created for this work, are publicly available at
https://github.com/Scientific-Computing-Lab-NRCN/MLography.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Domain and Disentangled Face Manipulation with 3D Guidance. (arXiv:2104.11228v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.11228">
<div class="article-summary-box-inner">
<span><p>Face image manipulation via three-dimensional guidance has been widely
applied in various interactive scenarios due to its semantically-meaningful
understanding and user-friendly controllability. However, existing
3D-morphable-model-based manipulation methods are not directly applicable to
out-of-domain faces, such as non-photorealistic paintings, cartoon portraits,
or even animals, mainly due to the formidable difficulties in building the
model for each specific face domain. To overcome this challenge, we propose, as
far as we know, the first method to manipulate faces in arbitrary domains using
human 3DMM. This is achieved through two major steps: 1) disentangled mapping
from 3DMM parameters to the latent space embedding of a pre-trained StyleGAN2
that guarantees disentangled and precise controls for each semantic attribute;
and 2) cross-domain adaptation that bridges domain discrepancies and makes
human 3DMM applicable to out-of-domain faces by enforcing a consistent latent
space embedding. Experiments and comparisons demonstrate the superiority of our
high-quality semantic manipulation method on a variety of face domains with all
major 3D facial attributes controllable-pose, expression, shape, albedo, and
illumination. Moreover, we develop an intuitive editing interface to support
user-friendly control and instant feedback. Our project page is
https://cassiepython.github.io/cddfm3d/index.html
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Blurs Behave Like Ensembles: Spatial Smoothings to Improve Accuracy, Uncertainty, and Robustness. (arXiv:2105.12639v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.12639">
<div class="article-summary-box-inner">
<span><p>Neural network ensembles, such as Bayesian neural networks (BNNs), have shown
success in the areas of uncertainty estimation and robustness. However, a
crucial challenge prohibits their use in practice. BNNs require a large number
of predictions to produce reliable results, leading to a significant increase
in computational cost. To alleviate this issue, we propose spatial smoothing, a
method that spatially ensembles neighboring feature map points of convolutional
neural networks. By simply adding a few blur layers to the models, we
empirically show that spatial smoothing improves accuracy, uncertainty
estimation, and robustness of BNNs across a whole range of ensemble sizes. In
particular, BNNs incorporating spatial smoothing achieve high predictive
performance merely with a handful of ensembles. Moreover, this method also can
be applied to canonical deterministic neural networks to improve the
performances. A number of evidences suggest that the improvements can be
attributed to the stabilized feature maps and the smoothing of the loss
landscape. In addition, we provide a fundamental explanation for prior works -
namely, global average pooling, pre-activation, and ReLU6 - by addressing them
as special cases of spatial smoothing. These not only enhance accuracy, but
also improve uncertainty estimation and robustness by making the loss landscape
smoother in the same manner as spatial smoothing. The code is available at
https://github.com/xxxnell/spatial-smoothing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Hamilton-Jacobi PDEs and image denoising models with certain non-additive noise. (arXiv:2105.13997v2 [math.OC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.13997">
<div class="article-summary-box-inner">
<span><p>We consider image denoising problems formulated as variational problems. It
is known that Hamilton-Jacobi PDEs govern the solution of such optimization
problems when the noise model is additive. In this work, we address certain
non-additive noise models and show that they are also related to
Hamilton-Jacobi PDEs. These findings allow us to establish new connections
between additive and non-additive noise imaging models. Specifically, we study
how the solutions to these optimization problems depend on the parameters and
the observed images. We show that the optimal values are ruled by some
Hamilton-Jacobi PDEs, while the optimizers are characterized by the spatial
gradient of the solution to the Hamilton-Jacobi PDEs. Moreover, we use these
relations to investigate the asymptotic behavior of the variational model as
the parameter goes to infinity, that is, when the influence of the noise
vanishes. With these connections, some non-convex models for non-additive noise
can be solved by applying convex optimization algorithms to the equivalent
convex models for additive noise. Several numerical results are provided for
denoising problems with Poisson noise or multiplicative noise.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incremental False Negative Detection for Contrastive Learning. (arXiv:2106.03719v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03719">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning has recently shown great potential in vision tasks
through contrastive learning, which aims to discriminate each image, or
instance, in the dataset. However, such instance-level learning ignores the
semantic relationship among instances and sometimes undesirably repels the
anchor from the semantically similar samples, termed as "false negatives". In
this work, we show that the unfavorable effect from false negatives is more
significant for the large-scale datasets with more semantic concepts. To
address the issue, we propose a novel self-supervised contrastive learning
framework that incrementally detects and explicitly removes the false negative
samples. Specifically, following the training process, our method dynamically
detects increasing high-quality false negatives considering that the encoder
gradually improves and the embedding space becomes more semantically
structural. Next, we discuss two strategies to explicitly remove the detected
false negatives during contrastive learning. Extensive experiments show that
our framework outperforms other self-supervised contrastive learning methods on
multiple benchmarks in a limited resource setup.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">It Takes Two to Tango: Mixup for Deep Metric Learning. (arXiv:2106.04990v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04990">
<div class="article-summary-box-inner">
<span><p>Metric learning involves learning a discriminative representation such that
embeddings of similar classes are encouraged to be close, while embeddings of
dissimilar classes are pushed far apart. State-of-the-art methods focus mostly
on sophisticated loss functions or mining strategies. On the one hand, metric
learning losses consider two or more examples at a time. On the other hand,
modern data augmentation methods for classification consider two or more
examples at a time. The combination of the two ideas is under-studied.
</p>
<p>In this work, we aim to bridge this gap and improve representations using
mixup, which is a powerful data augmentation approach interpolating two or more
examples and corresponding target labels at a time. This task is challenging
because unlike classification, the loss functions used in metric learning are
not additive over examples, so the idea of interpolating target labels is not
straightforward. To the best of our knowledge, we are the first to investigate
mixing both examples and target labels for deep metric learning. We develop a
generalized formulation that encompasses existing metric learning loss
functions and modify it to accommodate for mixup, introducing Metric Mix, or
Metrix. We also introduce a new metric - utilization, to demonstrate that by
mixing examples during training, we are exploring areas of the embedding space
beyond the training classes, thereby improving representations. To validate the
effect of improved representations, we show that mixing inputs, intermediate
representations or embeddings along with target labels significantly
outperforms state-of-the-art metric learning methods on four benchmark deep
metric learning datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning stochastic object models from medical imaging measurements by use of advanced ambient generative adversarial networks. (arXiv:2106.14324v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.14324">
<div class="article-summary-box-inner">
<span><p>Purpose: To objectively assess new medical imaging technologies via
computer-simulations, it is important to account for the variability in the
ensemble of objects to be imaged. This source of variability can be described
by stochastic object models (SOMs). It is generally desirable to establish SOMs
from experimental imaging measurements acquired by use of a well-characterized
imaging system, but this task has remained challenging. Approach: A generative
adversarial network (GAN)-based method that employs AmbientGANs with modern
progressive or multiresolution training approaches is proposed. AmbientGANs
established using the proposed training procedure are systematically validated
in a controlled way using computer-simulated magnetic resonance imaging (MRI)
data corresponding to a stylized imaging system. Emulated single-coil
experimental MRI data are also employed to demonstrate the methods under less
stylized conditions. Results: The proposed AmbientGAN method can generate clean
images when the imaging measurements are contaminated by measurement noise.
When the imaging measurement data are incomplete, the proposed AmbientGAN can
reliably learn the distribution of the measurement components of the objects.
Conclusions: Both visual examinations and quantitative analyses, including
task-specific validations using the Hotelling observer, demonstrated that the
proposed AmbientGAN method holds promise to establish realistic SOMs from
imaging measurements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Crowd Localization with Multi-focus Gaussian Neighborhood Attention and a Large-Scale Benchmark. (arXiv:2107.08645v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.08645">
<div class="article-summary-box-inner">
<span><p>Video crowd localization is a crucial yet challenging task, which aims to
estimate exact locations of human heads in the given crowded videos. To model
spatial-temporal dependencies of human mobility, we propose a multi-focus
Gaussian neighborhood attention (GNA), which can effectively exploit long-range
correspondences while maintaining the spatial topological structure of the
input videos. In particular, our GNA can also capture the scale variation of
human heads well using the equipped multi-focus mechanism. Based on the
multi-focus GNA, we develop a unified neural network called GNANet to
accurately locate head centers in video clips by fully aggregating
spatial-temporal information via a scene modeling module and a context
cross-attention module. Moreover, to facilitate future researches in this
field, we introduce a large-scale crowd video benchmark named SenseCrowd, which
consists of 60K+ frames captured in various surveillance scenarios and 2M+ head
annotations. Finally, we conduct extensive experiments on three datasets
including our SenseCrowd, and the experiment results show that the proposed
method is capable to achieve state-of-the-art performance for both video crowd
localization and counting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DCL: Differential Contrastive Learning for Geometry-Aware Depth Synthesis. (arXiv:2107.13087v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.13087">
<div class="article-summary-box-inner">
<span><p>We describe a method for unpaired realistic depth synthesis that learns
diverse variations from the real-world depth scans and ensures geometric
consistency between the synthetic and synthesized depth. The synthesized
realistic depth can then be used to train task-specific networks facilitating
label transfer from the synthetic domain. Unlike existing image synthesis
pipelines, where geometries are mostly ignored, we treat geometries carried by
the depth scans based on their own existence. We propose differential
contrastive learning that explicitly enforces the underlying geometric
properties to be invariant regarding the real variations been learned. The
resulting depth synthesis method is task-agnostic, and we demonstrate the
effectiveness of the proposed synthesis method by extensive evaluations on
real-world geometric reasoning tasks. The networks trained with the depth
synthesized by our method consistently achieve better performance across a wide
range of tasks than state of the art, and can even surpass the networks
supervised with full real-world annotations when slightly fine-tuned, showing
good transferability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards to Robust and Generalized Medical Image Segmentation Framework. (arXiv:2108.03823v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03823">
<div class="article-summary-box-inner">
<span><p>Deep learning-based computer-aided diagnosis is gradually deployed to review
and analyze medical images. However, this paradigm is restricted in real-world
clinical applications due to the poor robustness and generalization. The issue
is more sinister with a lack of training data. In this paper, we address the
challenge from the transfer learning point of view. Different from the common
setting that transferring knowledge from the natural image domain to the
medical image domain, we find the knowledge from the same domain further boosts
the model robustness and generalization. Therefore, we propose a novel
two-stage framework for robust generalized medical image segmentation. Firstly,
an unsupervised tile-wise autoencoder pretraining architecture is proposed to
learn local and global knowledge. Secondly, the downstream segmentation model
coupled with an auxiliary reconstruction network is designed. The
reconstruction branch encourages the model to capture more general semantic
features. Experiments of lung segmentation on multi chest X-ray datasets are
conducted. Comprehensive results demonstrate the superior robustness of the
proposed framework to corruption and high generalization performance on unseen
datasets, especially under the scenario of the limited training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ARCH++: Animation-Ready Clothed Human Reconstruction Revisited. (arXiv:2108.07845v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07845">
<div class="article-summary-box-inner">
<span><p>We present ARCH++, an image-based method to reconstruct 3D avatars with
arbitrary clothing styles. Our reconstructed avatars are animation-ready and
highly realistic, in both the visible regions from input views and the unseen
regions. While prior work shows great promise of reconstructing animatable
clothed humans with various topologies, we observe that there exist fundamental
limitations resulting in sub-optimal reconstruction quality. In this paper, we
revisit the major steps of image-based avatar reconstruction and address the
limitations with ARCH++. First, we introduce an end-to-end point based geometry
encoder to better describe the semantics of the underlying 3D human body, in
replacement of previous hand-crafted features. Second, in order to address the
occupancy ambiguity caused by topological changes of clothed humans in the
canonical pose, we propose a co-supervising framework with cross-space
consistency to jointly estimate the occupancy in both the posed and canonical
spaces. Last, we use image-to-image translation networks to further refine
detailed geometry and texture on the reconstructed surface, which improves the
fidelity and consistency across arbitrary viewpoints. In the experiments, we
demonstrate improvements over the state of the art on both public benchmarks
and user studies in reconstruction quality and realism.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">S4-Crowd: Semi-Supervised Learning with Self-Supervised Regularisation for Crowd Counting. (arXiv:2108.13969v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13969">
<div class="article-summary-box-inner">
<span><p>Crowd counting has drawn more attention because of its wide application in
smart cities. Recent works achieved promising performance but relied on the
supervised paradigm with expensive crowd annotations. To alleviate annotation
cost, in this work we proposed a semi-supervised learning framework S4-Crowd,
which can leverage both unlabeled/labeled data for robust crowd modelling. In
the unsupervised pathway, two self-supervised losses were proposed to simulate
the crowd variations such as scale, illumination, etc., based on which and the
supervised information pseudo labels were generated and gradually refined. We
also proposed a crowd-driven recurrent unit Gated-Crowd-Recurrent-Unit (GCRU),
which can preserve discriminant crowd information by extracting second-order
statistics, yielding pseudo labels with improved quality. A joint loss
including both unsupervised/supervised information was proposed, and a dynamic
weighting strategy was employed to balance the importance of the unsupervised
loss and supervised loss at different training stages. We conducted extensive
experiments on four popular crowd counting datasets in semi-supervised
settings. Experimental results suggested the effectiveness of each proposed
component in our S4-Crowd framework. Our method also outperformed other
state-of-the-art semi-supervised learning approaches on these crowd datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Binaural SoundNet: Predicting Semantics, Depth and Motion with Binaural Sounds. (arXiv:2109.02763v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02763">
<div class="article-summary-box-inner">
<span><p>Humans can robustly recognize and localize objects by using visual and/or
auditory cues. While machines are able to do the same with visual data already,
less work has been done with sounds. This work develops an approach for scene
understanding purely based on binaural sounds. The considered tasks include
predicting the semantic masks of sound-making objects, the motion of
sound-making objects, and the depth map of the scene. To this aim, we propose a
novel sensor setup and record a new audio-visual dataset of street scenes with
eight professional binaural microphones and a 360-degree camera. The
co-existence of visual and audio cues is leveraged for supervision transfer. In
particular, we employ a cross-modal distillation framework that consists of
multiple vision teacher methods and a sound student method -- the student
method is trained to generate the same results as the teacher methods do. This
way, the auditory system can be trained without using human annotations. To
further boost the performance, we propose another novel auxiliary task, coined
Spatial Sound Super-Resolution, to increase the directional resolution of
sounds. We then formulate the four tasks into one end-to-end trainable
multi-tasking network aiming to boost the overall performance. Experimental
results show that 1) our method achieves good results for all four tasks, 2)
the four tasks are mutually beneficial -- training them together achieves the
best performance, 3) the number and orientation of microphones are both
important, and 4) features learned from the standard spectrogram and features
obtained by the classic signal processing pipeline are complementary for
auditory perception tasks. The data and code are released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AirLoop: Lifelong Loop Closure Detection. (arXiv:2109.08975v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08975">
<div class="article-summary-box-inner">
<span><p>Loop closure detection is an important building block that ensures the
accuracy and robustness of simultaneous localization and mapping (SLAM)
systems. Due to their generalization ability, CNN-based approaches have
received increasing attention. Although they normally benefit from training on
datasets that are diverse and reflective of the environments, new environments
often emerge after the model is deployed. It is therefore desirable to
incorporate the data newly collected during operation for incremental learning.
Nevertheless, simply finetuning the model on new data is infeasible since it
may cause the model's performance on previously learned data to degrade over
time, which is also known as the problem of catastrophic forgetting. In this
paper, we present AirLoop, a method that leverages techniques from lifelong
learning to minimize forgetting when training loop closure detection models
incrementally. We experimentally demonstrate the effectiveness of AirLoop on
TartanAir, Nordland, and RobotCar datasets. To the best of our knowledge,
AirLoop is one of the first works to achieve lifelong learning of deep loop
closure detectors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CaTGrasp: Learning Category-Level Task-Relevant Grasping in Clutter from Simulation. (arXiv:2109.09163v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09163">
<div class="article-summary-box-inner">
<span><p>Task-relevant grasping is critical for industrial assembly, where downstream
manipulation tasks constrain the set of valid grasps. Learning how to perform
this task, however, is challenging, since task-relevant grasp labels are hard
to define and annotate. There is also yet no consensus on proper
representations for modeling or off-the-shelf tools for performing
task-relevant grasps. This work proposes a framework to learn task-relevant
grasping for industrial objects without the need of time-consuming real-world
data collection or manual annotation. To achieve this, the entire framework is
trained solely in simulation, including supervised training with synthetic
label generation and self-supervised, hand-object interaction. In the context
of this framework, this paper proposes a novel, object-centric canonical
representation at the category level, which allows establishing dense
correspondence across object instances and transferring task-relevant grasps to
novel instances. Extensive experiments on task-relevant grasping of
densely-cluttered industrial objects are conducted in both simulation and
real-world setups, demonstrating the effectiveness of the proposed framework.
Code and data are available at https://sites.google.com/view/catgrasp.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unseen Object Amodal Instance Segmentation via Hierarchical Occlusion Modeling. (arXiv:2109.11103v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11103">
<div class="article-summary-box-inner">
<span><p>Instance-aware segmentation of unseen objects is essential for a robotic
system in an unstructured environment. Although previous works achieved
encouraging results, they were limited to segmenting the only visible regions
of unseen objects. For robotic manipulation in a cluttered scene, amodal
perception is required to handle the occluded objects behind others. This paper
addresses Unseen Object Amodal Instance Segmentation (UOAIS) to detect 1)
visible masks, 2) amodal masks, and 3) occlusions on unseen object instances.
For this, we propose a Hierarchical Occlusion Modeling (HOM) scheme designed to
reason about the occlusion by assigning a hierarchy to a feature fusion and
prediction order. We evaluated our method on three benchmarks (tabletop,
indoors, and bin environments) and achieved state-of-the-art (SOTA)
performance. Robot demos for picking up occluded objects, codes, and datasets
are available at https://sites.google.com/view/uoais
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Domain Adaptation for LiDAR Panoptic Segmentation. (arXiv:2109.15286v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15286">
<div class="article-summary-box-inner">
<span><p>Scene understanding is a pivotal task for autonomous vehicles to safely
navigate in the environment. Recent advances in deep learning enable accurate
semantic reconstruction of the surroundings from LiDAR data. However, these
models encounter a large domain gap while deploying them on vehicles equipped
with different LiDAR setups which drastically decreases their performance.
Fine-tuning the model for every new setup is infeasible due to the expensive
and cumbersome process of recording and manually labeling new data.
Unsupervised Domain Adaptation (UDA) techniques are thus essential to fill this
domain gap and retain the performance of models on new sensor setups without
the need for additional data labeling. In this paper, we propose AdaptLPS, a
novel UDA approach for LiDAR panoptic segmentation that leverages task-specific
knowledge and accounts for variation in the number of scan lines, mounting
position, intensity distribution, and environmental conditions. We tackle the
UDA task by employing two complementary domain adaptation strategies,
data-based and model-based. While data-based adaptations reduce the domain gap
by processing the raw LiDAR scans to resemble the scans in the target domain,
model-based techniques guide the network in extracting features that are
representative for both domains. Extensive evaluations on three pairs of
real-world autonomous driving datasets demonstrate that AdaptLPS outperforms
existing UDA approaches by up to 6.41 pp in terms of the PQ score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">See Yourself in Others: Attending Multiple Tasks for Own Failure Detection. (arXiv:2110.02549v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02549">
<div class="article-summary-box-inner">
<span><p>Autonomous robots deal with unexpected scenarios in real environments. Given
input images, various visual perception tasks can be performed, e.g., semantic
segmentation, depth estimation and normal estimation. These different tasks
provide rich information for the whole robotic perception system. All tasks
have their own characteristics while sharing some latent correlations. However,
some of the task predictions may suffer from the unreliability dealing with
complex scenes and anomalies. We propose an attention-based failure detection
approach by exploiting the correlations among multiple tasks. The proposed
framework infers task failures by evaluating the individual prediction, across
multiple visual perception tasks for different regions in an image. The
formulation of the evaluations is based on an attention network supervised by
multi-task uncertainty estimation and their corresponding prediction errors.
Our proposed framework generates more accurate estimations of the prediction
error for the different task's predictions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reducing the Covariate Shift by Mirror Samples in Cross Domain Alignment. (arXiv:2110.06448v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06448">
<div class="article-summary-box-inner">
<span><p>Eliminating the covariate shift cross domains is one of the common methods to
deal with the issue of domain shift in visual unsupervised domain adaptation.
However, current alignment methods, especially the prototype based or
sample-level based methods neglect the structural properties of the underlying
distribution and even break the condition of covariate shift. To relieve the
limitations and conflicts, we introduce a novel concept named (virtual) mirror,
which represents the equivalent sample in another domain. The equivalent sample
pairs, named mirror pairs reflect the natural correspondence of the empirical
distributions. Then a mirror loss, which aligns the mirror pairs cross domains,
is constructed to enhance the alignment of the domains. The proposed method
does not distort the internal structure of the underlying distribution. We also
provide theoretical proof that the mirror samples and mirror loss have better
asymptotic properties in reducing the domain shift. By applying the virtual
mirror and mirror loss to the generic unsupervised domain adaptation model, we
achieved consistent superior performance on several mainstream benchmarks. Code
is available at https://github.com/CTI-VISION/Mirror-Sample
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A comprehensive review of Binary Neural Network. (arXiv:2110.06804v3 [cs.NE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06804">
<div class="article-summary-box-inner">
<span><p>Deep learning (DL) has recently changed the development of intelligent
systems and is widely adopted in many real-life applications. Despite their
various benefits and potentials, there is a high demand for DL processing in
different computationally limited and energy-constrained devices. It is natural
to study game-changing technologies such as Binary Neural Networks (BNN) to
increase deep learning capabilities. Recently remarkable progress has been made
in BNN since they can be implemented and embedded on tiny restricted devices
and save a significant amount of storage, computation cost, and energy
consumption. However, nearly all BNN acts trade with extra memory, computation
cost, and higher performance. This article provides a complete overview of
recent developments in BNN. This article focuses exclusively on 1-bit
activations and weights 1-bit convolution networks, contrary to previous
surveys in which low-bit works are mixed in. It conducted a complete
investigation of BNN's development -from their predecessors to the latest BNN
algorithms/techniques, presenting a broad design pipeline and discussing each
module's variants. Along the way, it examines BNN (a) purpose: their early
successes and challenges; (b) BNN optimization: selected representative works
that contain essential optimization techniques; (c) deployment: open-source
frameworks for BNN modeling and development; (d) terminal: efficient computing
architectures and devices for BNN and (e) applications: diverse applications
with BNN. Moreover, this paper discusses potential directions and future
research opportunities in each section.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ADMM-DAD net: a deep unfolding network for analysis compressed sensing. (arXiv:2110.06986v4 [cs.IT] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06986">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a new deep unfolding neural network based on the
ADMM algorithm for analysis Compressed Sensing. The proposed network jointly
learns a redundant analysis operator for sparsification and reconstructs the
signal of interest. We compare our proposed network with a state-of-the-art
unfolded ISTA decoder, that also learns an orthogonal sparsifier. Moreover, we
consider not only image, but also speech datasets as test examples.
Computational experiments demonstrate that our proposed network outperforms the
state-of-the-art deep unfolding network, consistently for both real-world image
and speech datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gray Matter Segmentation in Ultra High Resolution 7 Tesla ex vivo T2w MRI of Human Brain Hemispheres. (arXiv:2110.07711v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07711">
<div class="article-summary-box-inner">
<span><p>Ex vivo MRI of the brain provides remarkable advantages over in vivo MRI for
visualizing and characterizing detailed neuroanatomy. However, automated
cortical segmentation methods in ex vivo MRI are not well developed, primarily
due to limited availability of labeled datasets, and heterogeneity in scanner
hardware and acquisition protocols. In this work, we present a high resolution
7 Tesla dataset of 32 ex vivo human brain specimens. We benchmark the cortical
mantle segmentation performance of nine neural network architectures, trained
and evaluated using manually-segmented 3D patches sampled from specific
cortical regions, and show excellent generalizing capabilities across whole
brain hemispheres in different specimens, and also on unseen images acquired at
different magnetic field strength and imaging sequences. Finally, we provide
cortical thickness measurements across key regions in 3D ex vivo human brain
images. Our code and processed datasets are publicly available at
https://github.com/Pulkit-Khandelwal/picsl-ex-vivo-segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shape and Reflectance Reconstruction in Uncontrolled Environments by Differentiable Rendering. (arXiv:2110.12975v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12975">
<div class="article-summary-box-inner">
<span><p>Simultaneous reconstruction of geometry and reflectance properties in
uncontrolled environments remains a challenging problem. In this paper, we
propose an efficient method to reconstruct the scene's 3D geometry and
reflectance from multi-view photography using conventional hand-held cameras.
Our method automatically builds a virtual scene in a differentiable rendering
system that roughly matches the real world's scene parameters, optimized by
minimizing photometric objectives alternatingly and stochastically. With the
optimal scene parameters evaluated, photo-realistic novel views for various
viewing angles and distances can then be generated by our approach. We present
the results of captured scenes with complex geometry and various reflection
types. Our method also shows superior performance compared to state-of-the-art
alternatives in novel view synthesis visually and quantitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Model Fusion of Heterogeneous Neural Networks via Cross-Layer Alignment. (arXiv:2110.15538v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15538">
<div class="article-summary-box-inner">
<span><p>Layer-wise model fusion via optimal transport, named OTFusion, applies soft
neuron association for unifying different pre-trained networks to save
computational resources. While enjoying its success, OTFusion requires the
input networks to have the same number of layers. To address this issue, we
propose a novel model fusion framework, named CLAFusion, to fuse neural
networks with a different number of layers, which we refer to as heterogeneous
neural networks, via cross-layer alignment. The cross-layer alignment problem,
which is an unbalanced assignment problem, can be solved efficiently using
dynamic programming. Based on the cross-layer alignment, our framework balances
the number of layers of neural networks before applying layer-wise model
fusion. Our experiments indicate that CLAFusion, with an extra finetuning
process, improves the accuracy of residual networks on CIFAR10 and CIFAR100
datasets. Furthermore, we explore its practical usage for model compression and
knowledge distillation when applying to the teacher-student setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Loop closure detection using local 3D deep descriptors. (arXiv:2111.00440v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00440">
<div class="article-summary-box-inner">
<span><p>We present a simple yet effective method to address loop closure detection in
simultaneous localisation and mapping using local 3D deep descriptors (L3Ds).
L3Ds are emerging compact representations of patches extracted from point
clouds that are learnt from data using a deep learning algorithm. We propose a
novel overlap measure for loop detection by computing the metric error between
points that correspond to mutually-nearest-neighbour descriptors after
registering the loop candidate point cloud by its estimated relative pose. This
novel approach enables us to accurately detect loops and estimate six
degrees-of-freedom poses in the case of small overlaps. We compare our
L3D-based loop closure approach with recent approaches on LiDAR data and
achieve state-of-the-art loop closure detection accuracy. Additionally, we
embed our loop closure approach in RESLAM, a recent edge-based SLAM system, and
perform the evaluation on real-world RGBD-TUM and synthetic ICL datasets. Our
approach enables RESLAM to achieve a better localisation accuracy compared to
its original loop closure strategy. Our project page is available at
github.com/yiming107/l3d_loop_closure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Disentangle Scenes for Person Re-identification. (arXiv:2111.05476v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.05476">
<div class="article-summary-box-inner">
<span><p>There are many challenging problems in the person re-identification (ReID)
task, such as the occlusion and scale variation. Existing works usually tried
to solve them by employing a one-branch network. This one-branch network needs
to be robust to various challenging problems, which makes this network
overburdened. This paper proposes to divide-and-conquer the ReID task. For this
purpose, we employ several self-supervision operations to simulate different
challenging problems and handle each challenging problem using different
networks. Concretely, we use the random erasing operation and propose a novel
random scaling operation to generate new images with controllable
characteristics. A general multi-branch network, including one master branch
and two servant branches, is introduced to handle different scenes. These
branches learn collaboratively and achieve different perceptive abilities. In
this way, the complex scenes in the ReID task are effectively disentangled, and
the burden of each branch is relieved. The results from extensive experiments
demonstrate that the proposed method achieves state-of-the-art performances on
three ReID benchmarks and two occluded ReID benchmarks. Ablation study also
shows that the proposed scheme and operations significantly improve the
performance in various scenes. The code is available at
https://git.openi.org.cn/zangxh/LDS.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sci-Net: a Scale Invariant Model for Buildings Segmentation from Aerial Images. (arXiv:2111.06812v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.06812">
<div class="article-summary-box-inner">
<span><p>Buildings' segmentation is a fundamental task in the field of earth
observation and aerial imagery analysis. Most existing deep learning-based
methods in the literature can be applied to fixed or narrow-ranged spatial
resolution imagery. In practical scenarios, users deal with a broad spectrum of
image resolutions. Thus, a given aerial image often needs to be re-sampled to
match the spatial resolution of the dataset used to train the deep learning
model, which results in a degradation in segmentation performance. To overcome
this, we propose a Scale-invariant Neural Network (Sci-Net) that can segment
buildings present in aerial images at different spatial resolutions.
Specifically, our approach leverages UNet hierarchical representations and
dilated convolutions to extract fine-grained multi-scale representations. Our
method significantly outperforms other state of the art models on the Open
Cities AI dataset with a steady improvements margin across different
resolutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Finding Optimal Tangent Points for Reducing Distortions of Hard-label Attacks. (arXiv:2111.07492v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.07492">
<div class="article-summary-box-inner">
<span><p>One major problem in black-box adversarial attacks is the high query
complexity in the hard-label attack setting, where only the top-1 predicted
label is available. In this paper, we propose a novel geometric-based approach
called Tangent Attack (TA), which identifies an optimal tangent point of a
virtual hemisphere located on the decision boundary to reduce the distortion of
the attack. Assuming the decision boundary is locally flat, we theoretically
prove that the minimum $\ell_2$ distortion can be obtained by reaching the
decision boundary along the tangent line passing through such tangent point in
each iteration. To improve the robustness of our method, we further propose a
generalized method which replaces the hemisphere with a semi-ellipsoid to adapt
to curved decision boundaries. Our approach is free of pre-training. Extensive
experiments conducted on the ImageNet and CIFAR-10 datasets demonstrate that
our approach can consume only a small number of queries to achieve the
low-magnitude distortion. The implementation source code is released online at
https://github.com/machanic/TangentAttack.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lidar with Velocity: Motion Distortion Correction of Point Clouds from Oscillating Scanning Lidars. (arXiv:2111.09497v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.09497">
<div class="article-summary-box-inner">
<span><p>Lidar point cloud distortion from moving object is an important problem in
autonomous driving, and recently becomes even more demanding with the emerging
of newer lidars, which feature back-and-forth scanning patterns. Accurately
estimating moving object velocity would not only provide a tracking capability
but also correct the point cloud distortion with more accurate description of
the moving object. Since lidar measures the time-of-flight distance but with a
sparse angular resolution, the measurement is precise in the radial measurement
but lacks angularly. Camera on the other hand provides a dense angular
resolution. In this paper, Gaussian-based lidar and camera fusion is proposed
to estimate the full velocity and correct the lidar distortion. A probabilistic
Kalman-filter framework is provided to track the moving objects, estimate their
velocities and simultaneously correct the point clouds distortions. The
framework is evaluated on real road data and the fusion method outperforms the
traditional ICP-based and point-cloud only method. The complete working
framework is open-sourced
(https://github.com/ISEE-Technology/lidar-with-velocity) to accelerate the
adoption of the emerging lidars.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lebanon Solar Rooftop Potential Assessment using Buildings Segmentation from Aerial Images. (arXiv:2111.11397v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11397">
<div class="article-summary-box-inner">
<span><p>Estimating the solar rooftop potential of buildings' rooftops at a large
scale is a fundamental step for every country to utilize its solar power
efficiently. However, such estimation becomes time-consuming and costly if done
through on-site measurements. This paper uses deep learning-based multi-class
instance segmentation to extract buildings' footprints from satellite images.
Hence, we introduce Lebanon's first complete and comprehensive buildings'
footprints map. Furthermore, we propose a photovoltaic panels placement
algorithm to estimate the solar potential of every rooftop, which results in
Lebanon's first buildings' solar rooftop potential map too. Finally, we report
total and average solar rooftop potential per district and localize regions
corresponding to the highest solar rooftop potential yield. Conducted analysis
reveal solar rooftop potential urban patterns and provide policymakers and key
stakeholders with tangible insights.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting full Resolution Feature Context for Liver Tumor and Vessel Segmentation via Integrate Framework: Application to Liver Tumor and Vessel 3D Reconstruction under embedded microprocessor. (arXiv:2111.13299v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13299">
<div class="article-summary-box-inner">
<span><p>Liver cancer is one of the most common malignant diseases in the world.
Segmentation and labeling of liver tumors and blood vessels in CT images can
provide convenience for doctors in liver tumor diagnosis and surgical
intervention. In the past decades, many state-of-the-art medical image
segmentation algorithms appeared during this period. With the development of
embedded devices, embedded deployment for medical segmentation and automatic
reconstruction brings prospects for future automated surgical tasks. Yet, most
of the existing segmentation methods mostly care about the spatial feature
context and have a perception defect in the semantic relevance of medical
images, which significantly affects the segmentation accuracy of liver tumors
and blood vessels. Deploying large and complex models into embedded devices
requires a reasonable trade-off between model accuracy, reasoning speed and
model capacity. Given these problems, we introduce a multi-scale feature fusion
network called TransFusionNet based on Transformer. This network achieved very
competitive performance for liver vessel and liver tumor segmentation tasks,
meanwhile it can improve the recognition of morphologic margins of liver tumors
by exploiting the global information of CT images. Experiments show that in
vessel segmentation task TransFusionNet achieved mean Dice coefficients of
0.899 and in liver tumor segmentation task TransFusionNet achieved mean Dice
coefficients of 0.961. Compared with the state-of-the-art framework, our model
achieves the best segmentation result. In addition, we deployed the model into
an embedded micro-structure and constructed an integrated model for liver tumor
vascular segmentation and reconstruction. This proprietary structure will be
the exclusive component of the future medical field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Attentional Guided Image Filtering. (arXiv:2112.06401v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06401">
<div class="article-summary-box-inner">
<span><p>Guided filter is a fundamental tool in computer vision and computer graphics
which aims to transfer structure information from guidance image to target
image. Most existing methods construct filter kernels from the guidance itself
without considering the mutual dependency between the guidance and the target.
However, since there typically exist significantly different edges in the two
images, simply transferring all structural information of the guidance to the
target would result in various artifacts. To cope with this problem, we propose
an effective framework named deep attentional guided image filtering, the
filtering process of which can fully integrate the complementary information
contained in both images. Specifically, we propose an attentional kernel
learning module to generate dual sets of filter kernels from the guidance and
the target, respectively, and then adaptively combine them by modeling the
pixel-wise dependency between the two images. Meanwhile, we propose a
multi-scale guided image filtering module to progressively generate the
filtering result with the constructed kernels in a coarse-to-fine manner.
Correspondingly, a multi-scale fusion strategy is introduced to reuse the
intermediate results in the coarse-to-fine process. Extensive experiments show
that the proposed framework compares favorably with the state-of-the-art
methods in a wide range of guided image filtering applications, such as guided
super-resolution, cross-modality restoration, texture removal, and semantic
segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic-Based Few-Shot Learning by Interactive Psychometric Testing. (arXiv:2112.09201v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09201">
<div class="article-summary-box-inner">
<span><p>Few-shot classification tasks aim to classify images in query sets based on
only a few labeled examples in support sets. Most studies usually assume that
each image in a task has a single and unique class association. Under these
assumptions, these algorithms may not be able to identify the proper class
assignment when there is no exact matching between support and query classes.
For example, given a few images of lions, bikes, and apples to classify a
tiger. However, in a more general setting, we could consider the higher-level
concept, the large carnivores, to match the tiger to the lion for semantic
classification. Existing studies rarely considered this situation due to the
incompatibility of label-based supervision with complex conception
relationships. In this work, we advance the few-shot learning towards this more
challenging scenario, the semantic-based few-shot learning, and propose a
method to address the paradigm by capturing the inner semantic relationships
using interactive psychometric learning. The experiment results on the
CIFAR-100 dataset show the superiority of our method for the semantic-based
few-shot learning compared to the baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Deep Learning Based Workflow for Detection of Lung Nodules With Chest Radiograph. (arXiv:2112.10184v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10184">
<div class="article-summary-box-inner">
<span><p>PURPOSE: This study aimed to develop a deep learning-based tool to detect and
localize lung nodules with chest radiographs(CXRs). We expected it to enhance
the efficiency of interpreting CXRs and reduce the possibilities of delayed
diagnosis of lung cancer.
</p>
<p>MATERIALS AND METHODS: We collected CXRs from NCKUH database and VBD, an
open-source medical image dataset, as our training and validation data. A
number of CXRs from the Ministry of Health and Welfare(MOHW) database served as
our test data. We built a segmentation model to identify lung areas from CXRs,
and sliced them into 16 patches. Physicians labeled the CXRs by clicking the
patches. These labeled patches were then used to train and fine-tune a deep
neural network(DNN) model, classifying the patches as positive or negative.
Finally, we test the DNN model with the lung patches of CXRs from MOHW.
</p>
<p>RESULTS: Our segmentation model identified the lung regions well from the
whole CXR. The Intersection over Union(IoU) between the ground truth and the
segmentation result was 0.9228. In addition, our DNN model achieved a
sensitivity of 0.81, specificity of 0.82, and AUROC of 0.869 in 98 of 125
cases. For the other 27 difficult cases, the sensitivity was 0.54, specificity
0.494, and AUROC 0.682. Overall, we obtained a sensitivity of 0.78, specificity
of 0.79, and AUROC 0.837.
</p>
<p>CONCLUSIONS: Our two-step workflow is comparable to state-of-the-art
algorithms in the sensitivity and specificity of localizing lung nodules from
CXRs. Notably, our workflow provides an efficient way for specialists to label
the data, which is valuable for relevant researches because of the relative
rarity of labeled medical image data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">JoJoGAN: One Shot Face Stylization. (arXiv:2112.11641v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11641">
<div class="article-summary-box-inner">
<span><p>A style mapper applies some fixed style to its input images (so, for example,
taking faces to cartoons). This paper describes a simple procedure -- JoJoGAN
-- to learn a style mapper from a single example of the style. JoJoGAN uses a
GAN inversion procedure and StyleGAN's style-mixing property to produce a
substantial paired dataset from a single example style. The paired dataset is
then used to fine-tune a StyleGAN. An image can then be style mapped by
GAN-inversion followed by the fine-tuned StyleGAN. JoJoGAN needs just one
reference and as little as 30 seconds of training time. JoJoGAN can use extreme
style references (say, animal faces) successfully. Furthermore, one can control
what aspects of the style are used and how much of the style is applied.
Qualitative and quantitative evaluation show that JoJoGAN produces high quality
high resolution images that vastly outperform the current state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is it Possible to Predict MGMT Promoter Methylation from Brain Tumor MRI Scans using Deep Learning Models?. (arXiv:2201.06086v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06086">
<div class="article-summary-box-inner">
<span><p>Glioblastoma is a common brain malignancy that tends to occur in older adults
and is almost always lethal. The effectiveness of chemotherapy, being the
standard treatment for most cancer types, can be improved if a particular
genetic sequence in the tumor known as MGMT promoter is methylated. However, to
identify the state of the MGMT promoter, the conventional approach is to
perform a biopsy for genetic analysis, which is time and effort consuming. A
couple of recent publications proposed a connection between the MGMT promoter
state and the MRI scans of the tumor and hence suggested the use of deep
learning models for this purpose. Therefore, in this work, we use one of the
most extensive datasets, BraTS 2021, to study the potency of employing deep
learning solutions, including 2D and 3D CNN models and vision transformers.
After conducting a thorough analysis of the models' performance, we concluded
that there seems to be no connection between the MRI scans and the state of the
MGMT promoter.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MVPTR: Multi-Stage Vision-Language Pre-Training via Multi-Level Semantic Alignment. (arXiv:2201.12596v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12596">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a Multi-stage Vision-language Pre-TRaining (MVPTR)
framework to learn cross-modality representation via multi-level semantic
alignment. We introduce concepts in both modalities to construct two-level
semantic representations for language and vision. Based on the multi-level
input, we train the cross-modality model in two stages, namely, uni-modal
learning and cross-modal learning. The former stage enforces within-modality
interactions to learn multi-level semantics for each single modality. The
latter stage enforces interactions across modalities via both coarse-grain and
fine-grain semantic alignment tasks. Image-text matching and masked language
modeling are then used to further optimize the pre-training model. Our model
generates the-state-of-the-art results on several vision and language tasks.
Our code is available at https://github.com/Junction4Nako/mvp_pytorch.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probabilistically Robust Learning: Balancing Average- and Worst-case Performance. (arXiv:2202.01136v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01136">
<div class="article-summary-box-inner">
<span><p>Many of the successes of machine learning are based on minimizing an averaged
loss function. However, it is well-known that this paradigm suffers from
robustness issues that hinder its applicability in safety-critical domains.
These issues are often addressed by training against worst-case perturbations
of data, a technique known as adversarial training. Although empirically
effective, adversarial training can be overly conservative, leading to
unfavorable trade-offs between nominal performance and robustness. To this end,
in this paper we propose a framework called probabilistic robustness that
bridges the gap between the accurate, yet brittle average case and the robust,
yet conservative worst case by enforcing robustness to most rather than to all
perturbations. From a theoretical point of view, this framework overcomes the
trade-offs between the performance and the sample-complexity of worst-case and
average-case learning. From a practical point of view, we propose a novel
algorithm based on risk-aware optimization that effectively balances average-
and worst-case performance at a considerably lower computational cost relative
to adversarial training. Our results on MNIST, CIFAR-10, and SVHN illustrate
the advantages of this framework on the spectrum from average- to worst-case
robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simulation-to-Reality domain adaptation for offline 3D object annotation on pointclouds with correlation alignment. (arXiv:2202.02666v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02666">
<div class="article-summary-box-inner">
<span><p>Annotating objects with 3D bounding boxes in LiDAR pointclouds is a costly
human driven process in an autonomous driving perception system. In this paper,
we present a method to semi-automatically annotate real-world pointclouds
collected by deployment vehicles using simulated data. We train a 3D object
detector model on labeled simulated data from CARLA jointly with real world
pointclouds from our target vehicle. The supervised object detection loss is
augmented with a CORAL loss term to reduce the distance between labeled
simulated and unlabeled real pointcloud feature representations. The goal here
is to learn representations that are invariant to simulated (labeled) and
real-world (unlabeled) target domains. We also provide an updated survey on
domain adaptation methods for pointclouds.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-modal Sensor Fusion for Auto Driving Perception: A Survey. (arXiv:2202.02703v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02703">
<div class="article-summary-box-inner">
<span><p>Multi-modal fusion is a fundamental task for the perception of an autonomous
driving system, which has recently intrigued many researchers. However,
achieving a rather good performance is not an easy task due to the noisy raw
data, underutilized information, and the misalignment of multi-modal sensors.
In this paper, we provide a literature review of the existing multi-modal-based
methods for perception tasks in autonomous driving. Generally, we make a
detailed analysis including over 50 papers leveraging perception sensors
including LiDAR and camera trying to solve object detection and semantic
segmentation tasks. Different from traditional fusion methodology for
categorizing fusion models, we propose an innovative way that divides them into
two major classes, four minor classes by a more reasonable taxonomy in the view
of the fusion stage. Moreover, we dive deep into the current fusion methods,
focusing on the remaining problems and open-up discussions on the potential
research opportunities. In conclusion, what we expect to do in this paper is to
present a new taxonomy of multi-modal fusion methods for the autonomous driving
perception tasks and provoke thoughts of the fusion-based techniques in the
future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FCM-DNN: diagnosing coronary artery disease by deep accuracy Fuzzy C-Means clustering model. (arXiv:2202.04645v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04645">
<div class="article-summary-box-inner">
<span><p>Cardiovascular disease is one of the most challenging diseases in middle-aged
and older people, which causes high mortality. Coronary artery disease (CAD) is
known as a common cardiovascular disease. A standard clinical tool for
diagnosing CAD is angiography. The main challenges are dangerous side effects
and high angiography costs. Today, the development of artificial
intelligence-based methods is a valuable achievement for diagnosing disease.
Hence, in this paper, artificial intelligence methods such as neural network
(NN), deep neural network (DNN), and Fuzzy C-Means clustering combined with
deep neural network (FCM-DNN) are developed for diagnosing CAD on a cardiac
magnetic resonance imaging (CMRI) dataset. The original dataset is used in two
different approaches. First, the labeled dataset is applied to the NN and DNN
to create the NN and DNN models. Second, the labels are removed, and the
unlabeled dataset is clustered via the FCM method, and then, the clustered
dataset is fed to the DNN to create the FCM-DNN model. By utilizing the second
clustering and modeling, the training process is improved, and consequently,
the accuracy is increased. As a result, the proposed FCM-DNN model achieves the
best performance with a 99.91% accuracy specifying 10 clusters, i.e., 5
clusters for healthy subjects and 5 clusters for sick subjects, through the
10-fold cross-validation technique compared to the NN and DNN models reaching
the accuracies of 92.18% and 99.63%, respectively. To the best of our
knowledge, no study has been conducted for CAD diagnosis on the CMRI dataset
using artificial intelligence methods. The results confirm that the proposed
FCM-DNN model can be helpful for scientific and research centers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Do Vision Transformers Work?. (arXiv:2202.06709v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06709">
<div class="article-summary-box-inner">
<span><p>The success of multi-head self-attentions (MSAs) for computer vision is now
indisputable. However, little is known about how MSAs work. We present
fundamental explanations to help better understand the nature of MSAs. In
particular, we demonstrate the following properties of MSAs and Vision
Transformers (ViTs): (1) MSAs improve not only accuracy but also generalization
by flattening the loss landscapes. Such improvement is primarily attributable
to their data specificity, not long-range dependency. On the other hand, ViTs
suffer from non-convex losses. Large datasets and loss landscape smoothing
methods alleviate this problem; (2) MSAs and Convs exhibit opposite behaviors.
For example, MSAs are low-pass filters, but Convs are high-pass filters.
Therefore, MSAs and Convs are complementary; (3) Multi-stage neural networks
behave like a series connection of small individual models. In addition, MSAs
at the end of a stage play a key role in prediction. Based on these insights,
we propose AlterNet, a model in which Conv blocks at the end of a stage are
replaced with MSA blocks. AlterNet outperforms CNNs not only in large data
regimes but also in small data regimes. The code is available at
https://github.com/xxxnell/how-do-vits-work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Label fusion and training methods for reliable representation of inter-rater uncertainty. (arXiv:2202.07550v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07550">
<div class="article-summary-box-inner">
<span><p>Medical tasks are prone to inter-rater variability due to multiple factors
such as image quality, professional experience and training, or guideline
clarity. Training deep learning networks with annotations from multiple raters
is a common practice that mitigates the model's bias towards a single expert.
Reliable models generating calibrated outputs and reflecting the inter-rater
disagreement are key to the integration of artificial intelligence in clinical
practice. Various methods exist to take into account different expert labels.
We focus on comparing three label fusion methods: STAPLE, average of the
rater's segmentation, and random sampling of each rater's segmentation during
training. Each label fusion method is studied using both the conventional
training framework and the recently published SoftSeg framework that limits
information loss by treating the segmentation task as a regression. Our
results, across 10 data splittings on two public datasets, indicate that
SoftSeg models, regardless of the ground truth fusion method, had better
calibration and preservation of the inter-rater rater variability compared with
their conventional counterparts without impacting the segmentation performance.
Conventional models, i.e., trained with a Dice loss, with binary inputs, and
sigmoid/softmax final activate, were overconfident and underestimated the
uncertainty associated with inter-rater variability. Conversely, fusing labels
by averaging with the SoftSeg framework led to underconfident outputs and
overestimation of the rater disagreement. In terms of segmentation performance,
the best label fusion method was different for the two datasets studied,
indicating this parameter might be task-dependent. However, SoftSeg had
segmentation performance systematically superior or equal to the conventionally
trained models and had the best calibration and preservation of the inter-rater
variability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PMP-Net++: Point Cloud Completion by Transformer-Enhanced Multi-step Point Moving Paths. (arXiv:2202.09507v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09507">
<div class="article-summary-box-inner">
<span><p>Point cloud completion concerns to predict missing part for incomplete 3D
shapes. A common strategy is to generate complete shape according to incomplete
input. However, unordered nature of point clouds will degrade generation of
high-quality 3D shapes, as detailed topology and structure of unordered points
are hard to be captured during the generative process using an extracted latent
code. We address this problem by formulating completion as point cloud
deformation process. Specifically, we design a novel neural network, named
PMP-Net++, to mimic behavior of an earth mover. It moves each point of
incomplete input to obtain a complete point cloud, where total distance of
point moving paths (PMPs) should be the shortest. Therefore, PMP-Net++ predicts
unique PMP for each point according to constraint of point moving distances.
The network learns a strict and unique correspondence on point-level, and thus
improves quality of predicted complete shape. Moreover, since moving points
heavily relies on per-point features learned by network, we further introduce a
transformer-enhanced representation learning network, which significantly
improves completion performance of PMP-Net++. We conduct comprehensive
experiments in shape completion, and further explore application on point cloud
up-sampling, which demonstrate non-trivial improvement of PMP-Net++ over
state-of-the-art point cloud completion/up-sampling methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Going Deeper into Recognizing Actions in Dark Environments: A Comprehensive Benchmark Study. (arXiv:2202.09545v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09545">
<div class="article-summary-box-inner">
<span><p>While action recognition (AR) has gained large improvements with the
introduction of large-scale video datasets and the development of deep neural
networks, AR models robust to challenging environments in real-world scenarios
are still under-explored. We focus on the task of action recognition in dark
environments, which can be applied to fields such as surveillance and
autonomous driving at night. Intuitively, current deep networks along with
visual enhancement techniques should be able to handle AR in dark environments,
however, it is observed that this is not always the case in practice. To dive
deeper into exploring solutions for AR in dark environments, we launched the
UG2+ Challenge Track 2 (UG2-2) in IEEE CVPR 2021, with a goal of evaluating and
advancing the robustness of AR models in dark environments. The challenge
builds and expands on top of a novel ARID dataset, the first dataset for the
task of dark video AR, and guides models to tackle such a task in both fully
and semi-supervised manners. Baseline results utilizing current AR models and
enhancement methods are reported, justifying the challenging nature of this
task with substantial room for improvements. Thanks to the active participation
from the research community, notable advances have been made in participants'
solutions, while analysis of these solutions helped better identify possible
directions to tackle the challenge of AR in dark environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparsity Winning Twice: Better Robust Generalization from More Efficient Training. (arXiv:2202.09844v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09844">
<div class="article-summary-box-inner">
<span><p>Recent studies demonstrate that deep networks, even robustified by the
state-of-the-art adversarial training (AT), still suffer from large robust
generalization gaps, in addition to the much more expensive training costs than
standard training. In this paper, we investigate this intriguing problem from a
new perspective, i.e., injecting appropriate forms of sparsity during
adversarial training. We introduce two alternatives for sparse adversarial
training: (i) static sparsity, by leveraging recent results from the lottery
ticket hypothesis to identify critical sparse subnetworks arising from the
early training; (ii) dynamic sparsity, by allowing the sparse subnetwork to
adaptively adjust its connectivity pattern (while sticking to the same sparsity
ratio) throughout training. We find both static and dynamic sparse methods to
yield win-win: substantially shrinking the robust generalization gap and
alleviating the robust overfitting, meanwhile significantly saving training and
inference FLOPs. Extensive experiments validate our proposals with multiple
network architectures on diverse datasets, including CIFAR-10/100 and
Tiny-ImageNet. For example, our methods reduce robust generalization gap and
overfitting by 34.44% and 4.02%, with comparable robust/standard accuracy
boosts and 87.83%/87.82% training/inference FLOPs savings on CIFAR-100 with
ResNet-18. Besides, our approaches can be organically combined with existing
regularizers, establishing new state-of-the-art results in AT. Codes are
available in https://github.com/VITA-Group/Sparsity-Win-Robust-Generalization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Feature based Cross-slide Registration. (arXiv:2202.09971v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09971">
<div class="article-summary-box-inner">
<span><p>Cross-slide image analysis provides additional information by analysing the
expression of different biomarkers as compared to a single slide analysis.
Slides stained with different biomarkers are analysed side by side which may
reveal unknown relations between the different biomarkers. During the slide
preparation, a tissue section may be placed at an arbitrary orientation as
compared to other sections of the same tissue block. The problem is compounded
by the fact that tissue contents are likely to change from one section to the
next and there may be unique artefacts on some of the slides. This makes
registration of each section to a reference section of the same tissue block an
important pre-requisite task before any cross-slide analysis. We propose a deep
feature based registration (DFBR) method which utilises data-driven features to
estimate the rigid transformation. We adopted a multi-stage strategy for
improving the quality of registration. We also developed a visualisation tool
to view registered pairs of WSIs at different magnifications. With the help of
this tool, one can apply a transformation on the fly without the need to
generate transformed source WSI in a pyramidal form. We compared the
performance of data-driven features with that of hand-crafted features on the
COMET dataset. Our approach can align the images with low registration errors.
Generally, the success of non-rigid registration is dependent on the quality of
rigid registration. To evaluate the efficacy of the DFBR method, the first two
steps of the ANHIR winner's framework are replaced with our DFBR to register
challenge provided image pairs. The modified framework produce comparable
results to that of challenge winning team.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ABAW: Valence-Arousal Estimation, Expression Recognition, Action Unit Detection & Multi-Task Learning Challenges. (arXiv:2202.10659v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.10659">
<div class="article-summary-box-inner">
<span><p>This paper describes the third Affective Behavior Analysis in-the-wild (ABAW)
Competition, held in conjunction with IEEE International Conference on Computer
Vision and Pattern Recognition (CVPR), 2022. The 3rd ABAW Competition is a
continuation of the Competitions held at ICCV 2021, IEEE FG 2020 and IEEE CVPR
2017 Conferences, and aims at automatically analyzing affect. This year the
Competition encompasses four Challenges: i) uni-task Valence-Arousal
Estimation, ii) uni-task Expression Classification, iii) uni-task Action Unit
Detection, and iv) Multi-Task-Learning. All the Challenges are based on a
common benchmark database, Aff-Wild2, which is a large scale in-the-wild
database and the first one to be annotated in terms of valence-arousal,
expressions and action units. In this paper, we present the four Challenges,
with the utilized Competition corpora, we outline the evaluation metrics and
present the baseline systems along with their obtained results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One-shot Scene Graph Generation. (arXiv:2202.10824v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.10824">
<div class="article-summary-box-inner">
<span><p>As a structured representation of the image content, the visual scene graph
(visual relationship) acts as a bridge between computer vision and natural
language processing. Existing models on the scene graph generation task
notoriously require tens or hundreds of labeled samples. By contrast, human
beings can learn visual relationships from a few or even one example. Inspired
by this, we design a task named One-Shot Scene Graph Generation, where each
relationship triplet (e.g., "dog-has-head") comes from only one labeled
example. The key insight is that rather than learning from scratch, one can
utilize rich prior knowledge. In this paper, we propose Multiple Structured
Knowledge (Relational Knowledge and Commonsense Knowledge) for the one-shot
scene graph generation task. Specifically, the Relational Knowledge represents
the prior knowledge of relationships between entities extracted from the visual
content, e.g., the visual relationships "standing in", "sitting in", and "lying
in" may exist between "dog" and "yard", while the Commonsense Knowledge encodes
"sense-making" knowledge like "dog can guard yard". By organizing these two
kinds of knowledge in a graph structure, Graph Convolution Networks (GCNs) are
used to extract knowledge-embedded semantic features of the entities. Besides,
instead of extracting isolated visual features from each entity generated by
Faster R-CNN, we utilize an Instance Relation Transformer encoder to fully
explore their context information. Based on a constructed one-shot dataset, the
experimental results show that our method significantly outperforms existing
state-of-the-art methods by a large margin. Ablation studies also verify the
effectiveness of the Instance Relation Transformer encoder and the Multiple
Structured Knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Auto-scaling Vision Transformers without Training. (arXiv:2202.11921v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11921">
<div class="article-summary-box-inner">
<span><p>This work targets automated designing and scaling of Vision Transformers
(ViTs). The motivation comes from two pain spots: 1) the lack of efficient and
principled methods for designing and scaling ViTs; 2) the tremendous
computational cost of training ViT that is much heavier than its convolution
counterpart. To tackle these issues, we propose As-ViT, an auto-scaling
framework for ViTs without training, which automatically discovers and scales
up ViTs in an efficient and principled manner. Specifically, we first design a
"seed" ViT topology by leveraging a training-free search process. This
extremely fast search is fulfilled by a comprehensive study of ViT's network
complexity, yielding a strong Kendall-tau correlation with ground-truth
accuracies. Second, starting from the "seed" topology, we automate the scaling
rule for ViTs by growing widths/depths to different ViT layers. This results in
a series of architectures with different numbers of parameters in a single run.
Finally, based on the observation that ViTs can tolerate coarse tokenization in
early training stages, we propose a progressive tokenization strategy to train
ViTs faster and cheaper. As a unified framework, As-ViT achieves strong
performance on classification (83.5% top1 on ImageNet-1k) and detection (52.7%
mAP on COCO) without any manual crafting nor scaling of ViT architectures: the
end-to-end model design and scaling process cost only 12 hours on one V100 GPU.
Our code is available at https://github.com/VITA-Group/AsViT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring CLEVRness: Blackbox testing of Visual Reasoning Models. (arXiv:2202.12162v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12162">
<div class="article-summary-box-inner">
<span><p>How can we measure the reasoning capabilities of intelligence systems? Visual
question answering provides a convenient framework for testing the model's
abilities by interrogating the model through questions about the scene.
However, despite scores of various visual QA datasets and architectures, which
sometimes yield even a super-human performance, the question of whether those
architectures can actually reason remains open to debate. To answer this, we
extend the visual question answering framework and propose the following
behavioral test in the form of a two-player game. We consider black-box neural
models of CLEVR. These models are trained on a diagnostic dataset benchmarking
reasoning. Next, we train an adversarial player that re-configures the scene to
fool the CLEVR model. We show that CLEVR models, which otherwise could perform
at a human level, can easily be fooled by our agent. Our results put in doubt
whether data-driven approaches can do reasoning without exploiting the numerous
biases that are often present in those datasets. Finally, we also propose a
controlled experiment measuring the efficiency of such models to learn and
perform reasoning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Factorizer: A Scalable Interpretable Approach to Context Modeling for Medical Image Segmentation. (arXiv:2202.12295v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12295">
<div class="article-summary-box-inner">
<span><p>Convolutional Neural Networks (CNNs) with U-shaped architectures have
dominated medical image segmentation, which is crucial for various clinical
purposes. However, the inherent locality of convolution makes CNNs fail to
fully exploit global context, essential for better recognition of some
structures, e.g., brain lesions. Transformers have recently proved promising
performance on vision tasks, including semantic segmentation, mainly due to
their capability of modeling long-range dependencies. Nevertheless, the
quadratic complexity of attention makes existing Transformer-based models use
self-attention layers only after somehow reducing the image resolution, which
limits the ability to capture global contexts present at higher resolutions.
Therefore, this work introduces a family of models, dubbed Factorizer, which
leverages the power of low-rank matrix factorization for constructing an
end-to-end segmentation model. Specifically, we propose a linearly scalable
approach to context modeling, formulating Nonnegative Matrix Factorization
(NMF) as a differentiable layer integrated into a U-shaped architecture. The
shifted window technique is also utilized in combination with NMF to
effectively aggregate local information. Factorizers compete favorably with
CNNs and Transformers in terms of accuracy, scalability, and interpretability,
achieving state-of-the-art results on the BraTS dataset for brain tumor
segmentation, with Dice scores of 79.33%, 83.14%, and 90.16% for enhancing
tumor, tumor core, and whole tumor, respectively. Highly meaningful NMF
components give an additional interpretability advantage to Factorizers over
CNNs and Transformers. Moreover, our ablation studies reveal a distinctive
feature of Factorizers that enables a significant speed-up in inference for a
trained Factorizer without any extra steps and without sacrificing much
accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeuralFusion: Neural Volumetric Rendering under Human-object Interactions. (arXiv:2202.12825v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12825">
<div class="article-summary-box-inner">
<span><p>4D modeling of human-object interactions is critical for numerous
applications. However, efficient volumetric capture and rendering of complex
interaction scenarios, especially from sparse inputs, remain challenging. In
this paper, we propose NeuralFusion, a neural approach for volumetric
human-object capture and rendering using sparse consumer RGBD sensors. It
marries traditional non-rigid fusion with recent neural implicit modeling and
blending advances, where the captured humans and objects are layerwise
disentangled. For geometry modeling, we propose a neural implicit inference
scheme with non-rigid key-volume fusion, as well as a template-aid robust
object tracking pipeline. Our scheme enables detailed and complete geometry
generation under complex interactions and occlusions. Moreover, we introduce a
layer-wise human-object texture rendering scheme, which combines volumetric and
image-based rendering in both spatial and temporal domains to obtain
photo-realistic results. Extensive experiments demonstrate the effectiveness
and efficiency of our approach in synthesizing photo-realistic free-view
results under complex human-object interactions.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-03-01 23:07:45.590960032 UTC">2022-03-01 23:07:45 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>