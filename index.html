<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-03-21T01:30:00Z">03-21</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Privacy-Preserving Speech Representation Learning using Vector Quantization. (arXiv:2203.09518v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09518">
<div class="article-summary-box-inner">
<span><p>With the popularity of virtual assistants (e.g., Siri, Alexa), the use of
speech recognition is now becoming more and more widespread.However, speech
signals contain a lot of sensitive information, such as the speaker's identity,
which raises privacy concerns.The presented experiments show that the
representations extracted by the deep layers of speech recognition networks
contain speaker information.This paper aims to produce an anonymous
representation while preserving speech recognition performance.To this end, we
propose to use vector quantization to constrain the representation space and
induce the network to suppress the speaker identity.The choice of the
quantization dictionary size allows to configure the trade-off between utility
(speech recognition) and privacy (speaker identity concealment).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Federated Learning on Knowledge Graphs via Privacy-preserving Relation Embedding Aggregation. (arXiv:2203.09553v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09553">
<div class="article-summary-box-inner">
<span><p>Federated Learning (FL) on knowledge graphs (KGs) has yet to be as well
studied as other domains, such as computer vision and natural language
processing. A recent study FedE first proposes an FL framework that shares
entity embeddings of KGs across all clients. However, compared with model
sharing in vanilla FL, entity embedding sharing from FedE would incur severe
privacy leakage. Specifically, the known entity embedding can be used to infer
whether a specific relation between two entities exists in a private client. In
this paper, we first develop a novel attack that aims to recover the original
data based on embedding information, which is further used to evaluate the
vulnerabilities of FedE. Furthermore, we propose a Federated learning paradigm
with privacy-preserving Relation embedding aggregation (FedR) to tackle the
privacy issue in FedE. Compared to entity embedding sharing, relation embedding
sharing policy can significantly reduce the communication cost due to its
smaller size of queries. We conduct extensive experiments to evaluate FedR with
five different embedding learning models and three benchmark KG datasets.
Compared to FedE, FedR achieves similar utility and significant (nearly 2X)
improvements in both privacy and efficiency on link prediction task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhanced Temporal Knowledge Embeddings with Contextualized Language Representations. (arXiv:2203.09590v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09590">
<div class="article-summary-box-inner">
<span><p>With the emerging research effort to integrate structured and unstructured
knowledge, many approaches incorporate factual knowledge into pre-trained
language models (PLMs) and apply the knowledge-enhanced PLMs on downstream NLP
tasks. However, (1) they only consider \textit{static} factual knowledge, but
knowledge graphs (KGs) also contain \textit{temporal facts} or \textit{events}
indicating evolutionary relationships among entities at different timestamps.
(2) PLMs cannot be directly applied to many KG tasks, such as temporal KG
completion.
</p>
<p>In this paper, we focus on \textbf{e}nhancing temporal knowledge embeddings
with \textbf{co}ntextualized \textbf{la}nguage representations (ECOLA). We
align structured knowledge contained in temporal knowledge graphs with their
textual descriptions extracted from news articles and propose a novel
knowledge-text prediction task to inject the abundant information from
descriptions into temporal knowledge embeddings. ECOLA jointly optimizes the
knowledge-text prediction objective and the temporal knowledge embeddings,
which can simultaneously take full advantage of textual and knowledge
information. For training ECOLA, we introduce three temporal KG datasets with
aligned textual descriptions. Experimental results on the temporal knowledge
graph completion task show that ECOLA outperforms state-of-the-art temporal KG
models by a large margin. The proposed datasets can serve as new temporal KG
benchmarks and facilitate future research on structured and unstructured
knowledge integration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Responsible Natural Language Annotation for the Varieties of Arabic. (arXiv:2203.09597v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09597">
<div class="article-summary-box-inner">
<span><p>When building NLP models, there is a tendency to aim for broader coverage,
often overlooking cultural and (socio)linguistic nuance. In this position
paper, we make the case for care and attention to such nuances, particularly in
dataset annotation, as well as the inclusion of cultural and linguistic
expertise in the process. We present a playbook for responsible dataset
creation for polyglossic, multidialectal languages. This work is informed by a
study on Arabic annotation of social media content.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DP-KB: Data Programming with Knowledge Bases Improves Transformer Fine Tuning for Answer Sentence Selection. (arXiv:2203.09598v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09598">
<div class="article-summary-box-inner">
<span><p>While transformers demonstrate impressive performance on many knowledge
intensive (KI) tasks, their ability to serve as implicit knowledge bases (KBs)
remains limited, as shown on several slot-filling, question-answering (QA),
fact verification, and entity-linking tasks. In this paper, we implement an
efficient, data-programming technique that enriches training data with
KB-derived context and improves transformer utilization of encoded knowledge
when fine-tuning for a particular QA task, namely answer sentence selection
(AS2). Our method outperforms state of the art transformer approach on WikiQA
and TrecQA, two widely studied AS2 benchmarks, increasing by 2.0% p@1, 1.3%
MAP, 1.1% MRR, and 4.4% p@1, 0.9% MAP, 2.4% MRR, respectively. To demonstrate
our improvements in an industry setting, we additionally evaluate our approach
on a proprietary dataset of Alexa QA pairs, and show increase of 2.3% F1 and
2.0% MAP. We additionally find that these improvements remain even when KB
context is omitted at inference time, allowing for the use of our models within
existing transformer workflows without additional latency or deployment costs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robotic Speech Synthesis: Perspectives on Interactions, Scenarios, and Ethics. (arXiv:2203.09599v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09599">
<div class="article-summary-box-inner">
<span><p>In recent years, many works have investigated the feasibility of
conversational robots for performing specific tasks, such as healthcare and
interview. Along with this development comes a practical issue: how should we
synthesize robotic voices to meet the needs of different situations? In this
paper, we discuss this issue from three perspectives: 1) the difficulties of
synthesizing non-verbal and interaction-oriented speech signals, particularly
backchannels; 2) the scenario classification for robotic voice synthesis; 3)
the ethical issues regarding the design of robot voice for its emotion and
identity. We present the findings of relevant literature and our prior work,
trying to bring the attention of human-robot interaction researchers to design
better conversational robots in the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Importance of Data Size in Probing Fine-tuned Models. (arXiv:2203.09627v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09627">
<div class="article-summary-box-inner">
<span><p>Several studies have investigated the reasons behind the effectiveness of
fine-tuning, usually through the lens of probing. However, these studies often
neglect the role of the size of the dataset on which the model is fine-tuned.
In this paper, we highlight the importance of this factor and its undeniable
role in probing performance. We show that the extent of encoded linguistic
knowledge depends on the number of fine-tuning samples. The analysis also
reveals that larger training data mainly affects higher layers, and that the
extent of this change is a factor of the number of iterations updating the
model during fine-tuning rather than the diversity of the training samples.
Finally, we show through a set of experiments that fine-tuning data size
affects the recoverability of the changes made to the model's linguistic
knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HiStruct+: Improving Extractive Text Summarization with Hierarchical Structure Information. (arXiv:2203.09629v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09629">
<div class="article-summary-box-inner">
<span><p>Transformer-based language models usually treat texts as linear sequences.
However, most texts also have an inherent hierarchical structure, i.e., parts
of a text can be identified using their position in this hierarchy. In
addition, section titles usually indicate the common topic of their respective
sentences. We propose a novel approach to formulate, extract, encode and inject
hierarchical structure information explicitly into an extractive summarization
model based on a pre-trained, encoder-only Transformer language model
(HiStruct+ model), which improves SOTA ROUGEs for extractive summarization on
PubMed and arXiv substantially. Using various experimental settings on three
datasets (i.e., CNN/DailyMail, PubMed and arXiv), our HiStruct+ model
outperforms a strong baseline collectively, which differs from our model only
in that the hierarchical structure information is not injected. It is also
observed that the more conspicuous hierarchical structure the dataset has, the
larger improvements our method gains. The ablation study demonstrates that the
hierarchical position information is the main contributor to our model's SOTA
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dim Wihl Gat Tun: The Case for Linguistic Expertise in NLP for Underdocumented Languages. (arXiv:2203.09632v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09632">
<div class="article-summary-box-inner">
<span><p>Recent progress in NLP is driven by pretrained models leveraging massive
datasets and has predominantly benefited the world's political and economic
superpowers. Technologically underserved languages are left behind because they
lack such resources. Hundreds of underserved languages, nevertheless, have
available data sources in the form of interlinear glossed text (IGT) from
language documentation efforts. IGT remains underutilized in NLP work, perhaps
because its annotations are only semi-structured and often language-specific.
With this paper, we make the case that IGT data can be leveraged successfully
provided that target language expertise is available. We specifically advocate
for collaboration with documentary linguists. Our paper provides a roadmap for
successful projects utilizing IGT data: (1) It is essential to define which NLP
tasks can be accomplished with the given IGT data and how these will benefit
the speech community. (2) Great care and target language expertise is required
when converting the data into structured formats commonly employed in NLP. (3)
Task-specific and user-specific evaluation can help to ascertain that the tools
which are created benefit the target language speech community. We illustrate
each step through a case study on developing a morphological reinflection
system for the Tsimchianic language Gitksan.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Reinforcement Agent for Efficient Instant Search. (arXiv:2203.09644v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09644">
<div class="article-summary-box-inner">
<span><p>Instant Search is a paradigm where a search system retrieves answers on the
fly while typing. The na\"ive implementation of an Instant Search system would
hit the search back-end for results each time a user types a key, imposing a
very high load on the underlying search system. In this paper, we propose to
address the load issue by identifying tokens that are semantically more salient
towards retrieving relevant documents and utilize this knowledge to trigger an
instant search selectively. We train a reinforcement agent that interacts
directly with the search engine and learns to predict the word's importance.
Our proposed method treats the underlying search system as a black box and is
more universally applicable to a diverse set of architectures. Furthermore, a
novel evaluation framework is presented to study the trade-off between the
number of triggered searches and the system's performance. We utilize the
framework to evaluate and compare the proposed reinforcement method with other
intuitive baselines. Experimental results demonstrate the efficacy of the
proposed method towards achieving a superior trade-off.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hate speech, Censorship, and Freedom of Speech: The Changing Policies of Reddit. (arXiv:2203.09673v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09673">
<div class="article-summary-box-inner">
<span><p>This paper examines the shift in focus on content policies and user attitudes
on the social media platform Reddit. We do this by focusing on comments from
general Reddit users from five posts made by admins (moderators) on updates to
Reddit Content Policy. All five concern the nature of what kind of content is
allowed to be posted on Reddit, and which measures will be taken against
content that violates these policies. We use topic modeling to probe how the
general discourse for Redditors has changed around limitations on content, and
later, limitations on hate speech, or speech that incites violence against a
particular group. We show that there is a clear shift in both the contents and
the user attitudes that can be linked to contemporary societal upheaval as well
as newly passed laws and regulations, and contribute to the wider discussion on
hate speech moderation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Intensification for Sign Language Generation: A Computational Approach. (arXiv:2203.09679v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09679">
<div class="article-summary-box-inner">
<span><p>End-to-end sign language generation models do not accurately represent the
prosody in sign language. A lack of temporal and spatial variations leads to
poor-quality generated presentations that confuse human interpreters. In this
paper, we aim to improve the prosody in generated sign languages by modeling
intensification in a data-driven manner. We present different strategies
grounded in linguistics of sign language that inform how intensity modifiers
can be represented in gloss annotations. To employ our strategies, we first
annotate a subset of the benchmark PHOENIX-14T, a German Sign Language dataset,
with different levels of intensification. We then use a supervised intensity
tagger to extend the annotated dataset and obtain labels for the remaining
portion of it. This enhanced dataset is then used to train state-of-the-art
transformer models for sign language generation. We find that our efforts in
intensification modeling yield better results when evaluated with automatic
metrics. Human evaluation also indicates a higher preference of the videos
generated using our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A$^3$T: Alignment-Aware Acoustic and Text Pretraining for Speech Synthesis and Editing. (arXiv:2203.09690v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09690">
<div class="article-summary-box-inner">
<span><p>Recently, speech representation learning has improved many speech-related
tasks such as speech recognition, speech classification, and speech-to-text
translation. However, all the above tasks are in the direction of speech
understanding, but for the inverse direction, speech synthesis, the potential
of representation learning is yet to be realized, due to the challenging nature
of generating high-quality speech. To address this problem, we propose our
framework, Alignment-Aware Acoustic-Text Pretraining (A$^3$T), which
reconstructs masked acoustic signals with text input and acoustic-text
alignment during training. In this way, the pretrained model can generate high
quality of reconstructed spectrogram, which can be applied to the speech
editing and unseen speaker TTS directly. Experiments show A$^3$T outperforms
SOTA models on speech editing, and improves multi-speaker speech synthesis
without the external speaker verification model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improve few-shot voice cloning using multi-modal learning. (arXiv:2203.09708v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09708">
<div class="article-summary-box-inner">
<span><p>Recently, few-shot voice cloning has achieved a significant improvement.
However, most models for few-shot voice cloning are single-modal, and
multi-modal few-shot voice cloning has been understudied. In this paper, we
propose to use multi-modal learning to improve the few-shot voice cloning
performance. Inspired by the recent works on unsupervised speech
representation, the proposed multi-modal system is built by extending Tacotron2
with an unsupervised speech representation module. We evaluate our proposed
system in two few-shot voice cloning scenarios, namely few-shot
text-to-speech(TTS) and voice conversion(VC). Experimental results demonstrate
that the proposed multi-modal learning can significantly improve the few-shot
voice cloning performance over their counterpart single-modal systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DEAM: Dialogue Coherence Evaluation using AMR-based Semantic Manipulations. (arXiv:2203.09711v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09711">
<div class="article-summary-box-inner">
<span><p>Automatic evaluation metrics are essential for the rapid development of
open-domain dialogue systems as they facilitate hyper-parameter tuning and
comparison between models. Although recently proposed trainable
conversation-level metrics have shown encouraging results, the quality of the
metrics is strongly dependent on the quality of training data. Prior works
mainly resort to heuristic text-level manipulations (e.g. utterances shuffling)
to bootstrap incoherent conversations (negative examples) from coherent
dialogues (positive examples). Such approaches are insufficient to
appropriately reflect the incoherence that occurs in interactions between
advanced dialogue models and humans. To tackle this problem, we propose DEAM, a
Dialogue coherence Evaluation metric that relies on Abstract Meaning
Representation (AMR) to apply semantic-level Manipulations for incoherent
(negative) data generation. AMRs naturally facilitate the injection of various
types of incoherence sources, such as coreference inconsistency, irrelevancy,
contradictions, and decrease engagement, at the semantic level, thus resulting
in more natural incoherent samples. Our experiments show that DEAM achieves
higher correlations with human judgments compared to baseline methods on
several dialog datasets by significant margins. We also show that DEAM can
distinguish between coherent and incoherent dialogues generated by baseline
manipulations, whereas those baseline models cannot detect incoherent examples
generated by DEAM. Our results demonstrate the potential of AMR-based semantic
manipulations for natural negative example generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PRBoost: Prompt-Based Rule Discovery and Boosting for Interactive Weakly-Supervised Learning. (arXiv:2203.09735v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09735">
<div class="article-summary-box-inner">
<span><p>Weakly-supervised learning (WSL) has shown promising results in addressing
label scarcity on many NLP tasks, but manually designing a comprehensive,
high-quality labeling rule set is tedious and difficult. We study interactive
weakly-supervised learning -- the problem of iteratively and automatically
discovering novel labeling rules from data to improve the WSL model. Our
proposed model, named PRBoost, achieves this goal via iterative prompt-based
rule discovery and model boosting. It uses boosting to identify large-error
instances and then discovers candidate rules from them by prompting pre-trained
LMs with rule templates. The candidate rules are judged by human experts, and
the accepted rules are used to generate complementary weak labels and
strengthen the current model. Experiments on four tasks show PRBoost
outperforms state-of-the-art WSL baselines up to 7.1% and bridges the gaps with
fully supervised models. Our Implementation is available at
\url{https://github.com/rz-zhang/PRBoost}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GRS: Combining Generation and Revision in Unsupervised Sentence Simplification. (arXiv:2203.09742v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09742">
<div class="article-summary-box-inner">
<span><p>We propose GRS: an unsupervised approach to sentence simplification that
combines text generation and text revision. We start with an iterative
framework in which an input sentence is revised using explicit edit operations,
and add paraphrasing as a new edit operation. This allows us to combine the
advantages of generative and revision-based approaches: paraphrasing captures
complex edit operations, and the use of explicit edit operations in an
iterative manner provides controllability and interpretability. We demonstrate
these advantages of GRS compared to existing methods on the Newsela and ASSET
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prototypical Verbalizer for Prompt-based Few-shot Tuning. (arXiv:2203.09770v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09770">
<div class="article-summary-box-inner">
<span><p>Prompt-based tuning for pre-trained language models (PLMs) has shown its
effectiveness in few-shot learning. Typically, prompt-based tuning wraps the
input text into a cloze question. To make predictions, the model maps the
output words to labels via a verbalizer, which is either manually designed or
automatically built. However, manual verbalizers heavily depend on
domain-specific prior knowledge and human efforts, while finding appropriate
label words automatically still remains challenging.In this work, we propose
the prototypical verbalizer (ProtoVerb) which is built directly from training
data. Specifically, ProtoVerb learns prototype vectors as verbalizers by
contrastive learning. In this way, the prototypes summarize training instances
and are able to enclose rich class-level semantics. We conduct experiments on
both topic classification and entity typing tasks, and the results demonstrate
that ProtoVerb significantly outperforms current automatic verbalizers,
especially when training data is extremely scarce. More surprisingly, ProtoVerb
consistently boosts prompt-based tuning even on untuned PLMs, indicating an
elegant non-tuning way to utilize PLMs. Our codes are avaliable at
https://github.com/thunlp/OpenPrompt.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are You Robert or RoBERTa? Deceiving Online Authorship Attribution Models Using Neural Text Generators. (arXiv:2203.09813v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09813">
<div class="article-summary-box-inner">
<span><p>Recently, there has been a rise in the development of powerful pre-trained
natural language models, including GPT-2, Grover, and XLM. These models have
shown state-of-the-art capabilities towards a variety of different NLP tasks,
including question answering, content summarisation, and text generation.
Alongside this, there have been many studies focused on online authorship
attribution (AA). That is, the use of models to identify the authors of online
texts. Given the power of natural language models in generating convincing
texts, this paper examines the degree to which these language models can
generate texts capable of deceiving online AA models. Experimenting with both
blog and Twitter data, we utilise GPT-2 language models to generate texts using
the existing posts of online users. We then examine whether these AI-based text
generators are capable of mimicking authorial style to such a degree that they
can deceive typical AA models. From this, we find that current AI-based text
generators are able to successfully mimic authorship, showing capabilities
towards this on both datasets. Our findings, in turn, highlight the current
capacity of powerful natural language models to generate original online posts
capable of mimicking authorial style sufficiently to deceive popular AA
methods; a key finding given the proposed role of AA in real world applications
such as spam-detection and forensic investigation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdaVocoder: Adaptive Vocoder for Custom Voice. (arXiv:2203.09825v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09825">
<div class="article-summary-box-inner">
<span><p>Custom voice is to construct a personal speech synthesis system by adapting
the source speech synthesis model to the target model through the target few
recordings. The solution to constructing a custom voice is to combine an
adaptive acoustic model with a robust vocoder. However, training a robust
vocoder usually requires a multi-speaker dataset, which should include various
age groups and various timbres, so that the trained vocoder can be used for
unseen speakers. Collecting such a multi-speaker dataset is difficult, and the
dataset distribution always has a mismatch with the distribution of the target
speaker dataset. This paper proposes an adaptive vocoder for custom voice from
another novel perspective to solve the above problems. The adaptive vocoder
mainly uses a cross-domain consistency loss to solve the overfitting problem
encountered by the GAN-based neural vocoder in the transfer learning of
few-shot scenes. We construct two adaptive vocoders, AdaMelGAN and AdaHiFi-GAN.
First, We pre-train the source vocoder model on AISHELL3 and CSMSC datasets,
respectively. Then, fine-tune it on the internal dataset VXI-children with few
adaptation data. The empirical results show that a high-quality custom voice
system can be built by combining a adaptive acoustic model with a adaptive
vocoder.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Under the Morphosyntactic Lens: A Multifaceted Evaluation of Gender Bias in Speech Translation. (arXiv:2203.09866v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09866">
<div class="article-summary-box-inner">
<span><p>Gender bias is largely recognized as a problematic phenomenon affecting
language technologies, with recent studies underscoring that it might surface
differently across languages. However, most of current evaluation practices
adopt a word-level focus on a narrow set of occupational nouns under synthetic
conditions. Such protocols overlook key features of grammatical gender
languages, which are characterized by morphosyntactic chains of gender
agreement, marked on a variety of lexical items and parts-of-speech (POS). To
overcome this limitation, we enrich the natural, gender-sensitive MuST-SHE
corpus (Bentivogli et al., 2020) with two new linguistic annotation layers (POS
and agreement chains), and explore to what extent different lexical categories
and agreement phenomena are impacted by gender skews. Focusing on speech
translation, we conduct a multifaceted evaluation on three language directions
(English-French/Italian/Spanish), with models trained on varying amounts of
data and different word segmentation techniques. By shedding light on model
behaviours, gender bias, and its detection at several levels of granularity,
our findings emphasize the value of dedicated analyses beyond aggregated
overall results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SCoT: Sense Clustering over Time: a tool for the analysis of lexical change. (arXiv:2203.09892v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09892">
<div class="article-summary-box-inner">
<span><p>We present Sense Clustering over Time (SCoT), a novel network-based tool for
analysing lexical change. SCoT represents the meanings of a word as clusters of
similar words. It visualises their formation, change, and demise. There are two
main approaches to the exploration of dynamic networks: the discrete one
compares a series of clustered graphs from separate points in time. The
continuous one analyses the changes of one dynamic network over a time-span.
SCoT offers a new hybrid solution. First, it aggregates time-stamped documents
into intervals and calculates one sense graph per discrete interval. Then, it
merges the static graphs to a new type of dynamic semantic neighbourhood graph
over time. The resulting sense clusters offer uniquely detailed insights into
lexical change over continuous intervals with model transparency and
provenance. SCoT has been successfully used in a European study on the changing
meaning of `crisis'.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Multilingual Language Models Capture Differing Moral Norms?. (arXiv:2203.09904v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09904">
<div class="article-summary-box-inner">
<span><p>Massively multilingual sentence representations are trained on large corpora
of uncurated data, with a very imbalanced proportion of languages included in
the training. This may cause the models to grasp cultural values including
moral judgments from the high-resource languages and impose them on the
low-resource languages. The lack of data in certain languages can also lead to
developing random and thus potentially harmful beliefs. Both these issues can
negatively influence zero-shot cross-lingual model transfer and potentially
lead to harmful outcomes. Therefore, we aim to (1) detect and quantify these
issues by comparing different models in different languages, (2) develop
methods for improving undesirable properties of the models. Our initial
experiments using the multilingual model XLM-R show that indeed multilingual
LMs capture moral norms, even with potentially higher human-agreement than
monolingual ones. However, it is not yet clear to what extent these moral norms
differ between languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fake News Detection Using Majority Voting Technique. (arXiv:2203.09936v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09936">
<div class="article-summary-box-inner">
<span><p>Due to the evolution of the Web and social network platforms it becomes very
easy to disseminate the information. Peoples are creating and sharing more
information than ever before, which may be misleading, misinformation or fake
information. Fake news detection is a crucial and challenging task due to the
unstructured nature of the available information. In the recent years,
researchers have provided significant solutions to tackle with the problem of
fake news detection, but due to its nature there are still many open issues. In
this paper, we have proposed majority voting approach to detect fake news
articles. We have used different textual properties of fake and real news. We
have used publicly available fake news dataset, comprising of 20,800 news
articles among which 10,387 are real and 10,413 are fake news labeled as binary
0 and 1. For the evaluation of our approach, we have used commonly used machine
learning classifiers like, Decision Tree, Logistic Regression, XGBoost, Random
Forest, Extra Trees, AdaBoost, SVM, SGD and Naive Bayes. Using the
aforementioned classifiers, we built a multi-model fake news detection system
using Majority Voting technique to achieve the more accurate results. The
experimental results show that, our proposed approach achieved accuracy of
96.38%, precision of 96%, recall of 96% and F1-measure of 96%. The evaluation
confirms that, Majority Voting technique achieved more acceptable results as
compare to individual learning technique.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training a Tokenizer for Free with Private Federated Learning. (arXiv:2203.09943v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09943">
<div class="article-summary-box-inner">
<span><p>Federated learning with differential privacy, i.e. private federated learning
(PFL), makes it possible to train models on private data distributed across
users' devices without harming privacy. PFL is efficient for models, such as
neural networks, that have a fixed number of parameters, and thus a
fixed-dimensional gradient vector. Such models include neural-net language
models, but not tokenizers, the topic of this work. Training a tokenizer
requires frequencies of words from an unlimited vocabulary, and existing
methods for finding an unlimited vocabulary need a separate privacy budget.
</p>
<p>A workaround is to train the tokenizer on publicly available data. However,
in this paper we first show that a tokenizer trained on mismatched data results
in worse model performance compared to a privacy-violating "oracle" tokenizer
that accesses user data, with perplexity increasing by 20%. We also show that
sub-word tokenizers are better suited to the federated context than word-level
ones, since they can encode new words, though with more tokens per word.
</p>
<p>Second, we propose a novel method to obtain a tokenizer without using any
additional privacy budget. During private federated learning of the language
model, we sample from the model, train a new tokenizer on the sampled
sequences, and update the model embeddings. We then continue private federated
learning, and obtain performance within 1% of the "oracle" tokenizer. Since
this process trains the tokenizer only indirectly on private data, we can use
the "postprocessing guarantee" of differential privacy and thus use no
additional privacy budget.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt-based Generative Approach towards Multi-Hierarchical Medical Dialogue State Tracking. (arXiv:2203.09946v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09946">
<div class="article-summary-box-inner">
<span><p>The medical dialogue system is a promising application that can provide great
convenience for patients. The dialogue state tracking (DST) module in the
medical dialogue system which interprets utterances into the machine-readable
structure for downstream tasks is particularly challenging. Firstly, the states
need to be able to represent compound entities such as symptoms with their body
part or diseases with degrees of severity to provide enough information for
decision support. Secondly, these named entities in the utterance might be
discontinuous and scattered across sentences and speakers. These also make it
difficult to annotate a large corpus which is essential for most methods.
Therefore, we first define a multi-hierarchical state structure. We annotate
and publish a medical dialogue dataset in Chinese. To the best of our
knowledge, there are no publicly available ones before. Then we propose a
Prompt-based Generative Approach which can generate slot values with
multi-hierarchies incrementally using a top-down approach. A dialogue style
prompt is also supplemented to utilize the large unlabeled dialogue corpus to
alleviate the data scarcity problem. The experiments show that our approach
outperforms other DST methods and is rather effective in the scenario with
little data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Lithuanian grammatical error correction. (arXiv:2203.09963v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09963">
<div class="article-summary-box-inner">
<span><p>Everyone wants to write beautiful and correct text, yet the lack of language
skills, experience, or hasty typing can result in errors. By employing the
recent advances in transformer architectures, we construct a grammatical error
correction model for Lithuanian, the language rich in archaic features. We
compare subword and byte-level approaches and share our best trained model,
achieving F$_{0.5}$=0.92, and accompanying code, in an online open-source
repository.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BIOS: An Algorithmically Generated Biomedical Knowledge Graph. (arXiv:2203.09975v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09975">
<div class="article-summary-box-inner">
<span><p>Biomedical knowledge graphs (BioMedKGs) are essential infrastructures for
biomedical and healthcare big data and artificial intelligence (AI),
facilitating natural language processing, model development, and data exchange.
For many decades, these knowledge graphs have been built via expert curation,
which can no longer catch up with the speed of today's AI development, and a
transition to algorithmically generated BioMedKGs is necessary. In this work,
we introduce the Biomedical Informatics Ontology System (BIOS), the first large
scale publicly available BioMedKG that is fully generated by machine learning
algorithms. BIOS currently contains 4.1 million concepts, 7.4 million terms in
two languages, and 7.3 million relation triplets. We introduce the methodology
for developing BIOS, which covers curation of raw biomedical terms,
computationally identifying synonymous terms and aggregating them to create
concept nodes, semantic type classification of the concepts, relation
identification, and biomedical machine translation. We provide statistics about
the current content of BIOS and perform preliminary assessment for term
quality, synonym grouping, and relation extraction. Results suggest that
machine learning-based BioMedKG development is a totally viable solution for
replacing traditional expert curation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CrossAligner & Co: Zero-Shot Transfer Methods for Task-Oriented Cross-lingual Natural Language Understanding. (arXiv:2203.09982v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09982">
<div class="article-summary-box-inner">
<span><p>Task-oriented personal assistants enable people to interact with a host of
devices and services using natural language. One of the challenges of making
neural dialogue systems available to more users is the lack of training data
for all but a few languages. Zero-shot methods try to solve this issue by
acquiring task knowledge in a high-resource language such as English with the
aim of transferring it to the low-resource language(s). To this end, we
introduce CrossAligner, the principal method of a variety of effective
approaches for zero-shot cross-lingual transfer based on learning alignment
from unlabelled parallel data. We present a quantitative analysis of individual
methods as well as their weighted combinations, several of which exceed
state-of-the-art (SOTA) scores as evaluated across nine languages, fifteen test
sets and three benchmark multilingual datasets. A detailed qualitative error
analysis of the best methods shows that our fine-tuned language models can
zero-shot transfer the task knowledge better than anticipated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph-Text Multi-Modal Pre-training for Medical Representation Learning. (arXiv:2203.09994v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09994">
<div class="article-summary-box-inner">
<span><p>As the volume of Electronic Health Records (EHR) sharply grows, there has
been emerging interest in learning the representation of EHR for healthcare
applications. Representation learning of EHR requires appropriate modeling of
the two dominant modalities in EHR: structured data and unstructured text. In
this paper, we present MedGTX, a pre-trained model for multi-modal
representation learning of the structured and textual EHR data. MedGTX uses a
novel graph encoder to exploit the graphical nature of structured EHR data, and
a text encoder to handle unstructured text, and a cross-modal encoder to learn
a joint representation space. We pre-train our model through four proxy tasks
on MIMIC-III, an open-source EHR data, and evaluate our model on two clinical
benchmarks and three novel downstream tasks which tackle real-world problems in
EHR data. The results consistently show the effectiveness of pre-training the
model for joint representation of both structured and unstructured information
from EHR. Given the promising performance of MedGTX, we believe this work opens
a new door to jointly understanding the two fundamental modalities of EHR data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FORCE: A Framework of Rule-Based Conversational Recommender System. (arXiv:2203.10001v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10001">
<div class="article-summary-box-inner">
<span><p>The conversational recommender systems (CRSs) have received extensive
attention in recent years. However, most of the existing works focus on various
deep learning models, which are largely limited by the requirement of
large-scale human-annotated datasets. Such methods are not able to deal with
the cold-start scenarios in industrial products. To alleviate the problem, we
propose FORCE, a Framework Of Rule-based Conversational Recommender system that
helps developers to quickly build CRS bots by simple configuration. We conduct
experiments on two datasets in different languages and domains to verify its
effectiveness and usability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CaMEL: Case Marker Extraction without Labels. (arXiv:2203.10010v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10010">
<div class="article-summary-box-inner">
<span><p>We introduce CaMEL (Case Marker Extraction without Labels), a novel and
challenging task in computational morphology that is especially relevant for
low-resource languages. We propose a first model for CaMEL that uses a
massively multilingual corpus to extract case markers in 83 languages based
only on a noun phrase chunker and an alignment system. To evaluate CaMEL, we
automatically construct a silver standard from UniMorph. The case markers
extracted by our model can be used to detect and visualise similarities and
differences between the case systems of different languages as well as to
annotate fine-grained deep cases in languages in which they are not overtly
marked.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Report from the NSF Future Directions Workshop on Automatic Evaluation of Dialog: Research Directions and Challenges. (arXiv:2203.10012v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10012">
<div class="article-summary-box-inner">
<span><p>This is a report on the NSF Future Directions Workshop on Automatic
Evaluation of Dialog. The workshop explored the current state of the art along
with its limitations and suggested promising directions for future work in this
important and very rapidly changing area of research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Challenges and Strategies in Cross-Cultural NLP. (arXiv:2203.10020v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10020">
<div class="article-summary-box-inner">
<span><p>Various efforts in the Natural Language Processing (NLP) community have been
made to accommodate linguistic diversity and serve speakers of many different
languages. However, it is important to acknowledge that speakers and the
content they produce and require, vary not just by language, but also by
culture. Although language and culture are tightly linked, there are important
differences. Analogous to cross-lingual and multilingual NLP, cross-cultural
and multicultural NLP considers these differences in order to better serve
users of NLP systems. We propose a principled framework to frame these efforts,
and survey existing and potential strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Offensive Language Detection in Under-resourced Algerian Dialectal Arabic Language. (arXiv:2203.10024v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10024">
<div class="article-summary-box-inner">
<span><p>This paper addresses the problem of detecting the offensive and abusive
content in Facebook comments, where we focus on the Algerian dialectal Arabic
which is one of under-resourced languages. The latter has a variety of dialects
mixed with different languages (i.e. Berber, French and English). In addition,
we deal with texts written in both Arabic and Roman scripts (i.e. Arabizi). Due
to the scarcity of works on the same language, we have built a new corpus
regrouping more than 8.7k texts manually annotated as normal, abusive and
offensive. We have conducted a series of experiments using the state-of-the-art
classifiers of text categorisation, namely: BiLSTM, CNN, FastText, SVM and NB.
The results showed acceptable performances, but the problem requires further
investigation on linguistic features to increase the identification accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RELIC: Retrieving Evidence for Literary Claims. (arXiv:2203.10053v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10053">
<div class="article-summary-box-inner">
<span><p>Humanities scholars commonly provide evidence for claims that they make about
a work of literature (e.g., a novel) in the form of quotations from the work.
We collect a large-scale dataset (RELiC) of 78K literary quotations and
surrounding critical analysis and use it to formulate the novel task of
literary evidence retrieval, in which models are given an excerpt of literary
analysis surrounding a masked quotation and asked to retrieve the quoted
passage from the set of all passages in the work. Solving this retrieval task
requires a deep understanding of complex literary and linguistic phenomena,
which proves challenging to methods that overwhelmingly rely on lexical and
semantic similarity matching. We implement a RoBERTa-based dense passage
retriever for this task that outperforms existing pretrained information
retrieval baselines; however, experiments and analysis by human domain experts
indicate that there is substantial room for improvement over our dense
retriever.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simulating Bandit Learning from User Feedback for Extractive Question Answering. (arXiv:2203.10079v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10079">
<div class="article-summary-box-inner">
<span><p>We study learning from user feedback for extractive question answering by
simulating feedback using supervised data. We cast the problem as contextual
bandit learning, and analyze the characteristics of several learning scenarios
with focus on reducing data annotation. We show that systems initially trained
on a small number of examples can dramatically improve given feedback from
users on model-predicted answers, and that one can use existing datasets to
deploy systems in new domains without any annotation, but instead improving the
system on-the-fly via user feedback.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Globetrotter: Connecting Languages by Connecting Images. (arXiv:2012.04631v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.04631">
<div class="article-summary-box-inner">
<span><p>Machine translation between many languages at once is highly challenging,
since training with ground truth requires supervision between all language
pairs, which is difficult to obtain. Our key insight is that, while languages
may vary drastically, the underlying visual appearance of the world remains
consistent. We introduce a method that uses visual observations to bridge the
gap between languages, rather than relying on parallel corpora or topological
properties of the representations. We train a model that aligns segments of
text from different languages if and only if the images associated with them
are similar and each image in turn is well-aligned with its textual
description. We train our model from scratch on a new dataset of text in over
fifty languages with accompanying images. Experiments show that our method
outperforms previous work on unsupervised word and sentence translation using
retrieval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shellcode_IA32: A Dataset for Automatic Shellcode Generation. (arXiv:2104.13100v4 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.13100">
<div class="article-summary-box-inner">
<span><p>We take the first step to address the task of automatically generating
shellcodes, i.e., small pieces of code used as a payload in the exploitation of
a software vulnerability, starting from natural language comments. We assemble
and release a novel dataset (Shellcode_IA32), consisting of challenging but
common assembly instructions with their natural language descriptions. We
experiment with standard methods in neural machine translation (NMT) to
establish baseline performance levels on this task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assessing Multilingual Fairness in Pre-trained Multimodal Representations. (arXiv:2106.06683v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06683">
<div class="article-summary-box-inner">
<span><p>Recently pre-trained multimodal models, such as CLIP, have shown exceptional
capabilities towards connecting images and natural language. The textual
representations in English can be desirably transferred to multilingualism and
support downstream multimodal tasks for different languages. Nevertheless, the
principle of multilingual fairness is rarely scrutinized: do multilingual
multimodal models treat languages equally? Are their performances biased
towards particular languages? To answer these questions, we view language as
the fairness recipient and introduce two new fairness notions, multilingual
individual fairness and multilingual group fairness, for pre-trained multimodal
models. Multilingual individual fairness requires that text snippets expressing
similar semantics in different languages connect similarly to images, while
multilingual group fairness requires equalized predictive performance across
languages. We characterize the extent to which pre-trained multilingual
vision-and-language representations are individually fair across languages.
However, extensive experiments demonstrate that multilingual representations do
not satisfy group fairness: (1) there is a severe multilingual accuracy
disparity issue; (2) the errors exhibit biases across languages conditioning
the group of people in the images, including race, gender and age.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tailor: Generating and Perturbing Text with Semantic Controls. (arXiv:2107.07150v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.07150">
<div class="article-summary-box-inner">
<span><p>Controlled text perturbation is useful for evaluating and improving model
generalizability. However, current techniques rely on training a model for
every target perturbation, which is expensive and hard to generalize. We
present Tailor, a semantically-controlled text generation system. Tailor builds
on a pretrained seq2seq model and produces textual outputs conditioned on
control codes derived from semantic representations. We craft a set of
operations to modify the control codes, which in turn steer generation towards
targeted attributes. These operations can be further composed into higher-level
ones, allowing for flexible perturbation strategies. We demonstrate the
effectiveness of these perturbations in multiple applications. First, we use
Tailor to automatically create high-quality contrast sets for four distinct
natural language processing (NLP) tasks. These contrast sets contain fewer
spurious artifacts and are complementary to manually annotated ones in their
lexical diversity. Second, we show that Tailor perturbations can improve model
generalization through data augmentation. Perturbing just 2% of training data
leads to a 5.8-point gain on an NLI challenge set measuring reliance on
syntactic heuristics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification. (arXiv:2108.02035v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02035">
<div class="article-summary-box-inner">
<span><p>Tuning pre-trained language models (PLMs) with task-specific prompts has been
a promising approach for text classification. Particularly, previous studies
suggest that prompt-tuning has remarkable superiority in the low-data scenario
over the generic fine-tuning methods with extra classifiers. The core idea of
prompt-tuning is to insert text pieces, i.e., template, to the input and
transform a classification problem into a masked language modeling problem,
where a crucial step is to construct a projection, i.e., verbalizer, between a
label space and a label word space. A verbalizer is usually handcrafted or
searched by gradient descent, which may lack coverage and bring considerable
bias and high variances to the results. In this work, we focus on incorporating
external knowledge into the verbalizer, forming a knowledgeable prompt-tuning
(KPT), to improve and stabilize prompt-tuning. Specifically, we expand the
label word space of the verbalizer using external knowledge bases (KBs) and
refine the expanded label word space with the PLM itself before predicting with
the expanded label word space. Extensive experiments on zero and few-shot text
classification tasks demonstrate the effectiveness of knowledgeable
prompt-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MELM: Data Augmentation with Masked Entity Language Modeling for Low-Resource NER. (arXiv:2108.13655v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13655">
<div class="article-summary-box-inner">
<span><p>Data augmentation is an effective solution to data scarcity in low-resource
scenarios. However, when applied to token-level tasks such as NER, data
augmentation methods often suffer from token-label misalignment, which leads to
unsatsifactory performance. In this work, we propose Masked Entity Language
Modeling (MELM) as a novel data augmentation framework for low-resource NER. To
alleviate the token-label misalignment issue, we explicitly inject NER labels
into sentence context, and thus the fine-tuned MELM is able to predict masked
entity tokens by explicitly conditioning on their labels. Thereby, MELM
generates high-quality augmented data with novel entities, which provides rich
entity regularity knowledge and boosts NER performance. When training data from
multiple languages are available, we also integrate MELM with code-mixing for
further improvement. We demonstrate the effectiveness of MELM on monolingual,
cross-lingual and multilingual NER across various low-resource levels.
Experimental results show that our MELM presents substantial improvement over
the baseline methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformers in the loop: Polarity in neural models of language. (arXiv:2109.03926v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03926">
<div class="article-summary-box-inner">
<span><p>Representation of linguistic phenomena in computational language models is
typically assessed against the predictions of existing linguistic theories of
these phenomena. Using the notion of polarity as a case study, we show that
this is not always the most adequate set-up. We probe polarity via so-called
'negative polarity items' (in particular, English 'any') in two pre-trained
Transformer-based models (BERT and GPT-2). We show that - at least for polarity
- metrics derived from language models are more consistent with data from
psycholinguistic experiments than linguistic theory predictions. Establishing
this allows us to more adequately evaluate the performance of language models
and also to use language models to discover new insights into natural language
grammar beyond existing linguistic theories. This work contributes to
establishing closer ties between psycholinguistic experiments and experiments
with language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BiRdQA: A Bilingual Dataset for Question Answering on Tricky Riddles. (arXiv:2109.11087v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11087">
<div class="article-summary-box-inner">
<span><p>A riddle is a question or statement with double or veiled meanings, followed
by an unexpected answer. Solving riddle is a challenging task for both machine
and human, testing the capability of understanding figurative, creative natural
language and reasoning with commonsense knowledge. We introduce BiRdQA, a
bilingual multiple-choice question answering dataset with 6614 English riddles
and 8751 Chinese riddles. For each riddle-answer pair, we provide four
distractors with additional information from Wikipedia. The distractors are
automatically generated at scale with minimal bias. Existing monolingual and
multilingual QA models fail to perform well on our dataset, indicating that
there is a long way to go before machine can beat human on solving tricky
riddles. The dataset has been released to the community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts. (arXiv:2110.01691v3 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01691">
<div class="article-summary-box-inner">
<span><p>Although large language models (LLMs) have demonstrated impressive potential
on simple tasks, their breadth of scope, lack of transparency, and insufficient
controllability can make them less effective when assisting humans on more
complex tasks. In response, we introduce the concept of Chaining LLM steps
together, where the output of one step becomes the input for the next, thus
aggregating the gains per step. We first define a set of LLM primitive
operations useful for Chain construction, then present an interactive system
where users can modify these Chains, along with their intermediate results, in
a modular way. In a 20-person user study, we found that Chaining not only
improved the quality of task outcomes, but also significantly enhanced system
transparency, controllability, and sense of collaboration. Additionally, we saw
that users developed new ways of interacting with LLMs through Chains: they
leveraged sub-tasks to calibrate model expectations, compared and contrasted
alternative strategies by observing parallel downstream effects, and debugged
unexpected model outputs by "unit-testing" sub-components of a Chain. In two
case studies, we further explore how LLM Chains may be used in future
applications
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PAMA-TTS: Progression-Aware Monotonic Attention for Stable Seq2Seq TTS With Accurate Phoneme Duration Control. (arXiv:2110.04486v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04486">
<div class="article-summary-box-inner">
<span><p>Sequence expansion between encoder and decoder is a critical challenge in
sequence-to-sequence TTS. Attention-based methods achieve great naturalness but
suffer from unstable issues like missing and repeating phonemes, not to mention
accurate duration control. Duration-informed methods, on the contrary, seem to
easily adjust phoneme duration but show obvious degradation in speech
naturalness. This paper proposes PAMA-TTS to address the problem. It takes the
advantage of both flexible attention and explicit duration models. Based on the
monotonic attention mechanism, PAMA-TTS also leverages token duration and
relative position of a frame, especially countdown information, i.e. in how
many future frames the present phoneme will end. They help the attention to
move forward along the token sequence in a soft but reliable control.
Experimental results prove that PAMA-TTS achieves the highest naturalness,
while has on-par or even better duration controllability than the
duration-informed model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpreting the Robustness of Neural NLP Models to Textual Perturbations. (arXiv:2110.07159v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07159">
<div class="article-summary-box-inner">
<span><p>Modern Natural Language Processing (NLP) models are known to be sensitive to
input perturbations and their performance can decrease when applied to
real-world, noisy data. However, it is still unclear why models are less robust
to some perturbations than others. In this work, we test the hypothesis that
the extent to which a model is affected by an unseen textual perturbation
(robustness) can be explained by the learnability of the perturbation (defined
as how well the model learns to identify the perturbation with a small amount
of evidence). We further give a causal justification for the learnability
metric. We conduct extensive experiments with four prominent NLP models --
TextRNN, BERT, RoBERTa and XLNet -- over eight types of textual perturbations
on three datasets. We show that a model which is better at identifying a
perturbation (higher learnability) becomes worse at ignoring such a
perturbation at test time (lower robustness), providing empirical support for
our hypothesis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fusing ASR Outputs in Joint Training for Speech Emotion Recognition. (arXiv:2110.15684v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15684">
<div class="article-summary-box-inner">
<span><p>Alongside acoustic information, linguistic features based on speech
transcripts have been proven useful in Speech Emotion Recognition (SER).
However, due to the scarcity of emotion labelled data and the difficulty of
recognizing emotional speech, it is hard to obtain reliable linguistic features
and models in this research area. In this paper, we propose to fuse Automatic
Speech Recognition (ASR) outputs into the pipeline for joint training SER. The
relationship between ASR and SER is understudied, and it is unclear what and
how ASR features benefit SER. By examining various ASR outputs and fusion
methods, our experiments show that in joint ASR-SER training, incorporating
both ASR hidden and text output using a hierarchical co-attention fusion
approach improves the SER performance the most. On the IEMOCAP corpus, our
approach achieves 63.4% weighted accuracy, which is close to the baseline
results achieved by combining ground-truth transcripts. In addition, we also
present novel word error rate analysis on IEMOCAP and layer-difference analysis
of the Wav2vec 2.0 model to better understand the relationship between ASR and
SER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Study of Training End-to-End Vision-and-Language Transformers. (arXiv:2111.02387v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02387">
<div class="article-summary-box-inner">
<span><p>Vision-and-language (VL) pre-training has proven to be highly effective on
various VL downstream tasks. While recent work has shown that fully
transformer-based VL models can be more efficient than previous
region-feature-based methods, their performance on downstream tasks often
degrades significantly. In this paper, we present METER, a Multimodal
End-to-end TransformER framework, through which we investigate how to design
and pre-train a fully transformer-based VL model in an end-to-end manner.
Specifically, we dissect the model designs along multiple dimensions: vision
encoders (e.g., CLIP-ViT, Swin transformer), text encoders (e.g., RoBERTa,
DeBERTa), multimodal fusion module (e.g., merged attention vs. co-attention),
architectural design (e.g., encoder-only vs. encoder-decoder), and pre-training
objectives (e.g., masked image modeling). We conduct comprehensive experiments
and provide insights on how to train a performant VL transformer. METER
achieves an accuracy of 77.64% on the VQAv2 test-std set using only 4M images
for pre-training, surpassing the state-of-the-art region-feature-based model by
1.04%, and outperforming the previous best fully transformer-based model by
1.6%. Notably, when further scaled up, our best VQA model achieves an accuracy
of 80.54%. Code and pre-trained models are released at
https://github.com/zdou0830/METER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Correcting diacritics and typos with a ByT5 transformer model. (arXiv:2201.13242v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.13242">
<div class="article-summary-box-inner">
<span><p>Due to the fast pace of life and online communications and the prevalence of
English and the QWERTY keyboard, people tend to forgo using diacritics, make
typographical errors (typos) when typing in other languages. Restoring
diacritics and correcting spelling is important for proper language use and the
disambiguation of texts for both humans and downstream algorithms. However,
both of these problems are typically addressed separately: the state-of-the-art
diacritics restoration methods do not tolerate other typos, but classical
spellcheckers also cannot deal adequately with all the diacritics missing. In
this work, we tackle both problems at once by employing the newly-developed
universal ByT5 byte-level seq2seq transformer model that requires no
language-specific model structures. For a comparison, we perform diacritics
restoration on benchmark datasets of 12 languages, with the addition of
Lithuanian. The experimental investigation proves that our approach is able to
achieve results (&gt; 98%) comparable to the previous state-of-the-art, despite
being trained less and on fewer data. Our approach is also able to restore
diacritics in words not seen during training with &gt; 76% accuracy. Our
simultaneous diacritics restoration and typos correction approach reaches &gt; 94%
alpha-word accuracy on the 13 languages. It has no direct competitors and
strongly outperforms classical spell-checking or dictionary-based approaches.
We also demonstrate all the accuracies to further improve with more training.
Taken together, this shows the great real-world application potential of our
suggested methods to more data, languages, and error classes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Streaming Multi-Talker ASR with Token-Level Serialized Output Training. (arXiv:2202.00842v4 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00842">
<div class="article-summary-box-inner">
<span><p>This paper proposes a token-level serialized output training (t-SOT), a novel
framework for streaming multi-talker automatic speech recognition (ASR). Unlike
existing streaming multi-talker ASR models using multiple output layers, the
t-SOT model has only a single output layer that generates recognition tokens
(e.g., words, subwords) of multiple speakers in chronological order based on
their emission times. A special token that indicates the change of "virtual"
output channels is introduced to keep track of the overlapping utterances.
Compared to the prior streaming multi-talker ASR models, the t-SOT model has
the advantages of less inference cost and a simpler model architecture.
Moreover, in our experiments with LibriSpeechMix and LibriCSS datasets, the
t-SOT-based transformer transducer model achieves the state-of-the-art word
error rates by a significant margin to the prior results. For non-overlapping
speech, the t-SOT model is on par with a single-talker ASR model in terms of
both accuracy and computational cost, opening the door for deploying one model
for both single- and multi-talker scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing BERT's priors with serial reproduction chains. (arXiv:2202.12226v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12226">
<div class="article-summary-box-inner">
<span><p>Sampling is a promising bottom-up method for exposing what generative models
have learned about language, but it remains unclear how to generate
representative samples from popular masked language models (MLMs) like BERT.
The MLM objective yields a dependency network with no guarantee of consistent
conditional distributions, posing a problem for naive approaches. Drawing from
theories of iterated learning in cognitive science, we explore the use of
serial reproduction chains to sample from BERT's priors. In particular, we
observe that a unique and consistent estimator of the ground-truth joint
distribution is given by a Generative Stochastic Network (GSN) sampler, which
randomly selects which token to mask and reconstruct on each step. We show that
the lexical and syntactic statistics of sentences from GSN chains closely match
the ground-truth corpus distribution and perform better than other methods in a
large corpus of naturalness judgments. Our findings establish a firmer
theoretical foundation for bottom-up probing and highlight richer deviations
from human priors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Overcoming Catastrophic Forgetting beyond Continual Learning: Balanced Training for Neural Machine Translation. (arXiv:2203.03910v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03910">
<div class="article-summary-box-inner">
<span><p>Neural networks tend to gradually forget the previously learned knowledge
when learning multiple tasks sequentially from dynamic data distributions. This
problem is called \textit{catastrophic forgetting}, which is a fundamental
challenge in the continual learning of neural networks. In this work, we
observe that catastrophic forgetting not only occurs in continual learning but
also affects the traditional static training. Neural networks, especially
neural machine translation models, suffer from catastrophic forgetting even if
they learn from a static training set. To be specific, the final model pays
imbalanced attention to training samples, where recently exposed samples
attract more attention than earlier samples. The underlying cause is that
training samples do not get balanced training in each model update, so we name
this problem \textit{imbalanced training}. To alleviate this problem, we
propose Complementary Online Knowledge Distillation (COKD), which uses
dynamically updated teacher models trained on specific data orders to
iteratively provide complementary knowledge to the student model. Experimental
results on multiple machine translation tasks show that our method successfully
alleviates the problem of imbalanced training and achieves substantial
improvements over strong baseline systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BEAT: A Large-Scale Semantic and Emotional Multi-Modal Dataset for Conversational Gestures Synthesis. (arXiv:2203.05297v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05297">
<div class="article-summary-box-inner">
<span><p>Achieving realistic, vivid, and human-like synthesized conversational
gestures conditioned on multi-modal data is still an unsolved problem, due to
the lack of available datasets, models and standard evaluation metrics. To
address this, we build Body-Expression-Audio-Text dataset, BEAT, which has i)
76 hours, high-quality, multi-modal data captured from 30 speakers talking with
eight different emotions and in four different languages, ii) 32 millions
frame-level emotion and semantic relevance annotations.Our statistical analysis
on BEAT demonstrates the correlation of conversational gestures with facial
expressions, emotions, and semantics, in addition to the known correlation with
audio, text, and speaker identity. Qualitative and quantitative experiments
demonstrate metrics' validness, ground truth data quality, and baseline's
state-of-the-art performance. To the best of our knowledge, BEAT is the largest
motion capture dataset for investigating the human gestures, which may
contribute to a number of different research fields including controllable
gesture synthesis, cross-modality analysis, emotional gesture recognition. The
data, code and model will be released for research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Afrocentric NLP for African Languages: Where We Are and Where We Can Go. (arXiv:2203.08351v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08351">
<div class="article-summary-box-inner">
<span><p>Aligning with ACL 2022 special Theme on "Language Diversity: from Low
Resource to Endangered Languages", we discuss the major linguistic and
sociopolitical challenges facing development of NLP technologies for African
languages. Situating African languages in a typological framework, we discuss
how the particulars of these languages can be harnessed. To facilitate future
research, we also highlight current efforts, communities, venues, datasets, and
tools. Our main objective is to motivate and advocate for an Afrocentric
approach to technology development. With this in mind, we recommend
\textit{what} technologies to build and \textit{how} to build, evaluate, and
deploy them based on the needs of local African communities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Many Data Samples is an Additional Instruction Worth?. (arXiv:2203.09161v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09161">
<div class="article-summary-box-inner">
<span><p>Recently introduced instruction-paradigm empowers non-expert users to
leverage NLP resources by defining a new task in natural language.
Instruction-tuned models have significantly outperformed multitask learning
models (without instruction); however they are far from state of the art task
specific models. Conventional approaches to improve model performance via
creating large datasets with lots of task instances or architectural/training
changes in model may not be feasible for non-expert users. However, they can
write alternate instructions to represent an instruction task. Is
Instruction-augumentation helpful? We augment a subset of tasks in the expanded
version of NATURAL INSTRUCTIONS with additional instructions and find that
these significantly improve model performance (up to 35%), especially in the
low-data regime. Our results indicate that an additional instruction can be
equivalent to ~200 data samples on average across tasks.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Human Gait Analysis using Gait Energy Image. (arXiv:2203.09549v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09549">
<div class="article-summary-box-inner">
<span><p>Gait recognition is one of the most recent emerging techniques of human
biometric which can be used for security based purposes having unobtrusive
learning method. In comparison with other bio-metrics gait analysis has some
special security features. Most of the biometric technique uses sequential
template based component analysis for recognition. Comparing with those
methods, we proposed a developed technique for gait identification using the
feature Gait Energy Image (GEI). GEI representation of gait contains all
information of each image in one gait cycle and requires less storage and low
processing speed. As only one image is enough to store the necessary
information in GEI feature recognition process is very easier than any other
feature for gait recognition. Gait recognition has some limitations in
recognition process like viewing angle variation, walking speed, clothes,
carrying load etc. Our proposed method in the paper compares the recognition
performance with template based feature extraction which needs to process each
frame in the cycle. We use GEI which gives relatively all information about all
the frames in the cycle and results in better performance than other feature of
gait analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-similarity based Hyperrelation Network for few-shot segmentation. (arXiv:2203.09550v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09550">
<div class="article-summary-box-inner">
<span><p>Few-shot semantic segmentation aims at recognizing the object regions of
unseen categories with only a few annotated examples as supervision. The key to
few-shot segmentation is to establish a robust semantic relationship between
the support and query images and to prevent overfitting. In this paper, we
propose an effective Multi-similarity Hyperrelation Network (MSHNet) to tackle
the few-shot semantic segmentation problem. In MSHNet, we propose a new
Generative Prototype Similarity (GPS), which together with cosine similarity
can establish a strong semantic relation between the support and query images.
The locally generated prototype similarity based on global feature is logically
complementary to the global cosine similarity based on local feature, and the
relationship between the query image and the supported image can be expressed
more comprehensively by using the two similarities simultaneously. In addition,
we propose a Symmetric Merging Block (SMB) in MSHNet to efficiently merge
multi-layer, multi-shot and multi-similarity hyperrelational features. MSHNet
is built on the basis of similarity rather than specific category features,
which can achieve more general unity and effectively reduce overfitting. On two
benchmark semantic segmentation datasets Pascal-5i and COCO-20i, MSHNet
achieves new state-of-the-art performances on 1-shot and 5-shot semantic
segmentation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoGS: Controllable Generation and Search from Sketch and Style. (arXiv:2203.09554v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09554">
<div class="article-summary-box-inner">
<span><p>We present CoGS, a novel method for the style-conditioned, sketch-driven
synthesis of images. CoGS enables exploration of diverse appearance
possibilities for a given sketched object, enabling decoupled control over the
structure and the appearance of the output. Coarse-grained control over object
structure and appearance are enabled via an input sketch and an exemplar
"style" conditioning image to a transformer-based sketch and style encoder to
generate a discrete codebook representation. We map the codebook representation
into a metric space, enabling fine-grained control over selection and
interpolation between multiple synthesis options for a given image before
generating the image via a vector quantized GAN (VQGAN) decoder. Our framework
thereby unifies search and synthesis tasks, in that a sketch and style pair may
be used to run an initial synthesis which may be refined via combination with
similar results in a search corpus to produce an image more closely matching
the user's intent. We show that our model, trained on the 125 object classes of
our newly created Pseudosketches dataset, is capable of producing a diverse
gamut of semantic content and appearance styles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Surface Defect Detection and Evaluation for Marine Vessels using Multi-Stage Deep Learning. (arXiv:2203.09580v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09580">
<div class="article-summary-box-inner">
<span><p>Detecting and evaluating surface coating defects is important for marine
vessel maintenance. Currently, the assessment is carried out manually by
qualified inspectors using international standards and their own experience.
Automating the processes is highly challenging because of the high level of
variation in vessel type, paint surface, coatings, lighting condition, weather
condition, paint colors, areas of the vessel, and time in service. We present a
novel deep learning-based pipeline to detect and evaluate the percentage of
corrosion, fouling, and delamination on the vessel surface from normal
photographs. We propose a multi-stage image processing framework, including
ship section segmentation, defect segmentation, and defect classification, to
automatically recognize different types of defects and measure the coverage
percentage on the ship surface. Experimental results demonstrate that our
proposed pipeline can objectively perform a similar assessment as a qualified
inspector.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SepTr: Separable Transformer for Audio Spectrogram Processing. (arXiv:2203.09581v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09581">
<div class="article-summary-box-inner">
<span><p>Following the successful application of vision transformers in multiple
computer vision tasks, these models have drawn the attention of the signal
processing community. This is because signals are often represented as
spectrograms (e.g. through Discrete Fourier Transform) which can be directly
provided as input to vision transformers. However, naively applying
transformers to spectrograms is suboptimal. Since the axes represent distinct
dimensions, i.e. frequency and time, we argue that a better approach is to
separate the attention dedicated to each axis. To this end, we propose the
Separable Transformer (SepTr), an architecture that employs two transformer
blocks in a sequential manner, the first attending to tokens within the same
frequency bin, and the second attending to tokens within the same time
interval. We conduct experiments on three benchmark data sets, showing that our
separable architecture outperforms conventional vision transformers and other
state-of-the-art methods. Unlike standard transformers, SepTr linearly scales
the number of trainable parameters with the input size, thus having a lower
memory footprint. Our code is available as open source at
https://github.com/ristea/septr.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video-based Formative and Summative Assessment of Surgical Tasks using Deep Learning. (arXiv:2203.09589v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09589">
<div class="article-summary-box-inner">
<span><p>To ensure satisfactory clinical outcomes, surgical skill assessment must be
objective, time-efficient, and preferentially automated - none of which is
currently achievable. Video-based assessment (VBA) is being deployed in
intraoperative and simulation settings to evaluate technical skill execution.
However, VBA remains manually- and time-intensive and prone to subjective
interpretation and poor inter-rater reliability. Herein, we propose a deep
learning (DL) model that can automatically and objectively provide a
high-stakes summative assessment of surgical skill execution based on video
feeds and low-stakes formative assessment to guide surgical skill acquisition.
Formative assessment is generated using heatmaps of visual features that
correlate with surgical performance. Hence, the DL model paves the way to the
quantitative and reproducible evaluation of surgical tasks from videos with the
potential for broad dissemination in surgical training, certification, and
credentialing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Delta Distillation for Efficient Video Processing. (arXiv:2203.09594v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09594">
<div class="article-summary-box-inner">
<span><p>This paper aims to accelerate video stream processing, such as object
detection and semantic segmentation, by leveraging the temporal redundancies
that exist between video frames. Instead of propagating and warping features
using motion alignment, such as optical flow, we propose a novel knowledge
distillation schema coined as Delta Distillation. In our proposal, the student
learns the variations in the teacher's intermediate features over time. We
demonstrate that these temporal variations can be effectively distilled due to
the temporal redundancies within video frames. During inference, both teacher
and student cooperate for providing predictions: the former by providing
initial representations extracted only on the key-frame, and the latter by
iteratively estimating and applying deltas for the successive frames. Moreover,
we consider various design choices to learn optimal student architectures
including an end-to-end learnable architecture search. By extensive experiments
on a wide range of architectures, including the most efficient ones, we
demonstrate that delta distillation sets a new state of the art in terms of
accuracy vs. efficiency trade-off for semantic segmentation and object
detection in videos. Finally, we show that, as a by-product, delta distillation
improves the temporal consistency of the teacher model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified Line and Paragraph Detection by Graph Convolutional Networks. (arXiv:2203.09638v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09638">
<div class="article-summary-box-inner">
<span><p>We formulate the task of detecting lines and paragraphs in a document into a
unified two-level clustering problem. Given a set of text detection boxes that
roughly correspond to words, a text line is a cluster of boxes and a paragraph
is a cluster of lines. These clusters form a two-level tree that represents a
major part of the layout of a document. We use a graph convolutional network to
predict the relations between text detection boxes and then build both levels
of clusters from these predictions. Experimentally, we demonstrate that the
unified approach can be highly efficient while still achieving state-of-the-art
quality for detecting paragraphs in public benchmarks and real-world images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cascade Transformers for End-to-End Person Search. (arXiv:2203.09642v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09642">
<div class="article-summary-box-inner">
<span><p>The goal of person search is to localize a target person from a gallery set
of scene images, which is extremely challenging due to large scale variations,
pose/viewpoint changes, and occlusions. In this paper, we propose the Cascade
Occluded Attention Transformer (COAT) for end-to-end person search. Our
three-stage cascade design focuses on detecting people in the first stage,
while later stages simultaneously and progressively refine the representation
for person detection and re-identification. At each stage the occluded
attention transformer applies tighter intersection over union thresholds,
forcing the network to learn coarse-to-fine pose/scale invariant features.
Meanwhile, we calculate each detection's occluded attention to differentiate a
person's tokens from other people or the background. In this way, we simulate
the effect of other objects occluding a person of interest at the token-level.
Through comprehensive experiments, we demonstrate the benefits of our method by
achieving state-of-the-art performance on two benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MatchFormer: Interleaving Attention in Transformers for Feature Matching. (arXiv:2203.09645v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09645">
<div class="article-summary-box-inner">
<span><p>Local feature matching is a computationally intensive task at the subpixel
level. While detector-based methods coupled with feature descriptors struggle
in low-texture scenes, CNN-based methods with a sequential extract-to-match
pipeline, fail to make use of the matching capacity of the encoder and tend to
overburden the decoder for matching. In contrast, we propose a novel
hierarchical extract-and-match transformer, termed as MatchFormer. Inside each
stage of the hierarchical encoder, we interleave self-attention for feature
extraction and cross-attention for feature matching, enabling a human-intuitive
extract-and-match scheme. Such a match-aware encoder releases the overloaded
decoder and makes the model highly efficient. Further, combining self- and
cross-attention on multi-scale features in a hierarchical architecture improves
matching robustness, particularly in low-texture indoor scenes or with less
outdoor training data. Thanks to such a strategy, MatchFormer is a multi-win
solution in efficiency, robustness, and precision. Compared to the previous
best method in indoor pose estimation, our lite MatchFormer has only 45%
GFLOPs, yet achieves a +1.3% precision gain and a 41% running speed boost. The
large MatchFormer reaches state-of-the-art on four different benchmarks,
including indoor pose estimation (ScanNet), outdoor pose estimation
(MegaDepth), homography estimation and image matching (HPatch), and visual
localization (InLoc). Code will be made publicly available at
https://github.com/jamycheung/MatchFormer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Regional Semantic Contrast and Aggregation for Weakly Supervised Semantic Segmentation. (arXiv:2203.09653v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09653">
<div class="article-summary-box-inner">
<span><p>Learning semantic segmentation from weakly-labeled (e.g., image tags only)
data is challenging since it is hard to infer dense object regions from sparse
semantic tags. Despite being broadly studied, most current efforts directly
learn from limited semantic annotations carried by individual image or image
pairs, and struggle to obtain integral localization maps. Our work alleviates
this from a novel perspective, by exploring rich semantic contexts
synergistically among abundant weakly-labeled training data for network
learning and inference. In particular, we propose regional semantic contrast
and aggregation (RCA) . RCA is equipped with a regional memory bank to store
massive, diverse object patterns appearing in training data, which acts as
strong support for exploration of dataset-level semantic structure.
Particularly, we propose i) semantic contrast to drive network learning by
contrasting massive categorical object regions, leading to a more holistic
object pattern understanding, and ii) semantic aggregation to gather diverse
relational contexts in the memory to enrich semantic representations. In this
manner, RCA earns a strong capability of fine-grained semantic understanding,
and eventually establishes new state-of-the-art results on two popular
benchmarks, i.e., PASCAL VOC 2012 and COCO 2014.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A workflow for segmenting soil and plant X-ray CT images with deep learning in Googles Colaboratory. (arXiv:2203.09674v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09674">
<div class="article-summary-box-inner">
<span><p>X-ray micro-computed tomography (X-ray microCT) has enabled the
characterization of the properties and processes that take place in plants and
soils at the micron scale. Despite the widespread use of this advanced
technique, major limitations in both hardware and software limit the speed and
accuracy of image processing and data analysis. Recent advances in machine
learning, specifically the application of convolutional neural networks to
image analysis, have enabled rapid and accurate segmentation of image data.
Yet, challenges remain in applying convolutional neural networks to the
analysis of environmentally and agriculturally relevant images. Specifically,
there is a disconnect between the computer scientists and engineers, who build
these AI/ML tools, and the potential end users in agricultural research, who
may be unsure of how to apply these tools in their work. Additionally, the
computing resources required for training and applying deep learning models are
unique, more common to computer gaming systems or graphics design work, than to
traditional computational systems. To navigate these challenges, we developed a
modular workflow for applying convolutional neural networks to X-ray microCT
images, using low-cost resources in Googles Colaboratory web application. Here
we present the results of the workflow, illustrating how parameters can be
optimized to achieve best results using example scans from walnut leaves,
almond flower buds, and a soil aggregate. We expect that this framework will
accelerate the adoption and use of emerging deep learning techniques within the
plant and soil sciences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Intensification for Sign Language Generation: A Computational Approach. (arXiv:2203.09679v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09679">
<div class="article-summary-box-inner">
<span><p>End-to-end sign language generation models do not accurately represent the
prosody in sign language. A lack of temporal and spatial variations leads to
poor-quality generated presentations that confuse human interpreters. In this
paper, we aim to improve the prosody in generated sign languages by modeling
intensification in a data-driven manner. We present different strategies
grounded in linguistics of sign language that inform how intensity modifiers
can be represented in gloss annotations. To employ our strategies, we first
annotate a subset of the benchmark PHOENIX-14T, a German Sign Language dataset,
with different levels of intensification. We then use a supervised intensity
tagger to extend the annotated dataset and obtain labels for the remaining
portion of it. This enhanced dataset is then used to train state-of-the-art
transformer models for sign language generation. We find that our efforts in
intensification modeling yield better results when evaluated with automatic
metrics. Human evaluation also indicates a higher preference of the videos
generated using our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Facial Geometric Detail Recovery via Implicit Representation. (arXiv:2203.09692v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09692">
<div class="article-summary-box-inner">
<span><p>Learning a dense 3D model with fine-scale details from a single facial image
is highly challenging and ill-posed. To address this problem, many approaches
fit smooth geometries through facial prior while learning details as additional
displacement maps or personalized basis. However, these techniques typically
require vast datasets of paired multi-view data or 3D scans, whereas such
datasets are scarce and expensive. To alleviate heavy data dependency, we
present a robust texture-guided geometric detail recovery approach using only a
single in-the-wild facial image. More specifically, our method combines
high-quality texture completion with the powerful expressiveness of implicit
surfaces. Initially, we inpaint occluded facial parts, generate complete
textures, and build an accurate multi-view dataset of the same subject. In
order to estimate the detailed geometry, we define an implicit signed distance
function and employ a physically-based implicit renderer to reconstruct fine
geometric details from the generated multi-view images. Our method not only
recovers accurate facial details but also decomposes normals, albedos, and
shading parts in a self-supervised way. Finally, we register the implicit shape
details to a 3D Morphable Model template, which can be used in traditional
modeling and rendering pipelines. Extensive experiments demonstrate that the
proposed approach can reconstruct impressive facial details from a single
image, especially when compared with state-of-the-art methods trained on large
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Group Contextualization for Video Recognition. (arXiv:2203.09694v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09694">
<div class="article-summary-box-inner">
<span><p>Learning discriminative representation from the complex spatio-temporal
dynamic space is essential for video recognition. On top of those stylized
spatio-temporal computational units, further refining the learnt feature with
axial contexts is demonstrated to be promising in achieving this goal. However,
previous works generally focus on utilizing a single kind of contexts to
calibrate entire feature channels and could hardly apply to deal with diverse
video activities. The problem can be tackled by using pair-wise spatio-temporal
attentions to recompute feature response with cross-axis contexts at the
expense of heavy computations. In this paper, we propose an efficient feature
refinement method that decomposes the feature channels into several groups and
separately refines them with different axial contexts in parallel. We refer
this lightweight feature calibration as group contextualization (GC).
Specifically, we design a family of efficient element-wise calibrators, i.e.,
ECal-G/S/T/L, where their axial contexts are information dynamics aggregated
from other axes either globally or locally, to contextualize feature channel
groups. The GC module can be densely plugged into each residual layer of the
off-the-shelf video networks. With little computational overhead, consistent
improvement is observed when plugging in GC on different networks. By utilizing
calibrators to embed feature with four different kinds of contexts in parallel,
the learnt representation is expected to be more resilient to diverse types of
activities. On videos with rich temporal variations, empirically GC can boost
the performance of 2D-CNN (e.g., TSN and TSM) to a level comparable to the
state-of-the-art video networks. Code is available at
https://github.com/haoyanbin918/Group-Contextualization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VISTA: Boosting 3D Object Detection via Dual Cross-VIew SpaTial Attention. (arXiv:2203.09704v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09704">
<div class="article-summary-box-inner">
<span><p>Detecting objects from LiDAR point clouds is of tremendous significance in
autonomous driving. In spite of good progress, accurate and reliable 3D
detection is yet to be achieved due to the sparsity and irregularity of LiDAR
point clouds. Among existing strategies, multi-view methods have shown great
promise by leveraging the more comprehensive information from both bird's eye
view (BEV) and range view (RV). These multi-view methods either refine the
proposals predicted from single view via fused features, or fuse the features
without considering the global spatial context; their performance is limited
consequently. In this paper, we propose to adaptively fuse multi-view features
in a global spatial context via Dual Cross-VIew SpaTial Attention (VISTA). The
proposed VISTA is a novel plug-and-play fusion module, wherein the multi-layer
perceptron widely adopted in standard attention modules is replaced with a
convolutional one. Thanks to the learned attention mechanism, VISTA can produce
fused features of high quality for prediction of proposals. We decouple the
classification and regression tasks in VISTA, and an additional constraint of
attention variance is applied that enables the attention module to focus on
specific targets instead of generic points. We conduct thorough experiments on
the benchmarks of nuScenes and Waymo; results confirm the efficacy of our
designs. At the time of submission, our method achieves 63.0% in overall mAP
and 69.8% in NDS on the nuScenes benchmark, outperforming all published methods
by up to 24% in safety-crucial categories such as cyclist. The source code in
PyTorch is available at https://github.com/Gorilla-Lab-SCUT/VISTA
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deterministic Bridge Regression for Compressive Classification. (arXiv:2203.09721v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09721">
<div class="article-summary-box-inner">
<span><p>Pattern classification with compact representation is an important component
in machine intelligence. In this work, an analytic bridge solution is proposed
for compressive classification. The proposal has been based upon solving a
penalized error formulation utilizing an approximated $\ell_p$-norm. The
solution comes in a primal form for over-determined systems and in a dual form
for under-determined systems. While the primal form is suitable for problems of
low dimension with large data samples, the dual form is suitable for problems
of high dimension but with a small number of data samples. The solution has
also been extended for problems with multiple classification outputs. Numerical
studies based on simulated and real-world data validated the effectiveness of
the proposed solution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking the optimization process for self-supervised model-driven MRI reconstruction. (arXiv:2203.09724v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09724">
<div class="article-summary-box-inner">
<span><p>Recovering high-quality images from undersampled measurements is critical for
accelerated MRI reconstruction. Recently, various supervised deep
learning-based MRI reconstruction methods have been developed. Despite the
achieved promising performances, these methods require fully sampled reference
data, the acquisition of which is resource-intensive and time-consuming.
Self-supervised learning has emerged as a promising solution to alleviate the
reliance on fully sampled datasets. However, existing self-supervised methods
suffer from reconstruction errors due to the insufficient constraint enforced
on the non-sampled data points and the error accumulation happened alongside
the iterative image reconstruction process for model-driven deep learning
reconstrutions. To address these challenges, we propose K2Calibrate, a K-space
adaptation strategy for self-supervised model-driven MR reconstruction
optimization. By iteratively calibrating the learned measurements, K2Calibrate
can reduce the network's reconstruction deterioration caused by statistically
dependent noise. Extensive experiments have been conducted on the open-source
dataset FastMRI, and K2Calibrate achieves better results than five
state-of-the-art methods. The proposed K2Calibrate is plug-and-play and can be
easily integrated with different model-driven deep learning reconstruction
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">REALY: Rethinking the Evaluation of 3D Face Reconstruction. (arXiv:2203.09729v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09729">
<div class="article-summary-box-inner">
<span><p>The evaluation of 3D face reconstruction results typically relies on a rigid
shape alignment between the estimated 3D model and the ground-truth scan. We
observe that aligning two shapes with different reference points can largely
affect the evaluation results. This poses difficulties for precisely diagnosing
and improving a 3D face reconstruction method. In this paper, we propose a
novel evaluation approach with a new benchmark REALY, consists of 100 globally
aligned face scans with accurate facial keypoints, high-quality region masks,
and topology-consistent meshes. Our approach performs region-wise shape
alignment and leads to more accurate, bidirectional correspondences during
computing the shape errors. The fine-grained, region-wise evaluation results
provide us detailed understandings about the performance of state-of-the-art 3D
face reconstruction methods. For example, our experiments on single-image based
reconstruction methods reveal that DECA performs the best on nose regions,
while GANFit performs better on cheek regions. Besides, a new and high-quality
3DMM basis, HIFI3D++, is further derived using the same procedure as we
construct REALY to align and retopologize several 3D face datasets. We will
release REALY, HIFI3D++, and our new evaluation pipeline at
https://realy3dface.com.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Dual Weighting Label Assignment Scheme for Object Detection. (arXiv:2203.09730v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09730">
<div class="article-summary-box-inner">
<span><p>Label assignment (LA), which aims to assign each training sample a positive
(pos) and a negative (neg) loss weight, plays an important role in object
detection. Existing LA methods mostly focus on the design of pos weighting
function, while the neg weight is directly derived from the pos weight. Such a
mechanism limits the learning capacity of detectors. In this paper, we explore
a new weighting paradigm, termed dual weighting (DW), to specify pos and neg
weights separately. We first identify the key influential factors of pos/neg
weights by analyzing the evaluation metrics in object detection, and then
design the pos and neg weighting functions based on them. Specifically, the pos
weight of a sample is determined by the consistency degree between its
classification and localization scores, while the neg weight is decomposed into
two terms: the probability that it is a neg sample and its importance
conditioned on being a neg sample. Such a weighting strategy offers greater
flexibility to distinguish between important and less important samples,
resulting in a more effective object detector. Equipped with the proposed DW
method, a single FCOS-ResNet-50 detector can reach 41.5% mAP on COCO under 1x
schedule, outperforming other existing LA methods. It consistently improves the
baselines on COCO by a large margin under various backbones without bells and
whistles. Code is available at https://github.com/strongwolf/DW.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distortion-Tolerant Monocular Depth Estimation On Omnidirectional Images Using Dual-cubemap. (arXiv:2203.09733v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09733">
<div class="article-summary-box-inner">
<span><p>Estimating the depth of omnidirectional images is more challenging than that
of normal field-of-view (NFoV) images because the varying distortion can
significantly twist an object's shape. The existing methods suffer from
troublesome distortion while estimating the depth of omnidirectional images,
leading to inferior performance. To reduce the negative impact of the
distortion influence, we propose a distortion-tolerant omnidirectional depth
estimation algorithm using a dual-cubemap. It comprises two modules:
Dual-Cubemap Depth Estimation (DCDE) module and Boundary Revision (BR) module.
In DCDE module, we present a rotation-based dual-cubemap model to estimate the
accurate NFoV depth, reducing the distortion at the cost of boundary
discontinuity on omnidirectional depths. Then a boundary revision module is
designed to smooth the discontinuous boundaries, which contributes to the
precise and visually continuous omnidirectional depths. Extensive experiments
demonstrate the superiority of our method over other state-of-the-art
solutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Series Photo Selection via Multi-view Graph Learning. (arXiv:2203.09736v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09736">
<div class="article-summary-box-inner">
<span><p>Series photo selection (SPS) is an important branch of the image aesthetics
quality assessment, which focuses on finding the best one from a series of
nearly identical photos. While a great progress has been observed, most of the
existing SPS approaches concentrate solely on extracting features from the
original image, neglecting that multiple views, e.g, saturation level, color
histogram and depth of field of the image, will be of benefit to successfully
reflecting the subtle aesthetic changes. Taken multi-view into consideration,
we leverage a graph neural network to construct the relationships between
multi-view features. Besides, multiple views are aggregated with an
adaptive-weight self-attention module to verify the significance of each view.
Finally, a siamese network is proposed to select the best one from a series of
nearly identical photos. Experimental results demonstrate that our model
accomplish the highest success rates compared with competitive methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Supervised Learning with Mutual Distillation for Monocular Depth Estimation. (arXiv:2203.09737v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09737">
<div class="article-summary-box-inner">
<span><p>We propose a semi-supervised learning framework for monocular depth
estimation. Compared to existing semi-supervised learning methods, which
inherit limitations of both sparse supervised and unsupervised loss functions,
we achieve the complementary advantages of both loss functions, by building two
separate network branches for each loss and distilling each other through the
mutual distillation loss function. We also present to apply different data
augmentation to each branch, which improves the robustness. We conduct
experiments to demonstrate the effectiveness of our framework over the latest
methods and provide extensive ablation studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Deep Networks Transfer Invariances Across Classes?. (arXiv:2203.09739v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09739">
<div class="article-summary-box-inner">
<span><p>To generalize well, classifiers must learn to be invariant to nuisance
transformations that do not alter an input's class. Many problems have
"class-agnostic" nuisance transformations that apply similarly to all classes,
such as lighting and background changes for image classification. Neural
networks can learn these invariances given sufficient data, but many real-world
datasets are heavily class imbalanced and contain only a few examples for most
of the classes. We therefore pose the question: how well do neural networks
transfer class-agnostic invariances learned from the large classes to the small
ones? Through careful experimentation, we observe that invariance to
class-agnostic transformations is still heavily dependent on class size, with
the networks being much less invariant on smaller classes. This result holds
even when using data balancing techniques, and suggests poor invariance
transfer across classes. Our results provide one explanation for why
classifiers generalize poorly on unbalanced and long-tailed distributions.
Based on this analysis, we show how a generative approach for learning the
nuisance transformations can help transfer invariances across classes and
improve performance on a set of imbalanced image classification benchmarks.
Source code for our experiments is available at
https://github.com/AllanYangZhou/generative-invariance-transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Class-Balanced Pixel-Level Self-Labeling for Domain Adaptive Semantic Segmentation. (arXiv:2203.09744v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09744">
<div class="article-summary-box-inner">
<span><p>Domain adaptive semantic segmentation aims to learn a model with the
supervision of source domain data, and produce satisfactory dense predictions
on unlabeled target domain. One popular solution to this challenging task is
self-training, which selects high-scoring predictions on target samples as
pseudo labels for training. However, the produced pseudo labels often contain
much noise because the model is biased to source domain as well as majority
categories. To address the above issues, we propose to directly explore the
intrinsic pixel distributions of target domain data, instead of heavily relying
on the source domain. Specifically, we simultaneously cluster pixels and
rectify pseudo labels with the obtained cluster assignments. This process is
done in an online fashion so that pseudo labels could co-evolve with the
segmentation model without extra training rounds. To overcome the class
imbalance problem on long-tailed categories, we employ a distribution alignment
technique to enforce the marginal class distribution of cluster assignments to
be close to that of pseudo labels. The proposed method, namely Class-balanced
Pixel-level Self-Labeling (CPSL), improves the segmentation performance on
target domain over state-of-the-arts by a large margin, especially on
long-tailed categories.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robot peels banana with goal-conditioned dual-action deep imitation learning. (arXiv:2203.09749v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09749">
<div class="article-summary-box-inner">
<span><p>A long-horizon dexterous robot manipulation task of deformable objects, such
as banana peeling, is problematic because of difficulties in object modeling
and a lack of knowledge about stable and dexterous manipulation skills. This
paper presents a goal-conditioned dual-action deep imitation learning (DIL)
which can learn dexterous manipulation skills using human demonstration data.
Previous DIL methods map the current sensory input and reactive action, which
easily fails because of compounding errors in imitation learning caused by
recurrent computation of actions. The proposed method predicts reactive action
when the precise manipulation of the target object is required (local action)
and generates the entire trajectory when the precise manipulation is not
required. This dual-action formulation effectively prevents compounding error
with the trajectory-based global action while respond to unexpected changes in
the target object with the reactive local action. Furthermore, in this
formulation, both global/local actions are conditioned by a goal state which is
defined as the last step of each subtask, for robust policy prediction. The
proposed method was tested in the real dual-arm robot and successfully
accomplished the banana peeling task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoAdversary: A Pixel Pruning Method for Sparse Adversarial Attack. (arXiv:2203.09756v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09756">
<div class="article-summary-box-inner">
<span><p>Deep neural networks (DNNs) have been proven to be vulnerable to adversarial
examples. A special branch of adversarial examples, namely sparse adversarial
examples, can fool the target DNNs by perturbing only a few pixels. However,
many existing sparse adversarial attacks use heuristic methods to select the
pixels to be perturbed, and regard the pixel selection and the adversarial
attack as two separate steps. From the perspective of neural network pruning,
we propose a novel end-to-end sparse adversarial attack method, namely
AutoAdversary, which can find the most important pixels automatically by
integrating the pixel selection into the adversarial attack. Specifically, our
method utilizes a trainable neural network to generate a binary mask for the
pixel selection. After jointly optimizing the adversarial perturbation and the
neural network, only the pixels corresponding to the value 1 in the mask are
perturbed. Experiments demonstrate the superiority of our proposed method over
several state-of-the-art methods. Furthermore, since AutoAdversary does not
require a heuristic pixel selection process, it does not slow down excessively
as other methods when the image size increases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond a Video Frame Interpolator: A Space Decoupled Learning Approach to Continuous Image Transition. (arXiv:2203.09771v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09771">
<div class="article-summary-box-inner">
<span><p>Video frame interpolation (VFI) aims to improve the temporal resolution of a
video sequence. Most of the existing deep learning based VFI methods adopt
off-the-shelf optical flow algorithms to estimate the bidirectional flows and
interpolate the missing frames accordingly. Though having achieved a great
success, these methods require much human experience to tune the bidirectional
flows and often generate unpleasant results when the estimated flows are not
accurate. In this work, we rethink the VFI problem and formulate it as a
continuous image transition (CIT) task, whose key issue is to transition an
image from one space to another space continuously. More specifically, we learn
to implicitly decouple the images into a translatable flow space and a
non-translatable feature space. The former depicts the translatable states
between the given images, while the later aims to reconstruct the intermediate
features that cannot be directly translated. In this way, we can easily perform
image interpolation in the flow space and intermediate image synthesis in the
feature space, obtaining a CIT model. The proposed space decoupled learning
(SDL) approach is simple to implement, while it provides an effective framework
to a variety of CIT problems beyond VFI, such as style transfer and image
morphing. Our extensive experiments on a variety of CIT tasks demonstrate the
superiority of SDL to existing methods. The source code and models can be found
at \url{https://github.com/yangxy/SDL}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Completing Partial Point Clouds with Outliers by Collaborative Completion and Segmentation. (arXiv:2203.09772v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09772">
<div class="article-summary-box-inner">
<span><p>Most existing point cloud completion methods are only applicable to partial
point clouds without any noises and outliers, which does not always hold in
practice. We propose in this paper an end-to-end network, named CS-Net, to
complete the point clouds contaminated by noises or containing outliers. In our
CS-Net, the completion and segmentation modules work collaboratively to promote
each other, benefited from our specifically designed cascaded structure. With
the help of segmentation, more clean point cloud is fed into the completion
module. We design a novel completion decoder which harnesses the labels
obtained by segmentation together with FPS to purify the point cloud and
leverages KNN-grouping for better generation. The completion and segmentation
modules work alternately share the useful information from each other to
gradually improve the quality of prediction. To train our network, we build a
dataset to simulate the real case where incomplete point clouds contain
outliers. Our comprehensive experiments and comparisons against
state-of-the-art completion methods demonstrate our superiority. We also
compare with the scheme of segmentation followed by completion and their
end-to-end fusion, which also proves our efficacy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Local-Global Context Aware Transformer for Language-Guided Video Segmentation. (arXiv:2203.09773v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09773">
<div class="article-summary-box-inner">
<span><p>We explore the task of language-guided video segmentation (LVS). Previous
algorithms mostly adopt 3D CNNs to learn video representation, struggling to
capture long-term context and easily suffering from visual-linguistic
misalignment. In light of this, we present Locater (local-global context aware
Transformer), which augments the Transformer architecture with a finite memory
so as to query the entire video with the language expression in an efficient
manner. The memory is designed to involve two components -- one for
persistently preserving global video content, and one for dynamically gathering
local temporal context and segmentation history. Based on the memorized
local-global context and the particular content of each frame, Locater
holistically and flexibly comprehends the expression as an adaptive query
vector for each frame. The vector is used to query the corresponding frame for
mask generation. The memory also allows Locater to process videos with linear
time complexity and constant size memory, while Transformer-style
self-attention computation scales quadratically with sequence length. To
thoroughly examine the visual grounding capability of LVS models, we contribute
a new LVS dataset, A2D-S+, which is built upon A2D-S dataset but poses
increased challenges in disambiguating among similar objects. Experiments on
three LVS datasets and our A2D-S+ show that Locater outperforms previous
state-of-the-arts. Further, our Locater based solution achieved the 1st place
in the Referring Video Object Segmentation Track of the 3rd Large-scale Video
Object Segmentation Challenge. Our code and dataset are available at:
https://github.com/leonnnop/Locater
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ContrastMask: Contrastive Learning to Segment Every Thing. (arXiv:2203.09775v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09775">
<div class="article-summary-box-inner">
<span><p>Partially-supervised instance segmentation is a task which requests
segmenting objects from novel unseen categories via learning on limited seen
categories with annotated masks thus eliminating demands of heavy annotation
burden. The key to addressing this task is to build an effective class-agnostic
mask segmentation model. Unlike previous methods that learn such models only on
seen categories, in this paper, we propose a new method, named ContrastMask,
which learns a mask segmentation model on both seen and unseen categories under
a unified pixel-level contrastive learning framework. In this framework,
annotated masks of seen categories and pseudo masks of unseen categories serve
as a prior for contrastive learning, where features from the mask regions
(foreground) are pulled together, and are contrasted against those from the
background, and vice versa. Through this framework, feature discrimination
between foreground and background is largely improved, facilitating learning of
the class-agnostic mask segmentation model. Exhaustive experiments on the COCO
dataset demonstrate the superiority of our method, which outperforms previous
state-of-the-arts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transferable Class-Modelling for Decentralized Source Attribution of GAN-Generated Images. (arXiv:2203.09777v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09777">
<div class="article-summary-box-inner">
<span><p>GAN-generated deepfakes as a genre of digital images are gaining ground as
both catalysts of artistic expression and malicious forms of deception,
therefore demanding systems to enforce and accredit their ethical use. Existing
techniques for the source attribution of synthetic images identify subtle
intrinsic fingerprints using multiclass classification neural nets limited in
functionality and scalability. Hence, we redefine the deepfake detection and
source attribution problems as a series of related binary classification tasks.
We leverage transfer learning to rapidly adapt forgery detection networks for
multiple independent attribution problems, by proposing a semi-decentralized
modular design to solve them simultaneously and efficiently. Class activation
mapping is also demonstrated as an effective means of feature localization for
model interpretation. Our models are determined via experimentation to be
competitive with current benchmarks, and capable of decent performance on human
portraits in ideal conditions. Decentralized fingerprint-based attribution is
found to retain validity in the presence of novel sources, but is more
susceptible to type II errors that intensify with image perturbations and
attributive uncertainty. We describe both our conceptual framework and model
prototypes for further enhancement when investigating the technical limits of
reactive deepfake attribution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse Fuse Dense: Towards High Quality 3D Detection with Depth Completion. (arXiv:2203.09780v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09780">
<div class="article-summary-box-inner">
<span><p>Current LiDAR-only 3D detection methods inevitably suffer from the sparsity
of point clouds. Many multi-modal methods are proposed to alleviate this issue,
while different representations of images and point clouds make it difficult to
fuse them, resulting in suboptimal performance. In this paper, we present a
novel multi-modal framework SFD (Sparse Fuse Dense), which utilizes pseudo
point clouds generated from depth completion to tackle the issues mentioned
above. Different from prior works, we propose a new RoI fusion strategy 3D-GAF
(3D Grid-wise Attentive Fusion) to make fuller use of information from
different types of point clouds. Specifically, 3D-GAF fuses 3D RoI features
from the couple of point clouds in a grid-wise attentive way, which is more
fine-grained and more precise. In addition, we propose a SynAugment
(Synchronized Augmentation) to enable our multi-modal framework to utilize all
data augmentation approaches tailored to LiDAR-only methods. Lastly, we
customize an effective and efficient feature extractor CPConv (Color Point
Convolution) for pseudo point clouds. It can explore 2D image features and 3D
geometric features of pseudo point clouds simultaneously. Our method holds the
highest entry on the KITTI car 3D object detection leaderboard, demonstrating
the effectiveness of our SFD. Code will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Robust 2D Convolution for Reliable Visual Recognition. (arXiv:2203.09790v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09790">
<div class="article-summary-box-inner">
<span><p>2D convolution (Conv2d), which is responsible for extracting features from
the input image, is one of the key modules of a convolutional neural network
(CNN). However, Conv2d is vulnerable to image corruptions and adversarial
samples. It is an important yet rarely investigated problem that whether we can
design a more robust alternative of Conv2d for more reliable feature
extraction. In this paper, inspired by the recently developed learnable sparse
transform that learns to convert the CNN features into a compact and sparse
latent space, we design a novel building block, denoted by RConv-MK, to
strengthen the robustness of extracted convolutional features. Our method
leverages a set of learnable kernels of different sizes to extract features at
different frequencies and employs a normalized soft thresholding operator to
adaptively remove noises and trivial features at different corruption levels.
Extensive experiments on clean images, corrupted images as well as adversarial
samples validate the effectiveness of the proposed robust module for reliable
visual recognition. The source codes are enclosed in the submission.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Three things everyone should know about Vision Transformers. (arXiv:2203.09795v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09795">
<div class="article-summary-box-inner">
<span><p>After their initial success in natural language processing, transformer
architectures have rapidly gained traction in computer vision, providing
state-of-the-art results for tasks such as image classification, detection,
segmentation, and video analysis. We offer three insights based on simple and
easy to implement variants of vision transformers. (1) The residual layers of
vision transformers, which are usually processed sequentially, can to some
extent be processed efficiently in parallel without noticeably affecting the
accuracy. (2) Fine-tuning the weights of the attention layers is sufficient to
adapt vision transformers to a higher resolution and to other classification
tasks. This saves compute, reduces the peak memory consumption at fine-tuning
time, and allows sharing the majority of weights across tasks. (3) Adding
MLP-based patch pre-processing layers improves Bert-like self-supervised
training based on patch masking. We evaluate the impact of these design choices
using the ImageNet-1k dataset, and confirm our findings on the ImageNet-v2 test
set. Transfer performance is measured across six smaller datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Consistency from High-quality Pseudo-labels for Weakly Supervised Object Localization. (arXiv:2203.09803v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09803">
<div class="article-summary-box-inner">
<span><p>Pseudo-supervised learning methods have been shown to be effective for weakly
supervised object localization tasks. However, the effectiveness depends on the
powerful regularization ability of deep neural networks. Based on the
assumption that the localization network should have similar location
predictions on different versions of the same image, we propose a two-stage
approach to learn more consistent localization. In the first stage, we propose
a mask-based pseudo label generator algorithm, and use the pseudo-supervised
learning method to initialize an object localization network. In the second
stage, we propose a simple and effective method for evaluating the confidence
of pseudo-labels based on classification discrimination, and by learning
consistency from high-quality pseudo-labels, we further refine the localization
network to get better localization performance. Experimental results show that
our proposed approach achieves excellent performance in three benchmark
datasets including CUB-200-2011, ImageNet-1k and Tiny-ImageNet, which
demonstrates its effectiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stacked Hybrid-Attention and Group Collaborative Learning for Unbiased Scene Graph Generation. (arXiv:2203.09811v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09811">
<div class="article-summary-box-inner">
<span><p>Scene Graph Generation, which generally follows a regular encoder-decoder
pipeline, aims to first encode the visual contents within the given image and
then parse them into a compact summary graph. Existing SGG approaches generally
not only neglect the insufficient modality fusion between vision and language,
but also fail to provide informative predicates due to the biased relationship
predictions, leading SGG far from practical. Towards this end, in this paper,
we first present a novel Stacked Hybrid-Attention network, which facilitates
the intra-modal refinement as well as the inter-modal interaction, to serve as
the encoder. We then devise an innovative Group Collaborative Learning strategy
to optimize the decoder. Particularly, based upon the observation that the
recognition capability of one classifier is limited towards an extremely
unbalanced dataset, we first deploy a group of classifiers that are expert in
distinguishing different subsets of classes, and then cooperatively optimize
them from two aspects to promote the unbiased SGG. Experiments conducted on VG
and GQA datasets demonstrate that, we not only establish a new state-of-the-art
in the unbiased metric, but also nearly double the performance compared with
two baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grasp Pre-shape Selection by Synthetic Training: Eye-in-hand Shared Control on the Hannes Prosthesis. (arXiv:2203.09812v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09812">
<div class="article-summary-box-inner">
<span><p>We consider the task of object grasping with a prosthetic hand capable of
multiple grasp types. In this setting, communicating the intended grasp type
often requires a high user cognitive load which can be reduced adopting shared
autonomy frameworks. Among these, so-called eye-in-hand systems automatically
control the hand aperture and pre-shaping before the grasp, based on visual
input coming from a camera on the wrist. In this work, we present an
eye-in-hand learning-based approach for hand pre-shape classification from RGB
sequences. In order to reduce the need for tedious data collection sessions for
training the system, we devise a pipeline for rendering synthetic visual
sequences of hand trajectories for the purpose. We tackle the peculiarity of
the eye-in-hand setting by means of a model for the human arm trajectories,
with domain randomization over relevant visual elements. We develop a
sensorized setup to acquire real human grasping sequences for benchmarking and
show that, compared on practical use cases, models trained with our synthetic
dataset achieve better generalization performance than models trained on real
data. We finally integrate our model on the Hannes prosthetic hand and show its
practical effectiveness. Our code, real and synthetic datasets will be released
upon acceptance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Modal Perceptionist: Can Face Geometry be Gleaned from Voices?. (arXiv:2203.09824v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09824">
<div class="article-summary-box-inner">
<span><p>This work digs into a root question in human perception: can face geometry be
gleaned from one's voices? Previous works that study this question only adopt
developments in image synthesis and convert voices into face images to show
correlations, but working on the image domain unavoidably involves predicting
attributes that voices cannot hint, including facial textures, hairstyles, and
backgrounds. We instead investigate the ability to reconstruct 3D faces to
concentrate on only geometry, which is much more physiologically grounded. We
propose our analysis framework, Cross-Modal Perceptionist, under both
supervised and unsupervised learning. First, we construct a dataset,
Voxceleb-3D, which extends Voxceleb and includes paired voices and face meshes,
making supervised learning possible. Second, we use a knowledge distillation
mechanism to study whether face geometry can still be gleaned from voices
without paired voices and 3D face data under limited availability of 3D face
scans. We break down the core question into four parts and perform visual and
numerical analyses as responses to the core question. Our findings echo those
in physiology and neuroscience about the correlation between voices and facial
structures. The work provides future human-centric cross-modal learning with
explainable foundations. See our project page:
https://choyingw.github.io/works/Voice2Mesh/index.html
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Laneformer: Object-aware Row-Column Transformers for Lane Detection. (arXiv:2203.09830v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09830">
<div class="article-summary-box-inner">
<span><p>We present Laneformer, a conceptually simple yet powerful transformer-based
architecture tailored for lane detection that is a long-standing research topic
for visual perception in autonomous driving. The dominant paradigms rely on
purely CNN-based architectures which often fail in incorporating relations of
long-range lane points and global contexts induced by surrounding objects
(e.g., pedestrians, vehicles). Inspired by recent advances of the transformer
encoder-decoder architecture in various vision tasks, we move forwards to
design a new end-to-end Laneformer architecture that revolutionizes the
conventional transformers into better capturing the shape and semantic
characteristics of lanes, with minimal overhead in latency. First, coupling
with deformable pixel-wise self-attention in the encoder, Laneformer presents
two new row and column self-attention operations to efficiently mine point
context along with the lane shapes. Second, motivated by the appearing objects
would affect the decision of predicting lane segments, Laneformer further
includes the detected object instances as extra inputs of multi-head attention
blocks in the encoder and decoder to facilitate the lane point detection by
sensing semantic contexts. Specifically, the bounding box locations of objects
are added into Key module to provide interaction with each pixel and query
while the ROI-aligned features are inserted into Value module. Extensive
experiments demonstrate our Laneformer achieves state-of-the-art performances
on CULane benchmark, in terms of 77.1% F1 score. We hope our simple and
effective Laneformer will serve as a strong baseline for future research in
self-attention models for lane detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DTA: Physical Camouflage Attacks using Differentiable Transformation Network. (arXiv:2203.09831v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09831">
<div class="article-summary-box-inner">
<span><p>To perform adversarial attacks in the physical world, many studies have
proposed adversarial camouflage, a method to hide a target object by applying
camouflage patterns on 3D object surfaces. For obtaining optimal physical
adversarial camouflage, previous studies have utilized the so-called neural
renderer, as it supports differentiability. However, existing neural renderers
cannot fully represent various real-world transformations due to a lack of
control of scene parameters compared to the legacy photo-realistic renderers.
In this paper, we propose the Differentiable Transformation Attack (DTA), a
framework for generating a robust physical adversarial pattern on a target
object to camouflage it against object detection models with a wide range of
transformations. It utilizes our novel Differentiable Transformation Network
(DTN), which learns the expected transformation of a rendered object when the
texture is changed while preserving the original properties of the target
object. Using our attack framework, an adversary can gain both the advantages
of the legacy photo-realistic renderers including various physical-world
transformations and the benefit of white-box access by offering
differentiability. Our experiments show that our camouflaged 3D vehicles can
successfully evade state-of-the-art object detection models in the
photo-realistic environment (i.e., CARLA on Unreal Engine). Furthermore, our
demonstration on a scaled Tesla Model 3 proves the applicability and
transferability of our method to the real world.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perspective Flow Aggregation for Data-Limited 6D Object Pose Estimation. (arXiv:2203.09836v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09836">
<div class="article-summary-box-inner">
<span><p>Most recent 6D object pose estimation methods, including unsupervised ones,
require many real training images. Unfortunately, for some applications, such
as those in space or deep under water, acquiring real images, even unannotated,
is virtually impossible. In this paper, we propose a method that can be trained
solely on synthetic images, or optionally using a few additional real ones.
Given a rough pose estimate obtained from a first network, it uses a second
network to predict a dense 2D correspondence field between the image rendered
using the rough pose and the real image and infers the required pose
correction. This approach is much less sensitive to the domain shift between
synthetic and real images than state-of-the-art methods. It performs on par
with methods that require annotated real images for training when not using
any, and outperforms them considerably when using as few as twenty real images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Location-Free Camouflage Generation Network. (arXiv:2203.09845v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09845">
<div class="article-summary-box-inner">
<span><p>Camouflage is a common visual phenomenon, which refers to hiding the
foreground objects into the background images, making them briefly invisible to
the human eye. Previous work has typically been implemented by an iterative
optimization process. However, these methods struggle in 1) efficiently
generating camouflage images using foreground and background with arbitrary
structure; 2) camouflaging foreground objects to regions with multiple
appearances (e.g. the junction of the vegetation and the mountains), which
limit their practical application. To address these problems, this paper
proposes a novel Location-free Camouflage Generation Network (LCG-Net) that
fuse high-level features of foreground and background image, and generate
result by one inference. Specifically, a Position-aligned Structure Fusion
(PSF) module is devised to guide structure feature fusion based on the
point-to-point structure similarity of foreground and background, and introduce
local appearance features point-by-point. To retain the necessary identifiable
features, a new immerse loss is adopted under our pipeline, while a background
patch appearance loss is utilized to ensure that the hidden objects look
continuous and natural at regions with multiple appearances. Experiments show
that our method has results as satisfactory as state-of-the-art in the
single-appearance regions and are less likely to be completely invisible, but
far exceed the quality of the state-of-the-art in the multi-appearance regions.
Moreover, our method is hundreds of times faster than previous methods.
Benefitting from the unique advantages of our method, we provide some
downstream applications for camouflage generation, which show its potential.
The related code and dataset will be released at
https://github.com/Tale17/LCG-Net.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Modal Masked Pre-Training for Monocular Panoramic Depth Completion. (arXiv:2203.09855v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09855">
<div class="article-summary-box-inner">
<span><p>In this paper, we formulate a potentially valuable panoramic depth completion
(PDC) task as panoramic 3D cameras often produce 360{\deg} depth with missing
data in complex scenes. Its goal is to recover dense panoramic depths from raw
sparse ones and panoramic RGB images. To deal with the PDC task, we train a
deep network that takes both depth and image as inputs for the dense panoramic
depth recovery. However, it needs to face a challenging optimization problem of
the network parameters due to its non-convex objective function. To address
this problem, we propose a simple yet effective approach termed M{^3}PT:
multi-modal masked pre-training. Specifically, during pre-training, we
simultaneously cover up patches of the panoramic RGB image and sparse depth by
shared random mask, then reconstruct the sparse depth in the masked regions. To
our best knowledge, it is the first time that we show the effectiveness of
masked pre-training in a multi-modal vision task, instead of the single-modal
task resolved by masked autoencoders (MAE). Different from MAE where
fine-tuning completely discards the decoder part of pre-training, there is no
architectural difference between the pre-training and fine-tuning stages in our
M$^{3}$PT as they only differ in the prediction density, which potentially
makes the transfer learning more convenient and effective. Extensive
experiments verify the effectiveness of M{^3}PT on three panoramic datasets.
Notably, we improve the state-of-the-art baselines by averagely 26.2% in RMSE,
51.7% in MRE, 49.7% in MAE, and 37.5% in RMSElog on three benchmark datasets.
Codes and pre-trained models are available at
https://github.com/anonymoustbd/MMMPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pseudo Bias-Balanced Learning for Debiased Chest X-ray Classification. (arXiv:2203.09860v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09860">
<div class="article-summary-box-inner">
<span><p>Deep learning models were frequently reported to learn from shortcuts like
dataset biases. As deep learning is playing an increasingly important role in
the modern healthcare system, it is of great need to combat shortcut learning
in medical data as well as develop unbiased and trustworthy models. In this
paper, we study the problem of developing debiased chest X-ray diagnosis models
from the biased training data without knowing exactly the bias labels. We start
with the observations that the imbalance of bias distribution is one of the key
reasons causing shortcut learning, and the dataset biases are preferred by the
model if they were easier to be learned than the intended features. Based on
these observations, we propose a novel algorithm, pseudo bias-balanced
learning, which first captures and predicts per-sample bias labels via
generalized cross entropy loss and then trains a debiased model using pseudo
bias labels and bias-balanced softmax function. To our best knowledge, we are
pioneered in tackling dataset biases in medical images without explicit
labeling on the bias attributes. We constructed several chest X-ray datasets
with various dataset bias situations and demonstrated with extensive
experiments that our proposed method achieved consistent improvements over
other state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CodedVTR: Codebook-based Sparse Voxel Transformer with Geometric Guidance. (arXiv:2203.09887v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09887">
<div class="article-summary-box-inner">
<span><p>Transformers have gained much attention by outperforming convolutional neural
networks in many 2D vision tasks. However, they are known to have
generalization problems and rely on massive-scale pre-training and
sophisticated training techniques. When applying to 3D tasks, the irregular
data structure and limited data scale add to the difficulty of transformer's
application. We propose CodedVTR (Codebook-based Voxel TRansformer), which
improves data efficiency and generalization ability for 3D sparse voxel
transformers. On the one hand, we propose the codebook-based attention that
projects an attention space into its subspace represented by the combination of
"prototypes" in a learnable codebook. It regularizes attention learning and
improves generalization. On the other hand, we propose geometry-aware
self-attention that utilizes geometric information (geometric pattern, density)
to guide attention learning. CodedVTR could be embedded into existing sparse
convolution-based methods, and bring consistent performance improvements for
indoor and outdoor 3D semantic segmentation tasks
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Affordance Grounding from Exocentric Images. (arXiv:2203.09905v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09905">
<div class="article-summary-box-inner">
<span><p>Affordance grounding, a task to ground (i.e., localize) action possibility
region in objects, which faces the challenge of establishing an explicit link
with object parts due to the diversity of interactive affordance. Human has the
ability that transform the various exocentric interactions to invariant
egocentric affordance so as to counter the impact of interactive diversity. To
empower an agent with such ability, this paper proposes a task of affordance
grounding from exocentric view, i.e., given exocentric human-object interaction
and egocentric object images, learning the affordance knowledge of the object
and transferring it to the egocentric image using only the affordance label as
supervision. To this end, we devise a cross-view knowledge transfer framework
that extracts affordance-specific features from exocentric interactions and
enhances the perception of affordance regions by preserving affordance
correlation. Specifically, an Affordance Invariance Mining module is devised to
extract specific clues by minimizing the intra-class differences originated
from interaction habits in exocentric images. Besides, an Affordance
Co-relation Preserving strategy is presented to perceive and localize
affordance by aligning the co-relation matrix of predicted results between the
two views. Particularly, an affordance grounding dataset named AGD20K is
constructed by collecting and labeling over 20K images from 36 affordance
categories. Experimental results demonstrate that our method outperforms the
representative models in terms of objective metrics and visual quality. Code:
github.com/lhc1224/Cross-View-AG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fourier Document Restoration for Robust Document Dewarping and Recognition. (arXiv:2203.09910v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09910">
<div class="article-summary-box-inner">
<span><p>State-of-the-art document dewarping techniques learn to predict 3-dimensional
information of documents which are prone to errors while dealing with documents
with irregular distortions or large variations in depth. This paper presents
FDRNet, a Fourier Document Restoration Network that can restore documents with
different distortions and improve document recognition in a reliable and
simpler manner. FDRNet focuses on high-frequency components in the Fourier
space that capture most structural information but are largely free of
degradation in appearance. It dewarps documents by a flexible Thin-Plate Spline
transformation which can handle various deformations effectively without
requiring deformation annotations in training. These features allow FDRNet to
learn from a small amount of simply labeled training images, and the learned
model can dewarp documents with complex geometric distortion and recognize the
restored texts accurately. To facilitate document restoration research, we
create a benchmark dataset consisting of over one thousand camera documents
with different types of geometric and photometric distortion. Extensive
experiments show that FDRNet outperforms the state-of-the-art by large margins
on both dewarping and text recognition tasks. In addition, FDRNet requires a
small amount of simply labeled training data and is easy to deploy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Convolutional Simultaneous Sparse Approximation with Applications to RGB-NIR Image Fusion. (arXiv:2203.09913v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09913">
<div class="article-summary-box-inner">
<span><p>Simultaneous sparse approximation (SSA) seeks to represent a set of dependent
signals using sparse vectors with identical supports. The SSA model has been
used in various signal and image processing applications involving multiple
correlated input signals. In this paper, we propose algorithms for
convolutional SSA (CSSA) based on the alternating direction method of
multipliers. Specifically, we address the CSSA problem with different sparsity
structures and the convolutional feature learning problem in multimodal
data/signals based on the SSA model. We evaluate the proposed algorithms by
applying them to multimodal and multifocus image fusion problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revealing Reliable Signatures by Learning Top-Rank Pairs. (arXiv:2203.09927v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09927">
<div class="article-summary-box-inner">
<span><p>Signature verification, as a crucial practical documentation analysis task,
has been continuously studied by researchers in machine learning and pattern
recognition fields. In specific scenarios like confirming financial documents
and legal instruments, ensuring the absolute reliability of signatures is of
top priority. In this work, we proposed a new method to learn "top-rank pairs"
for writer-independent offline signature verification tasks. By this scheme, it
is possible to maximize the number of absolutely reliable signatures. More
precisely, our method to learn top-rank pairs aims at pushing positive samples
beyond negative samples, after pairing each of them with a genuine reference
signature. In the experiment, BHSig-B and BHSig-H datasets are used for
evaluation, on which the proposed model achieves overwhelming better pos@top
(the ratio of absolute top positive samples to all of the positive samples)
while showing encouraging performance on both Area Under the Curve (AUC) and
accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deepfake Style Transfer Mixture: a First Forensic Ballistics Study on Synthetic Images. (arXiv:2203.09928v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09928">
<div class="article-summary-box-inner">
<span><p>Most recent style-transfer techniques based on generative architectures are
able to obtain synthetic multimedia contents, or commonly called deepfakes,
with almost no artifacts. Researchers already demonstrated that synthetic
images contain patterns that can determine not only if it is a deepfake but
also the generative architecture employed to create the image data itself.
These traces can be exploited to study problems that have never been addressed
in the context of deepfakes. To this aim, in this paper a first approach to
investigate the image ballistics on deepfake images subject to style-transfer
manipulations is proposed. Specifically, this paper describes a study on
detecting how many times a digital image has been processed by a generative
architecture for style transfer. Moreover, in order to address and study
accurately forensic ballistics on deepfake images, some mathematical properties
of style-transfer operations were investigated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3DAC: Learning Attribute Compression for Point Clouds. (arXiv:2203.09931v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09931">
<div class="article-summary-box-inner">
<span><p>We study the problem of attribute compression for large-scale unstructured 3D
point clouds. Through an in-depth exploration of the relationships between
different encoding steps and different attribute channels, we introduce a deep
compression network, termed 3DAC, to explicitly compress the attributes of 3D
point clouds and reduce storage usage in this paper. Specifically, the point
cloud attributes such as color and reflectance are firstly converted to
transform coefficients. We then propose a deep entropy model to model the
probabilities of these coefficients by considering information hidden in
attribute transforms and previous encoded attributes. Finally, the estimated
probabilities are used to further compress these transform coefficients to a
final attributes bitstream. Extensive experiments conducted on both indoor and
outdoor large-scale open point cloud datasets, including ScanNet and
SemanticKITTI, demonstrated the superior compression rates and reconstruction
quality of the proposed 3DAC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the sensitivity of pose estimation neural networks: rotation parameterizations, Lipschitz constants, and provable bounds. (arXiv:2203.09937v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09937">
<div class="article-summary-box-inner">
<span><p>In this paper, we approach the task of determining sensitivity bounds for
pose estimation neural networks. This task is particularly challenging as it
requires characterizing the sensitivity of 3D rotations. We develop a
sensitivity measure that describes the maximum rotational change in a network's
output with respect to a Euclidean change in its input. We show that this
measure is a type of Lipschitz constant, and that it is bounded by the product
of a network's Euclidean Lipschitz constant and an intrinsic property of a
rotation parameterization which we call the "distance ratio constant". We
derive the distance ratio constant for several rotation parameterizations, and
then discuss why the structure of most of these parameterizations makes it
difficult to construct a pose estimation network with provable sensitivity
bounds. However, we show that sensitivity bounds can be computed for networks
which parameterize rotation using unconstrained exponential coordinates. We
then construct and train such a network and compute sensitivity bounds for it.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Enhanced Belief Propagation for Data Assocation in Multiobject Tracking. (arXiv:2203.09948v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09948">
<div class="article-summary-box-inner">
<span><p>Situation-aware technologies enabled by multiobject tracking (MOT) methods
will create new services and applications in fields such as autonomous
navigation and applied ocean sciences. Belief propagation (BP) is a
state-of-the-art method for Bayesian MOT but fully relies on a statistical
model and preprocessed sensor measurements. In this paper, we establish a
hybrid method for model-based and data-driven MOT. The proposed neural enhanced
belief propagation (NEBP) approach complements BP by information learned from
raw sensor data with the goal to improve data association and to reject false
alarm measurements. We evaluate the performance of our NEBP approach for MOT on
the nuScenes autonomous driving dataset and demonstrate that it can outperform
state-of-the-art reference methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancement of Novel View Synthesis Using Omnidirectional Image Completion. (arXiv:2203.09957v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09957">
<div class="article-summary-box-inner">
<span><p>We present a method for synthesizing novel views from a single 360-degree
image based on the neural radiance field (NeRF) . Prior studies rely on the
neighborhood interpolation capability of multi-layer perceptrons to complete
missing regions caused by occlusion and zooming, and this leads to artifacts.
In the proposed method, the input image is reprojected to 360-degree images at
other camera positions, the missing regions of the reprojected images are
completed by a self-supervised trained generative model, and the completed
images are utilized to train the NeRF. Because multiple completed images
contain inconsistencies in 3D, we introduce a method to train NeRF while
dynamically selecting a sparse set of completed images, to reduce the
discrimination error of the synthesized views with real images. Experiments
indicate that the proposed method can synthesize plausible novel views while
preserving the features of the scene for both artificial and real-world data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SynthStrip: Skull-Stripping for Any Brain Image. (arXiv:2203.09974v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09974">
<div class="article-summary-box-inner">
<span><p>The removal of non-brain signal from magnetic resonance imaging (MRI) data,
known as skull-stripping, is an integral component of many neuroimage analysis
streams. Despite their abundance, popular classical skull-stripping methods are
usually tailored to images with specific acquisition properties, namely
near-isotropic resolution and T1-weighted (T1w) MRI contrast, which are
prevalent in research settings. As a result, existing tools tend to adapt
poorly to other image types, such as stacks of thick slices acquired with fast
spin-echo (FSE) MRI that are common in the clinic. While learning-based
approaches for brain extraction have gained traction in recent years, these
methods face a similar burden, as they are only effective for image types seen
during the training procedure. To achieve robust skull-stripping across a
landscape of protocols, we introduce SynthStrip, a rapid, learning-based
brain-extraction tool. By leveraging anatomical segmentations to generate an
entirely synthetic training dataset with anatomies, intensity distributions,
and artifacts that far exceed the realistic range of medical images, SynthStrip
learns to successfully generalize to a variety of real acquired brain images,
removing the need for training data with target contrasts. We demonstrate the
efficacy of SynthStrip for a diverse set of image acquisitions and resolutions
across subject populations, ranging from newborn to adult. We show substantial
improvements in accuracy over popular skull-stripping baselines - all with a
single trained model. Our method and labeled evaluation data are available at
https://w3id.org/synthstrip.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GiNGR: Generalized Iterative Non-Rigid Point Cloud and Surface Registration Using Gaussian Process Regression. (arXiv:2203.09986v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09986">
<div class="article-summary-box-inner">
<span><p>In this paper, we unify popular non-rigid registration methods for point sets
and surfaces under our general framework, GiNGR. GiNGR builds upon Gaussian
Process Morphable Models (GPMM) and hence separates modeling the deformation
prior from model adaptation for registration. In addition, it provides
explainable hyperparameters, multi-resolution registration, trivial inclusion
of expert annotation, and the ability to use and combine analytical and
statistical deformation priors. But more importantly, the reformulation allows
for a direct comparison of registration methods. Instead of using a general
solver in the optimization step, we show how Gaussian process regression (GPR)
iteratively can warp a reference onto a target, leading to smooth deformations
following the prior for any dense, sparse, or partial estimated correspondences
in a principled way. We show how the popular CPD and ICP algorithms can be
directly explained with GiNGR. Furthermore, we show how existing algorithms in
the GiNGR framework can perform probabilistic registration to obtain a
distribution of different registrations instead of a single best registration.
This can be used to analyze the uncertainty e.g. when registering partial
observations. GiNGR is publicly available and fully modular to allow for
domain-specific prior construction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diffusion and Volume Maximization-Based Clustering of Highly Mixed Hyperspectral Images. (arXiv:2203.09992v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09992">
<div class="article-summary-box-inner">
<span><p>Hyperspectral images of a scene or object are a rich data source, often
encoding a hundred or more spectral bands of reflectance at each pixel. Despite
being very high-dimensional, these images typically encode latent
low-dimensional structure that can be exploited for material discrimination.
However, due to an inherent trade-off between spectral and spatial resolution,
many hyperspectral images are generated at a coarse spatial scale, and single
pixels may correspond to spatial regions containing multiple materials. This
article introduces the \emph{Diffusion and Volume maximization-based Image
Clustering} (\emph{D-VIC}) algorithm for unsupervised material discrimination.
D-VIC locates cluster modes -- high-density, high-purity pixels in the
hyperspectral image that are far in diffusion distance (a data-dependent
distance metric) from other high-density, high-purity pixels -- and assigns
these pixels unique labels, as these points are meant to exemplify underlying
material structure. Non-modal pixels are labeled according to their diffusion
distance nearest neighbor of higher density and purity that is already labeled.
By directly incorporating pixel purity into its modal and non-modal labeling,
D-VIC upweights pixels that correspond to a spatial region containing just a
single material, yielding more interpretable clusterings. D-VIC is shown to
outperform baseline and comparable state-of-the-art methods in extensive
numerical experiments on a range of hyperspectral images, implying that it is
well-equipped for material discrimination and clustering of these data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Elastica Models for Color Image Regularization. (arXiv:2203.09995v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09995">
<div class="article-summary-box-inner">
<span><p>One classical approach to regularize color is to tream them as two
dimensional surfaces embedded in a five dimensional spatial-chromatic space. In
this case, a natural regularization term arises as the image surface area.
Choosing the chromatic coordinates as dominating over the spatial ones, the
image spatial coordinates could be thought of as a paramterization of the image
surface manifold in a three dimensional color space. Minimizing the area of the
image manifold leads to the Beltrami flow or mean curvature flow of the image
surface in the 3D color space, while minimizing the elastica of the image
surface yields an additional interesting regularization. Recently, the authors
proposed a color elastica model, which minimizes both the surface area and
elastica of the image manifold. In this paper, we propose to modify the color
elastica and introduce two new models for color image regularization. The
revised measures are motivated by the relations between the color elastica
model, Euler's elastica model and the total variation model for gray level
images. Compared to our previous color elastica model, the new models are
direct extensions of Euler's elastica model to color images. The proposed
models are nonlinear and challenging to minimize. To overcome this difficulty,
two operator-splitting methods are suggested. Specifically, nonlinearities are
decoupled by introducing new vector- and matrix-valued variables. Then, the
minimization problems are converted to solving initial value problems which are
time-discretized by operator splitting. Each subproblem, after splitting
either, has a closed-form solution or can be solved efficiently. The
effectiveness and advantages of the proposed models are demonstrated by
comprehensive experiments. The benefits of incorporating the elastica of the
image surface as regularization terms compared to common alternatives are
empirically validated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Application of Top-hat Transformation for Enhanced Blood Vessel Extraction. (arXiv:2203.10005v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10005">
<div class="article-summary-box-inner">
<span><p>In the medical domain, different computer-aided diagnosis systems have been
proposed to extract blood vessels from retinal fundus images for the clinical
treatment of vascular diseases. Accurate extraction of blood vessels from the
fundus images using a computer-generated method can help the clinician to
produce timely and accurate reports for the patient suffering from these
diseases. In this article, we integrate top-hat based preprocessing approach
with fine-tuned B-COSFIRE filter to achieve more accurate segregation of blood
vessel pixels from the background. The use of top-hat transformation in the
preprocessing stage enhances the efficacy of the algorithm to extract blood
vessels in presence of structures like fovea, exudates, haemorrhages, etc.
Furthermore, to reduce the false positives, small clusters of blood vessel
pixels are removed in the postprocessing stage. Further, we find that the
proposed algorithm is more efficient as compared to various modern algorithms
reported in the literature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ultra-low Latency Spiking Neural Networks with Spatio-Temporal Compression and Synaptic Convolutional Block. (arXiv:2203.10006v1 [cs.NE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10006">
<div class="article-summary-box-inner">
<span><p>Spiking neural networks (SNNs), as one of the brain-inspired models, has
spatio-temporal information processing capability, low power feature, and high
biological plausibility. The effective spatio-temporal feature makes it
suitable for event streams classification. However, neuromorphic datasets, such
as N-MNIST, CIFAR10-DVS, DVS128-gesture, need to aggregate individual events
into frames with a new higher temporal resolution for event stream
classification, which causes high training and inference latency. In this work,
we proposed a spatio-temporal compression method to aggregate individual events
into a few time steps of synaptic current to reduce the training and inference
latency. To keep the accuracy of SNNs under high compression ratios, we also
proposed a synaptic convolutional block to balance the dramatic change between
adjacent time steps. And multi-threshold Leaky Integrate-and-Fire (LIF) with
learnable membrane time constant is introduced to increase its information
processing capability. We evaluate the proposed method for event streams
classification tasks on neuromorphic N-MNIST, CIFAR10-DVS, DVS128 gesture
datasets. The experiment results show that our proposed method outperforms the
state-of-the-art accuracy on nearly all datasets, using fewer time steps.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing EEG Data with Machine and Deep Learning: A Benchmark. (arXiv:2203.10009v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10009">
<div class="article-summary-box-inner">
<span><p>Nowadays, machine and deep learning techniques are widely used in different
areas, ranging from economics to biology. In general, these techniques can be
used in two ways: trying to adapt well-known models and architectures to the
available data, or designing custom architectures. In both cases, to speed up
the research process, it is useful to know which type of models work best for a
specific problem and/or data type. By focusing on EEG signal analysis, and for
the first time in literature, in this paper a benchmark of machine and deep
learning for EEG signal classification is proposed. For our experiments we used
the four most widespread models, i.e., multilayer perceptron, convolutional
neural network, long short-term memory, and gated recurrent unit, highlighting
which one can be a good starting point for developing EEG classification
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parametric Scaling of Preprocessing assisted U-net Architecture for Improvised Retinal Vessel Segmentation. (arXiv:2203.10014v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10014">
<div class="article-summary-box-inner">
<span><p>Extracting blood vessels from retinal fundus images plays a decisive role in
diagnosing the progression in pertinent diseases. In medical image analysis,
vessel extraction is a semantic binary segmentation problem, where blood
vasculature needs to be extracted from the background. Here, we present an
image enhancement technique based on the morphological preprocessing coupled
with a scaled U-net architecture. Despite a relatively less number of trainable
network parameters, the scaled version of U-net architecture provides better
performance compare to other methods in the domain. We validated the proposed
method on retinal fundus images from the DRIVE database. A significant
improvement as compared to the other algorithms in the domain, in terms of the
area under ROC curve (&gt;0.9762) and classification accuracy (&gt;95.47%) are
evident from the results. Furthermore, the proposed method is resistant to the
central vessel reflex while sensitive to detect blood vessels in the presence
of background items viz. exudates, optic disc, and fovea.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ESS: Learning Event-based Semantic Segmentation from Still Images. (arXiv:2203.10016v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10016">
<div class="article-summary-box-inner">
<span><p>Retrieving accurate semantic information in challenging high dynamic range
(HDR) and high-speed conditions remains an open challenge for image-based
algorithms due to severe image degradations. Event cameras promise to address
these challenges since they feature a much higher dynamic range and are
resilient to motion blur. Nonetheless, semantic segmentation with event cameras
is still in its infancy which is chiefly due to the novelty of the sensor, and
the lack of high-quality, labeled datasets. In this work, we introduce ESS,
which tackles this problem by directly transferring the semantic segmentation
task from existing labeled image datasets to unlabeled events via unsupervised
domain adaptation (UDA). Compared to existing UDA methods, our approach aligns
recurrent, motion-invariant event embeddings with image embeddings. For this
reason, our method neither requires video data nor per-pixel alignment between
images and events and, crucially, does not need to hallucinate motion from
still images. Additionally, to spur further research in event-based semantic
segmentation, we introduce DSEC-Semantic, the first large-scale event-based
dataset with fine-grained labels. We show that using image labels alone, ESS
outperforms existing UDA approaches, and when combined with event labels, it
even outperforms state-of-the-art supervised approaches on both DDD17 and
DSEC-Semantic. Finally, ESS is general-purpose, which unlocks the vast amount
of existing labeled image datasets and paves the way for new and exciting
research directions in new fields previously inaccessible for event cameras.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unbiased Subclass Regularization for Semi-Supervised Semantic Segmentation. (arXiv:2203.10026v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10026">
<div class="article-summary-box-inner">
<span><p>Semi-supervised semantic segmentation learns from small amounts of labelled
images and large amounts of unlabelled images, which has witnessed impressive
progress with the recent advance of deep neural networks. However, it often
suffers from severe class-bias problem while exploring the unlabelled images,
largely due to the clear pixel-wise class imbalance in the labelled images.
This paper presents an unbiased subclass regularization network (USRN) that
alleviates the class imbalance issue by learning class-unbiased segmentation
from balanced subclass distributions. We build the balanced subclass
distributions by clustering pixels of each original class into multiple
subclasses of similar sizes, which provide class-balanced pseudo supervision to
regularize the class-biased segmentation. In addition, we design an
entropy-based gate mechanism to coordinate learning between the original
classes and the clustered subclasses which facilitates subclass regularization
effectively by suppressing unconfident subclass predictions. Extensive
experiments over multiple public benchmarks show that USRN achieves superior
performance as compared with the state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nonnegative-Constrained Joint Collaborative Representation with Union Dictionary for Hyperspectral Anomaly Detection. (arXiv:2203.10030v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10030">
<div class="article-summary-box-inner">
<span><p>Recently, many collaborative representation-based (CR) algorithms have been
proposed for hyperspectral anomaly detection. CR-based detectors approximate
the image by a linear combination of background dictionaries and the
coefficient matrix, and derive the detection map by utilizing recovery
residuals. However, these CR-based detectors are often established on the
premise of precise background features and strong image representation, which
are very difficult to obtain. In addition, pursuing the coefficient matrix
reinforced by the general $l_2$-min is very time consuming. To address these
issues, a nonnegative-constrained joint collaborative representation model is
proposed in this paper for the hyperspectral anomaly detection task. To extract
reliable samples, a union dictionary consisting of background and anomaly
sub-dictionaries is designed, where the background sub-dictionary is obtained
at the superpixel level and the anomaly sub-dictionary is extracted by the
pre-detection process. And the coefficient matrix is jointly optimized by the
Frobenius norm regularization with a nonnegative constraint and a sum-to-one
constraint. After the optimization process, the abnormal information is finally
derived by calculating the residuals that exclude the assumed background
information. To conduct comparable experiments, the proposed
nonnegative-constrained joint collaborative representation (NJCR) model and its
kernel version (KNJCR) are tested in four HSI data sets and achieve superior
results compared with other state-of-the-art detectors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SHREC 2021: Classification in cryo-electron tomograms. (arXiv:2203.10035v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10035">
<div class="article-summary-box-inner">
<span><p>Cryo-electron tomography (cryo-ET) is an imaging technique that allows
three-dimensional visualization of macro-molecular assemblies under near-native
conditions. Cryo-ET comes with a number of challenges, mainly low
signal-to-noise and inability to obtain images from all angles. Computational
methods are key to analyze cryo-electron tomograms.
</p>
<p>To promote innovation in computational methods, we generate a novel simulated
dataset to benchmark different methods of localization and classification of
biological macromolecules in tomograms. Our publicly available dataset contains
ten tomographic reconstructions of simulated cell-like volumes. Each volume
contains twelve different types of complexes, varying in size, function and
structure.
</p>
<p>In this paper, we have evaluated seven different methods of finding and
classifying proteins. Seven research groups present results obtained with
learning-based methods and trained on the simulated dataset, as well as a
baseline template matching (TM), a traditional method widely used in cryo-ET
research. We show that learning-based approaches can achieve notably better
localization and classification performance than TM. We also experimentally
confirm that there is a negative relationship between particle size and
performance for all methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-input segmentation of damaged brain in acute ischemic stroke patients using slow fusion with skip connection. (arXiv:2203.10039v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10039">
<div class="article-summary-box-inner">
<span><p>Time is a fundamental factor during stroke treatments. A fast, automatic
approach that segments the ischemic regions helps treatment decisions. In
clinical use today, a set of color-coded parametric maps generated from
computed tomography perfusion (CTP) images are investigated manually to decide
a treatment plan. We propose an automatic method based on a neural network
using a set of parametric maps to segment the two ischemic regions (core and
penumbra) in patients affected by acute ischemic stroke. Our model is based on
a convolution-deconvolution bottleneck structure with multi-input and slow
fusion. A loss function based on the focal Tversky index addresses the data
imbalance issue. The proposed architecture demonstrates effective performance
and results comparable to the ground truth annotated by neuroradiologists. A
Dice coefficient of 0.81 for penumbra and 0.52 for core over the large vessel
occlusion test set is achieved. The full implementation is available at:
https://git.io/JtFGb.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Imaging-based histological features are predictive of MET alterations in Non-Small Cell Lung Cancer. (arXiv:2203.10062v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10062">
<div class="article-summary-box-inner">
<span><p>MET is a proto-oncogene whose somatic activation in non-small cell lung
cancer leads to increased cell growth and tumor progression. The two major
classes of MET alterations are gene amplification and exon 14 deletion, both of
which are therapeutic targets and detectable using existing molecular assays.
However, existing tests are limited by their consumption of valuable tissue,
cost and complexity that prevent widespread use. MET alterations could have an
effect on cell morphology, and quantifying these associations could open new
avenues for research and development of morphology-based screening tools. Using
H&amp;E-stained whole slide images (WSIs), we investigated the association of
distinct cell-morphological features with MET amplifications and MET exon 14
deletions. We found that cell shape, color, grayscale intensity and
texture-based features from both tumor infiltrating lymphocytes and tumor cells
distinguished MET wild-type from MET amplified or MET exon 14 deletion cases.
The association of individual cell features with MET alterations suggested a
predictive model could distinguish MET wild-type from MET amplification or MET
exon 14 deletion. We therefore developed an L1-penalized logistic regression
model, achieving a mean Area Under the Receiver Operating Characteristic Curve
(ROC-AUC) of 0.77 +/- 0.05sd in cross-validation and 0.77 on an independent
holdout test set. A sparse set of 43 features differentiated these classes,
which included features similar to what was found in the univariate analysis as
well as the percent of tumor cells in the tissue. Our study demonstrates that
MET alterations result in a detectable morphological signal in tumor cells and
lymphocytes. These results suggest that development of low-cost predictive
models based on H&amp;E-stained WSIs may improve screening for MET altered tumors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lunar Rover Localization Using Craters as Landmarks. (arXiv:2203.10073v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10073">
<div class="article-summary-box-inner">
<span><p>Onboard localization capabilities for planetary rovers to date have used
relative navigation, by integrating combinations of wheel odometry, visual
odometry, and inertial measurements during each drive to track position
relative to the start of each drive. At the end of each drive, a
ground-in-the-loop (GITL) interaction is used to get a position update from
human operators in a more global reference frame, by matching images or local
maps from onboard the rover to orbital reconnaissance images or maps of a large
region around the rover's current position. Autonomous rover drives are limited
in distance so that accumulated relative navigation error does not risk the
possibility of the rover driving into hazards known from orbital images.
However, several rover mission concepts have recently been studied that require
much longer drives between GITL cycles, particularly for the Moon. These
concepts require greater autonomy to minimize GITL cycles to enable such large
range; onboard global localization is a key element of such autonomy. Multiple
techniques have been studied in the past for onboard rover global localization,
but a satisfactory solution has not yet emerged. For the Moon, the ubiquitous
craters offer a new possibility, which involves mapping craters from orbit,
then recognizing crater landmarks with cameras and-or a lidar onboard the
rover. This approach is applicable everywhere on the Moon, does not require
high resolution stereo imaging from orbit as some other approaches do, and has
potential to enable position knowledge with order of 5 to 10 m accuracy at all
times. This paper describes our technical approach to crater-based lunar rover
localization and presents initial results on crater detection using 3D point
cloud data from onboard lidar or stereo cameras, as well as using shading cues
in monocular onboard imagery.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bayesian Inversion for Nonlinear Imaging Models using Deep Generative Priors. (arXiv:2203.10078v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10078">
<div class="article-summary-box-inner">
<span><p>Most modern imaging systems involve a computational reconstruction pipeline
to infer the image of interest from acquired measurements. The Bayesian
reconstruction framework relies on the characterization of the posterior
distribution, which depends on a model of the imaging system and prior
knowledge on the image, for solving such inverse problems. Here, the choice of
the prior distribution is critical for obtaining high-quality estimates. In
this work, we use deep generative models to represent the prior distribution.
We develop a posterior sampling scheme for the class of nonlinear inverse
problems where the forward model has a neural-network-like structure. This
class includes most existing imaging modalities. We introduce the notion of
augmented generative models in order to suitably handle quantitative image
recovery. We illustrate the advantages of our framework by applying it to two
nonlinear imaging modalities-phase retrieval and optical diffraction
tomography.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Artificial Fingerprinting for Generative Models: Rooting Deepfake Attribution in Training Data. (arXiv:2007.08457v7 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.08457">
<div class="article-summary-box-inner">
<span><p>Photorealistic image generation has reached a new level of quality due to the
breakthroughs of generative adversarial networks (GANs). Yet, the dark side of
such deepfakes, the malicious use of generated media, raises concerns about
visual misinformation. While existing research work on deepfake detection
demonstrates high accuracy, it is subject to advances in generation techniques
and adversarial iterations on detection countermeasure techniques. Thus, we
seek a proactive and sustainable solution on deepfake detection, that is
agnostic to the evolution of generative models, by introducing artificial
fingerprints into the models.
</p>
<p>Our approach is simple and effective. We first embed artificial fingerprints
into training data, then validate a surprising discovery on the transferability
of such fingerprints from training data to generative models, which in turn
appears in the generated deepfakes. Experiments show that our fingerprinting
solution (1) holds for a variety of cutting-edge generative models, (2) leads
to a negligible side effect on generation quality, (3) stays robust against
image-level and model-level perturbations, (4) stays hard to be detected by
adversaries, and (5) converts deepfake detection and attribution into trivial
tasks and outperforms the recent state-of-the-art baselines. Our solution
closes the responsibility loop between publishing pre-trained generative model
inventions and their possible misuses, which makes it independent of the
current arms race. Code and models are available at
https://github.com/ningyu1991/ArtificialGANFingerprints .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MODNet: Real-Time Trimap-Free Portrait Matting via Objective Decomposition. (arXiv:2011.11961v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.11961">
<div class="article-summary-box-inner">
<span><p>Existing portrait matting methods either require auxiliary inputs that are
costly to obtain or involve multiple stages that are computationally expensive,
making them less suitable for real-time applications. In this work, we present
a light-weight matting objective decomposition network (MODNet) for portrait
matting in real-time with a single input image. The key idea behind our
efficient design is by optimizing a series of sub-objectives simultaneously via
explicit constraints. In addition, MODNet includes two novel techniques for
improving model efficiency and robustness. First, an Efficient Atrous Spatial
Pyramid Pooling (e-ASPP) module is introduced to fuse multi-scale features for
semantic estimation. Second, a self-supervised sub-objectives consistency (SOC)
strategy is proposed to adapt MODNet to real-world data to address the domain
shift problem common to trimap-free methods. MODNet is easy to be trained in an
end-to-end manner. It is much faster than contemporaneous methods and runs at
67 frames per second on a 1080Ti GPU. Experiments show that MODNet outperforms
prior trimap-free methods by a large margin on both Adobe Matting Dataset and a
carefully designed photographic portrait matting (PPM-100) benchmark proposed
by us. Further, MODNet achieves remarkable results on daily photos and videos.
Our code and models are available at https://github.com/ZHKKKe/MODNet, and the
PPM-100 benchmark is released at https://github.com/ZHKKKe/PPM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Globetrotter: Connecting Languages by Connecting Images. (arXiv:2012.04631v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.04631">
<div class="article-summary-box-inner">
<span><p>Machine translation between many languages at once is highly challenging,
since training with ground truth requires supervision between all language
pairs, which is difficult to obtain. Our key insight is that, while languages
may vary drastically, the underlying visual appearance of the world remains
consistent. We introduce a method that uses visual observations to bridge the
gap between languages, rather than relying on parallel corpora or topological
properties of the representations. We train a model that aligns segments of
text from different languages if and only if the images associated with them
are similar and each image in turn is well-aligned with its textual
description. We train our model from scratch on a new dataset of text in over
fifty languages with accompanying images. Experiments show that our method
outperforms previous work on unsupervised word and sentence translation using
retrieval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Responsible Disclosure of Generative Models Using Scalable Fingerprinting. (arXiv:2012.08726v5 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.08726">
<div class="article-summary-box-inner">
<span><p>Over the past years, deep generative models have achieved a new level of
performance. Generated data has become difficult, if not impossible, to be
distinguished from real data. While there are plenty of use cases that benefit
from this technology, there are also strong concerns on how this new technology
can be misused to generate deep fakes and enable misinformation at scale.
Unfortunately, current deep fake detection methods are not sustainable, as the
gap between real and fake continues to close. In contrast, our work enables a
responsible disclosure of such state-of-the-art generative models, that allows
model inventors to fingerprint their models, so that the generated samples
containing a fingerprint can be accurately detected and attributed to a source.
Our technique achieves this by an efficient and scalable ad-hoc generation of a
large population of models with distinct fingerprints. Our recommended
operation point uses a 128-bit fingerprint which in principle results in more
than $10^{38}$ identifiable models. Experiments show that our method fulfills
key properties of a fingerprinting mechanism and achieves effectiveness in deep
fake detection and attribution. Code and models are available at
https://github.com/ningyu1991/ScalableGANFingerprints .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simple Mutual Information based Registration Method for Thermal-Optical Image Pairs applied on a Novel Dataset. (arXiv:2101.06910v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.06910">
<div class="article-summary-box-inner">
<span><p>While thermal optical registered datasets are becoming widely available, most
of these works are based on image pairs which are pre-registered. However,
thermal imagers where these images are registered by default are quite
expensive. We present in this work, a thermal image registration technique
which is computationally lightweight, and can be employed regardless of the
resolution of the images captured. We use 2 different thermal imagers to create
a completely new database and introduce it as a part of this work as well. The
images captured are based on 5 different classes and encompass subjects like
the Prayagraj Kumbh Mela, one of the largest public fairs in the world,
captured over a period of 2 years.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Adversarial Neural Networks for Domain Generalization: When It Works and How to Improve. (arXiv:2102.03924v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.03924">
<div class="article-summary-box-inner">
<span><p>Theoretically, domain adaptation is a well-researched problem. Further, this
theory has been well-used in practice. In particular, we note the bound on
target error given by Ben-David et al. (2010) and the well-known
domain-aligning algorithm based on this work using Domain Adversarial Neural
Networks (DANN) presented by Ganin and Lempitsky (2015). Recently, multiple
variants of DANN have been proposed for the related problem of domain
generalization, but without much discussion of the original motivating bound.
In this paper, we investigate the validity of DANN in domain generalization
from this perspective. We investigate conditions under which application of
DANN makes sense and further consider DANN as a dynamic process during
training. Our investigation suggests that the application of DANN to domain
generalization may not be as straightforward as it seems. To address this, we
design an algorithmic extension to DANN in the domain generalization case. Our
experimentation validates both theory and algorithm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ImageNet as a Representative Basis for Deriving Generally Effective CNN Architectures. (arXiv:2103.09108v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.09108">
<div class="article-summary-box-inner">
<span><p>We investigate and improve the representativeness of ImageNet as a basis for
deriving generally effective convolutional neural network (CNN) architectures
that perform well on a diverse set of datasets and application domains. To this
end, we conduct an extensive empirical study for which we train 500 CNN
architectures, sampled from the broad AnyNetX design space, on ImageNet as well
as 8 other image classification datasets. We observe that the performances of
the architectures are highly dataset-dependent. Some datasets even exhibit a
negative error correlation with ImageNet across all architectures. We show how
to significantly increase these correlations by utilizing ImageNet subsets
restricted to fewer classes. We also identify the cumulative width across
layers as well as the total depth of the network as the most sensitive design
parameter with respect to changing datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Collapsible Linear Blocks for Super-Efficient Super Resolution. (arXiv:2103.09404v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.09404">
<div class="article-summary-box-inner">
<span><p>With the advent of smart devices that support 4K and 8K resolution, Single
Image Super Resolution (SISR) has become an important computer vision problem.
However, most super resolution deep networks are computationally very
expensive. In this paper, we propose Super-Efficient Super Resolution (SESR)
networks that establish a new state-of-the-art for efficient super resolution.
Our approach is based on linear overparameterization of CNNs and creates an
efficient model architecture for SISR. With theoretical analysis, we uncover
the limitations of existing overparameterization methods and show how the
proposed method alleviates them. Detailed experiments across six benchmark
datasets demonstrate that SESR achieves similar or better image quality than
state-of-the-art models while requiring 2x to 330x fewer Multiply-Accumulate
(MAC) operations. As a result, SESR can be used on constrained hardware to
perform x2 (1080p to 4K) and x4 (1080p to 8K) SISR. Towards this, we estimate
hardware performance numbers for a commercial Arm mobile-Neural Processing Unit
(NPU) for 1080p to 4K (x2) and 1080p to 8K (x4) SISR. Our results highlight the
challenges faced by super resolution on AI accelerators and demonstrate that
SESR is significantly faster (e.g., 6x-8x higher FPS) than existing models on
mobile-NPU. Finally, SESR outperforms prior models by 1.5x-2x in latency on Arm
CPU and GPU when deployed on a real mobile device. The code for this work is
available at https://github.com/ARM-software/sesr.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ClawCraneNet: Leveraging Object-level Relation for Text-based Video Segmentation. (arXiv:2103.10702v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.10702">
<div class="article-summary-box-inner">
<span><p>Text-based video segmentation is a challenging task that segments out the
natural language referred objects in videos. It essentially requires semantic
comprehension and fine-grained video understanding. Existing methods introduce
language representation into segmentation models in a bottom-up manner, which
merely conducts vision-language interaction within local receptive fields of
ConvNets. We argue that such interaction is not fulfilled since the model can
barely construct region-level relationships given partial observations, which
is contrary to the description logic of natural language/referring expressions.
In fact, people usually describe a target object using relations with other
objects, which may not be easily understood without seeing the whole video. To
address the issue, we introduce a novel top-down approach by imitating how we
human segment an object with the language guidance. We first figure out all
candidate objects in videos and then choose the refereed one by parsing
relations among those high-level objects. Three kinds of object-level relations
are investigated for precise relationship understanding, i.e., positional
relation, text-guided semantic relation, and temporal relation. Extensive
experiments on A2D Sentences and J-HMDB Sentences show our method outperforms
state-of-the-art methods by a large margin. Qualitative results also show our
results are more explainable.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual Contrastive Loss and Attention for GANs. (arXiv:2103.16748v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.16748">
<div class="article-summary-box-inner">
<span><p>Generative Adversarial Networks (GANs) produce impressive results on
unconditional image generation when powered with large-scale image datasets.
Yet generated images are still easy to spot especially on datasets with high
variance (e.g. bedroom, church). In this paper, we propose various improvements
to further push the boundaries in image generation. Specifically, we propose a
novel dual contrastive loss and show that, with this loss, discriminator learns
more generalized and distinguishable representations to incentivize generation.
In addition, we revisit attention and extensively experiment with different
attention blocks in the generator. We find attention to be still an important
module for successful image generation even though it was not used in the
recent state-of-the-art models. Lastly, we study different attention
architectures in the discriminator, and propose a reference attention
mechanism. By combining the strengths of these remedies, we improve the
compelling state-of-the-art Fr\'{e}chet Inception Distance (FID) by at least
17.5% on several benchmark datasets. We obtain even more significant
improvements on compositional synthetic scenes (up to 47.5% in FID). Code and
models are available at https://github.com/ningyu1991/AttentionDualContrastGAN .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Adversarial Registration for Improved Conditional Deformable Templates. (arXiv:2105.04349v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.04349">
<div class="article-summary-box-inner">
<span><p>Deformable templates are essential to large-scale medical image registration,
segmentation, and population analysis. Current conventional and deep
network-based methods for template construction use only regularized
registration objectives and often yield templates with blurry and/or
anatomically implausible appearance, confounding downstream biomedical
interpretation. We reformulate deformable registration and conditional template
estimation as an adversarial game wherein we encourage realism in the moved
templates with a generative adversarial registration framework conditioned on
flexible image covariates. The resulting templates exhibit significant gain in
specificity to attributes such as age and disease, better fit underlying
group-wise spatiotemporal trends, and achieve improved sharpness and
centrality. These improvements enable more accurate population modeling with
diverse covariates for standardized downstream analyses and easier anatomical
delineation for structures of interest.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Online Coreset Selection for Rehearsal-based Continual Learning. (arXiv:2106.01085v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01085">
<div class="article-summary-box-inner">
<span><p>A dataset is a shred of crucial evidence to describe a task. However, each
data point in the dataset does not have the same potential, as some of the data
points can be more representative or informative than others. This unequal
importance among the data points may have a large impact in rehearsal-based
continual learning, where we store a subset of the training examples (coreset)
to be replayed later to alleviate catastrophic forgetting. In continual
learning, the quality of the samples stored in the coreset directly affects the
model's effectiveness and efficiency. The coreset selection problem becomes
even more important under realistic settings, such as imbalanced continual
learning or noisy data scenarios. To tackle this problem, we propose Online
Coreset Selection (OCS), a simple yet effective method that selects the most
representative and informative coreset at each iteration and trains them in an
online manner. Our proposed method maximizes the model's adaptation to a
current dataset while selecting high-affinity samples to past tasks, which
directly inhibits catastrophic forgetting. We validate the effectiveness of our
coreset selection mechanism over various standard, imbalanced, and noisy
datasets against strong continual learning baselines, demonstrating that it
improves task adaptation and prevents catastrophic forgetting in a
sample-efficient manner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Affiliate: Mutual Centralized Learning for Few-shot Classification. (arXiv:2106.05517v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05517">
<div class="article-summary-box-inner">
<span><p>Few-shot learning (FSL) aims to learn a classifier that can be easily adapted
to accommodate new tasks not seen during training, given only a few examples.
To handle the limited-data problem in few-shot regimes, recent methods tend to
collectively use a set of local features to densely represent an image instead
of using a mixed global feature. They generally explore a unidirectional
query-to-support paradigm in FSL, e.g., find the nearest/optimal support
feature for each query feature and aggregate these local matches for a joint
classification. In this paper, we propose a new method Mutual Centralized
Learning (MCL) to fully affiliate the two disjoint sets of dense features in a
bidirectional paradigm. We associate each local feature with a particle that
can bidirectionally random walk in a discrete feature space by the
affiliations. To estimate the class probability, we propose the features'
accessibility that measures the expected number of visits to the support
features of that class in a Markov process. We relate our method to learning a
centrality on an affiliation network and demonstrate its capability to be
plugged in existing methods by highlighting centralized local features.
Experiments show that our method achieves the state-of-the-art on both
miniImageNet and tieredImageNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gradient-Based Quantification of Epistemic Uncertainty for Deep Object Detectors. (arXiv:2107.04517v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.04517">
<div class="article-summary-box-inner">
<span><p>The vast majority of uncertainty quantification methods for deep object
detectors such as variational inference are based on the network output. Here,
we study gradient-based epistemic uncertainty metrics for deep object detectors
to obtain reliable confidence estimates. We show that they contain predictive
information and that they capture information orthogonal to that of common,
output-based uncertainty estimation methods like Monte-Carlo dropout and deep
ensembles. To this end, we use meta classification and meta regression to
produce confidence estimates using gradient metrics and other baselines for
uncertainty quantification which are in principle applicable to any object
detection architecture. Specifically, we employ false positive detection and
prediction of localization quality to investigate uncertainty content of our
metrics and compute the calibration errors of meta classifiers. Moreover, we
use them as a post-processing filter mechanism to the object detection pipeline
and compare object detection performance. Our results show that gradient-based
uncertainty is itself on par with output-based methods across different
detectors and datasets. More significantly, combined meta classifiers based on
gradient and output-based metrics outperform the standalone models. Based on
this result, we conclude that gradient uncertainty adds orthogonal information
to output-based methods. This suggests that variational inference may be
supplemented by gradient-based uncertainty to obtain improved confidence
measures, contributing to down-stream applications of deep object detectors and
improving their probabilistic reliability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HDMapNet: An Online HD Map Construction and Evaluation Framework. (arXiv:2107.06307v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.06307">
<div class="article-summary-box-inner">
<span><p>Constructing HD semantic maps is a central component of autonomous driving.
However, traditional pipelines require a vast amount of human efforts and
resources in annotating and maintaining the semantics in the map, which limits
its scalability. In this paper, we introduce the problem of HD semantic map
learning, which dynamically constructs the local semantics based on onboard
sensor observations. Meanwhile, we introduce a semantic map learning method,
dubbed HDMapNet. HDMapNet encodes image features from surrounding cameras
and/or point clouds from LiDAR, and predicts vectorized map elements in the
bird's-eye view. We benchmark HDMapNet on nuScenes dataset and show that in all
settings, it performs better than baseline methods. Of note, our camera-LiDAR
fusion-based HDMapNet outperforms existing methods by more than 50% in all
metrics. In addition, we develop semantic-level and instance-level metrics to
evaluate the map learning performance. Finally, we showcase our method is
capable of predicting a locally consistent map. By introducing the method and
metrics, we invite the community to study this novel map learning problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CycleMLP: A MLP-like Architecture for Dense Prediction. (arXiv:2107.10224v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.10224">
<div class="article-summary-box-inner">
<span><p>This paper presents a simple MLP-like architecture, CycleMLP, which is a
versatile backbone for visual recognition and dense predictions. As compared to
modern MLP architectures, e.g., MLP-Mixer, ResMLP, and gMLP, whose
architectures are correlated to image size and thus are infeasible in object
detection and segmentation, CycleMLP has two advantages compared to modern
approaches. (1) It can cope with various image sizes. (2) It achieves linear
computational complexity to image size by using local windows. In contrast,
previous MLPs have $O(N^2)$ computations due to fully spatial connections. We
build a family of models which surpass existing MLPs and even state-of-the-art
Transformer-based models, e.g., Swin Transformer, while using fewer parameters
and FLOPs. We expand the MLP-like models' applicability, making them a
versatile backbone for dense prediction tasks. CycleMLP achieves competitive
results on object detection, instance segmentation, and semantic segmentation.
In particular, CycleMLP-Tiny outperforms Swin-Tiny by 1.3% mIoU on ADE20K
dataset with fewer FLOPs. Moreover, CycleMLP also shows excellent zero-shot
robustness on ImageNet-C dataset. Code is available at
https://github.com/ShoufaChen/CycleMLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cervical Optical Coherence Tomography Image Classification Based on Contrastive Self-Supervised Texture Learning. (arXiv:2108.05081v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.05081">
<div class="article-summary-box-inner">
<span><p>Background: Cervical cancer seriously affects the health of the female
reproductive system. Optical coherence tomography (OCT) emerged as a
non-invasive, high-resolution imaging technology for cervical disease
detection. However, OCT image annotation is knowledge-intensive and
time-consuming, which impedes the training process of deep-learning-based
classification models. Purpose: This study aims to develop a computer-aided
diagnosis (CADx) approach to classifying in-vivo cervical OCT images based on
self-supervised learning. Methods: In addition to high-level semantic features
extracted by a convolutional neural network (CNN), the proposed CADx approach
leverages unlabeled cervical OCT images' texture features learned by
contrastive texture learning. We conducted ten-fold cross-validation on the OCT
image dataset from a multi-center clinical study on 733 patients from China.
Results: In a binary classification task for detecting high-risk diseases,
including high-grade squamous intraepithelial lesion and cervical cancer, our
method achieved an area-under-the-curve value of 0.9798 plus or minus 0.0157
with a sensitivity of 91.17 plus or minus 4.99% and a specificity of 93.96 plus
or minus 4.72% for OCT image patches; also, it outperformed two out of four
medical experts on the test set. Furthermore, our method achieved a 91.53%
sensitivity and 97.37% specificity on an external validation dataset containing
287 3D OCT volumes from 118 Chinese patients in a new hospital using a
cross-shaped threshold voting strategy. Conclusions: The proposed
contrastive-learning-based CADx method outperformed the end-to-end CNN models
and provided better interpretability based on texture features, which holds
great potential to be used in the clinical protocol of "see-and-treat."
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PGTRNet: Two-phase Weakly Supervised Object Detection with Pseudo Ground Truth Refinement. (arXiv:2108.11439v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11439">
<div class="article-summary-box-inner">
<span><p>Current state-of-the-art weakly supervised object detection (WSOD) studies
mainly follow a two-stage training strategy which integrates a fully supervised
detector (FSD) with a pure WSOD model. There are two main problems hindering
the performance of the two-phase WSOD approaches, i.e., insufficient learning
problem and strict reliance between the FSD and the pseudo ground truth (PGT)
generated by the WSOD model. This paper proposes pseudo ground truth refinement
network (PGTRNet), a simple yet effective method without introducing any extra
learnable parameters, to cope with these problems. PGTRNet utilizes multiple
bounding boxes to establish the PGT, mitigating the insufficient learning
problem. Besides, we propose a novel online PGT refinement approach to steadily
improve the quality of PGT by fully taking advantage of the power of FSD during
the second-phase training, decoupling the first and second-phase models.
Elaborate experiments are conducted on the PASCAL VOC 2007 benchmark to verify
the effectiveness of our methods. Experimental results demonstrate that PGTRNet
boosts the backbone model by 2.1% mAP and achieves the state-of-the-art
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Panoptic SegFormer: Delving Deeper into Panoptic Segmentation with Transformers. (arXiv:2109.03814v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03814">
<div class="article-summary-box-inner">
<span><p>Panoptic segmentation involves a combination of joint semantic segmentation
and instance segmentation, where image contents are divided into two types:
things and stuff. We present Panoptic SegFormer, a general framework for
panoptic segmentation with transformers. It contains three innovative
components: an efficient deeply-supervised mask decoder, a query decoupling
strategy, and an improved post-processing method. We also use Deformable DETR
to efficiently process multi-scale features, which is a fast and efficient
version of DETR. Specifically, we supervise the attention modules in the mask
decoder in a layer-wise manner. This deep supervision strategy lets the
attention modules quickly focus on meaningful semantic regions. It improves
performance and reduces the number of required training epochs by half compared
to Deformable DETR. Our query decoupling strategy decouples the
responsibilities of the query set and avoids mutual interference between things
and stuff. In addition, our post-processing strategy improves performance
without additional costs by jointly considering classification and segmentation
qualities to resolve conflicting mask overlaps. Our approach increases the
accuracy 6.2\% PQ over the baseline DETR model. Panoptic SegFormer achieves
state-of-the-art results on COCO test-dev with 56.2\% PQ. It also shows
stronger zero-shot robustness over existing methods. The code is released at
\url{https://github.com/zhiqi-li/Panoptic-SegFormer}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">M5Product: Self-harmonized Contrastive Learning for E-commercial Multi-modal Pretraining. (arXiv:2109.04275v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04275">
<div class="article-summary-box-inner">
<span><p>Despite the potential of multi-modal pre-training to learn highly
discriminative feature representations from complementary data modalities,
current progress is being slowed by the lack of large-scale modality-diverse
datasets. By leveraging the natural suitability of E-commerce, where different
modalities capture complementary semantic information, we contribute a
large-scale multi-modal pre-training dataset M5Product. The dataset comprises 5
modalities (image, text, table, video, and audio), covers over 6,000 categories
and 5,000 attributes, and is 500 larger than the largest publicly available
dataset with a similar number of modalities. Furthermore, M5Product contains
incomplete modality pairs and noise while also having a long-tailed
distribution, resembling most real-world problems. We further propose
Self-harmonized ContrAstive LEarning (SCALE), a novel pretraining framework
that integrates the different modalities into a unified model through an
adaptive feature fusion mechanism, where the importance of each modality is
learned directly from the modality embeddings and impacts the inter-modality
contrastive learning and masked tasks within a multi-modal transformer model.
We evaluate the current multi-modal pre-training state-of-the-art approaches
and benchmark their ability to learn from unlabeled data when faced with the
large number of modalities in the M5Product dataset. We conduct extensive
experiments on four downstream tasks and demonstrate the superiority of our
SCALE model, providing insights into the importance of dataset scale and
diversity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">METEOR:A Dense, Heterogeneous, and Unstructured Traffic Dataset With Rare Behaviors. (arXiv:2109.07648v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07648">
<div class="article-summary-box-inner">
<span><p>We present a new traffic dataset, METEOR, which captures traffic patterns and
multi-agent driving behaviors in unstructured scenarios. METEOR consists of
more than 1000 one-minute videos, over 2 million annotated frames with bounding
boxes and GPS trajectories for 16 unique agent categories, and more than 13
million bounding boxes for traffic agents. METEOR is a dataset for rare and
interesting, multi-agent driving behaviors that are grouped into traffic
violations, atypical interactions, and diverse scenarios. Every video in METEOR
is tagged using a diverse range of factors corresponding to weather, time of
the day, road conditions, and traffic density. We use METEOR to benchmark
perception methods for object detection and multi-agent behavior prediction.
Our key finding is that state-of-the-art models for object detection and
behavior prediction, which otherwise succeed on existing datasets such as
Waymo, fail on the METEOR dataset. METEOR marks the first step towards the
development of more sophisticated perception models for dense, heterogeneous,
and unstructured scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image-Based CLIP-Guided Essence Transfer. (arXiv:2110.12427v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12427">
<div class="article-summary-box-inner">
<span><p>We make the distinction between (i) style transfer, in which a source image
is manipulated to match the textures and colors of a target image, and (ii)
essence transfer, in which one edits the source image to include high-level
semantic attributes from the target. Crucially, the semantic attributes that
constitute the essence of an image may differ from image to image. Our blending
operator combines the powerful StyleGAN generator and the semantic encoder of
CLIP in a novel way that is simultaneously additive in both latent spaces,
resulting in a mechanism that guarantees both identity preservation and
high-level feature transfer without relying on a facial recognition network. We
present two variants of our method. The first is based on optimization, while
the second fine-tunes an existing inversion encoder to perform essence
extraction. Through extensive experiments, we demonstrate the superiority of
our methods for essence transfer over existing methods for style transfer,
domain adaptation, and text-based semantic editing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Study of Training End-to-End Vision-and-Language Transformers. (arXiv:2111.02387v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02387">
<div class="article-summary-box-inner">
<span><p>Vision-and-language (VL) pre-training has proven to be highly effective on
various VL downstream tasks. While recent work has shown that fully
transformer-based VL models can be more efficient than previous
region-feature-based methods, their performance on downstream tasks often
degrades significantly. In this paper, we present METER, a Multimodal
End-to-end TransformER framework, through which we investigate how to design
and pre-train a fully transformer-based VL model in an end-to-end manner.
Specifically, we dissect the model designs along multiple dimensions: vision
encoders (e.g., CLIP-ViT, Swin transformer), text encoders (e.g., RoBERTa,
DeBERTa), multimodal fusion module (e.g., merged attention vs. co-attention),
architectural design (e.g., encoder-only vs. encoder-decoder), and pre-training
objectives (e.g., masked image modeling). We conduct comprehensive experiments
and provide insights on how to train a performant VL transformer. METER
achieves an accuracy of 77.64% on the VQAv2 test-std set using only 4M images
for pre-training, surpassing the state-of-the-art region-feature-based model by
1.04%, and outperforming the previous best fully transformer-based model by
1.6%. Notably, when further scaled up, our best VQA model achieves an accuracy
of 80.54%. Code and pre-trained models are released at
https://github.com/zdou0830/METER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TimeMatch: Unsupervised Cross-Region Adaptation by Temporal Shift Estimation. (arXiv:2111.02682v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02682">
<div class="article-summary-box-inner">
<span><p>The recent developments of deep learning models that capture the complex
temporal patterns of crop phenology have greatly advanced crop classification
of Satellite Image Time Series (SITS). However, when applied to target regions
spatially different from the training region, these models perform poorly
without any target labels due to the temporal shift of crop phenology between
regions. To address this unsupervised cross-region adaptation setting, existing
methods learn domain-invariant features without any target supervision, but not
the temporal shift itself. As a consequence, these techniques provide only
limited benefits for SITS. In this paper, we propose TimeMatch, a new
unsupervised domain adaptation method for SITS that directly accounts for the
temporal shift. TimeMatch consists of two components: 1) temporal shift
estimation, which estimates the temporal shift of the unlabeled target region
with a source-trained model, and 2) TimeMatch learning, which combines temporal
shift estimation with semi-supervised learning to adapt a classifier to an
unlabeled target region. We also introduce an open-access dataset for
cross-region adaptation with SITS from four different regions in Europe. On
this dataset, we demonstrate that TimeMatch outperforms all competing methods
by 11% in F1-score across five different adaptation scenarios, setting a new
state-of-the-art for cross-region adaptation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Depth from Focus with Differential Focus Volume. (arXiv:2112.01712v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01712">
<div class="article-summary-box-inner">
<span><p>Depth-from-focus (DFF) is a technique that infers depth using the focus
change of a camera. In this work, we propose a convolutional neural network
(CNN) to find the best-focused pixels in a focal stack and infer depth from the
focus estimation. The key innovation of the network is the novel deep
differential focus volume (DFV). By computing the first-order derivative with
the stacked features over different focal distances, DFV is able to capture
both the focus and context information for focus analysis. Besides, we also
introduce a probability regression mechanism for focus estimation to handle
sparsely sampled focal stacks and provide uncertainty estimation to the final
prediction. Comprehensive experiments demonstrate that the proposed model
achieves state-of-the-art performance on multiple datasets with good
generalizability and fast speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Conditional Point Diffusion-Refinement Paradigm for 3D Point Cloud Completion. (arXiv:2112.03530v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03530">
<div class="article-summary-box-inner">
<span><p>3D point cloud is an important 3D representation for capturing real world 3D
objects. However, real-scanned 3D point clouds are often incomplete, and it is
important to recover complete point clouds for downstream applications. Most
existing point cloud completion methods use Chamfer Distance (CD) loss for
training. The CD loss estimates correspondences between two point clouds by
searching nearest neighbors, which does not capture the overall point density
distribution on the generated shape, and therefore likely leads to non-uniform
point cloud generation. To tackle this problem, we propose a novel Point
Diffusion-Refinement (PDR) paradigm for point cloud completion. PDR consists of
a Conditional Generation Network (CGNet) and a ReFinement Network (RFNet). The
CGNet uses a conditional generative model called the denoising diffusion
probabilistic model (DDPM) to generate a coarse completion conditioned on the
partial observation. DDPM establishes a one-to-one pointwise mapping between
the generated point cloud and the uniform ground truth, and then optimizes the
mean squared error loss to realize uniform generation. The RFNet refines the
coarse output of the CGNet and further improves quality of the completed point
cloud. Furthermore, we develop a novel dual-path architecture for both
networks. The architecture can (1) effectively and efficiently extract
multi-level features from partially observed point clouds to guide completion,
and (2) accurately manipulate spatial locations of 3D points to obtain smooth
surfaces and sharp details. Extensive experimental results on various benchmark
datasets show that our PDR paradigm outperforms previous state-of-the-art
methods for point cloud completion. Remarkably, with the help of the RFNet, we
can accelerate the iterative generation process of the DDPM by up to 50 times
without much performance drop.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">All You Need is RAW: Defending Against Adversarial Attacks with Camera Image Pipelines. (arXiv:2112.09219v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09219">
<div class="article-summary-box-inner">
<span><p>Existing neural networks for computer vision tasks are vulnerable to
adversarial attacks: adding imperceptible perturbations to the input images can
fool these methods to make a false prediction on an image that was correctly
predicted without the perturbation. Various defense methods have proposed
image-to-image mapping methods, either including these perturbations in the
training process or removing them in a preprocessing denoising step. In doing
so, existing methods often ignore that the natural RGB images in today's
datasets are not captured but, in fact, recovered from RAW color filter array
captures that are subject to various degradations in the capture. In this work,
we exploit this RAW data distribution as an empirical prior for adversarial
defense. Specifically, we proposed a model-agnostic adversarial defensive
method, which maps the input RGB images to Bayer RAW space and back to output
RGB using a learned camera image signal processing (ISP) pipeline to eliminate
potential adversarial patterns. The proposed method acts as an off-the-shelf
preprocessing module and, unlike model-specific adversarial training methods,
does not require adversarial images to train. As a result, the method
generalizes to unseen tasks without additional retraining. Experiments on
large-scale datasets (e.g., ImageNet, COCO) for different vision tasks (e.g.,
classification, semantic segmentation, object detection) validate that the
method significantly outperforms existing methods across task domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">iSegFormer: Interactive Segmentation via Transformers with Application to 3D Knee MR Images. (arXiv:2112.11325v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11325">
<div class="article-summary-box-inner">
<span><p>We propose iSegFormer, a memory-efficient transformer that combines a Swin
transformer with a lightweight multilayer perceptron (MLP) decoder. With the
efficient Swin transformer blocks for hierarchical self-attention and the
simple MLP decoder for aggregating both local and global attention, iSegFormer
learns powerful representations while achieving high computational
efficiencies. Specifically, we apply iSegFormer to interactive 3D medical image
segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SAMCNet for Spatial-configuration-based Classification: A Summary of Results. (arXiv:2112.12219v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.12219">
<div class="article-summary-box-inner">
<span><p>The goal of spatial-configuration-based classification is to build a
classifier to distinguish two classes (e.g., responder, non-responder) based on
the spatial arrangements (e.g., spatial interactions between different point
categories) given multi-category point data from two classes. This problem is
important for generating hypotheses in medical pathology towards discovering
new immunotherapies for cancer treatment as well as for other applications in
biomedical research and microbial ecology. This problem is challenging due to
an exponential number of category subsets which may vary in the strength of
spatial interactions. Most prior efforts on using human selected spatial
association measures may not be sufficient for capturing the relevant (e.g.,
surrounded by) spatial interactions which may be of biological significance. In
addition, the related deep neural networks are limited to category pairs and do
not explore larger subsets of point categories. To overcome these limitations,
we propose a Spatial-interaction Aware Multi-Category deep neural Network
(SAMCNet) architecture and contribute novel local reference frame
characterization and point pair prioritization layers for
spatial-configuration-based classification. Extensive experimental results on
multiple cancer datasets show that the proposed architecture provides higher
prediction accuracy over baseline methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Disturbance-Free Visual Mobile Manipulation. (arXiv:2112.12612v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.12612">
<div class="article-summary-box-inner">
<span><p>Deep reinforcement learning has shown promising results on an abundance of
robotic tasks in simulation, including visual navigation and manipulation.
Prior work generally aims to build embodied agents that solve their assigned
tasks as quickly as possible, while largely ignoring the problems caused by
collision with objects during interaction. This lack of prioritization is
understandable: there is no inherent cost in breaking virtual objects. As a
result, "well-trained" agents frequently collide with objects before achieving
their primary goals, a behavior that would be catastrophic in the real world.
In this paper, we study the problem of training agents to complete the task of
visual mobile manipulation in the ManipulaTHOR environment while avoiding
unnecessary collision (disturbance) with objects. We formulate disturbance
avoidance as a penalty term in the reward function, but find that directly
training with such penalized rewards often results in agents being unable to
escape poor local optima. Instead, we propose a two-stage training curriculum
where an agent is first allowed to freely explore and build basic competencies
without penalization, after which a disturbance penalty is introduced to refine
the agent's behavior. Results on testing scenes show that our curriculum not
only avoids these poor local optima, but also leads to 10% absolute gains in
success rate without disturbance, compared to our state-of-the-art baselines.
Moreover, we propose a novel disturbance-prediction auxiliary task accelerating
learning and further improving success rates.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CheXstray: Real-time Multi-Modal Data Concordance for Drift Detection in Medical Imaging AI. (arXiv:2202.02833v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02833">
<div class="article-summary-box-inner">
<span><p>Clinical Artificial lntelligence (AI) applications are rapidly expanding
worldwide, and have the potential to impact to all areas of medical practice.
Medical imaging applications constitute a vast majority of approved clinical AI
applications. Though healthcare systems are eager to adopt AI solutions a
fundamental question remains: \textit{what happens after the AI model goes into
production?} We use the CheXpert and PadChest public datasets to build and test
a medical imaging AI drift monitoring workflow to track data and model drift
without contemporaneous ground truth. We simulate drift in multiple experiments
to compare model performance with our novel multi-modal drift metric, which
uses DICOM metadata, image appearance representation from a variational
autoencoder (VAE), and model output probabilities as input. Through
experimentation, we demonstrate a strong proxy for ground truth performance
using unsupervised distributional shifts in relevant metadata, predicted
probabilities, and VAE latent representation. Our key contributions include (1)
proof-of-concept for medical imaging drift detection that includes the use of
VAE and domain specific statistical methods, (2) a multi-modal methodology to
measure and unify drift metrics, (3) new insights into the challenges and
solutions to observe deployed medical imaging AI, and (4) creation of
open-source tools that enable others to easily run their own workflows and
scenarios. This work has important implications. It addresses the concerning
translation gap found in continuous medical imaging AI model monitoring common
in dynamic healthcare environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A multiscale spatiotemporal approach for smallholder irrigation detection. (arXiv:2202.04239v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04239">
<div class="article-summary-box-inner">
<span><p>In presenting an irrigation detection methodology that leverages multiscale
satellite imagery of vegetation abundance, this paper introduces a process to
supplement limited ground-collected labels and ensure classifier applicability
in an area of interest. Spatiotemporal analysis of MODIS 250m Enhanced
Vegetation Index (EVI) timeseries characterizes native vegetation phenologies
at regional scale to provide the basis for a continuous phenology map that
guides supplementary label collection over irrigated and non-irrigated
agriculture. Subsequently, validated dry season greening and senescence cycles
observed in 10m Sentinel-2 imagery are used to train a suite of classifiers for
automated detection of potential smallholder irrigation. Strategies to improve
model robustness are demonstrated, including a method of data augmentation that
randomly shifts training samples; and an assessment of classifier types that
produce the best performance in withheld target regions. The methodology is
applied to detect smallholder irrigation in two states in the Ethiopian
highlands, Tigray and Amhara. Results show that a transformer-based neural
network architecture allows for the most robust prediction performance in
withheld regions, followed closely by a CatBoost random forest model. Over
withheld ground-collection survey labels, the transformer-based model achieves
96.7% accuracy over non-irrigated samples and 95.9% accuracy over irrigated
samples. Over a larger set of samples independently collected via the
introduced method of label supplementation, non-irrigated and irrigated labels
are predicted with 98.3% and 95.5% accuracy, respectively. The detection model
is then deployed over Tigray and Amhara, revealing crop rotation patterns and
year-over-year irrigated area change. Predictions suggest that irrigated area
in these two states has decreased by approximately 40% from 2020 to 2021.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Dirichlet uncertainty for unsupervised out-of-distribution detection of eye fundus photographs in glaucoma screening. (arXiv:2202.12634v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12634">
<div class="article-summary-box-inner">
<span><p>The development of automatic tools for early glaucoma diagnosis with color
fundus photographs can significantly reduce the impact of this disease.
However, current state-of-the-art solutions are not robust to real-world
scenarios, providing over-confident predictions for out-of-distribution cases.
With this in mind, we propose a model based on the Dirichlet distribution that
allows to obtain class-wise probabilities together with an uncertainty
estimation without exposure to out-of-distribution cases. We demonstrate our
approach on the AIROGS challenge. At the start of the final test phase (8 Feb.
2022), our method had the highest average score among all submissions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">X-Trans2Cap: Cross-Modal Knowledge Transfer using Transformer for 3D Dense Captioning. (arXiv:2203.00843v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00843">
<div class="article-summary-box-inner">
<span><p>3D dense captioning aims to describe individual objects by natural language
in 3D scenes, where 3D scenes are usually represented as RGB-D scans or point
clouds. However, only exploiting single modal information, e.g., point cloud,
previous approaches fail to produce faithful descriptions. Though aggregating
2D features into point clouds may be beneficial, it introduces an extra
computational burden, especially in inference phases. In this study, we
investigate a cross-modal knowledge transfer using Transformer for 3D dense
captioning, X-Trans2Cap, to effectively boost the performance of single-modal
3D caption through knowledge distillation using a teacher-student framework. In
practice, during the training phase, the teacher network exploits auxiliary 2D
modality and guides the student network that only takes point clouds as input
through the feature consistency constraints. Owing to the well-designed
cross-modal feature fusion module and the feature alignment in the training
phase, X-Trans2Cap acquires rich appearance information embedded in 2D images
with ease. Thus, a more faithful caption can be generated only using point
clouds during the inference. Qualitative and quantitative results confirm that
X-Trans2Cap outperforms previous state-of-the-art by a large margin, i.e.,
about +21 and about +16 absolute CIDEr score on ScanRefer and Nr3D datasets,
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bending Reality: Distortion-aware Transformers for Adapting to Panoramic Semantic Segmentation. (arXiv:2203.01452v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01452">
<div class="article-summary-box-inner">
<span><p>Panoramic images with their 360-degree directional view encompass exhaustive
information about the surrounding space, providing a rich foundation for scene
understanding. To unfold this potential in the form of robust panoramic
segmentation models, large quantities of expensive, pixel-wise annotations are
crucial for success. Such annotations are available, but predominantly for
narrow-angle, pinhole-camera images which, off the shelf, serve as sub-optimal
resources for training panoramic models. Distortions and the distinct
image-feature distribution in 360-degree panoramas impede the transfer from the
annotation-rich pinhole domain and therefore come with a big dent in
performance. To get around this domain difference and bring together semantic
annotations from pinhole- and 360-degree surround-visuals, we propose to learn
object deformations and panoramic image distortions in the Deformable Patch
Embedding (DPE) and Deformable MLP (DMLP) components which blend into our
Transformer for PAnoramic Semantic Segmentation (Trans4PASS) model. Finally, we
tie together shared semantics in pinhole- and panoramic feature embeddings by
generating multi-scale prototype features and aligning them in our Mutual
Prototypical Adaptation (MPA) for unsupervised domain adaptation. On the indoor
Stanford2D3D dataset, our Trans4PASS with MPA maintains comparable performance
to fully-supervised state-of-the-arts, cutting the need for over 1,400 labeled
panoramas. On the outdoor DensePASS dataset, we break state-of-the-art by
14.39% mIoU and set the new bar at 56.38%. Code will be made publicly available
at https://github.com/jamycheung/Trans4PASS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Universal Backward-Compatible Representation Learning. (arXiv:2203.01583v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01583">
<div class="article-summary-box-inner">
<span><p>Conventional model upgrades for visual search systems require offline refresh
of gallery features by feeding gallery images into new models (dubbed as
"backfill"), which is time-consuming and expensive, especially in large-scale
applications. The task of backward-compatible representation learning is
therefore introduced to support backfill-free model upgrades, where the new
query features are interoperable with the old gallery features. Despite the
success, previous works only investigated a close-set training scenario (i.e.,
the new training set shares the same classes as the old one), and are limited
by more realistic and challenging open-set scenarios. To this end, we first
introduce a new problem of universal backward-compatible representation
learning, covering all possible data split in model upgrades. We further
propose a simple yet effective method, dubbed as Universal Backward-Compatible
Training (UniBCT) with a novel structural prototype refinement algorithm, to
learn compatible representations in all kinds of model upgrading benchmarks in
a unified manner. Comprehensive experiments on the large-scale face recognition
datasets MS1Mv3 and IJB-C fully demonstrate the effectiveness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Texture for Fooling Person Detectors in the Physical World. (arXiv:2203.03373v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03373">
<div class="article-summary-box-inner">
<span><p>Nowadays, cameras equipped with AI systems can capture and analyze images to
detect people automatically. However, the AI system can make mistakes when
receiving deliberately designed patterns in the real world, i.e., physical
adversarial examples. Prior works have shown that it is possible to print
adversarial patches on clothes to evade DNN-based person detectors. However,
these adversarial examples could have catastrophic drops in the attack success
rate when the viewing angle (i.e., the camera's angle towards the object)
changes. To perform a multi-angle attack, we propose Adversarial Texture
(AdvTexture). AdvTexture can cover clothes with arbitrary shapes so that people
wearing such clothes can hide from person detectors from different viewing
angles. We propose a generative method, named Toroidal-Cropping-based
Expandable Generative Attack (TC-EGA), to craft AdvTexture with repetitive
structures. We printed several pieces of cloth with AdvTexure and then made
T-shirts, skirts, and dresses in the physical world. Experiments showed that
these clothes could fool person detectors in the physical world.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Region-Aware Face Swapping. (arXiv:2203.04564v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04564">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel Region-Aware Face Swapping (RAFSwap) network to
achieve identity-consistent harmonious high-resolution face generation in a
local-global manner: \textbf{1)} Local Facial Region-Aware (FRA) branch
augments local identity-relevant features by introducing the Transformer to
effectively model misaligned cross-scale semantic interaction. \textbf{2)}
Global Source Feature-Adaptive (SFA) branch further complements global
identity-relevant cues for generating identity-consistent swapped faces.
Besides, we propose a \textit{Face Mask Predictor} (FMP) module incorporated
with StyleGAN2 to predict identity-relevant soft facial masks in an
unsupervised manner that is more practical for generating harmonious
high-resolution faces. Abundant experiments qualitatively and quantitatively
demonstrate the superiority of our method for generating more
identity-consistent high-resolution swapped faces over SOTA methods, \eg,
obtaining 96.70 ID retrieval that outperforms SOTA MegaFS by 5.87$\uparrow$.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BEAT: A Large-Scale Semantic and Emotional Multi-Modal Dataset for Conversational Gestures Synthesis. (arXiv:2203.05297v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05297">
<div class="article-summary-box-inner">
<span><p>Achieving realistic, vivid, and human-like synthesized conversational
gestures conditioned on multi-modal data is still an unsolved problem, due to
the lack of available datasets, models and standard evaluation metrics. To
address this, we build Body-Expression-Audio-Text dataset, BEAT, which has i)
76 hours, high-quality, multi-modal data captured from 30 speakers talking with
eight different emotions and in four different languages, ii) 32 millions
frame-level emotion and semantic relevance annotations.Our statistical analysis
on BEAT demonstrates the correlation of conversational gestures with facial
expressions, emotions, and semantics, in addition to the known correlation with
audio, text, and speaker identity. Qualitative and quantitative experiments
demonstrate metrics' validness, ground truth data quality, and baseline's
state-of-the-art performance. To the best of our knowledge, BEAT is the largest
motion capture dataset for investigating the human gestures, which may
contribute to a number of different research fields including controllable
gesture synthesis, cross-modality analysis, emotional gesture recognition. The
data, code and model will be released for research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Generalization via Shuffled Style Assembly for Face Anti-Spoofing. (arXiv:2203.05340v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05340">
<div class="article-summary-box-inner">
<span><p>With diverse presentation attacks emerging continually, generalizable face
anti-spoofing (FAS) has drawn growing attention. Most existing methods
implement domain generalization (DG) on the complete representations. However,
different image statistics may have unique properties for the FAS tasks. In
this work, we separate the complete representation into content and style ones.
A novel Shuffled Style Assembly Network (SSAN) is proposed to extract and
reassemble different content and style features for a stylized feature space.
Then, to obtain a generalized representation, a contrastive learning strategy
is developed to emphasize liveness-related style information while suppress the
domain-specific one. Finally, the representations of the correct assemblies are
used to distinguish between living and spoofing during the inferring. On the
other hand, despite the decent performance, there still exists a gap between
academia and industry, due to the difference in data quantity and distribution.
Thus, a new large-scale benchmark for FAS is built up to further evaluate the
performance of algorithms in reality. Both qualitative and quantitative results
on existing and proposed benchmarks demonstrate the effectiveness of our
methods. The codes will be available at https://github.com/wangzhuo2019/SSAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeILF: Neural Incident Light Field for Physically-based Material Estimation. (arXiv:2203.07182v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07182">
<div class="article-summary-box-inner">
<span><p>We present a differentiable rendering framework for material and lighting
estimation from multi-view images and a reconstructed geometry. In the
framework, we represent scene lightings as the Neural Incident Light Field
(NeILF) and material properties as the surface BRDF modelled by multi-layer
perceptrons. Compared with recent approaches that approximate scene lightings
as the 2D environment map, NeILF is a fully 5D light field that is capable of
modelling illuminations of any static scenes. In addition, occlusions and
indirect lights can be handled naturally by the NeILF representation without
requiring multiple bounces of ray tracing, making it possible to estimate
material properties even for scenes with complex lightings and geometries. We
also propose a smoothness regularization and a Lambertian assumption to reduce
the material-lighting ambiguity during the optimization. Our method strictly
follows the physically-based rendering equation, and jointly optimizes material
and lighting through the differentiable rendering process. We have intensively
evaluated the proposed method on our in-house synthetic dataset, the DTU MVS
dataset, and real-world BlendedMVS scenes. Our method is able to outperform
previous methods by a significant margin in terms of novel view rendering
quality, setting a new state-of-the-art for image-based material and lighting
estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distribution-Aware Single-Stage Models for Multi-Person 3D Pose Estimation. (arXiv:2203.07697v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07697">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a novel Distribution-Aware Single-stage (DAS) model
for tackling the challenging multi-person 3D pose estimation problem. Different
from existing top-down and bottom-up methods, the proposed DAS model
simultaneously localizes person positions and their corresponding body joints
in the 3D camera space in a one-pass manner. This leads to a simplified
pipeline with enhanced efficiency. In addition, DAS learns the true
distribution of body joints for the regression of their positions, rather than
making a simple Laplacian or Gaussian assumption as previous works. This
provides valuable priors for model prediction and thus boosts the
regression-based scheme to achieve competitive performance with volumetric-base
ones. Moreover, DAS exploits a recursive update strategy for progressively
approaching to regression target, alleviating the optimization difficulty and
further lifting the regression performance. DAS is implemented with a fully
Convolutional Neural Network and end-to-end learnable. Comprehensive
experiments on benchmarks CMU Panoptic and MuPoTS-3D demonstrate the superior
efficiency of the proposed DAS model, specifically 1.5x speedup over previous
best model, and its stat-of-the-art accuracy for multi-person 3D pose
estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Hyperbolic Embeddings in 2D Object Detection. (arXiv:2203.08049v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08049">
<div class="article-summary-box-inner">
<span><p>Object detection, for the most part, has been formulated in the euclidean
space, where euclidean or spherical geodesic distances measure the similarity
of an image region to an object class prototype. In this work, we study whether
a hyperbolic geometry better matches the underlying structure of the object
classification space. We incorporate a hyperbolic classifier in two-stage,
keypoint-based, and transformer-based object detection architectures and
evaluate them on large-scale, long-tailed, and zero-shot object detection
benchmarks. In our extensive experimental evaluations, we observe categorical
class hierarchies emerging in the structure of the classification space,
resulting in lower classification errors and boosting the overall object
detection performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Adaptive Hand Keypoint and Pixel Localization in the Wild. (arXiv:2203.08344v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08344">
<div class="article-summary-box-inner">
<span><p>We aim to improve the performance of regressing hand keypoints and segmenting
pixel-level hand masks under new imaging conditions (e.g., outdoors) when we
only have labeled images taken under very different conditions (e.g., indoors).
In the real world, it is important that the model trained for both tasks works
under various imaging conditions. However, their variation covered by existing
labeled hand datasets is limited. Thus, it is necessary to adapt the model
trained on the labeled images (source) to unlabeled images (target) with unseen
imaging conditions. While self-training domain adaptation methods (i.e.,
learning from the unlabeled target images in a self-supervised manner) have
been developed for both tasks, their training may degrade performance when the
predictions on the target images are noisy. To avoid this, it is crucial to
assign a low importance (confidence) weight to the noisy predictions during
self-training. In this paper, we propose to utilize the divergence of two
predictions to estimate the confidence of the target image for both tasks.
These predictions are given from two separate networks, and their divergence
helps identify the noisy predictions. To integrate our proposed confidence
estimation into self-training, we propose a teacher-student framework where the
two networks (teachers) provide supervision to a network (student) for
self-training, and the teachers are learned from the student by knowledge
distillation. Our experiments show its superiority over state-of-the-art
methods in adaptation settings with different lighting, grasping objects,
backgrounds, and camera viewpoints. Our method improves by 4% the multi-task
score on HO3D compared to the latest adversarial adaptation method. We also
validate our method on Ego4D, egocentric videos with rapid changes in imaging
conditions outdoors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Many Data Samples is an Additional Instruction Worth?. (arXiv:2203.09161v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09161">
<div class="article-summary-box-inner">
<span><p>Recently introduced instruction-paradigm empowers non-expert users to
leverage NLP resources by defining a new task in natural language.
Instruction-tuned models have significantly outperformed multitask learning
models (without instruction); however they are far from state of the art task
specific models. Conventional approaches to improve model performance via
creating large datasets with lots of task instances or architectural/training
changes in model may not be feasible for non-expert users. However, they can
write alternate instructions to represent an instruction task. Is
Instruction-augumentation helpful? We augment a subset of tasks in the expanded
version of NATURAL INSTRUCTIONS with additional instructions and find that
these significantly improve model performance (up to 35%), especially in the
low-data regime. Our results indicate that an additional instruction can be
equivalent to ~200 data samples on average across tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Compression-Based Feature Learning for Video Restoration. (arXiv:2203.09208v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09208">
<div class="article-summary-box-inner">
<span><p>How to efficiently utilize the temporal features is crucial, yet challenging,
for video restoration. The temporal features usually contain various noisy and
uncorrelated information, and they may interfere with the restoration of the
current frame. This paper proposes learning noise-robust feature
representations to help video restoration. We are inspired by that the neural
codec is a natural denoiser. In neural codec, the noisy and uncorrelated
contents which are hard to predict but cost lots of bits are more inclined to
be discarded for bitrate saving. Therefore, we design a neural compression
module to filter the noise and keep the most useful information in features for
video restoration. To achieve robustness to noise, our compression module
adopts a spatial channel-wise quantization mechanism to adaptively determine
the quantization step size for each position in the latent. Experiments show
that our method can significantly boost the performance on video denoising,
where we obtain 0.13 dB improvement over BasicVSR++ with only 0.23x FLOPs.
Meanwhile, our method also obtains SOTA results on video deraining and
dehazing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interacting Attention Graph for Single Image Two-Hand Reconstruction. (arXiv:2203.09364v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09364">
<div class="article-summary-box-inner">
<span><p>Graph convolutional network (GCN) has achieved great success in single hand
reconstruction task, while interacting two-hand reconstruction by GCN remains
unexplored. In this paper, we present Interacting Attention Graph Hand
(IntagHand), the first graph convolution based network that reconstructs two
interacting hands from a single RGB image. To solve occlusion and interaction
challenges of two-hand reconstruction, we introduce two novel attention based
modules in each upsampling step of the original GCN. The first module is the
pyramid image feature attention (PIFA) module, which utilizes multiresolution
features to implicitly obtain vertex-to-image alignment. The second module is
the cross hand attention (CHA) module that encodes the coherence of interacting
hands by building dense cross-attention between two hand vertices. As a result,
our model outperforms all existing two-hand reconstruction methods by a large
margin on InterHand2.6M benchmark. Moreover, ablation studies verify the
effectiveness of both PIFA and CHA modules for improving the reconstruction
accuracy. Results on in-the-wild images and live video streams further
demonstrate the generalization ability of our network. Our code is available at
https://github.com/Dw1010/IntagHand.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Text Attention Network for Spatial Deformation Robust Scene Text Image Super-resolution. (arXiv:2203.09388v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09388">
<div class="article-summary-box-inner">
<span><p>Scene text image super-resolution aims to increase the resolution and
readability of the text in low-resolution images. Though significant
improvement has been achieved by deep convolutional neural networks (CNNs), it
remains difficult to reconstruct high-resolution images for spatially deformed
texts, especially rotated and curve-shaped ones. This is because the current
CNN-based methods adopt locality-based operations, which are not effective to
deal with the variation caused by deformations. In this paper, we propose a CNN
based Text ATTention network (TATT) to address this problem. The semantics of
the text are firstly extracted by a text recognition module as text prior
information. Then we design a novel transformer-based module, which leverages
global attention mechanism, to exert the semantic guidance of text prior to the
text reconstruction process. In addition, we propose a text structure
consistency loss to refine the visual appearance by imposing structural
consistency on the reconstructions of regular and deformed texts. Experiments
on the benchmark TextZoom dataset show that the proposed TATT not only achieves
state-of-the-art performance in terms of PSNR/SSIM metrics, but also
significantly improves the recognition accuracy in the downstream text
recognition task, particularly for text instances with multi-orientation and
curved shapes. Code is available at https://github.com/mjq11302010044/TATT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vox2Cortex: Fast Explicit Reconstruction of Cortical Surfaces from 3D MRI Scans with Geometric Deep Neural Networks. (arXiv:2203.09446v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09446">
<div class="article-summary-box-inner">
<span><p>The reconstruction of cortical surfaces from brain magnetic resonance imaging
(MRI) scans is essential for quantitative analyses of cortical thickness and
sulcal morphology. Although traditional and deep learning-based algorithmic
pipelines exist for this purpose, they have two major drawbacks: lengthy
runtimes of multiple hours (traditional) or intricate post-processing, such as
mesh extraction and topology correction (deep learning-based). In this work, we
address both of these issues and propose Vox2Cortex, a deep learning-based
algorithm that directly yields topologically correct, three-dimensional meshes
of the boundaries of the cortex. Vox2Cortex leverages convolutional and graph
convolutional neural networks to deform an initial template to the densely
folded geometry of the cortex represented by an input MRI scan. We show in
extensive experiments on three brain MRI datasets that our meshes are as
accurate as the ones reconstructed by state-of-the-art methods in the field,
without the need for time- and resource-intensive post-processing. To
accurately reconstruct the tightly folded cortex, we work with meshes
containing about 168,000 vertices at test time, scaling deep explicit
reconstruction methods to a new level.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transframer: Arbitrary Frame Prediction with Generative Models. (arXiv:2203.09494v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09494">
<div class="article-summary-box-inner">
<span><p>We present a general-purpose framework for image modelling and vision tasks
based on probabilistic frame prediction. Our approach unifies a broad range of
tasks, from image segmentation, to novel view synthesis and video
interpolation. We pair this framework with an architecture we term Transframer,
which uses U-Net and Transformer components to condition on annotated context
frames, and outputs sequences of sparse, compressed image features. Transframer
is the state-of-the-art on a variety of video generation benchmarks, is
competitive with the strongest models on few-shot view synthesis, and can
generate coherent 30 second videos from a single image without any explicit
geometric information. A single generalist Transframer simultaneously produces
promising results on 8 tasks, including semantic segmentation, image
classification and optical flow prediction with no task-specific architectural
components, demonstrating that multi-task computer vision can be tackled using
probabilistic image models. Our approach can in principle be applied to a wide
range of applications that require learning the conditional structure of
annotated image-formatted data.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-03-21 23:08:06.825628139 UTC">2022-03-21 23:08:06 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>