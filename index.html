<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-04-04T01:30:00Z">04-04</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Reproducibility Issues for BERT-based Evaluation Metrics. (arXiv:2204.00004v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00004">
<div class="article-summary-box-inner">
<span><p>Reproducibility is of utmost concern in machine learning and natural language
processing (NLP). In the field of natural language generation (especially
machine translation), the seminal paper of Post (2018) has pointed out problems
of reproducibility of the dominant metric, BLEU, at the time of publication.
Nowadays, BERT-based evaluation metrics considerably outperform BLEU. In this
paper, we ask whether results and claims from four recent BERT-based metrics
can be reproduced. We find that reproduction of claims and results often fails
because of (i) heavy undocumented preprocessing involved in the metrics, (ii)
missing code and (iii) reporting weaker results for the baseline metrics. (iv)
In one case, the problem stems from correlating not to human scores but to a
wrong column in the csv file, inflating scores by 5 points. Motivated by the
impact of preprocessing, we then conduct a second study where we examine its
effects more closely (for one of the metrics). We find that preprocessing can
have large effects, especially for highly inflectional languages. In this case,
the effect of preprocessing may be larger than the effect of the aggregation
mechanism (e.g., greedy alignment vs. Word Mover Distance).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data-augmented cross-lingual synthesis in a teacher-student framework. (arXiv:2204.00061v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00061">
<div class="article-summary-box-inner">
<span><p>Cross-lingual synthesis can be defined as the task of letting a speaker
generate fluent synthetic speech in another language. This is a challenging
task, and resulting speech can suffer from reduced naturalness, accented
speech, and/or loss of essential voice characteristics. Previous research shows
that many models appear to have insufficient generalization capabilities to
perform well on every of these cross-lingual aspects. To overcome these
generalization problems, we propose to apply the teacher-student paradigm to
cross-lingual synthesis. While a teacher model is commonly used to produce
teacher forced data, we propose to also use it to produce augmented data of
unseen speaker-language pairs, where the aim is to retain essential speaker
characteristics. Both sets of data are then used for student model training,
which is trained to retain the naturalness and prosodic variation present in
the teacher forced data, while learning the speaker identity from the augmented
data. Some modifications to the student model are proposed to make the
separation of teacher forced and augmented data more straightforward. Results
show that the proposed approach improves the retention of speaker
characteristics in the speech, while managing to retain high levels of
naturalness and prosodic variation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Filter-based Discriminative Autoencoders for Children Speech Recognition. (arXiv:2204.00164v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00164">
<div class="article-summary-box-inner">
<span><p>Children speech recognition is indispensable but challenging due to the
diversity of children's speech. In this paper, we propose a filter-based
discriminative autoencoder for acoustic modeling. To filter out the influence
of various speaker types and pitches, auxiliary information of the speaker and
pitch features is input into the encoder together with the acoustic features to
generate phonetic embeddings. In the training phase, the decoder uses the
auxiliary information and the phonetic embedding extracted by the encoder to
reconstruct the input acoustic features. The autoencoder is trained by
simultaneously minimizing the ASR loss and feature reconstruction error. The
framework can make the phonetic embedding purer, resulting in more accurate
senone (triphone-state) scores. Evaluated on the test set of the CMU Kids
corpus, our system achieves a 7.8% relative WER reduction compared to the
baseline system. In the domain adaptation experiment, our system also
outperforms the baseline system on the British-accent PF-STAR task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Making Pre-trained Language Models End-to-end Few-shot Learners with Contrastive Prompt Tuning. (arXiv:2204.00166v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00166">
<div class="article-summary-box-inner">
<span><p>Pre-trained Language Models (PLMs) have achieved remarkable performance for
various language understanding tasks in IR systems, which require the
fine-tuning process based on labeled training data. For low-resource scenarios,
prompt-based learning for PLMs exploits prompts as task guidance and turns
downstream tasks into masked language problems for effective few-shot
fine-tuning. In most existing approaches, the high performance of prompt-based
learning heavily relies on handcrafted prompts and verbalizers, which may limit
the application of such approaches in real-world scenarios. To solve this
issue, we present CP-Tuning, the first end-to-end Contrastive Prompt Tuning
framework for fine-tuning PLMs without any manual engineering of task-specific
prompts and verbalizers. It is integrated with the task-invariant continuous
prompt encoding technique with fully trainable prompt parameters. We further
propose the pair-wise cost-sensitive contrastive learning procedure to optimize
the model in order to achieve verbalizer-free class mapping and enhance the
task-invariance of prompts. It explicitly learns to distinguish different
classes and makes the decision boundary smoother by assigning different costs
to easy and hard cases. Experiments over a variety of language understanding
tasks used in IR systems and different PLMs show that CP-Tuning outperforms
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InterAug: Augmenting Noisy Intermediate Predictions for CTC-based ASR. (arXiv:2204.00174v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00174">
<div class="article-summary-box-inner">
<span><p>This paper proposes InterAug: a novel training method for CTC-based ASR using
augmented intermediate representations for conditioning. The proposed method
exploits the conditioning framework of self-conditioned CTC to train robust
models by conditioning with "noisy" intermediate predictions. During the
training, intermediate predictions are changed to incorrect intermediate
predictions, and fed into the next layer for conditioning. The subsequent
layers are trained to correct the incorrect intermediate predictions with the
intermediate losses. By repeating the augmentation and the correction,
iterative refinements, which generally require a special decoder, can be
realized only with the audio encoder. To produce noisy intermediate
predictions, we also introduce new augmentation: intermediate feature space
augmentation and intermediate token space augmentation that are designed to
simulate typical errors. The combination of the proposed InterAug framework
with new augmentation allows explicit training of the robust audio encoders. In
experiments using augmentations simulating deletion, insertion, and
substitution error, we confirmed that the trained model acquires robustness to
each error, boosting the speech recognition performance of the strong
self-conditioned CTC baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-sequence Intermediate Conditioning for CTC-based ASR. (arXiv:2204.00175v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00175">
<div class="article-summary-box-inner">
<span><p>End-to-end automatic speech recognition (ASR) directly maps input speech to a
character sequence without using pronunciation lexica. However, in languages
with thousands of characters, such as Japanese and Mandarin, modeling all these
characters is problematic due to data scarcity. To alleviate the problem, we
propose a multi-task learning model with explicit interaction between
characters and syllables by utilizing Self-conditioned connectionist temporal
classification (CTC) technique. While the original Self-conditioned CTC
estimates character-level intermediate predictions by applying auxiliary CTC
losses to a set of intermediate layers, the proposed method additionally
estimates syllable-level intermediate predictions in another set of
intermediate layers. The character-level and syllable-level predictions are
alternately used as conditioning features to deal with mutual dependency
between characters and syllables. Experimental results on Japanese and Mandarin
datasets show that the proposed multi-sequence intermediate conditioning
outperformed the conventional multi-task-based and Self-conditioned CTC-based
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Better Intermediates Improve CTC Inference. (arXiv:2204.00176v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00176">
<div class="article-summary-box-inner">
<span><p>This paper proposes a method for improved CTC inference with searched
intermediates and multi-pass conditioning. The paper first formulates
self-conditioned CTC as a probabilistic model with an intermediate prediction
as a latent representation and provides a tractable conditioning framework. We
then propose two new conditioning methods based on the new formulation: (1)
Searched intermediate conditioning that refines intermediate predictions with
beam-search, (2) Multi-pass conditioning that uses predictions of previous
inference for conditioning the next inference. These new approaches enable
better conditioning than the original self-conditioned CTC during inference and
improve the final performance. Experiments with the LibriSpeech dataset show
relative 3%/12% performance improvement at the maximum in test clean/other sets
compared to the original self-conditioned CTC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph Enhanced Contrastive Learning for Radiology Findings Summarization. (arXiv:2204.00203v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00203">
<div class="article-summary-box-inner">
<span><p>The impression section of a radiology report summarizes the most prominent
observation from the findings section and is the most important section for
radiologists to communicate to physicians. Summarizing findings is
time-consuming and can be prone to error for inexperienced radiologists, and
thus automatic impression generation has attracted substantial attention. With
the encoder-decoder framework, most previous studies explore incorporating
extra knowledge (e.g., static pre-defined clinical ontologies or extra
background information). Yet, they encode such knowledge by a separate encoder
to treat it as an extra input to their models, which is limited in leveraging
their relations with the original findings. To address the limitation, we
propose a unified framework for exploiting both extra knowledge and the
original findings in an integrated way so that the critical information (i.e.,
key words and their relations) can be extracted in an appropriate way to
facilitate impression generation. In detail, for each input findings, it is
encoded by a text encoder, and a graph is constructed through its entities and
dependency tree. Then, a graph encoder (e.g., graph neural networks (GNNs)) is
adopted to model relation information in the constructed graph. Finally, to
emphasize the key words in the findings, contrastive learning is introduced to
map positive samples (constructed by masking non-key words) closer and push
apart negative ones (constructed by masking key words). The experimental
results on OpenI and MIMIC-CXR confirm the effectiveness of our proposed
method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effect and Analysis of Large-scale Language Model Rescoring on Competitive ASR Systems. (arXiv:2204.00212v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00212">
<div class="article-summary-box-inner">
<span><p>Large-scale language models (LLMs) such as GPT-2, BERT and RoBERTa have been
successfully applied to ASR N-best rescoring. However, whether or how they can
benefit competitive, near state-of-the-art ASR systems remains unexplored. In
this study, we incorporate LLM rescoring into one of the most competitive ASR
baselines: the Conformer-Transducer model. We demonstrate that consistent
improvement is achieved by the LLM's bidirectionality, pretraining, in-domain
finetuning and context augmentation. Furthermore, our lexical analysis sheds
light on how each of these components may be contributing to the ASR
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Multi-speaker ASR with Independent Vector Analysis. (arXiv:2204.00218v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00218">
<div class="article-summary-box-inner">
<span><p>We develop an end-to-end system for multi-channel, multi-speaker automatic
speech recognition. We propose a frontend for joint source separation and
dereverberation based on the independent vector analysis (IVA) paradigm. It
uses the fast and stable iterative source steering algorithm together with a
neural source model. The parameters from the ASR module and the neural source
model are optimized jointly from the ASR loss itself. We demonstrate
competitive performance with previous systems using neural beamforming
frontends. First, we explore the trade-offs when using various number of
channels for training and testing. Second, we demonstrate that the proposed IVA
frontend performs well on noisy data, even when trained on clean mixtures only.
Furthermore, it extends without retraining to the separation of more speakers,
which is demonstrated on mixtures of three and four speakers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NC-DRE: Leveraging Non-entity Clue Information for Document-level Relation Extraction. (arXiv:2204.00255v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00255">
<div class="article-summary-box-inner">
<span><p>Document-level relation extraction (RE), which requires reasoning on multiple
entities in different sentences to identify complex inter-sentence relations,
is more challenging than sentence-level RE. To extract the complex
inter-sentence relations, previous studies usually employ graph neural networks
(GNN) to perform inference upon heterogeneous document-graphs. Despite their
great successes, these graph-based methods, which normally only consider the
words within the mentions in the process of building graphs and reasoning, tend
to ignore the non-entity clue words that are not in the mentions but provide
important clue information for relation reasoning. To alleviate this problem,
we treat graph-based document-level RE models as an encoder-decoder framework,
which typically uses a pre-trained language model as the encoder and a GNN
model as the decoder, and propose a novel graph-based model NC-DRE that
introduces decoder-to-encoder attention mechanism to leverage Non-entity Clue
information for Document-level Relation Extraction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multifaceted Improvements for Conversational Open-Domain Question Answering. (arXiv:2204.00266v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00266">
<div class="article-summary-box-inner">
<span><p>Open-domain question answering (OpenQA) is an important branch of textual QA
which discovers answers for the given questions based on a large number of
unstructured documents. Effectively mining correct answers from the open-domain
sources still has a fair way to go. Existing OpenQA systems might suffer from
the issues of question complexity and ambiguity, as well as insufficient
background knowledge. Recently, conversational OpenQA is proposed to address
these issues with the abundant contextual information in the conversation.
Promising as it might be, there exist several fundamental limitations including
the inaccurate question understanding, the coarse ranking for passage
selection, and the inconsistent usage of golden passage in the training and
inference phases. To alleviate these limitations, in this paper, we propose a
framework with Multifaceted Improvements for Conversational open-domain
Question Answering (MICQA). Specifically, MICQA has three significant
advantages. First, the proposed KL-divergence based regularization is able to
lead to a better question understanding for retrieval and answer reading.
Second, the added post-ranker module can push more relevant passages to the top
placements and be selected for reader with a two-aspect constrains. Third, the
well designed curriculum learning strategy effectively narrows the gap between
the golden passage settings of training and inference, and encourages the
reader to find true answer without the golden passage assistance. Extensive
experiments conducted on the publicly available dataset OR-QuAC demonstrate the
superiority of MICQA over the state-of-the-art model in conversational OpenQA
task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting Intervention Approval in Clinical Trials through Multi-Document Summarization. (arXiv:2204.00290v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00290">
<div class="article-summary-box-inner">
<span><p>Clinical trials offer a fundamental opportunity to discover new treatments
and advance the medical knowledge. However, the uncertainty of the outcome of a
trial can lead to unforeseen costs and setbacks. In this study, we propose a
new method to predict the effectiveness of an intervention in a clinical trial.
Our method relies on generating an informative summary from multiple documents
available in the literature about the intervention under study. Specifically,
our method first gathers all the abstracts of PubMed articles related to the
intervention. Then, an evidence sentence, which conveys information about the
effectiveness of the intervention, is extracted automatically from each
abstract. Based on the set of evidence sentences extracted from the abstracts,
a short summary about the intervention is constructed. Finally, the produced
summaries are used to train a BERT-based classifier, in order to infer the
effectiveness of an intervention. To evaluate our proposed method, we introduce
a new dataset which is a collection of clinical trials together with their
associated PubMed articles. Our experiments, demonstrate the effectiveness of
producing short informative summaries and using them to predict the
effectiveness of an intervention.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text-To-Speech Data Augmentation for Low Resource Speech Recognition. (arXiv:2204.00291v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00291">
<div class="article-summary-box-inner">
<span><p>Nowadays, the main problem of deep learning techniques used in the
development of automatic speech recognition (ASR) models is the lack of
transcribed data. The goal of this research is to propose a new data
augmentation method to improve ASR models for agglutinative and low-resource
languages. This novel data augmentation method generates both synthetic text
and synthetic audio. Some experiments were conducted using the corpus of the
Quechua language, which is an agglutinative and low-resource language. In this
study, a sequence-to-sequence (seq2seq) model was applied to generate synthetic
text, in addition to generating synthetic speech using a text-to-speech (TTS)
model for Quechua. The results show that the new data augmentation method works
well to improve the ASR model for Quechua. In this research, an 8.73%
improvement in the word-error-rate (WER) of the ASR model is obtained using a
combination of synthetic text and synthetic speech.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PriMock57: A Dataset Of Primary Care Mock Consultations. (arXiv:2204.00333v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00333">
<div class="article-summary-box-inner">
<span><p>Recent advances in Automatic Speech Recognition (ASR) have made it possible
to reliably produce automatic transcripts of clinician-patient conversations.
However, access to clinical datasets is heavily restricted due to patient
privacy, thus slowing down normal research practices. We detail the development
of a public access, high quality dataset comprising of57 mocked primary care
consultations, including audio recordings, their manual utterance-level
transcriptions, and the associated consultation notes. Our work illustrates how
the dataset can be used as a benchmark for conversational medical ASR as well
as consultation note generation from transcripts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cyberbullying detection across social media platforms via platform-aware adversarial encoding. (arXiv:2204.00334v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00334">
<div class="article-summary-box-inner">
<span><p>Despite the increasing interest in cyberbullying detection, existing efforts
have largely been limited to experiments on a single platform and their
generalisability across different social media platforms have received less
attention. We propose XP-CB, a novel cross-platform framework based on
Transformers and adversarial learning. XP-CB can enhance a Transformer
leveraging unlabelled data from the source and target platforms to come up with
a common representation while preventing platform-specific training. To
validate our proposed framework, we experiment on cyberbullying datasets from
three different platforms through six cross-platform configurations, showing
its effectiveness with both BERT and RoBERTa as the underlying Transformer
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WavFT: Acoustic model finetuning with labelled and unlabelled data. (arXiv:2204.00348v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00348">
<div class="article-summary-box-inner">
<span><p>Unsupervised and self-supervised learning methods have leveraged unlabelled
data to improve the pretrained models. However, these methods need
significantly large amount of unlabelled data and the computational cost of
training models with such large amount of data can be prohibitively high. We
address this issue by using unlabelled data during finetuning, instead of
pretraining. We propose acoustic model finetuning (FT) using labelled and
unlabelled data. The model is jointly trained to learn representations to
classify senones, as well as learn contextual acoustic representations. Our
training objective is a combination of cross entropy loss, suitable for
classification task, and contrastive loss, suitable to learn acoustic
representations. The proposed approach outperforms conventional finetuning with
11.2% and 9.19% word error rate relative (WERR) reduction on Gujarati and
Bengali languages respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Shallow Discourse Parsing in the PDTB-3: Handling Intra-sentential Implicits. (arXiv:2204.00350v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00350">
<div class="article-summary-box-inner">
<span><p>In the PDTB-3, several thousand implicit discourse relations were newly
annotated \textit{within} individual sentences, adding to the over 15,000
implicit relations annotated \textit{across} adjacent sentences in the PDTB-2.
Given that the position of the arguments to these \textit{intra-sentential
implicits} is no longer as well-defined as with \textit{inter-sentential
implicits}, a discourse parser must identify both their location and their
sense. That is the focus of the current work. The paper provides a
comprehensive analysis of our results, showcasing model performance under
different scenarios, pointing out limitations and noting future directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Biomedical Term Clustering by Learning Fine-grained Term Representations. (arXiv:2204.00391v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00391">
<div class="article-summary-box-inner">
<span><p>Term clustering is important in biomedical knowledge graph construction.
Using similarities between terms embedding is helpful for term clustering.
State-of-the-art term embeddings leverage pretrained language models to encode
terms, and use synonyms and relation knowledge from knowledge graphs to guide
contrastive learning. These embeddings provide close embeddings for terms
belonging to the same concept. However, from our probing experiments, these
embeddings are not sensitive to minor textual differences which leads to
failure for biomedical term clustering. To alleviate this problem, we adjust
the sampling strategy in pretraining term embeddings by providing dynamic hard
positive and negative samples during contrastive learning to learn fine-grained
representations which result in better biomedical term clustering. We name our
proposed method as CODER++, and it has been applied in clustering biomedical
concepts in the newly released Biomedical Knowledge Graph named BIOS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing Speech Emotion Recognition Transformers for Linguistic Knowledge. (arXiv:2204.00400v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00400">
<div class="article-summary-box-inner">
<span><p>Large, pre-trained neural networks consisting of self-attention layers
(transformers) have recently achieved state-of-the-art results on several
speech emotion recognition (SER) datasets. These models are typically
pre-trained in self-supervised manner with the goal to improve automatic speech
recognition performance -- and thus, to understand linguistic information. In
this work, we investigate the extent in which this information is exploited
during SER fine-tuning. Using a reproducible methodology based on open-source
tools, we synthesise prosodically neutral speech utterances while varying the
sentiment of the text. Valence predictions of the transformer model are very
reactive to positive and negative sentiment content, as well as negations, but
not to intensifiers or reducers, while none of those linguistic features impact
arousal or dominance. These findings show that transformers can successfully
leverage linguistic information to improve their valence predictions, and that
linguistic analysis should be included in their testing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structured Pruning Learns Compact and Accurate Models. (arXiv:2204.00408v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00408">
<div class="article-summary-box-inner">
<span><p>The growing size of neural language models has led to increased attention in
model compression. The two predominant approaches are pruning, which gradually
removes weights from a pre-trained model, and distillation, which trains a
smaller compact model to match a larger one. Pruning methods can significantly
reduce the model size but hardly achieve large speedups as distillation.
However, distillation methods require large amounts of unlabeled data and are
expensive to train. In this work, we propose a task-specific structured pruning
method CoFi (Coarse- and Fine-grained Pruning), which delivers highly
parallelizable subnetworks and matches the distillation methods in both
accuracy and latency, without resorting to any unlabeled data. Our key insight
is to jointly prune coarse-grained (e.g., layers) and fine-grained (e.g., heads
and hidden units) modules, which controls the pruning decision of each
parameter with masks of different granularity. We also devise a layerwise
distillation strategy to transfer knowledge from unpruned to pruned models
during optimization. Our experiments on GLUE and SQuAD datasets show that CoFi
yields models with over 10x speedups with a small accuracy drop, showing its
effectiveness and efficiency compared to previous pruning and distillation
approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sense disambiguation of compound constituents. (arXiv:2204.00429v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00429">
<div class="article-summary-box-inner">
<span><p>In distributional semantic accounts of the meaning of noun-noun compounds
(e.g. starfish, bank account, houseboat) the important role of constituent
polysemy remains largely unaddressed(cf. the meaning of star in starfish vs.
star cluster vs. star athlete). Instead of semantic vectors that average over
the different meanings of a constituent, disambiguated vectors of the
constituents would be needed in order to see what these more specific
constituent meanings contribute to the meaning of the compound as a whole. This
paper presents a novel approach to this specific problem of word sense
disambiguation: set expansion. We build on the approach developed by Mahabal et
al. (2018) which was originally designed to solve the analogy problem. We
modified their method in such a way that it can address the problem of sense
disambiguation of compound constituents. The results of experiments with a data
set of almost 9000 compounds (LADEC, Gagn\'e et al. 2019) suggest that this
approach is successful, yet the success is sensitive to the frequency with
which the compounds are attested.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdaSpeech 4: Adaptive Text to Speech in Zero-Shot Scenarios. (arXiv:2204.00436v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00436">
<div class="article-summary-box-inner">
<span><p>Adaptive text to speech (TTS) can synthesize new voices in zero-shot
scenarios efficiently, by using a well-trained source TTS model without
adapting it on the speech data of new speakers. Considering seen and unseen
speakers have diverse characteristics, zero-shot adaptive TTS requires strong
generalization ability on speaker characteristics, which brings modeling
challenges. In this paper, we develop AdaSpeech 4, a zero-shot adaptive TTS
system for high-quality speech synthesis. We model the speaker characteristics
systematically to improve the generalization on new speakers. Generally, the
modeling of speaker characteristics can be categorized into three steps:
extracting speaker representation, taking this speaker representation as
condition, and synthesizing speech/mel-spectrogram given this speaker
representation. Accordingly, we improve the modeling in three steps: 1) To
extract speaker representation with better generalization, we factorize the
speaker characteristics into basis vectors and extract speaker representation
by weighted combining of these basis vectors through attention. 2) We leverage
conditional layer normalization to integrate the extracted speaker
representation to TTS model. 3) We propose a novel supervision loss based on
the distribution of basis vectors to maintain the corresponding speaker
characteristics in generated mel-spectrograms. Without any fine-tuning,
AdaSpeech 4 achieves better voice quality and similarity than baselines in
multiple datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human Evaluation and Correlation with Automatic Metrics in Consultation Note Generation. (arXiv:2204.00447v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00447">
<div class="article-summary-box-inner">
<span><p>In recent years, machine learning models have rapidly become better at
generating clinical consultation notes; yet, there is little work on how to
properly evaluate the generated consultation notes to understand the impact
they may have on both the clinician using them and the patient's clinical
safety. To address this we present an extensive human evaluation study of
consultation notes where 5 clinicians (i) listen to 57 mock consultations, (ii)
write their own notes, (iii) post-edit a number of automatically generated
notes, and (iv) extract all the errors, both quantitative and qualitative. We
then carry out a correlation study with 18 automatic quality metrics and the
human judgements. We find that a simple, character-based Levenshtein distance
metric performs on par if not better than common model-based metrics like
BertScore. All our findings and annotations are open-sourced.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Cross-lingual Aphasia Detection using Automatic Speech Recognition. (arXiv:2204.00448v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00448">
<div class="article-summary-box-inner">
<span><p>Aphasia is a common speech and language disorder, typically caused by a brain
injury or a stroke, that affects millions of people worldwide. Detecting and
assessing Aphasia in patients is a difficult, time-consuming process, and
numerous attempts to automate it have been made, the most successful using
machine learning models trained on aphasic speech data. Like in many medical
applications, aphasic speech data is scarce and the problem is exacerbated in
so-called "low resource" languages, which are, for this task, most languages
excluding English. We attempt to leverage available data in English and achieve
zero-shot aphasia detection in low-resource languages such as Greek and French,
by using language-agnostic linguistic features. Current cross-lingual aphasia
detection approaches rely on manually extracted transcripts. We propose an
end-to-end pipeline using pre-trained Automatic Speech Recognition (ASR) models
that share cross-lingual speech representations and are fine-tuned for our
desired low-resource languages. To further boost our ASR model's performance,
we also combine it with a language model. We show that our ASR-based end-to-end
pipeline offers comparable results to previous setups using human-annotated
transcripts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluation of Fake News Detection with Knowledge-Enhanced Language Models. (arXiv:2204.00458v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00458">
<div class="article-summary-box-inner">
<span><p>Recent advances in fake news detection have exploited the success of
large-scale pre-trained language models (PLMs). The predominant
state-of-the-art approaches are based on fine-tuning PLMs on labelled fake news
datasets. However, large-scale PLMs are generally not trained on structured
factual data and hence may not possess priors that are grounded in factually
accurate knowledge. The use of existing knowledge bases (KBs) with rich
human-curated factual information has thus the potential to make fake news
detection more effective and robust. In this paper, we investigate the impact
of knowledge integration into PLMs for fake news detection. We study several
state-of-the-art approaches for knowledge integration, mostly using Wikidata as
KB, on two popular fake news datasets - LIAR, a politics-based dataset, and
COVID-19, a dataset of messages posted on social media relating to the COVID-19
pandemic. Our experiments show that knowledge-enhanced models can significantly
improve fake news detection on LIAR where the KB is relevant and up-to-date.
The mixed results on COVID-19 highlight the reliance on stylistic features and
the importance of domain specific and current KBs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty Determines the Adequacy of the Mode and the Tractability of Decoding in Sequence-to-Sequence Models. (arXiv:2204.00471v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00471">
<div class="article-summary-box-inner">
<span><p>In many natural language processing (NLP) tasks the same input (e.g. source
sentence) can have multiple possible outputs (e.g. translations). To analyze
how this ambiguity (also known as intrinsic uncertainty) shapes the
distribution learned by neural sequence models we measure sentence-level
uncertainty by computing the degree of overlap between references in
multi-reference test sets from two different NLP tasks: machine translation
(MT) and grammatical error correction (GEC). At both the sentence- and the
task-level, intrinsic uncertainty has major implications for various aspects of
search such as the inductive biases in beam search and the complexity of exact
search. In particular, we show that well-known pathologies such as a high
number of beam search errors, the inadequacy of the mode, and the drop in
system performance with large beam sizes apply to tasks with high level of
ambiguity such as MT but not to less uncertain tasks such as GEC. Furthermore,
we propose a novel exact $n$-best search algorithm for neural sequence models,
and show that intrinsic uncertainty affects model uncertainty as the model
tends to overly spread out the probability mass for uncertain tasks and
sentences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating the Text-to-SQL Capabilities of Large Language Models. (arXiv:2204.00498v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00498">
<div class="article-summary-box-inner">
<span><p>We perform an empirical evaluation of Text-to-SQL capabilities of the Codex
language model. We find that, without any finetuning, Codex is a strong
baseline on the Spider benchmark; we also analyze the failure modes of Codex in
this setting. Furthermore, we demonstrate on the GeoQuery and Scholar
benchmarks that a small number of in-domain examples provided in the prompt
enables Codex to perform better than state-of-the-art models finetuned on such
few-shot examples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Disentangled Representations of Negation and Uncertainty. (arXiv:2204.00511v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00511">
<div class="article-summary-box-inner">
<span><p>Negation and uncertainty modeling are long-standing tasks in natural language
processing. Linguistic theory postulates that expressions of negation and
uncertainty are semantically independent from each other and the content they
modify. However, previous works on representation learning do not explicitly
model this independence. We therefore attempt to disentangle the
representations of negation, uncertainty, and content using a Variational
Autoencoder. We find that simply supervising the latent representations results
in good disentanglement, but auxiliary objectives based on adversarial learning
and mutual information minimization can provide additional disentanglement
gains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Integration of Speech Recognition, Speech Enhancement, and Self-Supervised Learning Representation. (arXiv:2204.00540v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00540">
<div class="article-summary-box-inner">
<span><p>This work presents our end-to-end (E2E) automatic speech recognition (ASR)
model targetting at robust speech recognition, called Integraded speech
Recognition with enhanced speech Input for Self-supervised learning
representation (IRIS). Compared with conventional E2E ASR models, the proposed
E2E model integrates two important modules including a speech enhancement (SE)
module and a self-supervised learning representation (SSLR) module. The SE
module enhances the noisy speech. Then the SSLR module extracts features from
enhanced speech to be used for speech recognition (ASR). To train the proposed
model, we establish an efficient learning scheme. Evaluation results on the
monaural CHiME-4 task show that the IRIS model achieves the best performance
reported in the literature for the single-channel CHiME-4 benchmark (2.0% for
the real development and 3.9% for the real test) thanks to the powerful
pre-trained SSLR module and the fine-tuned SE module.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Multimodal Approach for Studying the Dynamics of Curiosity in Small Group Learning. (arXiv:2204.00545v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00545">
<div class="article-summary-box-inner">
<span><p>Curiosity is a vital metacognitive skill in educational contexts, leading to
creativity, and a love of learning. And while many school systems increasingly
undercut curiosity by teaching to the test, teachers are increasingly
interested in how to evoke curiosity in their students to prepare them for a
world in which lifelong learning and reskilling will be more and more
important. One aspect of curiosity that has received little attention, however,
is the role of peers in eliciting curiosity. We present what we believe to be
the first theoretical framework that articulates an integrated socio-cognitive
account of curiosity that ties observable behaviors in peers to underlying
curiosity states. We make a bipartite distinction between individual and
interpersonal functions that contribute to curiosity, and multimodal behaviors
that fulfill these functions. We validate the proposed framework by leveraging
a longitudinal latent variable modeling approach. Findings confirm a positive
predictive relationship between the latent variables of individual and
interpersonal functions and curiosity, with the interpersonal functions
exercising a comparatively stronger influence. Prominent behavioral
realizations of these functions are also discovered in a data-driven manner. We
instantiate the proposed theoretical framework in a set of strategies and
tactics that can be incorporated into learning technologies to indicate, evoke,
and scaffold curiosity. This work is a step towards designing learning
technologies that can recognize and evoke moment-by-moment curiosity during
learning in social contexts and towards a more complete multimodal learning
analytics. The underlying rationale is applicable more generally for developing
computer support for other metacognitive and socio-emotional skills.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified and Effective Ensemble Knowledge Distillation. (arXiv:2204.00548v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00548">
<div class="article-summary-box-inner">
<span><p>Ensemble knowledge distillation can extract knowledge from multiple teacher
models and encode it into a single student model. Many existing methods learn
and distill the student model on labeled data only. However, the teacher models
are usually learned on the same labeled data, and their predictions have high
correlations with groudtruth labels. Thus, they cannot provide sufficient
knowledge complementary to task labels for student teaching. Distilling on
unseen unlabeled data has the potential to enhance the knowledge transfer from
the teachers to the student. In this paper, we propose a unified and effective
ensemble knowledge distillation method that distills a single student model
from an ensemble of teacher models on both labeled and unlabeled data. Since
different teachers may have diverse prediction correctness on the same sample,
on labeled data we weight the predictions of different teachers according to
their correctness. In addition, we weight the distillation loss based on the
overall prediction correctness of the teacher ensemble to distill high-quality
knowledge. On unlabeled data, there is no groundtruth to evaluate prediction
correctness. Fortunately, the disagreement among teachers is an indication of
sample hardness, and thereby we weight the distillation loss based on teachers'
disagreement to emphasize knowledge distillation on important samples.
Extensive experiments on four datasets show the effectiveness of our proposed
ensemble distillation method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nowruz at SemEval-2022 Task 7: Tackling Cloze Tests with Transformers and Ordinal Regression. (arXiv:2204.00556v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00556">
<div class="article-summary-box-inner">
<span><p>This paper outlines the system using which team Nowruz participated in
SemEval 2022 Task 7 Identifying Plausible Clarifications of Implicit and
Underspecified Phrases for both subtasks A and B. Using a pre-trained
transformer as a backbone, the model targeted the task of multi-task
classification and ranking in the context of finding the best fillers for a
cloze task related to instructional texts on the website Wikihow.
</p>
<p>The system employed a combination of two ordinal regression components to
tackle this task in a multi-task learning scenario. According to the official
leaderboard of the shared task, this system was ranked 5th in the ranking and
7th in the classification subtasks out of 21 participating teams. With
additional experiments, the models have since been further optimised.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-task RNN-T with Semantic Decoder for Streamable Spoken Language Understanding. (arXiv:2204.00558v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00558">
<div class="article-summary-box-inner">
<span><p>End-to-end Spoken Language Understanding (E2E SLU) has attracted increasing
interest due to its advantages of joint optimization and low latency when
compared to traditionally cascaded pipelines. Existing E2E SLU models usually
follow a two-stage configuration where an Automatic Speech Recognition (ASR)
network first predicts a transcript which is then passed to a Natural Language
Understanding (NLU) module through an interface to infer semantic labels, such
as intent and slot tags. This design, however, does not consider the NLU
posterior while making transcript predictions, nor correct the NLU prediction
error immediately by considering the previously predicted word-pieces. In
addition, the NLU model in the two-stage system is not streamable, as it must
wait for the audio segments to complete processing, which ultimately impacts
the latency of the SLU system. In this work, we propose a streamable multi-task
semantic transducer model to address these considerations. Our proposed
architecture predicts ASR and NLU labels auto-regressively and uses a semantic
decoder to ingest both previously predicted word-pieces and slot tags while
aggregating them through a fusion network. Using an industry scale SLU and a
public FSC dataset, we show the proposed model outperforms the two-stage E2E
SLU model for both ASR and NLU metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language. (arXiv:2204.00598v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00598">
<div class="article-summary-box-inner">
<span><p>Large foundation models can exhibit unique capabilities depending on the
domain of data they are trained on. While these domains are generic, they may
only barely overlap. For example, visual-language models (VLMs) are trained on
Internet-scale image captions, but large language models (LMs) are further
trained on Internet-scale text with no images (e.g. from spreadsheets, to SAT
questions). As a result, these models store different forms of commonsense
knowledge across different domains. In this work, we show that this model
diversity is symbiotic, and can be leveraged to build AI systems with
structured Socratic dialogue -- in which new multimodal tasks are formulated as
a guided language-based exchange between different pre-existing foundation
models, without additional finetuning. In the context of egocentric perception,
we present a case study of Socratic Models (SMs) that can provide meaningful
results for complex tasks such as generating free-form answers to contextual
questions about egocentric video, by formulating video Q&amp;A as short story Q&amp;A,
i.e. summarizing the video into a short story, then answering questions about
it. Additionally, SMs can generate captions for Internet images, and are
competitive with state-of-the-art on zero-shot video-to-text retrieval with
42.8 R@1 on MSR-VTT 1k-A. SMs demonstrate how to compose foundation models
zero-shot to capture new multimodal functionalities, without domain-specific
data collection. Prototypes are available at socraticmodels.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Globetrotter: Connecting Languages by Connecting Images. (arXiv:2012.04631v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.04631">
<div class="article-summary-box-inner">
<span><p>Machine translation between many languages at once is highly challenging,
since training with ground truth requires supervision between all language
pairs, which is difficult to obtain. Our key insight is that, while languages
may vary drastically, the underlying visual appearance of the world remains
consistent. We introduce a method that uses visual observations to bridge the
gap between languages, rather than relying on parallel corpora or topological
properties of the representations. We train a model that aligns segments of
text from different languages if and only if the images associated with them
are similar and each image in turn is well-aligned with its textual
description. We train our model from scratch on a new dataset of text in over
fifty languages with accompanying images. Experiments show that our method
outperforms previous work on unsupervised word and sentence translation using
retrieval. Code, models and data are available on globetrotter.cs.columbia.edu.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Sensitivity and Stability of Model Interpretations in NLP. (arXiv:2104.08782v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08782">
<div class="article-summary-box-inner">
<span><p>Recent years have witnessed the emergence of a variety of post-hoc
interpretations that aim to uncover how natural language processing (NLP)
models make predictions. Despite the surge of new interpretation methods, it
remains an open problem how to define and quantitatively measure the
faithfulness of interpretations, i.e., to what extent interpretations reflect
the reasoning process by a model. We propose two new criteria, sensitivity and
stability, that provide complementary notions of faithfulness to the existed
removal-based criteria. Our results show that the conclusion for how faithful
interpretations are could vary substantially based on different notions.
Motivated by the desiderata of sensitivity and stability, we introduce a new
class of interpretation methods that adopt techniques from adversarial
robustness. Empirical results show that our proposed methods are effective
under the new criteria and overcome limitations of gradient-based methods on
removal-based criteria. Besides text classification, we also apply
interpretation methods and metrics to dependency parsing. Our results shed
light on understanding the diverse set of interpretations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A comparative evaluation and analysis of three generations of Distributional Semantic Models. (arXiv:2105.09825v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.09825">
<div class="article-summary-box-inner">
<span><p>Distributional semantics has deeply changed in the last decades. First,
predict models stole the thunder from traditional count ones, and more recently
both of them were replaced in many NLP applications by contextualized vectors
produced by Transformer neural language models. Although an extensive body of
research has been devoted to Distributional Semantic Model (DSM) evaluation, we
still lack a thorough comparison with respect to tested models, semantic tasks,
and benchmark datasets. Moreover, previous work has mostly focused on
task-driven evaluation, instead of exploring the differences between the way
models represent the lexical semantic space. In this paper, we perform a
comprehensive evaluation of type distributional vectors, either produced by
static DSMs or obtained by averaging the contextualized vectors generated by
BERT. First of all, we investigate the performance of embeddings in several
semantic tasks, carrying out an in-depth statistical analysis to identify the
major factors influencing the behavior of DSMs. The results show that i.) the
alleged superiority of predict based models is more apparent than real, and
surely not ubiquitous and ii.) static DSMs surpass contextualized
representations in most out-of-context semantic tasks and datasets.
Furthermore, we borrow from cognitive neuroscience the methodology of
Representational Similarity Analysis (RSA) to inspect the semantic spaces
generated by distributional models. RSA reveals important differences related
to the frequency and part-of-speech of lexical items.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LMMS Reloaded: Transformer-based Sense Embeddings for Disambiguation and Beyond. (arXiv:2105.12449v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.12449">
<div class="article-summary-box-inner">
<span><p>Distributional semantics based on neural approaches is a cornerstone of
Natural Language Processing, with surprising connections to human meaning
representation as well. Recent Transformer-based Language Models have proven
capable of producing contextual word representations that reliably convey
sense-specific information, simply as a product of self-supervision. Prior work
has shown that these contextual representations can be used to accurately
represent large sense inventories as sense embeddings, to the extent that a
distance-based solution to Word Sense Disambiguation (WSD) tasks outperforms
models trained specifically for the task. Still, there remains much to
understand on how to use these Neural Language Models (NLMs) to produce sense
embeddings that can better harness each NLM's meaning representation abilities.
In this work we introduce a more principled approach to leverage information
from all layers of NLMs, informed by a probing analysis on 14 NLM variants. We
also emphasize the versatility of these sense embeddings in contrast to
task-specific models, applying them on several sense-related tasks, besides
WSD, while demonstrating improved performance using our proposed approach over
prior work focused on sense embeddings. Finally, we discuss unexpected findings
regarding layer and model performance variations, and potential applications
for downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spanish Language Models. (arXiv:2107.07253v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.07253">
<div class="article-summary-box-inner">
<span><p>This paper presents the Spanish RoBERTa-base and RoBERTa-large models, as
well as the corresponding performance evaluations. Both models were pre-trained
using the largest Spanish corpus known to date, with a total of 570GB of clean
and deduplicated text processed for this work, compiled from the web crawlings
performed by the National Library of Spain from 2009 to 2019. We extended the
current evaluation datasets with an extractive Question Answering dataset and
our models outperform the existing Spanish models across tasks and settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GlobalWoZ: Globalizing MultiWoZ to Develop Multilingual Task-Oriented Dialogue Systems. (arXiv:2110.07679v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07679">
<div class="article-summary-box-inner">
<span><p>Much recent progress in task-oriented dialogue (ToD) systems has been driven
by available annotation data across multiple domains for training. Over the
last few years, there has been a move towards data curation for multilingual
ToD systems that are applicable to serve people speaking different languages.
However, existing multilingual ToD datasets either have a limited coverage of
languages due to the high cost of data curation, or ignore the fact that
dialogue entities barely exist in countries speaking these languages. To tackle
these limitations, we introduce a novel data curation method that generates
GlobalWoZ -- a large-scale multilingual ToD dataset globalized from an English
ToD dataset for three unexplored use cases. Our method is based on translating
dialogue templates and filling them with local entities in the target-language
countries. We release our dataset as well as a set of strong baselines to
encourage research on learning multilingual ToD systems for real use cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Power of Prompt Tuning for Low-Resource Semantic Parsing. (arXiv:2110.08525v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08525">
<div class="article-summary-box-inner">
<span><p>Prompt tuning has recently emerged as an effective method for adapting
pre-trained language models to a number of language understanding and
generation tasks. In this paper, we investigate prompt tuning for semantic
parsing -- the task of mapping natural language utterances onto formal meaning
representations. On the low-resource splits of Overnight and TOPv2, we find
that a prompt tuned T5-xl significantly outperforms its fine-tuned counterpart,
as well as strong GPT-3 and BART baselines. We also conduct ablation studies
across different model scales and target representations, finding that, with
increasing model scale, prompt tuned T5 models improve at generating target
representations that are far from the pre-training distribution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Paperswithtopic: Topic Identification from Paper Title Only. (arXiv:2110.15721v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15721">
<div class="article-summary-box-inner">
<span><p>The deep learning field is growing rapidly as witnessed by the exponential
growth of papers submitted to journals, conferences, and pre-print servers. To
cope with the sheer number of papers, several text mining tools from natural
language processing (NLP) have been proposed that enable researchers to keep
track of recent findings. In this context, our paper makes two main
contributions: first, we collected and annotated a dataset of papers paired by
title and sub-field from the field of artificial intelligence (AI), and,
second, we present results on how to predict a paper's AI sub-field from a
given paper title only. Importantly, for the latter, short-text classification
task we compare several algorithms from conventional machine learning all the
way up to recent, larger transformer architectures. Finally, for the
transformer models, we also present gradient-based, attention visualizations to
further explain the model's classification process. All code can be found at
\url{https://github.com/1pha/paperswithtopic}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Less is More: Generating Grounded Navigation Instructions from Landmarks. (arXiv:2111.12872v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12872">
<div class="article-summary-box-inner">
<span><p>We study the automatic generation of navigation instructions from 360-degree
images captured on indoor routes. Existing generators suffer from poor visual
grounding, causing them to rely on language priors and hallucinate objects. Our
MARKY-MT5 system addresses this by focusing on visual landmarks; it comprises a
first stage landmark detector and a second stage generator -- a multimodal,
multilingual, multitask encoder-decoder. To train it, we bootstrap grounded
landmark annotations on top of the Room-across-Room (RxR) dataset. Using text
parsers, weak supervision from RxR's pose traces, and a multilingual image-text
encoder trained on 1.8b images, we identify 1.1m English, Hindi and Telugu
landmark descriptions and ground them to specific regions in panoramas. On
Room-to-Room, human wayfinders obtain success rates (SR) of 71% following
MARKY-MT5's instructions, just shy of their 75% SR following human instructions
-- and well above SRs with other generators. Evaluations on RxR's longer,
diverse paths obtain 61-64% SRs on three languages. Generating such
high-quality navigation instructions in novel environments is a step towards
conversational navigation tools and could facilitate larger-scale training of
instruction-following agents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">General Facial Representation Learning in a Visual-Linguistic Manner. (arXiv:2112.03109v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03109">
<div class="article-summary-box-inner">
<span><p>How to learn a universal facial representation that boosts all face analysis
tasks? This paper takes one step toward this goal. In this paper, we study the
transfer performance of pre-trained models on face analysis tasks and introduce
a framework, called FaRL, for general Facial Representation Learning in a
visual-linguistic manner. On one hand, the framework involves a contrastive
loss to learn high-level semantic meaning from image-text pairs. On the other
hand, we propose exploring low-level information simultaneously to further
enhance the face representation, by adding a masked image modeling. We perform
pre-training on LAION-FACE, a dataset containing large amount of face
image-text pairs, and evaluate the representation capability on multiple
downstream tasks. We show that FaRL achieves better transfer performance
compared with previous pre-trained models. We also verify its superiority in
the low-data regime. More importantly, our model surpasses the state-of-the-art
methods on face analysis tasks including face parsing and face alignment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pair-Level Supervised Contrastive Learning for Natural Language Inference. (arXiv:2201.10927v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10927">
<div class="article-summary-box-inner">
<span><p>Natural language inference (NLI) is an increasingly important task for
natural language understanding, which requires one to infer the relationship
between the sentence pair (premise and hypothesis). Many recent works have used
contrastive learning by incorporating the relationship of the sentence pair
from NLI datasets to learn sentence representation. However, these methods only
focus on comparisons with sentence-level representations. In this paper, we
propose a Pair-level Supervised Contrastive Learning approach (PairSCL). We
adopt a cross attention module to learn the joint representations of the
sentence pairs. A contrastive learning objective is designed to distinguish the
varied classes of sentence pairs by pulling those in one class together and
pushing apart the pairs in other classes. We evaluate PairSCL on two public
datasets of NLI where the accuracy of PairSCL outperforms other methods by 2.1%
on average. Furthermore, our method outperforms the previous state-of-the-art
method on seven transfer tasks of text classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TimeLMs: Diachronic Language Models from Twitter. (arXiv:2202.03829v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03829">
<div class="article-summary-box-inner">
<span><p>Despite its importance, the time variable has been largely neglected in the
NLP and language model literature. In this paper, we present TimeLMs, a set of
language models specialized on diachronic Twitter data. We show that a
continual learning strategy contributes to enhancing Twitter-based language
models' capacity to deal with future and out-of-distribution tweets, while
making them competitive with standardized and more monolithic benchmarks. We
also perform a number of qualitative analyses showing how they cope with trends
and peaks in activity involving specific named entities or concept drift.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quality Controlled Paraphrase Generation. (arXiv:2203.10940v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10940">
<div class="article-summary-box-inner">
<span><p>Paraphrase generation has been widely used in various downstream tasks. Most
tasks benefit mainly from high quality paraphrases, namely those that are
semantically similar to, yet linguistically diverse from, the original
sentence. Generating high-quality paraphrases is challenging as it becomes
increasingly hard to preserve meaning as linguistic diversity increases. Recent
works achieve nice results by controlling specific aspects of the paraphrase,
such as its syntactic tree. However, they do not allow to directly control the
quality of the generated paraphrase, and suffer from low flexibility and
scalability. Here we propose $QCPG$, a quality-guided controlled paraphrase
generation model, that allows directly controlling the quality dimensions.
Furthermore, we suggest a method that given a sentence, identifies points in
the quality control space that are expected to yield optimal generated
paraphrases. We show that our method is able to generate paraphrases which
maintain the original meaning while achieving higher diversity than the
uncontrolled baseline. The models, the code, and the data can be found in
https://github.com/IBM/quality-controlled-paraphrase-generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VLSP 2021 -- ViMRC Challenge: Vietnamese Machine Reading Comprehension. (arXiv:2203.11400v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11400">
<div class="article-summary-box-inner">
<span><p>One of the emerging research trends in natural language understanding is
machine reading comprehension (MRC) which is the task to find answers to human
questions based on textual data. Existing Vietnamese datasets for MRC research
concentrate solely on answerable questions. However, in reality, questions can
be unanswerable for which the correct answer is not stated in the given textual
data. To address the weakness, we provide the research community with a
benchmark dataset named UIT-ViQuAD 2.0 for evaluating the MRC task and question
answering systems for the Vietnamese language. We use UIT-ViQuAD 2.0 as a
benchmark dataset for the challenge on Vietnamese MRC at the Eighth Workshop on
Vietnamese Language and Speech Processing (VLSP 2021). This task attracted 77
participant teams from 34 universities and other organizations. In this
article, we present details of the organization of the challenge, an overview
of the methods employed by shared-task participants, and the results. The
highest performances are 77.24% in F1-score and 67.43% in Exact Match on the
private test set. The Vietnamese MRC systems proposed by the top 3 teams use
XLM-RoBERTa, a powerful pre-trained language model based on the transformer
architecture. The UIT-ViQuAD 2.0 dataset motivates researchers to further
explore the Vietnamese machine reading comprehension task and related tasks
such as question answering, question generation, and natural language
inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LDKP: A Dataset for Identifying Keyphrases from Long Scientific Documents. (arXiv:2203.15349v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15349">
<div class="article-summary-box-inner">
<span><p>Identifying keyphrases (KPs) from text documents is a fundamental task in
natural language processing and information retrieval. Vast majority of the
benchmark datasets for this task are from the scientific domain containing only
the document title and abstract information. This limits keyphrase extraction
(KPE) and keyphrase generation (KPG) algorithms to identify keyphrases from
human-written summaries that are often very short (approx 8 sentences). This
presents three challenges for real-world applications: human-written summaries
are unavailable for most documents, the documents are almost always long, and a
high percentage of KPs are directly found beyond the limited context of title
and abstract. Therefore, we release two extensive corpora mapping KPs of ~1.3M
and ~100K scientific articles with their fully extracted text and additional
metadata including publication venue, year, author, field of study, and
citations for facilitating research on this real-world problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Overview of Indian Language Datasets used for Text Summarization. (arXiv:2203.16127v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16127">
<div class="article-summary-box-inner">
<span><p>In this paper, we survey Text Summarization (TS) datasets in Indian Languages
(ILs), which are also low-resource languages (LRLs). We seek to answer one
primary question: is the pool of Indian Language Text Summarization (ILTS)
dataset growing or is there a resource poverty? To an-swer the primary
question, we pose two sub-questions that we seek about ILTS datasets: first,
what characteristics: format and domain do ILTS datasets have? Second, how
different are those characteristics of ILTS datasets from high-resource
languages (HRLs) particularly English. We focus on datasets reported in
published ILTS research works during 2012-2022. The survey of ILTS and English
datasets reveals two similarities and one contrast. The two similarities are:
first, the domain of dataset commonly is news (Hermann et al., 2015). The
second similarity is the format of the dataset which is both extractive and
abstractive. The contrast is in how the research in dataset development has
progressed. ILs face a slow speed of development and public release of datasets
as compared with English. We argue that the relatively lower number of ILTS
datasets is because of two reasons: first, absence of a dedicated forum for
developing TS tools and resources; and second, lack of shareable standard
datasets in the public domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MMER: Multimodal Multi-task learning for Emotion Recognition in Spoken Utterances. (arXiv:2203.16794v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16794">
<div class="article-summary-box-inner">
<span><p>Emotion Recognition (ER) aims to classify human utterances into different
emotion categories. Based on early-fusion and self-attention-based multimodal
interaction between text and acoustic modalities, in this paper, we propose a
multimodal multitask learning approach for ER from individual utterances in
isolation. Experiments on the IEMOCAP benchmark show that our proposed model
performs better than our re-implementation of state-of-the-art and achieves
better performance than all other unimodal and multimodal approaches in
literature. In addition, strong baselines and ablation studies prove the
effectiveness of our proposed approach. We make all our codes publicly
available on GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Discourse Aware Sequence Learning Approach for Emotion Recognition in Conversations. (arXiv:2203.16799v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16799">
<div class="article-summary-box-inner">
<span><p>The expression of emotions is a crucial part of daily human communication.
Modeling the conversational and sequential context has seen much success and
plays a vital role in Emotion Recognition in Conversations (ERC). However,
existing approaches either model only one of the two or employ naive
late-fusion methodologies to obtain final utterance representations. This paper
proposes a novel idea to incorporate both these contexts and better model the
intrinsic structure within a conversation. More precisely, we propose a novel
architecture boosted by a modified LSTM cell, which we call DiscLSTM, that
better captures the interaction between conversational and sequential context.
DiscLSTM brings together the best of both worlds and provides a more intuitive
and efficient way to model the information flow between individual utterances
by better capturing long-distance conversational background through discourse
relations and sequential context through recurrence. We conduct experiments on
four benchmark datasets for ERC and show that our model achieves performance
competitive to state-of-the-art and at times performs better than other
graph-based approaches in literature, with a conversational graph that is both
sparse and avoids complicated edge relations like much of previous work. We
make all our codes publicly available on GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comparative Study on Speaker-attributed Automatic Speech Recognition in Multi-party Meetings. (arXiv:2203.16834v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16834">
<div class="article-summary-box-inner">
<span><p>In this paper, we conduct a comparative study on speaker-attributed automatic
speech recognition (SA-ASR) in the multi-party meeting scenario, a topic with
increasing attention in meeting rich transcription. Specifically, three
approaches are evaluated in this study. The first approach, FD-SOT, consists of
a frame-level diarization model to identify speakers and a multi-talker ASR to
recognize utterances. The speaker-attributed transcriptions are obtained by
aligning the diarization results and recognized hypotheses. However, such an
alignment strategy may suffer from erroneous timestamps due to the modular
independence, severely hindering the model performance. Therefore, we propose
the second approach, WD-SOT, to address alignment errors by introducing a
word-level diarization model, which can get rid of such timestamp alignment
dependency. To further mitigate the alignment issues, we propose the third
approach, TS-ASR, which trains a target-speaker separation module and an ASR
module jointly. By comparing various strategies for each SA-ASR approach,
experimental results on a real meeting scenario corpus, AliMeeting, reveal that
the WD-SOT approach achieves 10.7% relative reduction on averaged
speaker-dependent character error rate (SD-CER), compared with the FD-SOT
approach. In addition, the TS-ASR approach also outperforms the FD-SOT approach
and brings 16.5% relative average SD-CER reduction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perceptual Contrast Stretching on Target Feature for Speech Enhancement. (arXiv:2203.17152v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17152">
<div class="article-summary-box-inner">
<span><p>Speech enhancement (SE) performance has improved considerably since the use
of deep learning (DL) models as a base function. In this study, we propose a
perceptual contrast stretching (PCS) approach to further improve SE
performance. PCS is derived based on the critical band importance function and
applied to modify the targets of the SE model. Specifically, PCS stretches the
contract of target features according to perceptual importance, thereby
improving the overall SE performance. Compared to post-processing based
implementations, incorporating PCS into the training phase preserves
performance and reduces online computation. It is also worth noting that PCS
can be suitably combined with different SE model architectures and training
criteria. Meanwhile, PCS does not affect the causality or convergence of the SE
model training. Experimental results on the VoiceBank-DEMAND dataset showed
that the proposed method can achieve state-of-the-art performance on both
causal (PESQ=3.07) and non-causal (PESQ=3.35) SE tasks.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Ball 3D localization from a single calibrated image. (arXiv:2204.00003v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00003">
<div class="article-summary-box-inner">
<span><p>Ball 3D localization in team sports has various applications including
automatic offside detection in soccer, or shot release localization in
basketball. Today, this task is either resolved by using expensive multi-views
setups, or by restricting the analysis to ballistic trajectories. In this work,
we propose to address the task on a single image from a calibrated monocular
camera by estimating ball diameter in pixels and use the knowledge of real ball
diameter in meters. This approach is suitable for any game situation where the
ball is (even partly) visible. To achieve this, we use a small neural network
trained on image patches around candidates generated by a conventional ball
detector. Besides predicting ball diameter, our network outputs the confidence
of having a ball in the image patch. Validations on 3 basketball datasets
reveals that our model gives remarkable predictions on ball 3D localization. In
addition, through its confidence output, our model improves the detection rate
by filtering the candidates produced by the detector. The contributions of this
work are (i) the first model to address 3D ball localization on a single image,
(ii) an effective method for ball 3D annotation from single calibrated images,
(iii) a high quality 3D ball evaluation dataset annotated from a single
viewpoint. In addition, the code to reproduce this research is be made freely
available at https://github.com/gabriel-vanzandycke/deepsport.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph-based Active Learning for Semi-supervised Classification of SAR Data. (arXiv:2204.00005v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00005">
<div class="article-summary-box-inner">
<span><p>We present a novel method for classification of Synthetic Aperture Radar
(SAR) data by combining ideas from graph-based learning and neural network
methods within an active learning framework. Graph-based methods in machine
learning are based on a similarity graph constructed from the data. When the
data consists of raw images composed of scenes, extraneous information can make
the classification task more difficult. In recent years, neural network methods
have been shown to provide a promising framework for extracting patterns from
SAR images. These methods, however, require ample training data to avoid
overfitting. At the same time, such training data are often unavailable for
applications of interest, such as automatic target recognition (ATR) and SAR
data. We use a Convolutional Neural Network Variational Autoencoder (CNNVAE) to
embed SAR data into a feature space, and then construct a similarity graph from
the embedded data and apply graph-based semi-supervised learning techniques.
The CNNVAE feature embedding and graph construction requires no labeled data,
which reduces overfitting and improves the generalization performance of graph
learning at low label rates. Furthermore, the method easily incorporates a
human-in-the-loop for active learning in the data-labeling process. We present
promising results and compare them to other standard machine learning methods
on the Moving and Stationary Target Acquisition and Recognition (MSTAR) dataset
for ATR with small amounts of labeled data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Digitizing Historical Balance Sheet Data: A Practitioner's Guide. (arXiv:2204.00052v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00052">
<div class="article-summary-box-inner">
<span><p>This paper discusses how to successfully digitize large-scale historical
micro-data by augmenting optical character recognition (OCR) engines with pre-
and post-processing methods. Although OCR software has improved dramatically in
recent years due to improvements in machine learning, off-the-shelf OCR
applications still present high error rates which limits their applications for
accurate extraction of structured information. Complementing OCR with
additional methods can however dramatically increase its success rate, making
it a powerful and cost-efficient tool for economic historians. This paper
showcases these methods and explains why they are useful. We apply them against
two large balance sheet datasets and introduce "quipucamayoc", a Python package
containing these methods in a unified framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Classification of Alzheimer's Disease using brain MRI data and deep Convolutional Neural Networks. (arXiv:2204.00068v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00068">
<div class="article-summary-box-inner">
<span><p>Alzheimer's disease (AD) is one of the most common public health issues the
world is facing today. This disease has a high prevalence primarily in the
elderly accompanying memory loss and cognitive decline. AD detection is a
challenging task which many authors have developed numerous computerized
automatic diagnosis systems utilizing neuroimaging and other clinical data. MRI
scans provide high-intensity visible features, making these scans the most
widely used brain imaging technique. In recent years deep learning has achieved
leading success in medical image analysis. But a relatively little
investigation has been done to apply deep learning techniques for the brain MRI
classification. This paper explores the construction of several deep learning
architectures evaluated on brain MRI images and segmented images. The idea
behind segmented images investigates the influence of image segmentation step
on deep learning classification. The image processing presented a pipeline
consisting of pre-processing to enhance the MRI scans and post-processing
consisting of a segmentation method for segmenting the brain tissues. The
results show that the processed images achieved a better accuracy in the binary
classification of AD vs. CN (Cognitively Normal) across four different
architectures. ResNet architecture resulted in the highest prediction accuracy
amongst the other architectures (90.83% for the original brain images and
93.50% for the processed images).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Maximal Coding Rate Reduction by Variational Forms. (arXiv:2204.00077v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00077">
<div class="article-summary-box-inner">
<span><p>The principle of Maximal Coding Rate Reduction (MCR$^2$) has recently been
proposed as a training objective for learning discriminative low-dimensional
structures intrinsic to high-dimensional data to allow for more robust training
than standard approaches, such as cross-entropy minimization. However, despite
the advantages that have been shown for MCR$^2$ training, MCR$^2$ suffers from
a significant computational cost due to the need to evaluate and differentiate
a significant number of log-determinant terms that grows linearly with the
number of classes. By taking advantage of variational forms of spectral
functions of a matrix, we reformulate the MCR$^2$ objective to a form that can
scale significantly without compromising training accuracy. Experiments in
image classification demonstrate that our proposed formulation results in a
significant speed up over optimizing the original MCR$^2$ objective directly
and often results in higher quality learned representations. Further, our
approach may be of independent interest in other models that require
computation of log-determinant forms, such as in system identification or
normalizing flow models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">4Weed Dataset: Annotated Imagery Weeds Dataset. (arXiv:2204.00080v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00080">
<div class="article-summary-box-inner">
<span><p>Weeds are a major threat to crops and are responsible for reducing crop yield
worldwide. To mitigate their negative effect, it is advantageous to accurately
identify them early in the season to prevent their spread throughout the field.
Traditionally, farmers rely on manually scouting fields and applying herbicides
for different weeds. However, it is easy to confuse between crops with weeds
during the early growth stages. Recently, deep learning-based weed
identification has become popular as deep learning relies on convolutional
neural networks that are capable of learning important distinguishable features
between weeds and crops. However, training robust deep learning models requires
access to large imagery datasets. Therefore, an early-season weeds dataset was
acquired under field conditions. The dataset consists of 159 Cocklebur images,
139 Foxtail images, 170 Redroot Pigweed images and 150 Giant Ragweed images
corresponding to four common weed species found in corn and soybean production
systems.. Bounding box annotations were created for each image to prepare the
dataset for training both image classification and object detection deep
learning networks capable of accurately locating and identifying weeds within
corn and soybean fields. (https://osf.io/w9v3j/)
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Top-$k$ White-Box and Transferable Black-box Attack. (arXiv:2204.00089v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00089">
<div class="article-summary-box-inner">
<span><p>Existing works have identified the limitation of top-$1$ attack success rate
(ASR) as a metric to evaluate the attack strength but exclusively investigated
it in the white-box setting, while our work extends it to a more practical
black-box setting: transferable attack. It is widely reported that stronger
I-FGSM transfers worse than simple FGSM, leading to a popular belief that
transferability is at odds with the white-box attack strength. Our work
challenges this belief with empirical finding that stronger attack actually
transfers better for the general top-$k$ ASR indicated by the interest class
rank (ICR) after attack. For increasing the attack strength, with an intuitive
interpretation of the logit gradient from the geometric perspective, we
identify that the weakness of the commonly used losses lie in prioritizing the
speed to fool the network instead of maximizing its strength. To this end, we
propose a new normalized CE loss that guides the logit to be updated in the
direction of implicitly maximizing its rank distance from the ground-truth
class. Extensive results in various settings have verified that our proposed
new loss is simple yet effective for top-$k$ attack. Code is available at:
\url{https://bit.ly/3uCiomP}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tooth Instance Segmentation on Panoramic Dental Radiographs Using U-Nets and Morphological Processing. (arXiv:2204.00095v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00095">
<div class="article-summary-box-inner">
<span><p>Automatic teeth segmentation in panoramic x-ray images is an important
research subject of the image analysis in dentistry. In this study, we propose
a post-processing stage to obtain a segmentation map in which the objects in
the image are separated, and apply this technique to tooth instance
segmentation with U-Net network. The post-processing consists of grayscale
morphological and filtering operations, which are applied to the sigmoid output
of the network before binarization. A dice overlap score of 95.4 - 0.3% is
obtained in overall teeth segmentation. The proposed post-processing stages
reduce the mean error of tooth count to 6.15%, whereas the error without
post-processing is 26.81%. The performances of both segmentation and tooth
counting are the highest in the literature, to our knowledge. Moreover, this is
achieved by using a relatively small training dataset, which consists of 105
images. Although the aim in this study is to segment tooth instances, the
presented method is applicable to similar problems in other domains, such as
separating the cell instances
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransGeo: Transformer Is All You Need for Cross-view Image Geo-localization. (arXiv:2204.00097v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00097">
<div class="article-summary-box-inner">
<span><p>The dominant CNN-based methods for cross-view image geo-localization rely on
polar transform and fail to model global correlation. We propose a pure
transformer-based approach (TransGeo) to address these limitations from a
different perspective. TransGeo takes full advantage of the strengths of
transformer related to global information modeling and explicit position
information encoding. We further leverage the flexibility of transformer input
and propose an attention-guided non-uniform cropping method, so that
uninformative image patches are removed with negligible drop on performance to
reduce computation cost. The saved computation can be reallocated to increase
resolution only for informative patches, resulting in performance improvement
with no additional computation cost. This "attend and zoom-in" strategy is
highly similar to human behavior when observing images. Remarkably, TransGeo
achieves state-of-the-art results on both urban and rural datasets, with
significantly less computation cost than CNN-based methods. It does not rely on
polar transform and infers faster than CNN-based methods. Code is available at
https://github.com/Jeff-Zilence/TransGeo2022.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Multimodal Fusion. (arXiv:2204.00102v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00102">
<div class="article-summary-box-inner">
<span><p>Deep multimodal learning has achieved great progress in recent years.
However, current fusion approaches are static in nature, i.e., they process and
fuse multimodal inputs with identical computation, without accounting for
diverse computational demands of different multimodal data. In this work, we
propose dynamic multimodal fusion (DynMM), a new approach that adaptively fuses
multimodal data and generates data-dependent forward paths during inference.
DynMM can reduce redundant computations for "easy" multimodal inputs (that can
be predicted correctly using only one modality or simple fusion techniques) and
retain representation power for "hard" samples by adopting all modalities and
complex fusion operations for prediction. Results on various multimodal tasks
demonstrate the efficiency and wide applicability of our approach. For
instance, DynMM can reduce the computation cost by 46.5% with a negligible
accuracy loss on CMU-MOSEI sentiment analysis. For RGB-D semantic segmentation
on NYU Depth data, DynMM achieves a +0.7% mIoU improvement with over 21%
reductions for the depth encoder when compared with strong baselines. We
believe this opens a novel direction towards dynamic multimodal network design,
with applications to a wide range of multimodal tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Robust 3D Object Detection Methods in Point Clouds. (arXiv:2204.00106v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00106">
<div class="article-summary-box-inner">
<span><p>The purpose of this work is to review the state-of-the-art LiDAR-based 3D
object detection methods, datasets, and challenges. We describe novel data
augmentation methods, sampling strategies, activation functions, attention
mechanisms, and regularization methods. Furthermore, we list recently
introduced normalization methods, learning rate schedules and loss functions.
Moreover, we also cover advantages and limitations of 10 novel autonomous
driving datasets. We evaluate novel 3D object detectors on the KITTI, nuScenes,
and Waymo dataset and show their accuracy, speed, and robustness. Finally, we
mention the current challenges in 3D object detection in LiDAR point clouds and
list some open issues.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GALA: Toward Geometry-and-Lighting-Aware Object Search for Compositing. (arXiv:2204.00125v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00125">
<div class="article-summary-box-inner">
<span><p>Compositing-aware object search aims to find the most compatible objects for
compositing given a background image and a query bounding box. Previous works
focus on learning compatibility between the foreground object and background,
but fail to learn other important factors from large-scale data, i.e. geometry
and lighting. To move a step further, this paper proposes GALA
(Geometry-and-Lighting-Aware), a generic foreground object search method with
discriminative modeling on geometry and lighting compatibility for open-world
image compositing. Remarkably, it achieves state-of-the-art results on the CAIS
dataset and generalizes well on large-scale open-world datasets, i.e. Pixabay
and Open Images. In addition, our method can effectively handle non-box
scenarios, where users only provide background images without any input
bounding box. A web demo (see supplementary materials) is built to showcase
applications of the proposed method for compositing-aware search and automatic
location/scale prediction for the foreground object.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perceptual Quality Assessment of UGC Gaming Videos. (arXiv:2204.00128v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00128">
<div class="article-summary-box-inner">
<span><p>In recent years, with the vigorous development of the video game industry,
the proportion of gaming videos on major video websites like YouTube has
dramatically increased. However, relatively little research has been done on
the automatic quality prediction of gaming videos, especially on those that
fall in the category of "User-Generated-Content" (UGC). Since current leading
general-purpose Video Quality Assessment (VQA) models do not perform well on
this type of gaming videos, we have created a new VQA model specifically
designed to succeed on UGC gaming videos, which we call the Gaming Video
Quality Predictor (GAME-VQP). GAME-VQP successfully predicts the unique
statistical characteristics of gaming videos by drawing upon features designed
under modified natural scene statistics models, combined with gaming specific
features learned by a Convolution Neural Network. We study the performance of
GAME-VQP on a very recent large UGC gaming video database called
LIVE-YT-Gaming, and find that it both outperforms other mainstream general VQA
models as well as VQA models specifically designed for gaming videos. The new
model will be made public after paper being accepted.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-Time and Robust 3D Object Detection Within Road-Side LiDARs Using Domain Adaptation. (arXiv:2204.00132v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00132">
<div class="article-summary-box-inner">
<span><p>This work aims to address the challenges in domain adaptation of 3D object
detection using infrastructure LiDARs. We design a model DASE-ProPillars that
can detect vehicles in infrastructure-based LiDARs in real-time. Our model uses
PointPillars as the baseline model with additional modules to improve the 3D
detection performance. To prove the effectiveness of our proposed modules in
DASE-ProPillars, we train and evaluate the model on two datasets, the open
source A9-Dataset and a semi-synthetic infrastructure dataset created within
the Regensburg Next project. We do several sets of experiments for each module
in the DASE-ProPillars detector that show that our model outperforms the
SE-ProPillars baseline on the real A9 test set and a semi-synthetic A9 test
set, while maintaining an inference speed of 45 Hz (22 ms). We apply domain
adaptation from the semi-synthetic A9-Dataset to the semi-synthetic dataset
from the Regensburg Next project by applying transfer learning and achieve a 3D
mAP@0.25 of 93.49% on the Car class of the target test set using 40 recall
positions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Model Predictive Control for Fluid Human-to-Robot Handovers. (arXiv:2204.00134v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00134">
<div class="article-summary-box-inner">
<span><p>Human-robot handover is a fundamental yet challenging task in human-robot
interaction and collaboration. Recently, remarkable progressions have been made
in human-to-robot handovers of unknown objects by using learning-based grasp
generators. However, how to responsively generate smooth motions to take an
object from a human is still an open question. Specifically, planning motions
that take human comfort into account is not a part of the human-robot handover
process in most prior works. In this paper, we propose to generate smooth
motions via an efficient model-predictive control (MPC) framework that
integrates perception and complex domain-specific constraints into the
optimization problem. We introduce a learning-based grasp reachability model to
select candidate grasps which maximize the robot's manipulability, giving it
more freedom to satisfy these constraints. Finally, we integrate a neural net
force/torque classifier that detects contact events from noisy data. We
conducted human-to-robot handover experiments on a diverse set of objects with
several users (N=4) and performed a systematic evaluation of each module. The
study shows that the users preferred our MPC approach over the baseline system
by a large margin. More results and videos are available at
https://sites.google.com/nvidia.com/mpc-for-handover.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Weakly Supervised Object Detection by Sampling Pseudo Ground-Truth Boxes. (arXiv:2204.00147v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00147">
<div class="article-summary-box-inner">
<span><p>Semi- and weakly-supervised learning have recently attracted considerable
attention in the object detection literature since they can alleviate the cost
of annotation needed to successfully train deep learning models. State-of-art
approaches for semi-supervised learning rely on student-teacher models trained
using a multi-stage process, and considerable data augmentation. Custom
networks have been developed for the weakly-supervised setting, making it
difficult to adapt to different detectors. In this paper, a weakly
semi-supervised training method is introduced that reduces these training
challenges, yet achieves state-of-the-art performance by leveraging only a
small fraction of fully-labeled images with information in weakly-labeled
images. In particular, our generic sampling-based learning strategy produces
pseudo-ground-truth (GT) bounding box annotations in an online fashion,
eliminating the need for multi-stage training, and student-teacher network
configurations. These pseudo GT boxes are sampled from weakly-labeled images
based on the categorical score of object proposals accumulated via a score
propagation process. Empirical results on the Pascal VOC dataset, indicate that
the proposed approach improves performance by 5.0% when using VOC 2007 as
fully-labeled, and VOC 2012 as weak-labeled data. Also, with 5-10% fully
annotated images, we observed an improvement of more than 10% in mAP, showing
that a modest investment in image-level annotation, can substantially improve
detection performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An End-to-end Supervised Domain Adaptation Framework for Cross-Domain Change Detection. (arXiv:2204.00154v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00154">
<div class="article-summary-box-inner">
<span><p>Existing deep learning-based change detection methods try to elaborately
design complicated neural networks with powerful feature representations, but
ignore the universal domain shift induced by time-varying land cover changes,
including luminance fluctuations and season changes between pre-event and
post-event images, thereby producing sub-optimal results. In this paper, we
propose an end-to-end Supervised Domain Adaptation framework for cross-domain
Change Detection, namely SDACD, to effectively alleviate the domain shift
between bi-temporal images for better change predictions. Specifically, our
SDACD presents collaborative adaptations from both image and feature
perspectives with supervised learning. Image adaptation exploits generative
adversarial learning with cycle-consistency constraints to perform cross-domain
style transformation, effectively narrowing the domain gap in a two-side
generation fashion. As to feature adaptation, we extract domain-invariant
features to align different feature distributions in the feature space, which
could further reduce the domain gap of cross-domain images. To further improve
the performance, we combine three types of bi-temporal images for the final
change prediction, including the initial input bi-temporal images and two
generated bi-temporal images from the pre-event and post-event domains.
Extensive experiments and analyses on two benchmarks demonstrate the
effectiveness and universality of our proposed framework. Notably, our
framework pushes several representative baseline models up to new
State-Of-The-Art records, achieving 97.34% and 92.36% on the CDD and WHU
building datasets, respectively. The source code and models are publicly
available at https://github.com/Perfect-You/SDACD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stereo Unstructured Magnification: Multiple Homography Image for View Synthesis. (arXiv:2204.00156v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00156">
<div class="article-summary-box-inner">
<span><p>This paper studies the problem of view synthesis with certain amount of
rotations from a pair of images, what we called stereo unstructured
magnification. While the multi-plane image representation is well suited for
view synthesis with depth invariant, how to generalize it to unstructured views
remains a significant challenge. This is primarily due to the depth-dependency
caused by camera frontal parallel representation. Here we propose a novel
multiple homography image (MHI) representation, comprising of a set of scene
planes with fixed normals and distances. A two-stage network is developed for
novel view synthesis. Stage-1 is an MHI reconstruction module that predicts the
MHIs and composites layered multi-normal images along the normal direction.
Stage-2 is a normal-blending module to find blending weights. We also derive an
angle-based cost to guide the blending of multi-normal images by exploiting
per-normal geometry. Compared with the state-of-the-art methods, our method
achieves superior performance for view synthesis qualitatively and
quantitatively, especially for cases when the cameras undergo rotations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LASER: LAtent SpacE Rendering for 2D Visual Localization. (arXiv:2204.00157v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00157">
<div class="article-summary-box-inner">
<span><p>We present LASER, an image-based Monte Carlo Localization (MCL) framework for
2D floor maps. LASER introduces the concept of latent space rendering, where 2D
pose hypotheses on the floor map are directly rendered into a
geometrically-structured latent space by aggregating viewing ray features.
Through a tightly coupled rendering codebook scheme, the viewing ray features
are dynamically determined at rendering-time based on their geometries (i.e.
length, incident-angle), endowing our representation with view-dependent
fine-grain variability. Our codebook scheme effectively disentangles feature
encoding from rendering, allowing the latent space rendering to run at speeds
above 10KHz. Moreover, through metric learning, our geometrically-structured
latent space is common to both pose hypotheses and query images with arbitrary
field of views. As a result, LASER achieves state-of-the-art performance on
large-scale indoor localization datasets (i.e. ZInD and Structured3D) for both
panorama and perspective image queries, while significantly outperforming
existing learning-based methods in speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mutual Scene Synthesis for Mixed Reality Telepresence. (arXiv:2204.00161v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00161">
<div class="article-summary-box-inner">
<span><p>Remote telepresence via next-generation mixed reality platforms can provide
higher levels of immersion for computer-mediated communications, allowing
participants to engage in a wide spectrum of activities, previously not
possible in 2D screen-based communication methods. However, as mixed reality
experiences are limited to the local physical surrounding of each user, finding
a common virtual ground where users can freely move and interact with each
other is challenging. In this paper, we propose a novel mutual scene synthesis
method that takes the participants' spaces as input, and generates a virtual
synthetic scene that corresponds to the functional features of all
participants' local spaces. Our method combines a mutual function optimization
module with a deep-learning conditional scene augmentation process to generate
a scene mutually and physically accessible to all participants of a mixed
reality telepresence scenario. The synthesized scene can hold mutual walkable,
sittable and workable functions, all corresponding to physical objects in the
users' real environments. We perform experiments using the MatterPort3D dataset
and conduct comparative user studies to evaluate the effectiveness of our
system. Our results show that our proposed approach can be a promising research
direction for facilitating contextualized telepresence systems for
next-generation spatial computing platforms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unified Framework for Domain Adaptive Pose Estimation. (arXiv:2204.00172v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00172">
<div class="article-summary-box-inner">
<span><p>While pose estimation is an important computer vision task, it requires
expensive annotation and suffers from domain shift. In this paper, we
investigate the problem of domain adaptive 2D pose estimation that transfers
knowledge learned on a synthetic source domain to a target domain without
supervision. While several domain adaptive pose estimation models have been
proposed recently, they are not generic but only focus on either human pose or
animal pose estimation, and thus their effectiveness is somewhat limited to
specific scenarios. In this work, we propose a unified framework that
generalizes well on various domain adaptive pose estimation problems. We
propose to align representations using both input-level and output-level cues
(pixels and pose labels, respectively), which facilitates the knowledge
transfer from the source domain to the unlabeled target domain. Our experiments
show that our method achieves state-of-the-art performance under various domain
shifts. Our method outperforms existing baselines on human pose estimation by
up to 4.5 percent points (pp), hand pose estimation by up to 7.4 pp, and animal
pose estimation by up to 4.8 pp for dogs and 3.3 pp for sheep. These results
suggest that our method is able to mitigate domain shift on diverse tasks and
even unseen domains and objects (e.g., trained on horse and tested on dog).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GraftNet: Towards Domain Generalized Stereo Matching with a Broad-Spectrum and Task-Oriented Feature. (arXiv:2204.00179v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00179">
<div class="article-summary-box-inner">
<span><p>Although supervised deep stereo matching networks have made impressive
achievements, the poor generalization ability caused by the domain gap prevents
them from being applied to real-life scenarios. In this paper, we propose to
leverage the feature of a model trained on large-scale datasets to deal with
the domain shift since it has seen various styles of images. With the cosine
similarity based cost volume as a bridge, the feature will be grafted to an
ordinary cost aggregation module. Despite the broad-spectrum representation,
such a low-level feature contains much general information which is not aimed
at stereo matching. To recover more task-specific information, the grafted
feature is further input into a shallow network to be transformed before
calculating the cost. Extensive experiments show that the model generalization
ability can be improved significantly with this broad-spectrum and
task-oriented feature. Specifically, based on two well-known architectures
PSMNet and GANet, our methods are superior to other robust algorithms when
transferring from SceneFlow to KITTI 2015, KITTI 2012, and Middlebury. Code is
available at https://github.com/SpadeLiu/Graft-PSMNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Supervisor for Cross-dataset Object Detection. (arXiv:2204.00183v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00183">
<div class="article-summary-box-inner">
<span><p>The application of cross-dataset training in object detection tasks is
complicated because the inconsistency in the category range across datasets
transforms fully supervised learning into semi-supervised learning. To address
this problem, recent studies focus on the generation of high-quality missing
annotations. In this study, we first point out that it is not enough to
generate high-quality annotations using a single model, which only looks once
for annotations. Through detailed experimental analyses, we further conclude
that hard-label training is conducive to generating high-recall annotations,
while soft-label training tends to obtain high-precision annotations. Inspired
by the aspects mentioned above, we propose a dynamic supervisor framework that
updates the annotations multiple times through multiple-updated submodels
trained using hard and soft labels. In the final generated annotations, both
recall and precision improve significantly through the integration of
hard-label training with soft-label training. Extensive experiments conducted
on various dataset combination settings support our analyses and demonstrate
the superior performance of the proposed dynamic supervisor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Epipolar Focus Spectrum: A Novel Light Field Representation and Application in Dense-view Reconstruction. (arXiv:2204.00193v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00193">
<div class="article-summary-box-inner">
<span><p>Existing light field representations, such as epipolar plane image (EPI) and
sub-aperture images, do not consider the structural characteristics across the
views, so they usually require additional disparity and spatial structure cues
for follow-up tasks. Besides, they have difficulties dealing with occlusions or
larger disparity scenes. To this end, this paper proposes a novel Epipolar
Focus Spectrum (EFS) representation by rearranging the EPI spectrum. Different
from the classical EPI representation where an EPI line corresponds to a
specific depth, there is a one-to-one mapping from the EFS line to the view.
Accordingly, compared to a sparsely-sampled light field, a densely-sampled one
with the same field of view (FoV) leads to a more compact distribution of such
linear structures in the double-cone-shaped region with the identical opening
angle in its corresponding EFS. Hence the EFS representation is invariant to
the scene depth. To demonstrate its effectiveness, we develop a trainable
EFS-based pipeline for light field reconstruction, where a dense light field
can be reconstructed by compensating the "missing EFS lines" given a sparse
light field, yielding promising results with cross-view consistency, especially
in the presence of severe occlusion and large disparity. Experimental results
on both synthetic and real-world datasets demonstrate the validity and
superiority of the proposed method over SOTA methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridging the Gap between Classification and Localization for Weakly Supervised Object Localization. (arXiv:2204.00220v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00220">
<div class="article-summary-box-inner">
<span><p>Weakly supervised object localization aims to find a target object region in
a given image with only weak supervision, such as image-level labels. Most
existing methods use a class activation map (CAM) to generate a localization
map; however, a CAM identifies only the most discriminative parts of a target
object rather than the entire object region. In this work, we find the gap
between classification and localization in terms of the misalignment of the
directions between an input feature and a class-specific weight. We demonstrate
that the misalignment suppresses the activation of CAM in areas that are less
discriminative but belong to the target object. To bridge the gap, we propose a
method to align feature directions with a class-specific weight. The proposed
method achieves a state-of-the-art localization performance on the CUB-200-2011
and ImageNet-1K benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perception Prioritized Training of Diffusion Models. (arXiv:2204.00227v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00227">
<div class="article-summary-box-inner">
<span><p>Diffusion models learn to restore noisy data, which is corrupted with
different levels of noise, by optimizing the weighted sum of the corresponding
loss terms, i.e., denoising score matching loss. In this paper, we show that
restoring data corrupted with certain noise levels offers a proper pretext task
for the model to learn rich visual concepts. We propose to prioritize such
noise levels over other levels during training, by redesigning the weighting
scheme of the objective function. We show that our simple redesign of the
weighting scheme significantly improves the performance of diffusion models
regardless of the datasets, architectures, and sampling strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Online panoptic 3D reconstruction as a Linear Assignment Problem. (arXiv:2204.00231v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00231">
<div class="article-summary-box-inner">
<span><p>Real-time holistic scene understanding would allow machines to interpret
their surrounding in a much more detailed manner than is currently possible.
While panoptic image segmentation methods have brought image segmentation
closer to this goal, this information has to be described relative to the 3D
environment for the machine to be able to utilise it effectively. In this
paper, we investigate methods for sequentially reconstructing static
environments from panoptic image segmentations in 3D. We specifically target
real-time operation: the algorithm must process data strictly online and be
able to run at relatively fast frame rates. Additionally, the method should be
scalable for environments large enough for practical applications. By applying
a simple but powerful data-association algorithm, we outperform earlier similar
works when operating purely online. Our method is also capable of reaching
frame-rates high enough for real-time applications and is scalable to larger
environments as well. Source code and further demonstrations are released to
the public at: \url{https://tutvision.github.io/Online-Panoptic-3D/}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ObjectMix: Data Augmentation by Copy-Pasting Objects in Videos for Action Recognition. (arXiv:2204.00239v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00239">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a data augmentation method for action recognition
using instance segmentation. Although many data augmentation methods have been
proposed for image recognition, few methods have been proposed for action
recognition. Our proposed method, ObjectMix, extracts each object region from
two videos using instance segmentation and combines them to create new videos.
Experiments on two action recognition datasets, UCF101 and HMDB51, demonstrate
the effectiveness of the proposed method and show its superiority over
VideoMix, a prior work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MS-HLMO: Multi-scale Histogram of Local Main Orientation for Remote Sensing Image Registration. (arXiv:2204.00260v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00260">
<div class="article-summary-box-inner">
<span><p>Multi-source image registration is challenging due to intensity, rotation,
and scale differences among the images. Considering the characteristics and
differences of multi-source remote sensing images, a feature-based registration
algorithm named Multi-scale Histogram of Local Main Orientation (MS-HLMO) is
proposed. Harris corner detection is first adopted to generate feature points.
The HLMO feature of each Harris feature point is extracted on a Partial Main
Orientation Map (PMOM) with a Generalized Gradient Location and Orientation
Histogram-like (GGLOH) feature descriptor, which provides high intensity,
rotation, and scale invariance. The feature points are matched through a
multi-scale matching strategy. Comprehensive experiments on 17 multi-source
remote sensing scenes demonstrate that the proposed MS-HLMO and its simplified
version MS-HLMO$^+$ outperform other competitive registration algorithms in
terms of effectiveness and generalization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Selecting task with optimal transport self-supervised learning for few-shot classification. (arXiv:2204.00289v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00289">
<div class="article-summary-box-inner">
<span><p>Few-Shot classification aims at solving problems that only a few samples are
available in the training process. Due to the lack of samples, researchers
generally employ a set of training tasks from other domains to assist the
target task, where the distribution between assistant tasks and the target task
is usually different. To reduce the distribution gap, several lines of methods
have been proposed, such as data augmentation and domain alignment. However,
one common drawback of these algorithms is that they ignore the similarity task
selection before training. The fundamental problem is to push the auxiliary
tasks close to the target task. In this paper, we propose a novel task
selecting algorithm, named Optimal Transport Task Selecting (OTTS), to
construct a training set by selecting similar tasks for Few-Shot learning.
Specifically, the OTTS measures the task similarity by calculating the optimal
transport distance and completes the model training via a self-supervised
strategy. By utilizing the selected tasks with OTTS, the training process of
Few-Shot learning become more stable and effective. Other proposed methods
including data augmentation and domain alignment can be used in the meantime
with OTTS. We conduct extensive experiments on a variety of datasets, including
MiniImageNet, CIFAR, CUB, Cars, and Places, to evaluate the effectiveness of
OTTS. Experimental results validate that our OTTS outperforms the typical
baselines, i.e., MAML, matchingnet, protonet, by a large margin (averagely
1.72\% accuracy improvement).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GrowliFlower: An image time series dataset for GROWth analysis of cauLIFLOWER. (arXiv:2204.00294v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00294">
<div class="article-summary-box-inner">
<span><p>This article presents GrowliFlower, a georeferenced, image-based UAV time
series dataset of two monitored cauliflower fields of size 0.39 and 0.60 ha
acquired in 2020 and 2021. The dataset contains RGB and multispectral
orthophotos from which about 14,000 individual plant coordinates are derived
and provided. The coordinates enable the dataset users the extraction of
complete and incomplete time series of image patches showing individual plants.
The dataset contains collected phenotypic traits of 740 plants, including the
developmental stage as well as plant and cauliflower size. As the harvestable
product is completely covered by leaves, plant IDs and coordinates are provided
to extract image pairs of plants pre and post defoliation, to facilitate
estimations of cauliflower head size. Moreover, the dataset contains
pixel-accurate leaf and plant instance segmentations, as well as stem
annotations to address tasks like classification, detection, segmentation,
instance segmentation, and similar computer vision tasks. The dataset aims to
foster the development and evaluation of machine learning approaches. It
specifically focuses on the analysis of growth and development of cauliflower
and the derivation of phenotypic traits to foster the development of automation
in agriculture. Two baseline results of instance segmentation at plant and leaf
level based on the labeled instance segmentation data are presented. The entire
data set is publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unitail: Detecting, Reading, and Matching in Retail Scene. (arXiv:2204.00298v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00298">
<div class="article-summary-box-inner">
<span><p>To make full use of computer vision technology in stores, it is required to
consider the actual needs that fit the characteristics of the retail scene.
Pursuing this goal, we introduce the United Retail Datasets (Unitail), a
large-scale benchmark of basic visual tasks on products that challenges
algorithms for detecting, reading, and matching. With 1.8M quadrilateral-shaped
instances annotated, the Unitail offers a detection dataset to align product
appearance better. Furthermore, it provides a gallery-style OCR dataset
containing 1454 product categories, 30k text regions, and 21k transcriptions to
enable robust reading on products and motivate enhanced product matching.
Besides benchmarking the datasets using various state-of-the-arts, we customize
a new detector for product detection and provide a simple OCR-based matching
solution that verifies its effectiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Face identification by means of a neural net classifier. (arXiv:2204.00305v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00305">
<div class="article-summary-box-inner">
<span><p>This paper describes a novel face identification method that combines the
eigenfaces theory with the Neural Nets. We use the eigenfaces methodology in
order to reduce the dimensionality of the input image, and a neural net
classifier that performs the identification process. The method presented
recognizes faces in the presence of variations in facial expression, facial
details and lighting conditions. A recognition rate of more than 87% has been
achieved, while the classical method of Turk and Pentland achieves a 75.5%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unimodal-Concentrated Loss: Fully Adaptive Label Distribution Learning for Ordinal Regression. (arXiv:2204.00309v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00309">
<div class="article-summary-box-inner">
<span><p>Learning from a label distribution has achieved promising results on ordinal
regression tasks such as facial age and head pose estimation wherein, the
concept of adaptive label distribution learning (ALDL) has drawn lots of
attention recently for its superiority in theory. However, compared with the
methods assuming fixed form label distribution, ALDL methods have not achieved
better performance. We argue that existing ALDL algorithms do not fully exploit
the intrinsic properties of ordinal regression. In this paper, we emphatically
summarize that learning an adaptive label distribution on ordinal regression
tasks should follow three principles. First, the probability corresponding to
the ground-truth should be the highest in label distribution. Second, the
probabilities of neighboring labels should decrease with the increase of
distance away from the ground-truth, i.e., the distribution is unimodal. Third,
the label distribution should vary with samples changing, and even be distinct
for different instances with the same label, due to the different levels of
difficulty and ambiguity. Under the premise of these principles, we propose a
novel loss function for fully adaptive label distribution learning, namely
unimodal-concentrated loss. Specifically, the unimodal loss derived from the
learning to rank strategy constrains the distribution to be unimodal.
Furthermore, the estimation error and the variance of the predicted
distribution for a specific sample are integrated into the proposed
concentrated loss to make the predicted distribution maximize at the
ground-truth and vary according to the predicting uncertainty. Extensive
experimental results on typical ordinal regression tasks including age and head
pose estimation, show the superiority of our proposed unimodal-concentrated
loss compared with existing loss functions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CAT-Det: Contrastively Augmented Transformer for Multi-modal 3D Object Detection. (arXiv:2204.00325v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00325">
<div class="article-summary-box-inner">
<span><p>In autonomous driving, LiDAR point-clouds and RGB images are two major data
modalities with complementary cues for 3D object detection. However, it is
quite difficult to sufficiently use them, due to large inter-modal
discrepancies. To address this issue, we propose a novel framework, namely
Contrastively Augmented Transformer for multi-modal 3D object Detection
(CAT-Det). Specifically, CAT-Det adopts a two-stream structure consisting of a
Pointformer (PT) branch, an Imageformer (IT) branch along with a Cross-Modal
Transformer (CMT) module. PT, IT and CMT jointly encode intra-modal and
inter-modal long-range contexts for representing an object, thus fully
exploring multi-modal information for detection. Furthermore, we propose an
effective One-way Multi-modal Data Augmentation (OMDA) approach via
hierarchical contrastive learning at both the point and object levels,
significantly improving the accuracy only by augmenting point-clouds, which is
free from complex generation of paired samples of the two modalities. Extensive
experiments on the KITTI benchmark show that CAT-Det achieves a new
state-of-the-art, highlighting its effectiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DIP: Deep Inverse Patchmatch for High-Resolution Optical Flow. (arXiv:2204.00330v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00330">
<div class="article-summary-box-inner">
<span><p>Recently, the dense correlation volume method achieves state-of-the-art
performance in optical flow. However, the correlation volume computation
requires a lot of memory, which makes prediction difficult on high-resolution
images. In this paper, we propose a novel Patchmatch-based framework to work on
high-resolution optical flow estimation. Specifically, we introduce the first
end-to-end Patchmatch based deep learning optical flow. It can get
high-precision results with lower memory benefiting from propagation and local
search of Patchmatch. Furthermore, a new inverse propagation is proposed to
decouple the complex operations of propagation, which can significantly reduce
calculations in multiple iterations. At the time of submission, our method
ranks first on all the metrics on the popular KITTI2015 benchmark, and ranks
second on EPE on the Sintel clean benchmark among published optical flow
methods. Experiment shows our method has a strong cross-dataset generalization
ability that the F1-all achieves 13.73%, reducing 21% from the best published
result 17.4% on KITTI2015. What's more, our method shows a good details
preserving result on the high-resolution dataset DAVIS and consumes 2x less
memory than RAFT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RMS-FlowNet: Efficient and Robust Multi-Scale Scene Flow Estimation for Large-Scale Point Clouds. (arXiv:2204.00354v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00354">
<div class="article-summary-box-inner">
<span><p>The proposed RMS-FlowNet is a novel end-to-end learning-based architecture
for accurate and efficient scene flow estimation which can operate on point
clouds of high density. For hierarchical scene flow estimation, the existing
methods depend on either expensive Farthest-Point-Sampling (FPS) or
structure-based scaling which decrease their ability to handle a large number
of points. Unlike these methods, we base our fully supervised architecture on
Random-Sampling (RS) for multiscale scene flow prediction. To this end, we
propose a novel flow embedding design which can predict more robust scene flow
in conjunction with RS. Exhibiting high accuracy, our RMS-FlowNet provides a
faster prediction than state-of-the-art methods and works efficiently on
consecutive dense point clouds of more than 250K points at once. Our
comprehensive experiments verify the accuracy of RMS-FlowNet on the established
FlyingThings3D data set with different point cloud densities and validate our
design choices. Additionally, we show that our model presents a competitive
ability to generalize towards the real-world scenes of KITTI data set without
fine-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Deblur using Light Field Generated and Real Defocus Images. (arXiv:2204.00367v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00367">
<div class="article-summary-box-inner">
<span><p>Defocus deblurring is a challenging task due to the spatially varying nature
of defocus blur. While deep learning approach shows great promise in solving
image restoration problems, defocus deblurring demands accurate training data
that consists of all-in-focus and defocus image pairs, which is difficult to
collect. Naive two-shot capturing cannot achieve pixel-wise correspondence
between the defocused and all-in-focus image pairs. Synthetic aperture of light
fields is suggested to be a more reliable way to generate accurate image pairs.
However, the defocus blur generated from light field data is different from
that of the images captured with a traditional digital camera. In this paper,
we propose a novel deep defocus deblurring network that leverages the strength
and overcomes the shortcoming of light fields. We first train the network on a
light field-generated dataset for its highly accurate image correspondence.
Then, we fine-tune the network using feature loss on another dataset collected
by the two-shot method to alleviate the differences between the defocus blur
exists in the two domains. This strategy is proved to be highly effective and
able to achieve the state-of-the-art performance both quantitatively and
qualitatively on multiple test sets. Extensive ablation studies have been
conducted to analyze the effect of each network module to the final
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot One-class Domain Adaptation Based on Frequency for Iris Presentation Attack Detection. (arXiv:2204.00376v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00376">
<div class="article-summary-box-inner">
<span><p>Iris presentation attack detection (PAD) has achieved remarkable success to
ensure the reliability and security of iris recognition systems. Most existing
methods exploit discriminative features in the spatial domain and report
outstanding performance under intra-dataset settings. However, the degradation
of performance is inevitable under cross-dataset settings, suffering from
domain shift. In consideration of real-world applications, a small number of
bonafide samples are easily accessible. We thus define a new domain adaptation
setting called Few-shot One-class Domain Adaptation (FODA), where adaptation
only relies on a limited number of target bonafide samples. To address this
problem, we propose a novel FODA framework based on the expressive power of
frequency information. Specifically, our method integrates frequency-related
information through two proposed modules. Frequency-based Attention Module
(FAM) aggregates frequency information into spatial attention and explicitly
emphasizes high-frequency fine-grained features. Frequency Mixing Module (FMM)
mixes certain frequency components to generate large-scale target-style samples
for adaptation with limited target bonafide samples. Extensive experiments on
LivDet-Iris 2017 dataset demonstrate the proposed method achieves
state-of-the-art or competitive performance under both cross-dataset and
intra-dataset settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Supervised Regional and Temporal Learning for Facial Action Unit Recognition. (arXiv:2204.00379v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00379">
<div class="article-summary-box-inner">
<span><p>Automatic facial action unit (AU) recognition is a challenging task due to
the scarcity of manual annotations. To alleviate this problem, a large amount
of efforts has been dedicated to exploiting various weakly supervised methods
which leverage numerous unlabeled data. However, many aspects with regard to
some unique properties of AUs, such as the regional and relational
characteristics, are not sufficiently explored in previous works. Motivated by
this, we take the AU properties into consideration and propose two auxiliary AU
related tasks to bridge the gap between limited annotations and the model
performance in a self-supervised manner via the unlabeled data. Specifically,
to enhance the discrimination of regional features with AU relation embedding,
we design a task of RoI inpainting to recover the randomly cropped AU patches.
Meanwhile, a single image based optical flow estimation task is proposed to
leverage the dynamic change of facial muscles and encode the motion information
into the global feature representation. Based on these two self-supervised
auxiliary tasks, local features, mutual relation and motion cues of AUs are
better captured in the backbone network. Furthermore, by incorporating
semi-supervised learning, we propose an end-to-end trainable framework named
weakly supervised regional and temporal learning (WSRTL) for AU recognition.
Extensive experiments on BP4D and DISFA demonstrate the superiority of our
method and new state-of-the-art performances are achieved.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Autoencoder Attractors for Uncertainty Estimation. (arXiv:2204.00382v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00382">
<div class="article-summary-box-inner">
<span><p>The reliability assessment of a machine learning model's prediction is an
important quantity for the deployment in safety critical applications. Not only
can it be used to detect novel sceneries, either as out-of-distribution or
anomaly sample, but it also helps to determine deficiencies in the training
data distribution. A lot of promising research directions have either proposed
traditional methods like Gaussian processes or extended deep learning based
approaches, for example, by interpreting them from a Bayesian point of view. In
this work we propose a novel approach for uncertainty estimation based on
autoencoder models: The recursive application of a previously trained
autoencoder model can be interpreted as a dynamical system storing training
examples as attractors. While input images close to known samples will converge
to the same or similar attractor, input samples containing unknown features are
unstable and converge to different training samples by potentially removing or
changing characteristic features. The use of dropout during training and
inference leads to a family of similar dynamical systems, each one being robust
on samples close to the training distribution but unstable on new features.
Either the model reliably removes these features or the resulting instability
can be exploited to detect problematic input samples. We evaluate our approach
on several dataset combinations as well as on an industrial application for
occupant classification in the vehicle interior for which we additionally
release a new synthetic dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Autoencoder for Synthetic to Real Generalization: From Simple to More Complex Scenes. (arXiv:2204.00386v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00386">
<div class="article-summary-box-inner">
<span><p>Learning on synthetic data and transferring the resulting properties to their
real counterparts is an important challenge for reducing costs and increasing
safety in machine learning. In this work, we focus on autoencoder architectures
and aim at learning latent space representations that are invariant to
inductive biases caused by the domain shift between simulated and real images
showing the same scenario. We train on synthetic images only, present
approaches to increase generalizability and improve the preservation of the
semantics to real datasets of increasing visual complexity. We show that
pre-trained feature extractors (e.g. VGG) can be sufficient for generalization
on images of lower complexity, but additional improvements are required for
visually more complex scenes. To this end, we demonstrate a new sampling
technique, which matches semantically important parts of the image, while
randomizing the other parts, leads to salient feature extraction and a
neglection of unimportant parts. This helps the generalization to real data and
we further show that our approach outperforms fine-tuned classification models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparison of convolutional neural networks for cloudy optical images reconstruction from single or multitemporal joint SAR and optical images. (arXiv:2204.00424v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00424">
<div class="article-summary-box-inner">
<span><p>With the increasing availability of optical and synthetic aperture radar
(SAR) images thanks to the Sentinel constellation, and the explosion of deep
learning, new methods have emerged in recent years to tackle the reconstruction
of optical images that are impacted by clouds. In this paper, we focus on the
evaluation of convolutional neural networks that use jointly SAR and optical
images to retrieve the missing contents in one single polluted optical image.
We propose a simple framework that ease the creation of datasets for the
training of deep nets targeting optical image reconstruction, and for the
validation of machine learning based or deterministic approaches. These methods
are quite different in terms of input images constraints, and comparing them is
a problematic task not addressed in the literature. We show how space
partitioning data structures help to query samples in terms of cloud coverage,
relative acquisition date, pixel validity and relative proximity between SAR
and optical images. We generate several datasets to compare the reconstructed
images from networks that use a single pair of SAR and optical image, versus
networks that use multiple pairs, and a traditional deterministic approach
performing interpolation in temporal domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Fast and Efficient Conditional Learning for Tunable Trade-Off between Accuracy and Robustness. (arXiv:2204.00426v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00426">
<div class="article-summary-box-inner">
<span><p>Existing models that achieve state-of-the-art (SOTA) performance on both
clean and adversarially-perturbed images rely on convolution operations
conditioned with feature-wise linear modulation (FiLM) layers. These layers
require many new parameters and are hyperparameter sensitive. They
significantly increase training time, memory cost, and potential latency which
can prove costly for resource-limited or real-time applications. In this paper,
we present a fast learnable once-for-all adversarial training (FLOAT)
algorithm, which instead of the existing FiLM-based conditioning, presents a
unique weight conditioned learning that requires no additional layer, thereby
incurring no significant increase in parameter count, training time, or network
latency compared to standard adversarial training. In particular, we add
configurable scaled noise to the weight tensors that enables a trade-off
between clean and adversarial performance. Extensive experiments show that
FLOAT can yield SOTA performance improving both clean and perturbed image
classification by up to ~6% and ~10%, respectively. Moreover, real hardware
measurement shows that FLOAT can reduce the training time by up to 1.43x with
fewer model parameters of up to 1.47x on iso-hyperparameter settings compared
to the FiLM-based alternatives. Additionally, to further improve memory
efficiency we introduce FLOAT sparse (FLOATS), a form of non-iterative model
pruning and provide detailed empirical analysis to provide a three way
accuracy-robustness-complexity trade-off for these new class of pruned
conditionally trained models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Marginal Contrastive Correspondence for Guided Image Generation. (arXiv:2204.00442v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00442">
<div class="article-summary-box-inner">
<span><p>Exemplar-based image translation establishes dense correspondences between a
conditional input and an exemplar (from two different domains) for leveraging
detailed exemplar styles to achieve realistic image translation. Existing work
builds the cross-domain correspondences implicitly by minimizing feature-wise
distances across the two domains. Without explicit exploitation of
domain-invariant features, this approach may not reduce the domain gap
effectively which often leads to sub-optimal correspondences and image
translation. We design a Marginal Contrastive Learning Network (MCL-Net) that
explores contrastive learning to learn domain-invariant features for realistic
exemplar-based image translation. Specifically, we design an innovative
marginal contrastive loss that guides to establish dense correspondences
explicitly. Nevertheless, building correspondence with domain-invariant
semantics alone may impair the texture patterns and lead to degraded texture
generation. We thus design a Self-Correlation Map (SCM) that incorporates scene
structures as auxiliary information which improves the built correspondences
substantially. Quantitative and qualitative experiments on multifarious image
translation tasks show that the proposed method outperforms the
state-of-the-art consistently.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision Transformer with Cross-attention by Temporal Shift for Efficient Action Recognition. (arXiv:2204.00452v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00452">
<div class="article-summary-box-inner">
<span><p>We propose Multi-head Self/Cross-Attention (MSCA), which introduces a
temporal cross-attention mechanism for action recognition, based on the
structure of the Multi-head Self-Attention (MSA) mechanism of the Vision
Transformer (ViT). Simply applying ViT to each frame of a video frame can
capture frame features, but cannot model temporal features. However, simply
modeling temporal information with CNN or Transfomer is computationally
expensive. TSM that perform feature shifting assume a CNN and cannot take
advantage of the ViT structure. The proposed model captures temporal
information by shifting the Query, Key, and Value in the calculation of MSA of
ViT. This is efficient without additional coinformationmputational effort and
is a suitable structure for extending ViT over temporal. Experiments on
Kineitcs400 show the effectiveness of the proposed method and its superiority
over previous methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Autonomous crater detection on asteroids using a fully-convolutional neural network. (arXiv:2204.00477v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00477">
<div class="article-summary-box-inner">
<span><p>This paper shows the application of autonomous Crater Detection using the
U-Net, a Fully-Convolutional Neural Network, on Ceres. The U-Net is trained on
optical images of the Moon Global Morphology Mosaic based on data collected by
the LRO and manual crater catalogues. The Moon-trained network will be tested
on Dawn optical images of Ceres: this task is accomplished by means of a
Transfer Learning (TL) approach. The trained model has been fine-tuned using
100, 500 and 1000 additional images of Ceres. The test performance was measured
on 350 never before seen images, reaching a testing accuracy of 96.24%, 96.95%
and 97.19%, respectively. This means that despite the intrinsic differences
between the Moon and Ceres, TL works with encouraging results. The output of
the U-Net contains predicted craters: it will be post-processed applying global
thresholding for image binarization and a template matching algorithm to
extract craters positions and radii in the pixel space. Post-processed craters
will be counted and compared to the ground truth data in order to compute image
segmentation metrics: precision, recall and F1 score. These indices will be
computed, and their effect will be discussed for tasks such as automated crater
cataloguing and optical navigation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Proper Reuse of Image Classification Features Improves Object Detection. (arXiv:2204.00484v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00484">
<div class="article-summary-box-inner">
<span><p>A common practice in transfer learning is to initialize the downstream model
weights by pre-training on a data-abundant upstream task. In object detection
specifically, the feature backbone is typically initialized with Imagenet
classifier weights and fine-tuned on the object detection task. Recent works
show this is not strictly necessary under longer training regimes and provide
recipes for training the backbone from scratch. We investigate the opposite
direction of this end-to-end training trend: we show that an extreme form of
knowledge preservation -- freezing the classifier-initialized backbone --
consistently improves many different detection models, and leads to
considerable resource savings. We hypothesize and corroborate experimentally
that the remaining detector components capacity and structure is a crucial
factor in leveraging the frozen backbone. Immediate applications of our
findings include performance improvements on hard cases like detection of
long-tail object classes and computational and memory resource savings that
contribute to making the field more accessible to researchers with access to
fewer computational resources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generic Event Boundary Captioning: A Benchmark for Status Changes Understanding. (arXiv:2204.00486v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00486">
<div class="article-summary-box-inner">
<span><p>Cognitive science has shown that humans perceive videos in terms of events
separated by state changes of dominant subjects. State changes trigger new
events and are one of the most useful among the large amount of redundant
information perceived. However, previous research focuses on the overall
understanding of segments without evaluating the fine-grained status changes
inside. In this paper, we introduce a new dataset called Kinetic-GEBC (Generic
Event Boundary Captioning). The dataset consists of over 170k boundaries
associated with captions describing status changes in the generic events in 12K
videos. Upon this new dataset, we propose three tasks supporting the
development of a more fine-grained, robust, and human-like understanding of
videos through status changes. We evaluate many representative baselines in our
dataset, where we also design a new TPD (Temporal-based Pairwise Difference)
Modeling method for current state-of-the-art backbones and achieve significant
performance improvements. Besides, the results show there are still formidable
challenges for current methods in the utilization of different granularities,
representation of visual difference, and the accurate localization of status
changes. Further analysis shows that our dataset can drive developing more
powerful methods to understand status changes and thus improve video level
comprehension.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FrequencyLowCut Pooling -- Plug & Play against Catastrophic Overfitting. (arXiv:2204.00491v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00491">
<div class="article-summary-box-inner">
<span><p>Over the last years, Convolutional Neural Networks (CNNs) have been the
dominating neural architecture in a wide range of computer vision tasks. From
an image and signal processing point of view, this success might be a bit
surprising as the inherent spatial pyramid design of most CNNs is apparently
violating basic signal processing laws, i.e. Sampling Theorem in their
down-sampling operations. However, since poor sampling appeared not to affect
model accuracy, this issue has been broadly neglected until model robustness
started to receive more attention. Recent work [17] in the context of
adversarial attacks and distribution shifts, showed after all, that there is a
strong correlation between the vulnerability of CNNs and aliasing artifacts
induced by poor down-sampling operations. This paper builds on these findings
and introduces an aliasing free down-sampling operation which can easily be
plugged into any CNN architecture: FrequencyLowCut pooling. Our experiments
show, that in combination with simple and fast FGSM adversarial training, our
hyper-parameter free operator significantly improves model robustness and
avoids catastrophic overfitting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DFNet: Enhance Aboslute Pose Regression with Direct Feature Matching. (arXiv:2204.00559v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00559">
<div class="article-summary-box-inner">
<span><p>We introduce a camera relocalization pipeline that combines absolute pose
regression (APR) and direct feature matching. Existing photometric-based
methods have trouble on scenes with large photometric distortions, e.g. outdoor
environments. By incorporating an exposure-adaptive novel view synthesis, our
methods can successfully address the challenges. Moreover, by introducing
domain-invariant feature matching, our solution can improve pose regression
accuracy while using semi-supervised learning on unlabeled data. In particular,
the pipeline consists of two components, Novel View Synthesizer and FeatureNet
(DFNet). The former synthesizes novel views compensating for changes in
exposure and the latter regresses camera poses and extracts robust features
that bridge the domain gap between real images and synthetic ones. We show that
domain invariant feature matching effectively enhances camera pose estimation
both in indoor and outdoor scenes. Hence, our method achieves a
state-of-the-art accuracy by outperforming existing single-image APR methods by
as much as 56%, comparable to 3D structure-based methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Connect, Not Collapse: Explaining Contrastive Learning for Unsupervised Domain Adaptation. (arXiv:2204.00570v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00570">
<div class="article-summary-box-inner">
<span><p>We consider unsupervised domain adaptation (UDA), where labeled data from a
source domain (e.g., photographs) and unlabeled data from a target domain
(e.g., sketches) are used to learn a classifier for the target domain.
Conventional UDA methods (e.g., domain adversarial training) learn
domain-invariant features to improve generalization to the target domain. In
this paper, we show that contrastive pre-training, which learns features on
unlabeled source and target data and then fine-tunes on labeled source data, is
competitive with strong UDA methods. However, we find that contrastive
pre-training does not learn domain-invariant features, diverging from
conventional UDA intuitions. We show theoretically that contrastive
pre-training can learn features that vary subtantially across domains but still
generalize to the target domain, by disentangling domain and class information.
Our results suggest that domain invariance is not necessary for UDA. We
empirically validate our theory on benchmark vision datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast and Automatic Object Registration for Human-Robot Collaboration in Industrial Manufacturing. (arXiv:2204.00597v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00597">
<div class="article-summary-box-inner">
<span><p>We present an end-to-end framework for fast retraining of object detection
models in human-robot-collaboration. Our Faster R-CNN based setup covers the
whole workflow of automatic image generation and labeling, model retraining
on-site as well as inference on a FPGA edge device. The intervention of a human
operator reduces to providing the new object together with its label and
starting the training process. Moreover, we present a new loss, the
intraspread-objectosphere loss, to tackle the problem of open world
recognition. Though it fails to completely solve the problem, it significantly
reduces the number of false positive detections of unknown objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language. (arXiv:2204.00598v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00598">
<div class="article-summary-box-inner">
<span><p>Large foundation models can exhibit unique capabilities depending on the
domain of data they are trained on. While these domains are generic, they may
only barely overlap. For example, visual-language models (VLMs) are trained on
Internet-scale image captions, but large language models (LMs) are further
trained on Internet-scale text with no images (e.g. from spreadsheets, to SAT
questions). As a result, these models store different forms of commonsense
knowledge across different domains. In this work, we show that this model
diversity is symbiotic, and can be leveraged to build AI systems with
structured Socratic dialogue -- in which new multimodal tasks are formulated as
a guided language-based exchange between different pre-existing foundation
models, without additional finetuning. In the context of egocentric perception,
we present a case study of Socratic Models (SMs) that can provide meaningful
results for complex tasks such as generating free-form answers to contextual
questions about egocentric video, by formulating video Q&amp;A as short story Q&amp;A,
i.e. summarizing the video into a short story, then answering questions about
it. Additionally, SMs can generate captions for Internet images, and are
competitive with state-of-the-art on zero-shot video-to-text retrieval with
42.8 R@1 on MSR-VTT 1k-A. SMs demonstrate how to compose foundation models
zero-shot to capture new multimodal functionalities, without domain-specific
data collection. Prototypes are available at socraticmodels.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantized GAN for Complex Music Generation from Dance Videos. (arXiv:2204.00604v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00604">
<div class="article-summary-box-inner">
<span><p>We present Dance2Music-GAN (D2M-GAN), a novel adversarial multi-modal
framework that generates complex musical samples conditioned on dance videos.
Our proposed framework takes dance video frames and human body motion as input,
and learns to generate music samples that plausibly accompany the corresponding
input. Unlike most existing conditional music generation works that generate
specific types of mono-instrumental sounds using symbolic audio representations
(e.g., MIDI), and that heavily rely on pre-defined musical synthesizers, in
this work we generate dance music in complex styles (e.g., pop, breakdancing,
etc.) by employing a Vector Quantized (VQ) audio representation, and leverage
both its generality and the high abstraction capacity of its symbolic and
continuous counterparts. By performing an extensive set of experiments on
multiple datasets, and following a comprehensive evaluation protocol, we assess
the generative quality of our approach against several alternatives. The
quantitative results, which measure the music consistency, beats
correspondence, and music diversity, clearly demonstrate the effectiveness of
our proposed method. Last but not least, we curate a challenging dance-music
dataset of in-the-wild TikTok videos, which we use to further demonstrate the
efficacy of our approach in real-world applications - and which we hope to
serve as a starting point for relevant future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Importance of Asymmetry for Siamese Representation Learning. (arXiv:2204.00613v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00613">
<div class="article-summary-box-inner">
<span><p>Many recent self-supervised frameworks for visual representation learning are
based on certain forms of Siamese networks. Such networks are conceptually
symmetric with two parallel encoders, but often practically asymmetric as
numerous mechanisms are devised to break the symmetry. In this work, we conduct
a formal study on the importance of asymmetry by explicitly distinguishing the
two encoders within the network -- one produces source encodings and the other
targets. Our key insight is keeping a relatively lower variance in target than
source generally benefits learning. This is empirically justified by our
results from five case studies covering different variance-oriented designs,
and is aligned with our preliminary theoretical analysis on the baseline.
Moreover, we find the improvements from asymmetric designs generalize well to
longer training schedules, multiple other frameworks and newer backbones.
Finally, the combined effect of several asymmetric designs achieves a
state-of-the-art accuracy on ImageNet linear probing and competitive results on
downstream transfer. We hope our exploration will inspire more research in
exploiting asymmetry for Siamese representation learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simplicial Embeddings in Self-Supervised Learning and Downstream Classification. (arXiv:2204.00616v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00616">
<div class="article-summary-box-inner">
<span><p>We introduce Simplicial Embeddings (SEMs) as a way to constrain the encoded
representations of a self-supervised model to $L$ simplices of $V$ dimensions
each using a Softmax operation. This procedure imposes a structure on the
representations that reduce their expressivity for training downstream
classifiers, which helps them generalize better. Specifically, we show that the
temperature $\tau$ of the Softmax operation controls for the SEM
representation's expressivity, allowing us to derive a tighter downstream
classifier generalization bound than that for classifiers using unnormalized
representations. We empirically demonstrate that SEMs considerably improve
generalization on natural image datasets such as CIFAR-100 and ImageNet.
Finally, we also present evidence of the emergence of semantically relevant
features in SEMs, a pattern that is absent from baseline self-supervised
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified Probabilistic Deep Continual Learning through Generative Replay and Open Set Recognition. (arXiv:1905.12019v5 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1905.12019">
<div class="article-summary-box-inner">
<span><p>Modern deep neural networks are well known to be brittle in the face of
unknown data instances and recognition of the latter remains a challenge.
Although it is inevitable for continual-learning systems to encounter such
unseen concepts, the corresponding literature appears to nonetheless focus
primarily on alleviating catastrophic interference with learned
representations. In this work, we introduce a probabilistic approach that
connects these perspectives based on variational inference in a single deep
autoencoder model. Specifically, we propose to bound the approximate posterior
by fitting regions of high density on the basis of correctly classified data
points. These bounds are shown to serve a dual purpose: unseen unknown
out-of-distribution data can be distinguished from already trained known tasks
towards robust application. Simultaneously, to retain already acquired
knowledge, a generative replay process can be narrowed to strictly
in-distribution samples, in order to significantly alleviate catastrophic
interference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Night-time Scene Parsing with a Large Real Dataset. (arXiv:2003.06883v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.06883">
<div class="article-summary-box-inner">
<span><p>Although huge progress has been made on scene analysis in recent years, most
existing works assume the input images to be in day-time with good lighting
conditions. In this work, we aim to address the night-time scene parsing (NTSP)
problem, which has two main challenges: 1) labeled night-time data are scarce,
and 2) over- and under-exposures may co-occur in the input night-time images
and are not explicitly modeled in existing pipelines. To tackle the scarcity of
night-time data, we collect a novel labeled dataset, named {\it NightCity}, of
4,297 real night-time images with ground truth pixel-level semantic
annotations. To our knowledge, NightCity is the largest dataset for NTSP. In
addition, we also propose an exposure-aware framework to address the NTSP
problem through augmenting the segmentation process with explicitly learned
exposure features. Extensive experiments show that training on NightCity can
significantly improve NTSP performances and that our exposure-aware model
outperforms the state-of-the-art methods, yielding top performances on our
dataset as well as existing datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CNN-Based Image Reconstruction Method for Ultrafast Ultrasound Imaging. (arXiv:2008.12750v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.12750">
<div class="article-summary-box-inner">
<span><p>Ultrafast ultrasound (US) revolutionized biomedical imaging with its
capability of acquiring full-view frames at over 1 kHz, unlocking breakthrough
modalities such as shear-wave elastography and functional US neuroimaging. Yet,
it suffers from strong diffraction artifacts, mainly caused by grating lobes,
side lobes, or edge waves. Multiple acquisitions are typically required to
obtain a sufficient image quality, at the cost of a reduced frame rate. To
answer the increasing demand for high-quality imaging from single unfocused
acquisitions, we propose a two-step convolutional neural network (CNN)-based
image reconstruction method, compatible with real-time imaging. A low-quality
estimate is obtained by means of a backprojection-based operation, akin to
conventional delay-and-sum beamforming, from which a high-quality image is
restored using a residual CNN with multiscale and multichannel filtering
properties, trained specifically to remove the diffraction artifacts inherent
to ultrafast US imaging. To account for both the high dynamic range and the
oscillating properties of radio frequency US images, we introduce the mean
signed logarithmic absolute error (MSLAE) as a training loss function.
Experiments were conducted with a linear transducer array, in single plane-wave
(PW) imaging. Trainings were performed on a simulated dataset, crafted to
contain a wide diversity of structures and echogenicities. Extensive numerical
evaluations demonstrate that the proposed approach can reconstruct images from
single PWs with a quality similar to that of gold-standard synthetic aperture
imaging, on a dynamic range in excess of 60 dB. In vitro and in vivo
experiments show that trainings carried out on simulated data perform well in
experimental settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Globetrotter: Connecting Languages by Connecting Images. (arXiv:2012.04631v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.04631">
<div class="article-summary-box-inner">
<span><p>Machine translation between many languages at once is highly challenging,
since training with ground truth requires supervision between all language
pairs, which is difficult to obtain. Our key insight is that, while languages
may vary drastically, the underlying visual appearance of the world remains
consistent. We introduce a method that uses visual observations to bridge the
gap between languages, rather than relying on parallel corpora or topological
properties of the representations. We train a model that aligns segments of
text from different languages if and only if the images associated with them
are similar and each image in turn is well-aligned with its textual
description. We train our model from scratch on a new dataset of text in over
fifty languages with accompanying images. Experiments show that our method
outperforms previous work on unsupervised word and sentence translation using
retrieval. Code, models and data are available on globetrotter.cs.columbia.edu.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spectral Analysis for Semantic Segmentation with Applications on Feature Truncation and Weak Annotation. (arXiv:2012.14123v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.14123">
<div class="article-summary-box-inner">
<span><p>We propose spectral analysis to investigate the correlation between the
accuracy and the resolution of segmentation maps for semantic segmentation. The
current networks predict segmentation maps on the down-sampled grid of images
to alleviate the computational cost. Moreover, these networks can be trained by
weak annotations that utilize only the coarse contour of segmentation maps.
Despite the successful achievement of these works utilizing the low-frequency
information of segmentation maps, however, the accuracy of resultant
segmentation maps may also be degraded in the regions near object boundaries.
It is yet unclear for a theoretical guideline to determine an optimal
down-sampled grid to strike the balance between the cost and the accuracy of
segmentation. We analyze the objective function (cross-entropy) and network
back-propagation process in frequency domain. We discover that cross-entropy
and key features of CNN are mainly contributed by the low-frequency components
of segmentation maps. This further provides us quantitative results to
determine the efficacy of down-sampled grid of segmentation maps. The analysis
is then validated on the two applications: the feature truncation method and
the block-wise annotation that limit the high-frequency components of the CNN
features and annotation, respectively. The results agree with our analysis.
Thus the success of the existing work utilizing low-frequency information of
segmentation maps now has theoretical foundation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ROAD: The ROad event Awareness Dataset for Autonomous Driving. (arXiv:2102.11585v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.11585">
<div class="article-summary-box-inner">
<span><p>Humans drive in a holistic fashion which entails, in particular,
understanding dynamic road events and their evolution. Injecting these
capabilities in autonomous vehicles can thus take situational awareness and
decision making closer to human-level performance. To this purpose, we
introduce the ROad event Awareness Dataset (ROAD) for Autonomous Driving, to
our knowledge the first of its kind. ROAD is designed to test an autonomous
vehicle's ability to detect road events, defined as triplets composed by an
active agent, the action(s) it performs and the corresponding scene locations.
ROAD comprises videos originally from the Oxford RobotCar Dataset annotated
with bounding boxes showing the location in the image plane of each road event.
We benchmark various detection tasks, proposing as a baseline a new incremental
algorithm for online road event awareness termed 3D-RetinaNet. We also report
the performance on the ROAD tasks of Slowfast and YOLOv5 detectors, as well as
that of the winners of the ICCV2021 ROAD challenge, which highlight the
challenges faced by situation awareness in autonomous driving. ROAD is designed
to allow scholars to investigate exciting tasks such as complex (road) activity
detection, future event anticipation and continual learning. The dataset is
available at https://github.com/gurkirt/road-dataset; the baseline can be found
at https://github.com/gurkirt/3D-RetinaNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Low-Rank Simplicity Bias in Deep Networks. (arXiv:2103.10427v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.10427">
<div class="article-summary-box-inner">
<span><p>Modern deep neural networks are highly over-parameterized compared to the
data on which they are trained, yet they often generalize remarkably well. A
flurry of recent work has asked: why do deep networks not overfit to their
training data? In this work, we make a series of empirical observations that
investigate and extend the hypothesis that deeper networks are inductively
biased to find solutions with lower effective rank embeddings. We conjecture
that this bias exists because the volume of functions that maps to low
effective rank embedding increases with depth. We show empirically that our
claim holds true on finite width linear and non-linear models on practical
learning paradigms and show that on natural data, these are often the solutions
that generalize well. We then show that the simplicity bias exists at both
initialization and after training and is resilient to hyper-parameters and
learning methods. We further demonstrate how linear over-parameterization of
deep non-linear models can be used to induce low-rank bias, improving
generalization performance on CIFAR and ImageNet without changing the modeling
capacity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparing Correspondences: Video Prediction with Correspondence-wise Losses. (arXiv:2104.09498v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.09498">
<div class="article-summary-box-inner">
<span><p>Image prediction methods often struggle on tasks that require changing the
positions of objects, such as video prediction, producing blurry images that
average over the many positions that objects might occupy. In this paper, we
propose a simple change to existing image similarity metrics that makes them
more robust to positional errors: we match the images using optical flow, then
measure the visual similarity of corresponding pixels. This change leads to
crisper and more perceptually accurate predictions, and does not require
modifications to the image prediction network. We apply our method to a variety
of video prediction tasks, where it obtains strong performance with simple
network architectures, and to the closely related task of video interpolation.
Code and results are available at our webpage:
https://dangeng.github.io/CorrWiseLosses
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StyTr$^2$: Image Style Transfer with Transformers. (arXiv:2105.14576v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14576">
<div class="article-summary-box-inner">
<span><p>The goal of image style transfer is to render an image with artistic features
guided by a style reference while maintaining the original content. Owing to
the locality in convolutional neural networks (CNNs), extracting and
maintaining the global information of input images is difficult. Therefore,
traditional neural style transfer methods face biased content representation.
To address this critical issue, we take long-range dependencies of input images
into account for image style transfer by proposing a transformer-based approach
called StyTr$^2$. In contrast with visual transformers for other vision tasks,
StyTr$^2$ contains two different transformer encoders to generate
domain-specific sequences for content and style, respectively. Following the
encoders, a multi-layer transformer decoder is adopted to stylize the content
sequence according to the style sequence. We also analyze the deficiency of
existing positional encoding methods and propose the content-aware positional
encoding (CAPE), which is scale-invariant and more suitable for image style
transfer tasks. Qualitative and quantitative experiments demonstrate the
effectiveness of the proposed StyTr$^2$ compared with state-of-the-art
CNN-based and flow-based approaches. Code and models are available at
https://github.com/diyiiyiii/StyTR-2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tackling the Challenges in Scene Graph Generation with Local-to-Global Interactions. (arXiv:2106.08543v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.08543">
<div class="article-summary-box-inner">
<span><p>In this work, we seek new insights into the underlying challenges of the
Scene Graph Generation (SGG) task. Quantitative and qualitative analysis of the
Visual Genome dataset implies -- 1) Ambiguity: even if inter-object
relationship contains the same object (or predicate), they may not be visually
or semantically similar, 2) Asymmetry: despite the nature of the relationship
that embodied the direction, it was not well addressed in previous studies, and
3) Higher-order contexts: leveraging the identities of certain graph elements
can help to generate accurate scene graphs. Motivated by the analysis, we
design a novel SGG framework, Local-to-Global Interaction Networks (LOGIN).
Locally, interactions extract the essence between three instances of subject,
object, and background, while baking direction awareness into the network by
explicitly constraining the input order of subject and object. Globally,
interactions encode the contexts between every graph component (i.e., nodes and
edges). Finally, Attract &amp; Repel loss is utilized to fine-tune the distribution
of predicate embeddings. By design, our framework enables predicting the scene
graph in a bottom-up manner, leveraging the possible complementariness. To
quantify how much LOGIN is aware of relational direction, a new diagnostic task
called Bidirectional Relationship Classification (BRC) is also proposed.
Experimental results demonstrate that LOGIN can successfully distinguish
relational direction than existing methods (in BRC task), while showing
state-of-the-art results on the Visual Genome benchmark (in SGG task).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evolving Image Compositions for Feature Representation Learning. (arXiv:2106.09011v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.09011">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks for visual recognition require large amounts of
training samples and usually benefit from data augmentation. This paper
proposes PatchMix, a data augmentation method that creates new samples by
composing patches from pairs of images in a grid-like pattern. These new
samples are assigned label scores that are proportional to the number of
patches borrowed from each image. We then add a set of additional losses at the
patch-level to regularize and to encourage good representations at both the
patch and image levels. A ResNet-50 model trained on ImageNet using PatchMix
exhibits superior transfer learning capabilities across a wide array of
benchmarks. Although PatchMix can rely on random pairings and random grid-like
patterns for mixing, we explore evolutionary search as a guiding strategy to
jointly discover optimal grid-like patterns and image pairings. For this
purpose, we conceive a fitness function that bypasses the need to re-train a
model to evaluate each possible choice. In this way, PatchMix outperforms a
base model on CIFAR-10 (+1.91), CIFAR-100 (+5.31), Tiny Imagenet (+3.52), and
ImageNet (+1.16).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Probing: Cognitive Framework for Explaining Self-Supervised Image Representations. (arXiv:2106.11054v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.11054">
<div class="article-summary-box-inner">
<span><p>Recently introduced self-supervised methods for image representation learning
provide on par or superior results to their fully supervised competitors, yet
the corresponding efforts to explain the self-supervised approaches lag behind.
Motivated by this observation, we introduce a novel visual probing framework
for explaining the self-supervised models by leveraging probing tasks employed
previously in natural language processing. The probing tasks require knowledge
about semantic relationships between image parts. Hence, we propose a
systematic approach to obtain analogs of natural language in vision, such as
visual words, context, and taxonomy. Our proposal is grounded in Marr's
computational theory of vision and concerns features like textures, shapes, and
lines. We show the effectiveness and applicability of those analogs in the
context of explaining self-supervised representations. Our key findings
emphasize that relations between language and vision can serve as an effective
yet intuitive tool for discovering how machine learning models work,
independently of data modality. Our work opens a plethora of research pathways
towards more explainable and transparent AI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VAT-Mart: Learning Visual Action Trajectory Proposals for Manipulating 3D ARTiculated Objects. (arXiv:2106.14440v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.14440">
<div class="article-summary-box-inner">
<span><p>Perceiving and manipulating 3D articulated objects (e.g., cabinets, doors) in
human environments is an important yet challenging task for future
home-assistant robots. The space of 3D articulated objects is exceptionally
rich in their myriad semantic categories, diverse shape geometry, and
complicated part functionality. Previous works mostly abstract kinematic
structure with estimated joint parameters and part poses as the visual
representations for manipulating 3D articulated objects. In this paper, we
propose object-centric actionable visual priors as a novel
perception-interaction handshaking point that the perception system outputs
more actionable guidance than kinematic structure estimation, by predicting
dense geometry-aware, interaction-aware, and task-aware visual action
affordance and trajectory proposals. We design an interaction-for-perception
framework VAT-Mart to learn such actionable visual representations by
simultaneously training a curiosity-driven reinforcement learning policy
exploring diverse interaction trajectories and a perception module summarizing
and generalizing the explored knowledge for pointwise predictions among diverse
shapes. Experiments prove the effectiveness of the proposed approach using the
large-scale PartNet-Mobility dataset in SAPIEN environment and show promising
generalization capabilities to novel test shapes, unseen object categories, and
real-world data. Project page: https://hyperplane-lab.github.io/vat-mart
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pro-UIGAN: Progressive Face Hallucination from Occluded Thumbnails. (arXiv:2108.00602v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.00602">
<div class="article-summary-box-inner">
<span><p>In this paper, we study the task of hallucinating an authentic
high-resolution (HR) face from an occluded thumbnail. We propose a multi-stage
Progressive Upsampling and Inpainting Generative Adversarial Network, dubbed
Pro-UIGAN, which exploits facial geometry priors to replenish and upsample (8*)
the occluded and tiny faces (16*16 pixels). Pro-UIGAN iteratively (1) estimates
facial geometry priors for low-resolution (LR) faces and (2) acquires
non-occluded HR face images under the guidance of the estimated priors. Our
multi-stage hallucination network super-resolves and inpaints occluded LR faces
in a coarse-to-fine manner, thus reducing unwanted blurriness and artifacts
significantly. Specifically, we design a novel cross-modal transformer module
for facial priors estimation, in which an input face and its landmark features
are formulated as queries and keys, respectively. Such a design encourages
joint feature learning across the input facial and landmark features, and deep
feature correspondences will be discovered by attention. Thus, facial
appearance features and facial geometry priors are learned in a mutual
promotion manner. Extensive experiments demonstrate that our Pro-UIGAN achieves
visually pleasing HR faces, reaching superior performance in downstream tasks,
i.e., face alignment, face parsing, face recognition and expression
classification, compared with other state-of-the-art (SotA) methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MmWave Radar and Vision Fusion for Object Detection in Autonomous Driving: A Review. (arXiv:2108.03004v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03004">
<div class="article-summary-box-inner">
<span><p>With autonomous driving developing in a booming stage, accurate object
detection in complex scenarios attract wide attention to ensure the safety of
autonomous driving. Millimeter wave (mmWave) radar and vision fusion is a
mainstream solution for accurate obstacle detection. This article presents a
detailed survey on mmWave radar and vision fusion based obstacle detection
methods. First, we introduce the tasks, evaluation criteria, and datasets of
object detection for autonomous driving. The process of mmWave radar and vision
fusion is then divided into three parts: sensor deployment, sensor calibration,
and sensor fusion, which are reviewed comprehensively. Specifically, we
classify the fusion methods into data level, decision level, and feature level
fusion methods. In addition, we introduce three-dimensional(3D) object
detection, the fusion of lidar and vision in autonomous driving and multimodal
information fusion, which are promising for the future. Finally, we summarize
this article.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Expert Adversarial Attack Detection in Person Re-identification Using Context Inconsistency. (arXiv:2108.09891v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09891">
<div class="article-summary-box-inner">
<span><p>The success of deep neural networks (DNNs) has promoted the widespread
applications of person re-identification (ReID). However, ReID systems inherit
the vulnerability of DNNs to malicious attacks of visually inconspicuous
adversarial perturbations. Detection of adversarial attacks is, therefore, a
fundamental requirement for robust ReID systems. In this work, we propose a
Multi-Expert Adversarial Attack Detection (MEAAD) approach to achieve this goal
by checking context inconsistency, which is suitable for any DNN-based ReID
systems. Specifically, three kinds of context inconsistencies caused by
adversarial attacks are employed to learn a detector for distinguishing the
perturbed examples, i.e., a) the embedding distances between a perturbed query
person image and its top-K retrievals are generally larger than those between a
benign query image and its top-K retrievals, b) the embedding distances among
the top-K retrievals of a perturbed query image are larger than those of a
benign query image, c) the top-K retrievals of a benign query image obtained
with multiple expert ReID models tend to be consistent, which is not preserved
when attacks are present. Extensive experiments on the Market1501 and
DukeMTMC-ReID datasets show that, as the first adversarial attack detection
approach for ReID, MEAAD effectively detects various adversarial attacks and
achieves high ROC-AUC (over 97.5%).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RSI-Net: Two-Stream Deep Neural Network for Remote Sensing Imagesbased Semantic Segmentation. (arXiv:2109.09148v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09148">
<div class="article-summary-box-inner">
<span><p>For semantic segmentation of remote sensing images (RSI), trade-off between
representation power and location accuracy is quite important. How to get the
trade-off effectively is an open question,where current approaches of utilizing
very deep models result in complex models with large memory consumption. In
contrast to previous work that utilizes dilated convolutions or deep models, we
propose a novel two-stream deep neural network for semantic segmentation of RSI
(RSI-Net) to obtain improved performance through modeling and propagating
spatial contextual structure effectively and a decoding scheme with image-level
and graph-level combination. The first component explicitly models correlations
between adjacent land covers and conduct flexible convolution on arbitrarily
irregular image regions by using graph convolutional network, while densely
connected atrous convolution network (DenseAtrousCNet) with multi-scale atrous
convolution can expand the receptive fields and obtain image global
information. Extensive experiments are implemented on the Vaihingen, Potsdam
and Gaofen RSI datasets, where the comparison results demonstrate the superior
performance of RSI-Net in terms of overall accuracy (91.83%, 93.31% and 93.67%
on three datasets, respectively), F1 score (90.3%, 91.49% and 89.35% on three
datasets, respectively) and kappa coefficient (89.46%, 90.46% and 90.37% on
three datasets, respectively) when compared with six state-of-the-art RSI
semantic segmentation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MC-LCR: Multi-modal contrastive classification by locally correlated representations for effective face forgery detection. (arXiv:2110.03290v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03290">
<div class="article-summary-box-inner">
<span><p>As the remarkable development of facial manipulation technologies is
accompanied by severe security concerns, face forgery detection has become a
recent research hotspot. Most existing detection methods train a binary
classifier under global supervision to judge real or fake. However, advanced
manipulations only perform small-scale tampering, posing challenges to
comprehensively capture subtle and local forgery artifacts, especially in high
compression settings and cross-dataset scenarios. To address such limitations,
we propose a novel framework named Multi-modal Contrastive Classification by
Locally Correlated Representations(MC-LCR), for effective face forgery
detection. Instead of specific appearance features, our MC-LCR aims to amplify
implicit local discrepancies between authentic and forged faces from both
spatial and frequency domains. Specifically, we design the shallow style
representation block that measures the pairwise correlation of shallow feature
maps, which encodes local style information to extract more discriminative
features in the spatial domain. Moreover, we make a key observation that subtle
forgery artifacts can be further exposed in the patch-wise phase and amplitude
spectrum and exhibit different clues. According to the complementarity of
amplitude and phase information, we develop a patch-wise amplitude and phase
dual attention module to capture locally correlated inconsistencies with each
other in the frequency domain. Besides the above two modules, we further
introduce the collaboration of supervised contrastive loss with cross-entropy
loss. It helps the network learn more discriminative and generalized
representations. Through extensive experiments and comprehensive studies, we
achieve state-of-the-art performance and demonstrate the robustness and
generalization of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Optimal Correlational Object Search. (arXiv:2110.09991v3 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09991">
<div class="article-summary-box-inner">
<span><p>In realistic applications of object search, robots will need to locate target
objects in complex environments while coping with unreliable sensors,
especially for small or hard-to-detect objects. In such settings, correlational
information can be valuable for planning efficiently. Previous approaches that
consider correlational information typically resort to ad-hoc, greedy search
strategies. We introduce the Correlational Object Search POMDP (COS-POMDP),
which models correlations while preserving optimal solutions with a reduced
state space. We propose a hierarchical planning algorithm to scale up
COS-POMDPs for practical domains. Our evaluation, conducted with the AI2-THOR
household simulator and the YOLOv5 object detector, shows that our method finds
objects more successfully and efficiently compared to baselines,particularly
for hard-to-detect objects such as srub brush and remote control.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simple Baseline for Low-Budget Active Learning. (arXiv:2110.12033v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12033">
<div class="article-summary-box-inner">
<span><p>Active learning focuses on choosing a subset of unlabeled data to be labeled.
However, most such methods assume that a large subset of the data can be
annotated. We are interested in low-budget active learning where only a small
subset (e.g., 0.2% of ImageNet) can be annotated. Instead of proposing a new
query strategy to iteratively sample batches of unlabeled data given an initial
pool, we learn rich features by an off-the-shelf self-supervised learning
method only once, and then study the effectiveness of different sampling
strategies given a low labeling budget on a variety of datasets including
ImageNet. We show that although the state-of-the-art active learning methods
work well given a large labeling budget, a simple K-means clustering algorithm
can outperform them on low budgets. We believe this method can be used as a
simple baseline for low-budget active learning on image classification. Code is
available at: https://github.com/UCDvision/low-budget-al
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Less is More: Generating Grounded Navigation Instructions from Landmarks. (arXiv:2111.12872v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12872">
<div class="article-summary-box-inner">
<span><p>We study the automatic generation of navigation instructions from 360-degree
images captured on indoor routes. Existing generators suffer from poor visual
grounding, causing them to rely on language priors and hallucinate objects. Our
MARKY-MT5 system addresses this by focusing on visual landmarks; it comprises a
first stage landmark detector and a second stage generator -- a multimodal,
multilingual, multitask encoder-decoder. To train it, we bootstrap grounded
landmark annotations on top of the Room-across-Room (RxR) dataset. Using text
parsers, weak supervision from RxR's pose traces, and a multilingual image-text
encoder trained on 1.8b images, we identify 1.1m English, Hindi and Telugu
landmark descriptions and ground them to specific regions in panoramas. On
Room-to-Room, human wayfinders obtain success rates (SR) of 71% following
MARKY-MT5's instructions, just shy of their 75% SR following human instructions
-- and well above SRs with other generators. Evaluations on RxR's longer,
diverse paths obtain 61-64% SRs on three languages. Generating such
high-quality navigation instructions in novel environments is a step towards
conversational navigation tools and could facilitate larger-scale training of
instruction-following agents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiway Non-rigid Point Cloud Registration via Learned Functional Map Synchronization. (arXiv:2111.12878v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12878">
<div class="article-summary-box-inner">
<span><p>We present SyNoRiM, a novel way to jointly register multiple non-rigid shapes
by synchronizing the maps relating learned functions defined on the point
clouds. Even though the ability to process non-rigid shapes is critical in
various applications ranging from computer animation to 3D digitization, the
literature still lacks a robust and flexible framework to match and align a
collection of real, noisy scans observed under occlusions. Given a set of such
point clouds, our method first computes the pairwise correspondences
parameterized via functional maps. We simultaneously learn potentially
non-orthogonal basis functions to effectively regularize the deformations,
while handling the occlusions in an elegant way. To maximally benefit from the
multi-way information provided by the inferred pairwise deformation fields, we
synchronize the pairwise functional maps into a cycle-consistent whole thanks
to our novel and principled optimization formulation. We demonstrate via
extensive experiments that our method achieves a state-of-the-art performance
in registration accuracy, while being flexible and efficient as we handle both
non-rigid and multi-body cases in a unified framework and avoid the costly
optimization over point-wise permutations by the use of basis function maps.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Fair Classifiers with Partially Annotated Group Labels. (arXiv:2111.14581v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14581">
<div class="article-summary-box-inner">
<span><p>Recently, fairness-aware learning have become increasingly crucial, but most
of those methods operate by assuming the availability of fully annotated
demographic group labels. We emphasize that such assumption is unrealistic for
real-world applications since group label annotations are expensive and can
conflict with privacy issues. In this paper, we consider a more practical
scenario, dubbed as Algorithmic Group Fairness with the Partially annotated
Group labels (Fair-PG). We observe that the existing methods to achieve group
fairness perform even worse than the vanilla training, which simply uses full
data only with target labels, under Fair-PG. To address this problem, we
propose a simple Confidence-based Group Label assignment (CGL) strategy that is
readily applicable to any fairness-aware learning method. CGL utilizes an
auxiliary group classifier to assign pseudo group labels, where random labels
are assigned to low confident samples. We first theoretically show that our
method design is better than the vanilla pseudo-labeling strategy in terms of
fairness criteria. Then, we empirically show on several benchmark datasets that
by combining CGL and the state-of-the-art fairness-aware in-processing methods,
the target accuracies and the fairness metrics can be jointly improved compared
to the baselines. Furthermore, we convincingly show that CGL enables to
naturally augment the given group-labeled dataset with external target
label-only datasets so that both accuracy and fairness can be improved. Code is
available at https://github.com/naver-ai/cgl_fairness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Diverse 3D Reconstructions from a Single Occluded Face Image. (arXiv:2112.00879v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00879">
<div class="article-summary-box-inner">
<span><p>Occlusions are a common occurrence in unconstrained face images. Single image
3D reconstruction from such face images often suffers from corruption due to
the presence of occlusions. Furthermore, while a plurality of 3D
reconstructions is plausible in the occluded regions, existing approaches are
limited to generating only a single solution. To address both of these
challenges, we present Diverse3DFace, which is specifically designed to
simultaneously generate a diverse and realistic set of 3D reconstructions from
a single occluded face image. It consists of three components: a global+local
shape fitting process, a graph neural network-based mesh VAE, and a
Determinantal Point Process based diversity promoting iterative optimization
procedure. Quantitative and qualitative comparisons of 3D reconstruction on
occluded faces show that Diverse3DFace can estimate 3D shapes that are
consistent with the visible regions in the target image while exhibiting high,
yet realistic, levels of diversity on the occluded regions. On face images
occluded by masks, glasses, and other random objects, Diverse3DFace generates a
distribution of 3D shapes having ~50% higher diversity on the occluded regions
compared to the baselines. Moreover, our closest sample to the ground truth has
~40% lower MSE than the singular reconstructions by existing approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incremental Learning in Semantic Segmentation from Image Labels. (arXiv:2112.01882v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01882">
<div class="article-summary-box-inner">
<span><p>Although existing semantic segmentation approaches achieve impressive
results, they still struggle to update their models incrementally as new
categories are uncovered. Furthermore, pixel-by-pixel annotations are expensive
and time-consuming. This paper proposes a novel framework for Weakly
Incremental Learning for Semantic Segmentation, that aims at learning to
segment new classes from cheap and largely available image-level labels. As
opposed to existing approaches, that need to generate pseudo-labels offline, we
use an auxiliary classifier, trained with image-level labels and regularized by
the segmentation model, to obtain pseudo-supervision online and update the
model incrementally. We cope with the inherent noise in the process by using
soft-labels generated by the auxiliary classifier. We demonstrate the
effectiveness of our approach on the Pascal VOC and COCO datasets,
outperforming offline weakly-supervised methods and obtaining results
comparable with incremental learning methods with full supervision. Code can be
found at https://github.com/fcdl94/WILSON.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">General Facial Representation Learning in a Visual-Linguistic Manner. (arXiv:2112.03109v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03109">
<div class="article-summary-box-inner">
<span><p>How to learn a universal facial representation that boosts all face analysis
tasks? This paper takes one step toward this goal. In this paper, we study the
transfer performance of pre-trained models on face analysis tasks and introduce
a framework, called FaRL, for general Facial Representation Learning in a
visual-linguistic manner. On one hand, the framework involves a contrastive
loss to learn high-level semantic meaning from image-text pairs. On the other
hand, we propose exploring low-level information simultaneously to further
enhance the face representation, by adding a masked image modeling. We perform
pre-training on LAION-FACE, a dataset containing large amount of face
image-text pairs, and evaluate the representation capability on multiple
downstream tasks. We show that FaRL achieves better transfer performance
compared with previous pre-trained models. We also verify its superiority in
the low-data regime. More importantly, our model surpasses the state-of-the-art
methods on face analysis tasks including face parsing and face alignment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Models are Continual Learners. (arXiv:2112.04215v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04215">
<div class="article-summary-box-inner">
<span><p>Self-supervised models have been shown to produce comparable or better visual
representations than their supervised counterparts when trained offline on
unlabeled data at scale. However, their efficacy is catastrophically reduced in
a Continual Learning (CL) scenario where data is presented to the model
sequentially. In this paper, we show that self-supervised loss functions can be
seamlessly converted into distillation mechanisms for CL by adding a predictor
network that maps the current state of the representations to their past state.
This enables us to devise a framework for Continual self-supervised visual
representation Learning that (i) significantly improves the quality of the
learned representations, (ii) is compatible with several state-of-the-art
self-supervised objectives, and (iii) needs little to no hyperparameter tuning.
We demonstrate the effectiveness of our approach empirically by training six
popular self-supervised models in various CL settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Progressive Attention on Multi-Level Dense Difference Maps for Generic Event Boundary Detection. (arXiv:2112.04771v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04771">
<div class="article-summary-box-inner">
<span><p>Generic event boundary detection is an important yet challenging task in
video understanding, which aims at detecting the moments where humans naturally
perceive event boundaries. The main challenge of this task is perceiving
various temporal variations of diverse event boundaries. To this end, this
paper presents an effective and end-to-end learnable framework (DDM-Net). To
tackle the diversity and complicated semantics of event boundaries, we make
three notable improvements. First, we construct a feature bank to store
multi-level features of space and time, prepared for difference calculation at
multiple scales. Second, to alleviate inadequate temporal modeling of previous
methods, we present dense difference maps (DDM) to comprehensively characterize
the motion pattern. Finally, we exploit progressive attention on multi-level
DDM to jointly aggregate appearance and motion clues. As a result, DDM-Net
respectively achieves a significant boost of 14% and 8% on Kinetics-GEBD and
TAPOS benchmark, and outperforms the top-1 winner solution of LOVEU
Challenge@CVPR 2021 without bells and whistles. The state-of-the-art result
demonstrates the effectiveness of richer motion representation and more
sophisticated aggregation, in handling the diversity of generic event boundary
detection. The code is made available at \url{https://github.com/MCG-NJU/DDM}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimedia Datasets for Anomaly Detection: A Review. (arXiv:2112.05410v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05410">
<div class="article-summary-box-inner">
<span><p>Multimedia anomaly datasets play a crucial role in automated surveillance.
They have a wide range of applications expanding from outlier objects/
situation detection to the detection of life-threatening events. For more than
1.5 decades, this field has attracted a lot of research attention, and as a
result, more and more datasets dedicated to anomalous actions and object
detection have been developed. Tapping these public anomaly datasets enable
researchers to generate and compare various anomaly detection frameworks with
the same input data. This paper presents a comprehensive survey on a variety of
video, audio, as well as audio-visual datasets based on the application of
anomaly detection. This survey aims to address the lack of a comprehensive
comparison and analysis of multimedia public datasets based on anomaly
detection. Also, it can assist researchers in selecting the best available
dataset for bench-marking frameworks. Additionally, we discuss gaps in the
existing dataset and insights for future direction towards developing
multimodal anomaly detection datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Keypoint Detection with Uncertainty Learning for Unseen Species. (arXiv:2112.06183v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06183">
<div class="article-summary-box-inner">
<span><p>Current non-rigid object keypoint detectors perform well on a chosen kind of
species and body parts, and require a large amount of labelled keypoints for
training. Moreover, their heatmaps, tailored to specific body parts, cannot
recognize novel keypoints (keypoints not labelled for training) on unseen
species. We raise an interesting yet challenging question: how to detect both
base (annotated for training) and novel keypoints for unseen species given a
few annotated samples? Thus, we propose a versatile Few-shot Keypoint Detection
(FSKD) pipeline, which can detect a varying number of keypoints of different
kinds. Our FSKD provides the uncertainty estimation of predicted keypoints.
Specifically, FSKD involves main and auxiliary keypoint representation
learning, similarity learning, and keypoint localization with uncertainty
modeling to tackle the localization noise. Moreover, we model the uncertainty
across groups of keypoints by multivariate Gaussian distribution to exploit
implicit correlations between neighboring keypoints. We show the effectiveness
of our FSKD on (i) novel keypoint detection for unseen species, and (ii)
few-shot Fine-Grained Visual Recognition (FGVR) and (iii) Semantic Alignment
(SA) downstream tasks. For FGVR, detected keypoints improve the classification
accuracy. For SA, we showcase a novel thin-plate-spline warping that uses
estimated keypoint uncertainty under imperfect keypoint corespondences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Manifold Learning Benefits GANs. (arXiv:2112.12618v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.12618">
<div class="article-summary-box-inner">
<span><p>In this paper, we improve Generative Adversarial Networks by incorporating a
manifold learning step into the discriminator. We consider locality-constrained
linear and subspace-based manifolds, and locality-constrained non-linear
manifolds. In our design, the manifold learning and coding steps are
intertwined with layers of the discriminator, with the goal of attracting
intermediate feature representations onto manifolds. We adaptively balance the
discrepancy between feature representations and their manifold view, which is a
trade-off between denoising on the manifold and refining the manifold. We find
that locality-constrained non-linear manifolds outperform linear manifolds due
to their non-uniform density and smoothness. We also substantially outperform
state-of-the-art baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using maps to predict economic activity. (arXiv:2112.13850v2 [econ.GN] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13850">
<div class="article-summary-box-inner">
<span><p>We introduce a novel machine learning approach to leverage historical and
contemporary maps and systematically predict economic statistics. Our simple
algorithm extracts meaningful features from the maps based on their color
compositions for predictions. We apply our method to grid-level population
levels in Sub-Saharan Africa in the 1950s and South Korea in 1930, 1970, and
2015. Our results show that maps can reliably predict population density in the
mid-20th century Sub-Saharan Africa using 9,886 map grids (5km by 5 km).
Similarly, contemporary South Korean maps can generate robust predictions on
income, consumption, employment, population density, and electric consumption.
In addition, our method is capable of predicting historical South Korean
population growth over a century.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ACDNet: Adaptively Combined Dilated Convolution for Monocular Panorama Depth Estimation. (arXiv:2112.14440v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.14440">
<div class="article-summary-box-inner">
<span><p>Depth estimation is a crucial step for 3D reconstruction with panorama images
in recent years. Panorama images maintain the complete spatial information but
introduce distortion with equirectangular projection. In this paper, we propose
an ACDNet based on the adaptively combined dilated convolution to predict the
dense depth map for a monocular panoramic image. Specifically, we combine the
convolution kernels with different dilations to extend the receptive field in
the equirectangular projection. Meanwhile, we introduce an adaptive
channel-wise fusion module to summarize the feature maps and get diverse
attention areas in the receptive field along the channels. Due to the
utilization of channel-wise attention in constructing the adaptive channel-wise
fusion module, the network can capture and leverage the cross-channel
contextual information efficiently. Finally, we conduct depth estimation
experiments on three datasets (both virtual and real-world) and the
experimental results demonstrate that our proposed ACDNet substantially
outperforms the current state-of-the-art (SOTA) methods. Our codes and model
parameters are accessed in https://github.com/zcq15/ACDNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Negative Evidence Matters in Interpretable Histology Image Classification. (arXiv:2201.02445v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02445">
<div class="article-summary-box-inner">
<span><p>Using only global image-class labels, weakly-supervised learning methods,
such as class activation mapping, allow training CNNs to jointly classify an
image, and locate regions of interest associated with the predicted class.
However, without any guidance at the pixel level, such methods may yield
inaccurate regions. This problem is known to be more challenging with histology
images than with natural ones, since objects are less salient, structures have
more variations, and foreground and background regions have stronger
similarities. Therefore, computer vision methods for visual interpretation of
CNNs may not directly apply. In this paper, a simple yet efficient method based
on a composite loss is proposed to learn information from the fully negative
samples (i.e., samples without positive regions), and thereby reduce false
positives/negatives. Our new loss function contains two complementary terms:
the first exploits positive evidence collected from the CNN classifier, while
the second leverages the fully negative samples from training data. In
particular, a pre-trained CNN is equipped with a decoder that allows refining
the regions of interest. The CNN is exploited to collect both positive and
negative evidence at the pixel level to train the decoder. Our method called
NEGEV benefits from the fully negative samples that naturally occur in the
data, without any additional supervision signals beyond image-class labels.
Extensive experiments show that our proposed method can substantial outperform
related state-of-art methods on GlaS (public benchmark for colon cancer), and
Camelyon16 (patch-based benchmark for breast cancer using three different
backbones). Our results highlight the benefits of using both positive and
negative evidence, the first obtained from a classifier, and the other
naturally available in datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AttentionHTR: Handwritten Text Recognition Based on Attention Encoder-Decoder Networks. (arXiv:2201.09390v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09390">
<div class="article-summary-box-inner">
<span><p>This work proposes an attention-based sequence-to-sequence model for
handwritten word recognition and explores transfer learning for data-efficient
training of HTR systems. To overcome training data scarcity, this work
leverages models pre-trained on scene text images as a starting point towards
tailoring the handwriting recognition models. ResNet feature extraction and
bidirectional LSTM-based sequence modeling stages together form an encoder. The
prediction stage consists of a decoder and a content-based attention mechanism.
The effectiveness of the proposed end-to-end HTR system has been empirically
evaluated on a novel multi-writer dataset Imgur5K and the IAM dataset. The
experimental results evaluate the performance of the HTR framework, further
supported by an in-depth analysis of the error cases. Source code and
pre-trained models are available at https://github.com/dmitrijsk/AttentionHTR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Paired Image to Image Translation for Strikethrough Removal From Handwritten Words. (arXiv:2201.09633v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09633">
<div class="article-summary-box-inner">
<span><p>Transcribing struck-through, handwritten words, for example for the purpose
of genetic criticism, can pose a challenge to both humans and machines, due to
the obstructive properties of the superimposed strokes. This paper investigates
the use of paired image to image translation approaches to remove strikethrough
strokes from handwritten words. Four different neural network architectures are
examined, ranging from a few simple convolutional layers to deeper ones,
employing Dense blocks. Experimental results, obtained from one synthetic and
one genuine paired strikethrough dataset, confirm that the proposed paired
models outperform the CycleGAN-based state of the art, while using less than a
sixth of the trainable parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Aladdin: Joint Atlas Building and Diffeomorphic Registration Learning with Pairwise Alignment. (arXiv:2202.03563v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03563">
<div class="article-summary-box-inner">
<span><p>Atlas building and image registration are important tasks for medical image
analysis. Once one or multiple atlases from an image population have been
constructed, commonly (1) images are warped into an atlas space to study
intra-subject or inter-subject variations or (2) a possibly probabilistic atlas
is warped into image space to assign anatomical labels. Atlas estimation and
nonparametric transformations are computationally expensive as they usually
require numerical optimization. Additionally, previous approaches for atlas
building often define similarity measures between a fuzzy atlas and each
individual image, which may cause alignment difficulties because a fuzzy atlas
does not exhibit clear anatomical structures in contrast to the individual
images. This work explores using a convolutional neural network (CNN) to
jointly predict the atlas and a stationary velocity field (SVF)
parameterization for diffeomorphic image registration with respect to the
atlas. Our approach does not require affine pre-registrations and utilizes
pairwise image alignment losses to increase registration accuracy. We evaluate
our model on 3D knee magnetic resonance images (MRI) from the OAI-ZIB dataset.
Our results show that the proposed framework achieves better performance than
other state-of-the-art image registration algorithms, allows for end-to-end
training, and for fast inference at test time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveling Down in Computer Vision: Pareto Inefficiencies in Fair Deep Classifiers. (arXiv:2203.04913v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04913">
<div class="article-summary-box-inner">
<span><p>Algorithmic fairness is frequently motivated in terms of a trade-off in which
overall performance is decreased so as to improve performance on disadvantaged
groups where the algorithm would otherwise be less accurate. Contrary to this,
we find that applying existing fairness approaches to computer vision improve
fairness by degrading the performance of classifiers across all groups (with
increased degradation on the best performing groups).
</p>
<p>Extending the bias-variance decomposition for classification to fairness, we
theoretically explain why the majority of fairness classifiers designed for low
capacity models should not be used in settings involving high-capacity models,
a scenario common to computer vision. We corroborate this analysis with
extensive experimental support that shows that many of the fairness heuristics
used in computer vision also degrade performance on the most disadvantaged
groups. Building on these insights, we propose an adaptive augmentation
strategy that, uniquely, of all methods tested, improves performance for the
disadvantaged groups.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial amplitude swap towards robust image classifiers. (arXiv:2203.07138v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07138">
<div class="article-summary-box-inner">
<span><p>The vulnerability of convolutional neural networks (CNNs) to image
perturbations such as common corruptions and adversarial perturbations has
recently been investigated from the perspective of frequency. In this study, we
investigate the effect of the amplitude and phase spectra of adversarial images
on the robustness of CNN classifiers. Extensive experiments revealed that the
images generated by combining the amplitude spectrum of adversarial images and
the phase spectrum of clean images accommodates moderate and general
perturbations, and training with these images equips a CNN classifier with more
general robustness, performing well under both common corruptions and
adversarial perturbations. We also found that two types of overfitting
(catastrophic overfitting and robust overfitting) can be circumvented by the
aforementioned spectrum recombination. We believe that these results contribute
to the understanding and the training of truly robust classifiers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diffusion Probabilistic Modeling for Video Generation. (arXiv:2203.09481v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09481">
<div class="article-summary-box-inner">
<span><p>Denoising diffusion probabilistic models are a promising new class of
generative models that are competitive with GANs on perceptual metrics. In this
paper, we explore their potential for sequentially generating video. Inspired
by recent advances in neural video compression, we use denoising diffusion
models to stochastically generate a residual to a deterministic next-frame
prediction. We compare this approach to two sequential VAE and two GAN
baselines on four datasets, where we test the generated frames for perceptual
quality and forecasting accuracy against ground truth frames. We find
significant improvements in terms of perceptual quality on all data and
improvements in terms of frame forecasting for complex high-resolution videos.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Portrait Delighting. (arXiv:2203.12088v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12088">
<div class="article-summary-box-inner">
<span><p>We present a deep neural network for removing undesirable shading features
from an unconstrained portrait image, recovering the underlying texture. Our
training scheme incorporates three regularization strategies: masked loss, to
emphasize high-frequency shading features; soft-shadow loss, which improves
sensitivity to subtle changes in lighting; and shading-offset estimation, to
supervise separation of shading and texture. Our method demonstrates improved
delighting quality and generalization when compared with the state-of-the-art.
We further demonstrate how our delighting method can enhance the performance of
light-sensitive computer vision tasks such as face relighting and semantic
parsing, allowing them to handle extreme lighting conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Learning of Adversarial Example: Towards Good Generalizations for Deepfake Detection. (arXiv:2203.12208v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12208">
<div class="article-summary-box-inner">
<span><p>Recent studies in deepfake detection have yielded promising results when the
training and testing face forgeries are from the same dataset. However, the
problem remains challenging when one tries to generalize the detector to
forgeries created by unseen methods in the training dataset. This work
addresses the generalizable deepfake detection from a simple principle: a
generalizable representation should be sensitive to diverse types of forgeries.
Following this principle, we propose to enrich the "diversity" of forgeries by
synthesizing augmented forgeries with a pool of forgery configurations and
strengthen the "sensitivity" to the forgeries by enforcing the model to predict
the forgery configurations. To effectively explore the large forgery
augmentation space, we further propose to use the adversarial training strategy
to dynamically synthesize the most challenging forgeries to the current model.
Through extensive experiments, we show that the proposed strategies are
surprisingly effective (see Figure 1), and they could achieve superior
performance than the current state-of-the-art methods. Code is available at
\url{https://github.com/liangchen527/SLADD}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the (Limited) Generalization of MasterFace Attacks and Its Relation to the Capacity of Face Representations. (arXiv:2203.12387v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12387">
<div class="article-summary-box-inner">
<span><p>A MasterFace is a face image that can successfully match against a large
portion of the population. Since their generation does not require access to
the information of the enrolled subjects, MasterFace attacks represent a
potential security risk for widely-used face recognition systems. Previous
works proposed methods for generating such images and demonstrated that these
attacks can strongly compromise face recognition. However, previous works
followed evaluation settings consisting of older recognition models, limited
cross-dataset and cross-model evaluations, and the use of low-scale testing
data. This makes it hard to state the generalizability of these attacks. In
this work, we comprehensively analyse the generalizability of MasterFace
attacks in empirical and theoretical investigations. The empirical
investigations include the use of six state-of-the-art FR models, cross-dataset
and cross-model evaluation protocols, and utilizing testing datasets of
significantly higher size and variance. The results indicate a low
generalizability when MasterFaces are training on a different face recognition
model than the one used for testing. In these cases, the attack performance is
similar to zero-effort imposter attacks. In the theoretical investigations, we
define and estimate the face capacity and the maximum MasterFace coverage under
the assumption that identities in the face space are well separated. The
current trend of increasing the fairness and generalizability in face
recognition indicates that the vulnerability of future systems might further
decrease. Future works might analyse the utility of MasterFaces for
understanding and enhancing the robustness of face recognition models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StructToken : Rethinking Semantic Segmentation with Structural Prior. (arXiv:2203.12612v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12612">
<div class="article-summary-box-inner">
<span><p>In this paper, we present structure token (StructToken), a new paradigm for
semantic segmentation. From a perspective on semantic segmentation as per-pixel
classification, the previous deep learning-based methods learn the per-pixel
representation first through an encoder and a decoder head and then classify
each pixel representation to a specific category to obtain the semantic masks.
Differently, we propose a structure-aware algorithm that takes structural
information as prior to predict semantic masks directly without per-pixel
classification. Specifically, given an input image, the learnable structure
token interacts with the image representations to reason the final semantic
masks. Three interaction approaches are explored and the results not only
outperform the state-of-the-art methods but also contain more structural
information. Experiments are conducted on three widely used datasets including
ADE20k, Cityscapes, and COCO-Stuff 10K. We hope that structure token could
serve as an alternative for semantic segmentation and inspire future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simulation Benchmark for Vision-based Autonomous Navigation. (arXiv:2203.13048v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13048">
<div class="article-summary-box-inner">
<span><p>This work introduces a simulator benchmark for vision-based autonomous
navigation. The simulator offers control over real world variables such as the
environment, time of day, weather and traffic. The benchmark includes a modular
integration of different components of a full autonomous visual navigation
stack. In the experimental part of the paper, state-of-the-art visual
localization methods are evaluated as a part of the stack in realistic
navigation tasks. To the authors' best knowledge, the proposed benchmark is the
first to study modern visual localization methods as part of a full autonomous
visual navigation stack.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature Selective Transformer for Semantic Image Segmentation. (arXiv:2203.14124v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14124">
<div class="article-summary-box-inner">
<span><p>Recently, it has attracted more and more attentions to fuse multi-scale
features for semantic image segmentation. Various works were proposed to employ
progressive local or global fusion, but the feature fusions are not rich enough
for modeling multi-scale context features. In this work, we focus on fusing
multi-scale features from Transformer-based backbones for semantic
segmentation, and propose a Feature Selective Transformer (FeSeFormer), which
aggregates features from all scales (or levels) for each query feature.
Specifically, we first propose a Scale-level Feature Selection (SFS) module,
which can choose an informative subset from the whole multi-scale feature set
for each scale, where those features that are important for the current scale
(or level) are selected and the redundant are discarded. Furthermore, we
propose a Full-scale Feature Fusion (FFF) module, which can adaptively fuse
features of all scales for queries. Based on the proposed SFS and FFF modules,
we develop a Feature Selective Transformer (FeSeFormer), and evaluate our
FeSeFormer on four challenging semantic segmentation benchmarks, including
PASCAL Context, ADE20K, COCO-Stuff 10K, and Cityscapes, outperforming the
state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reverse Engineering of Imperceptible Adversarial Image Perturbations. (arXiv:2203.14145v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14145">
<div class="article-summary-box-inner">
<span><p>It has been well recognized that neural network based image classifiers are
easily fooled by images with tiny perturbations crafted by an adversary. There
has been a vast volume of research to generate and defend such adversarial
attacks. However, the following problem is left unexplored: How to
reverse-engineer adversarial perturbations from an adversarial image? This
leads to a new adversarial learning paradigm--Reverse Engineering of Deceptions
(RED). If successful, RED allows us to estimate adversarial perturbations and
recover the original images. However, carefully crafted, tiny adversarial
perturbations are difficult to recover by optimizing a unilateral RED
objective. For example, the pure image denoising method may overfit to
minimizing the reconstruction error but hardly preserve the classification
properties of the true adversarial perturbations. To tackle this challenge, we
formalize the RED problem and identify a set of principles crucial to the RED
approach design. Particularly, we find that prediction alignment and proper
data augmentation (in terms of spatial transformations) are two criteria to
achieve a generalizable RED approach. By integrating these RED principles with
image denoising, we propose a new Class-Discriminative Denoising based RED
framework, termed CDD-RED. Extensive experiments demonstrate the effectiveness
of CDD-RED under different evaluation metrics (ranging from the pixel-level,
prediction-level to the attribution-level alignment) and a variety of attack
generation methods (e.g., FGSM, PGD, CW, AutoAttack, and adaptive attacks).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Limited Parameter Denoising for Low-dose X-ray Computed Tomography Using Deep Reinforcement Learning. (arXiv:2203.14794v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14794">
<div class="article-summary-box-inner">
<span><p>The use of deep learning has successfully solved several problems in the
field of medical imaging. Deep learning has been applied to the CT denoising
problem successfully. However, the use of deep learning requires large amounts
of data to train deep convolutional networks (CNNs). Moreover, due to large
parameter count, such deep CNNs may cause unexpected results. In this study, we
introduce a novel CT denoising framework, which has interpretable behaviour,
and provides useful results with limited data. We employ bilateral filtering in
both the projection and volume domains to remove noise. To account for
non-stationary noise, we tune the $\sigma$ parameters of the volume for every
projection view, and for every volume pixel. The tuning is carried out by two
deep CNNs. Due to impracticality of labelling, the two deep CNNs are trained
via a Deep-Q reinforcement learning task. The reward for the task is generated
by using a custom reward function represented by a neural network. Our
experiments were carried out on abdominal scans for the Mayo Clinic TCIA
dataset, and the AAPM Low Dose CT Grand Challenge. Our denoising framework has
excellent denoising performance increasing the PSNR from 28.53 to 28.93, and
increasing the SSIM from 0.8952 to 0.9204. We outperform several
state-of-the-art deep CNNs, which have several orders of magnitude higher
number of parameters (p-value (PSNR) = 0.000, p-value (SSIM) = 0.000). Our
method does not introduce any blurring, which is introduced by MSE loss based
methods, or any deep learning artifacts, which are introduced by WGAN based
models. Our ablation studies show that parameter tuning and using our reward
network results in the best possible results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Deep is Your Art: An Experimental Study on the Limits of Artistic Understanding in a Single-Task, Single-Modality Neural Network. (arXiv:2203.16031v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16031">
<div class="article-summary-box-inner">
<span><p>Mathematical modeling and aesthetic rule extraction of works of art are
complex activities. This is because art is a multidimensional, subjective
discipline. Perception and interpretation of art are, to many extents, relative
and open-ended rather than measurable. Following the explainable Artificial
Intelligence paradigm, this paper investigated in a human-understandable
fashion the limits to which a single-task, single-modality benchmark computer
vision model performs in classifying contemporary 2D visual arts. It is
important to point out that this work does not introduce an interpreting method
to open the black box of Deep Neural Networks, instead it uses existing
evaluating metrics derived from the confusion matrix to try to uncover the
mechanism with which Deep Neural Networks understand art. To achieve so,
VGG-11, pre-trained on ImageNet and discriminatively fine-tuned, was used on
handcrafted small-data datasets designed from real-world photography gallery
shows. We demonstrated that the artwork's Exhibited Properties or formal
factors such as shape and color, rather than Non-Exhibited Properties or
content factors such as history and intention, have much higher potential to be
the determinant when art pieces have very similar Exhibited Properties. We also
showed that a single-task and single-modality model's understanding of art is
inadequate as it largely ignores Non-Exhibited Properties.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SIT: A Bionic and Non-Linear Neuron for Spiking Neural Network. (arXiv:2203.16117v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16117">
<div class="article-summary-box-inner">
<span><p>Spiking Neural Networks (SNNs) have piqued researchers' interest because of
their capacity to process temporal information and low power consumption.
However, current state-of-the-art methods limited their biological plausibility
and performance because their neurons are generally built on the simple
Leaky-Integrate-and-Fire (LIF) model. Due to the high level of dynamic
complexity, modern neuron models have seldom been implemented in SNN practice.
In this study, we adopt the Phase Plane Analysis (PPA) technique, a technique
often utilized in neurodynamics field, to integrate a recent neuron model,
namely, the Izhikevich neuron. Based on the findings in the advancement of
neuroscience, the Izhikevich neuron model can be biologically plausible while
maintaining comparable computational cost with LIF neurons. By utilizing the
adopted PPA, we have accomplished putting neurons built with the modified
Izhikevich model into SNN practice, dubbed as the Standardized Izhikevich Tonic
(SIT) neuron. For performance, we evaluate the suggested technique for image
classification tasks in self-built LIF-and-SIT-consisted SNNs, named Hybrid
Neural Network (HNN) on static MNIST, Fashion-MNIST, CIFAR-10 datasets and
neuromorphic N-MNIST, CIFAR10-DVS, and DVS128 Gesture datasets. The
experimental results indicate that the suggested method achieves comparable
accuracy while exhibiting more biologically realistic behaviors on nearly all
test datasets, demonstrating the efficiency of this novel strategy in bridging
the gap between neurodynamics and SNN practice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sensor Data Validation and Driving Safety in Autonomous Driving Systems. (arXiv:2203.16130v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16130">
<div class="article-summary-box-inner">
<span><p>Autonomous driving technology has drawn a lot of attention due to its fast
development and extremely high commercial values. The recent technological leap
of autonomous driving can be primarily attributed to the progress in the
environment perception. Good environment perception provides accurate
high-level environment information which is essential for autonomous vehicles
to make safe and precise driving decisions and strategies. Moreover, such
progress in accurate environment perception would not be possible without deep
learning models and advanced onboard sensors, such as optical sensors (LiDARs
and cameras), radars, GPS. However, the advanced sensors and deep learning
models are prone to recently invented attack methods. For example, LiDARs and
cameras can be compromised by optical attacks, and deep learning models can be
attacked by adversarial examples. The attacks on advanced sensors and deep
learning models can largely impact the accuracy of the environment perception,
posing great threats to the safety and security of autonomous vehicles. In this
thesis, we study the detection methods against the attacks on onboard sensors
and the linkage between attacked deep learning models and driving safety for
autonomous vehicles. To detect the attacks, redundant data sources can be
exploited, since information distortions caused by attacks in victim sensor
data result in inconsistency with the information from other redundant sources.
To study the linkage between attacked deep learning models and driving
safety...
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Robot Active Mapping via Neural Bipartite Graph Matching. (arXiv:2203.16319v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16319">
<div class="article-summary-box-inner">
<span><p>We study the problem of multi-robot active mapping, which aims for complete
scene map construction in minimum time steps. The key to this problem lies in
the goal position estimation to enable more efficient robot movements. Previous
approaches either choose the frontier as the goal position via a myopic
solution that hinders the time efficiency, or maximize the long-term value via
reinforcement learning to directly regress the goal position, but does not
guarantee the complete map construction. In this paper, we propose a novel
algorithm, namely NeuralCoMapping, which takes advantage of both approaches. We
reduce the problem to bipartite graph matching, which establishes the node
correspondences between two graphs, denoting robots and frontiers. We introduce
a multiplex graph neural network (mGNN) that learns the neural distance to fill
the affinity matrix for more effective graph matching. We optimize the mGNN
with a differentiable linear assignment layer by maximizing the long-term
values that favor time efficiency and map completeness via reinforcement
learning. We compare our algorithm with several state-of-the-art multi-robot
active mapping approaches and adapted reinforcement-learning baselines.
Experimental results demonstrate the superior performance and exceptional
generalization ability of our algorithm on various indoor scenes and unseen
number of robots, when only trained with 9 indoor scenes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Multimodal Depth Estimation from Light Fields. (arXiv:2203.16542v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16542">
<div class="article-summary-box-inner">
<span><p>Light field applications, especially light field rendering and depth
estimation, developed rapidly in recent years. While state-of-the-art light
field rendering methods handle semi-transparent and reflective objects well,
depth estimation methods either ignore these cases altogether or only deliver a
weak performance. We argue that this is due current methods only considering a
single "true" depth, even when multiple objects at different depths contributed
to the color of a single pixel. Based on the simple idea of outputting a
posterior depth distribution instead of only a single estimate, we develop and
explore several different deep-learning-based approaches to the problem.
Additionally, we contribute the first "multimodal light field depth dataset"
that contains the depths of all objects which contribute to the color of a
pixel. This allows us to supervise the multimodal depth prediction and also
validate all methods by measuring the KL divergence of the predicted
posteriors. With our thorough analysis and novel dataset, we aim to start a new
line of depth estimation research that overcomes some of the long-standing
limitations of this field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Federated Learning for the Classification of Tumor Infiltrating Lymphocytes. (arXiv:2203.16622v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16622">
<div class="article-summary-box-inner">
<span><p>We evaluate the performance of federated learning (FL) in developing deep
learning models for analysis of digitized tissue sections. A classification
application was considered as the example use case, on quantifiying the
distribution of tumor infiltrating lymphocytes within whole slide images
(WSIs). A deep learning classification model was trained using 50*50 square
micron patches extracted from the WSIs. We simulated a FL environment in which
a dataset, generated from WSIs of cancer from numerous anatomical sites
available by The Cancer Genome Atlas repository, is partitioned in 8 different
nodes. Our results show that the model trained with the federated training
approach achieves similar performance, both quantitatively and qualitatively,
to that of a model trained with all the training data pooled at a centralized
location. Our study shows that FL has tremendous potential for enabling
development of more robust and accurate models for histopathology image
analysis without having to collect large and diverse training data at a single
location.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controllable Augmentations for Video Representation Learning. (arXiv:2203.16632v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16632">
<div class="article-summary-box-inner">
<span><p>This paper focuses on self-supervised video representation learning. Most
existing approaches follow the contrastive learning pipeline to construct
positive and negative pairs by sampling different clips. However, this
formulation tends to bias to static background and have difficulty establishing
global temporal structures. The major reason is that the positive pairs, i.e.,
different clips sampled from the same video, have limited temporal receptive
field, and usually share similar background but differ in motions. To address
these problems, we propose a framework to jointly utilize local clips and
global videos to learn from detailed region-level correspondence as well as
general long-term temporal relations. Based on a set of controllable
augmentations, we achieve accurate appearance and motion pattern alignment
through soft spatio-temporal region contrast. Our formulation is able to avoid
the low-level redundancy shortcut by mutual information minimization to improve
the generalization. We also introduce local-global temporal order dependency to
further bridge the gap between clip-level and video-level representations for
robust temporal modeling. Extensive experiments demonstrate that our framework
is superior on three video benchmarks in action recognition and video
retrieval, capturing more accurate temporal dynamics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Dataset of Images of Public Streetlights with Operational Monitoring using Computer Vision Techniques. (arXiv:2203.16915v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16915">
<div class="article-summary-box-inner">
<span><p>A dataset of street light images is presented. Our dataset consists of
$\sim350\textrm{k}$ images, taken from 140 UMBRELLA nodes installed in the
South Gloucestershire region in the UK. Each UMBRELLA node is installed on the
pole of a lamppost and is equipped with a Raspberry Pi Camera Module v1 facing
upwards towards the sky and lamppost light bulb. Each node collects an image at
hourly intervals for 24h every day. The data collection spans for a period of
six months.
</p>
<p>Each image taken is logged as a single entry in the dataset along with the
Global Positioning System (GPS) coordinates of the lamppost. All entries in the
dataset have been post-processed and labelled based on the operation of the
lamppost, i.e., whether the lamppost is switched ON or OFF. The dataset can be
used to train deep neural networks and generate pre-trained models providing
feature representations for smart city CCTV applications, smart weather
detection algorithms, or street infrastructure monitoring. The dataset can be
found at \url{https://doi.org/10.5281/zenodo.6046758}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">It's All In the Teacher: Zero-Shot Quantization Brought Closer to the Teacher. (arXiv:2203.17008v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17008">
<div class="article-summary-box-inner">
<span><p>Model quantization is considered as a promising method to greatly reduce the
resource requirements of deep neural networks. To deal with the performance
drop induced by quantization errors, a popular method is to use training data
to fine-tune quantized networks. In real-world environments, however, such a
method is frequently infeasible because training data is unavailable due to
security, privacy, or confidentiality concerns. Zero-shot quantization
addresses such problems, usually by taking information from the weights of a
full-precision teacher network to compensate the performance drop of the
quantized networks. In this paper, we first analyze the loss surface of
state-of-the-art zero-shot quantization techniques and provide several
findings. In contrast to usual knowledge distillation problems, zero-shot
quantization often suffers from 1) the difficulty of optimizing multiple loss
terms together, and 2) the poor generalization capability due to the use of
synthetic samples. Furthermore, we observe that many weights fail to cross the
rounding threshold during training the quantized networks even when it is
necessary to do so for better performance. Based on the observations, we
propose AIT, a simple yet powerful technique for zero-shot quantization, which
addresses the aforementioned two problems in the following way: AIT i) uses a
KL distance loss only without a cross-entropy loss, and ii) manipulates
gradients to guarantee that a certain portion of weights are properly updated
after crossing the rounding thresholds. Experiments show that AIT outperforms
the performance of many existing methods by a great margin, taking over the
overall state-of-the-art position in the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Convolutional Neural Network Modelling for MODIS Land Surface Temperature Super-Resolution. (arXiv:2202.10753v2 [cs.CV] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.10753">
<div class="article-summary-box-inner">
<span><p>Nowadays, thermal infrared satellite remote sensors enable to extract very
interesting information at large scale, in particular Land Surface Temperature
(LST). However such data are limited in spatial and/or temporal resolutions
which prevents from an analysis at fine scales. For example, MODIS satellite
provides daily acquisitions with 1Km spatial resolutions which is not
sufficient to deal with highly heterogeneous environments as agricultural
parcels. Therefore, image super-resolution is a crucial task to better exploit
MODIS LSTs. This issue is tackled in this paper. We introduce a deep
learning-based algorithm, named Multi-residual U-Net, for super-resolution of
MODIS LST single-images. Our proposed network is a modified version of U-Net
architecture, which aims at super-resolving the input LST image from 1Km to
250m per pixel. The results show that our Multi-residual U-Net outperforms
other state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-04-04 23:09:10.932293202 UTC">2022-04-04 23:09:10 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>