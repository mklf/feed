<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-03-16T01:30:00Z">03-16</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">HIE-SQL: History Information Enhanced Network for Context-Dependent Text-to-SQL Semantic Parsing. (arXiv:2203.07376v1 [cs.DB])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07376">
<div class="article-summary-box-inner">
<span><p>Recently, context-dependent text-to-SQL semantic parsing which translates
natural language into SQL in an interaction process has attracted a lot of
attention. Previous works leverage context-dependence information either from
interaction history utterances or the previous predicted SQL queries but fail
in taking advantage of both since of the mismatch between natural language and
logic-form SQL. In this work, we propose a History Information Enhanced
text-to-SQL model (HIE-SQL) to exploit context-dependence information from both
history utterances and the last predicted SQL query. In view of the mismatch,
we treat natural language and SQL as two modalities and propose a bimodal
pre-trained model to bridge the gap between them. Besides, we design a
schema-linking graph to enhance connections from utterances and the SQL query
to the database schema. We show our history information enhanced methods
improve the performance of HIE-SQL by a significant margin, which achieves new
state-of-the-art results on the two context-dependent text-to-SQL benchmarks,
the SparC and CoSQL datasets, at the writing time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting the Compositional Generalization Abilities of Neural Sequence Models. (arXiv:2203.07402v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07402">
<div class="article-summary-box-inner">
<span><p>Compositional generalization is a fundamental trait in humans, allowing us to
effortlessly combine known phrases to form novel sentences. Recent works have
claimed that standard seq-to-seq models severely lack the ability to
compositionally generalize. In this paper, we focus on one-shot primitive
generalization as introduced by the popular SCAN benchmark. We demonstrate that
modifying the training distribution in simple and intuitive ways enables
standard seq-to-seq models to achieve near-perfect generalization performance,
thereby showing that their compositional generalization abilities were
previously underestimated. We perform detailed empirical analysis of this
phenomenon. Our results indicate that the generalization performance of models
is highly sensitive to the characteristics of the training data which should be
carefully considered while designing such benchmarks in future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sememe Prediction for BabelNet Synsets using Multilingual and Multimodal Information. (arXiv:2203.07426v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07426">
<div class="article-summary-box-inner">
<span><p>In linguistics, a sememe is defined as the minimum semantic unit of
languages. Sememe knowledge bases (KBs), which are built by manually annotating
words with sememes, have been successfully applied to various NLP tasks.
However, existing sememe KBs only cover a few languages, which hinders the wide
utilization of sememes. To address this issue, the task of sememe prediction
for BabelNet synsets (SPBS) is presented, aiming to build a multilingual sememe
KB based on BabelNet, a multilingual encyclopedia dictionary. By automatically
predicting sememes for a BabelNet synset, the words in many languages in the
synset would obtain sememe annotations simultaneously. However, previous SPBS
methods have not taken full advantage of the abundant information in BabelNet.
In this paper, we utilize the multilingual synonyms, multilingual glosses and
images in BabelNet for SPBS. We design a multimodal information fusion model to
encode and combine this information for sememe prediction. Experimental results
show the substantial outperformance of our model over previous methods (about
10 MAP and F1 scores). All the code and data of this paper can be obtained at
https://github.com/thunlp/MSGI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Neural Pairwise Ranking Model for Readability Assessment. (arXiv:2203.07450v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07450">
<div class="article-summary-box-inner">
<span><p>Automatic Readability Assessment (ARA), the task of assigning a reading level
to a text, is traditionally treated as a classification problem in NLP
research. In this paper, we propose the first neural, pairwise ranking approach
to ARA and compare it with existing classification, regression, and
(non-neural) ranking methods. We establish the performance of our model by
conducting experiments with three English, one French and one Spanish datasets.
We demonstrate that our approach performs well in monolingual single/cross
corpus testing scenarios and achieves a zero-shot cross-lingual ranking
accuracy of over 80% for both French and Spanish when trained on English data.
Additionally, we also release a new parallel bilingual readability dataset in
English and French. To our knowledge, this paper proposes the first neural
pairwise ranking model for ARA, and shows the first results of cross-lingual,
zero-shot evaluation of ARA with neural models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty Estimation for Language Reward Models. (arXiv:2203.07472v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07472">
<div class="article-summary-box-inner">
<span><p>Language models can learn a range of capabilities from unsupervised training
on text corpora. However, to solve a particular problem (such as text
summarization) it is typically necessary to fine-tune them on a task-specific
dataset. It is often easier for humans to choose between options than to
provide labeled data, and prior work has achieved state-of-the-art performance
by training a reward model from such preference comparisons. However,
collecting a large preference comparison dataset is still expensive -- and the
learned reward models are unreliable out-of-distribution. We seek to address
these problems via uncertainty estimation, which can improve sample efficiency
and robustness using active learning and risk-averse reinforcement learning
(RL). Specifically, we use bootstrap aggregating (bagging) to train an ensemble
of reward models differing in the initialization of their final layer.
Ensembles have proved successful in prior applications of active learning, but
we find that in our setting ensemble active learning does not outperform random
sampling. Further experiments show that while the aggregate predictions are
well-calibrated, the ensemble's estimated epistemic uncertainty is only weakly
correlated with model error. We suspect this is because the ensemble members
are fine-tuned from a single model and so are similar to one another. This
suggests current pre-training methods will need to be modified to support
uncertainty estimation, e.g. by training multiple language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VAST: The Valence-Assessing Semantics Test for Contextualizing Language Models. (arXiv:2203.07504v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07504">
<div class="article-summary-box-inner">
<span><p>VAST, the Valence-Assessing Semantics Test, is a novel intrinsic evaluation
task for contextualized word embeddings (CWEs). VAST uses valence, the
association of a word with pleasantness, to measure the correspondence of
word-level LM semantics with widely used human judgments, and examines the
effects of contextualization, tokenization, and LM-specific geometry. Because
prior research has found that CWEs from GPT-2 perform poorly on other intrinsic
evaluations, we select GPT-2 as our primary subject, and include results
showing that VAST is useful for 7 other LMs, and can be used in 7 languages.
GPT-2 results show that the semantics of a word incorporate the semantics of
context in layers closer to model output, such that VAST scores diverge between
our contextual settings, ranging from Pearson's rho of .55 to .77 in layer 11.
We also show that multiply tokenized words are not semantically encoded until
layer 8, where they achieve Pearson's rho of .46, indicating the presence of an
encoding process for multiply tokenized words which differs from that of singly
tokenized words, for which rho is highest in layer 0. We find that a few
neurons with values having greater magnitude than the rest mask word-level
semantics in GPT-2's top layer, but that word-level semantics can be recovered
by nullifying non-semantic principal components: Pearson's rho in the top layer
improves from .32 to .76. After isolating semantics, we show the utility of
VAST for understanding LM semantics via improvements over related work on four
word similarity tasks, with a score of .50 on SimLex-999, better than the
previous best of .45 for GPT-2. Finally, we show that 8 of 10 WEAT bias tests,
which compare differences in word embedding associations between groups of
words, exhibit more stereotype-congruent biases after isolating semantics,
indicating that non-semantic structures in LMs also mask biases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Visual Semantic Pretraining Magnifies the Semantics of Natural Language Representations. (arXiv:2203.07511v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07511">
<div class="article-summary-box-inner">
<span><p>We examine the effects of contrastive visual semantic pretraining by
comparing the geometry and semantic properties of contextualized English
language representations formed by GPT-2 and CLIP, a zero-shot multimodal image
classifier which adapts the GPT-2 architecture to encode image captions. We
find that contrastive visual semantic pretraining significantly mitigates the
anisotropy found in contextualized word embeddings from GPT-2, such that the
intra-layer self-similarity (mean pairwise cosine similarity) of CLIP word
embeddings is under .25 in all layers, compared to greater than .95 in the top
layer of GPT-2. CLIP word embeddings outperform GPT-2 on word-level semantic
intrinsic evaluation tasks, and achieve a new corpus-based state of the art for
the RG65 evaluation, at .88. CLIP also forms fine-grained semantic
representations of sentences, and obtains Spearman's rho = .73 on the
SemEval-2017 Semantic Textual Similarity Benchmark with no fine-tuning,
compared to no greater than rho = .45 in any layer of GPT-2. Finally,
intra-layer self-similarity of CLIP sentence embeddings decreases as the layer
index increases, finishing at .25 in the top layer, while the self-similarity
of GPT-2 sentence embeddings formed using the EOS token increases
layer-over-layer and never falls below .97. Our results indicate that high
anisotropy is not an inevitable consequence of contextualization, and that
visual semantic pretraining is beneficial not only for ordering visual
representations, but also for encoding useful semantic representations of
language, both on the word level and the sentence level.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Visual Knowledge in Language Tasks: An Empirical Study on Intermediate Pre-training for Cross-modal Knowledge Transfer. (arXiv:2203.07519v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07519">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models are still far from human performance in tasks
that need understanding of properties (e.g. appearance, measurable quantity)
and affordances of everyday objects in the real world since the text lacks such
information due to reporting bias. In this work, we study whether integrating
visual knowledge into a language model can fill the gap. We investigate two
types of knowledge transfer: (1) text knowledge transfer using image captions
that may contain enriched visual knowledge and (2) cross-modal knowledge
transfer using both images and captions with vision-language training
objectives. On 5 downstream tasks that may need visual knowledge to solve the
problem, we perform extensive empirical comparisons over the presented
objectives. Our experiments show that visual knowledge transfer can improve
performance in both low-resource and fully supervised settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Choose Your QA Model Wisely: A Systematic Study of Generative and Extractive Readers for Question Answering. (arXiv:2203.07522v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07522">
<div class="article-summary-box-inner">
<span><p>While both extractive and generative readers have been successfully applied
to the Question Answering (QA) task, little attention has been paid toward the
systematic comparison of them. Characterizing the strengths and weaknesses of
the two readers is crucial not only for making a more informed reader selection
in practice but also for developing a deeper understanding to foster further
research on improving readers in a principled manner. Motivated by this goal,
we make the first attempt to systematically study the comparison of extractive
and generative readers for question answering. To be aligned with the
state-of-the-art, we explore nine transformer-based large pre-trained language
models (PrLMs) as backbone architectures. Furthermore, we organize our findings
under two main categories: (1) keeping the architecture invariant, and (2)
varying the underlying PrLMs. Among several interesting findings, it is
important to highlight that (1) the generative readers perform better in long
context QA, (2) the extractive readers perform better in short context while
also showing better out-of-domain generalization, and (3) the encoder of
encoder-decoder PrLMs (e.g., T5) turns out to be a strong extractive reader and
outperforms the standard choice of encoder-only PrLMs (e.g., RoBERTa). We also
study the effect of multi-task learning on the two types of readers varying the
underlying PrLMs and perform qualitative and quantitative diagnosis to provide
further insights into future directions in modeling better readers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sense Embeddings are also Biased--Evaluating Social Biases in Static and Contextualised Sense Embeddings. (arXiv:2203.07523v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07523">
<div class="article-summary-box-inner">
<span><p>Sense embedding learning methods learn different embeddings for the different
senses of an ambiguous word. One sense of an ambiguous word might be socially
biased while its other senses remain unbiased. In comparison to the numerous
prior work evaluating the social biases in pretrained word embeddings, the
biases in sense embeddings have been relatively understudied. We create a
benchmark dataset for evaluating the social biases in sense embeddings and
propose novel sense-specific bias evaluation measures. We conduct an extensive
evaluation of multiple static and contextualised sense embeddings for various
types of social biases using the proposed measures. Our experimental results
show that even in cases where no biases are found at word-level, there still
exist worrying levels of social biases at sense-level, which are often ignored
by the word-level bias evaluation measures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ScienceWorld: Is your Agent Smarter than a 5th Grader?. (arXiv:2203.07540v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07540">
<div class="article-summary-box-inner">
<span><p>This paper presents a new benchmark, ScienceWorld, to test agents' scientific
reasoning abilities in a new interactive text environment at the level of a
standard elementary school science curriculum. Despite the recent
transformer-based progress seen in adjacent fields such as question-answering,
scientific text processing, and the wider area of natural language processing,
we find that current state-of-the-art models are unable to reason about or
explain learned science concepts in novel contexts. For instance, models can
easily answer what the conductivity of a previously seen material is but
struggle when asked how they would conduct an experiment in a grounded,
interactive environment to find the conductivity of an unknown material. This
begs the question of whether current models are simply retrieving answers by
way of seeing a large number of similar input examples or if they have learned
to reason about concepts in a reusable manner. We hypothesize that agents need
to be grounded in interactive environments to achieve such reasoning
capabilities. Our experiments provide empirical evidence supporting this
hypothesis -- showing that a 1.5 million parameter agent trained interactively
for 100k steps outperforms a 11 billion parameter model statically trained for
scientific question-answering and reasoning via millions of expert
demonstrations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Calibration of Pre-trained Language Models using Mixup Guided by Area Under the Margin and Saliency. (arXiv:2203.07559v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07559">
<div class="article-summary-box-inner">
<span><p>A well-calibrated neural model produces confidence (probability outputs)
closely approximated by the expected accuracy. While prior studies have shown
that mixup training as a data augmentation technique can improve model
calibration on image classification tasks, little is known about using mixup
for model calibration on natural language understanding (NLU) tasks. In this
paper, we explore mixup for model calibration on several NLU tasks and propose
a novel mixup strategy for pre-trained language models that improves model
calibration further. Our proposed mixup is guided by both the Area Under the
Margin (AUM) statistic (Pleiss et al., 2020) and the saliency map of each
sample (Simonyan et al.,2013). Moreover, we combine our mixup strategy with
model miscalibration correction techniques (i.e., label smoothing and
temperature scaling) and provide detailed analyses of their impact on our
proposed mixup. We focus on systematically designing experiments on three NLU
tasks: natural language inference, paraphrase detection, and commonsense
reasoning. Our method achieves the lowest expected calibration error compared
to strong baselines on both in-domain and out-of-domain test samples while
maintaining competitive accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TSM: Measuring the Enticement of Honeyfiles with Natural Language Processing. (arXiv:2203.07580v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07580">
<div class="article-summary-box-inner">
<span><p>Honeyfile deployment is a useful breach detection method in cyber deception
that can also inform defenders about the intent and interests of intruders and
malicious insiders. A key property of a honeyfile, enticement, is the extent to
which the file can attract an intruder to interact with it. We introduce a
novel metric, Topic Semantic Matching (TSM), which uses topic modelling to
represent files in the repository and semantic matching in an embedding vector
space to compare honeyfile text and topic words robustly. We also present a
honeyfile corpus created with different Natural Language Processing (NLP)
methods. Experiments show that TSM is effective in inter-corpus comparisons and
is a promising tool to measure the enticement of honeyfiles. TSM is the first
measure to use NLP techniques to quantify the enticement of honeyfile content
that compares the essential topical content of local contexts to honeyfiles and
is robust to paraphrasing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Long Document Summarization with Top-down and Bottom-up Inference. (arXiv:2203.07586v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07586">
<div class="article-summary-box-inner">
<span><p>Text summarization aims to condense long documents and retain key
information. Critical to the success of a summarization model is the faithful
inference of latent representations of words or tokens in the source documents.
Most recent models infer the latent representations with a transformer encoder,
which is purely bottom-up. Also, self-attention-based inference models face the
challenge of quadratic complexity with respect to sequence length. We propose a
principled inference framework to improve summarization models on these two
aspects. Our framework assumes a hierarchical latent structure of a document
where the top-level captures the long range dependency at a coarser time scale
and the bottom token level preserves the details. Critically, this hierarchical
structure enables token representations to be updated in both a bottom-up and
top-down manner. In the bottom-up pass, token representations are inferred with
local self-attention to leverage its efficiency. Top-down correction is then
applied to allow tokens to capture long-range dependency. We demonstrate the
effectiveness of the proposed framework on a diverse set of summarization
datasets, including narrative, conversational, scientific documents and news.
Our model achieves (1) competitive or better performance on short documents
with higher memory and compute efficiency, compared to full attention
transformers, and (2) state-of-the-art performance on a wide range of long
document summarization benchmarks, compared to recent efficient transformers.
We also show that our model can summarize an entire book and achieve
competitive performance using $0.27\%$ parameters (464M vs. 175B) and much less
training data, compared to a recent GPT-3-based model. These results indicate
the general applicability and benefits of the proposed framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Procedural Text Understanding via Scene-Wise Evolution. (arXiv:2203.07600v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07600">
<div class="article-summary-box-inner">
<span><p>Procedural text understanding requires machines to reason about entity states
within the dynamical narratives. Current procedural text understanding
approaches are commonly \textbf{entity-wise}, which separately track each
entity and independently predict different states of each entity. Such an
entity-wise paradigm does not consider the interaction between entities and
their states. In this paper, we propose a new \textbf{scene-wise} paradigm for
procedural text understanding, which jointly tracks states of all entities in a
scene-by-scene manner. Based on this paradigm, we propose \textbf{S}cene
\textbf{G}raph \textbf{R}easoner (\textbf{SGR}), which introduces a series of
dynamically evolving scene graphs to jointly formulate the evolution of
entities, states and their associations throughout the narrative. In this way,
the deep interactions between all entities and states can be jointly captured
and simultaneously derived from scene graphs. Experiments show that SGR not
only achieves the new state-of-the-art performance but also significantly
accelerates the speed of reasoning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CARETS: A Consistency And Robustness Evaluative Test Suite for VQA. (arXiv:2203.07613v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07613">
<div class="article-summary-box-inner">
<span><p>We introduce CARETS, a systematic test suite to measure consistency and
robustness of modern VQA models through a series of six fine-grained capability
tests. In contrast to existing VQA test sets, CARETS features balanced question
generation to create pairs of instances to test models, with each pair focusing
on a specific capability such as rephrasing, logical symmetry or image
obfuscation. We evaluate six modern VQA systems on CARETS and identify several
actionable weaknesses in model comprehension, especially with concepts such as
negation, disjunction, or hypernym invariance. Interestingly, even the most
sophisticated models are sensitive to aspects such as swapping the order of
terms in a conjunction or varying the number of answer choices mentioned in the
question. We release CARETS to be used as an extensible tool for evaluating
multi-modal model robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Language Models Plagiarize?. (arXiv:2203.07618v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07618">
<div class="article-summary-box-inner">
<span><p>Past literature has illustrated that language models do not fully understand
the context and sensitivity of text and can sometimes memorize phrases or
sentences present in their training sets. In this paper, we investigate whether
they not only memorize but also plagiarize training samples when generating
artificial texts. Our findings support that they, especially GPT-2, reuse
particular pieces of texts from the training corpus with or without
obfuscation. We have four main results: 1) language models with more capacity
plagiarize more; 2) fine-tuned language models demonstrate differing patterns
of plagiarism based on characteristics of auxiliary data; 3) sampling from
truncated language modeling distributions tends to heighten the degree of
plagiarism as opposed to temperature sampling, and 4) plagiarism in language
models can have serious privacy consequences. Overall, our work implies that
future research on neural language models should take precautions to avoid
models plagiarizing their training datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Mix: Example Interpolation Improves Multilingual Neural Machine Translation. (arXiv:2203.07627v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07627">
<div class="article-summary-box-inner">
<span><p>Multilingual neural machine translation models are trained to maximize the
likelihood of a mix of examples drawn from multiple language pairs. The
dominant inductive bias applied to these models is a shared vocabulary and a
shared set of parameters across languages; the inputs and labels corresponding
to examples drawn from different language pairs might still reside in distinct
sub-spaces. In this paper, we introduce multilingual crossover encoder-decoder
(mXEncDec) to fuse language pairs at an instance level. Our approach
interpolates instances from different language pairs into joint `crossover
examples' in order to encourage sharing input and output spaces across
languages. To ensure better fusion of examples in multilingual settings, we
propose several techniques to improve example interpolation across dissimilar
languages under heavy data imbalance. Experiments on a large-scale WMT
multilingual dataset demonstrate that our approach significantly improves
quality on English-to-Many, Many-to-English and zero-shot translation tasks
(from +0.5 BLEU up to +5.5 BLEU points). Results on code-switching sets
demonstrate the capability of our approach to improve model generalization to
out-of-distribution multilingual examples. We also conduct qualitative and
quantitative representation comparisons to analyze the advantages of our
approach at the representation level.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Event Representation via Simultaneous Weakly Supervised Contrastive Learning and Clustering. (arXiv:2203.07633v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07633">
<div class="article-summary-box-inner">
<span><p>Representations of events described in text are important for various tasks.
In this work, we present SWCC: a Simultaneous Weakly supervised Contrastive
learning and Clustering framework for event representation learning. SWCC
learns event representations by making better use of co-occurrence information
of events. Specifically, we introduce a weakly supervised contrastive learning
method that allows us to consider multiple positives and multiple negatives,
and a prototype-based clustering method that avoids semantically related events
being pulled apart. For model training, SWCC learns representations by
simultaneously performing weakly supervised contrastive learning and
prototype-based clustering. Experimental results show that SWCC outperforms
other baselines on Hard Similarity and Transitive Sentence Similarity tasks. In
addition, a thorough analysis of the prototype-based clustering method
demonstrates that the learned prototype vectors are able to implicitly capture
various relations between events.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Keyphrase Extraction via Interpretable Neural Networks. (arXiv:2203.07640v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07640">
<div class="article-summary-box-inner">
<span><p>Keyphrase extraction aims at automatically extracting a list of "important"
phrases which represent the key concepts in a document. Prior approaches for
unsupervised keyphrase extraction resort to heuristic notions of phrase
importance via embedding similarities or graph centrality, requiring extensive
domain expertise to develop them. Our work proposes an alternative operational
definition: phrases that are most useful for predicting the topic of a text are
important keyphrases. To this end, we propose INSPECT -- a self-explaining
neural framework for identifying influential keyphrases by measuring the
predictive impact of input phrases on the downstream task of topic
classification. We show that this novel approach not only alleviates the need
for ad-hoc heuristics but also achieves state-of-the-art results in
unsupervised keyphrase extraction across four diverse datasets in two domains:
scientific publications and news articles. Ultimately, our study suggests a new
usage of interpretable neural networks as an intrinsic component in NLP
systems, and not only as a tool for explaining model predictions to humans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Synthetic Translations Improve Bitext Quality?. (arXiv:2203.07643v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07643">
<div class="article-summary-box-inner">
<span><p>Synthetic translations have been used for a wide range of NLP tasks primarily
as a means of data augmentation. This work explores, instead, how synthetic
translations can be used to revise potentially imperfect reference translations
in mined bitext. We find that synthetic samples can improve bitext quality
without any additional bilingual supervision when they replace the originals
based on a semantic equivalence classifier that helps mitigate NMT noise. The
improved quality of the revised bitext is confirmed intrinsically via human
evaluation and extrinsically through bilingual induction and MT tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Long Sequence Encoding via Synchronization. (arXiv:2203.07644v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07644">
<div class="article-summary-box-inner">
<span><p>Pre-trained Transformer models have achieved successes in a wide range of NLP
tasks, but are inefficient when dealing with long input sequences. Existing
studies try to overcome this challenge via segmenting the long sequence
followed by hierarchical encoding or post-hoc aggregation. We propose a
synchronization mechanism for hierarchical encoding. Our approach first
identifies anchor tokens across segments and groups them by their roles in the
original input sequence. Then inside Transformer layer, anchor embeddings are
synchronized within their group via a self-attention module. Our approach is a
general framework with sufficient flexibility -- when adapted to a new task, it
is easy to be enhanced with the task-specific anchor definitions. Experiments
on two representative tasks with different types of long input texts,
NarrativeQA summary setting and wild multi-hop reasoning from HotpotQA,
demonstrate that our approach is able to improve the global information
exchange among segments while maintaining efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InfoDCL: A Distantly Supervised Contrastive Learning Framework for Social Meaning. (arXiv:2203.07648v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07648">
<div class="article-summary-box-inner">
<span><p>Existing supervised contrastive learning frameworks suffer from two major
drawbacks: (i) they depend on labeled data, which is limited for the majority
of tasks in real-world, and (ii) they incorporate inter-class relationships
based on instance-level information, while ignoring corpus-level information,
for weighting negative samples. To mitigate these challenges, we propose an
effective distantly supervised contrastive learning framework (InfoDCL) that
makes use of naturally occurring surrogate labels in the context of contrastive
learning and employs pointwise mutual information to leverage corpus-level
information. Our framework outperforms an extensive set of existing contrastive
learning methods (self-supervised, supervised, and weakly supervised) on a wide
range of social meaning tasks (in-domain and out-of-domain), in both the
general and few-shot settings. Our method is also language-agnostic, as we
demonstrate on three languages in addition to English.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalized but not Robust? Comparing the Effects of Data Modification Methods on Out-of-Domain Generalization and Adversarial Robustness. (arXiv:2203.07653v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07653">
<div class="article-summary-box-inner">
<span><p>Data modification, either via additional training datasets, data
augmentation, debiasing, and dataset filtering, has been proposed as an
effective solution for generalizing to out-of-domain (OOD) inputs, in both
natural language processing and computer vision literature. However, the effect
of data modification on adversarial robustness remains unclear. In this work,
we conduct a comprehensive study of common data modification strategies and
evaluate not only their in-domain and OOD performance, but also their
adversarial robustness (AR). We also present results on a two-dimensional
synthetic dataset to visualize the effect of each method on the training
distribution. This work serves as an empirical study towards understanding the
relationship between generalizing to unseen domains and defending against
adversarial perturbations. Our findings suggest that more data (either via
additional datasets or data augmentation) benefits both OOD accuracy and AR.
However, data filtering (previously shown to improve OOD accuracy on natural
language inference) hurts OOD accuracy on other tasks such as question
answering and image classification. We provide insights from our experiments to
inform future work in this direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Seamlessly Integrating Factual Information and Social Content with Persuasive Dialogue. (arXiv:2203.07657v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07657">
<div class="article-summary-box-inner">
<span><p>Effective human-chatbot conversations need to achieve both coherence and
efficiency. Complex conversation settings such as persuasion involve
communicating changes in attitude or behavior, so users' perspectives need to
be carefully considered and addressed, even when not directly related to the
topic. In this work, we contribute a novel modular dialogue system framework
that seamlessly integrates factual information and social content into
persuasive dialogue. Our framework is generalizable to any dialogue tasks that
have mixed social and task contents. We conducted a study that compared user
evaluations of our framework versus a baseline end-to-end generation model. We
found our model was evaluated to be more favorable in all dimensions including
competence and friendliness compared to the baseline model which does not
explicitly handle social content or factual questions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Agent To Rule Them All: Towards Multi-agent Conversational AI. (arXiv:2203.07665v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07665">
<div class="article-summary-box-inner">
<span><p>The increasing volume of commercially available conversational agents (CAs)
on the market has resulted in users being burdened with learning and adopting
multiple agents to accomplish their tasks. Though prior work has explored
supporting a multitude of domains within the design of a single agent, the
interaction experience suffers due to the large action space of desired
capabilities. To address these problems, we introduce a new task BBAI:
Black-Box Agent Integration, focusing on combining the capabilities of multiple
black-box CAs at scale. We explore two techniques: question agent pairing and
question response pairing aimed at resolving this task. Leveraging these
techniques, we design One For All (OFA), a scalable system that provides a
unified interface to interact with multiple CAs. Additionally, we introduce
MARS: Multi-Agent Response Selection, a new encoder model for question response
pairing that jointly encodes user question and agent response pairs. We
demonstrate that OFA is able to automatically and accurately integrate an
ensemble of commercially available CAs spanning disparate domains.
Specifically, using the MARS encoder we achieve the highest accuracy on our
BBAI task, outperforming strong baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compressing Sentence Representation for Semantic Retrieval via Homomorphic Projective Distillation. (arXiv:2203.07687v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07687">
<div class="article-summary-box-inner">
<span><p>How to learn highly compact yet effective sentence representation?
Pre-trained language models have been effective in many NLP tasks. However,
these models are often huge and produce large sentence embeddings. Moreover,
there is a big performance gap between large and small models. In this paper,
we propose Homomorphic Projective Distillation (HPD) to learn compressed
sentence embeddings. Our method augments a small Transformer encoder model with
learnable projection layers to produce compact representations while mimicking
a large pre-trained language model to retain the sentence representation
quality. We evaluate our method with different model sizes on both semantic
textual similarity (STS) and semantic retrieval (SR) tasks. Experiments show
that our method achieves 2.7-4.5 points performance gain on STS tasks compared
with previous best representations of the same size. In SR tasks, our method
improves retrieval speed (8.2$\times$) and memory usage (8.0$\times$) compared
with state-of-the-art large models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReACC: A Retrieval-Augmented Code Completion Framework. (arXiv:2203.07722v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07722">
<div class="article-summary-box-inner">
<span><p>Code completion, which aims to predict the following code token(s) according
to the code context, can improve the productivity of software development.
Recent work has proved that statistical language modeling with transformers can
greatly improve the performance in the code completion task via learning from
large-scale source code datasets. However, current approaches focus only on
code context within the file or project, i.e. internal context. Our distinction
is utilizing "external" context, inspired by human behaviors of copying from
the related code snippets when writing code. Specifically, we propose a
retrieval-augmented code completion framework, leveraging both lexical copying
and referring to code with similar semantics by retrieval. We adopt a
stage-wise training approach that combines a source code retriever and an
auto-regressive language model for programming language. We evaluate our
approach in the code completion task in Python and Java programming languages,
achieving a state-of-the-art performance on CodeXGLUE benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating BERT-based Pre-training Language Models for Detecting Misinformation. (arXiv:2203.07731v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07731">
<div class="article-summary-box-inner">
<span><p>It is challenging to control the quality of online information due to the
lack of supervision over all the information posted online. Manual checking is
almost impossible given the vast number of posts made on online media and how
quickly they spread. Therefore, there is a need for automated rumour detection
techniques to limit the adverse effects of spreading misinformation. Previous
studies mainly focused on finding and extracting the significant features of
text data. However, extracting features is time-consuming and not a highly
effective process. This study proposes the BERT- based pre-trained language
models to encode text data into vectors and utilise neural network models to
classify these vectors to detect misinformation. Furthermore, different
language models (LM) ' performance with different trainable parameters was
compared. The proposed technique is tested on different short and long text
datasets. The result of the proposed technique has been compared with the
state-of-the-art techniques on the same datasets. The results show that the
proposed technique performs better than the state-of-the-art techniques. We
also tested the proposed technique by combining the datasets. The results
demonstrated that the large data training and testing size considerably
improves the technique's performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ViWOZ: A Multi-Domain Task-Oriented Dialogue Systems Dataset For Low-resource Language. (arXiv:2203.07742v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07742">
<div class="article-summary-box-inner">
<span><p>Most of the current task-oriented dialogue systems (ToD), despite having
interesting results, are designed for a handful of languages like Chinese and
English. Therefore, their performance in low-resource languages is still a
significant problem due to the absence of a standard dataset and evaluation
policy. To address this problem, we proposed ViWOZ, a fully-annotated
Vietnamese task-oriented dialogue dataset. ViWOZ is the first multi-turn,
multi-domain tasked oriented dataset in Vietnamese, a low-resource language.
The dataset consists of a total of 5,000 dialogues, including 60,946 fully
annotated utterances. Furthermore, we provide a comprehensive benchmark of both
modular and end-to-end models in low-resource language scenarios. With those
characteristics, the ViWOZ dataset enables future studies on creating a
multilingual task-oriented dialogue system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniSAr: A Unified Structure-Aware Autoregressive Language Model for Text-to-SQL. (arXiv:2203.07781v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07781">
<div class="article-summary-box-inner">
<span><p>Existing text-to-SQL semantic parsers are typically designed for particular
settings such as handling queries that span multiple tables, domains or turns
which makes them ineffective when applied to different settings. We present
UniSAr (Unified Structure-Aware Autoregressive Language Model), which benefits
from directly using an off-the-shelf language model architecture and
demonstrates consistently high performance under different settings.
Specifically, UniSAr extends existing autoregressive language models to
incorporate three non-invasive extensions to make them structure-aware: (1)
adding structure mark to encode database schema, conversation context, and
their relationships; (2) constrained decoding to decode well structured SQL for
a given database schema; and (3) SQL completion to complete potential missing
JOIN relationships in SQL based on database schema. On seven well-known
text-to-SQL datasets covering multi-domain, multi-table and multi-turn, UniSAr
demonstrates highly comparable or better performance to the most advanced
specifically-designed text-to-SQL models. Importantly, our UniSAr is
non-invasive, such that other core model advances in text-to-SQL can also adopt
our extensions to further enhance performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Ghost in the Machine has an American accent: value conflict in GPT-3. (arXiv:2203.07785v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07785">
<div class="article-summary-box-inner">
<span><p>The alignment problem in the context of large language models must consider
the plurality of human values in our world. Whilst there are many resonant and
overlapping values amongst the world's cultures, there are also many
conflicting, yet equally valid, values. It is important to observe which
cultural values a model exhibits, particularly when there is a value conflict
between input prompts and generated outputs. We discuss how the co-creation of
language and cultural value impacts large language models (LLMs). We explore
the constitution of the training data for GPT-3 and compare that to the world's
language and internet access demographics, as well as to reported statistical
profiles of dominant values in some Nation-states. We stress tested GPT-3 with
a range of value-rich texts representing several languages and nations;
including some with values orthogonal to dominant US public opinion as reported
by the World Values Survey. We observed when values embedded in the input text
were mutated in the generated outputs and noted when these conflicting values
were more aligned with reported dominant US values. Our discussion of these
results uses a moral value pluralism (MVP) lens to better understand these
value mutations. Finally, we provide recommendations for how our work may
contribute to other current work in the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do BERTs Learn to Use Browser User Interface? Exploring Multi-Step Tasks with Unified Vision-and-Language BERTs. (arXiv:2203.07828v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07828">
<div class="article-summary-box-inner">
<span><p>Pre-trained Transformers are good foundations for unified multi-task models
owing to their task-agnostic representation. Pre-trained Transformers are often
combined with text-to-text framework to execute multiple tasks by a single
model. Performing a task through a graphical user interface (GUI) is another
candidate to accommodate various tasks, including multi-step tasks with vision
and language inputs. However, few papers combine pre-trained Transformers with
performing through GUI. To fill this gap, we explore a framework in which a
model performs a task by manipulating the GUI implemented with web pages in
multiple steps. We develop task pages with and without page transitions and
propose a BERT extension for the framework. We jointly trained our BERT
extension with those task pages, and made the following observations. (1) The
model learned to use both task pages with and without page transition. (2) In
four out of five tasks without page transitions, the model performs greater
than 75% of the performance of the original BERT, which does not use browsers.
(3) The model did not generalize effectively on unseen tasks. These results
suggest that we can fine-tune BERTs to multi-step tasks through GUIs, and there
is room for improvement in their generalizability. Code will be available
online.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph Pre-training for AMR Parsing and Generation. (arXiv:2203.07836v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07836">
<div class="article-summary-box-inner">
<span><p>Abstract meaning representation (AMR) highlights the core semantic
information of text in a graph structure. Recently, pre-trained language models
(PLMs) have advanced tasks of AMR parsing and AMR-to-text generation,
respectively. However, PLMs are typically pre-trained on textual data, thus are
sub-optimal for modeling structural knowledge. To this end, we investigate
graph self-supervised training to improve the structure awareness of PLMs over
AMR graphs. In particular, we introduce two graph auto-encoding strategies for
graph-to-graph pre-training and four tasks to integrate text and graph
information during pre-training. We further design a unified framework to
bridge the gap between pre-training and fine-tuning tasks. Experiments on both
AMR parsing and AMR-to-text generation show the superiority of our model. To
our knowledge, we are the first to consider pre-training on semantic graphs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SCD: Self-Contrastive Decorrelation for Sentence Embeddings. (arXiv:2203.07847v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07847">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose Self-Contrastive Decorrelation (SCD), a
self-supervised approach. Given an input sentence, it optimizes a joint
self-contrastive and decorrelation objective. Learning a representation is
facilitated by leveraging the contrast arising from the instantiation of
standard dropout at different rates. The proposed method is conceptually simple
yet empirically powerful. It achieves comparable results with state-of-the-art
methods on multiple benchmarks without using contrastive pairs. This study
opens up avenues for efficient self-supervised learning methods that are more
robust than current contrastive methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved Multi-label Classification under Temporal Concept Drift: Rethinking Group-Robust Algorithms in a Label-Wise Setting. (arXiv:2203.07856v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07856">
<div class="article-summary-box-inner">
<span><p>In document classification for, e.g., legal and biomedical text, we often
deal with hundreds of classes, including very infrequent ones, as well as
temporal concept drift caused by the influence of real world events, e.g.,
policy changes, conflicts, or pandemics. Class imbalance and drift can
sometimes be mitigated by resampling the training data to simulate (or
compensate for) a known target distribution, but what if the target
distribution is determined by unknown future events? Instead of simply
resampling uniformly to hedge our bets, we focus on the underlying optimization
algorithms used to train such document classifiers and evaluate several
group-robust optimization algorithms, initially proposed to mitigate
group-level disparities. Reframing group-robust algorithms as adaptation
algorithms under concept drift, we find that Invariant Risk Minimization and
Spectral Decoupling outperform sampling-based approaches to class imbalance and
concept drift, and lead to much better performance on minority classes. The
effect is more pronounced the larger the label set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Imputing Out-of-Vocabulary Embeddings with LOVE Makes Language Models Robust with Little Cost. (arXiv:2203.07860v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07860">
<div class="article-summary-box-inner">
<span><p>State-of-the-art NLP systems represent inputs with word embeddings, but these
are brittle when faced with Out-of-Vocabulary (OOV) words. To address this
issue, we follow the principle of mimick-like models to generate vectors for
unseen words, by learning the behavior of pre-trained embeddings using only the
surface form of words. We present a simple contrastive learning framework,
LOVE, which extends the word representation of an existing pre-trained language
model (such as BERT), and makes it robust to OOV with few additional
parameters. Extensive evaluations demonstrate that our lightweight model
achieves similar or even better performances than prior competitors, both on
original datasets and on corrupted variants. Moreover, it can be used in a
plug-and-play fashion with FastText and BERT, where it significantly improves
their robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">K-VQG: Knowledge-aware Visual Question Generation for Common-sense Acquisition. (arXiv:2203.07890v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07890">
<div class="article-summary-box-inner">
<span><p>Visual Question Generation (VQG) is a task to generate questions from images.
When humans ask questions about an image, their goal is often to acquire some
new knowledge. However, existing studies on VQG have mainly addressed question
generation from answers or question categories, overlooking the objectives of
knowledge acquisition. To introduce a knowledge acquisition perspective into
VQG, we constructed a novel knowledge-aware VQG dataset called K-VQG. This is
the first large, humanly annotated dataset in which questions regarding images
are tied to structured knowledge. We also developed a new VQG model that can
encode and use knowledge as the target for a question. The experiment results
show that our model outperforms existing models on the K-VQG dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gold Doesn't Always Glitter: Spectral Removal of Linear and Nonlinear Guarded Attribute Information. (arXiv:2203.07893v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07893">
<div class="article-summary-box-inner">
<span><p>We describe a simple and effective method (Spectral Attribute removaL; SAL)
to remove guarded information from neural representations. Our method uses
singular value decomposition and eigenvalue decomposition to project the input
representations into directions with reduced covariance with the guarded
information rather than maximal covariance as normally these factorization
methods are used. We begin with linear information removal and proceed to
generalize our algorithm to the case of nonlinear information removal through
the use of kernels. Our experiments demonstrate that our algorithm retains
better main task performance after removing the guarded information compared to
previous methods. In addition, our experiments demonstrate that we need a
relatively small amount of guarded attribute data to remove information about
these attributes, which lowers the exposure to such possibly sensitive data and
fits better low-resource scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Signal in Noise: Exploring Meaning Encoded in Random Character Sequences with Character-Aware Language Models. (arXiv:2203.07911v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07911">
<div class="article-summary-box-inner">
<span><p>Natural language processing models learn word representations based on the
distributional hypothesis, which asserts that word context (e.g.,
co-occurrence) correlates with meaning. We propose that $n$-grams composed of
random character sequences, or $garble$, provide a novel context for studying
word meaning both within and beyond extant language. In particular, randomly
generated character $n$-grams lack meaning but contain primitive information
based on the distribution of characters they contain. By studying the
embeddings of a large corpus of garble, extant language, and pseudowords using
CharacterBERT, we identify an axis in the model's high-dimensional embedding
space that separates these classes of $n$-grams. Furthermore, we show that this
axis relates to structure within extant language, including word
part-of-speech, morphology, and concept concreteness. Thus, in contrast to
studies that are mainly limited to extant language, our work reveals that
meaning and primitive information are intrinsically linked.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Extractive Opinion Summarization Using Sparse Coding. (arXiv:2203.07921v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07921">
<div class="article-summary-box-inner">
<span><p>Opinion summarization is the task of automatically generating summaries that
encapsulate information from multiple user reviews. We present Semantic
Autoencoder (SemAE) to perform extractive opinion summarization in an
unsupervised manner. SemAE uses dictionary learning to implicitly capture
semantic information from the review and learns a latent representation of each
sentence over semantic units. A semantic unit is supposed to capture an
abstract semantic concept. Our extractive summarization algorithm leverages the
representations to identify representative opinions among hundreds of reviews.
SemAE is also able to perform controllable summarization to generate
aspect-specific summaries. We report strong performance on SPACE and AMAZON
datasets, and perform experiments to investigate the functioning of our model.
Our code is publicly available at https://github.com/brcsomnath/SemAE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Robustness of Neural-Statistical Features in Detection of Generative Transformers. (arXiv:2203.07983v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07983">
<div class="article-summary-box-inner">
<span><p>The detection of computer-generated text is an area of rapidly increasing
significance as nascent generative models allow for efficient creation of
compelling human-like text, which may be abused for the purposes of spam,
disinformation, phishing, or online influence campaigns. Past work has studied
detection of current state-of-the-art models, but despite a developing threat
landscape, there has been minimal analysis of the robustness of detection
methods to adversarial attacks. To this end, we evaluate neural and non-neural
approaches on their ability to detect computer-generated text, their robustness
against text adversarial attacks, and the impact that successful adversarial
attacks have on human judgement of text quality. We find that while statistical
features underperform neural features, statistical features provide additional
adversarial robustness that can be leveraged in ensemble detection models. In
the process, we find that previously effective complex phrasal features for
detection of computer-generated text hold little predictive power against
contemporary generative models, and identify promising statistical features to
use instead. Finally, we pioneer the usage of $\Delta$MAUVE as a proxy measure
for human judgement of adversarial text quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UofA-Truth at Factify 2022 : Transformer And Transfer Learning Based Multi-Modal Fact-Checking. (arXiv:2203.07990v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07990">
<div class="article-summary-box-inner">
<span><p>Identifying fake news is a very difficult task, especially when considering
the multiple modes of conveying information through text, image, video and/or
audio. We attempted to tackle the problem of automated
misinformation/disinformation detection in multi-modal news sources (including
text and images) through our simple, yet effective, approach in the FACTIFY
shared task at De-Factify@AAAI2022. Our model produced an F1-weighted score of
74.807%, which was the fourth best out of all the submissions. In this paper we
will explain our approach to undertake the shared task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Uni-Modal Self-Supervised Learning for Multimodal Audio-Visual Speech Recognition. (arXiv:2203.07996v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07996">
<div class="article-summary-box-inner">
<span><p>Training Transformer-based models demands a large amount of data, while
obtaining parallel aligned and labelled data in multimodality is rather
cost-demanding, especially for audio-visual speech recognition (AVSR). Thus it
makes a lot of sense to make use of unlabelled uni-modal data. On the other
side, although the effectiveness of large-scale self-supervised learning is
well established in both audio and visual modalities, how to integrate those
pre-trained models into a multimodal scenario remains underexplored. In this
work, we successfully leverage uni-modal self-supervised learning to promote
the multimodal AVSR. In particular, we first train audio and visual encoders on
a large-scale uni-modal dataset, then we integrate components of both encoders
into a larger multimodal framework which learns to recognize paired
audio-visual data into characters through a combination of CTC and seq2seq
decoding. We show that both components inherited from uni-modal self-supervised
learning cooperate well, resulting in that the multimodal framework yields
competitive results through fine-tuning. Our model is experimentally validated
on both word-level and sentence-level AVSR tasks. Especially, even without an
external language model, our proposed model raises the state-of-the-art
performances on the widely accepted Lip Reading Sentences 2 (LRS2) dataset by a
large margin, with a relative improvement of 30%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modular and Parameter-Efficient Multimodal Fusion with Prompting. (arXiv:2203.08055v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08055">
<div class="article-summary-box-inner">
<span><p>Recent research has made impressive progress in large-scale multimodal
pre-training. In the context of the rapid growth of model size, it is necessary
to seek efficient and flexible methods other than finetuning. In this paper, we
propose to use prompt vectors to align the modalities. Our method achieves
comparable performance to several other multimodal fusion methods in
low-resource settings. We further show that our method is modular and
parameter-efficient for processing tasks involving two or more data modalities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Things not Written in Text: Exploring Spatial Commonsense from Visual Signals. (arXiv:2203.08075v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08075">
<div class="article-summary-box-inner">
<span><p>Spatial commonsense, the knowledge about spatial position and relationship
between objects (like the relative size of a lion and a girl, and the position
of a boy relative to a bicycle when cycling), is an important part of
commonsense knowledge. Although pretrained language models (PLMs) succeed in
many NLP tasks, they are shown to be ineffective in spatial commonsense
reasoning. Starting from the observation that images are more likely to exhibit
spatial commonsense than texts, we explore whether models with visual signals
learn more spatial commonsense than text-based PLMs. We propose a spatial
commonsense benchmark that focuses on the relative scales of objects, and the
positional relationship between people and objects under different actions. We
probe PLMs and models with visual signals, including vision-language pretrained
models and image synthesis models, on this benchmark, and find that image
synthesis models are more capable of learning accurate and consistent spatial
knowledge than other models. The spatial knowledge from image synthesis models
also helps in natural language understanding tasks that require spatial
commonsense.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring the Impact of (Psycho-)Linguistic and Readability Features and Their Spill Over Effects on the Prediction of Eye Movement Patterns. (arXiv:2203.08085v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08085">
<div class="article-summary-box-inner">
<span><p>There is a growing interest in the combined use of NLP and machine learning
methods to predict gaze patterns during naturalistic reading. While promising
results have been obtained through the use of transformer-based language
models, little work has been undertaken to relate the performance of such
models to general text characteristics. In this paper we report on experiments
with two eye-tracking corpora of naturalistic reading and two language models
(BERT and GPT-2). In all experiments, we test effects of a broad spectrum of
features for predicting human reading behavior that fall into five categories
(syntactic complexity, lexical richness, register-based multiword combinations,
readability and psycholinguistic word properties). Our experiments show that
both the features included and the architecture of the transformer-based
language models play a role in predicting multiple eye-tracking measures during
naturalistic reading. We also report the results of experiments aimed at
determining the relative importance of features from different groups using
SP-LIME.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does Corpus Quality Really Matter for Low-Resource Languages?. (arXiv:2203.08111v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08111">
<div class="article-summary-box-inner">
<span><p>The vast majority of non-English corpora are derived from automatically
filtered versions of CommonCrawl. While prior work has identified major issues
on the quality of these datasets (Kreutzer et al., 2021), it is not clear how
this impacts downstream performance. Taking Basque as a case study, we explore
tailored crawling (manually identifying and scraping websites with high-quality
content) as an alternative to filtering CommonCrawl. Our new corpus, called
EusCrawl, is similar in size to the Basque portion of popular multilingual
corpora like CC100 and mC4, yet it has a much higher quality according to
native annotators. For instance, 66% of documents are rated as high-quality for
EusCrawl, in contrast with &lt;33% for both mC4 and CC100. Nevertheless, we obtain
similar results on downstream tasks regardless of the corpus used for
pre-training. Our work suggests that NLU performance in low-resource languages
is primarily constrained by the quantity rather than the quality of the data,
prompting for methods to exploit more diverse data sources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Representation Learning for Resource-Constrained Keyphrase Generation. (arXiv:2203.08118v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08118">
<div class="article-summary-box-inner">
<span><p>State-of-the-art keyphrase generation methods generally depend on large
annotated datasets, limiting their performance in domains with constrained
resources. To overcome this challenge, we investigate strategies to learn an
intermediate representation suitable for the keyphrase generation task. We
introduce salient span recovery and salient span prediction as guided denoising
language modeling objectives that condense the domain-specific knowledge
essential for keyphrase generation. Through experiments on multiple scientific
keyphrase generation benchmarks, we show the effectiveness of the proposed
approach for facilitating low-resource and zero-shot keyphrase generation.
Furthermore, we observe that our method especially benefits the generation of
absent keyphrases, approaching the performance of SOTA methods trained with
large training sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Medical Code Assignment with Gated Convolution and Note-Code Interaction. (arXiv:2010.06975v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.06975">
<div class="article-summary-box-inner">
<span><p>Medical code assignment from clinical text is a fundamental task in clinical
information system management. As medical notes are typically lengthy and the
medical coding system's code space is large, this task is a long-standing
challenge. Recent work applies deep neural network models to encode the medical
notes and assign medical codes to clinical documents. However, these methods
are still ineffective as they do not fully encode and capture the lengthy and
rich semantic information of medical notes nor explicitly exploit the
interactions between the notes and codes. We propose a novel method, gated
convolutional neural networks, and a note-code interaction (GatedCNN-NCI), for
automatic medical code assignment to overcome these challenges. Our methods
capture the rich semantic information of the lengthy clinical text for better
representation by utilizing embedding injection and gated information
propagation in the medical note encoding module. With a novel note-code
interaction design and a graph message passing mechanism, we explicitly capture
the underlying dependency between notes and codes, enabling effective code
prediction. A weight sharing scheme is further designed to decrease the number
of trainable parameters. Empirical experiments on real-world clinical datasets
show that our proposed model outperforms state-of-the-art models in most cases,
and our model size is on par with light-weighted baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Divide and Rule: Effective Pre-Training for Context-Aware Multi-Encoder Translation Models. (arXiv:2103.17151v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.17151">
<div class="article-summary-box-inner">
<span><p>Multi-encoder models are a broad family of context-aware neural machine
translation systems that aim to improve translation quality by encoding
document-level contextual information alongside the current sentence. The
context encoding is undertaken by contextual parameters, trained on
document-level data. In this work, we discuss the difficulty of training these
parameters effectively, due to the sparsity of the words in need of context
(i.e., the training signal), and their relevant context. We propose to
pre-train the contextual parameters over split sentence pairs, which makes an
efficient use of the available data for two reasons. Firstly, it increases the
contextual training signal by breaking intra-sentential syntactic relations,
and thus pushing the model to search the context for disambiguating clues more
frequently. Secondly, it eases the retrieval of relevant context, since context
segments become shorter. We propose four different splitting methods, and
evaluate our approach with BLEU and contrastive test sets. Results show that it
consistently improves learning of contextual parameters, both in low and high
resource settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Question Answering Model Robustness with Synthetic Adversarial Data Generation. (arXiv:2104.08678v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08678">
<div class="article-summary-box-inner">
<span><p>Despite recent progress, state-of-the-art question answering models remain
vulnerable to a variety of adversarial attacks. While dynamic adversarial data
collection, in which a human annotator tries to write examples that fool a
model-in-the-loop, can improve model robustness, this process is expensive
which limits the scale of the collected data. In this work, we are the first to
use synthetic adversarial data generation to make question answering models
more robust to human adversaries. We develop a data generation pipeline that
selects source passages, identifies candidate answers, generates questions,
then finally filters or re-labels them to improve quality. Using this approach,
we amplify a smaller human-written adversarial dataset to a much larger set of
synthetic question-answer pairs. By incorporating our synthetic data, we
improve the state-of-the-art on the AdversarialQA dataset by 3.7F1 and improve
model generalisation on nine of the twelve MRQA datasets. We further conduct a
novel human-in-the-loop evaluation to show that our models are considerably
more robust to new human-written adversarial examples: crowdworkers can fool
our model only 8.8% of the time on average, compared to 17.6% for a model
trained without synthetic data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Memorisation versus Generalisation in Pre-trained Language Models. (arXiv:2105.00828v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.00828">
<div class="article-summary-box-inner">
<span><p>State-of-the-art pre-trained language models have been shown to memorise
facts and perform well with limited amounts of training data. To gain a better
understanding of how these models learn, we study their generalisation and
memorisation capabilities in noisy and low-resource scenarios. We find that the
training of these models is almost unaffected by label noise and that it is
possible to reach near-optimal results even on extremely noisy datasets.
However, our experiments also show that they mainly learn from high-frequency
patterns and largely fail when tested on low-resource tasks such as few-shot
learning and rare entity recognition. To mitigate such limitations, we propose
an extension based on prototypical networks that improves performance in
low-resource named entity recognition tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dependency Parsing as MRC-based Span-Span Prediction. (arXiv:2105.07654v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.07654">
<div class="article-summary-box-inner">
<span><p>Higher-order methods for dependency parsing can partially but not fully
address the issue that edges in dependency trees should be constructed at the
text span/subtree level rather than word level. In this paper, we propose a new
method for dependency parsing to address this issue. The proposed method
constructs dependency trees by directly modeling span-span (in other words,
subtree-subtree) relations. It consists of two modules: the {\it text span
proposal module} which proposes candidate text spans, each of which represents
a subtree in the dependency tree denoted by (root, start, end); and the {\it
span linking module}, which constructs links between proposed spans. We use the
machine reading comprehension (MRC) framework as the backbone to formalize the
span linking module, where one span is used as a query to extract the text
span/subtree it should be linked to. The proposed method has the following
merits: (1) it addresses the fundamental problem that edges in a dependency
tree should be constructed between subtrees; (2) the MRC framework allows the
method to retrieve missing spans in the span proposal stage, which leads to
higher recall for eligible spans. Extensive experiments on the PTB, CTB and
Universal Dependencies (UD) benchmarks demonstrate the effectiveness of the
proposed method. The code is available at
\url{https://github.com/ShannonAI/mrc-for-dependency-parsing}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exposing the Implicit Energy Networks behind Masked Language Models via Metropolis--Hastings. (arXiv:2106.02736v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02736">
<div class="article-summary-box-inner">
<span><p>While recent work has shown that scores from models trained by the ubiquitous
masked language modeling (MLM) objective effectively discriminate probable from
improbable sequences, it is still an open question if these MLMs specify a
principled probability distribution over the space of possible sequences. In
this paper, we interpret MLMs as energy-based sequence models and propose two
energy parametrizations derivable from the trained MLMs. In order to draw
samples correctly from these models, we develop a tractable sampling scheme
based on the Metropolis--Hastings Monte Carlo algorithm. In our approach,
samples are proposed from the same masked conditionals used for training the
masked language models, and they are accepted or rejected based on their energy
values according to the target distribution. We validate the effectiveness of
the proposed parametrizations by exploring the quality of samples drawn from
these energy-based models for both open-ended unconditional generation and a
conditional generation task of machine translation. We theoretically and
empirically justify our sampling algorithm by showing that the masked
conditionals on their own do not yield a Markov chain whose stationary
distribution is that of our target distribution, and our approach generates
higher quality samples than other recently proposed undirected generation
approaches (Wang et al., 2019, Ghazvininejad et al., 2019).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incorporating Word Sense Disambiguation in Neural Language Models. (arXiv:2106.07967v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07967">
<div class="article-summary-box-inner">
<span><p>We present two supervised (pre-)training methods to incorporate gloss
definitions from lexical resources into neural language models (LMs). The
training improves our models' performance for Word Sense Disambiguation (WSD)
but also benefits general language understanding tasks while adding almost no
parameters. We evaluate our techniques with seven different neural LMs and find
that XLNet is more suitable for WSD than BERT. Our best-performing methods
exceeds state-of-the-art WSD techniques on the SemCor 3.0 dataset by 0.5% F1
and increase BERT's performance on the GLUE benchmark by 1.1% on average.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Closer Look at How Fine-tuning Changes BERT. (arXiv:2106.14282v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.14282">
<div class="article-summary-box-inner">
<span><p>Given the prevalence of pre-trained contextualized representations in today's
NLP, there have been many efforts to understand what information they contain,
and why they seem to be universally successful. The most common approach to use
these representations involves fine-tuning them for an end task. Yet, how
fine-tuning changes the underlying embedding space is less studied. In this
work, we study the English BERT family and use two probing techniques to
analyze how fine-tuning changes the space. We hypothesize that fine-tuning
affects classification performance by increasing the distances between examples
associated with different labels. We confirm this hypothesis with carefully
designed experiments on five different NLP tasks. Via these experiments, we
also discover an exception to the prevailing wisdom that "fine-tuning always
improves performance". Finally, by comparing the representations before and
after fine-tuning, we discover that fine-tuning does not introduce arbitrary
changes to representations; instead, it adjusts the representations to
downstream tasks while largely preserving the original spatial structure of the
data points.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FaVIQ: FAct Verification from Information-seeking Questions. (arXiv:2107.02153v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.02153">
<div class="article-summary-box-inner">
<span><p>Despite significant interest in developing general purpose fact checking
models, it is challenging to construct a large-scale fact verification dataset
with realistic real-world claims. Existing claims are either authored by
crowdworkers, thereby introducing subtle biases that are difficult to control
for, or manually verified by professional fact checkers, causing them to be
expensive and limited in scale. In this paper, we construct a large-scale
challenging fact verification dataset called FAVIQ, consisting of 188k claims
derived from an existing corpus of ambiguous information-seeking questions. The
ambiguities in the questions enable automatically constructing true and false
claims that reflect user confusions (e.g., the year of the movie being filmed
vs. being released). Claims in FAVIQ are verified to be natural, contain little
lexical bias, and require a complete understanding of the evidence for
verification. Our experiments show that the state-of-the-art models are far
from solving our new task. Moreover, training on our data helps in professional
fact-checking, outperforming models trained on the widely used dataset FEVER or
in-domain data by up to 17% absolute. Altogether, our data will serve as a
challenging benchmark for natural language understanding and support future
progress in professional fact checking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Noisy Channel Language Model Prompting for Few-Shot Text Classification. (arXiv:2108.04106v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04106">
<div class="article-summary-box-inner">
<span><p>We introduce a noisy channel approach for language model prompting in
few-shot text classification. Instead of computing the likelihood of the label
given the input (referred as direct models), channel models compute the
conditional probability of the input given the label, and are thereby required
to explain every word in the input. We use channel models for recently proposed
few-shot learning methods with no or very limited updates to the language model
parameters, via either in-context demonstration or prompt tuning. Our
experiments show that, for both methods, channel models significantly
outperform their direct counterparts, which we attribute to their stability,
i.e., lower variance and higher worst-case accuracy. We also present extensive
ablations that provide recommendations for when to use channel prompt tuning
instead of other competitive methods (e.g., direct head tuning): channel prompt
tuning is preferred when the number of training examples is small, labels in
the training data are imbalanced, or generalization to unseen labels is
required.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FlipDA: Effective and Robust Data Augmentation for Few-Shot Learning. (arXiv:2108.06332v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06332">
<div class="article-summary-box-inner">
<span><p>Most previous methods for text data augmentation are limited to simple tasks
and weak baselines. We explore data augmentation on hard tasks (i.e., few-shot
natural language understanding) and strong baselines (i.e., pretrained models
with over one billion parameters). Under this setting, we reproduced a large
number of previous augmentation methods and found that these methods bring
marginal gains at best and sometimes degrade the performance much. To address
this challenge, we propose a novel data augmentation method FlipDA that jointly
uses a generative model and a classifier to generate label-flipped data.
Central to the idea of FlipDA is the discovery that generating label-flipped
data is more crucial to the performance than generating label-preserved data.
Experiments show that FlipDA achieves a good tradeoff between effectiveness and
robustness -- it substantially improves many tasks while not negatively
affecting the others.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Statutory Article Retrieval Dataset in French. (arXiv:2108.11792v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11792">
<div class="article-summary-box-inner">
<span><p>Statutory article retrieval is the task of automatically retrieving law
articles relevant to a legal question. While recent advances in natural
language processing have sparked considerable interest in many legal tasks,
statutory article retrieval remains primarily untouched due to the scarcity of
large-scale and high-quality annotated datasets. To address this bottleneck, we
introduce the Belgian Statutory Article Retrieval Dataset (BSARD), which
consists of 1,100+ French native legal questions labeled by experienced jurists
with relevant articles from a corpus of 22,600+ Belgian law articles. Using
BSARD, we benchmark several state-of-the-art retrieval approaches, including
lexical and dense architectures, both in zero-shot and supervised setups. We
find that fine-tuned dense retrieval models significantly outperform other
systems. Our best performing baseline achieves 74.8% R@100, which is promising
for the feasibility of the task and indicates there is still room for
improvement. By the specificity of the domain and addressed task, BSARD
presents a unique challenge problem for future research on legal information
retrieval. Our dataset and source code are publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spelling provides a precise (but sometimes misplaced) phonological target. Orthography and acoustic variability in second language word learning. (arXiv:2109.03490v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03490">
<div class="article-summary-box-inner">
<span><p>L1 French participants learned novel L2 English words over two days of
learning sessions, with half of the words presented with their orthographic
forms (Audio-Ortho) and half without (Audio only). One group heard the words
pronounced by a single talker, while another group heard them pronounced by
multiple talkers. On the third day, they completed a variety of tasks to
evaluate their learning. Our results show a robust influence of orthography,
with faster response times in both production (Picture naming) and recognition
(Picture mapping) tasks for words learned in the Audio-Ortho condition.
Moreover, formant analyses of the Picture naming responses show that
orthographic input pulls pronunciations of English novel words towards a
non-native (French) phonological target. Words learned with their orthographic
forms were pronounced more precisely (with smaller Dispersion Scores), but were
misplaced in the vowel space (as reflected by smaller Euclidian distances with
respect to French vowels). Finally, we found only limited evidence of an effect
of talker-based acoustic variability: novel words learned with multiple talkers
showed faster responses times in the Picture naming task, but only in the
Audio-only condition, which suggests that orthographic information may have
overwhelmed any advantage of talker-based acoustic variability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AraT5: Text-to-Text Transformers for Arabic Language Generation. (arXiv:2109.12068v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12068">
<div class="article-summary-box-inner">
<span><p>Transfer learning with a unified Transformer framework (T5) that converts all
language problems into a text-to-text format was recently proposed as a simple
and effective transfer learning approach. Although a multilingual version of
the T5 model (mT5) was also introduced, it is not clear how well it can fare on
non-English tasks involving diverse data. To investigate this question, we
apply mT5 on a language with a wide variety of dialects--Arabic. For
evaluation, we introduce a novel benchmark for ARabic language GENeration
(ARGEN), covering seven important tasks. For model comparison, we pre-train
three powerful Arabic T5-style models and evaluate them on ARGEN. Although
pre-trained with ~49 less data, our new models perform significantly better
than mT5 on all ARGEN tasks (in 52 out of 59 test sets) and set several new
SOTAs. Our models also establish new SOTA on the recently-proposed, large
Arabic language understanding evaluation benchmark ARLUE (Abdul-Mageed et al.,
2021). Our new models are publicly available. We also link to ARGEN datasets
through our repository: https://github.com/UBC-NLP/araT5.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FewNLU: Benchmarking State-of-the-Art Methods for Few-Shot Natural Language Understanding. (arXiv:2109.12742v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12742">
<div class="article-summary-box-inner">
<span><p>The few-shot natural language understanding (NLU) task has attracted much
recent attention. However, prior methods have been evaluated under a disparate
set of protocols, which hinders fair comparison and measuring progress of the
field. To address this issue, we introduce an evaluation framework that
improves previous evaluation procedures in three key aspects, i.e., test
performance, dev-test correlation, and stability. Under this new evaluation
framework, we re-evaluate several state-of-the-art few-shot methods for NLU
tasks. Our framework reveals new insights: (1) both the absolute performance
and relative gap of the methods were not accurately estimated in prior
literature; (2) no single method dominates most tasks with consistent
performance; (3) improvements of some methods diminish with a larger pretrained
model; and (4) gains from different methods are often complementary and the
best combined model performs close to a strong fully-supervised baseline. We
open-source our toolkit, FewNLU, that implements our evaluation framework along
with a number of state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantically Distributed Robust Optimization for Vision-and-Language Inference. (arXiv:2110.07165v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07165">
<div class="article-summary-box-inner">
<span><p>Analysis of vision-and-language models has revealed their brittleness under
linguistic phenomena such as paraphrasing, negation, textual entailment, and
word substitutions with synonyms or antonyms. While data augmentation
techniques have been designed to mitigate against these failure modes, methods
that can integrate this knowledge into the training pipeline remain
under-explored. In this paper, we present \textbf{SDRO}, a model-agnostic
method that utilizes a set linguistic transformations in a distributed robust
optimization setting, along with an ensembling technique to leverage these
transformations during inference. Experiments on benchmark datasets with images
(NLVR$^2$) and video (VIOLIN) demonstrate performance improvements as well as
robustness to adversarial attacks. Experiments on binary VQA explore the
generalizability of this method to other V\&amp;L tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Explanations Be Useful for Calibrating Black Box Models?. (arXiv:2110.07586v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07586">
<div class="article-summary-box-inner">
<span><p>NLP practitioners often want to take existing trained models and apply them
to data from new domains. While fine-tuning or few-shot learning can be used to
adapt a base model, there is no single recipe for making these techniques work;
moreover, one may not have access to the original model weights if it is
deployed as a black box. We study how to improve a black box model's
performance on a new domain by leveraging explanations of the model's behavior.
Our approach first extracts a set of features combining human intuition about
the task with model attributions generated by black box interpretation
techniques, then uses a simple calibrator, in the form of a classifier, to
predict whether the base model was correct or not. We experiment with our
method on two tasks, extractive question answering and natural language
inference, covering adaptation from several pairs of domains with limited
target-domain data. The experimental results across all the domain pairs show
that explanations are useful for calibrating these models, boosting accuracy
when predictions do not have to be returned on every example. We further show
that the calibration model transfers to some extent between tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structural Characterization for Dialogue Disentanglement. (arXiv:2110.08018v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08018">
<div class="article-summary-box-inner">
<span><p>Tangled multi-party dialogue contexts lead to challenges for dialogue reading
comprehension, where multiple dialogue threads flow simultaneously within a
common dialogue record, increasing difficulties in understanding the dialogue
history for both human and machine. Previous studies mainly focus on utterance
encoding methods with carefully designed features but pay inadequate attention
to characteristic features of the structure of dialogues. We specially take
structure factors into account and design a novel model for dialogue
disentangling. Based on the fact that dialogues are constructed on successive
participation and interactions between speakers, we model structural
information of dialogues in two aspects: 1)speaker property that indicates whom
a message is from, and 2) reference dependency that shows whom a message may
refer to. The proposed method achieves new state-of-the-art on the Ubuntu IRC
benchmark dataset and contributes to dialogue-related comprehension.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Dubber: Dubbing for Videos According to Scripts. (arXiv:2110.08243v3 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08243">
<div class="article-summary-box-inner">
<span><p>Dubbing is a post-production process of re-recording actors' dialogues, which
is extensively used in filmmaking and video production. It is usually performed
manually by professional voice actors who read lines with proper prosody, and
in synchronization with the pre-recorded videos. In this work, we propose
Neural Dubber, the first neural network model to solve a novel automatic video
dubbing (AVD) task: synthesizing human speech synchronized with the given video
from the text. Neural Dubber is a multi-modal text-to-speech (TTS) model that
utilizes the lip movement in the video to control the prosody of the generated
speech. Furthermore, an image-based speaker embedding (ISE) module is developed
for the multi-speaker setting, which enables Neural Dubber to generate speech
with a reasonable timbre according to the speaker's face. Experiments on the
chemistry lecture single-speaker dataset and LRS2 multi-speaker dataset show
that Neural Dubber can generate speech audios on par with state-of-the-art TTS
models in terms of speech quality. Most importantly, both qualitative and
quantitative evaluations show that Neural Dubber can control the prosody of
synthesized speech by the video, and generate high-fidelity speech temporally
synchronized with the video. Our project page is at
https://tsinghua-mars-lab.github.io/NeuralDubber/ .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ASPECTNEWS: Aspect-Oriented Summarization of News Documents. (arXiv:2110.08296v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08296">
<div class="article-summary-box-inner">
<span><p>Generic summaries try to cover an entire document and query-based summaries
try to answer document-specific questions. But real users' needs often fall in
between these extremes and correspond to aspects, high-level topics discussed
among similar types of documents. In this paper, we collect a dataset of
realistic aspect-oriented summaries, AspectNews, which covers different
subtopics about articles in news sub-domains. We annotate data across two
domains of articles, earthquakes and fraud investigations, where each article
is annotated with two distinct summaries focusing on different aspects for each
domain. A system producing a single generic summary cannot concisely satisfy
both aspects. Our focus in evaluation is how well existing techniques can
generalize to these domains without seeing in-domain training data, so we turn
to techniques to construct synthetic training data that have been used in
query-focused summarization work. We compare several training schemes that
differ in how strongly keywords are used and how oracle summaries are
extracted. Our evaluation shows that our final approach yields (a) focused
summaries, better than those from a generic summarization system or from
keyword matching; (b) a system sensitive to the choice of keywords.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training Dynamics for Text Summarization Models. (arXiv:2110.08370v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08370">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (e.g. BART) have shown impressive results when
fine-tuned on large summarization datasets. However, little is understood about
this fine-tuning process, including what knowledge is retained from
pre-training time or how content selection and generation strategies are learnt
across iterations. In this work, we analyze the training dynamics for
generation models, focusing on summarization. Across different datasets
(CNN/DM, XSum, MediaSum) and summary properties, such as abstractiveness and
hallucination, we study what the model learns at different stages of its
fine-tuning process. We find that a propensity to copy the input is learned
early in the training process consistently across all datasets studied. On the
other hand, factual errors, such as hallucination of unsupported facts, are
learnt in the later stages, though this behavior is more varied across domains.
Based on these observations, we explore complementary approaches for modifying
training: first, disregarding high-loss tokens that are challenging to learn
and second, disregarding low-loss tokens that are learnt very quickly in the
latter stages of the training process. We show that these simple training
modifications allow us to configure our model to achieve different goals, such
as improving factuality or improving abstractiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual unsupervised sequence segmentation transfers to extremely low-resource languages. (arXiv:2110.08415v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08415">
<div class="article-summary-box-inner">
<span><p>We show that unsupervised sequence-segmentation performance can be
transferred to extremely low-resource languages by pre-training a Masked
Segmental Language Model (Downey et al., 2021) multilingually. Further, we show
that this transfer can be achieved by training over a collection of
low-resource languages that are typologically similar (but phylogenetically
unrelated) to the target language. In our experiments, we transfer from a
collection of 10 Indigenous American languages (AmericasNLP, Mager et al.,
2021) to K'iche', a Mayan language. We compare our multilingual model to a
monolingual (from-scratch) baseline, as well as a model pre-trained on Quechua
only. We show that the multilingual pre-trained approach yields consistent
segmentation quality across target dataset sizes, exceeding the monolingual
baseline in 6/10 experimental settings. Our model yields especially strong
results at small target sizes, including a zero-shot performance of 20.6 F1.
These results have promising implications for low-resource NLP pipelines
involving human-like linguistic units, such as the sparse transcription
framework proposed by Bird (2020).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Natural Language Inference Using PHL Triplet Generation. (arXiv:2110.08438v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08438">
<div class="article-summary-box-inner">
<span><p>Transformer-based models achieve impressive performance on numerous Natural
Language Inference (NLI) benchmarks when trained on respective training
datasets. However, in certain cases, training samples may not be available or
collecting them could be time-consuming and resource-intensive. In this work,
we address the above challenge and present an explorative study on unsupervised
NLI, a paradigm in which no human-annotated training samples are available. We
investigate it under three settings: PH, P, and NPH that differ in the extent
of unlabeled data available for learning. As a solution, we propose a
procedural data generation approach that leverages a set of sentence
transformations to collect PHL (Premise, Hypothesis, Label) triplets for
training NLI models, bypassing the need for human-annotated training data.
Comprehensive experiments with several NLI datasets show that the proposed
approach results in accuracies of up to 66.75%, 65.9%, 65.39% in PH, P, and NPH
settings respectively, outperforming all existing unsupervised baselines.
Furthermore, fine-tuning our model with as little as ~0.1% of the
human-annotated training dataset (500 instances) leads to 12.2% higher accuracy
than the model trained from scratch on the same 500 instances. Supported by
this superior performance, we conclude with a recommendation for collecting
high-quality task-specific data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models. (arXiv:2110.08484v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08484">
<div class="article-summary-box-inner">
<span><p>Large pre-trained vision-language (VL) models can learn a new task with a
handful of examples and generalize to a new task without fine-tuning. However,
these VL models are hard to deploy for real-world applications due to their
impractically huge sizes and slow inference speed. To solve this limitation, we
study prompt-based low-resource learning of VL tasks with our proposed method,
FewVLM, relatively smaller than recent few-shot learners. For FewVLM, we
pre-train a sequence-to-sequence transformer model with prefix language
modeling (PrefixLM) and masked language modeling (MaskedLM). Furthermore, we
analyze the effect of diverse prompts for few-shot tasks. Experimental results
on VQA show that FewVLM with prompt-based learning outperforms Frozen which is
31x larger than FewVLM by 18.2% point and achieves comparable results to a 246x
larger model, PICa. In our analysis, we observe that (1) prompts significantly
affect zero-shot performance but marginally affect few-shot performance, (2)
models with noisy prompts learn as quickly as hand-crafted prompts given larger
training data, and (3) MaskedLM helps VQA tasks while PrefixLM boosts
captioning performance. Our code is publicly available at
\url{https://github.com/woojeongjin/FewVLM}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NewsClaims: A New Benchmark for Claim Detection from News with Background Knowledge. (arXiv:2112.08544v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08544">
<div class="article-summary-box-inner">
<span><p>Claim detection and verification are crucial for news understanding and have
emerged as promising technologies for mitigating news misinformation. However,
most existing work has focused on {\em claim sentence} analysis while
overlooking crucial background attributes (e.g., claimer, claim objects). In
this work, we present NewsClaims, a new benchmark for knowledge-aware claim
detection in the news domain. We redefine the claim detection problem to
include extraction of additional background attributes related to each claim
and release 889 claims annotated over 143 news articles. NewsClaims aims to
benchmark claim detection systems in emerging scenarios, comprising unseen
topics with little or no training data. To this end, we provide a comprehensive
evaluation of zero-shot and prompt-based baselines for NewsClaims.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incomplete Knowledge Graph Alignment. (arXiv:2112.09266v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09266">
<div class="article-summary-box-inner">
<span><p>Knowledge graph (KG) alignment - the task of recognizing entities referring
to the same thing in different KGs - is recognized as one of the most important
operations in the field of KG construction and completion. However, existing
alignment techniques often assume that the input KGs are complete and
isomorphic, which is not true due to the real-world heterogeneity in the
domain, size, and sparsity. In this work, we address the problem of aligning
incomplete KGs with representation learning. Our KG embedding framework
exploits two feature channels: transitivity-based and proximity-based. The
former captures the consistency constraints between entities via translation
paths, while the latter captures the neighbourhood structure of KGs via
attention guided relation-aware graph neural network. The two feature channels
are jointly learned to exchange important features between the input KGs while
enforcing the output representations of the input KGs in the same embedding
space. Also, we develop a missing links detector that discovers and recovers
the missing links in the input KGs during the training process, which helps
mitigate the incompleteness issue and thus improve the compatibility of the
learned representations. The embeddings then are fused to generate the
alignment result, and the high-confidence matched node pairs are updated to the
pre-aligned supervision data to improve the embeddings gradually. Empirical
results show that our model is more accurate than the SOTA and is robust
against different levels of incompleteness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine Translation from Signed to Spoken Languages: State of the Art and Challenges. (arXiv:2202.03086v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03086">
<div class="article-summary-box-inner">
<span><p>Automatic translation from signed to spoken languages is an interdisciplinary
research domain, lying on the intersection of computer vision, machine
translation and linguistics. Nevertheless, research in this domain is performed
mostly by computer scientists in isolation. As the domain is becoming
increasingly popular - the majority of scientific papers on the topic of sign
language translation have been published in the past three years - we provide
an overview of the state of the art as well as some required background in the
different related disciplines. We give a high-level introduction to sign
language linguistics and machine translation to illustrate the requirements of
automatic sign language translation. We present a systematic literature review
to illustrate the state of the art in the domain and then, harking back to the
requirements, lay out several challenges for future research. We find that
significant advances have been made on the shoulders of spoken language machine
translation research. However, current approaches are often not linguistically
motivated or are not adapted to the different input modality of sign languages.
We explore challenges related to the representation of sign language data, the
collection of datasets, the need for interdisciplinary research and
requirements for moving beyond research, towards applications. Based on our
findings, we advocate for interdisciplinary research and to base future
research on linguistic analysis of sign languages. Furthermore, the inclusion
of deaf and hearing end users of sign language translation applications in use
case identification, data collection and evaluation is of the utmost importance
in the creation of useful sign language translation models. We recommend
iterative, human-in-the-loop, design and development of sign language
translation models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StoryBuddy: A Human-AI Collaborative Chatbot for Parent-Child Interactive Storytelling with Flexible Parental Involvement. (arXiv:2202.06205v2 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06205">
<div class="article-summary-box-inner">
<span><p>Despite its benefits for children's skill development and parent-child
bonding, many parents do not often engage in interactive storytelling by having
story-related dialogues with their child due to limited availability or
challenges in coming up with appropriate questions. While recent advances made
AI generation of questions from stories possible, the fully-automated approach
excludes parent involvement, disregards educational goals, and underoptimizes
for child engagement. Informed by need-finding interviews and participatory
design (PD) results, we developed StoryBuddy, an AI-enabled system for parents
to create interactive storytelling experiences. StoryBuddy's design highlighted
the need for accommodating dynamic user needs between the desire for parent
involvement and parent-child bonding and the goal of minimizing parent
intervention when busy. The PD revealed varied assessment and educational goals
of parents, which StoryBuddy addressed by supporting configuring question types
and tracking child progress. A user study validated StoryBuddy's usability and
suggested design insights for future parent-AI collaboration systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Effects of Interactive AI Design on User Behavior: An Eye-tracking Study of Fact-checking COVID-19 Claims. (arXiv:2202.08901v2 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08901">
<div class="article-summary-box-inner">
<span><p>We conducted a lab-based eye-tracking study to investigate how the
interactivity of an AI-powered fact-checking system affects user interactions,
such as dwell time, attention, and mental resources involved in using the
system. A within-subject experiment was conducted, where participants used an
interactive and a non-interactive version of a mock AI fact-checking system and
rated their perceived correctness of COVID-19 related claims. We collected
web-page interactions, eye-tracking data, and mental workload using NASA-TLX.
We found that the presence of the affordance of interactively manipulating the
AI system's prediction parameters affected users' dwell times, and
eye-fixations on AOIs, but not mental workload. In the interactive system,
participants spent the most time evaluating claims' correctness, followed by
reading news. This promising result shows a positive role of interactivity in a
mixed-initiative AI-powered system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attentive Temporal Pooling for Conformer-based Streaming Language Identification in Long-form Speech. (arXiv:2202.12163v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12163">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce a novel language identification system based on
conformer layers. We propose an attentive temporal pooling mechanism to allow
the model to carry information in long-form audio via a recurrent form, such
that the inference can be performed in a streaming fashion. Additionally, a
simple domain adaptation mechanism is introduced to allow adapting an existing
language identification model to a new domain where the prior language
distribution is different. We perform a comparative study of different model
topologies under different constraints of model size, and find that
conformer-base models outperform LSTM and transformer based models. Our
experiments also show that attentive temporal pooling and domain adaptation
significantly improve the model accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Supervision: Enabling Generalization over Output Spaces. (arXiv:2202.13100v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13100">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose Semantic Supervision (SemSup) - a unified paradigm
for training classifiers that generalize over output spaces. In contrast to
standard classification, which treats classes as discrete symbols, SemSup
represents them as dense vector features obtained from descriptions of classes
(e.g., "The cat is a small carnivorous mammal"). This allows the output space
to be unbounded (in the space of descriptions) and enables models to generalize
both over unseen inputs and unseen outputs (e.g. "The aardvark is a nocturnal
burrowing mammal with long ears"). Specifically, SemSup enables four types of
generalization, to -- (1) unseen class descriptions, (2) unseen classes, (3)
unseen super-classes, and (4) unseen tasks. Through experiments on four
classification datasets across two variants (multi-class and multi-label), two
input modalities (text and images), and two output description modalities (text
and JSON), we show that our SemSup models significantly outperform standard
supervised models and existing models that leverage word embeddings over class
names. For instance, our model outperforms baselines by 40% and 15% precision
points on unseen descriptions and classes, respectively, on a news
categorization dataset (RCV1). SemSup can serve as a pathway for scaling neural
models to large unbounded output spaces and enabling better generalization and
model reuse for unseen tasks and domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Variational Autoencoder with Disentanglement Priors for Low-Resource Task-Specific Natural Language Generation. (arXiv:2202.13363v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13363">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a variational autoencoder with disentanglement
priors, VAE-DPRIOR, for conditional natural language generation with none or a
handful of task-specific labeled examples. In order to improve compositional
generalization, our model performs disentangled representation learning by
introducing a prior for the latent content space and another prior for the
latent label space. We show both empirically and theoretically that the
conditional priors can already disentangle representations even without
specific regularizations as in the prior work. We can also sample diverse
content representations from the content space without accessing data of the
seen tasks, and fuse them with the representations of novel tasks for
generating diverse texts in the low-resource settings. Our extensive
experiments demonstrate the superior performance of our model over competitive
baselines in terms of i) data augmentation in continuous zero/few-shot
learning, and ii) text style transfer in both zero/few-shot settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Candidate Retrieval with Entity Profile Generation for Wikidata Entity Linking. (arXiv:2202.13404v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13404">
<div class="article-summary-box-inner">
<span><p>Entity linking (EL) is the task of linking entity mentions in a document to
referent entities in a knowledge base (KB). Many previous studies focus on
Wikipedia-derived KBs. There is little work on EL over Wikidata, even though it
is the most extensive crowdsourced KB. The scale of Wikidata can open up many
new real-world applications, but its massive number of entities also makes EL
challenging. To effectively narrow down the search space, we propose a novel
candidate retrieval paradigm based on entity profiling. Wikidata entities and
their textual fields are first indexed into a text search engine (e.g.,
Elasticsearch). During inference, given a mention and its context, we use a
sequence-to-sequence (seq2seq) model to generate the profile of the target
entity, which consists of its title and description. We use the profile to
query the indexed search engine to retrieve candidate entities. Our approach
complements the traditional approach of using a Wikipedia anchor-text
dictionary, enabling us to further design a highly effective hybrid method for
candidate retrieval. Combined with a simple cross-attention reranker, our
complete EL framework achieves state-of-the-art results on three Wikidata-based
datasets and strong performance on TACKBP-2010.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Transformers use variable binding?. (arXiv:2203.00162v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00162">
<div class="article-summary-box-inner">
<span><p>Increasing the explainability of deep neural networks (DNNs) requires
evaluating whether they implement symbolic computation. One central symbolic
capacity is variable binding: linking an input value to an abstract variable
held in system-internal memory. Prior work on the computational abilities of
DNNs has not resolved the question of whether their internal processes involve
variable binding. We argue that the reason for this is fundamental, inherent in
the way experiments in prior work were designed. We provide the first
systematic evaluation of the variable binding capacities of the
state-of-the-art Transformer networks BERT and RoBERTa. Our experiments are
designed such that the model must generalize a rule across disjoint subsets of
the input vocabulary, and cannot rely on associative pattern matching alone.
The results show a clear discrepancy between classification and
sequence-to-sequence tasks: BERT and RoBERTa can easily learn to copy or
reverse strings even when trained on task-specific vocabularies that are
switched in the test set; but both models completely fail to generalize across
vocabularies in similar sequence classification tasks. These findings indicate
that the effectiveness of Transformers in sequence modelling may lie in their
extensive use of the input itself as an external "memory" rather than
network-internal symbolic operations involving variable binding. Therefore, we
propose a novel direction for future work: augmenting the inputs available to
circumvent the lack of network-internal variable binding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speciesist Language and Nonhuman Animal Bias in English Masked Language Models. (arXiv:2203.05140v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05140">
<div class="article-summary-box-inner">
<span><p>Various existing studies have analyzed what social biases are inherited by
NLP models. These biases may directly or indirectly harm people, therefore
previous studies have focused only on human attributes. If the social biases in
NLP models can be indirectly harmful to humans involved, then the models can
also indirectly harm nonhuman animals. However, until recently no research on
social biases in NLP regarding nonhumans existed. In this paper, we analyze
biases to nonhuman animals, i.e. speciesist bias, inherent in English Masked
Language Models. We analyze this bias using template-based and corpus-extracted
sentences which contain speciesist (or non-speciesist) language, to show that
these models tend to associate harmful words with nonhuman animals. Our code
for reproducing the experiments will be made available on GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Visual-Prompt Temporal Answering Grounding in Medical Instructional Video. (arXiv:2203.06667v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06667">
<div class="article-summary-box-inner">
<span><p>The temporal answering grounding in the video (TAGV) is a new task naturally
deriving from temporal sentence grounding in the video (TSGV). Given an
untrimmed video and a text question, this task aims at locating the matching
span from the video that can semantically answer the question. Existing methods
tend to formulate the TAGV task with a visual span-based question answering
(QA) approach by matching the visual frame span queried by the text question.
However, due to the weak correlations and huge gaps in semantics in features
between the textual question and visual answer, existing methods adopting
visual span predictor fail to perform well in the TAGV task. In this work, we
propose a visual-prompt text span localizing (VPTSL) method, which enhances the
text span localization in the pre-trained language model (PLM) with the visual
highlight features. Specifically, the context query attention is utilized to
perform cross-modal modeling between the textual and visual features. Then, the
highlight features are obtained through the highlight module with a linear
layer to provide the visual prompt. To alleviate the differences in semantics
and correlations between textual and visual features, we design the text span
predictor by encoding the question, the subtitles, and the visual prompt in the
PLM. As a result, the TAGV task is formulated to predict the span of subtitles
matching the answering frame timeline. Extensive experiments on the medical
instructional dataset, namely MedVidQA, show the proposed VPTSL outperforms
other state-of-the-art methods, which demonstrates the effectiveness of visual
prompt and the text span predictor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SciNLI: A Corpus for Natural Language Inference on Scientific Text. (arXiv:2203.06728v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06728">
<div class="article-summary-box-inner">
<span><p>Existing Natural Language Inference (NLI) datasets, while being instrumental
in the advancement of Natural Language Understanding (NLU) research, are not
related to scientific text. In this paper, we introduce SciNLI, a large dataset
for NLI that captures the formality in scientific text and contains 107,412
sentence pairs extracted from scholarly papers on NLP and computational
linguistics. Given that the text used in scientific literature differs vastly
from the text used in everyday language both in terms of vocabulary and
sentence structure, our dataset is well suited to serve as a benchmark for the
evaluation of scientific NLU models. Our experiments show that SciNLI is harder
to classify than the existing NLI datasets. Our best performing model with
XLNet achieves a Macro F1 score of only 78.18% and an accuracy of 78.23%
showing that there is substantial room for improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models. (arXiv:2203.06904v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06904">
<div class="article-summary-box-inner">
<span><p>Despite the success, the process of fine-tuning large-scale PLMs brings
prohibitive adaptation costs. In fact, fine-tuning all the parameters of a
colossal model and retaining separate instances for different tasks are
practically infeasible. This necessitates a new branch of research focusing on
the parameter-efficient adaptation of PLMs, dubbed as delta tuning in this
paper. In contrast with the standard fine-tuning, delta tuning only fine-tunes
a small portion of the model parameters while keeping the rest untouched,
largely reducing both the computation and storage costs. Recent studies have
demonstrated that a series of delta tuning methods with distinct tuned
parameter selection could achieve performance on a par with full-parameter
fine-tuning, suggesting a new promising way of stimulating large-scale PLMs. In
this paper, we first formally describe the problem of delta tuning and then
comprehensively review recent delta tuning approaches. We also propose a
unified categorization criterion that divide existing delta tuning methods into
three groups: addition-based, specification-based, and reparameterization-based
methods. Though initially proposed as an efficient method to steer large
models, we believe that some of the fascinating evidence discovered along with
delta tuning could help further reveal the mechanisms of PLMs and even deep
neural networks. To this end, we discuss the theoretical principles underlying
the effectiveness of delta tuning and propose frameworks to interpret delta
tuning from the perspective of optimization and optimal control, respectively.
Furthermore, we provide a holistic empirical study of representative methods,
where results on over 100 NLP tasks demonstrate a comprehensive performance
comparison of different approaches. The experimental results also cover the
analysis of combinatorial, scaling and transferable properties of delta tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">XYLayoutLM: Towards Layout-Aware Multimodal Networks For Visually-Rich Document Understanding. (arXiv:2203.06947v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06947">
<div class="article-summary-box-inner">
<span><p>Recently, various multimodal networks for Visually-Rich Document
Understanding(VRDU) have been proposed, showing the promotion of transformers
by integrating visual and layout information with the text embeddings. However,
most existing approaches utilize the position embeddings to incorporate the
sequence information, neglecting the noisy improper reading order obtained by
OCR tools. In this paper, we propose a robust layout-aware multimodal network
named XYLayoutLM to capture and leverage rich layout information from proper
reading orders produced by our Augmented XY Cut. Moreover, a Dilated
Conditional Position Encoding module is proposed to deal with the input
sequence of variable lengths, and it additionally extracts local layout
information from both textual and visual modalities while generating position
embeddings. Experiment results show that our XYLayoutLM achieves competitive
results on document understanding tasks.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing crowd flow prediction in various spatial and temporal granularities. (arXiv:2203.07372v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07372">
<div class="article-summary-box-inner">
<span><p>Thanks to the diffusion of the Internet of Things, nowadays it is possible to
sense human mobility almost in real time using unconventional methods (e.g.,
number of bikes in a bike station). Due to the diffusion of such technologies,
the last years have witnessed a significant growth of human mobility studies,
motivated by their importance in a wide range of applications, from traffic
management to public security and computational epidemiology. A mobility task
that is becoming prominent is crowd flow prediction, i.e., forecasting
aggregated incoming and outgoing flows in the locations of a geographic region.
Although several deep learning approaches have been proposed to solve this
problem, their usage is limited to specific types of spatial tessellations and
cannot provide sufficient explanations of their predictions. We propose
CrowdNet, a solution to crowd flow prediction based on graph convolutional
networks. Compared with state-of-the-art solutions, CrowdNet can be used with
regions of irregular shapes and provide meaningful explanations of the
predicted crowd flows. We conduct experiments on public data varying the
spatio-temporal granularity of crowd flows to show the superiority of our model
with respect to existing methods, and we investigate CrowdNet's reliability to
missing or noisy input data. Our model is a step forward in the design of
reliable deep learning models to predict and explain human displacements in
urban environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SATr: Slice Attention with Transformer for Universal Lesion Detection. (arXiv:2203.07373v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07373">
<div class="article-summary-box-inner">
<span><p>Universal Lesion Detection (ULD) in computed tomography plays an essential
role in computer-aided diagnosis. Promising ULD results have been reported by
multi-slice-input detection approaches which model 3D context from multiple
adjacent CT slices, but such methods still experience difficulty in obtaining a
global representation among different slices and within each individual slice
since they only use convolution-based fusion operations. In this paper, we
propose a novel Slice Attention Transformer (SATr) block which can be easily
plugged into convolution-based ULD backbones to form hybrid network structures.
Such newly formed hybrid backbones can better model long-distance feature
dependency via the cascaded self-attention modules in the Transformer block
while still holding a strong power of modeling local features with the
convolutional operations in the original backbone. Experiments with five
state-of-the-art methods show that the proposed SATr block can provide an
almost free boost to lesion detection accuracy without extra hyperparameters or
special network designs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">There's no difference: Convolutional Neural Networks for transient detection without template subtraction. (arXiv:2203.07390v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07390">
<div class="article-summary-box-inner">
<span><p>We present a Convolutional Neural Network (CNN) model for the separation of
astrophysical transients from image artifacts, a task known as "real-bogus"
classification, that does not rely on Difference Image Analysis (DIA) which is
a computationally expensive process involving image matching on small spatial
scales in large volumes of data. We explore the use of CNNs to (1) automate the
"real-bogus" classification, (2) reduce the computational costs of transient
discovery. We compare the efficiency of two CNNs with similar architectures,
one that uses "image triplets" (templates, search, and the corresponding
difference image) and one that adopts a similar architecture but takes as input
the template and search only. Without substantially changing the model
architecture or retuning the hyperparameters to the new input, we observe only
a small decrease in model efficiency (97% to 92% accuracy). We further
investigate how the model that does not receive the difference image learns the
required information from the template and search by exploring the saliency
maps. Our work demonstrates that (1) CNNs are excellent models for "real-bogus"
classification that rely exclusively on the imaging data and require no feature
engineering task; (2) high-accuracy models can be built without the need to
construct difference images. Since once trained, neural networks can generate
predictions at minimal computational costs, we argue that future
implementations of this methodology could dramatically reduce the computational
costs in the detection of genuine transients in synoptic surveys like Rubin
Observatory's Legacy Survey of Space and Time by bypassing the DIA step
entirely.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Panoptic animal pose estimators are zero-shot performers. (arXiv:2203.07436v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07436">
<div class="article-summary-box-inner">
<span><p>Animal pose estimation is critical in applications ranging from life science
research, agriculture, to veterinary medicine. Compared to human pose
estimation, the performance of animal pose estimation is limited by the size of
available datasets and the generalization of a model across datasets. Typically
different keypoints are labeled regardless of whether the species are the same
or not, leaving animal pose datasets to have disjoint or partially overlapping
keypoints. As a consequence, a model cannot be used as a plug-and-play solution
across datasets. This reality motivates us to develop panoptic animal pose
estimation models that are able to predict keypoints defined in all datasets.
In this work we propose a simple yet effective way to merge differentially
labeled datasets to obtain the largest quadruped and lab mouse pose dataset.
Using a gradient masking technique, so called SuperAnimal-models are able to
predict keypoints that are distributed across datasets and exhibit strong
zero-shot performance. The models can be further improved by (pseudo) labeled
fine-tuning. These models outperform ImageNet-initialized models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Clustering of Roman Potsherds via Variational Autoencoders. (arXiv:2203.07437v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07437">
<div class="article-summary-box-inner">
<span><p>In this paper we propose an artificial intelligence imaging solution to
support archaeologists in the classification task of Roman commonware
potsherds. Usually, each potsherd is represented by its sectional profile as a
two dimensional black-white image and printed in archaeological books related
to specific archaeological excavations. The partiality and handcrafted variance
of the fragments make their matching a challenging problem: we propose to pair
similar profiles via the unsupervised hierarchical clustering of non-linear
features learned in the latent space of a deep convolutional Variational
Autoencoder (VAE) network. Our contribution also include the creation of a
ROman COmmonware POTtery (ROCOPOT) database, with more than 4000 potsherds
profiles extracted from 25 Roman pottery corpora, and a MATLAB GUI software for
the easy inspection of shape similarities. Results are commented both from a
mathematical and archaeological perspective so as to unlock new research
directions in both communities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A deep learning pipeline for breast cancer ki-67 proliferation index scoring. (arXiv:2203.07452v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07452">
<div class="article-summary-box-inner">
<span><p>The Ki-67 proliferation index is an essential biomarker that helps
pathologists to diagnose and select appropriate treatments. However, automatic
evaluation of Ki-67 is difficult due to nuclei overlapping and complex
variations in their properties. This paper proposes an integrated pipeline for
accurate automatic counting of Ki-67, where the impact of nuclei separation
techniques is highlighted. First, semantic segmentation is performed by
combining the Squeez and Excitation Resnet and Unet algorithms to extract
nuclei from the background. The extracted nuclei are then divided into
overlapped and non-overlapped regions based on eight geometric and statistical
features. A marker-based Watershed algorithm is subsequently proposed and
applied only to the overlapped regions to separate nuclei. Finally, deep
features are extracted from each nucleus patch using Resnet18 and classified
into positive or negative by a random forest classifier. The proposed
pipeline's performance is validated on a dataset from the Department of
Pathology at H\^opital Nord Franche-Comt\'e hospital.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Skydiver: A Spiking Neural Network Accelerator Exploiting Spatio-Temporal Workload Balance. (arXiv:2203.07516v1 [cs.AR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07516">
<div class="article-summary-box-inner">
<span><p>Spiking Neural Networks (SNNs) are developed as a promising alternative to
Artificial Neural networks (ANNs) due to their more realistic brain-inspired
computing models. SNNs have sparse neuron firing over time, i.e.,
spatio-temporal sparsity; thus, they are useful to enable energy-efficient
hardware inference. However, exploiting spatio-temporal sparsity of SNNs in
hardware leads to unpredictable and unbalanced workloads, degrading the energy
efficiency. In this work, we propose an FPGA-based convolutional SNN
accelerator called Skydiver that exploits spatio-temporal workload balance. We
propose the Approximate Proportional Relation Construction (APRC) method that
can predict the relative workload channel-wisely and a Channel-Balanced
Workload Schedule (CBWS) method to increase the hardware workload balance ratio
to over 90%. Skydiver was implemented on a Xilinx XC7Z045 FPGA and verified on
image segmentation and MNIST classification tasks. Results show improved
throughput by 1.4X and 1.2X for the two tasks. Skydiver achieved 22.6 KFPS
throughput, and 42.4 uJ/Image prediction energy on the classification task with
98.5% accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast Active Monocular Distance Estimation from Time-to-Contact. (arXiv:2203.07530v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07530">
<div class="article-summary-box-inner">
<span><p>Distance estimation is fundamental for a variety of robotic applications
including navigation, manipulation and planning. Inspired by the mammal's
visual system, which gazes at specific objects (active fixation), and estimates
when the object will reach it (time-to-contact), we develop a novel constraint
between time-to-contact, acceleration, and distance that we call the
$\tau$-constraint. It allows an active monocular camera to estimate depth using
time-to-contact and inertial measurements (linear accelerations and angular
velocities) within a window of time.
</p>
<p>Our work differs from other approaches by focusing on patches instead of
feature points. This is, because the change in the patch area determines the
time-to-contact directly. The result enables efficient estimation of distance
while using only a small portion of the image, leading to a large speedup.
</p>
<p>We successfully validate the proposed $\tau$-constraint in the application of
estimating camera position with a monocular grayscale camera and an Inertial
Measurement Unit (IMU). Specifically, we test our method on different
real-world planar objects over trajectories 8-40 seconds in duration and 7-35
meters long. Our method achieves 8.5 cm Average Trajectory Error (ATE) while
the popular Visual-Inertial Odometry methods VINS-Mono and ROVIO achieve 12.2
and 16.9 cm ATE respectively. Additionally, our implementation runs 27$\times$
faster than VINS-Mono's and 6.8$\times$ faster than ROVIO's. We believe these
results indicate the $\tau$-constraints potential to be the basis of robust,
sophisticated algorithms for a multitude of applications involving an active
camera and an IMU.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VPFusion: Joint 3D Volume and Pixel-Aligned Feature Fusion for Single and Multi-view 3D Reconstruction. (arXiv:2203.07553v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07553">
<div class="article-summary-box-inner">
<span><p>We introduce a unified single and multi-view neural implicit 3D
reconstruction framework VPFusion. VPFusion~attains high-quality reconstruction
using both - 3D feature volume to capture 3D-structure-aware context, and
pixel-aligned image features to capture fine local detail. Existing approaches
use RNN, feature pooling, or attention computed independently in each view for
multi-view fusion. RNNs suffer from long-term memory loss and permutation
variance, while feature pooling or independently computed attention leads to
representation in each view being unaware of other views before the final
pooling step. In contrast, we show improved multi-view feature fusion by
establishing transformer-based pairwise view association. In particular, we
propose a novel interleaved 3D reasoning and pairwise view association
architecture for feature volume fusion across different views. Using this
structure-aware and multi-view-aware feature volume, we show improved 3D
reconstruction performance compared to existing methods. VPFusion improves the
reconstruction quality further by also incorporating pixel-aligned local image
features to capture fine detail. We verify the effectiveness of VPFusion~on the
ShapeNet and ModelNet datasets, where we outperform or perform on-par the
state-of-the-art single and multi-view 3D shape reconstruction methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task-Agnostic Robust Representation Learning. (arXiv:2203.07596v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07596">
<div class="article-summary-box-inner">
<span><p>It has been reported that deep learning models are extremely vulnerable to
small but intentionally chosen perturbations of its input. In particular, a
deep network, despite its near-optimal accuracy on the clean images, often
mis-classifies an image with a worst-case but humanly imperceptible
perturbation (so-called adversarial examples). To tackle this problem, a great
amount of research has been done to study the training procedure of a network
to improve its robustness. However, most of the research so far has focused on
the case of supervised learning. With the increasing popularity of
self-supervised learning methods, it is also important to study and improve the
robustness of their resulting representation on the downstream tasks. In this
paper, we study the problem of robust representation learning with unlabeled
data in a task-agnostic manner. Specifically, we first derive an upper bound on
the adversarial loss of a prediction model (which is based on the learned
representation) on any downstream task, using its loss on the clean data and a
robustness regularizer. Moreover, the regularizer is task-independent, thus we
propose to minimize it directly during the representation learning phase to
make the downstream prediction model more robust. Extensive experiments show
that our method achieves preferable adversarial performance compared to
relevant baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CARETS: A Consistency And Robustness Evaluative Test Suite for VQA. (arXiv:2203.07613v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07613">
<div class="article-summary-box-inner">
<span><p>We introduce CARETS, a systematic test suite to measure consistency and
robustness of modern VQA models through a series of six fine-grained capability
tests. In contrast to existing VQA test sets, CARETS features balanced question
generation to create pairs of instances to test models, with each pair focusing
on a specific capability such as rephrasing, logical symmetry or image
obfuscation. We evaluate six modern VQA systems on CARETS and identify several
actionable weaknesses in model comprehension, especially with concepts such as
negation, disjunction, or hypernym invariance. Interestingly, even the most
sophisticated models are sensitive to aspects such as swapping the order of
terms in a conjunction or varying the number of answer choices mentioned in the
question. We release CARETS to be used as an extensible tool for evaluating
multi-modal model robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning What Not to Segment: A New Perspective on Few-Shot Segmentation. (arXiv:2203.07615v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07615">
<div class="article-summary-box-inner">
<span><p>Recently few-shot segmentation (FSS) has been extensively developed. Most
previous works strive to achieve generalization through the meta-learning
framework derived from classification tasks; however, the trained models are
biased towards the seen classes instead of being ideally class-agnostic, thus
hindering the recognition of new concepts. This paper proposes a fresh and
straightforward insight to alleviate the problem. Specifically, we apply an
additional branch (base learner) to the conventional FSS model (meta learner)
to explicitly identify the targets of base classes, i.e., the regions that do
not need to be segmented. Then, the coarse results output by these two learners
in parallel are adaptively integrated to yield precise segmentation prediction.
Considering the sensitivity of meta learner, we further introduce an adjustment
factor to estimate the scene differences between the input image pairs for
facilitating the model ensemble forecasting. The substantial performance gains
on PASCAL-5i and COCO-20i verify the effectiveness, and surprisingly, our
versatile scheme sets a new state-of-the-art even with two plain learners.
Moreover, in light of the unique nature of the proposed approach, we also
extend it to a more realistic but challenging setting, i.e., generalized FSS,
where the pixels of both base and novel classes are required to be determined.
The source code is available at github.com/chunbolang/BAM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">P-STMO: Pre-Trained Spatial Temporal Many-to-One Model for 3D Human Pose Estimation. (arXiv:2203.07628v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07628">
<div class="article-summary-box-inner">
<span><p>This paper introduces a novel Pre-trained Spatial Temporal Many-to-One
(P-STMO) model for 2D-to-3D human pose estimation task. To reduce the
difficulty of capturing spatial and temporal information, we divide this task
into two stages: pre-training (Stage I) and fine-tuning (Stage II). In Stage I,
a self-supervised pre-training sub-task, termed masked pose modeling, is
proposed. The human joints in the input sequence are randomly masked in both
spatial and temporal domains. A general form of denoising auto-encoder is
exploited to recover the original 2D poses and the encoder is capable of
capturing spatial and temporal dependencies in this way. In Stage II, the
pre-trained encoder is loaded to STMO model and fine-tuned. The encoder is
followed by a many-to-one frame aggregator to predict the 3D pose in the
current frame. Especially, an MLP block is utilized as the spatial feature
extractor in STMO, which yields better performance than other methods. In
addition, a temporal downsampling strategy is proposed to diminish data
redundancy. Extensive experiments on two benchmarks show that our method
outperforms state-of-the-art methods with fewer parameters and less
computational overhead. For example, our P-STMO model achieves 42.1mm MPJPE on
Human3.6M dataset when using 2D poses from CPN as inputs. Meanwhile, it brings
a 1.5-7.1 times speedup to state-of-the-art methods. Code is available at
https://github.com/paTRICK-swk/P-STMO.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalized but not Robust? Comparing the Effects of Data Modification Methods on Out-of-Domain Generalization and Adversarial Robustness. (arXiv:2203.07653v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07653">
<div class="article-summary-box-inner">
<span><p>Data modification, either via additional training datasets, data
augmentation, debiasing, and dataset filtering, has been proposed as an
effective solution for generalizing to out-of-domain (OOD) inputs, in both
natural language processing and computer vision literature. However, the effect
of data modification on adversarial robustness remains unclear. In this work,
we conduct a comprehensive study of common data modification strategies and
evaluate not only their in-domain and OOD performance, but also their
adversarial robustness (AR). We also present results on a two-dimensional
synthetic dataset to visualize the effect of each method on the training
distribution. This work serves as an empirical study towards understanding the
relationship between generalizing to unseen domains and defending against
adversarial perturbations. Our findings suggest that more data (either via
additional datasets or data augmentation) benefits both OOD accuracy and AR.
However, data filtering (previously shown to improve OOD accuracy on natural
language inference) hurts OOD accuracy on other tasks such as question
answering and image classification. We provide insights from our experiments to
inform future work in this direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wave-SAN: Wavelet based Style Augmentation Network for Cross-Domain Few-Shot Learning. (arXiv:2203.07656v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07656">
<div class="article-summary-box-inner">
<span><p>Previous few-shot learning (FSL) works mostly are limited to natural images
of general concepts and categories. These works assume very high visual
similarity between the source and target classes. In contrast, the recently
proposed cross-domain few-shot learning (CD-FSL) aims at transferring knowledge
from general nature images of many labeled examples to novel domain-specific
target categories of only a few labeled examples. The key challenge of CD-FSL
lies in the huge data shift between source and target domains, which is
typically in the form of totally different visual styles. This makes it very
nontrivial to directly extend the classical FSL methods to address the CD-FSL
task. To this end, this paper studies the problem of CD-FSL by spanning the
style distributions of the source dataset. Particularly, wavelet transform is
introduced to enable the decomposition of visual representations into
low-frequency components such as shape and style and high-frequency components
e.g., texture. To make our model robust to visual styles, the source images are
augmented by swapping the styles of their low-frequency components with each
other. We propose a novel Style Augmentation (StyleAug) module to implement
this idea. Furthermore, we present a Self-Supervised Learning (SSL) module to
ensure the predictions of style-augmented images are semantically similar to
the unchanged ones. This avoids the potential semantic drift problem in
exchanging the styles. Extensive experiments on two CD-FSL benchmarks show the
effectiveness of our method. Our codes and models will be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Radiance Projection. (arXiv:2203.07658v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07658">
<div class="article-summary-box-inner">
<span><p>The proposed method, Neural Radiance Projection (NeRP), addresses the three
most fundamental shortages of training such a convolutional neural network on
X-ray image segmentation: dealing with missing/limited human-annotated
datasets; ambiguity on the per-pixel label; and the imbalance across positive-
and negative- classes distribution. By harnessing a generative adversarial
network, we can synthesize a massive amount of physics-based X-ray images,
so-called Variationally Reconstructed Radiographs (VRRs), alongside their
segmentation from more accurate labeled 3D Computed Tomography data. As a
result, VRRs present more faithfully than other projection methods in terms of
photo-realistic metrics. Adding outputs from NeRP also surpasses the vanilla
UNet models trained on the same pairs of X-ray images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Breast Cancer Molecular Subtypes Prediction on Pathological Images with Discriminative Patch Selecting and Multi-Instance Learning. (arXiv:2203.07659v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07659">
<div class="article-summary-box-inner">
<span><p>Molecular subtypes of breast cancer are important references to personalized
clinical treatment. For cost and labor savings, only one of the patient's
paraffin blocks is usually selected for subsequent immunohistochemistry (IHC)
to obtain molecular subtypes. Inevitable sampling error is risky due to tumor
heterogeneity and could result in a delay in treatment. Molecular subtype
prediction from conventional H&amp;E pathological whole slide images (WSI) using AI
method is useful and critical to assist pathologists pre-screen proper paraffin
block for IHC. It's a challenging task since only WSI level labels of molecular
subtypes can be obtained from IHC. Gigapixel WSIs are divided into a huge
number of patches to be computationally feasible for deep learning. While with
coarse slide-level labels, patch-based methods may suffer from abundant noise
patches, such as folds, overstained regions, or non-tumor tissues. A weakly
supervised learning framework based on discriminative patch selecting and
multi-instance learning was proposed for breast cancer molecular subtype
prediction from H&amp;E WSIs. Firstly, co-teaching strategy was adopted to learn
molecular subtype representations and filter out noise patches. Then, a
balanced sampling strategy was used to handle the imbalance in subtypes in the
dataset. In addition, a noise patch filtering algorithm that used local outlier
factor based on cluster centers was proposed to further select discriminative
patches. Finally, a loss function integrating patch with slide constraint
information was used to finetune MIL framework on obtained discriminative
patches and further improve the performance of molecular subtyping. The
experimental results confirmed the effectiveness of the proposed method and our
models outperformed even senior pathologists, with potential to assist
pathologists to pre-screen paraffin blocks for IHC in clinic.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What's in the Black Box? The False Negative Mechanisms Inside Object Detectors. (arXiv:2203.07662v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07662">
<div class="article-summary-box-inner">
<span><p>In object detection, false negatives arise when a detector fails to detect a
target object. To understand why object detectors produce false negatives, we
identify five 'false negative mechanisms', where each mechanism describes how a
specific component inside the detector architecture failed. Focusing on
two-stage and one-stage anchor-box object detector architectures, we introduce
a framework for quantifying these false negative mechanisms. Using this
framework, we investigate why Faster R-CNN and RetinaNet fail to detect objects
in benchmark vision datasets and robotics datasets. We show that a detector's
false negative mechanisms differ significantly between computer vision
benchmark datasets and robotics deployment scenarios. This has implications for
the translation of object detectors developed for benchmark datasets to
robotics applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can you even tell left from right? Presenting a new challenge for VQA. (arXiv:2203.07664v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07664">
<div class="article-summary-box-inner">
<span><p>Visual Question Answering (VQA) needs a means of evaluating the strengths and
weaknesses of models. One aspect of such an evaluation is the evaluation of
compositional generalisation, or the ability of a model to answer well on
scenes whose scene-setups are different from the training set. Therefore, for
this purpose, we need datasets whose train and test sets differ significantly
in composition. In this work, we present several quantitative measures of
compositional separation and find that popular datasets for VQA are not good
evaluators. To solve this, we present Uncommon Objects in Unseen Configurations
(UOUC), a synthetic dataset for VQA. UOUC is at once fairly complex while also
being well-separated, compositionally. The object-class of UOUC consists of 380
clasess taken from 528 characters from the Dungeons and Dragons game. The train
set of UOUC consists of 200,000 scenes; whereas the test set consists of 30,000
scenes. In order to study compositional generalisation, simple reasoning and
memorisation, each scene of UOUC is annotated with up to 10 novel questions.
These deal with spatial relationships, hypothetical changes to scenes,
counting, comparison, memorisation and memory-based reasoning. In total, UOUC
presents over 2 million questions. UOUC also finds itself as a strong challenge
to well-performing models for VQA. Our evaluation of recent models for VQA
shows poor compositional generalisation, and comparatively lower ability
towards simple reasoning. These results suggest that UOUC could lead to
advances in research by being a strong benchmark for VQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SATS: Self-Attention Transfer for Continual Semantic Segmentation. (arXiv:2203.07667v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07667">
<div class="article-summary-box-inner">
<span><p>Continually learning to segment more and more types of image regions is a
desired capability for many intelligent systems. However, such continual
semantic segmentation suffers from the same catastrophic forgetting issue as in
continual classification learning. While multiple knowledge distillation
strategies originally for continual classification have been well adapted to
continual semantic segmentation, they only consider transferring old knowledge
based on the outputs from one or more layers of deep fully convolutional
networks. Different from existing solutions, this study proposes to transfer a
new type of information relevant to knowledge, i.e. the relationships between
elements (Eg. pixels or small local regions) within each image which can
capture both within-class and between-class knowledge. The relationship
information can be effectively obtained from the self-attention maps in a
Transformer-style segmentation model. Considering that pixels belonging to the
same class in each image often share similar visual properties, a
class-specific region pooling is applied to provide more efficient relationship
information for knowledge transfer. Extensive evaluations on multiple public
benchmarks support that the proposed self-attention transfer method can further
effectively alleviate the catastrophic forgetting issue, and its flexible
combination with one or more widely adopted strategies significantly
outperforms state-of-the-art solu
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Progressive End-to-End Object Detection in Crowded Scenes. (arXiv:2203.07669v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07669">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a new query-based detection framework for crowd
detection. Previous query-based detectors suffer from two drawbacks: first,
multiple predictions will be inferred for a single object, typically in crowded
scenes; second, the performance saturates as the depth of the decoding stage
increases. Benefiting from the nature of the one-to-one label assignment rule,
we propose a progressive predicting method to address the above issues.
Specifically, we first select accepted queries prone to generate true positive
predictions, then refine the rest noisy queries according to the previously
accepted predictions. Experiments show that our method can significantly boost
the performance of query-based detectors in crowded scenes. Equipped with our
approach, Sparse RCNN achieves 92.0\% $\text{AP}$, 41.4\% $\text{MR}^{-2}$ and
83.2\% $\text{JI}$ on the challenging CrowdHuman \cite{shao2018crowdhuman}
dataset, outperforming the box-based method MIP \cite{chu2020detection} that
specifies in handling crowded scenarios. Moreover, the proposed method, robust
to crowdedness, can still obtain consistent improvements on moderately and
slightly crowded datasets like CityPersons \cite{zhang2017citypersons} and COCO
\cite{lin2014microsoft}. Code will be made publicly available at
https://github.com/megvii-model/Iter-E2EDET.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unpaired Deep Image Dehazing Using Contrastive Disentanglement Learning. (arXiv:2203.07677v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07677">
<div class="article-summary-box-inner">
<span><p>We present an effective unpaired learning based image dehazing network from
an unpaired set of clear and hazy images. This paper provides a new perspective
to treat image dehazing as a two-class separated factor disentanglement task,
i.e, the task-relevant factor of clear image reconstruction and the
task-irrelevant factor of haze-relevant distribution. To achieve the
disentanglement of these two-class factors in deep feature space, contrastive
learning is introduced into a CycleGAN framework to learn disentangled
representations by guiding the generated images to be associated with latent
factors. With such formulation, the proposed contrastive disentangled dehazing
method (CDD-GAN) first develops negative generators to cooperate with the
encoder network to update alternately, so as to produce a queue of challenging
negative adversaries. Then these negative adversaries are trained end-to-end
together with the backbone representation network to enhance the discriminative
information and promote factor disentanglement performance by maximizing the
adversarial contrastive loss. During the training, we further show that hard
negative examples can suppress the task-irrelevant factors and unpaired clear
exemples can enhance the task-relevant factors, in order to better facilitate
haze removal and help image restoration. Extensive experiments on both
synthetic and real-world datasets demonstrate that our method performs
favorably against existing state-of-the-art unpaired dehazing approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rich CNN-Transformer Feature Aggregation Networks for Super-Resolution. (arXiv:2203.07682v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07682">
<div class="article-summary-box-inner">
<span><p>Recent vision transformers along with self-attention have achieved promising
results on various computer vision tasks. In particular, a pure
transformer-based image restoration architecture surpasses the existing
CNN-based methods using multi-task pre-training with a large number of
trainable parameters. In this paper, we introduce an effective hybrid
architecture for super-resolution (SR) tasks, which leverages local features
from CNNs and long-range dependencies captured by transformers to further
improve the SR results. Specifically, our architecture comprises of transformer
and convolution branches, and we substantially elevate the performance by
mutually fusing two branches to complement each representation. Furthermore, we
propose a cross-scale token attention module, which allows the transformer to
efficiently exploit the informative relationships among tokens across different
scales. Our proposed method achieves state-of-the-art SR results on numerous
benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InsCon:Instance Consistency Feature Representation via Self-Supervised Learning. (arXiv:2203.07688v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07688">
<div class="article-summary-box-inner">
<span><p>Feature representation via self-supervised learning has reached remarkable
success in image-level contrastive learning, which brings impressive
performances on image classification tasks. While image-level feature
representation mainly focuses on contrastive learning in single instance, it
ignores the objective differences between pretext and downstream prediction
tasks such as object detection and instance segmentation. In order to fully
unleash the power of feature representation on downstream prediction tasks, we
propose a new end-to-end self-supervised framework called InsCon, which is
devoted to capturing multi-instance information and extracting cell-instance
features for object recognition and localization. On the one hand, InsCon
builds a targeted learning paradigm that applies multi-instance images as
input, aligning the learned feature between corresponding instance views, which
makes it more appropriate for multi-instance recognition tasks. On the other
hand, InsCon introduces the pull and push of cell-instance, which utilizes cell
consistency to enhance fine-grained feature representation for precise boundary
localization. As a result, InsCon learns multi-instance consistency on semantic
feature representation and cell-instance consistency on spatial feature
representation. Experiments demonstrate the method we proposed surpasses MoCo
v2 by 1.1% AP^{bb} on COCO object detection and 1.0% AP^{mk} on COCO instance
segmentation using Mask R-CNN R50-FPN network structure with 90k iterations,
2.1% APbb on PASCAL VOC objection detection using Faster R-CNN R50-C4 network
structure with 24k iterations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Implicit field supervision for robust non-rigid shape matching. (arXiv:2203.07694v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07694">
<div class="article-summary-box-inner">
<span><p>Establishing a correspondence between two non-rigidly deforming shapes is one
of the most fundamental problems in visual computing. Existing methods often
show weak resilience when presented with challenges innate to real-world data
such as noise, outliers, self-occlusion etc. On the other hand, auto-decoders
have demonstrated strong expressive power in learning geometrically meaningful
latent embeddings. However, their use in \emph{shape analysis} and especially
in non-rigid shape correspondence has been limited. In this paper, we introduce
an approach based on auto-decoder framework, that learns a continuous
shape-wise deformation field over a fixed template. By supervising the
deformation field for points on-surface and regularising for points off-surface
through a novel \emph{Signed Distance Regularisation} (SDR), we learn an
alignment between the template and shape \emph{volumes}. Unlike classical
correspondence techniques, our method is remarkably robust in the presence of
strong artefacts and can be generalised to arbitrary shape categories. Trained
on clean water-tight meshes, \emph{without} any data-augmentation, we
demonstrate compelling performance on compromised data and real-world scans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distribution-Aware Single-Stage Models for Multi-Person 3D Pose Estimation. (arXiv:2203.07697v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07697">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a novel Distribution-Aware Single-stage (DAS) model
for tackling the challenging multi-person 3D pose estimation problem. Different
from existing top-down and bottom-up methods, the proposed DAS model
simultaneously localizes person positions and their corresponding body joints
in the 3D camera space in a one-pass manner. This leads to a simplified
pipeline with enhanced efficiency. In addition, DAS learns the true
distribution of body joints for the regression of their positions, rather than
making a simple Laplacian or Gaussian assumption as previous works. This
provides valuable priors for model prediction and thus boosts the
regression-based scheme to achieve competitive performance with volumetric-base
ones. Moreover, DAS exploits a recursive update strategy for progressively
approaching to regression target, alleviating the optimization difficulty and
further lifting the regression performance. DAS is implemented with a fully
Convolutional Neural Network and end-to-end learnable. Comprehensive
experiments on benchmarks CMU Panoptic and MuPoTS-3D demonstrate the superior
efficiency of the proposed DAS model, specifically 1.5x speedup over previous
best model, and its stat-of-the-art accuracy for multi-person 3D pose
estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">APRNet: Attention-based Pixel-wise Rendering Network for Photo-Realistic Text Image Generation. (arXiv:2203.07705v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07705">
<div class="article-summary-box-inner">
<span><p>Style-guided text image generation tries to synthesize text image by
imitating reference image's appearance while keeping text content unaltered.
The text image appearance includes many aspects. In this paper, we focus on
transferring style image's background and foreground color patterns to the
content image to generate photo-realistic text image. To achieve this goal, we
propose 1) a content-style cross attention based pixel sampling approach to
roughly mimicking the style text image's background; 2) a pixel-wise style
modulation technique to transfer varying color patterns of the style image to
the content image spatial-adaptively; 3) a cross attention based multi-scale
style fusion approach to solving text foreground misalignment issue between
style and content images; 4) an image patch shuffling strategy to create style,
content and ground truth image tuples for training. Experimental results on
Chinese handwriting text image synthesis with SCUT-HCCDoc and CASIA-OLHWDB
datasets demonstrate that the proposed method can improve the quality of
synthetic text images and make them more photo-realistic.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ActFormer: A GAN Transformer Framework towards General Action-Conditioned 3D Human Motion Generation. (arXiv:2203.07706v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07706">
<div class="article-summary-box-inner">
<span><p>We present a GAN Transformer framework for general action-conditioned 3D
human motion generation, including not only single-person actions but also
multi-person interactive actions. Our approach consists of a powerful
Action-conditioned motion transFormer (ActFormer) under a GAN training scheme,
equipped with a Gaussian Process latent prior. Such a design combines the
strong spatio-temporal representation capacity of Transformer, superiority in
generative modeling of GAN, and inherent temporal correlations from latent
prior. Furthermore, ActFormer can be naturally extended to multi-person motions
by alternately modeling temporal correlations and human interactions with
Transformer encoders. We validate our approach by comparison with other methods
on larger-scale benchmarks, including NTU RGB+D 120 and BABEL. We also
introduce a new synthetic dataset of complex multi-person combat behaviors to
facilitate research on multi-person motion generation. Our method demonstrates
adaptability to various human motion representations and achieves leading
performance over SOTA methods on both single-person and multi-person motion
generation tasks, indicating a hopeful step towards a universal human motion
generator.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Magnification Prior: A Self-Supervised Method for Learning Representations on Breast Cancer Histopathological Images. (arXiv:2203.07707v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07707">
<div class="article-summary-box-inner">
<span><p>This work presents a novel self-supervised pre-training method to learn
efficient representations without labels on histopathology medical images
utilizing magnification factors. Other state-of-theart works mainly focus on
fully supervised learning approaches that rely heavily on human annotations.
However, the scarcity of labeled and unlabeled data is a long-standing
challenge in histopathology. Currently, representation learning without labels
remains unexplored for the histopathology domain. The proposed method,
Magnification Prior Contrastive Similarity (MPCS), enables self-supervised
learning of representations without labels on small-scale breast cancer dataset
BreakHis by exploiting magnification factor, inductive transfer, and reducing
human prior. The proposed method matches fully supervised learning
state-of-the-art performance in malignancy classification when only 20% of
labels are used in fine-tuning and outperform previous works in fully
supervised learning settings. It formulates a hypothesis and provides empirical
evidence to support that reducing human-prior leads to efficient representation
learning in self-supervision. The implementation of this work is available
online on GitHub -
https://github.com/prakashchhipa/Magnification-Prior-Self-Supervised-Method
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revitalize Region Feature for Democratizing Video-Language Pre-training. (arXiv:2203.07720v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07720">
<div class="article-summary-box-inner">
<span><p>Recent dominant methods for video-language pre-training (VLP) learn
transferable representations from the raw pixels in an end-to-end manner to
achieve advanced performance on downstream video-language tasks. Despite the
impressive results, VLP research becomes extremely expensive with the need for
massive data and a long training time, preventing further explorations. In this
work, we revitalize region features of sparsely sampled video clips to
significantly reduce both spatial and temporal visual redundancy towards
democratizing VLP research at the same time achieving state-of-the-art results.
Specifically, to fully explore the potential of region features, we introduce a
novel bidirectional region-word alignment regularization that properly
optimizes the fine-grained relations between regions and certain words in
sentences, eliminating the domain/modality disconnections between pre-extracted
region features and text. Extensive results of downstream text-to-video
retrieval and video question answering tasks on seven datasets demonstrate the
superiority of our method on both effectiveness and efficiency, e.g., our
method achieves competing results with 80\% fewer data and 85\% less
pre-training time compared to the most efficient VLP method so far. The code
will be available at \url{https://github.com/CuthbertCai/DemoVLP}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CODA: A Real-World Road Corner Case Dataset for Object Detection in Autonomous Driving. (arXiv:2203.07724v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07724">
<div class="article-summary-box-inner">
<span><p>Contemporary deep-learning object detection methods for autonomous driving
usually assume prefixed categories of common traffic participants, such as
pedestrians and cars. Most existing detectors are unable to detect uncommon
objects and corner cases (e.g., a dog crossing a street), which may lead to
severe accidents in some situations, making the timeline for the real-world
application of reliable autonomous driving uncertain. One main reason that
impedes the development of truly reliably self-driving systems is the lack of
public datasets for evaluating the performance of object detectors on corner
cases. Hence, we introduce a challenging dataset named CODA that exposes this
critical problem of vision-based detectors. The dataset consists of 1500
carefully selected real-world driving scenes, each containing four object-level
corner cases (on average), spanning 30+ object categories. On CODA, the
performance of standard object detectors trained on large-scale autonomous
driving datasets significantly drops to no more than 12.8% in mAR. Moreover, we
experiment with the state-of-the-art open-world object detector and find that
it also fails to reliably identify the novel objects in CODA, suggesting that a
robust perception system for autonomous driving is probably still far from
reach. We expect our CODA dataset to facilitate further research in reliable
detection for real-world autonomous driving. Our dataset will be released at
https://coda-dataset.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta Ordinal Regression Forest for Medical Image Classification with Ordinal Labels. (arXiv:2203.07725v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07725">
<div class="article-summary-box-inner">
<span><p>The performance of medical image classification has been enhanced by deep
convolutional neural networks (CNNs), which are typically trained with
cross-entropy (CE) loss. However, when the label presents an intrinsic ordinal
property in nature, e.g., the development from benign to malignant tumor, CE
loss cannot take into account such ordinal information to allow for better
generalization. To improve model generalization with ordinal information, we
propose a novel meta ordinal regression forest (MORF) method for medical image
classification with ordinal labels, which learns the ordinal relationship
through the combination of convolutional neural network and differential forest
in a meta-learning framework. The merits of the proposed MORF come from the
following two components: a tree-wise weighting net (TWW-Net) and a grouped
feature selection (GFS) module. First, the TWW-Net assigns each tree in the
forest with a specific weight that is mapped from the classification loss of
the corresponding tree. Hence, all the trees possess varying weights, which is
helpful for alleviating the tree-wise prediction variance. Second, the GFS
module enables a dynamic forest rather than a fixed one that was previously
used, allowing for random feature perturbation. During training, we
alternatively optimize the parameters of the CNN backbone and TWW-Net in the
meta-learning framework through calculating the Hessian matrix. Experimental
results on two medical image classification datasets with ordinal labels, i.e.,
LIDC-IDRI and Breast Ultrasound Dataset, demonstrate the superior performances
of our MORF method over existing state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Securing the Classification of COVID-19 in Chest X-ray Images: A Privacy-Preserving Deep Learning Approach. (arXiv:2203.07728v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07728">
<div class="article-summary-box-inner">
<span><p>Deep learning (DL) is being increasingly utilized in healthcare-related
fields due to its outstanding efficiency. However, we have to keep the
individual health data used by DL models private and secure. Protecting data
and preserving the privacy of individuals has become an increasingly prevalent
issue. The gap between the DL and privacy communities must be bridged. In this
paper, we propose privacy-preserving deep learning (PPDL)-based approach to
secure the classification of Chest X-ray images. This study aims to use Chest
X-ray images to their fullest potential without compromising the privacy of the
data that it contains. The proposed approach is based on two steps: encrypting
the dataset using partially homomorphic encryption and training/testing the DL
algorithm over the encrypted images. Experimental results on the COVID-19
Radiography database show that the MobileNetV2 model achieves an accuracy of
94.2% over the plain data and 93.3% over the encrypted data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">S2F2: Self-Supervised High Fidelity Face Reconstruction from Monocular Image. (arXiv:2203.07732v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07732">
<div class="article-summary-box-inner">
<span><p>We present a novel face reconstruction method capable of reconstructing
detailed face geometry, spatially varying face reflectance from a single
monocular image. We build our work upon the recent advances of DNN-based
auto-encoders with differentiable ray tracing image formation, trained in
self-supervised manner. While providing the advantage of learning-based
approaches and real-time reconstruction, the latter methods lacked fidelity. In
this work, we achieve, for the first time, high fidelity face reconstruction
using self-supervised learning only. Our novel coarse-to-fine deep architecture
allows us to solve the challenging problem of decoupling face reflectance from
geometry using a single image, at high computational speed. Compared to
state-of-the-art methods, our method achieves more visually appealing
reconstruction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Annotation-free Restoration Network for Cataractous Fundus Images. (arXiv:2203.07737v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07737">
<div class="article-summary-box-inner">
<span><p>Cataracts are the leading cause of vision loss worldwide. Restoration
algorithms are developed to improve the readability of cataract fundus images
in order to increase the certainty in diagnosis and treatment for cataract
patients. Unfortunately, the requirement of annotation limits the application
of these algorithms in clinics. This paper proposes a network to
annotation-freely restore cataractous fundus images (ArcNet) so as to boost the
clinical practicability of restoration. Annotations are unnecessary in ArcNet,
where the high-frequency component is extracted from fundus images to replace
segmentation in the preservation of retinal structures. The restoration model
is learned from the synthesized images and adapted to real cataract images.
Extensive experiments are implemented to verify the performance and
effectiveness of ArcNet. Favorable performance is achieved using ArcNet against
state-of-the-art algorithms, and the diagnosis of ocular fundus diseases in
cataract patients is promoted by ArcNet. The capability of properly restoring
cataractous images in the absence of annotated data promises the proposed
algorithm outstanding clinical practicability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CSN: Component-Supervised Network for Few-Shot Classification. (arXiv:2203.07738v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07738">
<div class="article-summary-box-inner">
<span><p>The few-shot classification (FSC) task has been a hot research topic in
recent years. It aims to address the classification problem with insufficient
labeled data on a cross-category basis. Typically, researchers pre-train a
feature extractor with base data, then use it to extract the features of novel
data and recognize them. Notably, the novel set only has a few annotated
samples and has entirely different categories from the base set, which leads to
that the pre-trained feature extractor can not adapt to the novel data
flawlessly. We dub this problem as Feature-Extractor-Maladaptive (FEM) problem.
Starting from the root cause of this problem, this paper presents a new scheme,
Component-Supervised Network (CSN), to improve the performance of FSC. We
believe that although the categories of base and novel sets are different, the
composition of the sample's components is similar. For example, both cat and
dog contain leg and head components. Actually, such entity components are
intra-class stable. They have fine cross-category versatility and new category
generalization. Therefore, we refer to WordNet, a dictionary commonly used in
natural language processing, to collect component information of samples and
construct a component-based auxiliary task to improve the adaptability of the
feature extractor. We conduct experiments on two benchmark datasets
(mini-ImageNet and tiered-ImageNet), the improvements of $0.9\%$-$5.8\%$
compared with state-of-the-arts have evaluated the efficiency of our CSN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exact Feature Distribution Matching for Arbitrary Style Transfer and Domain Generalization. (arXiv:2203.07740v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07740">
<div class="article-summary-box-inner">
<span><p>Arbitrary style transfer (AST) and domain generalization (DG) are important
yet challenging visual learning tasks, which can be cast as a feature
distribution matching problem. With the assumption of Gaussian feature
distribution, conventional feature distribution matching methods usually match
the mean and standard deviation of features. However, the feature distributions
of real-world data are usually much more complicated than Gaussian, which
cannot be accurately matched by using only the first-order and second-order
statistics, while it is computationally prohibitive to use high-order
statistics for distribution matching. In this work, we, for the first time to
our best knowledge, propose to perform Exact Feature Distribution Matching
(EFDM) by exactly matching the empirical Cumulative Distribution Functions
(eCDFs) of image features, which could be implemented by applying the Exact
Histogram Matching (EHM) in the image feature space. Particularly, a fast EHM
algorithm, named Sort-Matching, is employed to perform EFDM in a plug-and-play
manner with minimal cost. The effectiveness of our proposed EFDM method is
verified on a variety of AST and DG tasks, demonstrating new state-of-the-art
results. Codes are available at https://github.com/YBZh/EFDM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Curve Translator for Real-Time High-Resolution Image-to-Image Translation. (arXiv:2203.07756v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07756">
<div class="article-summary-box-inner">
<span><p>The dominant image-to-image translation methods are based on fully
convolutional networks, which extract and translate an image's features and
then reconstruct the image. However, they have unacceptable computational costs
when working with high-resolution images. To this end, we present the
Multi-Curve Translator (MCT), which not only predicts the translated pixels for
the corresponding input pixels but also for their neighboring pixels. And if a
high-resolution image is downsampled to its low-resolution version, the lost
pixels are the remaining pixels' neighboring pixels. So MCT makes it possible
to feed the network only the downsampled image to perform the mapping for the
full-resolution image, which can dramatically lower the computational cost.
Besides, MCT is a plug-in approach that utilizes existing base models and
requires only replacing their output layers. Experiments demonstrate that the
MCT variants can process 4K images in real-time and achieve comparable or even
better performance than the base models on various image-to-image translation
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast Autofocusing using Tiny Networks for Digital Holographic Microscopy. (arXiv:2203.07772v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07772">
<div class="article-summary-box-inner">
<span><p>The numerical wavefront backpropagation principle of digital holography
confers unique extended focus capabilities, without mechanical displacements
along z-axis. However, the determination of the correct focusing distance is a
non-trivial and time consuming issue. A deep learning (DL) solution is proposed
to cast the autofocusing as a regression problem and tested over both
experimental and simulated holograms. Single wavelength digital holograms were
recorded by a Digital Holographic Microscope (DHM) with a 10$\mathrm{x}$
microscope objective from a patterned target moving in 3D over an axial range
of 92 $\mu$m. Tiny DL models are proposed and compared such as a tiny Vision
Transformer (TViT), tiny VGG16 (TVGG) and a tiny Swin-Transfomer (TSwinT). The
experiments show that the predicted focusing distance $Z_R^{\mathrm{Pred}}$ is
accurately inferred with an accuracy of 1.2 $\mu$m in average in comparison
with the DHM depth of field of 15 $\mu$m. Numerical simulations show that all
tiny models give the $Z_R^{\mathrm{Pred}}$ with an error below 0.3 $\mu$m. Such
a prospect would significantly improve the current capabilities of computer
vision position sensing in applications such as 3D microscopy for life sciences
or micro-robotics. Moreover, all models reach state of the art inference time
on CPU, less than 25 ms per inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scalable Penalized Regression for Noise Detection in Learning with Noisy Labels. (arXiv:2203.07788v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07788">
<div class="article-summary-box-inner">
<span><p>Noisy training set usually leads to the degradation of generalization and
robustness of neural networks. In this paper, we propose using a theoretically
guaranteed noisy label detection framework to detect and remove noisy data for
Learning with Noisy Labels (LNL). Specifically, we design a penalized
regression to model the linear relation between network features and one-hot
labels, where the noisy data are identified by the non-zero mean shift
parameters solved in the regression model. To make the framework scalable to
datasets that contain a large number of categories and training data, we
propose a split algorithm to divide the whole training set into small pieces
that can be solved by the penalized regression in parallel, leading to the
Scalable Penalized Regression (SPR) framework. We provide the non-asymptotic
probabilistic condition for SPR to correctly identify the noisy data. While SPR
can be regarded as a sample selection module for standard supervised training
pipeline, we further combine it with semi-supervised algorithm to further
exploit the support of noisy data as unlabeled data. Experimental results on
several benchmark datasets and real-world noisy datasets show the effectiveness
of our framework. Our code and pretrained models are released at
https://github.com/Yikai-Wang/SPR-LNL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parking Analytics Framework using Deep Learning. (arXiv:2203.07792v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07792">
<div class="article-summary-box-inner">
<span><p>With the number of vehicles continuously increasing, parking monitoring and
analysis are becoming a substantial feature of modern cities. In this study, we
present a methodology to monitor car parking areas and to analyze their
occupancy in real-time. The solution is based on a combination between image
analysis and deep learning techniques. It incorporates four building blocks put
inside a pipeline: vehicle detection, vehicle tracking, manual annotation of
parking slots, and occupancy estimation using the Ray Tracing algorithm. The
aim of this methodology is to optimize the use of parking areas and to reduce
the time wasted by daily drivers to find the right parking slot for their cars.
Also, it helps to better manage the space of the parking areas and to discover
misuse cases. A demonstration of the provided solution is shown in the
following video link: https://www.youtube.com/watch?v=KbAt8zT14Tc.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the focusing of thermal images. (arXiv:2203.07805v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07805">
<div class="article-summary-box-inner">
<span><p>In this paper we present a new thermographic image database suitable for the
analysis of automatic focus measures. This database consists of 8 different
sets of scenes, where each scene contains one image for 96 different focus
positions. Using this database we evaluate the usefulness of six focus measures
with the goal to determine the optimal focus position. Experimental results
reveal that an accurate automatic detection of optimal focus position is
possible, even with a low computational burden. We also present an acquisition
tool able to help the acquisition of thermal images. To the best of our
knowledge, this is the first study about automatic focus of thermal images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interspace Pruning: Using Adaptive Filter Representations to Improve Training of Sparse CNNs. (arXiv:2203.07808v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07808">
<div class="article-summary-box-inner">
<span><p>Unstructured pruning is well suited to reduce the memory footprint of
convolutional neural networks (CNNs), both at training and inference time. CNNs
contain parameters arranged in $K \times K$ filters. Standard unstructured
pruning (SP) reduces the memory footprint of CNNs by setting filter elements to
zero, thereby specifying a fixed subspace that constrains the filter.
Especially if pruning is applied before or during training, this induces a
strong bias. To overcome this, we introduce interspace pruning (IP), a general
tool to improve existing pruning methods. It uses filters represented in a
dynamic interspace by linear combinations of an underlying adaptive filter
basis (FB). For IP, FB coefficients are set to zero while un-pruned
coefficients and FBs are trained jointly. In this work, we provide mathematical
evidence for IP's superior performance and demonstrate that IP outperforms SP
on all tested state-of-the-art unstructured pruning methods. Especially in
challenging situations, like pruning for ImageNet or pruning to high sparsity,
IP greatly exceeds SP with equal runtime and parameter costs. Finally, we show
that advances of IP are due to improved trainability and superior
generalization ability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Quality Assessment for Magnetic Resonance Imaging. (arXiv:2203.07809v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07809">
<div class="article-summary-box-inner">
<span><p>Image quality assessment (IQA) algorithms aim to reproduce the human's
perception of the image quality. The growing popularity of image enhancement,
generation, and recovery models instigated the development of many methods to
assess their performance. However, most IQA solutions are designed to predict
image quality in the general domain, with the applicability to specific areas,
such as medical imaging, remaining questionable. Moreover, the selection of
these IQA metrics for a specific task typically involves intentionally induced
distortions, such as manually added noise or artificial blurring; yet, the
chosen metrics are then used to judge the output of real-life computer vision
models. In this work, we aspire to fill these gaps by carrying out the most
extensive IQA evaluation study for Magnetic Resonance Imaging (MRI) to date
(14,700 subjective scores). We use outputs of neural network models trained to
solve problems relevant to MRI, including image reconstruction in the scan
acceleration, motion correction, and denoising. Seven trained radiologists
assess these distorted images, with their verdicts then correlated with 35
different image quality metrics (full-reference, no-reference, and
distribution-based metrics considered). Our emphasis is on reflecting the
radiologist's perception of the reconstructed images, gauging the most
diagnostically influential criteria for the quality of MRI scans:
signal-to-noise ratio, contrast-to-noise ratio, and the presence of artifacts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Counterfactual Augmentation: Application in Alzheimer's Disease Classification. (arXiv:2203.07815v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07815">
<div class="article-summary-box-inner">
<span><p>Data augmentation has been widely used in deep learning to reduce
over-fitting and improve the robustness of models. However, traditional data
augmentation techniques, e.g., rotation, cropping, flipping, etc., do not
consider \textit{semantic} transformations, e.g., changing the age of a brain
image. Previous works tried to achieve semantic augmentation by generating
\textit{counterfactuals}, but they focused on how to train deep generative
models and randomly created counterfactuals with the generative models without
considering which counterfactuals are most \textit{effective} for improving
downstream training. Different from these approaches, in this work, we propose
a novel adversarial counterfactual augmentation scheme that aims to find the
most \textit{effective} counterfactuals to improve downstream tasks with a
pre-trained generative model. Specifically, we construct an adversarial game
where we update the input \textit{conditional factor} of the generator and the
downstream \textit{classifier} with gradient backpropagation alternatively and
iteratively. The key idea is to find conditional factors that can result in
\textit{hard} counterfactuals for the classifier. This can be viewed as finding
the `\textit{weakness}' of the classifier and purposely forcing it to
\textit{overcome} its weakness via the generative model. To demonstrate the
effectiveness of the proposed approach, we validate the method with the
classification of Alzheimer's Disease (AD) as the downstream task based on a
pre-trained brain ageing synthesis model. We show the proposed approach
improves test accuracy and can alleviate spurious correlations. Code will be
released upon acceptance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SISL:Self-Supervised Image Signature Learning for Splicing Detection and Localization. (arXiv:2203.07824v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07824">
<div class="article-summary-box-inner">
<span><p>Recent algorithms for image manipulation detection almost exclusively use
deep network models. These approaches require either dense pixelwise
groundtruth masks, camera ids, or image metadata to train the networks. On one
hand, constructing a training set to represent the countless tampering
possibilities is impractical. On the other hand, social media platforms or
commercial applications are often constrained to remove camera ids as well as
metadata from images. A self-supervised algorithm for training manipulation
detection models without dense groundtruth or camera/image metadata would be
extremely useful for many forensics applications. In this paper, we propose
self-supervised approach for training splicing detection/localization models
from frequency transforms of images. To identify the spliced regions, our deep
network learns a representation to capture an image specific signature by
enforcing (image) self consistency . We experimentally demonstrate that our
proposed model can yield similar or better performances of multiple existing
methods on standard datasets without relying on labels or metadata.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPA-VAE: Similar-Parts-Assignment for Unsupervised 3D Point Cloud Generation. (arXiv:2203.07825v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07825">
<div class="article-summary-box-inner">
<span><p>This paper addresses the problem of unsupervised parts-aware point cloud
generation with learned parts-based self-similarity. Our SPA-VAE infers a set
of latent canonical candidate shapes for any given object, along with a set of
rigid body transformations for each such candidate shape to one or more
locations within the assembled object. In this way, noisy samples on the
surface of, say, each leg of a table, are effectively combined to estimate a
single leg prototype. When parts-based self-similarity exists in the raw data,
sharing data among parts in this way confers numerous advantages: modeling
accuracy, appropriately self-similar generative outputs, precise in-filling of
occlusions, and model parsimony. SPA-VAE is trained end-to-end using a
variational Bayesian approach which uses the Gumbel-softmax trick for the
shared part assignments, along with various novel losses to provide appropriate
inductive biases. Quantitative and qualitative analyses on ShapeNet demonstrate
the advantage of SPA-VAE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pose-MUM : Reinforcing Key Points Relationship for Semi-Supervised Human Pose Estimation. (arXiv:2203.07837v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07837">
<div class="article-summary-box-inner">
<span><p>A well-designed strong-weak augmentation strategy and the stable teacher to
generate reliable pseudo labels are essential in the teacher-student framework
of semi-supervised learning (SSL). Considering these in mind, to suit the
semi-supervised human pose estimation (SSHPE) task, we propose a novel approach
referred to as Pose-MUM that modifies Mix/UnMix (MUM) augmentation. Like MUM in
the dense prediction task, the proposed Pose-MUM makes strong-weak augmentation
for pose estimation and leads the network to learn the relationship between
each human key point much better than the conventional methods by adding the
mixing process in intermediate layers in a stochastic manner. In addition, we
employ the exponential-moving-average-normalization (EMAN) teacher, which is
stable and well-suited to the SSL framework and furthermore boosts the
performance. Extensive experiments on MS-COCO dataset show the superiority of
our proposed method by consistently improving the performance over the previous
methods following SSHPE benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bamboo: Building Mega-Scale Vision Dataset Continually with Human-Machine Synergy. (arXiv:2203.07845v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07845">
<div class="article-summary-box-inner">
<span><p>Large-scale datasets play a vital role in computer vision. Existing datasets
are either collected according to heuristic label systems or annotated blindly
without differentiation to samples, making them inefficient and unscalable. How
to systematically collect, annotate and build a mega-scale dataset remains an
open question. In this work, we advocate building a high-quality vision dataset
actively and continually on a comprehensive label system. Specifically, we
contribute Bamboo Dataset, a mega-scale and information-dense dataset for both
classification and detection. Bamboo aims to populate the comprehensive
categories with 69M image classification annotations and 170,586 object
bounding box annotations. Compared to ImageNet22K and Objects365, models
pre-trained on Bamboo achieve superior performance among various downstream
tasks (6.2% gains on classification and 2.1% gains on detection). In addition,
we provide valuable observations regarding large-scale pre-training from over
1,000 experiments. Due to its scalable nature on both label system and
annotation pipeline, Bamboo will continue to grow and benefit from the
collective efforts of the community, which we hope would pave the way for more
general vision models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recursive 3D Segmentation of Shoulder Joint with Coarse-scanned MR Image. (arXiv:2203.07846v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07846">
<div class="article-summary-box-inner">
<span><p>For diagnosis of shoulder illness, it is essential to look at the morphology
deviation of scapula and humerus from the medical images that are acquired from
Magnetic Resonance (MR) imaging. However, taking high-resolution MR images is
time-consuming and costly because the reduction of the physical distance
between image slices causes prolonged scanning time. Moreover, due to the lack
of training images, images from various sources must be utilized, which creates
the issue of high variance across the dataset. Also, there are human errors
among the images due to the fact that it is hard to take the spatial
relationship into consideration when labeling the 3D image in low resolution.
In order to combat all obstacles stated above, we develop a fully automated
algorithm for segmenting the humerus and scapula bone from coarsely scanned and
low-resolution MR images and a recursive learning framework that iterative
utilize the generated labels for reducing the errors among segmentations and
increase our dataset set for training the next round network. In this study, 50
MR images are collected from several institutions and divided into five
mutually exclusive sets for carrying five-fold cross-validation. Contours that
are generated by the proposed method demonstrated a high level of accuracy when
compared with ground truth and the traditional method. The proposed neural
network and the recursive learning scheme improve the overall quality of the
segmentation on humerus and scapula on the low-resolution dataset and reduced
incorrect segmentation in the ground truth, which could have a positive impact
on finding the cause of shoulder pain and patient's early relief.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Non-Rigid 3D Registration. (arXiv:2203.07858v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07858">
<div class="article-summary-box-inner">
<span><p>Non-rigid registration computes an alignment between a source surface with a
target surface in a non-rigid manner. In the past decade, with the advances in
3D sensing technologies that can measure time-varying surfaces, non-rigid
registration has been applied for the acquisition of deformable shapes and has
a wide range of applications. This survey presents a comprehensive review of
non-rigid registration methods for 3D shapes, focusing on techniques related to
dynamic shape acquisition and reconstruction. In particular, we review
different approaches for representing the deformation field, and the methods
for computing the desired deformation. Both optimization-based and
learning-based methods are covered. We also review benchmarks and datasets for
evaluating non-rigid registration methods, and discuss potential future
research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Don't Get Me Wrong: How to apply Deep Visual Interpretations to Time Series. (arXiv:2203.07861v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07861">
<div class="article-summary-box-inner">
<span><p>The correct interpretation and understanding of deep learning models is
essential in many applications. Explanatory visual interpretation approaches
for image and natural language processing allow domain experts to validate and
understand almost any deep learning model. However, they fall short when
generalizing to arbitrary time series data that is less intuitive and more
diverse. Whether a visualization explains the true reasoning or captures the
real features is difficult to judge. Hence, instead of blind trust we need an
objective evaluation to obtain reliable quality metrics. We propose a framework
of six orthogonal metrics for gradient- or perturbation-based post-hoc visual
interpretation methods designed for time series classification and segmentation
tasks. An experimental study includes popular neural network architectures for
time series and nine visual interpretation methods. We evaluate the visual
interpretation methods with diverse datasets from the UCR repository and a
complex real-world dataset, and study the influence of common regularization
techniques during training. We show that none of the methods consistently
outperforms any of the others on all metrics while some are ahead at times. Our
insights and recommendations allow experts to make informed choices of suitable
visualization techniques for the model and task at hand.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LiP-Flow: Learning Inference-time Priors for Codec Avatars via Normalizing Flows in Latent Space. (arXiv:2203.07881v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07881">
<div class="article-summary-box-inner">
<span><p>Neural face avatars that are trained from multi-view data captured in camera
domes can produce photo-realistic 3D reconstructions. However, at inference
time, they must be driven by limited inputs such as partial views recorded by
headset-mounted cameras or a front-facing camera, and sparse facial landmarks.
To mitigate this asymmetry, we introduce a prior model that is conditioned on
the runtime inputs and tie this prior space to the 3D face model via a
normalizing flow in the latent space. Our proposed model, LiP-Flow, consists of
two encoders that learn representations from the rich training-time and
impoverished inference-time observations. A normalizing flow bridges the two
representation spaces and transforms latent samples from one domain to another,
allowing us to define a latent likelihood objective. We trained our model
end-to-end to maximize the similarity of both representation spaces and the
reconstruction quality, making the 3D face model aware of the limited driving
signals. We conduct extensive evaluations where the latent codes are optimized
to reconstruct 3D avatars from partial or sparse observations. We show that our
approach leads to an expressive and effective prior, capturing facial dynamics
and subtle expressions better.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">K-VQG: Knowledge-aware Visual Question Generation for Common-sense Acquisition. (arXiv:2203.07890v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07890">
<div class="article-summary-box-inner">
<span><p>Visual Question Generation (VQG) is a task to generate questions from images.
When humans ask questions about an image, their goal is often to acquire some
new knowledge. However, existing studies on VQG have mainly addressed question
generation from answers or question categories, overlooking the objectives of
knowledge acquisition. To introduce a knowledge acquisition perspective into
VQG, we constructed a novel knowledge-aware VQG dataset called K-VQG. This is
the first large, humanly annotated dataset in which questions regarding images
are tied to structured knowledge. We also developed a new VQG model that can
encode and use knowledge as the target for a question. The experiment results
show that our model outperforms existing models on the K-VQG dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalized Rectifier Wavelet Covariance Models For Texture Synthesis. (arXiv:2203.07902v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07902">
<div class="article-summary-box-inner">
<span><p>State-of-the-art maximum entropy models for texture synthesis are built from
statistics relying on image representations defined by convolutional neural
networks (CNN). Such representations capture rich structures in texture images,
outperforming wavelet-based representations in this regard. However, conversely
to neural networks, wavelets offer meaningful representations, as they are
known to detect structures at multiple scales (e.g. edges) in images. In this
work, we propose a family of statistics built upon non-linear wavelet based
representations, that can be viewed as a particular instance of a one-layer
CNN, using a generalized rectifier non-linearity. These statistics
significantly improve the visual quality of previous classical wavelet-based
models, and allow one to produce syntheses of similar quality to
state-of-the-art models, on both gray-scale and color textures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Learning Based Focal Stack Camera Depth Estimation. (arXiv:2203.07904v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07904">
<div class="article-summary-box-inner">
<span><p>We propose an unsupervised deep learning based method to estimate depth from
focal stack camera images. On the NYU-v2 dataset, our method achieves much
better depth estimation accuracy compared to single-image based methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Panoptic SwiftNet: Pyramidal Fusion for Real-time Panoptic Segmentation. (arXiv:2203.07908v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07908">
<div class="article-summary-box-inner">
<span><p>Dense panoptic prediction is a key ingredient in many existing applications
such as autonomous driving, automated warehouses or agri-robotics. However,
most of these applications leverage the recovered dense semantics as an input
to visual closed-loop control. Hence, practical deployments require real-time
inference over large input resolutions on embedded hardware. These requirements
call for computationally efficient approaches which deliver high accuracy with
limited computational resources. We propose to achieve this goal by trading-off
backbone capacity for multi-scale feature extraction. In comparison with
contemporaneous approaches to panoptic segmentation, the main novelties of our
method are scale-equivariant feature extraction and cross-scale upsampling
through pyramidal fusion. Our best model achieves 55.9% PQ on Cityscapes val at
60 FPS on full resolution 2MPx images and RTX3090 with FP16 Tensor RT
optimization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Transfer Learning with Graph Neural Network for Sensor-Based Human Activity Recognition. (arXiv:2203.07910v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07910">
<div class="article-summary-box-inner">
<span><p>The sensor-based human activity recognition (HAR) in mobile application
scenarios is often confronted with sensor modalities variation and annotated
data deficiency. Given this observation, we devised a graph-inspired deep
learning approach toward the sensor-based HAR tasks, which was further used to
build a deep transfer learning model toward giving a tentative solution for
these two challenging problems. Specifically, we present a multi-layer residual
structure involved graph convolutional neural network (ResGCNN) toward the
sensor-based HAR tasks, namely the HAR-ResGCNN approach. Experimental results
on the PAMAP2 and mHealth data sets demonstrate that our ResGCNN is effective
at capturing the characteristics of actions with comparable results compared to
other sensor-based HAR models (with an average accuracy of 98.18% and 99.07%,
respectively). More importantly, the deep transfer learning experiments using
the ResGCNN model show excellent transferability and few-shot learning
performance. The graph-based framework shows good meta-learning ability and is
supposed to be a promising solution in sensor-based HAR tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GPV-Pose: Category-level Object Pose Estimation via Geometry-guided Point-wise Voting. (arXiv:2203.07918v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07918">
<div class="article-summary-box-inner">
<span><p>While 6D object pose estimation has recently made a huge leap forward, most
methods can still only handle a single or a handful of different objects, which
limits their applications. To circumvent this problem, category-level object
pose estimation has recently been revamped, which aims at predicting the 6D
pose as well as the 3D metric size for previously unseen instances from a given
set of object classes. This is, however, a much more challenging task due to
severe intra-class shape variations. To address this issue, we propose
GPV-Pose, a novel framework for robust category-level pose estimation,
harnessing geometric insights to enhance the learning of category-level
pose-sensitive features. First, we introduce a decoupled confidence-driven
rotation representation, which allows geometry-aware recovery of the associated
rotation matrix. Second, we propose a novel geometry-guided point-wise voting
paradigm for robust retrieval of the 3D object bounding box. Finally,
leveraging these different output streams, we can enforce several geometric
consistency terms, further increasing performance, especially for non-symmetric
categories. GPV-Pose produces superior results to state-of-the-art competitors
on common public benchmarks, whilst almost achieving real-time inference speed
at 20 FPS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relative Pose from SIFT Features. (arXiv:2203.07930v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07930">
<div class="article-summary-box-inner">
<span><p>This paper proposes the geometric relationship of epipolar geometry and
orientation- and scale-covariant, e.g., SIFT, features. We derive a new linear
constraint relating the unknown elements of the fundamental matrix and the
orientation and scale. This equation can be used together with the well-known
epipolar constraint to, e.g., estimate the fundamental matrix from four SIFT
correspondences, essential matrix from three, and to solve the semi-calibrated
case from three correspondences. Requiring fewer correspondences than the
well-known point-based approaches (e.g., 5PT, 6PT and 7PT solvers) for epipolar
geometry estimation makes RANSAC-like randomized robust estimation
significantly faster. The proposed constraint is tested on a number of problems
in a synthetic environment and on publicly available real-world datasets on
more than 80000 image pairs. It is superior to the state-of-the-art in terms of
processing time while often leading to more accurate results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DialogueNeRF: Towards Realistic Avatar Face-to-face Conversation Video Generation. (arXiv:2203.07931v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07931">
<div class="article-summary-box-inner">
<span><p>Conversation is an essential component of virtual avatar activities in the
metaverse. With the development of natural language processing, textual and
vocal conversation generation has achieved a significant breakthrough.
Face-to-face conversations account for the vast majority of daily
conversations. However, this task has not acquired enough attention. In this
paper, we propose a novel task that aims to generate a realistic human avatar
face-to-face conversation process and present a new dataset to explore this
target. To tackle this novel task, we propose a new framework that utilizes a
series of conversation signals, e.g. audio, head pose, and expression, to
synthesize face-to-face conversation videos between human avatars, with all the
interlocutors modeled within the same network. Our method is evaluated by
quantitative and qualitative experiments in different aspects, e.g. image
quality, pose sequence trend, and naturalness of the rendering videos. All the
code, data, and models will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Style Transformer for Image Inversion and Editing. (arXiv:2203.07932v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07932">
<div class="article-summary-box-inner">
<span><p>Existing GAN inversion methods fail to provide latent codes for reliable
reconstruction and flexible editing simultaneously. This paper presents a
transformer-based image inversion and editing model for pretrained StyleGAN
which is not only with less distortions, but also of high quality and
flexibility for editing. The proposed model employs a CNN encoder to provide
multi-scale image features as keys and values. Meanwhile it regards the style
code to be determined for different layers of the generator as queries. It
first initializes query tokens as learnable parameters and maps them into W+
space. Then the multi-stage alternate self- and cross-attention are utilized,
updating queries with the purpose of inverting the input by the generator.
Moreover, based on the inverted code, we investigate the reference- and
label-based attribute editing through a pretrained latent classifier, and
achieve flexible image-to-image translation with high quality results.
Extensive experiments are carried out, showing better performances on both
inversion and editing tasks within StyleGAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Intrinsic Neural Fields: Learning Functions on Manifolds. (arXiv:2203.07967v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07967">
<div class="article-summary-box-inner">
<span><p>Neural fields have gained significant attention in the computer vision
community due to their excellent performance in novel view synthesis, geometry
reconstruction, and generative modeling. Some of their advantages are a sound
theoretic foundation and an easy implementation in current deep learning
frameworks. While neural fields have been applied to signals on manifolds,
e.g., for texture reconstruction, their representation has been limited to
extrinsically embedding the shape into Euclidean space. The extrinsic embedding
ignores known intrinsic manifold properties and is inflexible wrt. transfer of
the learned function. To overcome these limitations, this work introduces
intrinsic neural fields, a novel and versatile representation for neural fields
on manifolds. Intrinsic neural fields combine the advantages of neural fields
with the spectral properties of the Laplace-Beltrami operator. We show
theoretically that intrinsic neural fields inherit many desirable properties of
the extrinsic neural field framework but exhibit additional intrinsic
qualities, like isometry invariance. In experiments, we show intrinsic neural
fields can reconstruct high-fidelity textures from images with state-of-the-art
quality and are robust to the discretization of the underlying manifold. We
demonstrate the versatility of intrinsic neural fields by tackling various
applications: texture transfer between deformed shapes &amp; different shapes,
texture reconstruction from real-world images with view dependence, and
discretization-agnostic learning on meshes and point clouds.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MOBDrone: a Drone Video Dataset for Man OverBoard Rescue. (arXiv:2203.07973v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07973">
<div class="article-summary-box-inner">
<span><p>Modern Unmanned Aerial Vehicles (UAV) equipped with cameras can play an
essential role in speeding up the identification and rescue of people who have
fallen overboard, i.e., man overboard (MOB). To this end, Artificial
Intelligence techniques can be leveraged for the automatic understanding of
visual data acquired from drones. However, detecting people at sea in aerial
imagery is challenging primarily due to the lack of specialized annotated
datasets for training and testing detectors for this task. To fill this gap, we
introduce and publicly release the MOBDrone benchmark, a collection of more
than 125K drone-view images in a marine environment under several conditions,
such as different altitudes, camera shooting angles, and illumination. We
manually annotated more than 180K objects, of which about 113K man overboard,
precisely localizing them with bounding boxes. Moreover, we conduct a thorough
performance analysis of several state-of-the-art object detectors on the
MOBDrone data, serving as baselines for further research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Pitfalls of Batch Normalization for End-to-End Video Learning: A Study on Surgical Workflow Analysis. (arXiv:2203.07976v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07976">
<div class="article-summary-box-inner">
<span><p>Batch Normalization's (BN) unique property of depending on other samples in a
batch is known to cause problems in several tasks, including sequential
modeling, and has led to the use of alternatives in these fields. In video
learning, however, these problems are less studied, despite the ubiquitous use
of BN in CNNs for visual feature extraction. We argue that BN's properties
create major obstacles for training CNNs and temporal models end to end in
video tasks. Yet, end-to-end learning seems preferable in specialized domains
such as surgical workflow analysis, which lack well-pretrained feature
extractors. While previous work in surgical workflow analysis has avoided
BN-related issues through complex, multi-stage learning procedures, we show
that even simple, end-to-end CNN-LSTMs can outperform the state of the art when
CNNs without BN are used. Moreover, we analyze in detail when BN-related issues
occur, including a "cheating" phenomenon in surgical anticipation tasks. We
hope that a deeper understanding of BN's limitations and a reconsideration of
end-to-end approaches can be beneficial for future research in surgical
workflow analysis and general video learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OcclusionFusion: Occlusion-aware Motion Estimation for Real-time Dynamic 3D Reconstruction. (arXiv:2203.07977v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07977">
<div class="article-summary-box-inner">
<span><p>RGBD-based real-time dynamic 3D reconstruction suffers from inaccurate
inter-frame motion estimation as errors may accumulate with online tracking.
This problem is even more severe for single-view-based systems due to strong
occlusions. Based on these observations, we propose OcclusionFusion, a novel
method to calculate occlusion-aware 3D motion to guide the reconstruction. In
our technique, the motion of visible regions is first estimated and combined
with temporal information to infer the motion of the occluded regions through
an LSTM-involved graph neural network. Furthermore, our method computes the
confidence of the estimated motion by modeling the network output with a
probabilistic model, which alleviates untrustworthy motions and enables robust
tracking. Experimental results on public datasets and our own recorded data
show that our technique outperforms existing single-view-based real-time
methods by a large margin. With the reduction of the motion errors, the
proposed technique can handle long and challenging motion sequences. Please
check out the project page for sequence results:
https://wenbin-lin.github.io/OcclusionFusion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Object Detection as Probabilistic Set Prediction. (arXiv:2203.07980v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07980">
<div class="article-summary-box-inner">
<span><p>Accurate uncertainty estimates are essential for deploying deep object
detectors in safety-critical systems. The development and evaluation of
probabilistic object detectors have been hindered by shortcomings in existing
performance measures, which tend to involve arbitrary thresholds or limit the
detector's choice of distributions. In this work, we propose to view object
detection as a set prediction task where detectors predict the distribution
over the set of objects. Using the negative log-likelihood for random finite
sets, we present a proper scoring rule for evaluating and training
probabilistic object detectors. The proposed method can be applied to existing
probabilistic detectors, is free from thresholds, and enables fair comparison
between architectures. Three different types of detectors are evaluated on the
COCO dataset. Our results indicate that the training of existing detectors is
optimized toward non-probabilistic metrics. We hope to encourage the
development of new object detectors that can accurately estimate their own
uncertainty. Code will be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Smoothing Matters: Momentum Transformer for Domain Adaptive Semantic Segmentation. (arXiv:2203.07988v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07988">
<div class="article-summary-box-inner">
<span><p>After the great success of Vision Transformer variants (ViTs) in computer
vision, it has also demonstrated great potential in domain adaptive semantic
segmentation. Unfortunately, straightforwardly applying local ViTs in domain
adaptive semantic segmentation does not bring in expected improvement. We find
that the pitfall of local ViTs is due to the severe high-frequency components
generated during both the pseudo-label construction and features alignment for
target domains. These high-frequency components make the training of local ViTs
very unsmooth and hurt their transferability. In this paper, we introduce a
low-pass filtering mechanism, momentum network, to smooth the learning dynamics
of target domain features and pseudo labels. Furthermore, we propose a dynamic
of discrepancy measurement to align the distributions in the source and target
domains via dynamic weights to evaluate the importance of the samples. After
tackling the above issues, extensive experiments on sim2real benchmarks show
that the proposed method outperforms the state-of-the-art methods. Our codes
are available at https://github.com/alpc91/TransDA
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Uni-Modal Self-Supervised Learning for Multimodal Audio-Visual Speech Recognition. (arXiv:2203.07996v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07996">
<div class="article-summary-box-inner">
<span><p>Training Transformer-based models demands a large amount of data, while
obtaining parallel aligned and labelled data in multimodality is rather
cost-demanding, especially for audio-visual speech recognition (AVSR). Thus it
makes a lot of sense to make use of unlabelled uni-modal data. On the other
side, although the effectiveness of large-scale self-supervised learning is
well established in both audio and visual modalities, how to integrate those
pre-trained models into a multimodal scenario remains underexplored. In this
work, we successfully leverage uni-modal self-supervised learning to promote
the multimodal AVSR. In particular, we first train audio and visual encoders on
a large-scale uni-modal dataset, then we integrate components of both encoders
into a larger multimodal framework which learns to recognize paired
audio-visual data into characters through a combination of CTC and seq2seq
decoding. We show that both components inherited from uni-modal self-supervised
learning cooperate well, resulting in that the multimodal framework yields
competitive results through fine-tuning. Our model is experimentally validated
on both word-level and sentence-level AVSR tasks. Especially, even without an
external language model, our proposed model raises the state-of-the-art
performances on the widely accepted Lip Reading Sentences 2 (LRS2) dataset by a
large margin, with a relative improvement of 30%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inverted Pyramid Multi-task Transformer for Dense Scene Understanding. (arXiv:2203.07997v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07997">
<div class="article-summary-box-inner">
<span><p>Multi-task dense scene understanding is a thriving research domain that
requires simultaneous perception and reasoning on a series of correlated tasks
with pixel-wise prediction. Most existing works encounter a severe limitation
of modeling in the locality due to heavy utilization of convolution operations,
while learning interactions and inference in a global spatial-position and
multi-task context is critical for this problem. In this paper, we propose a
novel end-to-end Inverted Pyramid multi-task (InvPT) Transformer to perform
simultaneous modeling of spatial positions and multiple tasks in a unified
framework. To the best of our knowledge, this is the first work that explores
designing a transformer structure for multi-task dense prediction for scene
understanding. Besides, it is widely demonstrated that a higher spatial
resolution is remarkably beneficial for dense predictions, while it is very
challenging for existing transformers to go deeper with higher resolutions due
to huge complexity to large spatial size. InvPT presents an efficient
UP-Transformer block to learn multi-task feature interaction at gradually
increased resolutions, which also incorporates effective self-attention message
passing and multi-scale feature aggregation to produce task-specific prediction
at a high resolution. Our method achieves superior multi-task performance on
NYUD-v2 and PASCAL-Context datasets respectively, and significantly outperforms
previous state-of-the-arts. Code and trained models will be publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Modeling via Information Tree for One-Shot Natural Language Spatial Video Grounding. (arXiv:2203.08013v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08013">
<div class="article-summary-box-inner">
<span><p>Natural language spatial video grounding aims to detect the relevant objects
in video frames with descriptive sentences as the query. In spite of the great
advances, most existing methods rely on dense video frame annotations, which
require a tremendous amount of human effort. To achieve effective grounding
under a limited annotation budget, we investigate one-shot video grounding, and
learn to ground natural language in all video frames with solely one frame
labeled, in an end-to-end manner. One major challenge of end-to-end one-shot
video grounding is the existence of videos frames that are either irrelevant to
the language query or the labeled frames. Another challenge relates to the
limited supervision, which might result in ineffective representation learning.
To address these challenges, we designed an end-to-end model via Information
Tree for One-Shot video grounding (IT-OS). Its key module, the information
tree, can eliminate the interference of irrelevant frames based on branch
search and branch cropping techniques. In addition, several self-supervised
tasks are proposed based on the information tree to improve the representation
learning under insufficient labeling. Experiments on the benchmark dataset
demonstrate the effectiveness of our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Noise-level-aware Framework for PET Image Denoising. (arXiv:2203.08034v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08034">
<div class="article-summary-box-inner">
<span><p>In PET, the amount of relative (signal-dependent) noise present in different
body regions can be significantly different and is inherently related to the
number of counts present in that region. The number of counts in a region
depends, in principle and among other factors, on the total administered
activity, scanner sensitivity, image acquisition duration, radiopharmaceutical
tracer uptake in the region, and patient local body morphometry surrounding the
region. In theory, less amount of denoising operations is needed to denoise a
high-count (low relative noise) image than images a low-count (high relative
noise) image, and vice versa. The current deep-learning-based methods for PET
image denoising are predominantly trained on image appearance only and have no
special treatment for images of different noise levels. Our hypothesis is that
by explicitly providing the local relative noise level of the input image to a
deep convolutional neural network (DCNN), the DCNN can outperform itself
trained on image appearance only. To this end, we propose a noise-level-aware
framework denoising framework that allows embedding of local noise level into a
DCNN. The proposed is trained and tested on 30 and 15 patient PET images
acquired on a GE Discovery MI PET/CT system. Our experiments showed that the
increases in both PSNR and SSIM from our backbone network with relative noise
level embedding (NLE) versus the same network without NLE were statistically
significant with p&lt;0.001, and the proposed method significantly outperformed a
strong baseline method by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep learning for radar data exploitation of autonomous vehicle. (arXiv:2203.08038v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08038">
<div class="article-summary-box-inner">
<span><p>Autonomous driving requires a detailed understanding of complex driving
scenes. The redundancy and complementarity of the vehicle's sensors provide an
accurate and robust comprehension of the environment, thereby increasing the
level of performance and safety. This thesis focuses the on automotive RADAR,
which is a low-cost active sensor measuring properties of surrounding objects,
including their relative speed, and has the key advantage of not being impacted
by adverse weather conditions. With the rapid progress of deep learning and the
availability of public driving datasets, the perception ability of vision-based
driving systems has considerably improved. The RADAR sensor is seldom used for
scene understanding due to its poor angular resolution, the size, noise, and
complexity of RADAR raw data as well as the lack of available datasets. This
thesis proposes an extensive study of RADAR scene understanding, from the
construction of an annotated dataset to the conception of adapted deep learning
architectures. First, this thesis details approaches to tackle the current lack
of data. A simple simulation as well as generative methods for creating
annotated data will be presented. It will also describe the CARRADA dataset,
composed of synchronised camera and RADAR data with a semi-automatic annotation
method. This thesis then present a proposed set of deep learning architectures
with their associated loss functions for RADAR semantic segmentation. It also
introduces a method to open up research into the fusion of LiDAR and RADAR
sensors for scene understanding. Finally, this thesis exposes a collaborative
contribution, the RADIal dataset with synchronised High-Definition (HD) RADAR,
LiDAR and camera. A deep learning architecture is also proposed to estimate the
RADAR signal processing pipeline while performing multitask learning for object
detection and free driving space segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simultaneous Localisation and Mapping with Quadric Surfaces. (arXiv:2203.08040v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08040">
<div class="article-summary-box-inner">
<span><p>There are many possibilities for how to represent the map in simultaneous
localisation and mapping (SLAM). While sparse, keypoint-based SLAM systems have
achieved impressive levels of accuracy and robustness, their maps may not be
suitable for many robotic tasks. Dense SLAM systems are capable of producing
dense reconstructions, but can be computationally expensive and, like sparse
systems, lack higher-level information about the structure of a scene.
Human-made environments contain a lot of structure, and we seek to take
advantage of this by enabling the use of quadric surfaces as features in SLAM
systems. We introduce a minimal representation for quadric surfaces and show
how this can be included in a least-squares formulation. We also show how our
representation can be easily extended to include additional constraints on
quadrics such as those found in quadrics of revolution. Finally, we introduce a
proof-of-concept SLAM system using our representation, and provide some
experimental results using an RGB-D dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A multi-organ point cloud registration algorithm for abdominal CT registration. (arXiv:2203.08041v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08041">
<div class="article-summary-box-inner">
<span><p>Registering CT images of the chest is a crucial step for several tasks such
as disease progression tracking or surgical planning. It is also a challenging
step because of the heterogeneous content of the human abdomen which implies
complex deformations. In this work, we focus on accurately registering a subset
of organs of interest. We register organ surface point clouds, as may typically
be extracted from an automatic segmentation pipeline, by expanding the Bayesian
Coherent Point Drift algorithm (BCPD). We introduce MO-BCPD, a multi-organ
version of the BCPD algorithm which explicitly models three important aspects
of this task: organ individual elastic properties, inter-organ motion coherence
and segmentation inaccuracy. This model also provides an interpolation
framework to estimate the deformation of the entire volume. We demonstrate the
efficiency of our method by registering different patients from the LITS
challenge dataset. The target registration error on anatomical landmarks is
almost twice as small for MO-BCPD compared to standard BCPD while imposing the
same constraints on individual organs deformation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Hyperbolic Embeddings in 2D Object Detection. (arXiv:2203.08049v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08049">
<div class="article-summary-box-inner">
<span><p>Object detection, for the most part, has been formulated in the euclidean
space, where euclidean or spherical geodesic distances measure the similarity
of an image region to an object class prototype. In this work, we study whether
a hyperbolic geometry better matches the underlying structure of the object
classification space. We incorporate a hyperbolic classifier in two-stage,
keypoint-based, and transformer-based object detection architectures and
evaluate them on large-scale, long-tailed, and zero-shot object detection
benchmarks. In our extensive experimental evaluations, we observe categorical
class hierarchies emerging in the structure of the classification space,
resulting in lower classification errors and boosting the overall object
detection performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Seeking Commonness and Inconsistencies: A Jointly Smoothed Approach to Multi-view Subspace Clustering. (arXiv:2203.08060v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08060">
<div class="article-summary-box-inner">
<span><p>Multi-view subspace clustering aims to discover the hidden subspace
structures from multiple views for robust clustering, and has been attracting
considerable attention in recent years. Despite significant progress, most of
the previous multi-view subspace clustering algorithms are still faced with two
limitations. First, they usually focus on the consistency (or commonness) of
multiple views, yet often lack the ability to capture the cross-view
inconsistencies in subspace representations. Second, many of them overlook the
local structures of multiple views and cannot jointly leverage multiple local
structures to enhance the subspace representation learning. To address these
two limitations, in this paper, we propose a jointly smoothed multi-view
subspace clustering (JSMC) approach. Specifically, we simultaneously
incorporate the cross-view commonness and inconsistencies into the subspace
representation learning. The view-consensus grouping effect is presented to
jointly exploit the local structures of multiple views to regularize the
view-commonness representation, which is further associated with the low-rank
constraint via the nuclear norm to strengthen its cluster structure. Thus the
cross-view commonness and inconsistencies, the view-consensus grouping effect,
and the low-rank representation are seamlessly incorporated into a unified
objective function, upon which an alternating optimization algorithm is
performed to achieve a robust subspace representation for clustering.
Experimental results on a variety of real-world multi-view datasets have
confirmed the superiority of the proposed approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MotionCLIP: Exposing Human Motion Generation to CLIP Space. (arXiv:2203.08063v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08063">
<div class="article-summary-box-inner">
<span><p>We introduce MotionCLIP, a 3D human motion auto-encoder featuring a latent
embedding that is disentangled, well behaved, and supports highly semantic
textual descriptions. MotionCLIP gains its unique power by aligning its latent
space with that of the Contrastive Language-Image Pre-training (CLIP) model.
Aligning the human motion manifold to CLIP space implicitly infuses the
extremely rich semantic knowledge of CLIP into the manifold. In particular, it
helps continuity by placing semantically similar motions close to one another,
and disentanglement, which is inherited from the CLIP-space structure.
MotionCLIP comprises a transformer-based motion auto-encoder, trained to
reconstruct motion while being aligned to its text label's position in
CLIP-space. We further leverage CLIP's unique visual understanding and inject
an even stronger signal through aligning motion to rendered frames in a
self-supervised manner. We show that although CLIP has never seen the motion
domain, MotionCLIP offers unprecedented text-to-motion abilities, allowing
out-of-domain actions, disentangled editing, and abstract language
specification. For example, the text prompt "couch" is decoded into a sitting
down motion, due to lingual similarity, and the prompt "Spiderman" results in a
web-swinging-like solution that is far from seen during training. In addition,
we show how the introduced latent space can be leveraged for motion
interpolation, editing and recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Things not Written in Text: Exploring Spatial Commonsense from Visual Signals. (arXiv:2203.08075v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08075">
<div class="article-summary-box-inner">
<span><p>Spatial commonsense, the knowledge about spatial position and relationship
between objects (like the relative size of a lion and a girl, and the position
of a boy relative to a bicycle when cycling), is an important part of
commonsense knowledge. Although pretrained language models (PLMs) succeed in
many NLP tasks, they are shown to be ineffective in spatial commonsense
reasoning. Starting from the observation that images are more likely to exhibit
spatial commonsense than texts, we explore whether models with visual signals
learn more spatial commonsense than text-based PLMs. We propose a spatial
commonsense benchmark that focuses on the relative scales of objects, and the
positional relationship between people and objects under different actions. We
probe PLMs and models with visual signals, including vision-language pretrained
models and image synthesis models, on this benchmark, and find that image
synthesis models are more capable of learning accurate and consistent spatial
knowledge than other models. The spatial knowledge from image synthesis models
also helps in natural language understanding tasks that require spatial
commonsense.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Implicit Feature Decoupling with Depthwise Quantization. (arXiv:2203.08080v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08080">
<div class="article-summary-box-inner">
<span><p>Quantization has been applied to multiple domains in Deep Neural Networks
(DNNs). We propose Depthwise Quantization (DQ) where $\textit{quantization}$ is
applied to a decomposed sub-tensor along the $\textit{feature axis}$ of weak
statistical dependence. The feature decomposition leads to an exponential
increase in $\textit{representation capacity}$ with a linear increase in memory
and parameter cost. In addition, DQ can be directly applied to existing
encoder-decoder frameworks without modification of the DNN architecture. We use
DQ in the context of Hierarchical Auto-Encoder and train end-to-end on an image
feature representation. We provide an analysis on cross-correlation between
spatial and channel features and we propose a decomposition of the image
feature representation along the channel axis. The improved performance of the
depthwise operator is due to the increased representation capacity from
implicit feature decoupling. We evaluate DQ on the likelihood estimation task,
where it outperforms the previous state-of-the-art on CIFAR-10, ImageNet-32 and
ImageNet-64. We progressively train with increasing image size a single
hierarchical model that uses 69% less parameters and has a faster convergence
than the previous works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ARTEMIS: Attention-based Retrieval with Text-Explicit Matching and Implicit Similarity. (arXiv:2203.08101v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08101">
<div class="article-summary-box-inner">
<span><p>An intuitive way to search for images is to use queries composed of an
example image and a complementary text. While the first provides rich and
implicit context for the search, the latter explicitly calls for new traits, or
specifies how some elements of the example image should be changed to retrieve
the desired target image. Current approaches typically combine the features of
each of the two elements of the query into a single representation, which can
then be compared to the ones of the potential target images. Our work aims at
shedding new light on the task by looking at it through the prism of two
familiar and related frameworks: text-to-image and image-to-image retrieval.
Taking inspiration from them, we exploit the specific relation of each query
element with the targeted image and derive light-weight attention mechanisms
which enable to mediate between the two complementary modalities. We validate
our approach on several retrieval benchmarks, querying with images and their
associated free-form text modifiers. Our method obtains state-of-the-art
results without resorting to side information, multi-level features, heavy
pre-training nor large architectures as in previous works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From 2D to 3D: Re-thinking Benchmarking of Monocular Depth Prediction. (arXiv:2203.08122v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08122">
<div class="article-summary-box-inner">
<span><p>There have been numerous recently proposed methods for monocular depth
prediction (MDP) coupled with the equally rapid evolution of benchmarking
tools. However, we argue that MDP is currently witnessing benchmark
over-fitting and relying on metrics that are only partially helpful to gauge
the usefulness of the predictions for 3D applications. This limits the design
and development of novel methods that are truly aware of - and improving
towards estimating - the 3D structure of the scene rather than optimizing
2D-based distances. In this work, we aim to bring structural awareness to MDP,
an inherently 3D task, by exhibiting the limits of evaluation metrics towards
assessing the quality of the 3D geometry. We propose a set of metrics well
suited to evaluate the 3D geometry of MDP approaches and a novel indoor
benchmark, RIO-D3D, crucial for the proposed evaluation methodology. Our
benchmark is based on a real-world dataset featuring high-quality rendered
depth maps obtained from RGB-D reconstructions. We further demonstrate this to
help benchmark the closely-tied task of 3D scene completion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Neural Nets Learn the Same Model Twice? Investigating Reproducibility and Double Descent from the Decision Boundary Perspective. (arXiv:2203.08124v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08124">
<div class="article-summary-box-inner">
<span><p>We discuss methods for visualizing neural network decision boundaries and
decision regions. We use these visualizations to investigate issues related to
reproducibility and generalization in neural network training. We observe that
changes in model architecture (and its associate inductive bias) cause visible
changes in decision boundaries, while multiple runs with the same architecture
yield results with strong similarities, especially in the case of wide
architectures. We also use decision boundary methods to visualize double
descent phenomena. We see that decision boundary reproducibility depends
strongly on model width. Near the threshold of interpolation, neural network
decision boundaries become fragmented into many small decision regions, and
these regions are non-reproducible. Meanwhile, very narrows and very wide
networks have high levels of reproducibility in their decision boundaries with
relatively few decision regions. We discuss how our observations relate to the
theory of double descent phenomena in convex models. Code is available at
https://github.com/somepago/dbViz
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Network Doesn't Rule Them All: Moving Beyond Handcrafted Architectures in Self-Supervised Learning. (arXiv:2203.08130v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08130">
<div class="article-summary-box-inner">
<span><p>The current literature on self-supervised learning (SSL) focuses on
developing learning objectives to train neural networks more effectively on
unlabeled data. The typical development process involves taking
well-established architectures, e.g., ResNet demonstrated on ImageNet, and
using them to evaluate newly developed objectives on downstream scenarios.
While convenient, this does not take into account the role of architectures
which has been shown to be crucial in the supervised learning literature. In
this work, we establish extensive empirical evidence showing that a network
architecture plays a significant role in SSL. We conduct a large-scale study
with over 100 variants of ResNet and MobileNet architectures and evaluate them
across 11 downstream scenarios in the SSL setting. We show that there is no one
network that performs consistently well across the scenarios. Based on this, we
propose to learn not only network weights but also architecture topologies in
the SSL regime. We show that "self-supervised architectures" outperform popular
handcrafted architectures (ResNet18 and MobileNetV2) while performing
competitively with the larger and computationally heavy ResNet50 on major image
classification benchmarks (ImageNet-1K, iNat2021, and more). Our results
suggest that it is time to consider moving beyond handcrafted architectures in
SSL and start thinking about incorporating architecture search into
self-supervised learning objectives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Animatable Neural Implicit Surfaces for Creating Avatars from Videos. (arXiv:2203.08133v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08133">
<div class="article-summary-box-inner">
<span><p>This paper aims to reconstruct an animatable human model from a video of very
sparse camera views. Some recent works represent human geometry and appearance
with neural radiance fields and utilize parametric human models to produce
deformation fields for animation, which enables them to recover detailed 3D
human models from videos. However, their reconstruction results tend to be
noisy due to the lack of surface constraints on radiance fields. Moreover, as
they generate the human appearance in 3D space, their rendering quality heavily
depends on the accuracy of deformation fields. To solve these problems, we
propose Animatable Neural Implicit Surface (AniSDF), which models the human
geometry with a signed distance field and defers the appearance generation to
the 2D image space with a 2D neural renderer. The signed distance field
naturally regularizes the learned geometry, enabling the high-quality
reconstruction of human bodies, which can be further used to improve the
rendering speed. Moreover, the 2D neural renderer can be learned to compensate
for geometric errors, making the rendering more robust to inaccurate
deformations. Experiments on several datasets show that the proposed approach
outperforms recent human reconstruction and synthesis methods by a large
margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CryoAI: Amortized Inference of Poses for Ab Initio Reconstruction of 3D Molecular Volumes from Real Cryo-EM Images. (arXiv:2203.08138v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08138">
<div class="article-summary-box-inner">
<span><p>Cryo-electron microscopy (cryo-EM) has become a tool of fundamental
importance in structural biology, helping us understand the basic building
blocks of life. The algorithmic challenge of cryo-EM is to jointly estimate the
unknown 3D poses and the 3D electron scattering potential of a biomolecule from
millions of extremely noisy 2D images. Existing reconstruction algorithms,
however, cannot easily keep pace with the rapidly growing size of cryo-EM
datasets due to their high computational and memory cost. We introduce cryoAI,
an ab initio reconstruction algorithm for homogeneous conformations that uses
direct gradient-based optimization of particle poses and the electron
scattering potential from single-particle cryo-EM data. CryoAI combines a
learned encoder that predicts the poses of each particle image with a
physics-based decoder to aggregate each particle image into an implicit
representation of the scattering potential volume. This volume is stored in the
Fourier domain for computational efficiency and leverages a modern coordinate
network architecture for memory efficiency. Combined with a symmetrized loss
function, this framework achieves results of a quality on par with
state-of-the-art cryo-EM solvers for both simulated and experimental data, one
order of magnitude faster for large datasets and with significantly lower
memory requirements than existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Spatio-Temporal Downsampling for Effective Video Upscaling. (arXiv:2203.08140v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08140">
<div class="article-summary-box-inner">
<span><p>Downsampling is one of the most basic image processing operations. Improper
spatio-temporal downsampling applied on videos can cause aliasing issues such
as moir\'e patterns in space and the wagon-wheel effect in time. Consequently,
the inverse task of upscaling a low-resolution, low frame-rate video in space
and time becomes a challenging ill-posed problem due to information loss and
aliasing artifacts. In this paper, we aim to solve the space-time aliasing
problem by learning a spatio-temporal downsampler. Towards this goal, we
propose a neural network framework that jointly learns spatio-temporal
downsampling and upsampling. It enables the downsampler to retain the key
patterns of the original video and maximizes the reconstruction performance of
the upsampler. To make the downsamping results compatible with popular image
and video storage formats, the downsampling results are encoded to uint8 with a
differentiable quantization layer. To fully utilize the space-time
correspondences, we propose two novel modules for explicit temporal propagation
and space-time feature rearrangement. Experimental results show that our
proposed method significantly boosts the space-time reconstruction quality by
preserving spatial textures and motion patterns in both downsampling and
upscaling. Moreover, our framework enables a variety of applications, including
arbitrary video resampling, blurry frame reconstruction, and efficient video
storage.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Object Manipulation via Visual Target Localization. (arXiv:2203.08141v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08141">
<div class="article-summary-box-inner">
<span><p>Object manipulation is a critical skill required for Embodied AI agents
interacting with the world around them. Training agents to manipulate objects,
poses many challenges. These include occlusion of the target object by the
agent's arm, noisy object detection and localization, and the target frequently
going out of view as the agent moves around in the scene. We propose
Manipulation via Visual Object Location Estimation (m-VOLE), an approach that
explores the environment in search for target objects, computes their 3D
coordinates once they are located, and then continues to estimate their 3D
locations even when the objects are not visible, thus robustly aiding the task
of manipulating these objects throughout the episode. Our evaluations show a
massive 3x improvement in success rate over a model that has access to the same
sensory suite but is trained without the object location estimator, and our
analysis shows that our agent is robust to noise in depth perception and agent
localization. Importantly, our proposed approach relaxes several assumptions
about idealized localization and perception that are commonly employed by
recent works in embodied AI -- an important step towards training agents for
object manipulation in the real world.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Deep Neural Network for Photo-realistic Image Super-Resolution. (arXiv:1903.02240v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1903.02240">
<div class="article-summary-box-inner">
<span><p>Recent progress in deep learning-based models has improved photo-realistic
(or perceptual) single-image super-resolution significantly. However, despite
their powerful performance, many methods are difficult to apply to real-world
applications because of the heavy computational requirements. To facilitate the
use of a deep model under such demands, we focus on keeping the network
efficient while maintaining its performance. In detail, we design an
architecture that implements a cascading mechanism on a residual network to
boost the performance with limited resources via multi-level feature fusion. In
addition, our proposed model adopts group convolution and recursive schemes in
order to achieve extreme efficiency. We further improve the perceptual quality
of the output by employing the adversarial learning paradigm and a multi-scale
discriminator approach. The performance of our method is investigated through
extensive internal experiments and benchmarks using various datasets. Our
results show that our models outperform the recent methods with similar
complexity, for both traditional pixel-based and perception-based tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MTP: Multi-Task Pruning for Efficient Semantic Segmentation Networks. (arXiv:2007.08386v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.08386">
<div class="article-summary-box-inner">
<span><p>This paper focuses on channel pruning for semantic segmentation networks.
Previous methods to compress and accelerate deep neural networks in the
classification task cannot be straightforwardly applied to the semantic
segmentation network that involves an implicit multi-task learning problem via
pre-training. To identify the redundancy in segmentation networks, we present a
multi-task channel pruning approach. The importance of each convolution filter
\wrt the channel of an arbitrary layer will be simultaneously determined by the
classification and segmentation tasks. In addition, we develop an alternative
scheme for optimizing importance scores of filters in the entire network.
Experimental results on several benchmarks illustrate the superiority of the
proposed algorithm over the state-of-the-art pruning methods. Notably, we can
obtain an about $2\times$ FLOPs reduction on DeepLabv3 with only an about $1\%$
mIoU drop on the PASCAL VOC 2012 dataset and an about $1.3\%$ mIoU drop on
Cityscapes dataset, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Extrinsic Calibration Method for LiDAR and Camera Sensor Setups. (arXiv:2101.04431v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.04431">
<div class="article-summary-box-inner">
<span><p>Most sensor setups for onboard autonomous perception are composed of LiDARs
and vision systems, as they provide complementary information that improves the
reliability of the different algorithms necessary to obtain a robust scene
understanding. However, the effective use of information from different sources
requires an accurate calibration between the sensors involved, which usually
implies a tedious and burdensome process. We present a method to calibrate the
extrinsic parameters of any pair of sensors involving LiDARs, monocular or
stereo cameras, of the same or different modalities. The procedure is composed
of two stages: first, reference points belonging to a custom calibration target
are extracted from the data provided by the sensors to be calibrated, and
second, the optimal rigid transformation is found through the registration of
both point sets. The proposed approach can handle devices with very different
resolutions and poses, as usually found in vehicle setups. In order to assess
the performance of the proposed method, a novel evaluation suite built on top
of a popular simulation framework is introduced. Experiments on the synthetic
environment show that our calibration algorithm significantly outperforms
existing methods, whereas real data tests corroborate the results obtained in
the evaluation suite. Open-source code is available at
https://github.com/beltransen/velo2cam_calibration
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Large-Vocabulary Object Detectors: The Devil is in the Details. (arXiv:2102.01066v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.01066">
<div class="article-summary-box-inner">
<span><p>By design, average precision (AP) for object detection aims to treat all
classes independently: AP is computed independently per category and averaged.
On one hand, this is desirable as it treats all classes equally. On the other
hand, it ignores cross-category confidence calibration, a key property in
real-world use cases. Unfortunately, under important conditions (i.e., large
vocabulary, high instance counts) the default implementation of AP is neither
category independent, nor does it directly reward properly calibrated
detectors. In fact, we show that on LVIS the default implementation produces a
gameable metric, where a simple, un-intuitive re-ranking policy can improve AP
by a large margin. To address these limitations, we introduce two complementary
metrics. First, we present a simple fix to the default AP implementation,
ensuring that it is independent across categories as originally intended. We
benchmark recent LVIS detection advances and find that many reported gains do
not translate to improvements under our new evaluation, suggesting recent
improvements may arise from difficult to interpret changes to cross-category
rankings. Given the importance of reliably benchmarking cross-category
rankings, we consider a pooled version of AP (AP-Pool) that rewards properly
calibrated detectors by directly comparing cross-category rankings. Finally, we
revisit classical approaches for calibration and find that explicitly
calibrating detectors improves state-of-the-art on AP-Pool by 1.7 points
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Building A Group-based Unsupervised Representation Disentanglement Framework. (arXiv:2102.10303v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.10303">
<div class="article-summary-box-inner">
<span><p>Disentangled representation learning is one of the major goals of deep
learning, and is a key step for achieving explainable and generalizable models.
A well-defined theoretical guarantee still lacks for the VAE-based unsupervised
methods, which are a set of popular methods to achieve unsupervised
disentanglement. The Group Theory based definition of representation
disentanglement mathematically connects the data transformations to the
representations using the formalism of group. In this paper, built on the
group-based definition and inspired by the n-th dihedral group, we first
propose a theoretical framework towards achieving unsupervised representation
disentanglement. We then propose a model, based on existing VAE-based methods,
to tackle the unsupervised learning problem of the framework. In the
theoretical framework, we prove three sufficient conditions on model, group
structure, and data respectively in an effort to achieve, in an unsupervised
way, disentangled representation per group-based definition. With the first two
of the conditions satisfied and a necessary condition derived for the third
one, we offer additional constraints, from the perspective of the group-based
definition, for the existing VAE-based models. Experimentally, we train 1800
models covering the most prominent VAE-based methods on five datasets to verify
the effectiveness of our theoretical framework. Compared to the original
VAE-based methods, these Groupified VAEs consistently achieve better mean
performance with smaller variances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unveiling the Power of Mixup for Stronger Classifiers. (arXiv:2103.13027v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.13027">
<div class="article-summary-box-inner">
<span><p>Data mixing augmentation have proved to be effective for improving the
generalization ability of deep neural networks. While early methods mix samples
by hand-crafted policies (e.g., linear interpolation), recent methods utilize
saliency information to match the mixed samples and labels via complex offline
optimization. However, there arises a trade-off between precise mixing policies
and optimization complexity. To address this challenge, we propose a novel
automatic mixup (AutoMix) framework, where the mixup policy is parameterized
and serves the ultimate classification goal directly. Specifically, AutoMix
reformulates the mixup classification into two sub-tasks (i.e., mixed sample
generation and mixup classification) with corresponding sub-networks and solves
them in a bi-level optimization framework. For the generation, a learnable
lightweight mixup generator, Mix Block, is designed to generate mixed samples
by modeling patch-wise relationships under the direct supervision of the
corresponding mixed labels. To prevent the degradation and instability of
bi-level optimization, we further introduce a momentum pipeline to train
AutoMix in an end-to-end manner. Extensive experiments on nine image benchmarks
prove the superiority of AutoMix compared with state-of-the-arts in various
classification scenarios and downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural RGB-D Surface Reconstruction. (arXiv:2104.04532v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.04532">
<div class="article-summary-box-inner">
<span><p>Obtaining high-quality 3D reconstructions of room-scale scenes is of
paramount importance for upcoming applications in AR or VR. These range from
mixed reality applications for teleconferencing, virtual measuring, virtual
room planing, to robotic applications. While current volume-based view
synthesis methods that use neural radiance fields (NeRFs) show promising
results in reproducing the appearance of an object or scene, they do not
reconstruct an actual surface. The volumetric representation of the surface
based on densities leads to artifacts when a surface is extracted using
Marching Cubes, since during optimization, densities are accumulated along the
ray and are not used at a single sample point in isolation. Instead of this
volumetric representation of the surface, we propose to represent the surface
using an implicit function (truncated signed distance function). We show how to
incorporate this representation in the NeRF framework, and extend it to use
depth measurements from a commodity RGB-D sensor, such as a Kinect. In
addition, we propose a pose and camera refinement technique which improves the
overall reconstruction quality. In contrast to concurrent work on integrating
depth priors in NeRF which concentrates on novel view synthesis, our approach
is able to reconstruct high-quality, metrical 3D reconstructions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep 3D-to-2D Watermarking: Embedding Messages in 3D Meshes and Extracting Them from 2D Renderings. (arXiv:2104.13450v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.13450">
<div class="article-summary-box-inner">
<span><p>Digital watermarking is widely used for copyright protection. Traditional 3D
watermarking approaches or commercial software are typically designed to embed
messages into 3D meshes, and later retrieve the messages directly from
distorted/undistorted watermarked 3D meshes. However, in many cases, users only
have access to rendered 2D images instead of 3D meshes. Unfortunately,
retrieving messages from 2D renderings of 3D meshes is still challenging and
underexplored. We introduce a novel end-to-end learning framework to solve this
problem through: 1) an encoder to covertly embed messages in both mesh geometry
and textures; 2) a differentiable renderer to render watermarked 3D objects
from different camera angles and under varied lighting conditions; 3) a decoder
to recover the messages from 2D rendered images. From our experiments, we show
that our model can learn to embed information visually imperceptible to humans,
and to retrieve the embedded information from 2D renderings that undergo 3D
distortions. In addition, we demonstrate that our method can also work with
other renderers, such as ray tracers and real-time renderers with and without
fine-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Connection between Local Attention and Dynamic Depth-wise Convolution. (arXiv:2106.04263v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04263">
<div class="article-summary-box-inner">
<span><p>Vision Transformer (ViT) attains state-of-the-art performance in visual
recognition, and the variant, Local Vision Transformer, makes further
improvements. The major component in Local Vision Transformer, local attention,
performs the attention separately over small local windows. We rephrase local
attention as a channel-wise locally-connected layer and analyze it from two
network regularization manners, sparse connectivity and weight sharing, as well
as weight computation. Sparse connectivity: there is no connection across
channels, and each position is connected to the positions within a small local
window. Weight sharing: the connection weights for one position are shared
across channels or within each group of channels. Dynamic weight: the
connection weights are dynamically predicted according to each image instance.
We point out that local attention resembles depth-wise convolution and its
dynamic version in sparse connectivity. The main difference lies in weight
sharing - depth-wise convolution shares connection weights (kernel weights)
across spatial positions. We empirically observe that the models based on
depth-wise convolution and the dynamic variant with lower computation
complexity perform on-par with or sometimes slightly better than Swin
Transformer, an instance of Local Vision Transformer, for ImageNet
classification, COCO object detection and ADE semantic segmentation. These
observations suggest that Local Vision Transformer takes advantage of two
regularization forms and dynamic weight to increase the network capacity. Code
is available at https://github.com/Atten4Vis/DemystifyLocalViT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdaMatch: A Unified Approach to Semi-Supervised Learning and Domain Adaptation. (arXiv:2106.04732v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04732">
<div class="article-summary-box-inner">
<span><p>We extend semi-supervised learning to the problem of domain adaptation to
learn significantly higher-accuracy models that train on one data distribution
and test on a different one. With the goal of generality, we introduce
AdaMatch, a method that unifies the tasks of unsupervised domain adaptation
(UDA), semi-supervised learning (SSL), and semi-supervised domain adaptation
(SSDA). In an extensive experimental study, we compare its behavior with
respective state-of-the-art techniques from SSL, SSDA, and UDA on vision
classification tasks. We find AdaMatch either matches or significantly exceeds
the state-of-the-art in each case using the same hyper-parameters regardless of
the dataset or task. For example, AdaMatch nearly doubles the accuracy compared
to that of the prior state-of-the-art on the UDA task for DomainNet and even
exceeds the accuracy of the prior state-of-the-art obtained with pre-training
by 6.4% when AdaMatch is trained completely from scratch. Furthermore, by
providing AdaMatch with just one labeled example per class from the target
domain (i.e., the SSDA setting), we increase the target accuracy by an
additional 6.1%, and with 5 labeled examples, by 13.6%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Models as a Data Source for Multiview Representation Learning. (arXiv:2106.05258v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05258">
<div class="article-summary-box-inner">
<span><p>Generative models are now capable of producing highly realistic images that
look nearly indistinguishable from the data on which they are trained. This
raises the question: if we have good enough generative models, do we still need
datasets? We investigate this question in the setting of learning
general-purpose visual representations from a black-box generative model rather
than directly from data. Given an off-the-shelf image generator without any
access to its training data, we train representations from the samples output
by this generator. We compare several representation learning methods that can
be applied to this setting, using the latent space of the generator to generate
multiple "views" of the same semantic content. We show that for contrastive
methods, this multiview data can naturally be used to identify positive pairs
(nearby in latent space) and negative pairs (far apart in latent space). We
find that the resulting representations rival or even outperform those learned
directly from real data, but that good performance requires care in the
sampling strategy applied and the training method. Generative models can be
viewed as a compressed and organized copy of a dataset, and we envision a
future where more and more "model zoos" proliferate while datasets become
increasingly unwieldy, missing, or private. This paper suggests several
techniques for dealing with visual representation learning in such a future.
Code is available on our project page https://ali-design.github.io/GenRep/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Coarse-to-Fine Q-attention: Efficient Learning for Visual Robotic Manipulation via Discretisation. (arXiv:2106.12534v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.12534">
<div class="article-summary-box-inner">
<span><p>We present a coarse-to-fine discretisation method that enables the use of
discrete reinforcement learning approaches in place of unstable and
data-inefficient actor-critic methods in continuous robotics domains. This
approach builds on the recently released ARM algorithm, which replaces the
continuous next-best pose agent with a discrete one, with coarse-to-fine
Q-attention. Given a voxelised scene, coarse-to-fine Q-attention learns what
part of the scene to 'zoom' into. When this 'zooming' behaviour is applied
iteratively, it results in a near-lossless discretisation of the translation
space, and allows the use of a discrete action, deep Q-learning method. We show
that our new coarse-to-fine algorithm achieves state-of-the-art performance on
several difficult sparsely rewarded RLBench vision-based robotics tasks, and
can train real-world policies, tabula rasa, in a matter of minutes, with as
little as 3 demonstrations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From General to Specific: Online Updating for Blind Super-Resolution. (arXiv:2107.02398v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.02398">
<div class="article-summary-box-inner">
<span><p>Most deep learning-based super-resolution (SR) methods are not
image-specific: 1) They are trained on samples synthesized by predefined
degradations (e.g. bicubic downsampling), regardless of the domain gap between
training and testing data. 2) During testing, they super-resolve all images by
the same set of model weights, ignoring the degradation variety. As a result,
most previous methods may suffer a performance drop when the degradations of
test images are unknown and various (i.e. the case of blind SR). To address
these issues, we propose an online SR (ONSR) method. It does not rely on
predefined degradations and allows the model weights to be updated according to
the degradation of the test image. Specifically, ONSR consists of two branches,
namely internal branch (IB) and external branch (EB). IB could learn the
specific degradation of the given test LR image, and EB could learn to super
resolve images degraded by the learned degradation. In this way, ONSR could
customize a specific model for each test image, and thus get more robust to
various degradations. Extensive experiments on both synthesized and real-world
images show that ONSR can generate more visually favorable SR results and
achieve state-of-the-art performance in blind SR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Set Similarity for Dense Self-supervised Representation Learning. (arXiv:2107.08712v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.08712">
<div class="article-summary-box-inner">
<span><p>By considering the spatial correspondence, dense self-supervised
representation learning has achieved superior performance on various dense
prediction tasks. However, the pixel-level correspondence tends to be noisy
because of many similar misleading pixels, e.g., backgrounds. To address this
issue, in this paper, we propose to explore \textbf{set} \textbf{sim}ilarity
(SetSim) for dense self-supervised representation learning. We generalize
pixel-wise similarity learning to set-wise one to improve the robustness
because sets contain more semantic and structure information. Specifically, by
resorting to attentional features of views, we establish corresponding sets,
thus filtering out noisy backgrounds that may cause incorrect correspondences.
Meanwhile, these attentional features can keep the coherence of the same image
across different views to alleviate semantic inconsistency. We further search
the cross-view nearest neighbours of sets and employ the structured
neighbourhood information to enhance the robustness. Empirical evaluations
demonstrate that SetSim is superior to state-of-the-art methods on object
detection, keypoint detection, instance segmentation, and semantic
segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human Pose Estimation from Sparse Inertial Measurements through Recurrent Graph Convolution. (arXiv:2107.11214v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.11214">
<div class="article-summary-box-inner">
<span><p>We propose the adjacency adaptive graph convolutional long-short term memory
network (AAGC-LSTM) for human pose estimation from sparse inertial
measurements, obtained from only 6 measurement units. The AAGC-LSTM combines
both spatial and temporal dependency in a single network operation. This is
made possible by equipping graph convolutions with adjacency adaptivity, which
also allows for learning unknown dependencies of the human body joints. To
further boost accuracy, we propose longitudinal loss weighting to consider
natural movement patterns, as well as body-aware contralateral data
augmentation. By combining these contributions, we are able to utilize the
inherent graph nature of the human body, and can thus outperform the state of
the art for human pose estimation from sparse inertial measurements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Adversarial RetinaNet as a Reference Algorithm for the MItosis DOmain Generalization Challenge. (arXiv:2108.11269v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11269">
<div class="article-summary-box-inner">
<span><p>Assessing the Mitotic Count has a known high degree of intra- and inter-rater
variability. Computer-aided systems have proven to decrease this variability
and reduce labeling time. These systems, however, are generally highly
dependent on their training domain and show poor applicability to unseen
domains. In histopathology, these domain shifts can result from various
sources, including different slide scanning systems used to digitize histologic
samples. The MItosis DOmain Generalization challenge focused on this specific
domain shift for the task of mitotic figure detection. This work presents a
mitotic figure detection algorithm developed as a baseline for the challenge,
based on domain adversarial training. On the challenge's test set, the
algorithm scored an F$_1$ score of 0.7183. The corresponding network weights
and code for implementing the network are made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Application of Convolutional Neural Networks for Tomographic Reconstruction of Hyperspectral Images. (arXiv:2108.13458v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13458">
<div class="article-summary-box-inner">
<span><p>A novel method, utilizing convolutional neural networks (CNNs), is proposed
to reconstruct hyperspectral cubes from computed tomography imaging
spectrometer (CTIS) images. Current reconstruction algorithms are usually
subject to long reconstruction times and mediocre precision in cases of a large
number of spectral channels. The constructed CNNs deliver higher precision and
shorter reconstruction time than a sparse expectation maximization algorithm.
In addition, the network can handle two different types of real-world images at
the same time -- specifically ColorChecker and carrot spectral images are
considered. This work paves the way toward real-time reconstruction of
hyperspectral cubes from CTIS images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Distribution Alignment via Adversarial Learning for Domain Adaptive Object Detection. (arXiv:2109.09033v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09033">
<div class="article-summary-box-inner">
<span><p>Unsupervised domain adaptive object detection aims to adapt a well-trained
detector from its original source domain with rich labeled data to a new target
domain with unlabeled data. Recently, mainstream approaches perform this task
through adversarial learning, yet still suffer from two limitations. First,
they mainly align marginal distribution by unsupervised cross-domain feature
matching, and ignore each feature's categorical and positional information that
can be exploited for conditional alignment; Second, they treat all classes as
equally important for transferring cross-domain knowledge and ignore that
different classes usually have different transferability. In this paper, we
propose a joint adaptive detection framework (JADF) to address the above
challenges. First, an end-to-end joint adversarial adaptation framework for
object detection is proposed, which aligns both marginal and conditional
distributions between domains without introducing any extra hyperparameter.
Next, to consider the transferability of each object class, a metric for
class-wise transferability assessment is proposed, which is incorporated into
the JADF objective for domain adaptation. Further, an extended study from
unsupervised domain adaptation (UDA) to unsupervised few-shot domain adaptation
(UFDA) is conducted, where only a few unlabeled training images are available
in unlabeled target domain. Extensive experiments validate that JADF is
effective in both the UDA and UFDA settings, achieving significant performance
gains over existing state-of-the-art cross-domain detection methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StereOBJ-1M: Large-scale Stereo Image Dataset for 6D Object Pose Estimation. (arXiv:2109.10115v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10115">
<div class="article-summary-box-inner">
<span><p>We present a large-scale stereo RGB image object pose estimation dataset
named the $\textbf{StereOBJ-1M}$ dataset. The dataset is designed to address
challenging cases such as object transparency, translucency, and specular
reflection, in addition to the common challenges of occlusion, symmetry, and
variations in illumination and environments. In order to collect data of
sufficient scale for modern deep learning models, we propose a novel method for
efficiently annotating pose data in a multi-view fashion that allows data
capturing in complex and flexible environments. Fully annotated with 6D object
poses, our dataset contains over 393K frames and over 1.5M annotations of 18
objects recorded in 182 scenes constructed in 11 different environments. The 18
objects include 8 symmetric objects, 7 transparent objects, and 8 reflective
objects. We benchmark two state-of-the-art pose estimation frameworks on
StereOBJ-1M as baselines for future work. We also propose a novel object-level
pose optimization method for computing 6D pose from keypoint predictions in
multiple images. Project website: https://sites.google.com/view/stereobj-1m.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DAMix: A Density-Aware Mixup Augmentation for Single Image Dehazing under Domain Shift. (arXiv:2109.12544v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12544">
<div class="article-summary-box-inner">
<span><p>Deep learning-based methods have achieved considerable success on single
image dehazing in recent years. However, these methods are often subject to
performance degradation when domain shifts are confronted. Specifically, haze
density gaps exist among the existing datasets, often resulting in poor
performance when these methods are tested across datasets. To address this
issue, we propose a density-aware mixup augmentation (DAMix). DAMix generates
samples in an attempt to minimize the Wasserstein distance with the hazy images
in the target domain. These DAMix-ed samples not only mitigate domain gaps but
are also proven to comply with the atmospheric scattering model. Thus, DAMix
achieves comprehensive improvements on domain adaptation. Furthermore, we show
that DAMix is helpful with respect to data efficiency. Specifically, a network
trained with half of the source dataset using DAMix can achieve even better
adaptivity than that trained with the whole source dataset but without DAMix.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">All-Around Real Label Supervision: Cyclic Prototype Consistency Learning for Semi-supervised Medical Image Segmentation. (arXiv:2109.13930v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13930">
<div class="article-summary-box-inner">
<span><p>Semi-supervised learning has substantially advanced medical image
segmentation since it alleviates the heavy burden of acquiring the costly
expert-examined annotations. Especially, the consistency-based approaches have
attracted more attention for their superior performance, wherein the real
labels are only utilized to supervise their paired images via supervised loss
while the unlabeled images are exploited by enforcing the perturbation-based
\textit{"unsupervised"} consistency without explicit guidance from those real
labels. However, intuitively, the expert-examined real labels contain more
reliable supervision signals. Observing this, we ask an unexplored but
interesting question: can we exploit the unlabeled data via explicit real label
supervision for semi-supervised training? To this end, we discard the previous
perturbation-based consistency but absorb the essence of non-parametric
prototype learning. Based on the prototypical network, we then propose a novel
cyclic prototype consistency learning (CPCL) framework, which is constructed by
a labeled-to-unlabeled (L2U) prototypical forward process and an
unlabeled-to-labeled (U2L) backward process. Such two processes synergistically
enhance the segmentation network by encouraging more discriminative and compact
features. In this way, our framework turns previous \textit{"unsupervised"}
consistency into new \textit{"supervised"} consistency, obtaining the
\textit{"all-around real label supervision"} property of our method. Extensive
experiments on brain tumor segmentation from MRI and kidney segmentation from
CT images show that our CPCL can effectively exploit the unlabeled data and
outperform other state-of-the-art semi-supervised medical image segmentation
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Trivial or impossible -- dichotomous data difficulty masks model differences (on ImageNet and beyond). (arXiv:2110.05922v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05922">
<div class="article-summary-box-inner">
<span><p>"The power of a generalization system follows directly from its biases"
(Mitchell 1980). Today, CNNs are incredibly powerful generalisation systems --
but to what degree have we understood how their inductive bias influences model
decisions? We here attempt to disentangle the various aspects that determine
how a model decides. In particular, we ask: what makes one model decide
differently from another? In a meticulously controlled setting, we find that
(1.) irrespective of the network architecture or objective (e.g.
self-supervised, semi-supervised, vision transformers, recurrent models) all
models end up with a similar decision boundary. (2.) To understand these
findings, we analysed model decisions on the ImageNet validation set from epoch
to epoch and image by image. We find that the ImageNet validation set, among
others, suffers from dichotomous data difficulty (DDD): For the range of
investigated models and their accuracies, it is dominated by 46.0% "trivial"
and 11.5% "impossible" images (beyond label errors). Only 42.5% of the images
could possibly be responsible for the differences between two models' decision
boundaries. (3.) Only removing the "impossible" and "trivial" images allows us
to see pronounced differences between models. (4.) Humans are highly accurate
at predicting which images are "trivial" and "impossible" for CNNs (81.4%).
This implies that in future comparisons of brains, machines and behaviour, much
may be gained from investigating the decisive role of images and the
distribution of their difficulties.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting the Certified Robustness of L-infinity Distance Nets. (arXiv:2110.06850v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06850">
<div class="article-summary-box-inner">
<span><p>Recently, Zhang et al. (2021) developed a new neural network architecture
based on $\ell_\infty$-distance functions, which naturally possesses certified
$\ell_\infty$ robustness by its construction. Despite the novel design and
theoretical foundation, so far the model only achieved comparable performance
to conventional networks. In this paper, we make the following two
contributions: $\mathrm{(i)}$ We demonstrate that $\ell_\infty$-distance nets
enjoy a fundamental advantage in certified robustness over conventional
networks (under typical certification approaches); $\mathrm{(ii)}$ With an
improved training process we are able to significantly boost the certified
accuracy of $\ell_\infty$-distance nets. Our training approach largely
alleviates the optimization problem that arose in the previous training scheme,
in particular, the unexpected large Lipschitz constant due to the use of a
crucial trick called $\ell_p$-relaxation. The core of our training approach is
a novel objective function that combines scaled cross-entropy loss and clipped
hinge loss with a decaying mixing coefficient. Experiments show that using the
proposed training strategy, the certified accuracy of $\ell_\infty$-distance
net can be dramatically improved from 33.30% to 40.06% on CIFAR-10
($\epsilon=8/255$), meanwhile outperforming other approaches in this area by a
large margin. Our results clearly demonstrate the effectiveness and potential
of $\ell_\infty$-distance net for certified robustness. Codes are available at
https://github.com/zbh2047/L_inf-dist-net-v2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantically Distributed Robust Optimization for Vision-and-Language Inference. (arXiv:2110.07165v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07165">
<div class="article-summary-box-inner">
<span><p>Analysis of vision-and-language models has revealed their brittleness under
linguistic phenomena such as paraphrasing, negation, textual entailment, and
word substitutions with synonyms or antonyms. While data augmentation
techniques have been designed to mitigate against these failure modes, methods
that can integrate this knowledge into the training pipeline remain
under-explored. In this paper, we present \textbf{SDRO}, a model-agnostic
method that utilizes a set linguistic transformations in a distributed robust
optimization setting, along with an ensembling technique to leverage these
transformations during inference. Experiments on benchmark datasets with images
(NLVR$^2$) and video (VIOLIN) demonstrate performance improvements as well as
robustness to adversarial attacks. Experiments on binary VQA explore the
generalizability of this method to other V\&amp;L tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Dubber: Dubbing for Videos According to Scripts. (arXiv:2110.08243v3 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08243">
<div class="article-summary-box-inner">
<span><p>Dubbing is a post-production process of re-recording actors' dialogues, which
is extensively used in filmmaking and video production. It is usually performed
manually by professional voice actors who read lines with proper prosody, and
in synchronization with the pre-recorded videos. In this work, we propose
Neural Dubber, the first neural network model to solve a novel automatic video
dubbing (AVD) task: synthesizing human speech synchronized with the given video
from the text. Neural Dubber is a multi-modal text-to-speech (TTS) model that
utilizes the lip movement in the video to control the prosody of the generated
speech. Furthermore, an image-based speaker embedding (ISE) module is developed
for the multi-speaker setting, which enables Neural Dubber to generate speech
with a reasonable timbre according to the speaker's face. Experiments on the
chemistry lecture single-speaker dataset and LRS2 multi-speaker dataset show
that Neural Dubber can generate speech audios on par with state-of-the-art TTS
models in terms of speech quality. Most importantly, both qualitative and
quantitative evaluations show that Neural Dubber can control the prosody of
synthesized speech by the video, and generate high-fidelity speech temporally
synchronized with the video. Our project page is at
https://tsinghua-mars-lab.github.io/NeuralDubber/ .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models. (arXiv:2110.08484v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08484">
<div class="article-summary-box-inner">
<span><p>Large pre-trained vision-language (VL) models can learn a new task with a
handful of examples and generalize to a new task without fine-tuning. However,
these VL models are hard to deploy for real-world applications due to their
impractically huge sizes and slow inference speed. To solve this limitation, we
study prompt-based low-resource learning of VL tasks with our proposed method,
FewVLM, relatively smaller than recent few-shot learners. For FewVLM, we
pre-train a sequence-to-sequence transformer model with prefix language
modeling (PrefixLM) and masked language modeling (MaskedLM). Furthermore, we
analyze the effect of diverse prompts for few-shot tasks. Experimental results
on VQA show that FewVLM with prompt-based learning outperforms Frozen which is
31x larger than FewVLM by 18.2% point and achieves comparable results to a 246x
larger model, PICa. In our analysis, we observe that (1) prompts significantly
affect zero-shot performance but marginally affect few-shot performance, (2)
models with noisy prompts learn as quickly as hand-crafted prompts given larger
training data, and (3) MaskedLM helps VQA tasks while PrefixLM boosts
captioning performance. Our code is publicly available at
\url{https://github.com/woojeongjin/FewVLM}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Regularization Method to Improve Adversarial Robustness of Neural Networks for ECG Signal Classification. (arXiv:2110.09759v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09759">
<div class="article-summary-box-inner">
<span><p>Electrocardiogram (ECG) is the most widely used diagnostic tool to monitor
the condition of the human heart. By using deep neural networks (DNNs),
interpretation of ECG signals can be fully automated for the identification of
potential abnormalities in a patient's heart in a fraction of a second. Studies
have shown that given a sufficiently large amount of training data, DNN
accuracy for ECG classification could reach human-expert cardiologist level.
However, despite of the excellent performance in classification accuracy, DNNs
are highly vulnerable to adversarial noises that are subtle changes in the
input of a DNN and may lead to a wrong class-label prediction. It is
challenging and essential to improve robustness of DNNs against adversarial
noises, which are a threat to life-critical applications. In this work, we
proposed a regularization method to improve DNN robustness from the perspective
of noise-to-signal ratio (NSR) for the application of ECG signal
classification. We evaluated our method on PhysioNet MIT-BIH dataset and
CPSC2018 ECG dataset, and the results show that our method can substantially
enhance DNN robustness against adversarial noises generated from adversarial
attacks, with a minimal change in accuracy on clean data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Illiterate DALL-E Learns to Compose. (arXiv:2110.11405v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.11405">
<div class="article-summary-box-inner">
<span><p>Although DALL-E has shown an impressive ability of composition-based
systematic generalization in image generation, it requires the dataset of
text-image pairs and the compositionality is provided by the text. In contrast,
object-centric representation models like the Slot Attention model learn
composable representations without the text prompt. However, unlike DALL-E its
ability to systematically generalize for zero-shot generation is significantly
limited. In this paper, we propose a simple but novel slot-based autoencoding
architecture, called SLATE, for combining the best of both worlds: learning
object-centric representations that allows systematic generalization in
zero-shot image generation without text. As such, this model can also be seen
as an illiterate DALL-E model. Unlike the pixel-mixture decoders of existing
object-centric representation models, we propose to use the Image GPT decoder
conditioned on the slots for capturing complex interactions among the slots and
pixels. In experiments, we show that this simple and easy-to-implement
architecture not requiring a text prompt achieves significant improvement in
in-distribution and out-of-distribution (zero-shot) image generation and
qualitatively comparable or better slot-attention structure than the models
based on mixture decoders.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SILT: Self-supervised Lighting Transfer Using Implicit Image Decomposition. (arXiv:2110.12914v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12914">
<div class="article-summary-box-inner">
<span><p>We present SILT, a Self-supervised Implicit Lighting Transfer method. Unlike
previous research on scene relighting, we do not seek to apply arbitrary new
lighting configurations to a given scene. Instead, we wish to transfer the
lighting style from a database of other scenes, to provide a uniform lighting
style regardless of the input. The solution operates as a two-branch network
that first aims to map input images of any arbitrary lighting style to a
unified domain, with extra guidance achieved through implicit image
decomposition. We then remap this unified input domain using a discriminator
that is presented with the generated outputs and the style reference, i.e.
images of the desired illumination conditions. Our method is shown to
outperform supervised relighting solutions across two different datasets
without requiring lighting supervision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Equivariant Contrastive Learning. (arXiv:2111.00899v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00899">
<div class="article-summary-box-inner">
<span><p>In state-of-the-art self-supervised learning (SSL) pre-training produces
semantically good representations by encouraging them to be invariant under
meaningful transformations prescribed from human knowledge. In fact, the
property of invariance is a trivial instance of a broader class called
equivariance, which can be intuitively understood as the property that
representations transform according to the way the inputs transform. Here, we
show that rather than using only invariance, pre-training that encourages
non-trivial equivariance to some transformations, while maintaining invariance
to other transformations, can be used to improve the semantic quality of
representations. Specifically, we extend popular SSL methods to a more general
framework which we name Equivariant Self-Supervised Learning (E-SSL). In E-SSL,
a simple additional pre-training objective encourages equivariance by
predicting the transformations applied to the input. We demonstrate E-SSL's
effectiveness empirically on several popular computer vision benchmarks, e.g.
improving SimCLR to 72.5% linear probe accuracy on ImageNet. Furthermore, we
demonstrate usefulness of E-SSL for applications beyond computer vision; in
particular, we show its utility on regression problems in photonics science.
Our code, datasets and pre-trained models are available at
https://github.com/rdangovs/essl to aid further research in E-SSL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Semantic Segmentation of the Lumbar Spine: Clinical Applicability in a Multi-parametric and Multi-centre Study on Magnetic Resonance Images. (arXiv:2111.08712v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.08712">
<div class="article-summary-box-inner">
<span><p>One of the major difficulties in medical image segmentation is the high
variability of these images, which is caused by their origin (multi-centre),
the acquisition protocols (multi-parametric), as well as the variability of
human anatomy, the severity of the illness, the effect of age and gender, among
others. The problem addressed in this work is the automatic semantic
segmentation of lumbar spine Magnetic Resonance images using convolutional
neural networks. The purpose is to assign a class label to each pixel of an
image. Classes were defined by radiologists and correspond to different
structural elements like vertebrae, intervertebral discs, nerves, blood
vessels, and other tissues. The proposed network topologies are variants of the
U-Net architecture. Several complementary blocks were used to define the
variants: Three types of convolutional blocks, spatial attention models, deep
supervision and multilevel feature extractor. This document describes the
topologies and analyses the results of the neural network designs that obtained
the most accurate segmentations. Several of the proposed designs outperform the
standard U-Net used as baseline, especially when used in ensembles where the
output of multiple neural networks is combined according to different
strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RAANet: Range-Aware Attention Network for LiDAR-based 3D Object Detection with Auxiliary Density Level Estimation. (arXiv:2111.09515v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.09515">
<div class="article-summary-box-inner">
<span><p>3D object detection from LiDAR data for autonomous driving has been making
remarkable strides in recent years. Among the state-of-the-art methodologies,
encoding point clouds into a bird's-eye view (BEV) has been demonstrated to be
both effective and efficient. Different from perspective views, BEV preserves
rich spatial and distance information between objects; and while farther
objects of the same type do not appear smaller in the BEV, they contain sparser
point cloud features. This fact weakens BEV feature extraction using
shared-weight convolutional neural networks. In order to address this
challenge, we propose Range-Aware Attention Network (RAANet), which extracts
more powerful BEV features and generates superior 3D object detections. The
range-aware attention (RAA) convolutions significantly improve feature
extraction for near as well as far objects. Moreover, we propose a novel
auxiliary loss for density estimation to further enhance the detection accuracy
of RAANet for occluded objects. It is worth to note that our proposed RAA
convolution is lightweight and compatible to be integrated into any CNN
architecture used for the BEV detection. Extensive experiments on the nuScenes
dataset demonstrate that our proposed approach outperforms the state-of-the-art
methods for LiDAR-based 3D object detection, with real-time inference speed of
16 Hz for the full version and 22 Hz for the lite version. The code is publicly
available at an anonymous Github repository
https://github.com/anonymous0522/RAAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MUM : Mix Image Tiles and UnMix Feature Tiles for Semi-Supervised Object Detection. (arXiv:2111.10958v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10958">
<div class="article-summary-box-inner">
<span><p>Many recent semi-supervised learning (SSL) studies build teacher-student
architecture and train the student network by the generated supervisory signal
from the teacher. Data augmentation strategy plays a significant role in the
SSL framework since it is hard to create a weak-strong augmented input pair
without losing label information. Especially when extending SSL to
semi-supervised object detection (SSOD), many strong augmentation methodologies
related to image geometry and interpolation-regularization are hard to utilize
since they possibly hurt the location information of the bounding box in the
object detection task. To address this, we introduce a simple yet effective
data augmentation method, Mix/UnMix (MUM), which unmixes feature tiles for the
mixed image tiles for the SSOD framework. Our proposed method makes mixed input
image tiles and reconstructs them in the feature space. Thus, MUM can enjoy the
interpolation-regularization effect from non-interpolated pseudo-labels and
successfully generate a meaningful weak-strong pair. Furthermore, MUM can be
easily equipped on top of various SSOD methods. Extensive experiments on
MS-COCO and PASCAL VOC datasets demonstrate the superiority of MUM by
consistently improving the mAP performance over the baseline in all the tested
SSOD benchmark protocols.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PointMixer: MLP-Mixer for Point Cloud Understanding. (arXiv:2111.11187v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11187">
<div class="article-summary-box-inner">
<span><p>MLP-Mixer has newly appeared as a new challenger against the realm of CNNs
and transformer. Despite its simplicity compared to transformer, the concept of
channel-mixing MLPs and token-mixing MLPs achieves noticeable performance in
visual recognition tasks. Unlike images, point clouds are inherently sparse,
unordered and irregular, which limits the direct use of MLP-Mixer for point
cloud understanding. In this paper, we propose PointMixer, a universal point
set operator that facilitates information sharing among unstructured 3D points.
By simply replacing token-mixing MLPs with a softmax function, PointMixer can
"mix" features within/between point sets. By doing so, PointMixer can be
broadly used in the network as inter-set mixing, intra-set mixing, and pyramid
mixing. Extensive experiments show the competitive or superior performance of
PointMixer in semantic segmentation, classification, and point reconstruction
against transformer-based methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Point Cloud Reconstruction. (arXiv:2111.11704v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11704">
<div class="article-summary-box-inner">
<span><p>Point cloud obtained from 3D scanning is often sparse, noisy, and irregular.
To cope with these issues, recent studies have been separately conducted to
densify, denoise, and complete inaccurate point cloud. In this paper, we
advocate that jointly solving these tasks leads to significant improvement for
point cloud reconstruction. To this end, we propose a deep point cloud
reconstruction network consisting of two stages: 1) a 3D sparse
stacked-hourglass network as for the initial densification and denoising, 2) a
refinement via transformers converting the discrete voxels into 3D points. In
particular, we further improve the performance of transformer by a newly
proposed module called amplified positional encoding. This module has been
designed to differently amplify the magnitude of positional encoding vectors
based on the points' distances for adaptive refinements. Extensive experiments
demonstrate that our network achieves state-of-the-art performance among the
recent studies in the ScanNet, ICL-NUIM, and ShapeNetPart datasets. Moreover,
we underline the ability of our network to generalize toward real-world and
unmet scenes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conditional Object-Centric Learning from Video. (arXiv:2111.12594v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12594">
<div class="article-summary-box-inner">
<span><p>Object-centric representations are a promising path toward more systematic
generalization by providing flexible abstractions upon which compositional
world models can be built. Recent work on simple 2D and 3D datasets has shown
that models with object-centric inductive biases can learn to segment and
represent meaningful objects from the statistical structure of the data alone
without the need for any supervision. However, such fully-unsupervised methods
still fail to scale to diverse realistic data, despite the use of increasingly
complex inductive biases such as priors for the size of objects or the 3D
geometry of the scene. In this paper, we instead take a weakly-supervised
approach and focus on how 1) using the temporal dynamics of video data in the
form of optical flow and 2) conditioning the model on simple object location
cues can be used to enable segmenting and tracking objects in significantly
more realistic synthetic data. We introduce a sequential extension to Slot
Attention which we train to predict optical flow for realistic looking
synthetic scenes and show that conditioning the initial state of this model on
a small set of hints, such as center of mass of objects in the first frame, is
sufficient to significantly improve instance segmentation. These benefits
generalize beyond the training distribution to novel objects, novel
backgrounds, and to longer video sequences. We also find that such
initial-state-conditioning can be used during inference as a flexible interface
to query the model for specific objects or parts of objects, which could pave
the way for a range of weakly-supervised approaches and allow more effective
interaction with trained models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual Active Learning Using Pseudo-Domains for Limited Labelling Resources and Changing Acquisition Characteristics. (arXiv:2111.13069v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13069">
<div class="article-summary-box-inner">
<span><p>Machine learning in medical imaging during clinical routine is impaired by
changes in scanner protocols, hardware, or policies resulting in a
heterogeneous set of acquisition settings. When training a deep learning model
on an initial static training set, model performance and reliability suffer
from changes of acquisition characteristics as data and targets may become
inconsistent. Continual learning can help to adapt models to the changing
environment by training on a continuous data stream. However, continual manual
expert labelling of medical imaging requires substantial effort. Thus, ways to
use labelling resources efficiently on a well chosen sub-set of new examples is
necessary to render this strategy feasible.
</p>
<p>Here, we propose a method for continual active learning operating on a stream
of medical images in a multi-scanner setting. The approach automatically
recognizes shifts in image acquisition characteristics - new domains -, selects
optimal examples for labelling and adapts training accordingly. Labelling is
subject to a limited budget, resembling typical real world scenarios. To
demonstrate generalizability, we evaluate the effectiveness of our method on
three tasks: cardiac segmentation, lung nodule detection and brain age
estimation. Results show that the proposed approach outperforms other active
learning methods, while effectively counteracting catastrophic forgetting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Confounder Identification-free Causal Visual Feature Learning. (arXiv:2111.13420v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13420">
<div class="article-summary-box-inner">
<span><p>Confounders in deep learning are in general detrimental to model's
generalization where they infiltrate feature representations. Therefore,
learning causal features that are free of interference from confounders is
important. Most previous causal learning based approaches employ back-door
criterion to mitigate the adverse effect of certain specific confounder, which
require the explicit identification of confounder. However, in real scenarios,
confounders are typically diverse and difficult to be identified. In this
paper, we propose a novel Confounder Identification-free Causal Visual Feature
Learning (CICF) method, which obviates the need for identifying confounders.
CICF models the interventions among different samples based on front-door
criterion, and then approximates the global-scope intervening effect upon the
instance-level interventions from the perspective of optimization. In this way,
we aim to find a reliable optimization direction, which avoids the intervening
effects of confounders, to learn causal features. Furthermore, we uncover the
relation between CICF and the popular meta-learning strategy MAML, and provide
an interpretation of why MAML works from the theoretical perspective of causal
learning for the first time. Thanks to the effective learning of causal
features, our CICF enables models to have superior generalization capability.
Extensive experiments on domain generalization benchmark datasets demonstrate
the effectiveness of our CICF, which achieves the state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CDGNet: Class Distribution Guided Network for Human Parsing. (arXiv:2111.14173v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14173">
<div class="article-summary-box-inner">
<span><p>The objective of human parsing is to partition a human in an image into
constituent parts. This task involves labeling each pixel of the human image
according to the classes. Since the human body comprises hierarchically
structured parts, each body part of an image can have its sole position
distribution characteristic. Probably, a human head is less likely to be under
the feet, and arms are more likely to be near the torso. Inspired by this
observation, we make instance class distributions by accumulating the original
human parsing label in the horizontal and vertical directions, which can be
utilized as supervision signals. Using these horizontal and vertical class
distribution labels, the network is guided to exploit the intrinsic position
distribution of each class. We combine two guided features to form a spatial
guidance map, which is then superimposed onto the baseline network by
multiplication and concatenation to distinguish the human parts precisely. We
conducted extensive experiments to demonstrate the effectiveness and
superiority of our method on three well-known benchmarks: LIP, ATR, and CIHP
databases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MeshUDF: Fast and Differentiable Meshing of Unsigned Distance Field Networks. (arXiv:2111.14549v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14549">
<div class="article-summary-box-inner">
<span><p>Unsigned Distance Fields (UDFs) can be used to represent non-watertight
surfaces. However, current approaches to converting them into explicit meshes
tend to either be expensive or to degrade the accuracy. Here, we extend the
marching cube algorithm to handle UDFs, both fast and accurately. Moreover, our
approach to surface extraction is differentiable, which is key to using
pretrained UDF networks to fit sparse data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CRIS: CLIP-Driven Referring Image Segmentation. (arXiv:2111.15174v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.15174">
<div class="article-summary-box-inner">
<span><p>Referring image segmentation aims to segment a referent via a natural
linguistic expression.Due to the distinct data properties between text and
image, it is challenging for a network to well align text and pixel-level
features. Existing approaches use pretrained models to facilitate learning, yet
separately transfer the language/vision knowledge from pretrained models,
ignoring the multi-modal corresponding information. Inspired by the recent
advance in Contrastive Language-Image Pretraining (CLIP), in this paper, we
propose an end-to-end CLIP-Driven Referring Image Segmentation framework
(CRIS). To transfer the multi-modal knowledge effectively, CRIS resorts to
vision-language decoding and contrastive learning for achieving the
text-to-pixel alignment. More specifically, we design a vision-language decoder
to propagate fine-grained semantic information from textual representations to
each pixel-level activation, which promotes consistency between the two
modalities. In addition, we present text-to-pixel contrastive learning to
explicitly enforce the text feature similar to the related pixel-level features
and dissimilar to the irrelevances. The experimental results on three benchmark
datasets demonstrate that our proposed framework significantly outperforms the
state-of-the-art performance without any post-processing. The code will be
released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Auxiliary Learning for Self-Supervised Video Representation via Similarity-based Knowledge Distillation. (arXiv:2112.04011v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04011">
<div class="article-summary-box-inner">
<span><p>Despite the outstanding success of self-supervised pretraining methods for
video representation learning, they generalise poorly when the unlabeled
dataset for pretraining is small or the domain difference between unlabelled
data in source task (pretraining) and labeled data in target task (finetuning)
is significant. To mitigate these issues, we propose a novel approach to
complement self-supervised pretraining via an auxiliary pretraining phase,
based on knowledge similarity distillation, auxSKD, for better generalisation
with a significantly smaller amount of video data, e.g. Kinetics-100 rather
than Kinetics-400. Our method deploys a teacher network that iteratively
distills its knowledge to the student model by capturing the similarity
information between segments of unlabelled video data. The student model
meanwhile solves a pretext task by exploiting this prior knowledge. We also
introduce a novel pretext task, Video Segment Pace Prediction or VSPP, which
requires our model to predict the playback speed of a randomly selected segment
of the input video to provide more reliable self-supervised representations.
Our experimental results show superior results to the state of the art on both
UCF101 and HMDB51 datasets when pretraining on K100 in apple-to-apple
comparisons. Additionally, we show that our auxiliary pretraining, auxSKD, when
added as an extra pretraining phase to recent state of the art self-supervised
methods (i.e. VCOP, VideoPace, and RSPNet), improves their results on UCF101
and HMDB51. Our code is available at https://github.com/Plrbear/auxSKD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Responsive Listening Head Generation: A Benchmark Dataset and Baseline. (arXiv:2112.13548v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13548">
<div class="article-summary-box-inner">
<span><p>We present a new listening head generation benchmark, for synthesizing
responsive feedbacks of a listener (e.g., nod, smile) during a face-to-face
conversation. As the indispensable complement to talking heads generation,
listening head generation has seldomly been studied in literature.
Automatically synthesizing listening behavior that actively responds to a
talking head, is critical to applications such as digital human, virtual agents
and social robots. In this work, we propose a novel dataset "ViCo",
highlighting the listening head generation during a face-to-face conversation.
A total number of 92 identities (67 speakers and 76 listeners) are involved in
ViCo, featuring 483 clips in a paired "speaking-listening" pattern, where
listeners show three listening styles based on their attitudes: positive,
neutral, negative. Different from traditional speech-to-gesture or talking-head
generation, listening head generation takes as input both the audio and visual
signals from the speaker, and gives non-verbal feedbacks (e.g., head motions,
facial expressions) in a real-time manner. Our dataset supports a wide range of
applications such as human-to-human interaction, video-to-video translation,
cross-modal understanding and generation. To encourage further research, we
also release a listening head generation baseline, conditioning on different
listening attitudes. Project page: \url{https://project.mhzhou.com/rld}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Transformer-Based Siamese Network for Change Detection. (arXiv:2201.01293v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01293">
<div class="article-summary-box-inner">
<span><p>This paper presents a transformer-based Siamese network architecture
(abbreviated by ChangeFormer) for Change Detection (CD) from a pair of
co-registered remote sensing images. Different from recent CD frameworks, which
are based on fully convolutional networks (ConvNets), the proposed method
unifies hierarchically structured transformer encoder with Multi-Layer
Perception (MLP) decoder in a Siamese network architecture to efficiently
render multi-scale long-range details required for accurate CD. Experiments on
two CD datasets show that the proposed end-to-end trainable ChangeFormer
architecture achieves better CD performance than previous counterparts. Our
code is available at https://github.com/wgcban/ChangeFormer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attention-based Dual Supervised Decoder for RGBD Semantic Segmentation. (arXiv:2201.01427v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01427">
<div class="article-summary-box-inner">
<span><p>Encoder-decoder models have been widely used in RGBD semantic segmentation,
and most of them are designed via a two-stream network. In general, jointly
reasoning the color and geometric information from RGBD is beneficial for
semantic segmentation. However, most existing approaches fail to
comprehensively utilize multimodal information in both the encoder and decoder.
In this paper, we propose a novel attention-based dual supervised decoder for
RGBD semantic segmentation. In the encoder, we design a simple yet effective
attention-based multimodal fusion module to extract and fuse deeply multi-level
paired complementary information. To learn more robust deep representations and
rich multi-modal information, we introduce a dual-branch decoder to effectively
leverage the correlations and complementary cues of different tasks. Extensive
experiments on NYUDv2 and SUN-RGBD datasets demonstrate that our method
achieves superior performance against the state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prototype Guided Network for Anomaly Segmentation. (arXiv:2201.05869v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05869">
<div class="article-summary-box-inner">
<span><p>Semantic segmentation methods can not directly identify abnormal objects in
images. Anomaly Segmentation algorithm from this realistic setting can
distinguish between in-distribution objects and Out-Of-Distribution (OOD)
objects and output the anomaly probability for pixels. In this paper, a
Prototype Guided Anomaly segmentation Network (PGAN) is proposed to extract
semantic prototypes for in-distribution training data from limited annotated
images. In the model, prototypes are used to model the hierarchical category
semantic information and distinguish OOD pixels. The proposed PGAN model
includes a semantic segmentation network and a prototype extraction network.
Similarity measures are adopted to optimize the prototypes. The learned
semantic prototypes are used as category semantics to compare the similarity
with features extracted from test images and then to generate semantic
segmentation prediction. The proposed prototype extraction network can also be
integrated into most semantic segmentation networks and recognize OOD pixels.
On the StreetHazards dataset, the proposed PGAN model produced mIoU of 53.4%
for anomaly segmentation. The experimental results demonstrate PGAN may achieve
the SOTA performance in the anomaly segmentation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Object Counting with Similarity-Aware Feature Enhancement. (arXiv:2201.08959v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08959">
<div class="article-summary-box-inner">
<span><p>This work studies the problem of few-shot object counting, which counts the
number of exemplar objects (i.e., described by one or several support images)
occurring in the query image. The major challenge lies in that the target
objects can be densely packed in the query image, making it hard to recognize
every single one. To tackle the obstacle, we propose a novel learning block,
equipped with a similarity comparison module (SCM) and a feature enhancement
module (FEM). Concretely, given a support image and a query image, we first
derive a score map by comparing their projected features at every spatial
position. The score maps regarding all support images are collected together
and normalized across both the exemplar dimension and the spatial dimensions,
producing a reliable similarity map. We then enhance the query feature with the
support features by employing the developed point-wise similarities as the
weighting coefficients. Such a design encourages the model to inspect the query
image by focusing more on the regions akin to the support images, leading to
much clearer boundaries between different objects. Extensive experiments on
various benchmarks and training setups suggest that our method surpasses the
state-of-the-art approaches by a sufficiently large margin. For instance, on
the very recent large-scale FSC-147 dataset, we beat the second competitor by
improving the mean absolute counting error from 22.08 to 14.32 (35%
$\uparrow$).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparative study of 3D object detection frameworks based on LiDAR data and sensor fusion techniques. (arXiv:2202.02521v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02521">
<div class="article-summary-box-inner">
<span><p>Estimating and understanding the surroundings of the vehicle precisely forms
the basic and crucial step for the autonomous vehicle. The perception system
plays a significant role in providing an accurate interpretation of a vehicle's
environment in real-time. Generally, the perception system involves various
subsystems such as localization, obstacle (static and dynamic) detection, and
avoidance, mapping systems, and others. For perceiving the environment, these
vehicles will be equipped with various exteroceptive (both passive and active)
sensors in particular cameras, Radars, LiDARs, and others. These systems are
equipped with deep learning techniques that transform the huge amount of data
from the sensors into semantic information on which the object detection and
localization tasks are performed. For numerous driving tasks, to provide
accurate results, the location and depth information of a particular object is
necessary. 3D object detection methods, by utilizing the additional pose data
from the sensors such as LiDARs, stereo cameras, provides information on the
size and location of the object. Based on recent research, 3D object detection
frameworks performing object detection and localization on LiDAR data and
sensor fusion techniques show significant improvement in their performance. In
this work, a comparative study of the effect of using LiDAR data for object
detection frameworks and the performance improvement seen by using sensor
fusion techniques are performed. Along with discussing various state-of-the-art
methods in both the cases, performing experimental analysis, and providing
future research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-modal unsupervised brain image registration using edge maps. (arXiv:2202.04647v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04647">
<div class="article-summary-box-inner">
<span><p>Diffeomorphic deformable multi-modal image registration is a challenging task
which aims to bring images acquired by different modalities to the same
coordinate space and at the same time to preserve the topology and the
invertibility of the transformation. Recent research has focused on leveraging
deep learning approaches for this task as these have been shown to achieve
competitive registration accuracy while being computationally more efficient
than traditional iterative registration methods. In this work, we propose a
simple yet effective unsupervised deep learning-based {\em multi-modal} image
registration approach that benefits from auxiliary information coming from the
gradient magnitude of the image, i.e. the image edges, during the training. The
intuition behind this is that image locations with a strong gradient are
assumed to denote a transition of tissues, which are locations of high
information value able to act as a geometry constraint. The task is similar to
using segmentation maps to drive the training, but the edge maps are easier and
faster to acquire and do not require annotations. We evaluate our approach in
the context of registering multi-modal (T1w to T2w) magnetic resonance (MR)
brain images of different subjects using three different loss functions that
are said to assist multi-modal registration, showing that in all cases the
auxiliary information leads to better results without compromising the runtime.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Point Cloud Denoising via Momentum Ascent in Gradient Fields. (arXiv:2202.10094v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.10094">
<div class="article-summary-box-inner">
<span><p>To achieve point cloud denoising, traditional methods heavily rely on
geometric priors, and most learning-based approaches suffer from outliers and
loss of details. Recently, the gradient-based method was proposed to estimate
the gradient fields from the noisy point clouds using neural networks, and
refine the position of each point according to the estimated gradient. However,
the predicted gradient could fluctuate, leading to perturbed and unstable
solutions, as well as a large inference time. To address these issues, we
develop the momentum gradient ascent method that leverages the information of
previous iterations in determining the trajectories of the points, thus
improving the stability of the solution and reducing the inference time.
Experiments demonstrate that the proposed method outperforms state-of-the-art
methods with a variety of point clouds and noise levels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking the Zigzag Flattening for Image Reading. (arXiv:2202.10240v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.10240">
<div class="article-summary-box-inner">
<span><p>Sequence ordering of word vector matters a lot to text reading, which has
been proven in natural language processing (NLP). However, the rule of
different sequence ordering in computer vision (CV) was not well explored,
e.g., why the "zigzag" flattening (ZF) is commonly utilized as a default option
to get the image patches ordering in vision transformers (ViTs). Notably, when
decomposing multi-scale images, the ZF could not maintain the invariance of
feature point positions. To this end, we investigate the Hilbert fractal
flattening (HF) as another method for sequence ordering in CV and contrast it
against ZF. The HF has proven to be superior to other curves in maintaining
spatial locality, when performing multi-scale transformations of dimensional
space. And it can be easily plugged into most deep neural networks (DNNs).
Extensive experiments demonstrate that it can yield consistent and significant
performance boosts for a variety of architectures. Finally, we hope that our
studies spark further research about the flattening strategy of image reading.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Estimation of Looming from LiDAR. (arXiv:2202.10972v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.10972">
<div class="article-summary-box-inner">
<span><p>Looming, traditionally defined as the relative expansion of objects in the
observer's retina, is a fundamental visual cue for perception of threat and can
be used to accomplish collision free navigation. The measurement of the looming
cue is not only limited to vision, and can also be obtained from range sensors
like LiDAR (Light Detection and Ranging). In this article we present two
methods that process raw LiDAR data to estimate the looming cue. Using looming
values we show how to obtain threat zones for collision avoidance tasks. The
methods are general enough to be suitable for any six-degree-of-freedom motion
and can be implemented in real-time without the need for fine matching,
point-cloud registration, object classification or object segmentation.
Quantitative results using the KITTI dataset shows advantages and limitations
of the methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Transferable Reward for Query Object Localization with Policy Adaptation. (arXiv:2202.12403v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12403">
<div class="article-summary-box-inner">
<span><p>We propose a reinforcement learning based approach to query object
localization, for which an agent is trained to localize objects of interest
specified by a small exemplary set. We learn a transferable reward signal
formulated using the exemplary set by ordinal metric learning. Our proposed
method enables test-time policy adaptation to new environments where the reward
signals are not readily available, and outperforms fine-tuning approaches that
are limited to annotated images. In addition, the transferable reward allows
repurposing the trained agent from one specific class to another class.
Experiments on corrupted MNIST, CU-Birds, and COCO datasets demonstrate the
effectiveness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Supervision: Enabling Generalization over Output Spaces. (arXiv:2202.13100v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13100">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose Semantic Supervision (SemSup) - a unified paradigm
for training classifiers that generalize over output spaces. In contrast to
standard classification, which treats classes as discrete symbols, SemSup
represents them as dense vector features obtained from descriptions of classes
(e.g., "The cat is a small carnivorous mammal"). This allows the output space
to be unbounded (in the space of descriptions) and enables models to generalize
both over unseen inputs and unseen outputs (e.g. "The aardvark is a nocturnal
burrowing mammal with long ears"). Specifically, SemSup enables four types of
generalization, to -- (1) unseen class descriptions, (2) unseen classes, (3)
unseen super-classes, and (4) unseen tasks. Through experiments on four
classification datasets across two variants (multi-class and multi-label), two
input modalities (text and images), and two output description modalities (text
and JSON), we show that our SemSup models significantly outperform standard
supervised models and existing models that leverage word embeddings over class
names. For instance, our model outperforms baselines by 40% and 15% precision
points on unseen descriptions and classes, respectively, on a news
categorization dataset (RCV1). SemSup can serve as a pathway for scaling neural
models to large unbounded output spaces and enabling better generalization and
model reuse for unseen tasks and domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GlideNet: Global, Local and Intrinsic based Dense Embedding NETwork for Multi-category Attributes Prediction. (arXiv:2203.03079v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03079">
<div class="article-summary-box-inner">
<span><p>Attaching attributes (such as color, shape, state, action) to object
categories is an important computer vision problem. Attribute prediction has
seen exciting recent progress and is often formulated as a multi-label
classification problem. Yet significant challenges remain in: 1) predicting
diverse attributes over multiple categories, 2) modeling attributes-category
dependency, 3) capturing both global and local scene context, and 4) predicting
attributes of objects with low pixel-count. To address these issues, we propose
a novel multi-category attribute prediction deep architecture named GlideNet,
which contains three distinct feature extractors. A global feature extractor
recognizes what objects are present in a scene, whereas a local one focuses on
the area surrounding the object of interest. Meanwhile, an intrinsic feature
extractor uses an extension of standard convolution dubbed Informed Convolution
to retrieve features of objects with low pixel-count. GlideNet uses gating
mechanisms with binary masks and its self-learned category embedding to combine
the dense embeddings. Collectively, the Global-Local-Intrinsic blocks
comprehend the scene's global context while attending to the characteristics of
the local object of interest. Finally, using the combined features, an
interpreter predicts the attributes, and the length of the output is determined
by the category, thereby removing unnecessary attributes. GlideNet can achieve
compelling results on two recent and challenging datasets -- VAW and CAR -- for
large-scale attribute prediction. For instance, it obtains more than 5\% gain
over state of the art in the mean recall (mR) metric. GlideNet's advantages are
especially apparent when predicting attributes of objects with low pixel counts
as well as attributes that demand global context understanding. Finally, we
show that GlideNet excels in training starved real-world scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RC-MVSNet: Unsupervised Multi-View Stereo with Neural Rendering. (arXiv:2203.03949v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03949">
<div class="article-summary-box-inner">
<span><p>Finding accurate correspondences among different views is the Achilles' heel
of unsupervised Multi-View Stereo (MVS). Existing methods are built upon the
assumption that corresponding pixels share similar photometric features.
However, multi-view images in real scenarios observe non-Lambertian surfaces
and experience occlusions. In this work, we propose a novel approach with
neural rendering (RC-MVSNet) to solve such ambiguity issues of correspondences
among views. Specifically, we impose a depth rendering consistency loss to
constrain the geometry features close to the object surface to alleviate
occlusions. Concurrently, we introduce a reference view synthesis loss to
generate consistent supervision, even for non-Lambertian surfaces. Extensive
experiments on DTU and Tanks\&amp;Temples benchmarks demonstrate that our RC-MVSNet
approach achieves state-of-the-art performance over unsupervised MVS frameworks
and competitive performance to many supervised methods.The trained models and
code will be released at https://github.com/Boese0601/RC-MVSNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EdgeFormer: Improving Light-weight ConvNets by Learning from Vision Transformers. (arXiv:2203.03952v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03952">
<div class="article-summary-box-inner">
<span><p>Recently, vision transformers started to show impressive results which
outperform large convolution based models significantly. However, in the area
of small models for mobile or resource constrained devices, ConvNet still has
its own advantages in both performance and model complexity. We propose
EdgeFormer, a pure ConvNet based backbone model that further strengthens these
advantages by fusing the merits of vision transformers into ConvNets.
Specifically, we propose global circular convolution (GCC) with position
embeddings, a light-weight convolution op which boasts a global receptive field
while producing location sensitive features as in local convolutions. We
combine the GCCs and squeeze-exictation ops to form a meta-former like model
block, which further has the attention mechanism like transformers. The
aforementioned block can be used in plug-and-play manner to replace relevant
blocks in ConvNets or transformers. Experiment results show that the proposed
EdgeFormer achieves better performance than popular light-weight ConvNets and
vision transformer based models in common vision tasks and datasets, while
having fewer parameters and faster inference speed. For classification on
ImageNet-1k, EdgeFormer achieves 78.6% top-1 accuracy with about 5.0 million
parameters, saving 11% parameters and 13% computational cost but gaining 0.2%
higher accuracy and 23% faster inference speed (on ARM based Rockchip RK3288)
compared with MobileViT, and uses only 0.5 times parameters but gaining 2.7%
accuracy compared with DeIT. On MS-COCO object detection and PASCAL VOC
segmentation tasks, EdgeFormer also shows better performance. Code is available
at https://github.com/hkzhang91/EdgeFormer
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Dual-Output Diffusion Models. (arXiv:2203.04304v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04304">
<div class="article-summary-box-inner">
<span><p>Iterative denoising-based generation, also known as denoising diffusion
models, has recently been shown to be comparable in quality to other classes of
generative models, and even surpass them. Including, in particular, Generative
Adversarial Networks, which are currently the state of the art in many
sub-tasks of image generation. However, a major drawback of this method is that
it requires hundreds of iterations to produce a competitive result. Recent
works have proposed solutions that allow for faster generation with fewer
iterations, but the image quality gradually deteriorates with increasingly
fewer iterations being applied during generation. In this paper, we reveal some
of the causes that affect the generation quality of diffusion models,
especially when sampling with few iterations, and come up with a simple, yet
effective, solution to mitigate them. We consider two opposite equations for
the iterative denoising, the first predicts the applied noise, and the second
predicts the image directly. Our solution takes the two options and learns to
dynamically alternate between them through the denoising process. Our proposed
solution is general and can be applied to any existing diffusion model. As we
show, when applied to various SOTA architectures, our solution immediately
improves their generation quality, with negligible added complexity and
parameters. We experiment on multiple datasets and configurations and run an
extensive ablation study to support these findings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiscale Convolutional Transformer with Center Mask Pretraining for Hyperspectral Image Classificationtion. (arXiv:2203.04771v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04771">
<div class="article-summary-box-inner">
<span><p>Hyperspectral images (HSI) not only have a broad macroscopic field of view
but also contain rich spectral information, and the types of surface objects
can be identified through spectral information, which is one of the main
applications in hyperspectral image related research.In recent years, more and
more deep learning methods have been proposed, among which convolutional neural
networks (CNN) are the most influential. However, CNN-based methods are
difficult to capture long-range dependencies, and also require a large amount
of labeled data for model training.Besides, most of the self-supervised
training methods in the field of HSI classification are based on the
reconstruction of input samples, and it is difficult to achieve effective use
of unlabeled samples. To address the shortcomings of CNN networks, we propose a
noval multi-scale convolutional embedding module for HSI to realize effective
extraction of spatial-spectral information, which can be better combined with
Transformer network.In order to make more efficient use of unlabeled data, we
propose a new self-supervised pretask. Similar to Mask autoencoder, but our
pre-training method only masks the corresponding token of the central pixel in
the encoder, and inputs the remaining token into the decoder to reconstruct the
spectral information of the central pixel.Such a pretask can better model the
relationship between the central feature and the domain feature, and obtain
more stable training results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-supervision semantic segmentation with uncertainty-guided self cross supervision. (arXiv:2203.05118v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05118">
<div class="article-summary-box-inner">
<span><p>As a powerful way of realizing semi-supervised segmentation, the cross
supervision method learns cross consistency based on independent ensemble
models using abundant unlabeled images. However, the wrong pseudo labeling
information generated by cross supervision would confuse the training process
and negatively affect the effectiveness of the segmentation model. Besides, the
training process of ensemble models in such methods also multiplies the cost of
computation resources and decreases the training efficiency. To solve these
problems, we propose a novel cross supervision method, namely
uncertainty-guided self cross supervision (USCS). In addition to ensemble
models, we first design a multi-input multi-output (MIMO) segmentation model
which can generate multiple outputs with shared model and consequently impose
consistency over the outputs, saving the cost on parameters and calculations.
On the other hand, we employ uncertainty as guided information to encourage the
model to focus on the high confident regions of pseudo labels and mitigate the
effects of wrong pseudo labeling in self cross supervision, improving the
performance of the segmentation model. Extensive experiments show that our
method achieves state-of-the-art performance while saving 40.5% and 49.1% cost
on parameters and calculations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Frequency-driven Imperceptible Adversarial Attack on Semantic Similarity. (arXiv:2203.05151v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05151">
<div class="article-summary-box-inner">
<span><p>Current adversarial attack research reveals the vulnerability of
learning-based classifiers against carefully crafted perturbations. However,
most existing attack methods have inherent limitations in cross-dataset
generalization as they rely on a classification layer with a closed set of
categories. Furthermore, the perturbations generated by these methods may
appear in regions easily perceptible to the human visual system (HVS). To
circumvent the former problem, we propose a novel algorithm that attacks
semantic similarity on feature representations. In this way, we are able to
fool classifiers without limiting attacks to a specific dataset. For
imperceptibility, we introduce the low-frequency constraint to limit
perturbations within high-frequency components, ensuring perceptual similarity
between adversarial examples and originals. Extensive experiments on three
datasets (CIFAR-10, CIFAR-100, and ImageNet-1K) and three public online
platforms indicate that our attack can yield misleading and transferable
adversarial examples across architectures and datasets. Additionally,
visualization results and quantitative performance (in terms of four different
metrics) show that the proposed algorithm generates more imperceptible
perturbations than the state-of-the-art methods. Code is made available at.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Back to Reality: Weakly-supervised 3D Object Detection with Shape-guided Label Enhancement. (arXiv:2203.05238v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05238">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a weakly-supervised approach for 3D object
detection, which makes it possible to train strong 3D detector with
position-level annotations (i.e. annotations of object centers). In order to
remedy the information loss from box annotations to centers, our method, namely
Back to Reality (BR), makes use of synthetic 3D shapes to convert the weak
labels into fully-annotated virtual scenes as stronger supervision, and in turn
utilizes the perfect virtual labels to complement and refine the real labels.
Specifically, we first assemble 3D shapes into physically reasonable virtual
scenes according to the coarse scene layout extracted from position-level
annotations. Then we go back to reality by applying a virtual-to-real domain
adaptation method, which refine the weak labels and additionally supervise the
training of detector with the virtual scenes. Furthermore, we propose a more
challenging benckmark for indoor 3D object detection with more diversity in
object sizes to better show the potential of BR. With less than 5% of the
labeling labor, we achieve comparable detection performance with some popular
fully-supervised approaches on the widely used ScanNet dataset. Code is
available at: https://github.com/wyf-ACCEPT/BackToReality
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Generalization via Shuffled Style Assembly for Face Anti-Spoofing. (arXiv:2203.05340v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05340">
<div class="article-summary-box-inner">
<span><p>With diverse presentation attacks emerging continually, generalizable face
anti-spoofing (FAS) has drawn growing attention. Most existing methods
implement domain generalization (DG) on the complete representations. However,
different image statistics may have unique properties for the FAS tasks. In
this work, we separate the complete representation into content and style ones.
A novel Shuffled Style Assembly Network (SSAN) is proposed to extract and
reassemble different content and style features for a stylized feature space.
Then, to obtain a generalized representation, a contrastive learning strategy
is developed to emphasize liveness-related style information while suppress the
domain-specific one. Finally, the representations of the correct assemblies are
used to distinguish between living and spoofing during the inferring. On the
other hand, despite the decent performance, there still exists a gap between
academia and industry, due to the difference in data quantity and distribution.
Thus, a new large-scale benchmark for FAS is built up to further evaluate the
performance of algorithms in reality. Both qualitative and quantitative results
on existing and proposed benchmarks demonstrate the effectiveness of our
methods. The codes will be available at https://github.com/wangzhuo2019/SSAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep AutoAugment. (arXiv:2203.06172v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06172">
<div class="article-summary-box-inner">
<span><p>While recent automated data augmentation methods lead to state-of-the-art
results, their design spaces and the derived data augmentation strategies still
incorporate strong human priors. In this work, instead of fixing a set of
hand-picked default augmentations alongside the searched data augmentations, we
propose a fully automated approach for data augmentation search named Deep
AutoAugment (DeepAA). DeepAA progressively builds a multi-layer data
augmentation pipeline from scratch by stacking augmentation layers one at a
time until reaching convergence. For each augmentation layer, the policy is
optimized to maximize the cosine similarity between the gradients of the
original and augmented data along the direction with low variance. Our
experiments show that even without default augmentations, we can learn an
augmentation policy that achieves strong performance with that of previous
works. Extensive ablation studies show that the regularized gradient matching
is an effective search method for data augmentation policies. Our code is
available at: https://github.com/MSU-MLSys-Lab/DeepAA .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PillarGrid: Deep Learning-based Cooperative Perception for 3D Object Detection from Onboard-Roadside LiDAR. (arXiv:2203.06319v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06319">
<div class="article-summary-box-inner">
<span><p>3D object detection plays a fundamental role in enabling autonomous driving,
which is regarded as the significant key to unlocking the bottleneck of
contemporary transportation systems from the perspectives of safety, mobility,
and sustainability. Most of the state-of-the-art (SOTA) object detection
methods from point clouds are developed based on a single onboard LiDAR, whose
performance will be inevitably limited by the range and occlusion, especially
in dense traffic scenarios. In this paper, we propose \textit{PillarGrid}, a
novel cooperative perception method fusing information from multiple 3D LiDARs
(both on-board and roadside), to enhance the situation awareness for connected
and automated vehicles (CAVs). PillarGrid consists of four main phases: 1)
cooperative preprocessing of point clouds, 2) pillar-wise voxelization and
feature extraction, 3) grid-wise deep fusion of features from multiple sensors,
and 4) convolutional neural network (CNN)-based augmented 3D object detection.
A novel cooperative perception platform is developed for model training and
testing. Extensive experimentation shows that PillarGrid outperforms the SOTA
single-LiDAR-based 3D object detection methods with respect to both accuracy
and range by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bringing Rolling Shutter Images Alive with Dual Reversed Distortion. (arXiv:2203.06451v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06451">
<div class="article-summary-box-inner">
<span><p>Rolling shutter (RS) distortion can be interpreted as the result of picking a
row of pixels from instant global shutter (GS) frames over time during the
exposure of the RS camera. This means that the information of each instant GS
frame is partially, yet sequentially, embedded into the row-dependent
distortion. Inspired by this fact, we address the challenging task of reversing
this process, i.e., extracting undistorted GS frames from images suffering from
RS distortion. However, since RS distortion is coupled with other factors such
as readout settings and the relative velocity of scene elements to the camera,
models that only exploit the geometric correlation between temporally adjacent
images suffer from poor generality in processing data with different readout
settings and dynamic scenes with both camera motion and object motion. In this
paper, instead of two consecutive frames, we propose to exploit a pair of
images captured by dual RS cameras with reversed RS directions for this highly
challenging task. Grounded on the symmetric and complementary nature of dual
reversed distortion, we develop a novel end-to-end model, IFED, to generate
dual optical flow sequence through iterative learning of the velocity field
during the RS time. Extensive experimental results demonstrate that IFED is
superior to naive cascade schemes, as well as the state-of-the-art which
utilizes adjacent RS images. Most importantly, although it is trained on a
synthetic dataset, IFED is shown to be effective at retrieving GS frame
sequences from real-world RS distorted images of dynamic scenes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Depth-Aware Generative Adversarial Network for Talking Head Video Generation. (arXiv:2203.06605v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06605">
<div class="article-summary-box-inner">
<span><p>Talking head video generation aims to produce a synthetic human face video
that contains the identity and pose information respectively from a given
source image and a driving video.Existing works for this task heavily rely on
2D representations (e.g. appearance and motion) learned from the input images.
However, dense 3D facial geometry (e.g. pixel-wise depth) is extremely
important for this task as it is particularly beneficial for us to essentially
generate accurate 3D face structures and distinguish noisy information from the
possibly cluttered background. Nevertheless, dense 3D geometry annotations are
prohibitively costly for videos and are typically not available for this video
generation task. In this paper, we first introduce a self-supervised geometry
learning method to automatically recover the dense 3D geometry (i.e.depth) from
the face videos without the requirement of any expensive 3D annotation data.
Based on the learned dense depth maps, we further propose to leverage them to
estimate sparse facial keypoints that capture the critical movement of the
human head. In a more dense way, the depth is also utilized to learn 3D-aware
cross-modal (i.e. appearance and depth) attention to guide the generation of
motion fields for warping source image representations. All these contributions
compose a novel depth-aware generative adversarial network (DaGAN) for talking
head generation. Extensive experiments conducted demonstrate that our proposed
method can generate highly realistic faces, and achieve significant results on
the unseen human faces.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Visual-Prompt Temporal Answering Grounding in Medical Instructional Video. (arXiv:2203.06667v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06667">
<div class="article-summary-box-inner">
<span><p>The temporal answering grounding in the video (TAGV) is a new task naturally
deriving from temporal sentence grounding in the video (TSGV). Given an
untrimmed video and a text question, this task aims at locating the matching
span from the video that can semantically answer the question. Existing methods
tend to formulate the TAGV task with a visual span-based question answering
(QA) approach by matching the visual frame span queried by the text question.
However, due to the weak correlations and huge gaps in semantics in features
between the textual question and visual answer, existing methods adopting
visual span predictor fail to perform well in the TAGV task. In this work, we
propose a visual-prompt text span localizing (VPTSL) method, which enhances the
text span localization in the pre-trained language model (PLM) with the visual
highlight features. Specifically, the context query attention is utilized to
perform cross-modal modeling between the textual and visual features. Then, the
highlight features are obtained through the highlight module with a linear
layer to provide the visual prompt. To alleviate the differences in semantics
and correlations between textual and visual features, we design the text span
predictor by encoding the question, the subtitles, and the visual prompt in the
PLM. As a result, the TAGV task is formulated to predict the span of subtitles
matching the answering frame timeline. Extensive experiments on the medical
instructional dataset, namely MedVidQA, show the proposed VPTSL outperforms
other state-of-the-art methods, which demonstrates the effectiveness of visual
prompt and the text span predictor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Similarity Equivariant Linear Transformation of Joint Orientation-Scale Space Representations. (arXiv:2203.06786v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06786">
<div class="article-summary-box-inner">
<span><p>Convolution is conventionally defined as a linear operation on functions of
one or more variables which commutes with shifts. Group convolution generalizes
the concept to linear operations on functions of group elements representing
more general geometric transformations and which commute with those
transformations. Since similarity transformation is the most general geometric
transformation on images that preserves shape, the group convolution that is
equivariant to similarity transformation is the most general shape preserving
linear operator. Because similarity transformations have four free parameters,
group convolutions are defined on four-dimensional, joint orientation-scale
spaces. Although prior work on equivariant linear operators has been limited to
discrete groups, the similarity group is continuous. In this paper, we describe
linear operators on discrete representations that are equivariant to continuous
similarity transformation. This is achieved by using a basis of functions that
is it joint shiftable-twistable-scalable. These pinwheel functions use Fourier
series in the orientation dimension and Laplace transform in the log-scale
dimension to form a basis of spatially localized functions that can be
continuously interpolated in position, orientation and scale. Although this
result is potentially significant with respect to visual computation generally,
we present an initial demonstration of its utility by using it to compute a
shape equivariant distribution of closed contours traced by particles
undergoing Brownian motion in velocity. The contours are constrained by sets of
points and line endings representing well known bistable illusory contour
inducing patterns.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">XYLayoutLM: Towards Layout-Aware Multimodal Networks For Visually-Rich Document Understanding. (arXiv:2203.06947v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06947">
<div class="article-summary-box-inner">
<span><p>Recently, various multimodal networks for Visually-Rich Document
Understanding(VRDU) have been proposed, showing the promotion of transformers
by integrating visual and layout information with the text embeddings. However,
most existing approaches utilize the position embeddings to incorporate the
sequence information, neglecting the noisy improper reading order obtained by
OCR tools. In this paper, we propose a robust layout-aware multimodal network
named XYLayoutLM to capture and leverage rich layout information from proper
reading orders produced by our Augmented XY Cut. Moreover, a Dilated
Conditional Position Encoding module is proposed to deal with the input
sequence of variable lengths, and it additionally extracts local layout
information from both textual and visual modalities while generating position
embeddings. Experiment results show that our XYLayoutLM achieves competitive
results on document understanding tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Computer Vision and Deep Learning for Fish Classification in Underwater Habitats: A Survey. (arXiv:2203.06951v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06951">
<div class="article-summary-box-inner">
<span><p>Marine scientists use remote underwater video recording to survey fish
species in their natural habitats. This helps them understand and predict how
fish respond to climate change, habitat degradation, and fishing pressure. This
information is essential for developing sustainable fisheries for human
consumption, and for preserving the environment. However, the enormous volume
of collected videos makes extracting useful information a daunting and
time-consuming task for a human. A promising method to address this problem is
the cutting-edge Deep Learning (DL) technology.DL can help marine scientists
parse large volumes of video promptly and efficiently, unlocking niche
information that cannot be obtained using conventional manual monitoring
methods. In this paper, we provide an overview of the key concepts of DL, while
presenting a survey of literature on fish habitat monitoring with a focus on
underwater fish classification. We also discuss the main challenges faced when
developing DL for underwater image processing and propose approaches to address
them. Finally, we provide insights into the marine habitat monitoring research
domain and shed light on what the future of DL for underwater image processing
may hold. This paper aims to inform a wide range of readers from marine
scientists who would like to apply DL in their research to computer scientists
who would like to survey state-of-the-art DL-based underwater fish habitat
monitoring literature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Blind2Unblind: Self-Supervised Image Denoising with Visible Blind Spots. (arXiv:2203.06967v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06967">
<div class="article-summary-box-inner">
<span><p>Real noisy-clean pairs on a large scale are costly and difficult to obtain.
Meanwhile, supervised denoisers trained on synthetic data perform poorly in
practice. Self-supervised denoisers, which learn only from single noisy images,
solve the data collection problem. However, self-supervised denoising methods,
especially blindspot-driven ones, suffer sizable information loss during input
or network design. The absence of valuable information dramatically reduces the
upper bound of denoising performance. In this paper, we propose a simple yet
efficient approach called Blind2Unblind to overcome the information loss in
blindspot-driven denoising methods. First, we introduce a global-aware mask
mapper that enables global perception and accelerates training. The mask mapper
samples all pixels at blind spots on denoised volumes and maps them to the same
channel, allowing the loss function to optimize all blind spots at once.
Second, we propose a re-visible loss to train the denoising network and make
blind spots visible. The denoiser can learn directly from raw noise images
without losing information or being trapped in identity mapping. We also
theoretically analyze the convergence of the re-visible loss. Extensive
experiments on synthetic and real-world datasets demonstrate the superior
performance of our approach compared to previous work. Code is available at
https://github.com/demonsjin/Blind2Unblind.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial amplitude swap towards robust image classifiers. (arXiv:2203.07138v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07138">
<div class="article-summary-box-inner">
<span><p>The vulnerability of convolutional neural networks (CNNs) to image
perturbations such as common corruptions and adversarial perturbations has
recently been investigated from the perspective of frequency. In this study, we
investigate the effect of the amplitude and phase spectra of adversarial images
on the robustness of CNN classifiers. Extensive experiments revealed that the
images generated by combining the amplitude spectrum of adversarial images and
the phase spectrum of clean images accommodates moderate and general
perturbations, and training with these images equips a CNN classifier with more
general robustness, performing well under both common corruptions and
adversarial perturbations. We also found that two types of overfitting
(catastrophic overfitting and robust overfitting) can be circumvented by the
aforementioned spectrum recombination. We believe that these results contribute
to the understanding and the training of truly robust classifiers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Implicit Motion Handling for Video Camouflaged Object Detection. (arXiv:2203.07363v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07363">
<div class="article-summary-box-inner">
<span><p>We propose a new video camouflaged object detection (VCOD) framework that can
exploit both short-term dynamics and long-term temporal consistency to detect
camouflaged objects from video frames. An essential property of camouflaged
objects is that they usually exhibit patterns similar to the background and
thus make them hard to identify from still images. Therefore, effectively
handling temporal dynamics in videos becomes the key for the VCOD task as the
camouflaged objects will be noticeable when they move. However, current VCOD
methods often leverage homography or optical flows to represent motions, where
the detection error may accumulate from both the motion estimation error and
the segmentation error. On the other hand, our method unifies motion estimation
and object segmentation within a single optimization framework. Specifically,
we build a dense correlation volume to implicitly capture motions between
neighbouring frames and utilize the final segmentation supervision to optimize
the implicit motion estimation and segmentation jointly. Furthermore, to
enforce temporal consistency within a video sequence, we jointly utilize a
spatio-temporal transformer to refine the short-term predictions. Extensive
experiments on VCOD benchmarks demonstrate the architectural effectiveness of
our approach. We also provide a large-scale VCOD dataset named MoCA-Mask with
pixel-level handcrafted ground-truth masks and construct a comprehensive VCOD
benchmark with previous methods to facilitate research in this direction.
Dataset Link: https://xueliancheng.github.io/SLT-Net-project.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-03-16 23:07:48.441838408 UTC">2022-03-16 23:07:48 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>