<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-02-11T01:30:00Z">02-11</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">TamilEmo: Finegrained Emotion Detection Dataset for Tamil. (arXiv:2202.04725v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04725">
<div class="article-summary-box-inner">
<span><p>Emotional Analysis from textual input has been considered both a challenging
and interesting task in Natural Language Processing. However, due to the lack
of datasets in low-resource languages (i.e. Tamil), it is difficult to conduct
research of high standard in this area. Therefore we introduce this labelled
dataset (a largest manually annotated dataset of more than 42k Tamil YouTube
comments, labelled for 31 emotions including neutral) for emotion recognition.
The goal of this dataset is to improve emotion detection in multiple downstream
tasks in Tamil. We have also created three different groupings of our emotions
(3-class, 7-class and 31-class) and evaluated the model's performance on each
category of the grouping. Our MURIL-base model has achieved a 0.60 macro
average F1-score across our 3-class group dataset. With 7-class and 31-class
groups, the Random Forest model performed well with a macro average F1-scores
of 0.42 and 0.29 respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting Human Similarity Judgments Using Large Language Models. (arXiv:2202.04728v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04728">
<div class="article-summary-box-inner">
<span><p>Similarity judgments provide a well-established method for accessing mental
representations, with applications in psychology, neuroscience and machine
learning. However, collecting similarity judgments can be prohibitively
expensive for naturalistic datasets as the number of comparisons grows
quadratically in the number of stimuli. One way to tackle this problem is to
construct approximation procedures that rely on more accessible proxies for
predicting similarity. Here we leverage recent advances in language models and
online recruitment, proposing an efficient domain-general procedure for
predicting human similarity judgments based on text descriptions. Intuitively,
similar stimuli are likely to evoke similar descriptions, allowing us to use
description similarity to predict pairwise similarity judgments. Crucially, the
number of descriptions required grows only linearly with the number of stimuli,
drastically reducing the amount of data required. We test this procedure on six
datasets of naturalistic images and show that our models outperform previous
approaches based on visual information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FedQAS: Privacy-aware machine reading comprehension with federated learning. (arXiv:2202.04742v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04742">
<div class="article-summary-box-inner">
<span><p>Machine reading comprehension (MRC) of text data is one important task in
Natural Language Understanding. It is a complex NLP problem with a lot of
ongoing research fueled by the release of the Stanford Question Answering
Dataset (SQuAD) and Conversational Question Answering (CoQA). It is considered
to be an effort to teach computers how to "understand" a text, and then to be
able to answer questions about it using deep learning. However, until now
large-scale training on private text data and knowledge sharing has been
missing for this NLP task. Hence, we present FedQAS, a privacy-preserving
machine reading system capable of leveraging large-scale private data without
the need to pool those datasets in a central location. The proposed approach
combines transformer models and federated learning technologies. The system is
developed using the FEDn framework and deployed as a proof-of-concept alliance
initiative. FedQAS is flexible, language-agnostic, and allows intuitive
participation and execution of local model training. In addition, we present
the architecture and implementation of the system, as well as provide a
reference evaluation based on the SQUAD dataset, to showcase how it overcomes
data privacy issues and enables knowledge sharing between alliance members in a
Federated learning setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SHAS: Approaching optimal Segmentation for End-to-End Speech Translation. (arXiv:2202.04774v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04774">
<div class="article-summary-box-inner">
<span><p>Speech translation models are unable to directly process long audios, like
TED talks, which have to be split into shorter segments. Speech translation
datasets provide manual segmentations of the audios, which are not available in
real-world scenarios, and existing segmentation methods usually significantly
reduce translation quality at inference time. To bridge the gap between the
manual segmentation of training and the automatic one at inference, we propose
Supervised Hybrid Audio Segmentation (SHAS), a method that can effectively
learn the optimal segmentation from any manually segmented speech corpus.
First, we train a classifier to identify the included frames in a segmentation,
using speech representations from a pre-trained wav2vec 2.0. The optimal
splitting points are then found by a probabilistic Divide-and-Conquer algorithm
that progressively splits at the frame of lowest probability until all segments
are below a pre-specified length. Experiments on MuST-C and mTEDx show that the
translation of the segments produced by our method approaches the quality of
the manual segmentation on 5 languages pairs. Namely, SHAS retains 95-98% of
the manual segmentation's BLEU score, compared to the 87-93% of the best
existing methods. Our method is additionally generalizable to different domains
and achieves high zero-shot performance in unseen languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Abduction of Sherlock Holmes: A Dataset for Visual Abductive Reasoning. (arXiv:2202.04800v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04800">
<div class="article-summary-box-inner">
<span><p>Humans have remarkable capacity to reason abductively and hypothesize about
what lies beyond the literal content of an image. By identifying concrete
visual clues scattered throughout a scene, we almost can't help but draw
probable inferences beyond the literal scene based on our everyday experience
and knowledge about the world. For example, if we see a "20 mph" sign alongside
a road, we might assume the street sits in a residential area (rather than on a
highway), even if no houses are pictured. Can machines perform similar visual
reasoning?
</p>
<p>We present Sherlock, an annotated corpus of 103K images for testing machine
capacity for abductive reasoning beyond literal image contents. We adopt a
free-viewing paradigm: participants first observe and identify salient clues
within images (e.g., objects, actions) and then provide a plausible inference
about the scene, given the clue. In total, we collect 363K (clue, inference)
pairs, which form a first-of-its-kind abductive visual reasoning dataset. Using
our corpus, we test three complementary axes of abductive reasoning. We
evaluate the capacity of models to: i) retrieve relevant inferences from a
large candidate corpus; ii) localize evidence for inferences via bounding
boxes, and iii) compare plausible inferences to match human judgments on a
newly-collected diagnostic corpus of 19K Likert-scale judgments. While we find
that fine-tuning CLIP-RN50x64 with a multitask objective outperforms strong
baselines, significant headroom exists between model performance and human
agreement. We provide analysis that points towards future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdaPrompt: Adaptive Model Training for Prompt-based NLP. (arXiv:2202.04824v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04824">
<div class="article-summary-box-inner">
<span><p>Prompt-based learning, with its capability to tackle zero-shot and few-shot
NLP tasks, has gained much attention in community. The main idea is to bridge
the gap between NLP downstream tasks and language modeling (LM), by mapping
these tasks into natural language prompts, which are then filled by pre-trained
language models (PLMs). However, for prompt learning, there are still two
salient gaps between NLP tasks and pretraining. First, prompt information is
not necessarily sufficiently present during LM pretraining. Second,
task-specific data are not necessarily well represented during pretraining. We
address these two issues by proposing AdaPrompt, adaptively retrieving external
data for continual pretraining of PLMs by making use of both task and prompt
characteristics. In addition, we make use of knowledge in Natural Language
Inference models for deriving adaptive verbalizers. Experimental results on
five NLP benchmarks show that AdaPrompt can improve over standard PLMs in
few-shot settings. In addition, in zero-shot settings, our method outperforms
standard prompt-based methods by up to 26.35\% relative error reduction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Networks and Identity Drive Geographic Properties of the Diffusion of Linguistic Innovation. (arXiv:2202.04842v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04842">
<div class="article-summary-box-inner">
<span><p>Adoption of cultural innovation (e.g., music, beliefs, language) is often
geographically correlated, with adopters largely residing within the boundaries
of relatively few well-studied, socially significant areas. These cultural
regions are often hypothesized to be the result of either (i) identity
performance driving the adoption of cultural innovation, or (ii) homophily in
the networks underlying diffusion. In this study, we show that demographic
identity and network topology are both required to model the diffusion of
innovation, as they play complementary roles in producing its spatial
properties. We develop an agent-based model of cultural adoption, and validate
geographic patterns of transmission in our model against a novel dataset of
innovative words that we identify from a 10% sample of Twitter. Using our
model, we are able to directly compare a combined network + identity model of
diffusion to simulated network-only and identity-only counterfactuals --
allowing us to test the separate and combined roles of network and identity.
While social scientists often treat either network or identity as the core
social structure in modeling culture change, we show that key geographic
properties of diffusion actually depend on both factors as each one influences
different mechanisms of diffusion. Specifically, the network principally drives
spread among urban counties via weak-tie diffusion, while identity plays a
disproportionate role in transmission among rural counties via strong-tie
diffusion. Diffusion between urban and rural areas, a key component in
innovation diffusing nationally, requires both network and identity. Our work
suggests that models must integrate both factors in order to understand and
reproduce the adoption of innovation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Artificial Intelligence for Source Code: A Dialogue Systems Perspective. (arXiv:2202.04847v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04847">
<div class="article-summary-box-inner">
<span><p>In this survey paper, we overview major deep learning methods used in Natural
Language Processing (NLP) and source code over the last 35 years. Next, we
present a survey of the applications of Artificial Intelligence (AI) for source
code, also known as Code Intelligence (CI) and Programming Language Processing
(PLP). We survey over 287 publications and present a software-engineering
centered taxonomy for CI placing each of the works into one category describing
how it best assists the software development cycle. Then, we overview the field
of conversational assistants and their applications in software engineering and
education. Lastly, we highlight research opportunities at the intersection of
AI for code and conversational assistants and provide future directions for
researching conversational assistants with CI capabilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The USTC-Ximalaya system for the ICASSP 2022 multi-channel multi-party meeting transcription (M2MeT) challenge. (arXiv:2202.04855v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04855">
<div class="article-summary-box-inner">
<span><p>We propose two improvements to target-speaker voice activity detection
(TS-VAD), the core component in our proposed speaker diarization system that
was submitted to the 2022 Multi-Channel Multi-Party Meeting Transcription
(M2MeT) challenge. These techniques are designed to handle multi-speaker
conversations in real-world meeting scenarios with high speaker-overlap ratios
and under heavy reverberant and noisy condition. First, for data preparation
and augmentation in training TS-VAD models, speech data containing both real
meetings and simulated indoor conversations are used. Second, in refining
results obtained after TS-VAD based decoding, we perform a series of
post-processing steps to improve the VAD results needed to reduce diarization
error rates (DERs). Tested on the ALIMEETING corpus, the newly released
Mandarin meeting dataset used in M2MeT, we demonstrate that our proposed system
can decrease the DER by up to 66.55/60.59% relatively when compared with
classical clustering based diarization on the Eval/Test set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distilling Hypernymy Relations from Language Models: On the Effectiveness of Zero-Shot Taxonomy Induction. (arXiv:2202.04876v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04876">
<div class="article-summary-box-inner">
<span><p>In this paper, we analyze zero-shot taxonomy learning methods which are based
on distilling knowledge from language models via prompting and sentence
scoring. We show that, despite their simplicity, these methods outperform some
supervised strategies and are competitive with the current state-of-the-art
under adequate conditions. We also show that statistical and linguistic
properties of prompts dictate downstream performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TaxoEnrich: Self-Supervised Taxonomy Completion via Structure-Semantic Representations. (arXiv:2202.04887v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04887">
<div class="article-summary-box-inner">
<span><p>Taxonomies are fundamental to many real-world applications in various
domains, serving as structural representations of knowledge. To deal with the
increasing volume of new concepts needed to be organized as taxonomies,
researchers turn to automatically completion of an existing taxonomy with new
concepts. In this paper, we propose TaxoEnrich, a new taxonomy completion
framework, which effectively leverages both semantic features and structural
information in the existing taxonomy and offers a better representation of
candidate position to boost the performance of taxonomy completion.
Specifically, TaxoEnrich consists of four components: (1)
taxonomy-contextualized embedding which incorporates both semantic meanings of
concept and taxonomic relations based on powerful pretrained language models;
(2) a taxonomy-aware sequential encoder which learns candidate position
representations by encoding the structural information of taxonomy; (3) a
query-aware sibling encoder which adaptively aggregates candidate siblings to
augment candidate position representations based on their importance to the
query-position matching; (4) a query-position matching model which extends
existing work with our new candidate position representations. Extensive
experiments on four large real-world datasets from different domains show that
\TaxoEnrich achieves the best performance among all evaluation metrics and
outperforms previous state-of-the-art methods by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InterHT: Knowledge Graph Embeddings by Interaction between Head and Tail Entities. (arXiv:2202.04897v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04897">
<div class="article-summary-box-inner">
<span><p>Knowledge graph embedding (KGE) models learn the representation of entities
and relations in knowledge graphs. Distance-based methods show promising
performance on link prediction task, which predicts the result by the distance
between two entity representations. However, most of these methods represent
the head entity and tail entity separately, which limits the model capacity. We
propose a novel distance-based method named InterHT that allows the head and
tail entities to interact better and get better entity representation.
Experimental results show that our proposed method achieves the best results on
ogbl-wikikg2 dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Slovene SuperGLUE Benchmark: Translation and Evaluation. (arXiv:2202.04994v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04994">
<div class="article-summary-box-inner">
<span><p>We present a Slovene combined machine-human translated SuperGLUE benchmark.
We describe the translation process and problems arising due to differences in
morphology and grammar. We evaluate the translated datasets in several modes:
monolingual, cross-lingual, and multilingual, taking into account differences
between machine and human translated training sets. The results show that the
monolingual Slovene SloBERTa model is superior to massively multilingual and
trilingual BERT models, but these also show a good cross-lingual performance on
certain tasks. The performance of Slovene models still lags behind the best
English models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Natural Language in Requirements Engineering for Structure Inference -- An Integrative Review. (arXiv:2202.05065v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05065">
<div class="article-summary-box-inner">
<span><p>The automatic extraction of structure from text can be difficult for
machines. Yet, the elicitation of this information can provide many benefits
and opportunities for various applications. Benefits have also been identified
for the area of Requirements Engineering. To evaluate what work has been done
and is currently available, the paper at hand provides an integrative review
regarding Natural Language Processing (NLP) tools for Requirements Engineering.
This assessment was conducted to provide a foundation for future work as well
as deduce insights from the stats quo. To conduct the review, the history of
Requirements Engineering and NLP are described as well as an evaluation of over
136 NLP tools. To assess these tools, a set of criteria was defined. The
results are that currently no open source approach exists that allows for the
direct/primary extraction of information structure and even closed source
solutions show limitations such as supervision or input limitations, which
eliminates the possibility for fully automatic and universal application. As a
results, the authors deduce that the current approaches are not applicable and
a different methodology is necessary. An approach that allows for individual
management of the algorithm, knowledge base, and text corpus is a possibility
being pursued.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-speaker style transfer for text-to-speech using data augmentation. (arXiv:2202.05083v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05083">
<div class="article-summary-box-inner">
<span><p>We address the problem of cross-speaker style transfer for text-to-speech
(TTS) using data augmentation via voice conversion. We assume to have a corpus
of neutral non-expressive data from a target speaker and supporting
conversational expressive data from different speakers. Our goal is to build a
TTS system that is expressive, while retaining the target speaker's identity.
The proposed approach relies on voice conversion to first generate high-quality
data from the set of supporting expressive speakers. The voice converted data
is then pooled with natural data from the target speaker and used to train a
single-speaker multi-style TTS system. We provide evidence that this approach
is efficient, flexible, and scalable. The method is evaluated using one or more
supporting speakers, as well as a variable amount of supporting data. We
further provide evidence that this approach allows some controllability of
speaking style, when using multiple supporting speakers. We conclude by scaling
our proposed technology to a set of 14 speakers across 7 languages. Results
indicate that our technology consistently improves synthetic samples in terms
of style similarity, while retaining the target speaker's identity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InPars: Data Augmentation for Information Retrieval using Large Language Models. (arXiv:2202.05144v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05144">
<div class="article-summary-box-inner">
<span><p>The information retrieval community has recently witnessed a revolution due
to large pretrained transformer models. Another key ingredient for this
revolution was the MS MARCO dataset, whose scale and diversity has enabled
zero-shot transfer learning to various tasks. However, not all IR tasks and
domains can benefit from one single dataset equally. Extensive research in
various NLP tasks has shown that using domain-specific training data, as
opposed to a general-purpose one, improves the performance of neural models. In
this work, we harness the few-shot capabilities of large pretrained language
models as synthetic data generators for IR tasks. We show that models finetuned
solely on our unsupervised dataset outperform strong baselines such as BM25 as
well as recently proposed self-supervised dense retrieval methods. Furthermore,
retrievers finetuned on both supervised and our synthetic data achieve better
zero-shot transfer than models finetuned only on supervised data. Code, models,
and data are available at https://github.com/zetaalphavector/inpars .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying Weaknesses in Machine Translation Metrics Through Minimum Bayes Risk Decoding: A Case Study for COMET. (arXiv:2202.05148v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05148">
<div class="article-summary-box-inner">
<span><p>Neural metrics have achieved impressive correlation with human judgements in
the evaluation of machine translation systems, but before we can safely
optimise towards such metrics, we should be aware of (and ideally eliminate)
biases towards bad translations that receive high scores. Our experiments show
that sample-based Minimum Bayes Risk decoding can be used to explore and
quantify such weaknesses. When applying this strategy to COMET for en-de and
de-en, we find that COMET models are not sensitive enough to discrepancies in
numbers and named entities. We further show that these biases cannot be fully
removed by simply training on additional synthetic data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Automatic Speech Recognition for Non-Native English with Transfer Learning and Language Model Decoding. (arXiv:2202.05209v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05209">
<div class="article-summary-box-inner">
<span><p>ASR systems designed for native English (L1) usually underperform on
non-native English (L2). To address this performance gap, \textbf{(i)} we
extend our previous work to investigate fine-tuning of a pre-trained wav2vec
2.0 model \cite{baevski2020wav2vec,xu2021self} under a rich set of L1 and L2
training conditions. We further \textbf{(ii)} incorporate language model
decoding in the ASR system, along with the fine-tuning method. Quantifying
gains acquired from each of these two approaches separately and an error
analysis allows us to identify different sources of improvement within our
models. We find that while the large self-trained wav2vec 2.0 may be
internalizing sufficient decoding knowledge for clean L1 speech
\cite{xu2021self}, this does not hold for L2 speech and accounts for the
utility of employing language model decoding on L2 data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Locating and Editing Factual Knowledge in GPT. (arXiv:2202.05262v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05262">
<div class="article-summary-box-inner">
<span><p>We investigate the mechanisms underlying factual knowledge recall in
autoregressive transformer language models. First, we develop a causal
intervention for identifying neuron activations capable of altering a model's
factual predictions. Within large GPT-style models, this reveals two distinct
sets of neurons that we hypothesize correspond to knowing an abstract fact and
saying a concrete word, respectively. This insight inspires the development of
ROME, a novel method for editing facts stored in model weights. For evaluation,
we assemble CounterFact, a dataset of over twenty thousand counterfactuals and
tools to facilitate sensitive measurements of knowledge editing. Using
CounterFact, we confirm the distinction between saying and knowing neurons, and
we find that ROME achieves state-of-the-art performance in knowledge editing
compared to other methods. An interactive demo notebook, full code
implementation, and the dataset are available at https://rome.baulab.info/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fairness-aware Summarization for Justified Decision-Making. (arXiv:2107.06243v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.06243">
<div class="article-summary-box-inner">
<span><p>In consequential domains such as recidivism prediction, facility inspection,
and benefit assignment, it's important for individuals to know the
decision-relevant information for the model's prediction. In addition,
predictions should be fair both in terms of the outcome and the justification
of the outcome. In other words, decision-relevant features should provide
sufficient information for the predicted outcome and should be independent of
the membership of individuals in protected groups such as race and gender. In
this work, we focus on the problem of (un)fairness in the justification of the
text-based neural models. We tie the explanatory power of the model to fairness
in the outcome and propose a fairness-aware summarization mechanism to detect
and counteract the bias in such models. Given a potentially biased natural
language explanation for a decision, we use a multi-task neural model and an
attribution mechanism based on integrated gradients to extract high-utility and
low-bias justifications in form of a summary. The extracted summary is then
used for training a model to make decisions for individuals. Results on several
real world datasets suggest that our method drastically limits the demographic
leakage in the input (fairness in justification) while moderately enhancing the
fairness in the outcome. Our model is also effective in detecting and
counteracting several types of data poisoning attacks that synthesize
race-coded reasoning or irrelevant justifications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Exploratory Study on Utilising the Web of Linked Data for Product Data Mining. (arXiv:2109.01411v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01411">
<div class="article-summary-box-inner">
<span><p>The Linked Open Data practice has led to a significant growth of structured
data on the Web in the last decade. Such structured data describe real-world
entities in a machine-readable way, and have created an unprecedented
opportunity for research in the field of Natural Language Processing. However,
there is a lack of studies on how such data can be used, for what kind of
tasks, and to what extent they can be useful for these tasks. This work focuses
on the e-commerce domain to explore methods of utilising such structured data
to create language resources that may be used for product classification and
linking. We process billions of structured data points in the form of RDF
n-quads, to create multi-million words of product-related corpora that are
later used in three different ways for creating of language resources: training
word embedding models, continued pre-training of BERT-like language models, and
training Machine Translation models that are used as a proxy to generate
product-related keywords. Our evaluation on an extensive set of benchmarks
shows word embeddings to be the most reliable and consistent method to improve
the accuracy on both tasks (with up to 6.9 percentage points in macro-average
F1 on some datasets). The other two methods however, are not as useful. Our
analysis shows that this could be due to a number of reasons, including the
biased domain representation in the structured data and lack of vocabulary
coverage. We share our datasets and discuss how our lessons learned could be
taken forward to inform future research in this direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tamizhi-Net OCR: Creating A Quality Large Scale Tamil-Sinhala-English Parallel Corpus Using Deep Learning Based Printed Character Recognition (PCR). (arXiv:2109.05952v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05952">
<div class="article-summary-box-inner">
<span><p>Most of the low resource languages do not have the necessary resources to
create even a substantial monolingual corpus. These languages may often be
found in government proceedings but mainly in Portable Document Formats (PDFs)
that contain legacy fonts. Extracting text from these documents to create a
monolingual corpus is challenging due to legacy font usage and printer-friendly
encoding, which are not optimised for text extraction. Therefore, we propose a
simple, automatic, and novel idea that can scale for Tamil, Sinhala, English
languages and many documents. For this purpose, we enhanced the performance of
Tesseract 4.1.1 by employing LSTM-based training on many legacy fonts to
recognise printed characters in the above languages. Especially, our model
detects code-mix text, numbers, and special characters from the printed
document. It is shown that this approach can boost the character-level accuracy
of Tesseract 4.1.1 from 85.5 to 98.2 for Tamil (+12.9% relative change) and
91.8 to 94.8 for Sinhala (+3.26% relative change) on a dataset that is
considered as challenging by its authors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-turn RNN-T for streaming recognition of multi-party speech. (arXiv:2112.10200v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10200">
<div class="article-summary-box-inner">
<span><p>Automatic speech recognition (ASR) of single channel far-field recordings
with an unknown number of speakers is traditionally tackled by cascaded
modules. Recent research shows that end-to-end (E2E) multi-speaker ASR models
can achieve superior recognition accuracy compared to modular systems. However,
these models do not ensure real-time applicability due to their dependency on
full audio context. This work takes real-time applicability as the first
priority in model design and addresses a few challenges in previous work on
multi-speaker recurrent neural network transducer (MS-RNN-T). First, we
introduce on-the-fly overlapping speech simulation during training, yielding
14% relative word error rate (WER) improvement on LibriSpeechMix test set.
Second, we propose a novel multi-turn RNN-T (MT-RNN-T) model with an
overlap-based target arrangement strategy that generalizes to an arbitrary
number of speakers without changes in the model architecture. We investigate
the impact of the maximum number of speakers seen during training on MT-RNN-T
performance on LibriCSS test set, and report 28% relative WER improvement over
the two-speaker MS-RNN-T. Third, we experiment with a rich transcription
strategy for joint recognition and segmentation of multi-party speech. Through
an in-depth analysis, we discuss potential pitfalls of the proposed system as
well as promising future research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Dark Side of the Language: Pre-trained Transformers in the DarkNet. (arXiv:2201.05613v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05613">
<div class="article-summary-box-inner">
<span><p>Pre-trained Transformers are challenging human performances in many natural
language processing tasks. The gigantic datasets used for pre-training seem to
be the key for their success on existing tasks. In this paper, we explore how a
range of pre-trained natural language understanding models perform on truly
novel and unexplored data, provided by classification tasks over a DarkNet
corpus. Surprisingly, results show that syntactic and lexical neural networks
largely outperform pre-trained Transformers. This seems to suggest that
pre-trained Transformers have serious difficulties in adapting to radically
novel texts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LaMDA: Language Models for Dialog Applications. (arXiv:2201.08239v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08239">
<div class="article-summary-box-inner">
<span><p>We present LaMDA: Language Models for Dialog Applications. LaMDA is a family
of Transformer-based neural language models specialized for dialog, which have
up to 137B parameters and are pre-trained on 1.56T words of public dialog data
and web text. While model scaling alone can improve quality, it shows less
improvements on safety and factual grounding. We demonstrate that fine-tuning
with annotated data and enabling the model to consult external knowledge
sources can lead to significant improvements towards the two key challenges of
safety and factual grounding. The first challenge, safety, involves ensuring
that the model's responses are consistent with a set of human values, such as
preventing harmful suggestions and unfair bias. We quantify safety using a
metric based on an illustrative set of human values, and we find that filtering
candidate responses using a LaMDA classifier fine-tuned with a small amount of
crowdworker-annotated data offers a promising approach to improving model
safety. The second challenge, factual grounding, involves enabling the model to
consult external knowledge sources, such as an information retrieval system, a
language translator, and a calculator. We quantify factuality using a
groundedness metric, and we find that our approach enables the model to
generate responses grounded in known sources, rather than responses that merely
sound plausible. Finally, we explore the use of LaMDA in the domains of
education and content recommendations, and analyze their helpfulness and role
consistency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Typical Decoding for Natural Language Generation. (arXiv:2202.00666v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00666">
<div class="article-summary-box-inner">
<span><p>Despite achieving incredibly low perplexities on myriad natural language
corpora, today's language models still often underperform when used to generate
text. This dichotomy has puzzled the language generation community for the last
few years. In this work, we posit that the abstraction of natural language as a
communication channel (\`a la Shannon, 1948) can provide new insights into the
behaviors of probabilistic language generators, e.g., why high-probability
texts can be dull or repetitive. Humans use language as a means of
communicating information, and do so in an efficient yet error-minimizing
manner, choosing each word in a string with this (perhaps subconscious) goal in
mind. We propose that generation from probabilistic models should mimic this
behavior. Rather than always choosing words from the high-probability region of
the distribution--which have a low Shannon information content--we sample from
the set of words with an information content close to its expected value, i.e.,
close to the conditional entropy of our model. This decision criterion can be
realized through a simple and efficient implementation, which we call typical
sampling. Automatic and human evaluations show that, in comparison to nucleus
and top-k sampling, typical sampling offers competitive performance in terms of
quality while consistently reducing the number of degenerate repetitions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to pronounce as measuring cross-lingual joint orthography-phonology complexity. (arXiv:2202.00794v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00794">
<div class="article-summary-box-inner">
<span><p>Machine learning models allow us to compare languages by showing how hard a
task in each language might be to learn and perform well on. Following this
line of investigation, we explore what makes a language "hard to pronounce" by
modelling the task of grapheme-to-phoneme (g2p) transliteration. By training a
character-level transformer model on this task across 22 languages and
measuring the model's proficiency against its grapheme and phoneme inventories,
we show that certain characteristics emerge that separate easier and harder
languages with respect to learning to pronounce. Namely the complexity of a
language's pronunciation from its orthography is due to the expressive or
simplicity of its grapheme-to-phoneme mapping. Further discussion illustrates
how future studies should consider relative data sparsity per language to
design fairer cross-lingual comparison tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Platform Difference in Facebook and Text Messages Language Use: Illustrated by Depression Diagnosis. (arXiv:2202.01802v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01802">
<div class="article-summary-box-inner">
<span><p>How does language differ across one's Facebook status updates vs. one's text
messages (SMS)? In this study, we show how Facebook and SMS use differs in
psycho-linguistic characteristics and how these differences drive downstream
analyses with an illustration of depression diagnosis. We use a sample of
consenting participants who shared Facebook status updates, SMS data, and
answered a standard psychological depression screener. We quantify domain
differences using psychologically driven lexical methods and find that language
on Facebook involves more personal concerns, experiences, and content features
while the language in SMS contains more informal and style features. Next, we
estimate depression from both text domains, using a depression model trained on
Facebook data, and find a drop in accuracy when predicting self-reported
depression assessments from the SMS-based depression estimates. Finally, we
evaluate a simple domain adaption correction based on words driving the
cross-platform differences and applied it to the SMS-derived depression
estimates, resulting in significant improvement in prediction. Our work shows
the Facebook vs. SMS difference in language use and suggests the necessity of
cross-domain adaption for text-based predictions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What are the best systems? New perspectives on NLP Benchmarking. (arXiv:2202.03799v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03799">
<div class="article-summary-box-inner">
<span><p>In Machine Learning, a benchmark refers to an ensemble of datasets associated
with one or multiple metrics together with a way to aggregate different systems
performances. They are instrumental in (i) assessing the progress of new
methods along different axes and (ii) selecting the best systems for practical
use. This is particularly the case for NLP with the development of large
pre-trained models (e.g. GPT, BERT) that are expected to generalize well on a
variety of tasks. While the community mainly focused on developing new datasets
and metrics, there has been little interest in the aggregation procedure, which
is often reduced to a simple average over various performance measures.
However, this procedure can be problematic when the metrics are on a different
scale, which may lead to spurious conclusions. This paper proposes a new
procedure to rank systems based on their performance across different tasks.
Motivated by the social choice theory, the final system ordering is obtained
through aggregating the rankings induced by each task and is theoretically
grounded. We conduct extensive numerical experiments (on over 270k scores) to
assess the soundness of our approach both on synthetic and real scores (e.g.
GLUE, EXTREM, SEVAL, TAC, FLICKR). In particular, we show that our method
yields different conclusions on state-of-the-art systems than the
mean-aggregation procedure while being both more reliable and robust.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differentiable N-gram Objective on Abstractive Summarization. (arXiv:2202.04003v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04003">
<div class="article-summary-box-inner">
<span><p>ROUGE is a standard automatic evaluation metric based on n-grams for
sequence-to-sequence tasks, while cross-entropy loss is an essential objective
of neural network language model that optimizes at a unigram level. We present
differentiable n-gram objectives, attempting to alleviate the discrepancy
between training criterion and evaluating criterion. The objective maximizes
the probabilistic weight of matched sub-sequences, and the novelty of our work
is the objective weights the matched sub-sequences equally and does not ceil
the number of matched sub-sequences by the ground truth count of n-grams in
reference sequence. We jointly optimize cross-entropy loss and the proposed
objective, providing decent ROUGE score enhancement over abstractive
summarization dataset CNN/DM and XSum, outperforming alternative n-gram
objectives.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">FCM-DNN: diagnosing coronary artery disease by deep accuracy Fuzzy C-Means clustering model. (arXiv:2202.04645v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04645">
<div class="article-summary-box-inner">
<span><p>Cardiovascular disease is one of the most challenging diseases in middle-aged
and older people, which causes high mortality. Coronary artery disease (CAD) is
known as a common cardiovascular disease. A standard clinical tool for
diagnosing CAD is angiography. The main challenges are dangerous side effects
and high angiography costs. Today, the development of artificial
intelligence-based methods is a valuable achievement for diagnosing disease.
Hence, in this paper, artificial intelligence methods such as neural network
(NN), deep neural network (DNN), and Fuzzy C-Means clustering combined with
deep neural network (FCM-DNN) are developed for diagnosing CAD on a cardiac
magnetic resonance imaging (CMRI) dataset. The original dataset is used in two
different approaches. First, the labeled dataset is applied to the NN and DNN
to create the NN and DNN models. Second, the labels are removed, and the
unlabeled dataset is clustered via the FCM method, and then, the clustered
dataset is fed to the DNN to create the FCM-DNN model. By utilizing the second
clustering and modeling, the training process is improved, and consequently,
the accuracy is increased. As a result, the proposed FCM-DNN model achieves the
best performance with a 99.91% accuracy specifying 10 clusters, i.e., 5
clusters for healthy subjects and 5 clusters for sick subjects, through the
10-fold cross-validation technique compared to the NN and DNN models reaching
the accuracies of 92.18% and 99.63%, respectively. To the best of our
knowledge, no study has been conducted for CAD diagnosis on the CMRI dataset
using artificial intelligence methods. The results confirm that the proposed
FCM-DNN model can be helpful for scientific and research centers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-modal unsupervised brain image registration using edge maps. (arXiv:2202.04647v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04647">
<div class="article-summary-box-inner">
<span><p>Diffeomorphic deformable multi-modal image registration is a challenging task
which aims to bring images acquired by different modalities to the same
coordinate space and at the same time to preserve the topology and the
invertibility of the transformation. Recent research has focused on leveraging
deep learning approaches for this task as these have been shown to achieve
competitive registration accuracy while being computationally more efficient
than traditional iterative registration methods. In this work, we propose a
simple yet effective unsupervised deep learning-based {\em multi-modal} image
registration approach that benefits from auxiliary information coming from the
gradient magnitude of the image, i.e. the image edges, during the training. The
intuition behind this is that image locations with a strong gradient are
assumed to denote a transition of tissues, which are locations of high
information value able to act as a geometry constraint. The task is similar to
using segmentation maps to drive the training, but the edge maps are easier and
faster to acquire and do not require annotations. We evaluate our approach in
the context of registering multi-modal (T1w to T2w) magnetic resonance (MR)
brain images of different subjects using three different loss functions that
are said to assist multi-modal registration, showing that in all cases the
auxiliary information leads to better results without compromising the runtime.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Segmentation of Anaemic RBCs Using Multilevel Deep Convolutional Encoder-Decoder Network. (arXiv:2202.04650v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04650">
<div class="article-summary-box-inner">
<span><p>Pixel-level analysis of blood images plays a pivotal role in diagnosing
blood-related diseases, especially Anaemia. These analyses mainly rely on an
accurate diagnosis of morphological deformities like shape, size, and precise
pixel counting. In traditional segmentation approaches, instance or
object-based approaches have been adopted that are not feasible for pixel-level
analysis. The convolutional neural network (CNN) model required a large dataset
with detailed pixel-level information for the semantic segmentation of red
blood cells in the deep learning domain. In current research work, we address
these problems by proposing a multi-level deep convolutional encoder-decoder
network along with two state-of-the-art healthy and Anaemic-RBC datasets. The
proposed multi-level CNN model preserved pixel-level semantic information
extracted in one layer and then passed to the next layer to choose relevant
features. This phenomenon helps to precise pixel-level counting of healthy and
anaemic-RBC elements along with morphological analysis. For experimental
purposes, we proposed two state-of-the-art RBC datasets, i.e., Healthy-RBCs and
Anaemic-RBCs dataset. Each dataset contains 1000 images, ground truth masks,
relevant, complete blood count (CBC), and morphology reports for performance
evaluation. The proposed model results were evaluated using crossmatch analysis
with ground truth mask by finding IoU, individual training, validation, testing
accuracies, and global accuracies using a 05-fold training procedure. This
model got training, validation, and testing accuracies as 0.9856, 0.9760, and
0.9720 on the Healthy-RBC dataset and 0.9736, 0.9696, and 0.9591 on an
Anaemic-RBC dataset. The IoU and BFScore of the proposed model were 0.9311,
0.9138, and 0.9032, 0.8978 on healthy and anaemic datasets, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Humans Do Less-Than-One-Shot Learning?. (arXiv:2202.04670v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04670">
<div class="article-summary-box-inner">
<span><p>Being able to learn from small amounts of data is a key characteristic of
human intelligence, but exactly {\em how} small? In this paper, we introduce a
novel experimental paradigm that allows us to examine classification in an
extremely data-scarce setting, asking whether humans can learn more categories
than they have exemplars (i.e., can humans do "less-than-one shot" learning?).
An experiment conducted using this paradigm reveals that people are capable of
learning in such settings, and provides several insights into underlying
mechanisms. First, people can accurately infer and represent high-dimensional
feature spaces from very little data. Second, having inferred the relevant
spaces, people use a form of prototype-based categorization (as opposed to
exemplar-based) to make categorical inferences. Finally, systematic,
machine-learnable patterns in responses indicate that people may have efficient
inductive biases for dealing with this class of data-scarce problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Joint Variational Multichannel Multiphase Segmentation Framework. (arXiv:2202.04680v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04680">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a variational image segmentation framework for
multichannel multiphase image segmentation based on the Chan-Vese active
contour model. The core of our method lies in finding a variable u encoding the
segmentation, by minimizing a multichannel energy functional that combines the
information of multiple images. We create a decomposition of the input, either
by multichannel filtering, or simply by using plain natural RGB, or medical
images, which already consist of several channels. Subsequently we minimize the
proposed functional for each of the channels simultaneously. Our model meets
the necessary assumptions such that it can be solved efficiently by
optimization techniques like the Chambolle-Pock method. We prove that the
proposed energy functional has global minimizers, and show its stability and
convergence with respect to noisy inputs. Experimental results show that the
proposed method performs well in single- and multichannel segmentation tasks,
and can be employed to the segmentation of various types of images, such as
natural and texture images as well as medical images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PINs: Progressive Implicit Networks for Multi-Scale Neural Representations. (arXiv:2202.04713v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04713">
<div class="article-summary-box-inner">
<span><p>Multi-layer perceptrons (MLP) have proven to be effective scene encoders when
combined with higher-dimensional projections of the input, commonly referred to
as \textit{positional encoding}. However, scenes with a wide frequency spectrum
remain a challenge: choosing high frequencies for positional encoding
introduces noise in low structure areas, while low frequencies result in poor
fitting of detailed regions. To address this, we propose a progressive
positional encoding, exposing a hierarchical MLP structure to incremental sets
of frequency encodings. Our model accurately reconstructs scenes with wide
frequency bands and learns a scene representation at progressive level of
detail \textit{without explicit per-level supervision}. The architecture is
modular: each level encodes a continuous implicit representation that can be
leveraged separately for its respective resolution, meaning a smaller network
for coarser reconstructions. Experiments on several 2D and 3D datasets show
improvements in reconstruction accuracy, representational capacity and training
speed compared to baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph Neural Network for Cell Tracking in Microscopy Videos. (arXiv:2202.04731v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04731">
<div class="article-summary-box-inner">
<span><p>We present a novel graph neural network (GNN) approach for cell tracking in
high-throughput microscopy videos. By modeling the entire time-lapse sequence
as a direct graph where cell instances are represented by its nodes and their
associations by its edges, we extract the entire set of cell trajectories by
looking for the maximal paths in the graph. This is accomplished by several key
contributions incorporated into an end-to-end deep learning framework. We
exploit a deep metric learning algorithm to extract cell feature vectors that
distinguish between instances of different biological cells and assemble same
cell instances. We introduce a new GNN block type which enables a mutual update
of node and edge feature vectors, thus facilitating the underlying message
passing process. The message passing concept, whose extent is determined by the
number of GNN blocks, is of fundamental importance as it enables the `flow' of
information between nodes and edges much behind their neighbors in consecutive
frames. Finally, we solve an edge classification problem and use the identified
active edges to construct the cells' tracks and lineage trees. We demonstrate
the strengths of the proposed cell tracking approach by applying it to 2D and
3D datasets of different cell types, imaging setups, and experimental
conditions. We show that our framework outperforms most of the current
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Estimation of Clinical Workload and Patient Activity using Deep Learning and Optical Flow. (arXiv:2202.04748v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04748">
<div class="article-summary-box-inner">
<span><p>Contactless monitoring using thermal imaging has become increasingly proposed
to monitor patient deterioration in hospital, most recently to detect fevers
and infections during the COVID-19 pandemic. In this letter, we propose a novel
method to estimate patient motion and observe clinical workload using a similar
technical setup but combined with open source object detection algorithms
(YOLOv4) and optical flow. Patient motion estimation was used to approximate
patient agitation and sedation, while worker motion was used as a surrogate for
caregiver workload. Performance was illustrated by comparing over 32000 frames
from videos of patients recorded in an Intensive Care Unit, to clinical
agitation scores recorded by clinical workers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discovering Concepts in Learned Representations using Statistical Inference and Interactive Visualization. (arXiv:2202.04753v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04753">
<div class="article-summary-box-inner">
<span><p>Concept discovery is one of the open problems in the interpretability
literature that is important for bridging the gap between non-deep learning
experts and model end-users. Among current formulations, concepts defines them
by as a direction in a learned representation space. This definition makes it
possible to evaluate whether a particular concept significantly influences
classification decisions for classes of interest. However, finding relevant
concepts is tedious, as representation spaces are high-dimensional and hard to
navigate. Current approaches include hand-crafting concept datasets and then
converting them to latent space directions; alternatively, the process can be
automated by clustering the latent space. In this study, we offer another two
approaches to guide user discovery of meaningful concepts, one based on
multiple hypothesis testing, and another on interactive visualization. We
explore the potential value and limitations of these approaches through
simulation experiments and an demo visual interface to real data. Overall, we
find that these techniques offer a promising strategy for discovering relevant
concepts in settings where users do not have predefined descriptions of them,
but without completely automating the process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wireless Transmission of Images With The Assistance of Multi-level Semantic Information. (arXiv:2202.04754v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04754">
<div class="article-summary-box-inner">
<span><p>Semantic-oriented communication has been considered as a promising to boost
the bandwidth efficiency by only transmitting the semantics of the data. In
this paper, we propose a multi-level semantic aware communication system for
wireless image transmission, named MLSC-image, which is based on the deep
learning techniques and trained in an end to end manner. In particular, the
proposed model includes a multilevel semantic feature extractor, that extracts
both the highlevel semantic information, such as the text semantics and the
segmentation semantics, and the low-level semantic information, such as local
spatial details of the images. We employ a pretrained image caption to capture
the text semantics and a pretrained image segmentation model to obtain the
segmentation semantics. These high-level and low-level semantic features are
then combined and encoded by a joint semantic and channel encoder into symbols
to transmit over the physical channel. The numerical results validate the
effectiveness and efficiency of the proposed semantic communication system,
especially under the limited bandwidth condition, which indicates the
advantages of the high-level semantics in the compression of images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepSSN: a deep convolutional neural network to assess spatial scene similarity. (arXiv:2202.04755v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04755">
<div class="article-summary-box-inner">
<span><p>Spatial-query-by-sketch is an intuitive tool to explore human spatial
knowledge about geographic environments and to support communication with scene
database queries. However, traditional sketch-based spatial search methods
perform insufficiently due to their inability to find hidden multi-scale map
features from mental sketches. In this research, we propose a deep
convolutional neural network, namely Deep Spatial Scene Network (DeepSSN), to
better assess the spatial scene similarity. In DeepSSN, a triplet loss function
is designed as a comprehensive distance metric to support the similarity
assessment. A positive and negative example mining strategy using qualitative
constraint networks in spatial reasoning is designed to ensure a consistently
increasing distinction of triplets during the training process. Moreover, we
develop a prototype spatial scene search system using the proposed DeepSSN, in
which the users input spatial query via sketch maps and the system can
automatically augment the sketch training data. The proposed model is validated
using multi-source conflated map data including 131,300 labeled scene samples
after data augmentation. The empirical results demonstrate that the DeepSSN
outperforms baseline methods including k-nearest-neighbors, multilayer
perceptron, AlexNet, DenseNet, and ResNet using mean reciprocal rank and
precision metrics. This research advances geographic information retrieval
studies by introducing a novel deep learning method tailored to spatial scene
queries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Encoder-Decoder Network with Guided Transmission Map for Single Image Dehazing. (arXiv:2202.04757v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04757">
<div class="article-summary-box-inner">
<span><p>A novel Encoder-Decoder Network with Guided Transmission Map (EDN-GTM) for
single image dehazing scheme is proposed in this paper. The proposed EDN-GTM
takes conventional RGB hazy image in conjunction with its transmission map
estimated by adopting dark channel prior as the inputs of the network. The
proposed EDN-GTM utilizes U-Net for image segmentation as the core network and
utilizes various modifications including spatial pyramid pooling module and
Swish activation to achieve state-of-the-art dehazing performance. Experiments
on benchmark datasets show that the proposed EDN-GTM outperforms most of
traditional and deep learning-based image dehazing schemes in terms of PSNR and
SSIM metrics. The proposed EDN-GTM furthermore proves its applicability to
object detection problems. Specifically, when applied to an image preprocessing
tool for driving object detection, the proposed EDN-GTM can efficiently remove
haze and significantly improve detection accuracy by 4.73% in terms of mAP
measure. The code is available at: https://github.com/tranleanh/edn-gtm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sampling Strategy for Fine-Tuning Segmentation Models to Crisis Area under Scarcity of Data. (arXiv:2202.04766v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04766">
<div class="article-summary-box-inner">
<span><p>The use of remote sensing in humanitarian crisis response missions is
well-established and has proven relevant repeatedly. One of the problems is
obtaining gold annotations as it is costly and time consuming which makes it
almost impossible to fine-tune models to new regions affected by the crisis.
Where time is critical, resources are limited and environment is constantly
changing, models has to evolve and provide flexible ways to adapt to a new
situation. The question that we want to answer is if prioritization of samples
provide better results in fine-tuning vs other classical sampling methods under
annotated data scarcity? We propose a method to guide data collection during
fine-tuning, based on estimated model and sample properties, like predicted IOU
score. We propose two formulas for calculating sample priority. Our approach
blends techniques from interpretability, representation learning and active
learning. We have applied our method to a deep learning model for semantic
segmentation, U-Net, in a remote sensing application of building detection -
one of the core use cases of remote sensing in humanitarian applications.
Preliminary results shows utility in prioritization of samples for tuning
semantic segmentation models under scarcity of data condition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Attack and Defense of YOLO Detectors in Autonomous Driving Scenarios. (arXiv:2202.04781v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04781">
<div class="article-summary-box-inner">
<span><p>Visual detection is a key task in autonomous driving, and it serves as one
foundation for self-driving planning and control. Deep neural networks have
achieved promising results in various computer vision tasks, but they are known
to be vulnerable to adversarial attacks. A comprehensive understanding of deep
visual detectors' vulnerability is required before people can improve their
robustness. However, only a few adversarial attack/defense works have focused
on object detection, and most of them employed only classification and/or
localization losses, ignoring the objectness aspect. In this paper, we identify
a serious objectness-related adversarial vulnerability in YOLO detectors and
present an effective attack strategy aiming the objectness aspect of visual
detection in autonomous vehicles. Furthermore, to address such vulnerability,
we propose a new objectness-aware adversarial training approach for visual
detection. Experiments show that the proposed attack targeting the objectness
aspect is 45.17% and 43.50% more effective than those generated from
classification and/or localization losses on the KITTI and COCO_traffic
datasets, respectively. Also, the proposed adversarial defense approach can
improve the detectors' robustness against objectness-oriented attacks by up to
21% and 12% mAP on KITTI and COCO_traffic, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiclass histogram-based thresholding using kernel density estimation and scale-space representations. (arXiv:2202.04785v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04785">
<div class="article-summary-box-inner">
<span><p>We present a new method for multiclass thresholding of a histogram which is
based on the nonparametric Kernel Density (KD) estimation, where the unknown
parameters of the KD estimate are defined using the Expectation-Maximization
(EM) iterations. The method compares the number of extracted minima of the KD
estimate with the number of the requested clusters minus one. If these numbers
match, the algorithm returns positions of the minima as the threshold values,
otherwise, the method gradually decreases/increases the kernel bandwidth until
the numbers match. We verify the method using synthetic histograms with known
threshold values and using the histogram of real X-ray computed tomography
images. After thresholding of the real histogram, we estimated the porosity of
the sample and compare it with the direct experimental measurements. The
comparison shows the meaningfulness of the thresholding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Abduction of Sherlock Holmes: A Dataset for Visual Abductive Reasoning. (arXiv:2202.04800v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04800">
<div class="article-summary-box-inner">
<span><p>Humans have remarkable capacity to reason abductively and hypothesize about
what lies beyond the literal content of an image. By identifying concrete
visual clues scattered throughout a scene, we almost can't help but draw
probable inferences beyond the literal scene based on our everyday experience
and knowledge about the world. For example, if we see a "20 mph" sign alongside
a road, we might assume the street sits in a residential area (rather than on a
highway), even if no houses are pictured. Can machines perform similar visual
reasoning?
</p>
<p>We present Sherlock, an annotated corpus of 103K images for testing machine
capacity for abductive reasoning beyond literal image contents. We adopt a
free-viewing paradigm: participants first observe and identify salient clues
within images (e.g., objects, actions) and then provide a plausible inference
about the scene, given the clue. In total, we collect 363K (clue, inference)
pairs, which form a first-of-its-kind abductive visual reasoning dataset. Using
our corpus, we test three complementary axes of abductive reasoning. We
evaluate the capacity of models to: i) retrieve relevant inferences from a
large candidate corpus; ii) localize evidence for inferences via bounding
boxes, and iii) compare plausible inferences to match human judgments on a
newly-collected diagnostic corpus of 19K Likert-scale judgments. While we find
that fine-tuning CLIP-RN50x64 with a multitask objective outperforms strong
baselines, significant headroom exists between model performance and human
agreement. We provide analysis that points towards future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly-Supervised Semantic Segmentation with Visual Words Learning and Hybrid Pooling. (arXiv:2202.04812v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04812">
<div class="article-summary-box-inner">
<span><p>Weakly-Supervised Semantic Segmentation (WSSS) methods with image-level
labels generally train a classification network to generate the Class
Activation Maps (CAMs) as the initial coarse segmentation labels. However,
current WSSS methods still perform far from satisfactorily because their
adopted CAMs 1) typically focus on partial discriminative object regions and 2)
usually contain useless background regions. These two problems are attributed
to the sole image-level supervision and aggregation of global information when
training the classification networks. In this work, we propose the visual words
learning module and hybrid pooling approach, and incorporate them in the
classification network to mitigate the above problems. In the visual words
learning module, we counter the first problem by enforcing the classification
network to learn fine-grained visual word labels so that more object extents
could be discovered. Specifically, the visual words are learned with a
codebook, which could be updated via two proposed strategies, i.e.
learning-based strategy and memory-bank strategy. The second drawback of CAMs
is alleviated with the proposed hybrid pooling, which incorporates the global
average and local discriminative information to simultaneously ensure object
completeness and reduce background regions. We evaluated our methods on PASCAL
VOC 2012 and MS COCO 2014 datasets. Without any extra saliency prior, our
method achieved 70.6% and 70.7% mIoU on the $val$ and $test$ set of PASCAL VOC
dataset, respectively, and 36.2% mIoU on the $val$ set of MS COCO dataset,
which significantly surpassed the performance of state-of-the-art WSSS methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decreasing Annotation Burden of Pairwise Comparisons with Human-in-the-Loop Sorting: Application in Medical Image Artifact Rating. (arXiv:2202.04823v1 [q-bio.QM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04823">
<div class="article-summary-box-inner">
<span><p>Ranking by pairwise comparisons has shown improved reliability over ordinal
classification. However, as the annotations of pairwise comparisons scale
quadratically, this becomes less practical when the dataset is large. We
propose a method for reducing the number of pairwise comparisons required to
rank by a quantitative metric, demonstrating the effectiveness of the approach
in ranking medical images by image quality in this proof of concept study.
Using the medical image annotation software that we developed, we actively
subsample pairwise comparisons using a sorting algorithm with a human rater in
the loop. We find that this method substantially reduces the number of
comparisons required for a full ordinal ranking without compromising
inter-rater reliability when compared to pairwise comparisons without sorting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bias-Eliminated Semantic Refinement for Any-Shot Learning. (arXiv:2202.04827v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04827">
<div class="article-summary-box-inner">
<span><p>When training samples are scarce, the semantic embedding technique, ie,
describing class labels with attributes, provides a condition to generate
visual features for unseen objects by transferring the knowledge from seen
objects. However, semantic descriptions are usually obtained in an external
paradigm, such as manual annotation, resulting in weak consistency between
descriptions and visual features. In this paper, we refine the coarse-grained
semantic description for any-shot learning tasks, ie, zero-shot learning (ZSL),
generalized zero-shot learning (GZSL), and few-shot learning (FSL). A new
model, namely, the semantic refinement Wasserstein generative adversarial
network (SRWGAN) model, is designed with the proposed multihead representation
and hierarchical alignment techniques. Unlike conventional methods, semantic
refinement is performed with the aim of identifying a bias-eliminated condition
for disjoint-class feature generation and is applicable in both inductive and
transductive settings. We extensively evaluate model performance on six
benchmark datasets and observe state-of-the-art results for any-shot learning;
eg, we obtain 70.2% harmonic accuracy for the Caltech UCSD Birds (CUB) dataset
and 82.2% harmonic accuracy for the Oxford Flowers (FLO) dataset in the
standard GZSL setting. Various visualizations are also provided to show the
bias-eliminated generation of SRWGAN. Our code is available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Geometric Digital Twinning of Industrial Facilities: Retrieval of Industrial Shapes. (arXiv:2202.04834v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04834">
<div class="article-summary-box-inner">
<span><p>This paper devises, implements and benchmarks a novel shape retrieval method
that can accurately match individual labelled point clusters (instances) of
existing industrial facilities with their respective CAD models. It employs a
combination of image and point cloud deep learning networks to classify and
match instances to their geometrically similar CAD model. It extends our
previous research on geometric digital twin generation from point cloud data,
which currently is a tedious, manual process. Experiments with our joint
network reveal that it can reliably retrieve CAD models at 85.2\% accuracy. The
proposed research is a fundamental framework to enable the geometric Digital
Twin (gDT) pipeline and incorporate the real geometric configuration into the
Digital Twin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Consistency and Diversity induced Human Motion Segmentation. (arXiv:2202.04861v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04861">
<div class="article-summary-box-inner">
<span><p>Subspace clustering is a classical technique that has been widely used for
human motion segmentation and other related tasks. However, existing
segmentation methods often cluster data without guidance from prior knowledge,
resulting in unsatisfactory segmentation results. To this end, we propose a
novel Consistency and Diversity induced human Motion Segmentation (CDMS)
algorithm. Specifically, our model factorizes the source and target data into
distinct multi-layer feature spaces, in which transfer subspace learning is
conducted on different layers to capture multi-level information. A
multi-mutual consistency learning strategy is carried out to reduce the domain
gap between the source and target data. In this way, the domain-specific
knowledge and domain-invariant properties can be explored simultaneously.
Besides, a novel constraint based on the Hilbert Schmidt Independence Criterion
(HSIC) is introduced to ensure the diversity of multi-level subspace
representations, which enables the complementarity of multi-level
representations to be explored to boost the transfer learning performance.
Moreover, to preserve the temporal correlations, an enhanced graph regularizer
is imposed on the learned representation coefficients and the multi-level
representations of the source data. The proposed model can be efficiently
solved using the Alternating Direction Method of Multipliers (ADMM) algorithm.
Extensive experimental results on public human motion datasets demonstrate the
effectiveness of our method against several state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Memory-based gaze prediction in deep imitation learning for robot manipulation. (arXiv:2202.04877v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04877">
<div class="article-summary-box-inner">
<span><p>Deep imitation learning is a promising approach that does not require
hard-coded control rules in autonomous robot manipulation. The current
applications of deep imitation learning to robot manipulation have been limited
to reactive control based on the states at the current time step. However,
future robots will also be required to solve tasks utilizing their memory
obtained by experience in complicated environments (e.g., when the robot is
asked to find a previously used object on a shelf). In such a situation, simple
deep imitation learning may fail because of distractions caused by complicated
environments. We propose that gaze prediction from sequential visual input
enables the robot to perform a manipulation task that requires memory. The
proposed algorithm uses a Transformer-based self-attention architecture for the
gaze estimation based on sequential data to implement memory. The proposed
method was evaluated with a real robot multi-object manipulation task that
requires memory of the previous states.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PVSeRF: Joint Pixel-, Voxel- and Surface-Aligned Radiance Field for Single-Image Novel View Synthesis. (arXiv:2202.04879v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04879">
<div class="article-summary-box-inner">
<span><p>We present PVSeRF, a learning framework that reconstructs neural radiance
fields from single-view RGB images, for novel view synthesis. Previous
solutions, such as pixelNeRF, rely only on pixel-aligned features and suffer
from feature ambiguity issues. As a result, they struggle with the
disentanglement of geometry and appearance, leading to implausible geometries
and blurry results. To address this challenge, we propose to incorporate
explicit geometry reasoning and combine it with pixel-aligned features for
radiance field prediction. Specifically, in addition to pixel-aligned features,
we further constrain the radiance field learning to be conditioned on i)
voxel-aligned features learned from a coarse volumetric grid and ii) fine
surface-aligned features extracted from a regressed point cloud. We show that
the introduction of such geometry-aware features helps to achieve a better
disentanglement between appearance and geometry, i.e. recovering more accurate
geometries and synthesizing higher quality images of novel views. Extensive
experiments against state-of-the-art methods on ShapeNet benchmarks demonstrate
the superiority of our approach for single-image novel view synthesis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards the automated large-scale reconstruction of past road networks from historical maps. (arXiv:2202.04883v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04883">
<div class="article-summary-box-inner">
<span><p>Transportation infrastructure, such as road or railroad networks, represent a
fundamental component of our civilization. For sustainable planning and
informed decision making, a thorough understanding of the long-term evolution
of transportation infrastructure such as road networks is crucial. However,
spatially explicit, multi-temporal road network data covering large spatial
extents are scarce and rarely available prior to the 2000s. Herein, we propose
a framework that employs increasingly available scanned and georeferenced
historical map series to reconstruct past road networks, by integrating
abundant, contemporary road network data and color information extracted from
historical maps. Specifically, our method uses contemporary road segments as
analytical units and extracts historical roads by inferring their existence in
historical map series based on image processing and clustering techniques. We
tested our method on over 300,000 road segments representing more than 50,000
km of the road network in the United States, extending across three study areas
that cover 53 historical topographic map sheets dated between 1890 and 1950. We
evaluated our approach by comparison to other historical datasets and against
manually created reference data, achieving F-1 scores of up to 0.95, and showed
that the extracted road network statistics are highly plausible over time,
i.e., following general growth patterns. We demonstrated that contemporary
geospatial data integrated with information extracted from historical map
series open up new avenues for the quantitative analysis of long-term
urbanization processes and landscape changes far beyond the era of operational
remote sensing and digital cartography.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving performance of aircraft detection in satellite imagery while limiting the labelling effort: Hybrid active learning. (arXiv:2202.04890v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04890">
<div class="article-summary-box-inner">
<span><p>The earth observation industry provides satellite imagery with high spatial
resolution and short revisit time. To allow efficient operational employment of
these images, automating certain tasks has become necessary. In the defense
domain, aircraft detection on satellite imagery is a valuable tool for
analysts. Obtaining high performance detectors on such a task can only be
achieved by leveraging deep learning and thus us-ing a large amount of labeled
data. To obtain labels of a high enough quality, the knowledge of military
experts is needed.We propose a hybrid clustering active learning method to
select the most relevant data to label, thus limiting the amount of data
required and further improving the performances. It combines diversity- and
uncertainty-based active learning selection methods. For aircraft detection by
segmentation, we show that this method can provide better or competitive
results compared to other active learning methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FILM: Frame Interpolation for Large Motion. (arXiv:2202.04901v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04901">
<div class="article-summary-box-inner">
<span><p>We present a frame interpolation algorithm that synthesizes multiple
intermediate frames from two input images with large in-between motion. Recent
methods use multiple networks to estimate optical flow or depth and a separate
network dedicated to frame synthesis. This is often complex and requires scarce
optical flow or depth ground-truth. In this work, we present a single unified
network, distinguished by a multi-scale feature extractor that shares weights
at all scales, and is trainable from frames alone. To synthesize crisp and
pleasing frames, we propose to optimize our network with the Gram matrix loss
that measures the correlation difference between feature maps. Our approach
outperforms state-of-the-art methods on the Xiph large motion benchmark. We
also achieve higher scores on Vimeo-90K, Middlebury and UCF101, when comparing
to methods that use perceptual losses. We study the effect of weight sharing
and of training with datasets of increasing motion range. Finally, we
demonstrate our model's effectiveness in synthesizing high quality and
temporally coherent videos on a challenging near-duplicate photos dataset.
Codes and pre-trained models are available at
https://github.com/google-research/frame-interpolation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spherical Transformer. (arXiv:2202.04942v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04942">
<div class="article-summary-box-inner">
<span><p>Using convolutional neural networks for 360images can induce sub-optimal
performance due to distortions entailed by a planar projection. The distortion
gets deteriorated when a rotation is applied to the 360image. Thus, many
researches based on convolutions attempt to reduce the distortions to learn
accurate representation. In contrast, we leverage the transformer architecture
to solve image classification problems for 360images. Using the proposed
transformer for 360images has two advantages. First, our method does not
require the erroneous planar projection process by sampling pixels from the
sphere surface. Second, our sampling method based on regular polyhedrons makes
low rotation equivariance errors, because specific rotations can be reduced to
permutations of faces. In experiments, we validate our network on two aspects,
as follows. First, we show that using a transformer with highly uniform
sampling methods can help reduce the distortion. Second, we demonstrate that
the transformer architecture can achieve rotation equivariance on specific
rotations. We compare our method to other state-of-the-art algorithms using the
SPH-MNIST, SPH-CIFAR, and SUN360 datasets and show that our method is
competitive with other methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OWL (Observe, Watch, Listen): Localizing Actions in Egocentric Video via Audiovisual Temporal Context. (arXiv:2202.04947v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04947">
<div class="article-summary-box-inner">
<span><p>Temporal action localization (TAL) is an important task extensively explored
and improved for third-person videos in recent years. Recent efforts have been
made to perform fine-grained temporal localization on first-person videos.
However, current TAL methods only use visual signals, neglecting the audio
modality that exists in most videos and that shows meaningful action
information in egocentric videos. In this work, we take a deep look into the
effectiveness of audio in detecting actions in egocentric videos and introduce
a simple-yet-effective approach via Observing, Watching, and Listening (OWL) to
leverage audio-visual information and context for egocentric TAL. For doing
that, we: 1) compare and study different strategies for where and how to fuse
the two modalities; 2) propose a transformer-based model to incorporate
temporal audio-visual context. Our experiments show that our approach achieves
state-of-the-art performance on EPIC-KITCHENS-100.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Monotonically Convergent Regularization by Denoising. (arXiv:2202.04961v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04961">
<div class="article-summary-box-inner">
<span><p>Regularization by denoising (RED) is a widely-used framework for solving
inverse problems by leveraging image denoisers as image priors. Recent work has
reported the state-of-the-art performance of RED in a number of imaging
applications using pre-trained deep neural nets as denoisers. Despite the
recent progress, the stable convergence of RED algorithms remains an open
problem. The existing RED theory only guarantees stability for convex
data-fidelity terms and nonexpansive denoisers. This work addresses this issue
by developing a new monotone RED (MRED) algorithm, whose convergence does not
require nonexpansiveness of the deep denoising prior. Simulations on image
deblurring and compressive sensing recovery from random matrices show the
stability of MRED even when the traditional RED algorithm diverges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-Time Siamese Multiple Object Tracker with Enhanced Proposals. (arXiv:2202.04966v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04966">
<div class="article-summary-box-inner">
<span><p>Maintaining the identity of multiple objects in real-time video is a
challenging task, as it is not always possible to run a detector on every
frame. Thus, motion estimation systems are often employed, which either do not
scale well with the number of targets or produce features with limited semantic
information. To solve the aforementioned problems and allow the tracking of
dozens of arbitrary objects in real-time, we propose SiamMOTION. SiamMOTION
includes a novel proposal engine that produces quality features through an
attention mechanism and a region-of-interest extractor fed by an inertia module
and powered by a feature pyramid network. Finally, the extracted tensors enter
a comparison head that efficiently matches pairs of exemplars and search areas,
generating quality predictions via a pairwise depthwise region proposal network
and a multi-object penalization module. SiamMOTION has been validated on five
public benchmarks, achieving leading performance against current
state-of-the-art trackers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Assessing and Characterizing the Semantic Robustness of Face Recognition. (arXiv:2202.04978v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04978">
<div class="article-summary-box-inner">
<span><p>Deep Neural Networks (DNNs) lack robustness against imperceptible
perturbations to their input. Face Recognition Models (FRMs) based on DNNs
inherit this vulnerability. We propose a methodology for assessing and
characterizing the robustness of FRMs against semantic perturbations to their
input. Our methodology causes FRMs to malfunction by designing adversarial
attacks that search for identity-preserving modifications to faces. In
particular, given a face, our attacks find identity-preserving variants of the
face such that an FRM fails to recognize the images belonging to the same
identity. We model these identity-preserving semantic modifications via
direction- and magnitude-constrained perturbations in the latent space of
StyleGAN. We further propose to characterize the semantic robustness of an FRM
by statistically describing the perturbations that induce the FRM to
malfunction. Finally, we combine our methodology with a certification
technique, thus providing (i) theoretical guarantees on the performance of an
FRM, and (ii) a formal description of how an FRM may model the notion of face
identity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">N\"UWA-LIP: Language Guided Image Inpainting with Defect-free VQGAN. (arXiv:2202.05009v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05009">
<div class="article-summary-box-inner">
<span><p>Language guided image inpainting aims to fill in the defective regions of an
image under the guidance of text while keeping non-defective regions unchanged.
However, the encoding process of existing models suffers from either receptive
spreading of defective regions or information loss of non-defective regions,
giving rise to visually unappealing inpainting results. To address the above
issues, this paper proposes N\"UWA-LIP by incorporating defect-free VQGAN
(DF-VQGAN) with multi-perspective sequence to sequence (MP-S2S). In particular,
DF-VQGAN introduces relative estimation to control receptive spreading and
adopts symmetrical connections to protect information. MP-S2S further enhances
visual information from complementary perspectives, including both low-level
pixels and high-level tokens. Experiments show that DF-VQGAN performs more
robustness than VQGAN. To evaluate the inpainting performance of our model, we
built up 3 open-domain benchmarks, where N\"UWA-LIP is also superior to recent
strong baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Spatial Sparsity for Event Cameras with Visual Transformers. (arXiv:2202.05054v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05054">
<div class="article-summary-box-inner">
<span><p>Event cameras report local changes of brightness through an asynchronous
stream of output events. Events are spatially sparse at pixel locations with
little brightness variation. We propose using a visual transformer (ViT)
architecture to leverage its ability to process a variable-length input. The
input to the ViT consists of events that are accumulated into time bins and
spatially separated into non-overlapping sub-regions called patches. Patches
are selected when the number of nonzero pixel locations within a sub-region is
above a threshold. We show that by fine-tuning a ViT model on the selected
active patches, we can reduce the average number of patches fed into the
backbone during the inference by at least 50% with only a minor drop (0.34%) of
the classification accuracy on the N-Caltech101 dataset. This reduction
translates into a decrease of 51% in Multiply-Accumulate (MAC) operations and
an increase of 46% in the inference speed using a server CPU.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Equivariance Regularization for Image Reconstruction. (arXiv:2202.05062v1 [math.OC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05062">
<div class="article-summary-box-inner">
<span><p>In this work, we propose Regularization-by-Equivariance (REV), a novel
structure-adaptive regularization scheme for solving imaging inverse problems
under incomplete measurements. Our regularization scheme utilizes the
equivariant structure in the physics of the measurements -- which is prevalent
in many inverse problems such as tomographic image reconstruction -- to
mitigate the ill-poseness of the inverse problem. Our proposed scheme can be
applied in a plug-and-play manner alongside with any classic first-order
optimization algorithm such as the accelerated gradient descent/FISTA for
simplicity and fast convergence. Our numerical experiments in sparse-view X-ray
CT image reconstruction tasks demonstrate the effectiveness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning for Computational Cytology: A Survey. (arXiv:2202.05126v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05126">
<div class="article-summary-box-inner">
<span><p>Computational cytology is a critical, rapid-developing, yet challenging topic
in the field of medical image computing which analyzes the digitized cytology
image by computer-aided technologies for cancer screening. Recently, an
increasing number of deep learning (DL) algorithms have made significant
progress in medical image analysis, leading to the boosting publications of
cytological studies. To investigate the advanced methods and comprehensive
applications, we survey more than 120 publications of DL-based cytology image
analysis in this article. We first introduce various deep learning methods,
including fully supervised, weakly supervised, unsupervised, and transfer
learning. Then, we systematically summarize the public datasets, evaluation
metrics, versatile cytology image analysis applications including
classification, detection, segmentation, and other related tasks. Finally, we
discuss current challenges and potential research directions of computational
cytology.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature-level augmentation to improve robustness of deep neural networks to affine transformations. (arXiv:2202.05152v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05152">
<div class="article-summary-box-inner">
<span><p>Recent studies revealed that convolutional neural networks do not generalize
well to small image transformations, e.g. rotations by a few degrees or
translations of a few pixels. To improve the robustness to such
transformations, we propose to introduce data augmentation at intermediate
layers of the neural architecture, in addition to the common data augmentation
applied on the input images. By introducing small perturbations to activation
maps (features) at various levels, we develop the capacity of the neural
network to cope with such transformations. We conduct experiments on three
image classification benchmarks (Tiny ImageNet, Caltech-256 and Food-101),
considering two different convolutional architectures (ResNet-18 and
DenseNet-121). When compared with two state-of-the-art methods, the empirical
results show that our approach consistently attains the best trade-off between
accuracy and mean flip rate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Class Distance Weighted Cross-Entropy Loss for Ulcerative Colitis Severity Estimation. (arXiv:2202.05167v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05167">
<div class="article-summary-box-inner">
<span><p>Endoscopic Mayo score and Ulcerative Colitis Endoscopic Index of Severity are
commonly used scoring systems for the assessment of endoscopic severity of
ulcerative colitis. They are based on assigning a score in relation to the
disease activity, which creates a rank among the levels, making it an ordinal
regression problem. On the other hand, most studies use categorical
cross-entropy loss function, which is not optimal for the ordinal regression
problem, to train the deep learning models. In this study, we propose a novel
loss function called class distance weighted cross-entropy (CDW-CE) that
respects the order of the classes and takes the distance of the classes into
account in calculation of cost. Experimental evaluations show that CDW-CE
outperforms the conventional categorical cross-entropy and CORN framework,
which is designed for the ordinal regression problems. In addition, CDW-CE does
not require any modifications at the output layer and is compatible with the
class activation map visualization techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adults as Augmentations for Children in Facial Emotion Recognition with Contrastive Learning. (arXiv:2202.05187v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05187">
<div class="article-summary-box-inner">
<span><p>Emotion recognition in children can help the early identification of, and
intervention on, psychological complications that arise in stressful situations
such as cancer treatment. Though deep learning models are increasingly being
adopted, data scarcity is often an issue in pediatric medicine, including for
facial emotion recognition in children. In this paper, we study the application
of data augmentation-based contrastive learning to overcome data scarcity in
facial emotion recognition for children. We explore the idea of ignoring
generational gaps, by adding abundantly available adult data to pediatric data,
to learn better representations. We investigate different ways by which adult
facial expression images can be used alongside those of children. In
particular, we propose to explicitly incorporate within each mini-batch adult
images as augmentations for children's. Out of $84$ combinations of learning
approaches and training set sizes, we find that supervised contrastive learning
with the proposed training scheme performs best, reaching a test accuracy that
typically surpasses the one of the second-best approach by 2% to 3%. Our
results indicate that adult data can be considered to be a meaningful
augmentation of pediatric data for the recognition of emotional facial
expression in children, and open up the possibility for other applications of
contrastive learning to improve pediatric care by complementing data of
children with that of adults.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Human-Centered Machine-Learning Approach for Muscle-Tendon Junction Tracking in Ultrasound Images. (arXiv:2202.05199v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05199">
<div class="article-summary-box-inner">
<span><p>Biomechanical and clinical gait research observes muscles and tendons in
limbs to study their functions and behaviour. Therefore, movements of distinct
anatomical landmarks, such as muscle-tendon junctions, are frequently measured.
We propose a reliable and time efficient machine-learning approach to track
these junctions in ultrasound videos and support clinical biomechanists in gait
analysis. In order to facilitate this process, a method based on deep-learning
was introduced. We gathered an extensive dataset, covering 3 functional
movements, 2 muscles, collected on 123 healthy and 38 impaired subjects with 3
different ultrasound systems, and providing a total of 66864 annotated
ultrasound images in our network training. Furthermore, we used data collected
across independent laboratories and curated by researchers with varying levels
of experience. For the evaluation of our method a diverse test-set was selected
that is independently verified by four specialists. We show that our model
achieves similar performance scores to the four human specialists in
identifying the muscle-tendon junction position. Our method provides
time-efficient tracking of muscle-tendon junctions, with prediction times of up
to 0.078 seconds per frame (approx. 100 times faster than manual labeling). All
our codes, trained models and test-set were made publicly available and our
model is provided as a free-to-use online service on https://deepmtj.org/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Servoing for Pose Control of Soft Continuum Arm in a Structured Environment. (arXiv:2202.05200v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05200">
<div class="article-summary-box-inner">
<span><p>For soft continuum arms, visual servoing is a popular control strategy that
relies on visual feedback to close the control loop. However, robust visual
servoing is challenging as it requires reliable feature extraction from the
image, accurate control models and sensors to perceive the shape of the arm,
both of which can be hard to implement in a soft robot. This letter circumvents
these challenges by presenting a deep neural network-based method to perform
smooth and robust 3D positioning tasks on a soft arm by visual servoing using a
camera mounted at the distal end of the arm. A convolutional neural network is
trained to predict the actuations required to achieve the desired pose in a
structured environment. Integrated and modular approaches for estimating the
actuations from the image are proposed and are experimentally compared. A
proportional control law is implemented to reduce the error between the desired
and current image as seen by the camera. The model together with the
proportional feedback control makes the described approach robust to several
variations such as new targets, lighting, loads, and diminution of the soft
arm. Furthermore, the model lends itself to be transferred to a new environment
with minimal effort.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Predicting Fine Finger Motions from Ultrasound Images via Kinematic Representation. (arXiv:2202.05204v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05204">
<div class="article-summary-box-inner">
<span><p>A central challenge in building robotic prostheses is the creation of a
sensor-based system able to read physiological signals from the lower limb and
instruct a robotic hand to perform various tasks. Existing systems typically
perform discrete gestures such as pointing or grasping, by employing
electromyography (EMG) or ultrasound (US) technologies to analyze the state of
the muscles. In this work, we study the inference problem of identifying the
activation of specific fingers from a sequence of US images when performing
dexterous tasks such as keyboard typing or playing the piano. While estimating
finger gestures has been done in the past by detecting prominent gestures, we
are interested in classification done in the context of fine motions that
evolve over time. We consider this task as an important step towards higher
adoption rates of robotic prostheses among arm amputees, as it has the
potential to dramatically increase functionality in performing daily tasks. Our
key observation, motivating this work, is that modeling the hand as a robotic
manipulator allows to encode an intermediate representation wherein US images
are mapped to said configurations. Given a sequence of such learned
configurations, coupled with a neural-network architecture that exploits
temporal coherence, we are able to infer fine finger motions. We evaluated our
method by collecting data from a group of subjects and demonstrating how our
framework can be used to replay music played or text typed. To the best of our
knowledge, this is the first study demonstrating these downstream tasks within
an end-to-end system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">F8Net: Fixed-Point 8-bit Only Multiplication for Network Quantization. (arXiv:2202.05239v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05239">
<div class="article-summary-box-inner">
<span><p>Neural network quantization is a promising compression technique to reduce
memory footprint and save energy consumption, potentially leading to real-time
inference. However, there is a performance gap between quantized and
full-precision models. To reduce it, existing quantization approaches require
high-precision INT32 or full-precision multiplication during inference for
scaling or dequantization. This introduces a noticeable cost in terms of
memory, speed, and required energy. To tackle these issues, we present F8Net, a
novel quantization framework consisting of only fixed-point 8-bit
multiplication. To derive our method, we first discuss the advantages of
fixed-point multiplication with different formats of fixed-point numbers and
study the statistical behavior of the associated fixed-point numbers. Second,
based on the statistical and algorithmic analysis, we apply different
fixed-point formats for weights and activations of different layers. We
introduce a novel algorithm to automatically determine the right format for
each layer during training. Third, we analyze a previous quantization algorithm
-- parameterized clipping activation (PACT) -- and reformulate it using
fixed-point arithmetic. Finally, we unify the recently proposed method for
quantization fine-tuning and our fixed-point approach to show the potential of
our method. We verify F8Net on ImageNet for MobileNet V1/V2 and ResNet18/50.
Our approach achieves comparable and better performance, when compared not only
to existing quantization techniques with INT32 multiplication or floating-point
arithmetic, but also to the full-precision counterparts, achieving
state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Block-NeRF: Scalable Large Scene Neural View Synthesis. (arXiv:2202.05263v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05263">
<div class="article-summary-box-inner">
<span><p>We present Block-NeRF, a variant of Neural Radiance Fields that can represent
large-scale environments. Specifically, we demonstrate that when scaling NeRF
to render city-scale scenes spanning multiple blocks, it is vital to decompose
the scene into individually trained NeRFs. This decomposition decouples
rendering time from scene size, enables rendering to scale to arbitrarily large
environments, and allows per-block updates of the environment. We adopt several
architectural changes to make NeRF robust to data captured over months under
different environmental conditions. We add appearance embeddings, learned pose
refinement, and controllable exposure to each individual NeRF, and introduce a
procedure for aligning appearance between adjacent NeRFs so that they can be
seamlessly combined. We build a grid of Block-NeRFs from 2.8 million images to
create the largest neural scene representation to date, capable of rendering an
entire neighborhood of San Francisco.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image-to-Image Regression with Distribution-Free Uncertainty Quantification and Applications in Imaging. (arXiv:2202.05265v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05265">
<div class="article-summary-box-inner">
<span><p>Image-to-image regression is an important learning task, used frequently in
biological imaging. Current algorithms, however, do not generally offer
statistical guarantees that protect against a model's mistakes and
hallucinations. To address this, we develop uncertainty quantification
techniques with rigorous statistical guarantees for image-to-image regression
problems. In particular, we show how to derive uncertainty intervals around
each pixel that are guaranteed to contain the true value with a user-specified
confidence probability. Our methods work in conjunction with any base machine
learning model, such as a neural network, and endow it with formal mathematical
guarantees -- regardless of the true unknown data distribution or choice of
model. Furthermore, they are simple to implement and computationally
inexpensive. We evaluate our procedure on three image-to-image regression
tasks: quantitative phase microscopy, accelerated magnetic resonance imaging,
and super-resolution transmission electron microscopy of a Drosophila
melanogaster brain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diverse Image Generation via Self-Conditioned GANs. (arXiv:2006.10728v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.10728">
<div class="article-summary-box-inner">
<span><p>We introduce a simple but effective unsupervised method for generating
realistic and diverse images. We train a class-conditional GAN model without
using manually annotated class labels. Instead, our model is conditional on
labels automatically derived from clustering in the discriminator's feature
space. Our clustering step automatically discovers diverse modes, and
explicitly requires the generator to cover them. Experiments on standard mode
collapse benchmarks show that our method outperforms several competing methods
when addressing mode collapse. Our method also performs well on large-scale
datasets such as ImageNet and Places365, improving both image diversity and
standard quality metrics, compared to previous methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Real-Time Predictive Pedestrian Collision Warning Service for Cooperative Intelligent Transportation Systems Using 3D Pose Estimation. (arXiv:2009.10868v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.10868">
<div class="article-summary-box-inner">
<span><p>Minimizing traffic accidents between vehicles and pedestrians is one of the
primary research goals in intelligent transportation systems. To achieve the
goal, pedestrian orientation recognition and prediction of pedestrian's
crossing or not-crossing intention play a central role. Contemporary approaches
do not guarantee satisfactory performance due to limited field-of-view, lack of
generalization, and high computational complexity. To overcome these
limitations, we propose a real-time predictive pedestrian collision warning
service (P2CWS) for two tasks: pedestrian orientation recognition (100.53 FPS)
and intention prediction (35.76 FPS). Our framework obtains satisfying
generalization over multiple sites because of the proposed site-independent
features. At the center of the feature extraction lies 3D pose estimation. The
3D pose analysis enables robust and accurate recognition of pedestrian
orientations and prediction of intentions over multiple sites. The proposed
vision framework realizes 89.3% accuracy in the behavior recognition task on
the TUD dataset without any training process and 91.28% accuracy in intention
prediction on our dataset achieving new state-of-the-art performance. To
contribute to the corresponding research community, we make our source codes
public which are available at https://github.com/Uehwan/VisionForPedestrian
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Complex-valued Iris Recognition Network. (arXiv:2011.11198v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.11198">
<div class="article-summary-box-inner">
<span><p>In this work, we design a fully complex-valued neural network for the task of
iris recognition. Unlike the problem of general object recognition, where
real-valued neural networks can be used to extract pertinent features, iris
recognition depends on the extraction of both phase and magnitude information
from the input iris texture in order to better represent its biometric content.
This necessitates the extraction and processing of phase information that
cannot be effectively handled by a real-valued neural network. In this regard,
we design a fully complex-valued neural network that can better capture the
multi-scale, multi-resolution, and multi-orientation phase and amplitude
features of the iris texture. We show a strong correspondence of the proposed
complex-valued iris recognition network with Gabor wavelets that are used to
generate the classical IrisCode; however, the proposed method enables a new
capability of automatic complex-valued feature learning that is tailored for
iris recognition. We conduct experiments on three benchmark datasets -
ND-CrossSensor-2013, CASIA-Iris-Thousand and UBIRIS.v2 - and show the benefit
of the proposed network for the task of iris recognition. We exploit
visualization schemes to convey how the complex-valued network, when compared
to standard real-valued networks, extracts fundamentally different features
from the iris texture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing and Improving Adversarial Training for Generative Modeling. (arXiv:2012.06568v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.06568">
<div class="article-summary-box-inner">
<span><p>We study a new generative modeling technique based on adversarial training
(AT). We show that in a setting where the model is trained to discriminate
in-distribution data from adversarial examples perturbed from out-distribution
samples, the model learns the support of the in-distribution data. The learning
process is also closely related to MCMC-based maximum likelihood learning of
energy-based models (EBMs), and can be considered as an approximate maximum
likelihood learning method. We show that this AT generative model achieves
competitive image generation performance to state-of-the-art EBMs, and at the
same time is stable to train and has better sampling efficiency. We demonstrate
that the AT generative model is well-suited for the task of image translation
and worst-case out-of-distribution detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Online Body Schema Adaptation through Cost-Sensitive Active Learning. (arXiv:2101.10892v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.10892">
<div class="article-summary-box-inner">
<span><p>Humanoid robots have complex bodies and kinematic chains with several
Degrees-of-Freedom (DoF) which are difficult to model. Learning the parameters
of a kinematic model can be achieved by observing the position of the robot
links during prospective motions and minimising the prediction errors. This
work proposes a movement efficient approach for estimating online the
body-schema of a humanoid robot arm in the form of Denavit-Hartenberg (DH)
parameters. A cost-sensitive active learning approach based on the A-Optimality
criterion is used to select optimal joint configurations. The chosen joint
configurations simultaneously minimise the error in the estimation of the body
schema and minimise the movement between samples. This reduces energy
consumption, along with mechanical fatigue and wear, while not compromising the
learning accuracy. The work was implemented in a simulation environment, using
the 7DoF arm of the iCub robot simulator. The hand pose is measured with a
single camera via markers placed in the palm and back of the robot's hand. A
non-parametric occlusion model is proposed to avoid choosing joint
configurations where the markers are not visible, thus preventing worthless
attempts. The results show cost-sensitive active learning has similar accuracy
to the standard active learning approach, while reducing in about half the
executed movement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distilling Image Classifiers in Object Detectors. (arXiv:2106.05209v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05209">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation constitutes a simple yet effective way to improve the
performance of a compact student network by exploiting the knowledge of a more
powerful teacher. Nevertheless, the knowledge distillation literature remains
limited to the scenario where the student and the teacher tackle the same task.
Here, we investigate the problem of transferring knowledge not only across
architectures but also across tasks. To this end, we study the case of object
detection and, instead of following the standard detector-to-detector
distillation approach, introduce a classifier-to-detector knowledge transfer
framework. In particular, we propose strategies to exploit the classification
teacher to improve both the detector's recognition accuracy and localization
performance. Our experiments on several detectors with different backbones
demonstrate the effectiveness of our approach, allowing us to outperform the
state-of-the-art detector-to-detector distillation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning Image Recognition for Non-images. (arXiv:2106.14350v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.14350">
<div class="article-summary-box-inner">
<span><p>Powerful deep learning algorithms open an opportunity for solving non-image
Machine Learning (ML) problems by transforming these problems to into the image
recognition problems. The CPC-R algorithm presented in this chapter converts
non-image data into images by visualizing non-image data. Then deep learning
CNN algorithms solve the learning problems on these images. The design of the
CPC-R algorithm allows preserving all high-dimensional information in 2-D
images. The use of pair values mapping instead of single value mapping used in
the alternative approaches allows encoding each n-D point with 2 times fewer
visual elements. The attributes of an n-D point are divided into pairs of its
values and each pair is visualized as 2-D points in the same 2-D Cartesian
coordinates. Next, grey scale or color intensity values are assigned to each
pair to encode the order of pairs. This is resulted in the heatmap image. The
computational experiments with CPC-R are conducted for different CNN
architectures, and methods to optimize the CPC-R images showing that the
combined CPC-R and deep learning CNN algorithms are able to solve non-image ML
problems reaching high accuracy on the benchmark datasets. This chapter expands
our prior work by adding more experiments to test accuracy of classification,
exploring saliency and informativeness of discovered features to test their
interpretability, and generalizing the approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Convolutional Neural Bandit for Visual-aware Recommendation. (arXiv:2107.07438v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.07438">
<div class="article-summary-box-inner">
<span><p>Online recommendation/advertising is ubiquitous in web business. Image
displaying is considered as one of the most commonly used formats to interact
with customers. Contextual multi-armed bandit has shown success in the
application of advertising to solve the exploration-exploitation dilemma
existing in the recommendation procedure. Inspired by the visual-aware
recommendation, in this paper, we propose a contextual bandit algorithm, where
the convolutional neural network (CNN) is utilized to learn the reward function
along with an upper confidence bound (UCB) for exploration. We also prove a
near-optimal regret bound $\tilde{\mathcal{O}}(\sqrt{T})$ when the network is
over-parameterized, and establish strong connections with convolutional neural
tangent kernel (CNTK). Finally, we evaluate the empirical performance of the
proposed algorithm and show that it outperforms other state-of-the-art
UCB-based bandit algorithms on real-world image data sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Rail Component Detection Based on AttnConv-Net. (arXiv:2108.02423v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02423">
<div class="article-summary-box-inner">
<span><p>The automatic detection of major rail components using railway images is
beneficial to ensure the rail transport safety. In this paper, we propose an
attention-powered deep convolutional network (AttnConv-net) to detect multiple
rail components including the rail, clips, and bolts. The proposed method
consists of a deep convolutional neural network (DCNN) as the backbone,
cascading attention blocks (CAB), and two feed forward networks (FFN). Two
types of positional embedding are applied to enrich information in latent
features extracted from the backbone. Based on processed latent features, the
CAB aims to learn the local context of rail components including their
categories and component boundaries. Final categories and bounding boxes are
generated via two FFN implemented in parallel. To enhance the detection of
small components, various data augmentation methods are employed in the
training process. The effectiveness of the proposed AttnConv-net is validated
with one real dataset and another synthesized dataset. Compared with classic
convolutional neural network based methods, our proposed method simplifies the
detection pipeline by eliminating the need of prior- and post-processing, which
offers a new speed-quality solution to enable faster and more accurate
image-based rail component detections
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Single-Image Defocus Deblurring: How Dual-Pixel Images Help Through Multi-Task Learning. (arXiv:2108.05251v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.05251">
<div class="article-summary-box-inner">
<span><p>Many camera sensors use a dual-pixel (DP) design that operates as a
rudimentary light field providing two sub-aperture views of a scene in a single
capture. The DP sensor was developed to improve how cameras perform autofocus.
Since the DP sensor's introduction, researchers have found additional uses for
the DP data, such as depth estimation, reflection removal, and defocus
deblurring. We are interested in the latter task of defocus deblurring. In
particular, we propose a single-image deblurring network that incorporates the
two sub-aperture views into a multi-task framework. Specifically, we show that
jointly learning to predict the two DP views from a single blurry input image
improves the network's ability to learn to deblur the image. Our experiments
show this multi-task strategy achieves +1dB PSNR improvement over
state-of-the-art defocus deblurring methods. In addition, our multi-task
framework allows accurate DP-view synthesis (e.g., ~39dB PSNR) from the single
input image. These high-quality DP views can be used for other DP-based
applications, such as reflection removal. As part of this effort, we have
captured a new dataset of 7,059 high-quality images to support our training for
the DP-view synthesis task. Our dataset, code, and trained models are publicly
available at
https://github.com/Abdullah-Abuolaim/multi-task-defocus-deblurring-dual-pixel-nimat.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EKTVQA: Generalized use of External Knowledge to empower Scene Text in Text-VQA. (arXiv:2108.09717v7 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09717">
<div class="article-summary-box-inner">
<span><p>The open-ended question answering task of Text-VQA often requires reading and
reasoning about rarely seen or completely unseen scene-text content of an
image. We address this zero-shot nature of the problem by proposing the
generalized use of external knowledge to augment our understanding of the scene
text. We design a framework to extract, validate, and reason with knowledge
using a standard multimodal transformer for vision language understanding
tasks. Through empirical evidence and qualitative results, we demonstrate how
external knowledge can highlight instance-only cues and thus help deal with
training data bias, improve answer entity type correctness, and detect
multiword named entities. We generate results comparable to the
state-of-the-art on three publicly available datasets, under the constraints of
similar upstream OCR systems and training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active label cleaning for improved dataset quality under resource constraints. (arXiv:2109.00574v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00574">
<div class="article-summary-box-inner">
<span><p>Imperfections in data annotation, known as label noise, are detrimental to
the training of machine learning models and have an often-overlooked
confounding effect on the assessment of model performance. Nevertheless,
employing experts to remove label noise by fully re-annotating large datasets
is infeasible in resource-constrained settings, such as healthcare. This work
advocates for a data-driven approach to prioritising samples for re-annotation
- which we term "active label cleaning". We propose to rank instances according
to estimated label correctness and labelling difficulty of each sample, and
introduce a simulation framework to evaluate relabelling efficacy. Our
experiments on natural images and on a new medical imaging benchmark show that
cleaning noisy labels mitigates their negative impact on model training,
evaluation, and selection. Crucially, the proposed active label cleaning
enables correcting labels up to 4 times more effectively than typical random
selection in realistic conditions, making better use of experts' valuable time
for improving dataset quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spiking Neural Networks for Visual Place Recognition via Weighted Neuronal Assignments. (arXiv:2109.06452v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06452">
<div class="article-summary-box-inner">
<span><p>Spiking neural networks (SNNs) offer both compelling potential advantages,
including energy efficiency and low latencies and challenges including the
non-differentiable nature of event spikes. Much of the initial research in this
area has converted deep neural networks to equivalent SNNs, but this conversion
approach potentially negates some of the advantages of SNN-based approaches
developed from scratch. One promising area for high-performance SNNs is
template matching and image recognition. This research introduces the first
high-performance SNN for the Visual Place Recognition (VPR) task: given a query
image, the SNN has to find the closest match out of a list of reference images.
At the core of this new system is a novel assignment scheme that implements a
form of ambiguity-informed salience, by up-weighting single-place-encoding
neurons and down-weighting "ambiguous" neurons that respond to multiple
different reference places. In a range of experiments on the challenging
Nordland, Oxford RobotCar, SPEDTest, Synthia, and St Lucia datasets, we show
that our SNN achieves comparable VPR performance to state-of-the-art and
classical techniques, and degrades gracefully in performance with an increasing
number of reference places. Our results provide a significant milestone towards
SNNs that can provide robust, energy-efficient, and low latency robot
localization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Which Shortcut Cues Will DNNs Choose? A Study from the Parameter-Space Perspective. (arXiv:2110.03095v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03095">
<div class="article-summary-box-inner">
<span><p>Deep neural networks (DNNs) often rely on easy-to-learn discriminatory
features, or cues, that are not necessarily essential to the problem at hand.
For example, ducks in an image may be recognized based on their typical
background scenery, such as lakes or streams. This phenomenon, also known as
shortcut learning, is emerging as a key limitation of the current generation of
machine learning models. In this work, we introduce a set of experiments to
deepen our understanding of shortcut learning and its implications. We design a
training setup with several shortcut cues, named WCST-ML, where each cue is
equally conducive to the visual recognition problem at hand. Even under equal
opportunities, we observe that (1) certain cues are preferred to others, (2)
solutions biased to the easy-to-learn cues tend to converge to relatively flat
minima on the loss surface, and (3) the solutions focusing on those preferred
cues are far more abundant in the parameter space. We explain the abundance of
certain cues via their Kolmogorov (descriptional) complexity: solutions
corresponding to Kolmogorov-simple cues are abundant in the parameter space and
are thus preferred by DNNs. Our studies are based on the synthetic dataset
DSprites and the face dataset UTKFace. In our WCST-ML, we observe that the
inborn bias of models leans toward simple cues, such as color and ethnicity.
Our findings emphasize the importance of active human intervention to remove
the inborn model biases that may cause negative societal impacts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Color To Identify Insider Threats. (arXiv:2111.13176v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13176">
<div class="article-summary-box-inner">
<span><p>Insider threats are costly, hard to detect, and unfortunately rising in
occurrence. Seeking to improve detection of such threats, we develop novel
techniques to enable us to extract powerful features and augment attack vectors
for greater classification power. Most importantly, we generate high quality
color image encodings of user behavior that do not have the downsides of
traditional greyscale image encodings. Combined, they form Computer Vision User
and Entity Behavior Analytics, a detection system designed from the ground up
to improve upon advancements in academia and mitigate the issues that prevent
the usage of advanced models in industry. The proposed system beats
state-of-art methods used in academia and as well as in industry on a gold
standard benchmarking dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Vision Transformers for Incremental Learning. (arXiv:2112.06103v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06103">
<div class="article-summary-box-inner">
<span><p>This paper studies using Vision Transformers (ViT) in class incremental
learning. Surprisingly, naive application of ViT to replace convolutional
neural networks (CNNs) results in performance degradation. Our analysis reveals
three issues of naively using ViT: (a) ViT has very slow convergence when class
number is small, (b) more bias towards new classes is observed in ViT than
CNN-based models, and (c) the proper learning rate of ViT is too low to learn a
good classifier. Base on this analysis, we show these issues can be simply
addressed by using existing techniques: using convolutional stem, balanced
finetuning to correct bias, and higher learning rate for the classifier. Our
simple solution, named ViTIL (ViT for Incremental Learning), achieves the new
state-of-the-art for all three class incremental learning setups by a clear
margin, providing a strong baseline for the research community. For instance,
on ImageNet-1000, our ViTIL achieves 69.20% top-1 accuracy for the protocol of
500 initial classes with 5 incremental steps (100 new classes for each),
outperforming LUCIR+DDE by 1.69%. For more challenging protocol of 10
incremental steps (100 new classes), our method outperforms PODNet by 7.27%
(65.13% vs. 57.86%).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Birds Eye View Social Distancing Analysis System. (arXiv:2112.07159v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07159">
<div class="article-summary-box-inner">
<span><p>Social distancing can reduce the infection rates in respiratory pandemics
such as COVID-19. Traffic intersections are particularly suitable for
monitoring and evaluation of social distancing behavior in metropolises. We
propose and evaluate a privacy-preserving social distancing analysis system
(B-SDA), which uses bird's-eye view video recordings of pedestrians who cross
traffic intersections. We devise algorithms for video pre-processing, object
detection and tracking which are rooted in the known computer-vision and deep
learning techniques, but modified to address the problem of detecting very
small objects/pedestrians captured by a highly elevated camera. We propose a
method for incorporating pedestrian grouping for detection of social distancing
violations. B-SDA is used to compare pedestrian behavior based on pre-pandemic
and pandemic videos in a major metropolitan area. The accomplished pedestrian
detection performance is $63.0\%$ $AP_{50}$ and the tracking performance is
$47.6\%$ MOTA. The social distancing violation rate of $15.6\%$ during the
pandemic is notably lower than $31.4\%$ pre-pandemic baseline, indicating that
pedestrians followed CDC-prescribed social distancing recommendations. The
proposed system is suitable for deployment in real-world applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond ImageNet Attack: Towards Crafting Adversarial Examples for Black-box Domains. (arXiv:2201.11528v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11528">
<div class="article-summary-box-inner">
<span><p>Adversarial examples have posed a severe threat to deep neural networks due
to their transferable nature. Currently, various works have paid great efforts
to enhance the cross-model transferability, which mostly assume the substitute
model is trained in the same domain as the target model. However, in reality,
the relevant information of the deployed model is unlikely to leak. Hence, it
is vital to build a more practical black-box threat model to overcome this
limitation and evaluate the vulnerability of deployed models. In this paper,
with only the knowledge of the ImageNet domain, we propose a Beyond ImageNet
Attack (BIA) to investigate the transferability towards black-box domains
(unknown classification tasks). Specifically, we leverage a generative model to
learn the adversarial function for disrupting low-level features of input
images. Based on this framework, we further propose two variants to narrow the
gap between the source and target domains from the data and model perspectives,
respectively. Extensive experiments on coarse-grained and fine-grained domains
demonstrate the effectiveness of our proposed methods. Notably, our methods
outperform state-of-the-art approaches by up to 7.71\% (towards coarse-grained
domains) and 25.91\% (towards fine-grained domains) on average. Our code is
available at \url{https://github.com/qilong-zhang/Beyond-ImageNet-Attack}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Trajectory Forecasting from Detection with Uncertainty-Aware Motion Encoding. (arXiv:2202.01478v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01478">
<div class="article-summary-box-inner">
<span><p>Trajectory forecasting is critical for autonomous platforms to make safe
planning and actions. Currently, most trajectory forecasting methods assume
that object trajectories have been extracted and directly develop trajectory
predictors based on the ground truth trajectories. However, this assumption
does not hold in practical situations. Trajectories obtained from object
detection and tracking are inevitably noisy, which could cause serious
forecasting errors to predictors built on ground truth trajectories. In this
paper, we propose a trajectory predictor directly based on detection results
without relying on explicitly formed trajectories. Different from the
traditional methods which encode the motion cue of an agent based on its
clearly defined trajectory, we extract the motion information only based on the
affinity cues among detection results, in which an affinity-aware state update
mechanism is designed to take the uncertainty of association into account. In
addition, considering that there could be multiple plausible matching
candidates, we aggregate the states of them. This design relaxes the
undesirable effect of noisy trajectory obtained from data association.
Extensive ablation experiments validate the effectiveness of our method and its
generalization ability on different detectors. Cross-comparison to other
forecasting schemes further proves the superiority of our method. Code will be
released upon acceptance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Layer-wise Regularized Adversarial Training using Layers Sustainability Analysis (LSA) framework. (arXiv:2202.02626v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02626">
<div class="article-summary-box-inner">
<span><p>Deep neural network models are used today in various applications of
artificial intelligence, the strengthening of which, in the face of adversarial
attacks is of particular importance. An appropriate solution to adversarial
attacks is adversarial training, which reaches a trade-off between robustness
and generalization. This paper introduces a novel framework (Layer
Sustainability Analysis (LSA)) for the analysis of layer vulnerability in a
given neural network in the scenario of adversarial attacks. LSA can be a
helpful toolkit to assess deep neural networks and to extend adversarial
training approaches towards improving the sustainability of model layers via
layer monitoring and analysis. The LSA framework identifies a list of Most
Vulnerable Layers (MVL list) of a given network. The relative error, as a
comparison measure, is used to evaluate the representation sustainability of
each layer against adversarial attack inputs. The proposed approach for
obtaining robust neural networks to fend off adversarial attacks is based on a
layer-wise regularization (LR) over LSA proposal(s) for adversarial training
(AT); i.e. the AT-LR procedure. AT-LR could be used with any benchmark
adversarial attack to reduce the vulnerability of network layers and to improve
conventional adversarial training approaches. The proposed idea performs well
theoretically and experimentally for state-of-the-art multilayer perceptron and
convolutional neural network architectures. Compared with the AT-LR and its
corresponding base adversarial training, the classification accuracy of more
significant perturbations increased by 16.35%, 21.79%, and 10.730% on Moon,
MNIST, and CIFAR-10 benchmark datasets in comparison with the AT-LR and its
corresponding base adversarial training, respectively. The LSA framework is
available and published at https://github.com/khalooei/LSA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hybrid Contrastive Quantization for Efficient Cross-View Video Retrieval. (arXiv:2202.03384v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03384">
<div class="article-summary-box-inner">
<span><p>With the recent boom of video-based social platforms (e.g., YouTube and
TikTok), video retrieval using sentence queries has become an important demand
and attracts increasing research attention. Despite the decent performance,
existing text-video retrieval models in vision and language communities are
impractical for large-scale Web search because they adopt brute-force search
based on high-dimensional embeddings. To improve efficiency, Web search engines
widely apply vector compression libraries (e.g., FAISS) to post-process the
learned embeddings. Unfortunately, separate compression from feature encoding
degrades the robustness of representations and incurs performance decay. To
pursue a better balance between performance and efficiency, we propose the
first quantized representation learning method for cross-view video retrieval,
namely Hybrid Contrastive Quantization (HCQ). Specifically, HCQ learns both
coarse-grained and fine-grained quantizations with transformers, which provide
complementary understandings for texts and videos and preserve comprehensive
semantic information. By performing Asymmetric-Quantized Contrastive Learning
(AQ-CL) across views, HCQ aligns texts and videos at coarse-grained and
multiple fine-grained levels. This hybrid-grained learning strategy serves as
strong supervision on the cross-view video quantization model, where
contrastive learning at different levels can be mutually promoted. Extensive
experiments on three Web video benchmark datasets demonstrate that HCQ achieves
competitive performance with state-of-the-art non-compressed retrieval methods
while showing high efficiency in storage and computation. Code and
configurations are available at https://github.com/gimpong/WWW22-HCQ.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Motion-Aware Transformer For Occluded Person Re-identification. (arXiv:2202.04243v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04243">
<div class="article-summary-box-inner">
<span><p>Recently, occluded person re-identification(Re-ID) remains a challenging task
that people are frequently obscured by other people or obstacles, especially in
a crowd massing situation. In this paper, we propose a self-supervised deep
learning method to improve the location performance for human parts through
occluded person Re-ID. Unlike previous works, we find that motion information
derived from the photos of various human postures can help identify major human
body components. Firstly, a motion-aware transformer encoder-decoder
architecture is designed to obtain keypoints heatmaps and part-segmentation
maps. Secondly, an affine transformation module is utilized to acquire motion
information from the keypoint detection branch. Then the motion information
will support the segmentation branch to achieve refined human part segmentation
maps, and effectively divide the human body into reasonable groups. Finally,
several cases demonstrate the efficiency of the proposed model in
distinguishing different representative parts of the human body, which can
avoid the background and occlusion disturbs. Our method consistently achieves
state-of-the-art results on several popular datasets, including occluded,
partial, and holistic.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CRAT-Pred: Vehicle Trajectory Prediction with Crystal Graph Convolutional Neural Networks and Multi-Head Self-Attention. (arXiv:2202.04488v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04488">
<div class="article-summary-box-inner">
<span><p>Predicting the motion of surrounding vehicles is essential for autonomous
vehicles, as it governs their own motion plan. Current state-of-the-art vehicle
prediction models heavily rely on map information. In reality, however, this
information is not always available. We therefore propose CRAT-Pred, a
multi-modal and non-rasterization-based trajectory prediction model,
specifically designed to effectively model social interactions between
vehicles, without relying on map information. CRAT-Pred applies a graph
convolution method originating from the field of material science to vehicle
prediction, allowing to efficiently leverage edge features, and combines it
with multi-head self-attention. Compared to other map-free approaches, the
model achieves state-of-the-art performance with a significantly lower number
of model parameters. In addition to that, we quantitatively show that the
self-attention mechanism is able to learn social interactions between vehicles,
with the weights representing a measurable interaction score. The source code
is publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Structural Sparsity in Neural Image Compression. (arXiv:2202.04595v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04595">
<div class="article-summary-box-inner">
<span><p>Neural image compression have reached or out-performed traditional methods
(such as JPEG, BPG, WebP). However,their sophisticated network structures with
cascaded convolution layers bring heavy computational burden for practical
deployment. In this paper, we explore the structural sparsity in neural image
compression network to obtain real-time acceleration without any specialized
hardware design or algorithm. We propose a simple plug-in adaptive binary
channel masking(ABCM) to judge the importance of each convolution channel and
introduce sparsity during training. During inference, the unimportant channels
are pruned to obtain slimmer network and less computation. We implement our
method into three neural image compression networks with different entropy
models to verify its effectiveness and generalization, the experiment results
show that up to 7x computation reduction and 3x acceleration can be achieved
with negligible performance drop.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-02-11 23:06:56.154985559 UTC">2022-02-11 23:06:56 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>