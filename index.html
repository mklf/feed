<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-04-06T01:30:00Z">04-06</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Product Market Demand Analysis Using NLP in Banglish Text with Sentiment Analysis and Named Entity Recognition. (arXiv:2204.01827v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01827">
<div class="article-summary-box-inner">
<span><p>Product market demand analysis plays a significant role for originating
business strategies due to its noticeable impact on the competitive business
field. Furthermore, there are roughly 228 million native Bengali speakers, the
majority of whom use Banglish text to interact with one another on social
media. Consumers are buying and evaluating items on social media with Banglish
text as social media emerges as an online marketplace for entrepreneurs. People
use social media to find preferred smartphone brands and models by sharing
their positive and bad experiences with them. For this reason, our goal is to
gather Banglish text data and use sentiment analysis and named entity
identification to assess Bangladeshi market demand for smartphones in order to
determine the most popular smartphones by gender. We scraped product related
data from social media with instant data scrapers and crawled data from
Wikipedia and other sites for product information with python web scrapers.
Using Python's Pandas and Seaborn libraries, the raw data is filtered using NLP
methods. To train our datasets for named entity recognition, we utilized
Spacey's custom NER model, Amazon Comprehend Custom NER. A tensorflow
sequential model was deployed with parameter tweaking for sentiment analysis.
Meanwhile, we used the Google Cloud Translation API to estimate the gender of
the reviewers using the BanglaLinga library. In this article, we use natural
language processing (NLP) approaches and several machine learning models to
identify the most in-demand items and services in the Bangladeshi market. Our
model has an accuracy of 87.99% in Spacy Custom Named Entity recognition,
95.51% in Amazon Comprehend Custom NER, and 87.02% in the Sequential model for
demand analysis. After Spacy's study, we were able to manage 80% of mistakes
related to misspelled words using a mix of Levenshtein distance and ratio
algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Applying Automatic Text Summarization for Fake News Detection. (arXiv:2204.01841v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01841">
<div class="article-summary-box-inner">
<span><p>The distribution of fake news is not a new but a rapidly growing problem. The
shift to news consumption via social media has been one of the drivers for the
spread of misleading and deliberately wrong information, as in addition to it
of easy use there is rarely any veracity monitoring. Due to the harmful effects
of such fake news on society, the detection of these has become increasingly
important. We present an approach to the problem that combines the power of
transformer-based language models while simultaneously addressing one of their
inherent problems. Our framework, CMTR-BERT, combines multiple text
representations, with the goal of circumventing sequential limits and related
loss of information the underlying transformer architecture typically suffers
from. Additionally, it enables the incorporation of contextual information.
Extensive experiments on two very different, publicly available datasets
demonstrates that our approach is able to set new state-of-the-art performance
benchmarks. Apart from the benefit of using automatic text summarization
techniques we also find that the incorporation of contextual information
contributes to performance gains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compliance Checking with NLI: Privacy Policies vs. Regulations. (arXiv:2204.01845v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01845">
<div class="article-summary-box-inner">
<span><p>A privacy policy is a document that states how a company intends to handle
and manage their customers' personal data. One of the problems that arises with
these privacy policies is that their content might violate data privacy
regulations. Because of the enormous number of privacy policies that exist, the
only realistic way to check for legal inconsistencies in all of them is through
an automated method. In this work, we use Natural Language Inference (NLI)
techniques to compare privacy regulations against sections of privacy policies
from a selection of large companies. Our NLI model uses pre-trained embeddings,
along with BiLSTM in its attention mechanism. We tried two versions of our
model: one that was trained on the Stanford Natural Language Inference (SNLI)
and the second on the Multi-Genre Natural Language Inference (MNLI) dataset. We
found that our test accuracy was higher on our model trained on the SNLI, but
when actually doing NLI tasks on real world privacy policies, the model trained
on MNLI generalized and performed much better.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probabilistic Embeddings with Laplacian Graph Priors. (arXiv:2204.01846v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01846">
<div class="article-summary-box-inner">
<span><p>We introduce probabilistic embeddings using Laplacian priors (PELP). The
proposed model enables incorporating graph side-information into static word
embeddings. We theoretically show that the model unifies several previously
proposed embedding methods under one umbrella. PELP generalises graph-enhanced,
group, dynamic, and cross-lingual static word embeddings. PELP also enables any
combination of these previous models in a straightforward fashion. Furthermore,
we empirically show that our model matches the performance of previous models
as special cases. In addition, we demonstrate its flexibility by applying it to
the comparison of political sociolects over time. Finally, we provide code as a
TensorFlow implementation enabling flexible estimation in different settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Abusiveness Identification on Code-Mixed Social Media Text. (arXiv:2204.01848v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01848">
<div class="article-summary-box-inner">
<span><p>Social Media platforms have been seeing adoption and growth in their usage
over time. This growth has been further accelerated with the lockdown in the
past year when people's interaction, conversation, and expression were limited
physically. It is becoming increasingly important to keep the platform safe
from abusive content for better user experience. Much work has been done on
English social media content but text analysis on non-English social media is
relatively underexplored. Non-English social media content have the additional
challenges of code-mixing, transliteration and using different scripture in
same sentence. In this work, we propose an approach for abusiveness
identification on the multilingual Moj dataset which comprises of Indic
languages. Our approach tackles the common challenges of non-English social
media content and can be extended to other languages as well.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Text Summarization Methods: A Comprehensive Review. (arXiv:2204.01849v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01849">
<div class="article-summary-box-inner">
<span><p>One of the most pressing issues that have arisen due to the rapid growth of
the Internet is known as information overloading. Simplifying the relevant
information in the form of a summary will assist many people because the
material on any topic is plentiful on the Internet. Manually summarising
massive amounts of text is quite challenging for humans. So, it has increased
the need for more complex and powerful summarizers. Researchers have been
trying to improve approaches for creating summaries since the 1950s, such that
the machine-generated summary matches the human-created summary. This study
provides a detailed state-of-the-art analysis of text summarization concepts
such as summarization approaches, techniques used, standard datasets,
evaluation metrics and future scopes for research. The most commonly accepted
approaches are extractive and abstractive, studied in detail in this work.
Evaluating the summary and increasing the development of reusable resources and
infrastructure aids in comparing and replicating findings, adding competition
to improve the outcomes. Different evaluation methods of generated summaries
are also discussed in this study. Finally, at the end of this study, several
challenges and research opportunities related to text summarization research
are mentioned that may be useful for potential researchers working in this
area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deliberation Model for On-Device Spoken Language Understanding. (arXiv:2204.01893v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01893">
<div class="article-summary-box-inner">
<span><p>We propose a novel deliberation-based approach to end-to-end (E2E) spoken
language understanding (SLU), where a streaming automatic speech recognition
(ASR) model produces the first-pass hypothesis and a second-pass natural
language understanding (NLU) component generates the semantic parse by
conditioning on both ASR's text and audio embeddings. By formulating E2E SLU as
a generalized decoder, our system is able to support complex compositional
semantic structures. Furthermore, the sharing of parameters between ASR and NLU
makes the system especially suitable for resource-constrained (on-device)
environments; our proposed approach consistently outperforms strong pipeline
NLU baselines by 0.82% to 1.34% across various operating points on the spoken
version of the TOPv2 dataset. We demonstrate that the fusion of text and audio
features, coupled with the system's ability to rewrite the first-pass
hypothesis, makes our approach more robust to ASR errors. Finally, we show that
our approach can significantly reduce the degradation when moving from natural
speech to synthetic speech training, but more work is required to make
text-to-speech (TTS) a viable solution for scaling up E2E SLU.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynatask: A Framework for Creating Dynamic AI Benchmark Tasks. (arXiv:2204.01906v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01906">
<div class="article-summary-box-inner">
<span><p>We introduce Dynatask: an open source system for setting up custom NLP tasks
that aims to greatly lower the technical knowledge and effort required for
hosting and evaluating state-of-the-art NLP models, as well as for conducting
model in the loop data collection with crowdworkers. Dynatask is integrated
with Dynabench, a research platform for rethinking benchmarking in AI that
facilitates human and model in the loop data collection and evaluation. To
create a task, users only need to write a short task configuration file from
which the relevant web interfaces and model hosting infrastructure are
automatically generated. The system is available at https://dynabench.org/ and
the full library can be found at https://github.com/facebookresearch/dynabench.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Augmentation for Intent Classification with Off-the-shelf Large Language Models. (arXiv:2204.01959v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01959">
<div class="article-summary-box-inner">
<span><p>Data augmentation is a widely employed technique to alleviate the problem of
data scarcity. In this work, we propose a prompting-based approach to generate
labelled training data for intent classification with off-the-shelf language
models (LMs) such as GPT-3. An advantage of this method is that no
task-specific LM-fine-tuning for data generation is required; hence the method
requires no hyper-parameter tuning and is applicable even when the available
training data is very scarce. We evaluate the proposed method in a few-shot
setting on four diverse intent classification tasks. We find that GPT-generated
data significantly boosts the performance of intent classifiers when intents in
consideration are sufficiently distinct from each other. In tasks with
semantically close intents, we observe that the generated data is less helpful.
Our analysis shows that this is because GPT often generates utterances that
belong to a closely-related intent instead of the desired one. We present
preliminary evidence that a prompting-based GPT classifier could be helpful in
filtering the generated data to enhance its quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The COVMis-Stance dataset: Stance Detection on Twitter for COVID-19 Misinformation. (arXiv:2204.02000v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02000">
<div class="article-summary-box-inner">
<span><p>During the COVID-19 pandemic, large amounts of COVID-19 misinformation are
spreading on social media. We are interested in the stance of Twitter users
towards COVID-19 misinformation. However, due to the relative recent nature of
the pandemic, only a few stance detection datasets fit our task. We have
constructed a new stance dataset consisting of 2631 tweets annotated with the
stance towards COVID-19 misinformation. In contexts with limited labeled data,
we fine-tune our models by leveraging the MNLI dataset and two existing stance
detection datasets (RumourEval and COVIDLies), and evaluate the model
performance on our dataset. Our experimental results show that the model
performs the best when fine-tuned sequentially on the MNLI dataset and the
combination of the undersampled RumourEval and COVIDLies datasets. Our code and
dataset are publicly available at
https://github.com/yanfangh/covid-rumor-stance
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fact Checking with Insufficient Evidence. (arXiv:2204.02007v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02007">
<div class="article-summary-box-inner">
<span><p>Automating the fact checking (FC) process relies on information obtained from
external sources. In this work, we posit that it is crucial for FC models to
make veracity predictions only when there is sufficient evidence and otherwise
indicate when it is not enough. To this end, we are the first to study what
information FC models consider sufficient by introducing a novel task and
advancing it with three main contributions. First, we conduct an in-depth
empirical analysis of the task with a new fluency-preserving method for
omitting information from the evidence at the constituent and sentence level.
We identify when models consider the remaining evidence (in)sufficient for FC,
based on three trained models with different Transformer architectures and
three FC datasets. Second, we ask annotators whether the omitted evidence was
important for FC, resulting in a novel diagnostic dataset, SufficientFacts, for
FC with omitted evidence. We find that models are least successful in detecting
missing evidence when adverbial modifiers are omitted (21% accuracy), whereas
it is easiest for omitted date modifiers (63% accuracy). Finally, we propose a
novel data augmentation strategy for contrastive self-learning of missing
evidence by employing the proposed omission method combined with tri-training.
It improves performance for Evidence Sufficiency Prediction by up to 17.8 F1
score, which in turn improves FC performance by up to 2.6 F1 score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Complementary Joint Training Approach Using Unpaired Speech and Text for Low-Resource Automatic Speech Recognition. (arXiv:2204.02023v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02023">
<div class="article-summary-box-inner">
<span><p>Unpaired data has shown to be beneficial for low-resource automatic speech
recognition~(ASR), which can be involved in the design of hybrid models with
multi-task training or language model dependent pre-training. In this work, we
leverage unpaired data to train a general sequence-to-sequence model. Unpaired
speech and text are used in the form of data pairs by generating the
corresponding missing parts in prior to model training. Inspired by the
complementarity of speech-PseudoLabel pair and SynthesizedAudio-text pair in
both acoustic features and linguistic features, we propose a complementary
joint training~(CJT) method that trains a model alternatively with two data
pairs. Furthermore, label masking for pseudo-labels and gradient restriction
for synthesized audio are proposed to further cope with the deviations from
real data, termed as CJT++. Experimental results show that compared to
speech-only training, the proposed basic CJT achieves great performance
improvements on clean/other test sets, and the CJT++ re-training yields further
performance enhancements. It is also apparent that the proposed method
outperforms the wav2vec2.0 model with the same model size and beam size,
particularly in extreme low-resource cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">$\textit{latent}$-GLAT: Glancing at Latent Variables for Parallel Text Generation. (arXiv:2204.02030v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02030">
<div class="article-summary-box-inner">
<span><p>Recently, parallel text generation has received widespread attention due to
its success in generation efficiency. Although many advanced techniques are
proposed to improve its generation quality, they still need the help of an
autoregressive model for training to overcome the one-to-many multi-modal
phenomenon in the dataset, limiting their applications. In this paper, we
propose $\textit{latent}$-GLAT, which employs the discrete latent variables to
capture word categorical information and invoke an advanced curriculum learning
technique, alleviating the multi-modality problem. Experiment results show that
our method outperforms strong baselines without the help of an autoregressive
model, which further broadens the application scenarios of the parallel
decoding paradigm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HyperBox: A Supervised Approach for Hypernym Discovery using Box Embeddings. (arXiv:2204.02058v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02058">
<div class="article-summary-box-inner">
<span><p>Hypernymy plays a fundamental role in many AI tasks like taxonomy learning,
ontology learning, etc. This has motivated the development of many automatic
identification methods for extracting this relation, most of which rely on word
distribution. We present a novel model HyperBox to learn box embeddings for
hypernym discovery. Given an input term, HyperBox retrieves its suitable
hypernym from a target corpus. For this task, we use the dataset published for
SemEval 2018 Shared Task on Hypernym Discovery. We compare the performance of
our model on two specific domains of knowledge: medical and music.
Experimentally, we show that our model outperforms existing methods on the
majority of the evaluation metrics. Moreover, our model generalize well over
unseen hypernymy pairs using only a small set of training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Design considerations for a hierarchical semantic compositional framework for medical natural language understanding. (arXiv:2204.02067v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02067">
<div class="article-summary-box-inner">
<span><p>Medical natural language processing (NLP) systems are a key enabling
technology for transforming Big Data from clinical report repositories to
information used to support disease models and validate intervention methods.
However, current medical NLP systems fall considerably short when faced with
the task of logically interpreting clinical text. In this paper, we describe a
framework inspired by mechanisms of human cognition in an attempt to jump the
NLP performance curve. The design centers about a hierarchical semantic
compositional model (HSCM) which provides an internal substrate for guiding the
interpretation process. The paper describes insights from four key cognitive
aspects including semantic memory, semantic composition, semantic activation,
and hierarchical predictive coding. We discuss the design of a generative
semantic model and an associated semantic parser used to transform a free-text
sentence into a logical representation of its meaning. The paper discusses
supportive and antagonistic arguments for the key features of the architecture
as a long-term foundational framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How do media talk about the Covid-19 pandemic? Metaphorical thematic clustering in Italian online newspapers. (arXiv:2204.02106v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02106">
<div class="article-summary-box-inner">
<span><p>The contribution presents a study on figurative language of the first months
of the COVID-19 crisis in Italian online newspapers. Particularly, we contrast
topics and metaphorical language used by journalists in the first and second
phase of the government response to the pandemic in Spring 2020. The analysis
is conducted on a journalistic corpus collected between February 24th and June
3rd, 2020. The analysis is performed using both quantitative and qualitative
approaches, combining Structural Topic Modelling (Roberts et al. 2016),
Conceptual Metaphor Theory (Lakoff &amp; Johnson, 1980), and qualitative-corpus
based metaphor analysis (Charteris-Black, 2004). We find a significant shift in
topics discussed across Phase 1 and Phase 2, and interesting overlaps in
topic-specific metaphors. Using qualitative corpus analysis, we present a more
in-depth case study discussing metaphorical collocations of the topics of
Economy and Society
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved and Efficient Conversational Slot Labeling through Question Answering. (arXiv:2204.02123v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02123">
<div class="article-summary-box-inner">
<span><p>Transformer-based pretrained language models (PLMs) offer unmatched
performance across the majority of natural language understanding (NLU) tasks,
including a body of question answering (QA) tasks. We hypothesize that
improvements in QA methodology can also be directly exploited in dialog NLU;
however, dialog tasks must be \textit{reformatted} into QA tasks. In
particular, we focus on modeling and studying \textit{slot labeling} (SL), a
crucial component of NLU for dialog, through the QA optics, aiming to improve
both its performance and efficiency, and make it more effective and resilient
to working with limited task data. To this end, we make a series of
contributions: 1) We demonstrate how QA-tuned PLMs can be applied to the SL
task, reaching new state-of-the-art performance, with large gains especially
pronounced in such low-data regimes. 2) We propose to leverage contextual
information, required to tackle ambiguous values, simply through natural
language. 3) Efficiency and compactness of QA-oriented fine-tuning are boosted
through the use of lightweight yet effective adapter modules. 4) Trading-off
some of the quality of QA datasets for their size, we experiment with larger
automatically generated QA datasets for QA-tuning, arriving at even higher
performance. Finally, our analysis suggests that our novel QA-based slot
labeling models, supported by the PLMs, reach a performance ceiling in
high-data regimes, calling for more challenging and more nuanced benchmarks in
future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Anchors' Opinion in Hinghlish News Delivery. (arXiv:2204.02155v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02155">
<div class="article-summary-box-inner">
<span><p>Humans like to express their opinions and crave the opinions of others.
Mining and detecting opinions from various sources are beneficial to
individuals, organisations, and even governments. One such organisation is news
media, where a general norm is not to showcase opinions from their side.
Anchors are the face of the digital media, and it is required for them not to
be opinionated. However, at times, they diverge from the accepted norm and
insert their opinions into otherwise straightforward news reports, either
purposefully or unintentionally. This is primarily seen in debates as it
requires the anchors to be spontaneous, thus making them vulnerable to add
their opinions. The consequence of such mishappening might lead to biased news
or even supporting a certain agenda at the worst. To this end, we propose a
novel task of anchors' opinion detection in debates. We curate code-mixed news
debates and develop the ODIN dataset. A total of 2054 anchors' utterances in
the dataset are marked as opinionated or non-opinionated. Lastly, we propose
DetONADe, an interactive attention-based framework for classifying anchors'
utterances and obtain the best weighted-F1 score of 0.703. A thorough analysis
and evaluation show many interesting patterns in the dataset and predictions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilinguals at SemEval-2022 Task 11: Transformer Based Architecture for Complex NER. (arXiv:2204.02173v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02173">
<div class="article-summary-box-inner">
<span><p>We investigate the task of complex NER for the English language. The task is
non-trivial due to the semantic ambiguity of the textual structure and the
rarity of occurrence of such entities in the prevalent literature. Using
pre-trained language models such as BERT, we obtain a competitive performance
on this task. We qualitatively analyze the performance of multiple
architectures for this task. All our models are able to outperform the baseline
by a significant margin. Our best performing model beats the baseline F1-score
by over 9%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-View Transformer for 3D Visual Grounding. (arXiv:2204.02174v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02174">
<div class="article-summary-box-inner">
<span><p>The 3D visual grounding task aims to ground a natural language description to
the targeted object in a 3D scene, which is usually represented in 3D point
clouds. Previous works studied visual grounding under specific views. The
vision-language correspondence learned by this way can easily fail once the
view changes. In this paper, we propose a Multi-View Transformer (MVT) for 3D
visual grounding. We project the 3D scene to a multi-view space, in which the
position information of the 3D scene under different views are modeled
simultaneously and aggregated together. The multi-view space enables the
network to learn a more robust multi-modal representation for 3D visual
grounding and eliminates the dependence on specific views. Extensive
experiments show that our approach significantly outperforms all
state-of-the-art methods. Specifically, on Nr3D and Sr3D datasets, our method
outperforms the best competitor by 11.2% and 7.1% and even surpasses recent
work with extra 2D assistance by 5.9% and 6.6%. Our code is available at
https://github.com/sega-hsj/MVT-3DVG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Abstractive summarization of hospitalisation histories with transformer networks. (arXiv:2204.02208v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02208">
<div class="article-summary-box-inner">
<span><p>In this paper we present a novel approach to abstractive summarization of
patient hospitalisation histories. We applied an encoder-decoder framework with
Longformer neural network as an encoder and BERT as a decoder. Our experiments
show improved quality on some summarization tasks compared with
pointer-generator networks. We also conducted a study with experienced
physicians evaluating the results of our model in comparison with PGN baseline
and human-generated abstracts, which showed the effectiveness of our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EntSUM: A Data Set for Entity-Centric Summarization. (arXiv:2204.02213v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02213">
<div class="article-summary-box-inner">
<span><p>Controllable summarization aims to provide summaries that take into account
user-specified aspects and preferences to better assist them with their
information need, as opposed to the standard summarization setup which build a
single generic summary of a document. We introduce a human-annotated data set
EntSUM for controllable summarization with a focus on named entities as the
aspects to control. We conduct an extensive quantitative analysis to motivate
the task of entity-centric summarization and show that existing methods for
controllable summarization fail to generate entity-centric summaries. We
propose extensions to state-of-the-art summarization approaches that achieve
substantially better results on our data set. Our analysis and results show the
challenging nature of this task and of the proposed data set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Generalizability in Implicitly Abusive Language Detection with Concept Activation Vectors. (arXiv:2204.02261v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02261">
<div class="article-summary-box-inner">
<span><p>Robustness of machine learning models on ever-changing real-world data is
critical, especially for applications affecting human well-being such as
content moderation. New kinds of abusive language continually emerge in online
discussions in response to current events (e.g., COVID-19), and the deployed
abuse detection systems should be updated regularly to remain accurate. In this
paper, we show that general abusive language classifiers tend to be fairly
reliable in detecting out-of-domain explicitly abusive utterances but fail to
detect new types of more subtle, implicit abuse. Next, we propose an
interpretability technique, based on the Testing Concept Activation Vector
(TCAV) method from computer vision, to quantify the sensitivity of a trained
model to the human-defined concepts of explicit and implicit abusive language,
and use that to explain the generalizability of the model on new data, in this
case, COVID-related anti-Asian hate speech. Extending this technique, we
introduce a novel metric, Degree of Explicitness, for a single instance and
show that the new metric is beneficial in suggesting out-of-domain unlabeled
examples to effectively enrich the training data with informative, implicitly
abusive texts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual and Multimodal Abuse Detection. (arXiv:2204.02263v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02263">
<div class="article-summary-box-inner">
<span><p>The presence of abusive content on social media platforms is undesirable as
it severely impedes healthy and safe social media interactions. While automatic
abuse detection has been widely explored in textual domain, audio abuse
detection still remains unexplored. In this paper, we attempt abuse detection
in conversational audio from a multimodal perspective in a multilingual social
media setting. Our key hypothesis is that along with the modelling of audio,
incorporating discriminative information from other modalities can be highly
beneficial for this task. Our proposed method, MADA, explicitly focuses on two
modalities other than the audio itself, namely, the underlying emotions
expressed in the abusive audio and the semantic information encapsulated in the
corresponding textual form. Observations prove that MADA demonstrates gains
over audio-only approaches on the ADIMA dataset. We test the proposed approach
on 10 different languages and observe consistent gains in the range 0.6%-5.2%
by leveraging multiple modalities. We also perform extensive ablation
experiments for studying the contributions of every modality and observe the
best results while leveraging all the modalities together. Additionally, we
perform experiments to empirically confirm that there is a strong correlation
between underlying emotions and abusive behaviour.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Repeat after me: Self-supervised learning of acoustic-to-articulatory mapping by vocal imitation. (arXiv:2204.02269v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02269">
<div class="article-summary-box-inner">
<span><p>We propose a computational model of speech production combining a pre-trained
neural articulatory synthesizer able to reproduce complex speech stimuli from a
limited set of interpretable articulatory parameters, a DNN-based internal
forward model predicting the sensory consequences of articulatory commands, and
an internal inverse model based on a recurrent neural network recovering
articulatory commands from the acoustic speech input. Both forward and inverse
models are jointly trained in a self-supervised way from raw acoustic-only
speech data from different speakers. The imitation simulations are evaluated
objectively and subjectively and display quite encouraging performances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SwapMix: Diagnosing and Regularizing the Over-Reliance on Visual Context in Visual Question Answering. (arXiv:2204.02285v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02285">
<div class="article-summary-box-inner">
<span><p>While Visual Question Answering (VQA) has progressed rapidly, previous works
raise concerns about robustness of current VQA models. In this work, we study
the robustness of VQA models from a novel perspective: visual context. We
suggest that the models over-rely on the visual context, i.e., irrelevant
objects in the image, to make predictions. To diagnose the model's reliance on
visual context and measure their robustness, we propose a simple yet effective
perturbation technique, SwapMix. SwapMix perturbs the visual context by
swapping features of irrelevant context objects with features from other
objects in the dataset. Using SwapMix we are able to change answers to more
than 45 % of the questions for a representative VQA model. Additionally, we
train the models with perfect sight and find that the context over-reliance
highly depends on the quality of visual representations. In addition to
diagnosing, SwapMix can also be applied as a data augmentation strategy during
training in order to regularize the context over-reliance. By swapping the
context object features, the model reliance on context can be suppressed
effectively. Two representative VQA models are studied using SwapMix: a
co-attention model MCAN and a large-scale pretrained model LXMERT. Our
experiments on the popular GQA dataset show the effectiveness of SwapMix for
both diagnosing model robustness and regularizing the over-reliance on visual
context. The code for our method is available at
https://github.com/vipulgupta1011/swapmix
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parameter-Efficient Neural Reranking for Cross-Lingual and Multilingual Retrieval. (arXiv:2204.02292v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02292">
<div class="article-summary-box-inner">
<span><p>State-of-the-art neural (re)rankers are notoriously data hungry which - given
the lack of large-scale training data in languages other than English - makes
them rarely used in multilingual and cross-lingual retrieval settings. Current
approaches therefore typically transfer rankers trained on English data to
other languages and cross-lingual setups by means of multilingual encoders:
they fine-tune all the parameters of a pretrained massively multilingual
Transformer (MMT, e.g., multilingual BERT) on English relevance judgments and
then deploy it in the target language. In this work, we show that two
parameter-efficient approaches to cross-lingual transfer, namely Sparse
Fine-Tuning Masks (SFTMs) and Adapters, allow for a more lightweight and more
effective zero-shot transfer to multilingual and cross-lingual retrieval tasks.
We first train language adapters (or SFTMs) via Masked Language Modelling and
then train retrieval (i.e., reranking) adapters (SFTMs) on top while keeping
all other parameters fixed. At inference, this modular design allows us to
compose the ranker by applying the task adapter (or SFTM) trained with source
language data together with the language adapter (or SFTM) of a target
language. Besides improved transfer performance, these two approaches offer
faster ranker training, with only a fraction of parameters being updated
compared to full MMT fine-tuning. We benchmark our models on the CLEF-2003
benchmark, showing that our parameter-efficient methods outperform standard
zero-shot transfer with full MMT fine-tuning, while enabling modularity and
reducing training times. Further, we show on the example of Swahili and Somali
that, for low(er)-resource languages, our parameter-efficient neural re-rankers
can improve the ranking of the competitive machine translation-based ranker.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PaLM: Scaling Language Modeling with Pathways. (arXiv:2204.02311v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02311">
<div class="article-summary-box-inner">
<span><p>Large language models have been shown to achieve remarkable performance
across a variety of natural language tasks using few-shot learning, which
drastically reduces the number of task-specific training examples needed to
adapt the model to a particular application. To further our understanding of
the impact of scale on few-shot learning, we trained a 540-billion parameter,
densely activated, Transformer language model, which we call Pathways Language
Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML
system which enables highly efficient training across multiple TPU Pods. We
demonstrate continued benefits of scaling by achieving state-of-the-art
few-shot learning results on hundreds of language understanding and generation
benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough
performance, outperforming the finetuned state-of-the-art on a suite of
multi-step reasoning tasks, and outperforming average human performance on the
recently released BIG-bench benchmark. A significant number of BIG-bench tasks
showed discontinuous improvements from model scale, meaning that performance
steeply increased as we scaled to our largest model. PaLM also has strong
capabilities in multilingual tasks and source code generation, which we
demonstrate on a wide array of benchmarks. We additionally provide a
comprehensive analysis on bias and toxicity, and study the extent of training
data memorization with respect to model scale. Finally, we discuss the ethical
considerations related to large language models and discuss potential
mitigation strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can language models learn from explanations in context?. (arXiv:2204.02329v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02329">
<div class="article-summary-box-inner">
<span><p>Large language models can perform new tasks by adapting to a few in-context
examples. For humans, rapid learning from examples can benefit from
explanations that connect examples to task principles. We therefore investigate
whether explanations of few-shot examples can allow language models to adapt
more effectively. We annotate a set of 40 challenging tasks from BIG-Bench with
explanations of answers to a small subset of questions, as well as a variety of
matched control explanations. We evaluate the effects of various zero-shot and
few-shot prompts that include different types of explanations, instructions,
and controls on the performance of a range of large language models. We analyze
these results using statistical multilevel modeling techniques that account for
the nested dependencies among conditions, tasks, prompts, and models. We find
that explanations of examples can improve performance. Adding untuned
explanations to a few-shot prompt offers a modest improvement in performance;
about 1/3 the effect size of adding few-shot examples, but twice the effect
size of task instructions. We then show that explanations tuned for performance
on a small validation set offer substantially larger benefits; building a
prompt by selecting examples and explanations together substantially improves
performance over selecting examples alone. Hand-tuning explanations can
substantially improve performance on challenging tasks. Furthermore, even
untuned explanations outperform carefully matched controls, suggesting that the
benefits are due to the link between an example and its explanation, rather
than lower-level features of the language used. However, only large models can
benefit from explanations. In summary, explanations can support the in-context
learning abilities of large language models on
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Best Practices for Training Multilingual Dense Retrieval Models. (arXiv:2204.02363v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02363">
<div class="article-summary-box-inner">
<span><p>Dense retrieval models using a transformer-based bi-encoder design have
emerged as an active area of research. In this work, we focus on the task of
monolingual retrieval in a variety of typologically diverse languages using one
such design. Although recent work with multilingual transformers demonstrates
that they exhibit strong cross-lingual generalization capabilities, there
remain many open research questions, which we tackle here. Our study is
organized as a "best practices" guide for training multilingual dense retrieval
models, broken down into three main scenarios: where a multilingual transformer
is available, but relevance judgments are not available in the language of
interest; where both models and training data are available; and, where
training data are available not but models. In considering these scenarios, we
gain a better understanding of the role of multi-stage fine-tuning, the
strength of cross-lingual transfer under various conditions, the usefulness of
out-of-language data, and the advantages of multilingual vs. monolingual
transformers. Our recommendations offer a guide for practitioners building
search applications, particularly for low-resource languages, and while our
work leaves open a number of research questions, we provide a solid foundation
for future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLEVR-X: A Visual Reasoning Dataset for Natural Language Explanations. (arXiv:2204.02380v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02380">
<div class="article-summary-box-inner">
<span><p>Providing explanations in the context of Visual Question Answering (VQA)
presents a fundamental problem in machine learning. To obtain detailed insights
into the process of generating natural language explanations for VQA, we
introduce the large-scale CLEVR-X dataset that extends the CLEVR dataset with
natural language explanations. For each image-question pair in the CLEVR
dataset, CLEVR-X contains multiple structured textual explanations which are
derived from the original scene graphs. By construction, the CLEVR-X
explanations are correct and describe the reasoning and visual information that
is necessary to answer a given question. We conducted a user study to confirm
that the ground-truth explanations in our proposed dataset are indeed complete
and relevant. We present baseline results for generating natural language
explanations in the context of VQA using two state-of-the-art frameworks on the
CLEVR-X dataset. Furthermore, we provide a detailed analysis of the explanation
generation quality for different question and answer types. Additionally, we
study the influence of using different numbers of ground-truth explanations on
the convergence of natural language generation (NLG) metrics. The CLEVR-X
dataset is publicly available at
\url{https://explainableml.github.io/CLEVR-X/}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Memory limitations are hidden in grammar. (arXiv:1908.06629v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1908.06629">
<div class="article-summary-box-inner">
<span><p>The ability to produce and understand an unlimited number of different
sentences is a hallmark of human language. Linguists have sought to define the
essence of this generative capacity using formal grammars that describe the
syntactic dependencies between constituents, independent of the computational
limitations of the human brain. Here, we evaluate this independence assumption
by sampling sentences uniformly from the space of possible syntactic
structures. We find that the average dependency distance between syntactically
related words, a proxy for memory limitations, is less than expected by chance
in a collection of state-of-the-art classes of dependency grammars. Our
findings indicate that memory limitations have permeated grammatical
descriptions, suggesting that it may be impossible to build a parsimonious
theory of human linguistic productivity independent of non-linguistic cognitive
constraints.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning the grammar of drug prescription: recurrent neural network grammars for medication information extraction in clinical texts. (arXiv:2004.11622v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.11622">
<div class="article-summary-box-inner">
<span><p>In this study, we evaluated the RNNG, a neural top-down transition based
parser, for medication information extraction in clinical texts. We evaluated
this model on a French clinical corpus. The task was to extract the name of a
drug (or a drug class), as well as attributes informing its administration:
frequency, dosage, duration, condition and route of administration. We compared
the RNNG model that jointly identifies entities, events and their relations
with separate BiLSTMs models for entities, events and relations as baselines.
We call seq-BiLSTMs the baseline models for relations extraction that takes as
extra-input the output of the BiLSTMs for entities and events. Similarly, we
evaluated seq-RNNG, a hybrid RNNG model that takes as extra-input the output of
the BiLSTMs for entities and events. RNNG outperforms seq-BiLSTM for
identifying complex relations, with on average 88.1 [84.4-91.6] % versus 69.9
[64.0-75.4] F-measure. However, RNNG tends to be weaker than the baseline
BiLSTM on detecting entities, with on average 82.4 [80.8-83.8] versus 84.1
[82.7-85.6] % F- measure. RNNG trained only for detecting relations tends to be
weaker than RNNG with the joint modelling objective, 87.4% [85.8-88.8] versus
88.5% [87.2-89.8]. Seq-RNNG is on par with BiLSTM for entities (84.0
[82.6-85.4] % F-measure) and with RNNG for relations (88.7 [87.4-90.0] %
F-measure). The performance of RNNG on relations can be explained both by the
model architecture, which provides inductive bias to capture the hierarchy in
the targets, and the joint modeling objective which allows the RNNG to learn
richer representations. RNNG is efficient for modeling relations between
entities or/and events in medical texts and its performances are close to those
of a BiLSTM for entity and event detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scalable Optimal Transport in High Dimensions for Graph Distances, Embedding Alignment, and More. (arXiv:2107.06876v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.06876">
<div class="article-summary-box-inner">
<span><p>The current best practice for computing optimal transport (OT) is via entropy
regularization and Sinkhorn iterations. This algorithm runs in quadratic time
as it requires the full pairwise cost matrix, which is prohibitively expensive
for large sets of objects. In this work we propose two effective log-linear
time approximations of the cost matrix: First, a sparse approximation based on
locality-sensitive hashing (LSH) and, second, a Nystr\"om approximation with
LSH-based sparse corrections, which we call locally corrected Nystr\"om (LCN).
These approximations enable general log-linear time algorithms for
entropy-regularized OT that perform well even for the complex, high-dimensional
spaces common in deep learning. We analyse these approximations theoretically
and evaluate them experimentally both directly and end-to-end as a component
for real-world applications. Using our approximations for unsupervised word
embedding alignment enables us to speed up a state-of-the-art method by a
factor of 3 while also improving the accuracy by 3.1 percentage points without
any additional model changes. For graph distance regression we propose the
graph transport network (GTN), which combines graph neural networks (GNNs) with
enhanced Sinkhorn. GTN outcompetes previous models by 48% and still scales
log-linearly in the number of nodes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MarIA: Spanish Language Models. (arXiv:2107.07253v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.07253">
<div class="article-summary-box-inner">
<span><p>This work presents MarIA, a family of Spanish language models and associated
resources made available to the industry and the research community. Currently,
MarIA includes RoBERTa-base, RoBERTa-large, GPT2 and GPT2-large Spanish
language models, which can arguably be presented as the largest and most
proficient language models in Spanish. The models were pretrained using a
massive corpus of 570GB of clean and deduplicated texts with 135 billion words
extracted from the Spanish Web Archive crawled by the National Library of Spain
between 2009 and 2019. We assessed the performance of the models with nine
existing evaluation datasets and with a novel extractive Question Answering
dataset created ex novo. Overall, MarIA models outperform the existing Spanish
models across a variety of NLU tasks and training settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BROS: A Pre-trained Language Model Focusing on Text and Layout for Better Key Information Extraction from Documents. (arXiv:2108.04539v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04539">
<div class="article-summary-box-inner">
<span><p>Key information extraction (KIE) from document images requires understanding
the contextual and spatial semantics of texts in two-dimensional (2D) space.
Many recent studies try to solve the task by developing pre-trained language
models focusing on combining visual features from document images with texts
and their layout. On the other hand, this paper tackles the problem by going
back to the basic: effective combination of text and layout. Specifically, we
propose a pre-trained language model, named BROS (BERT Relying On Spatiality),
that encodes relative positions of texts in 2D space and learns from unlabeled
documents with area-masking strategy. With this optimized training scheme for
understanding texts in 2D space, BROS shows comparable or better performance
compared to previous methods on four KIE benchmarks (FUNSD, SROIE*, CORD, and
SciTSR) without relying on visual features. This paper also reveals two
real-world challenges in KIE tasks-(1) minimizing the error from incorrect text
ordering and (2) efficient learning from fewer downstream examples-and
demonstrates the superiority of BROS over previous methods. Code is available
at https://github.com/clovaai/bros.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fusing task-oriented and open-domain dialogues in conversational agents. (arXiv:2109.04137v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04137">
<div class="article-summary-box-inner">
<span><p>The goal of building intelligent dialogue systems has largely been separately
pursued under two paradigms: task-oriented dialogue (TOD) systems, which
perform goal-oriented functions, and open-domain dialogue (ODD) systems, which
focus on non-goal-oriented chitchat. The two dialogue modes can potentially be
intertwined together seamlessly in the same conversation, as easily done by a
friendly human assistant. Such ability is desirable in conversational agents,
as the integration makes them more accessible and useful. Our paper addresses
this problem of fusing TODs and ODDs in multi-turn dialogues. Based on the
popular TOD dataset MultiWOZ, we build a new dataset FusedChat, by rewriting
the existing TOD turns and adding new ODD turns. This procedure constructs
conversation sessions containing exchanges from both dialogue modes. It
features inter-mode contextual dependency, i.e., the dialogue turns from the
two modes depend on each other. Rich dependency patterns including co-reference
and ellipsis are features. The new dataset, with 60k new human-written ODD
turns and 5k re-written TOD turns, offers a benchmark to test a dialogue
model's ability to perform inter-mode conversations. This is a more challenging
task since the model has to determine the appropriate dialogue mode and
generate the response based on the inter-mode context. But such models would
better mimic human-level conversation capabilities. We evaluate baseline models
on this task, including classification-based two-stage models and two-in-one
fused models. We publicly release FusedChat and the baselines to propel future
work on inter-mode dialogue systems https://github.com/tomyoung903/FusedChat.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MoEfication: Transformer Feed-forward Layers are Mixtures of Experts. (arXiv:2110.01786v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01786">
<div class="article-summary-box-inner">
<span><p>Recent work has shown that feed-forward networks (FFNs) in pre-trained
Transformers are a key component, storing various linguistic and factual
knowledge. However, the computational patterns of FFNs are still unclear. In
this work, we study the computational patterns of FFNs and observe that most
inputs only activate a tiny ratio of neurons of FFNs. This phenomenon is
similar to the sparsity of the human brain, which drives research on functional
partitions of the human brain. To verify whether functional partitions also
emerge in FFNs, we propose to convert a model into its MoE version with the
same parameters, namely MoEfication. Specifically, MoEfication consists of two
phases: (1) splitting the parameters of FFNs into multiple functional
partitions as experts, and (2) building expert routers to decide which experts
will be used for each input. Experimental results show that MoEfication can
conditionally use 10% to 30% of FFN parameters while maintaining over 95%
original performance for different models on various downstream tasks. Besides,
MoEfication brings two advantages: (1) it significantly reduces the FLOPS of
inference, i.e., 2x speedup with 25% of FFN parameters, and (2) it provides a
fine-grained perspective to study the inner mechanism of FFNs. The source code
of this paper can be obtained from https://github.com/thunlp/MoEfication.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Breaking Down Multilingual Machine Translation. (arXiv:2110.08130v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08130">
<div class="article-summary-box-inner">
<span><p>While multilingual training is now an essential ingredient in machine
translation (MT) systems, recent work has demonstrated that it has different
effects in different multilingual settings, such as many-to-one, one-to-many,
and many-to-many learning. These training settings expose the encoder and the
decoder in a machine translation model with different data distributions. In
this paper, we examine how different varieties of multilingual training
contribute to learning these two components of the MT model. Specifically, we
compare bilingual models with encoders and/or decoders initialized by
multilingual training. We show that multilingual training is beneficial to
encoders in general, while it only benefits decoders for low-resource languages
(LRLs). We further find the important attention heads for each language pair
and compare their correlations during inference. Our analysis sheds light on
how multilingual translation models work and enables us to propose methods to
improve performance by training with highly related languages. Our many-to-one
models for high-resource languages and one-to-many models for LRL outperform
the best results reported by Aharoni et al. (2019)
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm. (arXiv:2110.08190v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08190">
<div class="article-summary-box-inner">
<span><p>Conventional wisdom in pruning Transformer-based language models is that
pruning reduces the model expressiveness and thus is more likely to underfit
rather than overfit. However, under the trending pretrain-and-finetune
paradigm, we postulate a counter-traditional hypothesis, that is: pruning
increases the risk of overfitting when performed at the fine-tuning phase. In
this paper, we aim to address the overfitting problem and improve pruning
performance via progressive knowledge distillation with error-bound properties.
We show for the first time that reducing the risk of overfitting can help the
effectiveness of pruning under the pretrain-and-finetune paradigm. Ablation
studies and experiments on the GLUE benchmark show that our method outperforms
the leading competitors across different tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Less is More: Generating Grounded Navigation Instructions from Landmarks. (arXiv:2111.12872v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12872">
<div class="article-summary-box-inner">
<span><p>We study the automatic generation of navigation instructions from 360-degree
images captured on indoor routes. Existing generators suffer from poor visual
grounding, causing them to rely on language priors and hallucinate objects. Our
MARKY-MT5 system addresses this by focusing on visual landmarks; it comprises a
first stage landmark detector and a second stage generator -- a multimodal,
multilingual, multitask encoder-decoder. To train it, we bootstrap grounded
landmark annotations on top of the Room-across-Room (RxR) dataset. Using text
parsers, weak supervision from RxR's pose traces, and a multilingual image-text
encoder trained on 1.8b images, we identify 971k English, Hindi and Telugu
landmark descriptions and ground them to specific regions in panoramas. On
Room-to-Room, human wayfinders obtain success rates (SR) of 71% following
MARKY-MT5's instructions, just shy of their 75% SR following human instructions
-- and well above SRs with other generators. Evaluations on RxR's longer,
diverse paths obtain 61-64% SRs on three languages. Generating such
high-quality navigation instructions in novel environments is a step towards
conversational navigation tools and could facilitate larger-scale training of
instruction-following agents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Directed Speech Separation for Automatic Speech Recognition of Long Form Conversational Speech. (arXiv:2112.05863v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05863">
<div class="article-summary-box-inner">
<span><p>Many of the recent advances in speech separation are primarily aimed at
synthetic mixtures of short audio utterances with high degrees of overlap. Most
of these approaches need an additional stitching step to stitch the separated
speech chunks for long form audio. Since most of the approaches involve
Permutation Invariant training (PIT), the order of separated speech chunks is
nondeterministic and leads to difficulty in accurately stitching homogenous
speaker chunks for downstream tasks like Automatic Speech Recognition (ASR).
Also, most of these models are trained with synthetic mixtures and do not
generalize to real conversational data. In this paper, we propose a speaker
conditioned separator trained on speaker embeddings extracted directly from the
mixed signal using an over-clustering based approach. This model naturally
regulates the order of the separated chunks without the need for an additional
stitching step. We also introduce a data sampling strategy with real and
synthetic mixtures which generalizes well to real conversation speech. With
this model and data sampling technique, we show significant improvements in
speaker-attributed word error rate (SA-WER) on Hub5 data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fortunately, Discourse Markers Can Enhance Language Models for Sentiment Analysis. (arXiv:2201.02026v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02026">
<div class="article-summary-box-inner">
<span><p>In recent years, pretrained language models have revolutionized the NLP
world, while achieving state of the art performance in various downstream
tasks. However, in many cases, these models do not perform well when labeled
data is scarce and the model is expected to perform in the zero or few shot
setting. Recently, several works have shown that continual pretraining or
performing a second phase of pretraining (inter-training) which is better
aligned with the downstream task, can lead to improved results, especially in
the scarce data setting. Here, we propose to leverage sentiment-carrying
discourse markers to generate large-scale weakly-labeled data, which in turn
can be used to adapt language models for sentiment analysis. Extensive
experimental results show the value of our approach on various benchmark
datasets, including the finance domain. Code, models and data are available at
https://github.com/ibm/tslm-discourse-markers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language-Agnostic Website Embedding and Classification. (arXiv:2201.03677v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03677">
<div class="article-summary-box-inner">
<span><p>Currently, publicly available models for website classification do not offer
an embedding method and have limited support for languages beyond English. We
release a dataset of more than two million category-labeled websites in 92
languages collected from Curlie, the largest multilingual human-edited Web
directory. The dataset contains 14 website categories aligned across languages.
Alongside it, we introduce Homepage2Vec, a machine-learned pre-trained model
for classifying and embedding websites based on their homepage in a
language-agnostic way. Homepage2Vec, thanks to its feature set (textual
content, metadata tags, and visual attributes) and recent progress in natural
language representation, is language-independent by design and generates
embedding-based representations. We show that Homepage2Vec correctly classifies
websites with a macro-averaged F1-score of 0.90, with stable performance across
low- as well as high-resource languages. Feature analysis shows that a small
subset of efficiently computable features suffices to achieve high performance
even with limited computational resources. We make publicly available the
curated Curlie dataset aligned across languages, the pre-trained Homepage2Vec
model, and libraries
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Context-Free Ambiguity of Emoji. (arXiv:2201.06302v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06302">
<div class="article-summary-box-inner">
<span><p>Emojis come with prepacked semantics making them great candidates to create
new forms of more accessible communications. Yet, little is known about how
much of this emojis semantic is agreed upon by humans, outside of textual
contexts. Thus, we collected a crowdsourced dataset of one-word emoji
descriptions for 1,289 emojis presented to participants with no surrounding
text. The emojis and their interpretations were then examined for ambiguity. We
find that with 30 annotations per emoji, 16 emojis (1.2%) are completely
unambiguous, whereas 55 emojis (4.3%) are so ambiguous that their descriptions
are indistinguishable from randomly chosen descriptions. Most of studied emojis
are spread out between the two extremes. Furthermore, investigating the
ambiguity of different types of emojis, we find that an important factor is the
extent to which an emoji has an embedded symbolical meaning drawn from an
established code-book of symbols. We conclude by discussing design
implications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Survey of Hallucination in Natural Language Generation. (arXiv:2202.03629v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03629">
<div class="article-summary-box-inner">
<span><p>Natural Language Generation (NLG) has improved exponentially in recent years
thanks to the development of sequence-to-sequence deep learning technologies
such as Transformer-based language models. This advancement has led to more
fluent and coherent NLG, leading to improved development in downstream tasks
such as abstractive summarization, dialogue generation and data-to-text
generation. However, it is also apparent that deep learning based generation is
prone to hallucinate unintended text, which degrades the system performance and
fails to meet user expectations in many real-world scenarios. To address this
issue, many studies have been presented in measuring and mitigating
hallucinated texts, but these have never been reviewed in a comprehensive
manner before. In this survey, we thus provide a broad overview of the research
progress and challenges in the hallucination problem in NLG. The survey is
organized into two parts: (1) a general overview of metrics, mitigation
methods, and future directions; and (2) an overview of task-specific research
progress on hallucinations in the following downstream tasks, namely
abstractive summarization, dialogue generation, generative question answering,
data-to-text generation, and machine translation. This survey serves to
facilitate collaborative efforts among researchers in tackling the challenge of
hallucinated texts in NLG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Complementarity of Images and Text for the Expression of Emotions in Social Media. (arXiv:2202.07427v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07427">
<div class="article-summary-box-inner">
<span><p>Authors of posts in social media communicate their emotions and what causes
them with text and images. While there is work on emotion and stimulus
detection for each modality separately, it is yet unknown if the modalities
contain complementary emotion information in social media. We aim at filling
this research gap and contribute a novel, annotated corpus of English
multimodal Reddit posts. On this resource, we develop models to automatically
detect the relation between image and text, an emotion stimulus category and
the emotion class. We evaluate if these tasks require both modalities and find
for the image-text relations, that text alone is sufficient for most categories
(complementary, illustrative, opposing): the information in the text allows to
predict if an image is required for emotion understanding. The emotions of
anger and sadness are best predicted with a multimodal model, while text alone
is sufficient for disgust, joy, and surprise. Stimuli depicted by objects,
animals, food, or a person are best predicted by image-only models, while
multimodal models are most effective on art, events, memes, places, or
screenshots.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"splink" is happy and "phrouth" is scary: Emotion Intensity Analysis for Nonsense Words. (arXiv:2202.12132v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12132">
<div class="article-summary-box-inner">
<span><p>People associate affective meanings to words - "death" is scary and sad while
"party" is connotated with surprise and joy. This raises the question if the
association is purely a product of the learned affective imports inherent to
semantic meanings, or is also an effect of other features of words, e.g.,
morphological and phonological patterns. We approach this question with an
annotation-based analysis leveraging nonsense words. Specifically, we conduct a
best-worst scaling crowdsourcing study in which participants assign intensity
scores for joy, sadness, anger, disgust, fear, and surprise to 272 non-sense
words and, for comparison of the results to previous work, to 68 real words.
Based on this resource, we develop character-level and phonology-based
intensity regressors. We evaluate them on both nonsense words and real words
(making use of the NRC emotion intensity lexicon of 7493 words), across six
emotion categories. The analysis of our data reveals that some phonetic
patterns show clear differences between emotion intensities. For instance, s as
a first phoneme contributes to joy, sh to surprise, p as last phoneme more to
disgust than to anger and fear. In the modelling experiments, a regressor
trained on real words from the NRC emotion intensity lexicon shows a higher
performance (r = 0.17) than regressors that aim at learning the emotion
connotation purely from nonsense words. We conclude that humans do associate
affective meaning to words based on surface patterns, but also based on
similarities to existing words ("juy" to "joy", or "flike" to "like").
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual Sequence Generation with Adaptive Compositional Modules. (arXiv:2203.10652v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10652">
<div class="article-summary-box-inner">
<span><p>Continual learning is essential for real-world deployment when there is a
need to quickly adapt the model to new tasks without forgetting knowledge of
old tasks. Existing work on continual sequence generation either always reuses
existing parameters to learn new tasks, which is vulnerable to catastrophic
forgetting on dissimilar tasks, or blindly adds new parameters for every new
task, which could prevent knowledge sharing between similar tasks. To get the
best of both worlds, in this work, we propose continual sequence generation
with adaptive compositional modules to adaptively add modules in transformer
architectures and compose both old and new modules for new tasks. We also
incorporate pseudo experience replay to facilitate knowledge transfer in those
shared modules. Experiment results on various sequences of generation tasks
show that our framework can adaptively add modules or reuse modules based on
task similarity, outperforming state-of-the-art baselines in terms of both
performance and parameter efficiency. We make our code public at
https://github.com/GT-SALT/Adaptive-Compositional-Modules.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Speech Recognition Decoding via Layer Aggregation. (arXiv:2203.11325v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11325">
<div class="article-summary-box-inner">
<span><p>Recently proposed speech recognition systems are designed to predict using
representations generated by their top layers, employing greedy decoding which
isolates each timestep from the rest of the sequence. Aiming for improved
performance, a beam search algorithm is frequently utilized and a language
model is incorporated to assist with ranking the top candidates. In this work,
we experiment with several speech recognition models and find that logits
predicted using the top layers may hamper beam search from achieving optimal
results. Specifically, we show that fined-tuned Wav2Vec 2.0 and HuBERT yield
highly confident predictions, and hypothesize that the predictions are based on
local information and may not take full advantage of the information encoded in
intermediate layers. To this end, we perform a layer analysis to reveal and
visualize how predictions evolve throughout the inference flow. We then propose
a prediction method that aggregates the top M layers, potentially leveraging
useful information encoded in intermediate layers and relaxing model
confidence. We showcase the effectiveness of our approach via beam search
decoding, conducting our experiments on Librispeech test and dev sets and
achieving WER, and CER reduction of up to 10% and 22%, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CICERO: A Dataset for Contextualized Commonsense Inference in Dialogues. (arXiv:2203.13926v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13926">
<div class="article-summary-box-inner">
<span><p>This paper addresses the problem of dialogue reasoning with contextualized
commonsense inference. We curate CICERO, a dataset of dyadic conversations with
five types of utterance-level reasoning-based inferences: cause, subsequent
event, prerequisite, motivation, and emotional reaction. The dataset contains
53,105 of such inferences from 5,672 dialogues. We use this dataset to solve
relevant generative and discriminative tasks: generation of cause and
subsequent event; generation of prerequisite, motivation, and listener's
emotional reaction; and selection of plausible alternatives. Our results
ascertain the value of such dialogue-centric commonsense knowledge datasets. It
is our hope that CICERO will open new research avenues into commonsense-based
dialogue reasoning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text2Pos: Text-to-Point-Cloud Cross-Modal Localization. (arXiv:2203.15125v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15125">
<div class="article-summary-box-inner">
<span><p>Natural language-based communication with mobile devices and home appliances
is becoming increasingly popular and has the potential to become natural for
communicating with mobile robots in the future. Towards this goal, we
investigate cross-modal text-to-point-cloud localization that will allow us to
specify, for example, a vehicle pick-up or goods delivery location. In
particular, we propose Text2Pos, a cross-modal localization module that learns
to align textual descriptions with localization cues in a coarse- to-fine
manner. Given a point cloud of the environment, Text2Pos locates a position
that is specified via a natural language-based description of the immediate
surroundings. To train Text2Pos and study its performance, we construct
KITTI360Pose, the first dataset for this task based on the recently introduced
KITTI360 dataset. Our experiments show that we can localize 65% of textual
queries within 15m distance to query locations for top-10 retrieved locations.
This is a starting point that we hope will spark future developments towards
language-based navigation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Federated Domain Adaptation for ASR with Full Self-Supervision. (arXiv:2203.15966v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15966">
<div class="article-summary-box-inner">
<span><p>Cross-device federated learning (FL) protects user privacy by collaboratively
training a model on user devices, therefore eliminating the need for
collecting, storing, and manually labeling user data. While important topics
such as the FL training algorithm, non-IID-ness, and Differential Privacy have
been well studied in the literature, this paper focuses on two challenges of
practical importance for improving on-device ASR: the lack of ground-truth
transcriptions and the scarcity of compute resource and network bandwidth on
edge devices. First, we propose a FL system for on-device ASR domain adaptation
with full self-supervision, which uses self-labeling together with data
augmentation and filtering techniques. The system can improve a strong
Emformer-Transducer based ASR model pretrained on out-of-domain data, using
in-domain audio without any ground-truth transcriptions. Second, to reduce the
training cost, we propose a self-restricted RNN Transducer (SR-RNN-T) loss, a
variant of alignment-restricted RNN-T that uses Viterbi alignments from
self-supervision. To further reduce the compute and network cost, we
systematically explore adapting only a subset of weights in the
Emformer-Transducer. Our best training recipe achieves a $12.9\%$ relative WER
reduction over the strong out-of-domain baseline, which equals $70\%$ of the
reduction achievable with full human supervision and centralized training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Metaphorical User Simulators for Evaluating Task-oriented Dialogue Systems. (arXiv:2204.00763v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00763">
<div class="article-summary-box-inner">
<span><p>Task-oriented dialogue systems (TDSs) are assessed mainly in an offline
setting or through human evaluation. The evaluation is often limited to
single-turn or very time-intensive. As an alternative, user simulators that
mimic user behavior allow us to consider a broad set of user goals to generate
human-like conversations for simulated evaluation. Employing existing user
simulators to evaluate TDSs is challenging as user simulators are primarily
designed to optimize dialogue policies for TDSs and have limited evaluation
capability. Moreover, the evaluation of user simulators is an open challenge.
In this work, we proposes a metaphorical user simulator for endto-end TDS
evaluation. We also propose a tester-based evaluation framework to generate
variants, i.e., dialogue systems with different capabilities. Our user
simulator constructs a metaphorical user model that assists the simulator in
reasoning by referring to prior knowledge when encountering new items. We
estimate the quality of simulators by checking the simulated interactions
between simulators and variants. Our experiments are conducted using three TDS
datasets. The metaphorical user simulator demonstrates better consistency with
manual evaluation than Agenda-based simulator and Seq2seq model on three
datasets; our tester framework demonstrates efficiency, and our approach
demonstrates better generalization and scalability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pragmatic constraints and pronoun reference disambiguation: the possible and the impossible. (arXiv:2204.01166v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01166">
<div class="article-summary-box-inner">
<span><p>Pronoun disambiguation in understanding text and discourse often requires the
application of both general pragmatic knowledge and context-specific
information. In AI and linguistics research, this has mostly been studied in
cases where the referent is explicitly stated in the preceding text nearby.
However, pronouns in natural text often refer to entities, collections, or
events that are only implicitly mentioned previously; in those cases the need
to use pragmatic knowledge to disambiguate becomes much more acute and the
characterization of the knowledge becomes much more difficult. Extended
literary texts at times employ both extremely complex patterns of reference and
extremely rich and subtle forms of knowledge. Indeed, it is occasionally
possible to have a pronoun that is far separated from its referent in a text.
In the opposite direction, pronoun use is affected by considerations of focus
of attention and by formal constraints such as a preference for parallel
syntactic structures; these can be so strong that no pragmatic knowledge
suffices to overrule them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Aligned Weight Regularizers for Pruning Pretrained Neural Networks. (arXiv:2204.01385v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01385">
<div class="article-summary-box-inner">
<span><p>While various avenues of research have been explored for iterative pruning,
little is known what effect pruning has on zero-shot test performance and its
potential implications on the choice of pruning criteria. This pruning setup is
particularly important for cross-lingual models that implicitly learn alignment
between language representations during pretraining, which if distorted via
pruning, not only leads to poorer performance on language data used for
retraining but also on zero-shot languages that are evaluated.
</p>
<p>In this work, we show that there is a clear performance discrepancy in
magnitude-based pruning when comparing standard supervised learning to the
zero-shot setting. From this finding, we propose two weight regularizers that
aim to maximize the alignment between units of pruned and unpruned networks to
mitigate alignment distortion in pruned cross-lingual models and perform well
for both non zero-shot and zero-shot settings.
</p>
<p>We provide experimental results on cross-lingual tasks for the zero-shot
setting using XLM-RoBERTa$_{\mathrm{Base}}$, where we also find that pruning
has varying degrees of representational degradation depending on the language
corresponding to the zero-shot test set. This is also the first study that
focuses on cross-lingual language model compression.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Estimating the Entropy of Linguistic Distributions. (arXiv:2204.01469v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01469">
<div class="article-summary-box-inner">
<span><p>Shannon entropy is often a quantity of interest to linguists studying the
communicative capacity of human language. However, entropy must typically be
estimated from observed data because researchers do not have access to the
underlying probability distribution that gives rise to these data. While
entropy estimation is a well-studied problem in other fields, there is not yet
a comprehensive exploration of the efficacy of entropy estimators for use with
linguistic data. In this work, we fill this void, studying the empirical
effectiveness of different entropy estimators for linguistic distributions. In
a replication of two recent information-theoretic linguistic studies, we find
evidence that the reported effect size is over-estimated due to over-reliance
on poor entropy estimators. Finally, we end our paper with concrete
recommendations for entropy estimation depending on distribution type and data
availability.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">QuadraLib: A Performant Quadratic Neural Network Library for Architecture Optimization and Design Exploration. (arXiv:2204.01701v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01701">
<div class="article-summary-box-inner">
<span><p>The significant success of Deep Neural Networks (DNNs) is highly promoted by
the multiple sophisticated DNN libraries. On the contrary, although some work
have proved that Quadratic Deep Neuron Networks (QDNNs) show better
non-linearity and learning capability than the first-order DNNs, their neuron
design suffers certain drawbacks from theoretical performance to practical
deployment. In this paper, we first proposed a new QDNN neuron architecture
design, and further developed QuadraLib, a QDNN library to provide architecture
optimization and design exploration for QDNNs. Extensive experiments show that
our design has good performance regarding prediction accuracy and computation
consumption on multiple learning tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Personalized Prediction of Future Lesion Activity and Treatment Effect in Multiple Sclerosis from Baseline MRI. (arXiv:2204.01702v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01702">
<div class="article-summary-box-inner">
<span><p>Precision medicine for chronic diseases such as multiple sclerosis (MS)
involves choosing a treatment which best balances efficacy and side
effects/preferences for individual patients. Making this choice as early as
possible is important, as delays in finding an effective therapy can lead to
irreversible disability accrual. To this end, we present the first deep neural
network model for individualized treatment decisions from baseline magnetic
resonance imaging (MRI) (with clinical information if available) for MS
patients. Our model (a) predicts future new and enlarging T2 weighted (NE-T2)
lesion counts on follow-up MRI on multiple treatments and (b) estimates the
conditional average treatment effect (CATE), as defined by the predicted future
suppression of NE-T2 lesions, between different treatment options relative to
placebo. Our model is validated on a proprietary federated dataset of 1817
multi-sequence MRIs acquired from MS patients during four multi-centre
randomized clinical trials. Our framework achieves high average precision in
the binarized regression of future NE-T2 lesions on five different treatments,
identifies heterogeneous treatment effects, and provides a personalized
treatment recommendation that accounts for treatment-associated risk (e.g. side
effects, patient preference, administration difficulties).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data and Physics Driven Learning Models for Fast MRI -- Fundamentals and Methodologies from CNN, GAN to Attention and Transformers. (arXiv:2204.01706v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01706">
<div class="article-summary-box-inner">
<span><p>Research studies have shown no qualms about using data driven deep learning
models for downstream tasks in medical image analysis, e.g., anatomy
segmentation and lesion detection, disease diagnosis and prognosis, and
treatment planning. However, deep learning models are not the sovereign remedy
for medical image analysis when the upstream imaging is not being conducted
properly (with artefacts). This has been manifested in MRI studies, where the
scanning is typically slow, prone to motion artefacts, with a relatively low
signal to noise ratio, and poor spatial and/or temporal resolution. Recent
studies have witnessed substantial growth in the development of deep learning
techniques for propelling fast MRI. This article aims to (1) introduce the deep
learning based data driven techniques for fast MRI including convolutional
neural network and generative adversarial network based methods, (2) survey the
attention and transformer based models for speeding up MRI reconstruction, and
(3) detail the research in coupling physics and data driven models for MRI
acceleration. Finally, we will demonstrate through a few clinical applications,
explain the importance of data harmonisation and explainable models for such
fast MRI techniques in multicentre and multi-scanner studies, and discuss
common pitfalls in current research and recommendations for future research
directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MRI-based Multi-task Decoupling Learning for Alzheimer's Disease Detection and MMSE Score Prediction: A Multi-site Validation. (arXiv:2204.01708v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01708">
<div class="article-summary-box-inner">
<span><p>Accurately detecting Alzheimer's disease (AD) and predicting mini-mental
state examination (MMSE) score are important tasks in elderly health by
magnetic resonance imaging (MRI). Most of the previous methods on these two
tasks are based on single-task learning and rarely consider the correlation
between them. Since the MMSE score, which is an important basis for AD
diagnosis, can also reflect the progress of cognitive impairment, some studies
have begun to apply multi-task learning methods to these two tasks. However,
how to exploit feature correlation remains a challenging problem for these
methods. To comprehensively address this challenge, we propose a MRI-based
multi-task decoupled learning method for AD detection and MMSE score
prediction. First, a multi-task learning network is proposed to implement AD
detection and MMSE score prediction, which exploits feature correlation by
adding three multi-task interaction layers between the backbones of the two
tasks. Each multi-task interaction layer contains two feature decoupling
modules and one feature interaction module. Furthermore, to enhance the
generalization between tasks of the features selected by the feature decoupling
module, we propose the feature consistency loss constrained feature decoupling
module. Finally, in order to exploit the specific distribution information of
MMSE score in different groups, a distribution loss is proposed to further
enhance the model performance. We evaluate our proposed method on multi-site
datasets. Experimental results show that our proposed multi-task decoupled
representation learning method achieves good performance, outperforming
single-task learning and other existing state-of-the-art methods. The source
code of our proposed method is available at https://github.com/miacsu/MTDL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Forestry digital twin with machine learning in Landsat 7 data. (arXiv:2204.01709v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01709">
<div class="article-summary-box-inner">
<span><p>Modeling forests using historical data allows for more accurately evolution
analysis, thus providing an important basis for other studies. As a recognized
and effective tool, remote sensing plays an important role in forestry
analysis. We can use it to derive information about the forest, including tree
type, coverage and canopy density. There are many forest time series modeling
studies using statistic values, but few using remote sensing images. Image
prediction digital twin is an implementation of digital twin, which aims to
predict future images bases on historical data. In this paper, we propose an
LSTM-based digital twin approach for forest modeling, using Landsat 7 remote
sensing image within 20 years. The experimental results show that the
prediction twin method in this paper can effectively predict the future images
of study area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Convolutional Neural Networks for Image Spam Detection. (arXiv:2204.01710v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01710">
<div class="article-summary-box-inner">
<span><p>Spam can be defined as unsolicited bulk email. In an effort to evade
text-based filters, spammers sometimes embed spam text in an image, which is
referred to as image spam. In this research, we consider the problem of image
spam detection, based on image analysis. We apply convolutional neural networks
(CNN) to this problem, we compare the results obtained using CNNs to other
machine learning techniques, and we compare our results to previous related
work. We consider both real-world image spam and challenging image spam-like
datasets. Our results improve on previous work by employing CNNs based on a
novel feature set consisting of a combination of the raw image and Canny edges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Single Image Internal Distribution Measurement Using Non-Local Variational Autoencoder. (arXiv:2204.01711v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01711">
<div class="article-summary-box-inner">
<span><p>Deep learning-based super-resolution methods have shown great promise,
especially for single image super-resolution (SISR) tasks. Despite the
performance gain, these methods are limited due to their reliance on copious
data for model training. In addition, supervised SISR solutions rely on local
neighbourhood information focusing only on the feature learning processes for
the reconstruction of low-dimensional images. Moreover, they fail to capitalize
on global context due to their constrained receptive field. To combat these
challenges, this paper proposes a novel image-specific solution, namely
non-local variational autoencoder (\texttt{NLVAE}), to reconstruct a
high-resolution (HR) image from a single low-resolution (LR) image without the
need for any prior training. To harvest maximum details for various receptive
regions and high-quality synthetic images, \texttt{NLVAE} is introduced as a
self-supervised strategy that reconstructs high-resolution images using
disentangled information from the non-local neighbourhood. Experimental results
from seven benchmark datasets demonstrate the effectiveness of the
\texttt{NLVAE} model. Moreover, our proposed model outperforms a number of
baseline and state-of-the-art methods as confirmed through extensive
qualitative and quantitative evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Histogram of Oriented Gradients Meet Deep Learning: A Novel Multi-task Deep Network for Medical Image Semantic Segmentation. (arXiv:2204.01712v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01712">
<div class="article-summary-box-inner">
<span><p>We present our novel deep multi-task learning method for medical image
segmentation. Existing multi-task methods demand ground truth annotations for
both the primary and auxiliary tasks. Contrary to it, we propose to generate
the pseudo-labels of an auxiliary task in an unsupervised manner. To generate
the pseudo-labels, we leverage Histogram of Oriented Gradients (HOGs), one of
the most widely used and powerful hand-crafted features for detection. Together
with the ground truth semantic segmentation masks for the primary task and
pseudo-labels for the auxiliary task, we learn the parameters of the deep
network to minimise the loss of both the primary task and the auxiliary task
jointly. We employed our method on two powerful and widely used semantic
segmentation networks: UNet and U2Net to train in a multi-task setup. To
validate our hypothesis, we performed experiments on two different medical
image segmentation data sets. From the extensive quantitative and qualitative
results, we observe that our method consistently improves the performance
compared to the counter-part method. Moreover, our method is the winner of
FetReg Endovis Sub-challenge on Semantic Segmentation organised in conjunction
with MICCAI 2021.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exemplar Learning for Medical Image Segmentation. (arXiv:2204.01713v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01713">
<div class="article-summary-box-inner">
<span><p>Medical image annotation typically requires expert knowledge and hence incurs
time-consuming and expensive data annotation costs. To reduce this burden, we
propose a novel learning scenario, Exemplar Learning (EL), to explore automated
learning processes for medical image segmentation from a single annotated image
example. This innovative learning task is particularly suitable for medical
image segmentation, where all categories of organs can be presented in one
single image for annotation all at once. To address this challenging EL task,
we propose an Exemplar Learning-based Synthesis Net (ELSNet) framework for
medical image segmentation that enables innovative exemplar-based data
synthesis, pixel-prototype based contrastive embedding learning, and
pseudo-label based exploitation of the unlabeled data. Specifically, ELSNet
introduces two new modules for image segmentation: an exemplar-guided synthesis
module, which enriches and diversifies the training set by synthesizing
annotated samples from the given exemplar, and a pixel-prototype based
contrastive embedding module, which enhances the discriminative capacity of the
base segmentation model via contrastive self-supervised learning. Moreover, we
deploy a two-stage process for segmentation model training, which exploits the
unlabeled data with predicted pseudo segmentation labels. To evaluate this new
learning framework, we conduct extensive experiments on several organ
segmentation datasets and present an in-depth analysis. The empirical results
show that the proposed exemplar learning framework produces effective
segmentation results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Estimating Fine-Grained Noise Model via Contrastive Learning. (arXiv:2204.01716v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01716">
<div class="article-summary-box-inner">
<span><p>Image denoising has achieved unprecedented progress as great efforts have
been made to exploit effective deep denoisers. To improve the denoising
performance in realworld, two typical solutions are used in recent trends:
devising better noise models for the synthesis of more realistic training data,
and estimating noise level function to guide non-blind denoisers. In this work,
we combine both noise modeling and estimation, and propose an innovative noise
model estimation and noise synthesis pipeline for realistic noisy image
generation. Specifically, our model learns a noise estimation model with
fine-grained statistical noise model in a contrastive manner. Then, we use the
estimated noise parameters to model camera-specific noise distribution, and
synthesize realistic noisy training data. The most striking thing for our work
is that by calibrating noise models of several sensors, our model can be
extended to predict other cameras. In other words, we can estimate
cameraspecific noise models for unknown sensors with only testing images,
without laborious calibration frames or paired noisy/clean data. The proposed
pipeline endows deep denoisers with competitive performances with
state-of-the-art real noise modeling methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RestoreX-AI: A Contrastive Approach towards Guiding Image Restoration via Explainable AI Systems. (arXiv:2204.01719v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01719">
<div class="article-summary-box-inner">
<span><p>Modern applications such as self-driving cars and drones rely heavily upon
robust object detection techniques. However, weather corruptions can hinder the
object detectability and pose a serious threat to their navigation and
reliability. Thus, there is a need for efficient denoising, deraining, and
restoration techniques. Generative adversarial networks and transformers have
been widely adopted for image restoration. However, the training of these
methods is often unstable and time-consuming. Furthermore, when used for object
detection (OD), the output images generated by these methods may provide
unsatisfactory results despite image clarity. In this work, we propose a
contrastive approach towards mitigating this problem, by evaluating images
generated by restoration models during and post training. This approach
leverages OD scores combined with attention maps for predicting the usefulness
of restored images for the OD task. We conduct experiments using two novel
use-cases of conditional GANs and two transformer methods that probe the
robustness of the proposed approach on multi-weather corruptions in the OD
task. Our approach achieves an averaged 178 percent increase in mAP between the
input and restored images under adverse weather conditions like dust tornadoes
and snowfall. We report unique cases where greater denoising does not improve
OD performance and conversely where noisy generated images demonstrate good
results. We conclude the need for explainability frameworks to bridge the gap
between human and machine perception, especially in the context of robust
object detection for autonomous vehicles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distinguishing Homophenes Using Multi-Head Visual-Audio Memory for Lip Reading. (arXiv:2204.01725v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01725">
<div class="article-summary-box-inner">
<span><p>Recognizing speech from silent lip movement, which is called lip reading, is
a challenging task due to 1) the inherent information insufficiency of lip
movement to fully represent the speech, and 2) the existence of homophenes that
have similar lip movement with different pronunciations. In this paper, we try
to alleviate the aforementioned two challenges in lip reading by proposing a
Multi-head Visual-audio Memory (MVM). Firstly, MVM is trained with audio-visual
datasets and remembers audio representations by modelling the
inter-relationships of paired audio-visual representations. At the inference
stage, visual input alone can extract the saved audio representation from the
memory by examining the learned inter-relationships. Therefore, the lip reading
model can complement the insufficient visual information with the extracted
audio representations. Secondly, MVM is composed of multi-head key memories for
saving visual features and one value memory for saving audio knowledge, which
is designed to distinguish the homophenes. With the multi-head key memories,
MVM extracts possible candidate audio features from the memory, which allows
the lip reading model to consider the possibility of which pronunciations can
be represented from the input lip movement. This also can be viewed as an
explicit implementation of the one-to-many mapping of viseme-to-phoneme.
Moreover, MVM is employed in multi-temporal levels to consider the context when
retrieving the memory and distinguish the homophenes. Extensive experimental
results verify the effectiveness of the proposed method in lip reading and in
distinguishing the homophenes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lip to Speech Synthesis with Visual Context Attentional GAN. (arXiv:2204.01726v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01726">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel lip-to-speech generative adversarial
network, Visual Context Attentional GAN (VCA-GAN), which can jointly model
local and global lip movements during speech synthesis. Specifically, the
proposed VCA-GAN synthesizes the speech from local lip visual features by
finding a mapping function of viseme-to-phoneme, while global visual context is
embedded into the intermediate layers of the generator to clarify the ambiguity
in the mapping induced by homophene. To achieve this, a visual context
attention module is proposed where it encodes global representations from the
local visual features, and provides the desired global visual context
corresponding to the given coarse speech representation to the generator
through audio-visual attention. In addition to the explicit modelling of local
and global visual representations, synchronization learning is introduced as a
form of contrastive learning that guides the generator to synthesize a speech
in sync with the given input lip movements. Extensive experiments demonstrate
that the proposed VCA-GAN outperforms existing state-of-the-art and is able to
effectively synthesize the speech from multi-speaker that has been barely
handled in the previous works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalized Zero Shot Learning For Medical Image Classification. (arXiv:2204.01728v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01728">
<div class="article-summary-box-inner">
<span><p>In many real world medical image classification settings we do not have
access to samples of all possible disease classes, while a robust system is
expected to give high performance in recognizing novel test data. We propose a
generalized zero shot learning (GZSL) method that uses self supervised learning
(SSL) for: 1) selecting anchor vectors of different disease classes; and 2)
training a feature generator. Our approach does not require class attribute
vectors which are available for natural images but not for medical images. SSL
ensures that the anchor vectors are representative of each class. SSL is also
used to generate synthetic features of unseen classes. Using a simpler
architecture, our method matches a state of the art SSL based GZSL method for
natural images and outperforms all methods for medical images. Our method is
adaptable enough to accommodate class attribute vectors when they are available
for natural images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing the Effects of Handling Data Imbalance on Learned Features from Medical Images by Looking Into the Models. (arXiv:2204.01729v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01729">
<div class="article-summary-box-inner">
<span><p>One challenging property lurking in medical datasets is the imbalanced data
distribution, where the frequency of the samples between the different classes
is not balanced. Training a model on an imbalanced dataset can introduce unique
challenges to the learning problem where a model is biased towards the highly
frequent class. Many methods are proposed to tackle the distributional
differences and the imbalanced problem. However, the impact of these approaches
on the learned features is not well studied. In this paper, we look deeper into
the internal units of neural networks to observe how handling data imbalance
affects the learned features. We study several popular cost-sensitive
approaches for handling data imbalance and analyze the feature maps of the
convolutional neural networks from multiple perspectives: analyzing the
alignment of salient features with pathologies and analyzing the
pathology-related concepts encoded by the networks. Our study reveals
differences and insights regarding the trained models that are not reflected by
quantitative metrics such as AUROC and AP and show up only by looking at the
models through a lens.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transient motion classification through turbid volumes via parallelized single-photon detection and deep contrastive embedding. (arXiv:2204.01733v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01733">
<div class="article-summary-box-inner">
<span><p>Fast noninvasive probing of spatially varying decorrelating events, such as
cerebral blood flow beneath the human skull, is an essential task in various
scientific and clinical settings. One of the primary optical techniques used is
diffuse correlation spectroscopy (DCS), whose classical implementation uses a
single or few single-photon detectors, resulting in poor spatial localization
accuracy and relatively low temporal resolution. Here, we propose a technique
termed Classifying Rapid decorrelation Events via Parallelized single photon
dEtection (CREPE)}, a new form of DCS that can probe and classify different
decorrelating movements hidden underneath turbid volume with high sensitivity
using parallelized speckle detection from a $32\times32$ pixel SPAD array. We
evaluate our setup by classifying different spatiotemporal-decorrelating
patterns hidden beneath a 5mm tissue-like phantom made with rapidly
decorrelating dynamic scattering media. Twelve multi-mode fibers are used to
collect scattered light from different positions on the surface of the tissue
phantom. To validate our setup, we generate perturbed decorrelation patterns by
both a digital micromirror device (DMD) modulated at multi-kilo-hertz rates, as
well as a vessel phantom containing flowing fluid. Along with a deep
contrastive learning algorithm that outperforms classic unsupervised learning
methods, we demonstrate our approach can accurately detect and classify
different transient decorrelation events (happening in 0.1-0.4s) underneath
turbid scattering media, without any data labeling. This has the potential to
be applied to noninvasively monitor deep tissue motion patterns, for example
identifying normal or abnormal cerebral blood flow events, at multi-Hertz rates
within a compact and static detection probe.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Explaining Multimodal Hateful Meme Detection Models. (arXiv:2204.01734v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01734">
<div class="article-summary-box-inner">
<span><p>Hateful meme detection is a new multimodal task that has gained significant
traction in academic and industry research communities. Recently, researchers
have applied pre-trained visual-linguistic models to perform the multimodal
classification task, and some of these solutions have yielded promising
results. However, what these visual-linguistic models learn for the hateful
meme classification task remains unclear. For instance, it is unclear if these
models are able to capture the derogatory or slurs references in multimodality
(i.e., image and text) of the hateful memes. To fill this research gap, this
paper propose three research questions to improve our understanding of these
visual-linguistic models performing the hateful meme classification task. We
found that the image modality contributes more to the hateful meme
classification task, and the visual-linguistic models are able to perform
visual-text slurs grounding to a certain extent. Our error analysis also shows
that the visual-linguistic models have acquired biases, which resulted in
false-positive predictions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tracking Urbanization in Developing Regions with Remote Sensing Spatial-Temporal Super-Resolution. (arXiv:2204.01736v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01736">
<div class="article-summary-box-inner">
<span><p>Automated tracking of urban development in areas where construction
information is not available became possible with recent advancements in
machine learning and remote sensing. Unfortunately, these solutions perform
best on high-resolution imagery, which is expensive to acquire and infrequently
available, making it difficult to scale over long time spans and across large
geographies. In this work, we propose a pipeline that leverages a single
high-resolution image and a time series of publicly available low-resolution
images to generate accurate high-resolution time series for object tracking in
urban construction. Our method achieves significant improvement in comparison
to baselines using single image super-resolution, and can assist in extending
the accessibility and scalability of building construction tracking across the
developing world.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature robustness and sex differences in medical imaging: a case study in MRI-based Alzheimer's disease detection. (arXiv:2204.01737v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01737">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks have enabled significant improvements in
medical image-based disease classification. It has, however, become
increasingly clear that these models are susceptible to performance degradation
due to spurious correlations and dataset shifts, which may lead to
underperformance on underrepresented patient groups, among other problems. In
this paper, we compare two classification schemes on the ADNI MRI dataset: a
very simple logistic regression model that uses manually selected volumetric
features as inputs, and a convolutional neural network trained on 3D MRI data.
We assess the robustness of the trained models in the face of varying dataset
splits, training set sex composition, and stage of disease. In contrast to
earlier work on diagnosing lung diseases based on chest x-ray data, we do not
find a strong dependence of model performance for male and female test subjects
on the sex composition of the training dataset. Moreover, in our analysis, the
low-dimensional model with manually selected features outperforms the 3D CNN,
thus emphasizing the need for automatic robust feature extraction methods and
the value of manual feature specification (based on prior knowledge) for
robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Face Recognition In Children: A Longitudinal Study. (arXiv:2204.01760v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01760">
<div class="article-summary-box-inner">
<span><p>The lack of high fidelity and publicly available longitudinal children face
datasets is one of the main limiting factors in the development of face
recognition systems for children. In this work, we introduce the Young Face
Aging (YFA) dataset for analyzing the performance of face recognition systems
over short age-gaps in children. We expand previous work by comparing YFA with
several publicly available cross-age adult datasets to quantify the effects of
short age-gap in adults and children. Our analysis confirms a statistically
significant and matcher independent decaying relationship between the match
scores of ArcFace-Focal, MagFace, and Facenet matchers and the age-gap between
the gallery and probe images in children, even at the short age-gap of 6
months. However, our result indicates that the low verification performance
reported in previous work might be due to the intra-class structure of the
matcher and the lower quality of the samples. Our experiment using YFA and a
state-of-the-art, quality-aware face matcher (MagFace) indicates 98.3% and
94.9% TAR at 0.1% FAR over 6 and 36 Months age-gaps, respectively, suggesting
that face recognition may be feasible for children for age-gaps of up to three
years.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The First Principles of Deep Learning and Compression. (arXiv:2204.01782v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01782">
<div class="article-summary-box-inner">
<span><p>The deep learning revolution incited by the 2012 Alexnet paper has been
transformative for the field of computer vision. Many problems which were
severely limited using classical solutions are now seeing unprecedented
success. The rapid proliferation of deep learning methods has led to a sharp
increase in their use in consumer and embedded applications. One consequence of
consumer and embedded applications is lossy multimedia compression which is
required to engineer the efficient storage and transmission of data in these
real-world scenarios. As such, there has been increased interest in a deep
learning solution for multimedia compression which would allow for higher
compression ratios and increased visual quality.
</p>
<p>The deep learning approach to multimedia compression, so called Learned
Multimedia Compression, involves computing a compressed representation of an
image or video using a deep network for the encoder and the decoder. While
these techniques have enjoyed impressive academic success, their industry
adoption has been essentially non-existent. Classical compression techniques
like JPEG and MPEG are too entrenched in modern computing to be easily
replaced. This dissertation takes an orthogonal approach and leverages deep
learning to improve the compression fidelity of these classical algorithms.
This allows the incredible advances in deep learning to be used for multimedia
compression without threatening the ubiquity of the classical methods.
</p>
<p>The key insight of this work is that methods which are motivated by first
principles, i.e., the underlying engineering decisions that were made when the
compression algorithms were developed, are more effective than general methods.
By encoding prior knowledge into the design of the algorithm, the flexibility,
performance, and/or accuracy are improved at the cost of generality...
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Object Permanence Emerges in a Random Walk along Memory. (arXiv:2204.01784v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01784">
<div class="article-summary-box-inner">
<span><p>This paper proposes a self-supervised objective for learning representations
that localize objects under occlusion - a property known as object permanence.
A central question is the choice of learning signal in cases of total
occlusion. Rather than directly supervising the locations of invisible objects,
we propose a self-supervised objective that requires neither human annotation,
nor assumptions about object dynamics. We show that object permanence can
emerge by optimizing for temporal coherence of memory: we fit a Markov walk
along a space-time graph of memories, where the states in each time step are
non-Markovian features from a sequence encoder. This leads to a memory
representation that stores occluded objects and predicts their motion, to
better localize them. The resulting model outperforms existing approaches on
several datasets of increasing complexity and realism, despite requiring
minimal supervision and assumptions, and hence being broadly applicable.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lightweight HDR Camera ISP for Robust Perception in Dynamic Illumination Conditions via Fourier Adversarial Networks. (arXiv:2204.01795v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01795">
<div class="article-summary-box-inner">
<span><p>The limited dynamic range of commercial compact camera sensors results in an
inaccurate representation of scenes with varying illumination conditions,
adversely affecting image quality and subsequently limiting the performance of
underlying image processing algorithms. Current state-of-the-art (SoTA)
convolutional neural networks (CNN) are developed as post-processing techniques
to independently recover under-/over-exposed images. However, when applied to
images containing real-world degradations such as glare, high-beam, color
bleeding with varying noise intensity, these algorithms amplify the
degradations, further degrading image quality. We propose a lightweight
two-stage image enhancement algorithm sequentially balancing illumination and
noise removal using frequency priors for structural guidance to overcome these
limitations. Furthermore, to ensure realistic image quality, we leverage the
relationship between frequency and spatial domain properties of an image and
propose a Fourier spectrum-based adversarial framework (AFNet) for consistent
image enhancement under varying illumination conditions. While current
formulations of image enhancement are envisioned as post-processing techniques,
we examine if such an algorithm could be extended to integrate the
functionality of the Image Signal Processing (ISP) pipeline within the camera
sensor benefiting from RAW sensor data and lightweight CNN architecture. Based
on quantitative and qualitative evaluations, we also examine the practicality
and effects of image enhancement techniques on the performance of common
perception tasks such as object detection and semantic segmentation in varying
illumination conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Near/Remote Sensing with Geospatial Attention. (arXiv:2204.01807v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01807">
<div class="article-summary-box-inner">
<span><p>This work addresses the task of overhead image segmentation when auxiliary
ground-level images are available. Recent work has shown that performing joint
inference over these two modalities, often called near/remote sensing, can
yield significant accuracy improvements. Extending this line of work, we
introduce the concept of geospatial attention, a geometry-aware attention
mechanism that explicitly considers the geospatial relationship between the
pixels in a ground-level image and a geographic location. We propose an
approach for computing geospatial attention that incorporates geometric
features and the appearance of the overhead and ground-level imagery. We
introduce a novel architecture for near/remote sensing that is based on
geospatial attention and demonstrate its use for five segmentation tasks. The
results demonstrate that our method significantly outperforms the previous
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Infield Navigation: leveraging simulated data for crop row detection. (arXiv:2204.01811v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01811">
<div class="article-summary-box-inner">
<span><p>Agricultural datasets for crop row detection are often bound by their limited
number of images. This restricts the researchers from developing deep learning
based models for precision agricultural tasks involving crop row detection. We
suggest the utilization of small real-world datasets along with additional data
generated by simulations to yield similar crop row detection performance as
that of a model trained with a large real world dataset. Our method could reach
the performance of a deep learning based crop row detection model trained with
real-world data by using 60% less labelled real-world data. Our model performed
well against field variations such as shadows, sunlight and grow stages. We
introduce an automated pipeline to generate labelled images for crop row
detection in simulation domain. An extensive comparison is done to analyze the
contribution of simulated data towards reaching robust crop row detection in
various real-world field scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">High Efficiency Pedestrian Crossing Prediction. (arXiv:2204.01862v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01862">
<div class="article-summary-box-inner">
<span><p>Predicting pedestrian crossing intention is an indispensable aspect of
deploying advanced driving systems (ADS) or advanced driver-assistance systems
(ADAS) to real life. State-of-the-art methods in predicting pedestrian crossing
intention often rely on multiple streams of information as inputs, each of
which requires massive computational resources and heavy network architectures
to generate. However, such reliance limits the practical application of the
systems. In this paper, driven the the real-world demands of pedestrian
crossing intention prediction models with both high efficiency and accuracy, we
introduce a network with only frames of pedestrians as the input. Every
component in the introduced network is driven by the goal of light weight.
Specifically, we reduce the multi-source input dependency and employ light
neural networks that are tailored for mobile devices. These smaller neural
networks can fit into computer memory and can be transmitted over a computer
network more easily, thus making them more suitable for real-life deployment
and real-time prediction. To compensate the removal of the multi-source input,
we enhance the network effectiveness by adopting a multi-task learning
training, named "side task learning", to include multiple auxiliary tasks to
jointly learn the feature extractor for improved robustness. Each head handles
a specific task that potentially shares knowledge with other heads. In the
meantime, the feature extractor is shared across all tasks to ensure the
sharing of basic knowledge across all layers. The light weight but high
efficiency characteristics of our model endow it the potential of being
deployed on vehicle-based systems. Experiments validate that our model
consistently delivers outstanding performances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Truck Axle Detection with Convolutional Neural Networks. (arXiv:2204.01868v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01868">
<div class="article-summary-box-inner">
<span><p>Axle count in trucks is important to the classification of vehicles and to
the operation of road systems, and is used in the determination of service fees
and the impact on the pavement. Although axle count can be achieved with
traditional methods, such as manual labor, it is increasingly possible to count
axles using deep learning and computer vision methods. This paper aims to
compare three deep learning object detection algorithms, YOLO, Faster R-CNN and
SSD, for the detection of truck axles. A dataset was built to provide training
and testing examples for the neural networks. Training was done on different
base models, to increase training time efficiency and to compare results. We
evaluated results based on three metrics: mAP, F1-score, and FPS count. Results
indicate that YOLO and SSD have similar accuracy and performance, with more
than 96% mAP for both models. Dataset and codes are publicly available for
download.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MonoTrack: Shuttle trajectory reconstruction from monocular badminton video. (arXiv:2204.01899v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01899">
<div class="article-summary-box-inner">
<span><p>Trajectory estimation is a fundamental component of racket sport analytics,
as the trajectory contains information not only about the winning and losing of
each point, but also how it was won or lost. In sports such as badminton,
players benefit from knowing the full 3D trajectory, as the height of
shuttlecock or ball provides valuable tactical information. Unfortunately, 3D
reconstruction is a notoriously hard problem, and standard trajectory
estimators can only track 2D pixel coordinates. In this work, we present the
first complete end-to-end system for the extraction and segmentation of 3D
shuttle trajectories from monocular badminton videos. Our system integrates
badminton domain knowledge such as court dimension, shot placement, physical
laws of motion, along with vision-based features such as player poses and
shuttle tracking. We find that significant engineering efforts and model
improvements are needed to make the overall system robust, and as a by-product
of our work, improve state-of-the-art results on court recognition, 2D
trajectory estimation, and hit recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Exploration of Active Learning for Affective Digital Phenotyping. (arXiv:2204.01915v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01915">
<div class="article-summary-box-inner">
<span><p>Some of the most severe bottlenecks preventing widespread development of
machine learning models for human behavior include a dearth of labeled training
data and difficulty of acquiring high quality labels. Active learning is a
paradigm for using algorithms to computationally select a useful subset of data
points to label using metrics for model uncertainty and data similarity. We
explore active learning for naturalistic computer vision emotion data, a
particularly heterogeneous and complex data space due to inherently subjective
labels. Using frames collected from gameplay acquired from a therapeutic
smartphone game for children with autism, we run a simulation of active
learning using gameplay prompts as metadata to aid in the active learning
process. We find that active learning using information generated during
gameplay slightly outperforms random selection of the same number of labeled
frames. We next investigate a method to conduct active learning with subjective
data, such as in affective computing, and where multiple crowdsourced labels
can be acquired for each image. Using the Child Affective Facial Expression
(CAFE) dataset, we simulate an active learning process for crowdsourcing many
labels and find that prioritizing frames using the entropy of the crowdsourced
label distribution results in lower categorical cross-entropy loss compared to
random frame selection. Collectively, these results demonstrate pilot
evaluations of two novel active learning approaches for subjective affective
data collected in noisy settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text Spotting Transformers. (arXiv:2204.01918v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01918">
<div class="article-summary-box-inner">
<span><p>In this paper, we present TExt Spotting TRansformers (TESTR), a generic
end-to-end text spotting framework using Transformers for text detection and
recognition in the wild. TESTR builds upon a single encoder and dual decoders
for the joint text-box control point regression and character recognition.
Other than most existing literature, our method is free from Region-of-Interest
operations and heuristics-driven post-processing procedures; TESTR is
particularly effective when dealing with curved text-boxes where special cares
are needed for the adaptation of the traditional bounding-box representations.
We show our canonical representation of control points suitable for text
instances in both Bezier curve and polygon annotations. In addition, we design
a bounding-box guided polygon detection (box-to-polygon) process. Experiments
on curved and arbitrarily shaped datasets demonstrate state-of-the-art
performances of the proposed TESTR algorithm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">High-Quality Pluralistic Image Completion via Code Shared VQGAN. (arXiv:2204.01931v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01931">
<div class="article-summary-box-inner">
<span><p>PICNet pioneered the generation of multiple and diverse results for image
completion task, but it required a careful balance between $\mathcal{KL}$ loss
(diversity) and reconstruction loss (quality), resulting in a limited diversity
and quality . Separately, iGPT-based architecture has been employed to infer
distributions in a discrete space derived from a pixel-level pre-clustered
palette, which however cannot generate high-quality results directly. In this
work, we present a novel framework for pluralistic image completion that can
achieve both high quality and diversity at much faster inference speed. The
core of our design lies in a simple yet effective code sharing mechanism that
leads to a very compact yet expressive image representation in a discrete
latent domain. The compactness and the richness of the representation further
facilitate the subsequent deployment of a transformer to effectively learn how
to composite and complete a masked image at the discrete code domain. Based on
the global context well-captured by the transformer and the available visual
regions, we are able to sample all tokens simultaneously, which is completely
different from the prevailing autoregressive approach of iGPT-based works, and
leads to more than 100$\times$ faster inference speed. Experiments show that
our framework is able to learn semantically-rich discrete codes efficiently and
robustly, resulting in much better image reconstruction quality. Our diverse
image completion framework significantly outperforms the state-of-the-art both
quantitatively and qualitatively on multiple benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attention Distraction: Watermark Removal Through Continual Learning with Selective Forgetting. (arXiv:2204.01934v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01934">
<div class="article-summary-box-inner">
<span><p>Fine-tuning attacks are effective in removing the embedded watermarks in deep
learning models. However, when the source data is unavailable, it is
challenging to just erase the watermark without jeopardizing the model
performance. In this context, we introduce Attention Distraction (AD), a novel
source data-free watermark removal attack, to make the model selectively forget
the embedded watermarks by customizing continual learning. In particular, AD
first anchors the model's attention on the main task using some unlabeled data.
Then, through continual learning, a small number of \textit{lures} (randomly
selected natural images) that are assigned a new label distract the model's
attention away from the watermarks. Experimental results from different
datasets and networks corroborate that AD can thoroughly remove the watermark
with a small resource budget without compromising the model's performance on
the main task, which outperforms the state-of-the-art works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified Implicit Neural Stylization. (arXiv:2204.01943v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01943">
<div class="article-summary-box-inner">
<span><p>Representing visual signals by implicit representation (e.g., a coordinate
based deep network) has prevailed among many vision tasks. This work explores a
new intriguing direction: training a stylized implicit representation, using a
generalized approach that can apply to various 2D and 3D scenarios. We conduct
a pilot study on a variety of implicit functions, including 2D coordinate-based
representation, neural radiance field, and signed distance function. Our
solution is a Unified Implicit Neural Stylization framework, dubbed INS. In
contrary to vanilla implicit representation, INS decouples the ordinary
implicit function into a style implicit module and a content implicit module,
in order to separately encode the representations from the style image and
input scenes. An amalgamation module is then applied to aggregate these
information and synthesize the stylized output. To regularize the geometry in
3D scenes, we propose a novel self-distillation geometry consistency loss which
preserves the geometry fidelity of the stylized scenes. Comprehensive
experiments are conducted on multiple task settings, including novel view
synthesis of complex scenes, stylization for implicit surfaces, and fitting
images using MLPs. We further demonstrate that the learned representation is
continuous not only spatially but also style-wise, leading to effortlessly
interpolating between different styles and generating images with new mixed
styles. Please refer to the video on our project page for more view synthesis
results: https://zhiwenfan.github.io/INS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards On-Board Panoptic Segmentation of Multispectral Satellite Images. (arXiv:2204.01952v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01952">
<div class="article-summary-box-inner">
<span><p>With tremendous advancements in low-power embedded computing devices and
remote sensing instruments, the traditional satellite image processing pipeline
which includes an expensive data transfer step prior to processing data on the
ground is being replaced by on-board processing of captured data. This paradigm
shift enables critical and time-sensitive analytic intelligence to be acquired
in a timely manner on-board the satellite itself. However, at present, the
on-board processing of multi-spectral satellite images is limited to
classification and segmentation tasks. Extending this processing to its next
logical level, in this paper we propose a lightweight pipeline for on-board
panoptic segmentation of multi-spectral satellite images. Panoptic segmentation
offers major economic and environmental insights, ranging from yield estimation
from agricultural lands to intelligence for complex military applications.
Nevertheless, the on-board intelligence extraction raises several challenges
due to the loss of temporal observations and the need to generate predictions
from a single image sample. To address this challenge, we propose a multimodal
teacher network based on a cross-modality attention-based fusion strategy to
improve the segmentation accuracy by exploiting data from multiple modes. We
also propose an online knowledge distillation framework to transfer the
knowledge learned by this multi-modal teacher network to a uni-modal student
which receives only a single frame input, and is more appropriate for an
on-board environment. We benchmark our approach against existing
state-of-the-art panoptic segmentation models using the PASTIS multi-spectral
panoptic segmentation dataset considering an on-board processing setting. Our
evaluations demonstrate a substantial increase in accuracy metrics compared to
the existing state-of-the-art models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Autoregressive 3D Shape Generation via Canonical Mapping. (arXiv:2204.01955v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01955">
<div class="article-summary-box-inner">
<span><p>With the capacity of modeling long-range dependencies in sequential data,
transformers have shown remarkable performances in a variety of generative
tasks such as image, audio, and text generation. Yet, taming them in generating
less structured and voluminous data formats such as high-resolution point
clouds have seldom been explored due to ambiguous sequentialization processes
and infeasible computation burden. In this paper, we aim to further exploit the
power of transformers and employ them for the task of 3D point cloud
generation. The key idea is to decompose point clouds of one category into
semantically aligned sequences of shape compositions, via a learned canonical
space. These shape compositions can then be quantized and used to learn a
context-rich composition codebook for point cloud generation. Experimental
results on point cloud reconstruction and unconditional generation show that
our model performs favorably against state-of-the-art approaches. Furthermore,
our model can be easily extended to multi-modal shape completion as an
application for conditional shape generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FaceSigns: Semi-Fragile Neural Watermarks for Media Authentication and Countering Deepfakes. (arXiv:2204.01960v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01960">
<div class="article-summary-box-inner">
<span><p>Deepfakes and manipulated media are becoming a prominent threat due to the
recent advances in realistic image and video synthesis techniques. There have
been several attempts at combating Deepfakes using machine learning
classifiers. However, such classifiers do not generalize well to black-box
image synthesis techniques and have been shown to be vulnerable to adversarial
examples. To address these challenges, we introduce a deep learning based
semi-fragile watermarking technique that allows media authentication by
verifying an invisible secret message embedded in the image pixels. Instead of
identifying and detecting fake media using visual artifacts, we propose to
proactively embed a semi-fragile watermark into a real image so that we can
prove its authenticity when needed. Our watermarking framework is designed to
be fragile to facial manipulations or tampering while being robust to benign
image-processing operations such as image compression, scaling, saturation,
contrast adjustments etc. This allows images shared over the internet to retain
the verifiable watermark as long as face-swapping or any other Deepfake
modification technique is not applied. We demonstrate that FaceSigns can embed
a 128 bit secret as an imperceptible image watermark that can be recovered with
a high bit recovery accuracy at several compression levels, while being
non-recoverable when unseen Deepfake manipulations are applied. For a set of
unseen benign and Deepfake manipulations studied in our work, FaceSigns can
reliably detect manipulated content with an AUC score of 0.996 which is
significantly higher than prior image watermarking and steganography
techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controllable Garment Transfer. (arXiv:2204.01965v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01965">
<div class="article-summary-box-inner">
<span><p>Image-based garment transfer replaces the garment on the target human with
the desired garment; this enables users to virtually view themselves in the
desired garment. To this end, many approaches have been proposed using the
generative model and have shown promising results. However, most fail to
provide the user with on the fly garment modification functionality. We aim to
add this customizable option of "garment tweaking" to our model to control
garment attributes, such as sleeve length, waist width, and garment texture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PSDoodle: Searching for App Screens via Interactive Sketching. (arXiv:2204.01968v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01968">
<div class="article-summary-box-inner">
<span><p>Keyword-based mobile screen search does not account for screen content and
fails to operate as a universal tool for all levels of users. Visual searching
(e.g., image, sketch) is structured and easy to adopt. Current visual search
approaches count on a complete screen and are therefore slow and tedious.
PSDoodle employs a deep neural network to recognize partial screen element
drawings instantly on a digital drawing interface and shows results in
real-time. PSDoodle is the first tool that utilizes partial sketches and
searches for screens in an interactive iterative way. PSDoodle supports
different drawing styles and retrieves search results that are relevant to the
user's sketch query. A short video demonstration is available online at:
https://youtu.be/3cVLHFm5pY4
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Region Rebalance for Long-Tailed Semantic Segmentation. (arXiv:2204.01969v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01969">
<div class="article-summary-box-inner">
<span><p>In this paper, we study the problem of class imbalance in semantic
segmentation. We first investigate and identify the main challenges of
addressing this issue through pixel rebalance. Then a simple and yet effective
region rebalance scheme is derived based on our analysis. In our solution,
pixel features belonging to the same class are grouped into region features,
and a rebalanced region classifier is applied via an auxiliary region rebalance
branch during training. To verify the flexibility and effectiveness of our
method, we apply the region rebalance module into various semantic segmentation
methods, such as Deeplabv3+, OCRNet, and Swin. Our strategy achieves consistent
improvement on the challenging ADE20K and COCO-Stuff benchmark. In particular,
with the proposed region rebalance scheme, state-of-the-art BEiT receives +0.7%
gain in terms of mIoU on the ADE20K val set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-Local Latent Relation Distillation for Self-Adaptive 3D Human Pose Estimation. (arXiv:2204.01971v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01971">
<div class="article-summary-box-inner">
<span><p>Available 3D human pose estimation approaches leverage different forms of
strong (2D/3D pose) or weak (multi-view or depth) paired supervision. Barring
synthetic or in-studio domains, acquiring such supervision for each new target
environment is highly inconvenient. To this end, we cast 3D pose learning as a
self-supervised adaptation problem that aims to transfer the task knowledge
from a labeled source domain to a completely unpaired target. We propose to
infer image-to-pose via two explicit mappings viz. image-to-latent and
latent-to-pose where the latter is a pre-learned decoder obtained from a
prior-enforcing generative adversarial auto-encoder. Next, we introduce
relation distillation as a means to align the unpaired cross-modal samples i.e.
the unpaired target videos and unpaired 3D pose sequences. To this end, we
propose a new set of non-local relations in order to characterize long-range
latent pose interactions unlike general contrastive relations where positive
couplings are limited to a local neighborhood structure. Further, we provide an
objective way to quantify non-localness in order to select the most effective
relation set. We evaluate different self-adaptation settings and demonstrate
state-of-the-art 3D human pose estimation performance on standard benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Audio-visual multi-channel speech separation, dereverberation and recognition. (arXiv:2204.01977v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01977">
<div class="article-summary-box-inner">
<span><p>Despite the rapid advance of automatic speech recognition (ASR) technologies,
accurate recognition of cocktail party speech characterised by the interference
from overlapping speakers, background noise and room reverberation remains a
highly challenging task to date. Motivated by the invariance of visual modality
to acoustic signal corruption, audio-visual speech enhancement techniques have
been developed, although predominantly targeting overlapping speech separation
and recognition tasks. In this paper, an audio-visual multi-channel speech
separation, dereverberation and recognition approach featuring a full
incorporation of visual information into all three stages of the system is
proposed. The advantage of the additional visual modality over using audio only
is demonstrated on two neural dereverberation approaches based on DNN-WPE and
spectral mapping respectively. The learning cost function mismatch between the
separation and dereverberation models and their integration with the back-end
recognition system is minimised using fine-tuning on the MSE and LF-MMI
criteria. Experiments conducted on the LRS2 dataset suggest that the proposed
audio-visual multi-channel speech separation, dereverberation and recognition
system outperforms the baseline audio-visual multi-channel speech separation
and recognition system containing no dereverberation module by a statistically
significant word error rate (WER) reduction of 2.06% absolute (8.77% relative).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Weight Respecification of Scan-specific Learning for Parallel Imaging. (arXiv:2204.01979v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01979">
<div class="article-summary-box-inner">
<span><p>Parallel imaging is widely used in magnetic resonance imaging as an
acceleration technology. Traditional linear reconstruction methods in parallel
imaging often suffer from noise amplification. Recently, a non-linear robust
artificial-neural-network for k-space interpolation (RAKI) exhibits superior
noise resilience over other linear methods. However, RAKI performs poorly at
high acceleration rates, and needs a large amount of autocalibration signals as
the training samples. In order to tackle these issues, we propose a
multi-weight method that implements multiple weighting matrices on the
undersampled data, named as MW-RAKI. Enforcing multiple weighted matrices on
the measurements can effectively reduce the influence of noise and increase the
data constraints. Furthermore, we incorporate the strategy of multiple
weighting matrixes into a residual version of RAKI, and form
MW-rRAKI.Experimental compari-sons with the alternative methods demonstrated
noticeably better reconstruction performances, particularly at high
acceleration rates.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bimodal Distributed Binarized Neural Networks. (arXiv:2204.02004v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02004">
<div class="article-summary-box-inner">
<span><p>Binary Neural Networks (BNNs) are an extremely promising method to reduce
deep neural networks' complexity and power consumption massively. Binarization
techniques, however, suffer from ineligible performance degradation compared to
their full-precision counterparts.
</p>
<p>Prior work mainly focused on strategies for sign function approximation
during forward and backward phases to reduce the quantization error during the
binarization process. In this work, we propose a Bi-Modal Distributed
binarization method (\methodname{}). That imposes bi-modal distribution of the
network weights by kurtosis regularization. The proposed method consists of a
training scheme that we call Weight Distribution Mimicking (WDM), which
efficiently imitates the full-precision network weight distribution to their
binary counterpart. Preserving this distribution during binarization-aware
training creates robust and informative binary feature maps and significantly
reduces the generalization error of the BNN. Extensive evaluations on CIFAR-10
and ImageNet demonstrate the superiority of our method over current
state-of-the-art schemes. Our source code, experimental settings, training
logs, and binary models are available at
\url{https://github.com/BlueAnon/BD-BNN}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Video Salient Object Detection Progressively from Unlabeled Videos. (arXiv:2204.02008v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02008">
<div class="article-summary-box-inner">
<span><p>Recent deep learning-based video salient object detection (VSOD) has achieved
some breakthrough, but these methods rely on expensive annotated videos with
pixel-wise annotations, weak annotations, or part of the pixel-wise
annotations. In this paper, based on the similarities and the differences
between VSOD and image salient object detection (SOD), we propose a novel VSOD
method via a progressive framework that locates and segments salient objects in
sequence without utilizing any video annotation. To use the knowledge learned
in the SOD dataset for VSOD efficiently, we introduce dynamic saliency to
compensate for the lack of motion information of SOD during the locating
process but retain the same fine segmenting process. Specifically, an algorithm
for generating spatiotemporal location labels, which consists of generating
high-saliency location labels and tracking salient objects in adjacent frames,
is proposed. Based on these location labels, a two-stream locating network that
introduces an optical flow branch for video salient object locating is
presented. Although our method does not require labeled video at all, the
experimental results on five public benchmarks of DAVIS, FBMS, ViSal, VOS, and
DAVSOD demonstrate that our proposed method is competitive with fully
supervised methods and outperforms the state-of-the-art weakly and unsupervised
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LatentGAN Autoencoder: Learning Disentangled Latent Distribution. (arXiv:2204.02010v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02010">
<div class="article-summary-box-inner">
<span><p>In autoencoder, the encoder generally approximates the latent distribution
over the dataset, and the decoder generates samples using this learned latent
distribution. There is very little control over the latent vector as using the
random latent vector for generation will lead to trivial outputs. This work
tries to address this issue by using the LatentGAN generator to directly learn
to approximate the latent distribution of the autoencoder and show meaningful
results on MNIST, 3D Chair, and CelebA datasets, an additional
information-theoretic constrain is used which successfully learns to control
autoencoder latent distribution. With this, our model also achieves an error
rate of 2.38 on MNIST unsupervised image classification, which is better as
compared to InfoGAN and AAE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Generative Deep Learning Approach to Stochastic Downscaling of Precipitation Forecasts. (arXiv:2204.02028v1 [physics.ao-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02028">
<div class="article-summary-box-inner">
<span><p>Despite continuous improvements, precipitation forecasts are still not as
accurate and reliable as those of other meteorological variables. A major
contributing factor to this is that several key processes affecting
precipitation distribution and intensity occur below the resolved scale of
global weather models. Generative adversarial networks (GANs) have been
demonstrated by the computer vision community to be successful at
super-resolution problems, i.e., learning to add fine-scale structure to coarse
images. Leinonen et al. (2020) previously applied a GAN to produce ensembles of
reconstructed high-resolution atmospheric fields, given coarsened input data.
In this paper, we demonstrate this approach can be extended to the more
challenging problem of increasing the accuracy and resolution of comparatively
low-resolution input from a weather forecasting model, using high-resolution
radar measurements as a "ground truth". The neural network must learn to add
resolution and structure whilst accounting for non-negligible forecast error.
We show that GANs and VAE-GANs can match the statistical properties of
state-of-the-art pointwise post-processing methods whilst creating
high-resolution, spatially coherent precipitation maps. Our model compares
favourably to the best existing downscaling methods in both pixel-wise and
pooled CRPS scores, power spectrum information and rank histograms (used to
assess calibration). We test our models and show that they perform in a range
of scenarios, including heavy rainfall.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Reduce Information Bottleneck for Object Detection in Aerial Images. (arXiv:2204.02033v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02033">
<div class="article-summary-box-inner">
<span><p>Object detection in aerial images is a fundamental research topic in the
domain of geoscience and remote sensing. However, advanced progresses on this
topic are mainly focused on the designment of backbone networks or header
networks, but surprisingly ignored the neck ones. In this letter, we first
analyse the importance of the neck network in object detection frameworks from
the theory of information bottleneck. Then, to alleviate the information loss
problem in the current neck network, we propose a global semantic network,
which acts as a bridge from the backbone to the head network in a bidirectional
global convolution manner. Compared to the existing neck networks, our method
has advantages of capturing rich detailed information and less computational
costs. Moreover, we further propose a fusion refinement module, which is used
for feature fusion with rich details from different scales. To demonstrate the
effectiveness and efficiency of our method, experiments are carried out on two
challenging datasets (i.e., DOTA and HRSC2016). Results in terms of accuracy
and computational complexity both can verify the superiority of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DT2I: Dense Text-to-Image Generation from Region Descriptions. (arXiv:2204.02035v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02035">
<div class="article-summary-box-inner">
<span><p>Despite astonishing progress, generating realistic images of complex scenes
remains a challenging problem. Recently, layout-to-image synthesis approaches
have attracted much interest by conditioning the generator on a list of
bounding boxes and corresponding class labels. However, previous approaches are
very restrictive because the set of labels is fixed a priori. Meanwhile,
text-to-image synthesis methods have substantially improved and provide a
flexible way for conditional image generation. In this work, we introduce dense
text-to-image (DT2I) synthesis as a new task to pave the way toward more
intuitive image generation. Furthermore, we propose DTC-GAN, a novel method to
generate images from semantically rich region descriptions, and a multi-modal
region feature matching loss to encourage semantic image-text matching. Our
results demonstrate the capability of our approach to generate plausible images
of complex scenes using region captions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An efficient real-time target tracking algorithm using adaptive feature fusion. (arXiv:2204.02054v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02054">
<div class="article-summary-box-inner">
<span><p>Visual-based target tracking is easily influenced by multiple factors, such
as background clutter, targets fast-moving, illumination variation, object
shape change, occlusion, etc. These factors influence the tracking accuracy of
a target tracking task. To address this issue, an efficient real-time target
tracking method based on a low-dimension adaptive feature fusion is proposed to
allow us the simultaneous implementation of the high-accuracy and real-time
target tracking. First, the adaptive fusion of a histogram of oriented gradient
(HOG) feature and color feature is utilized to improve the tracking accuracy.
Second, a convolution dimension reduction method applies to the fusion between
the HOG feature and color feature to reduce the over-fitting caused by their
high-dimension fusions. Third, an average correlation energy estimation method
is used to extract the relative confidence adaptive coefficients to ensure
tracking accuracy. We experimentally confirm the proposed method on an OTB100
data set. Compared with nine popular target tracking algorithms, the proposed
algorithm gains the highest tracking accuracy and success tracking rate.
Compared with the traditional Sum of Template and Pixel-wise LEarners (STAPLE)
algorithm, the proposed algorithm can obtain a higher success rate and
accuracy, improving by 0.023 and 0.019, respectively. The experimental results
also demonstrate that the proposed algorithm can reach the real-time target
tracking with 50 fps. The proposed method paves a more promising way for
real-time target tracking tasks under a complex environment, such as appearance
deformation, illumination change, motion blur, background, similarity, scale
change, and occlusion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spread Spurious Attribute: Improving Worst-group Accuracy with Spurious Attribute Estimation. (arXiv:2204.02070v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02070">
<div class="article-summary-box-inner">
<span><p>The paradigm of worst-group loss minimization has shown its promise in
avoiding to learn spurious correlations, but requires costly additional
supervision on spurious attributes. To resolve this, recent works focus on
developing weaker forms of supervision -- e.g., hyperparameters discovered with
a small number of validation samples with spurious attribute annotation -- but
none of the methods retain comparable performance to methods using full
supervision on the spurious attribute. In this paper, instead of searching for
weaker supervisions, we ask: Given access to a fixed number of samples with
spurious attribute annotations, what is the best achievable worst-group loss if
we "fully exploit" them? To this end, we propose a pseudo-attribute-based
algorithm, coined Spread Spurious Attribute (SSA), for improving the
worst-group accuracy. In particular, we leverage samples both with and without
spurious attribute annotations to train a model to predict the spurious
attribute, then use the pseudo-attribute predicted by the trained model as
supervision on the spurious attribute to train a new robust model having
minimal worst-group loss. Our experiments on various benchmark datasets show
that our algorithm consistently outperforms the baseline methods using the same
number of validation samples with spurious attribute annotations. We also
demonstrate that the proposed SSA can achieve comparable performances to
methods using full (100%) spurious attribute supervision, by using a much
smaller number of annotated samples -- from 0.6% and up to 1.5%, depending on
the dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Split Hierarchical Variational Compression. (arXiv:2204.02071v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02071">
<div class="article-summary-box-inner">
<span><p>Variational autoencoders (VAEs) have witnessed great success in performing
the compression of image datasets. This success, made possible by the bits-back
coding framework, has produced competitive compression performance across many
benchmarks. However, despite this, VAE architectures are currently limited by a
combination of coding practicalities and compression ratios. That is, not only
do state-of-the-art methods, such as normalizing flows, often demonstrate
out-performance, but the initial bits required in coding makes single and
parallel image compression challenging. To remedy this, we introduce Split
Hierarchical Variational Compression (SHVC). SHVC introduces two novelties.
Firstly, we propose an efficient autoregressive prior, the autoregressive
sub-pixel convolution, that allows a generalisation between per-pixel
autoregressions and fully factorised probability models. Secondly, we define
our coding framework, the autoregressive initial bits, that flexibly supports
parallel coding and avoids -- for the first time -- many of the practicalities
commonly associated with bits-back coding. In our experiments, we demonstrate
SHVC is able to achieve state-of-the-art compression performance across
full-resolution lossless image compression tasks, with up to 100x fewer model
parameters than competing VAE approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Complex-Valued Autoencoders for Object Discovery. (arXiv:2204.02075v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02075">
<div class="article-summary-box-inner">
<span><p>Object-centric representations form the basis of human perception and enable
us to reason about the world and to systematically generalize to new settings.
Currently, most machine learning work on unsupervised object discovery focuses
on slot-based approaches, which explicitly separate the latent representations
of individual objects. While the result is easily interpretable, it usually
requires the design of involved architectures. In contrast to this, we propose
a distributed approach to object-centric representations: the Complex
AutoEncoder. Following a coding scheme theorized to underlie object
representations in biological neurons, its complex-valued activations represent
two messages: their magnitudes express the presence of a feature, while the
relative phase differences between neurons express which features should be
bound together to create joint object representations. We show that this simple
and efficient approach achieves better reconstruction performance than an
equivalent real-valued autoencoder on simple multi-object datasets.
Additionally, we show that it achieves competitive unsupervised object
discovery performance to a SlotAttention model on two datasets, and manages to
disentangle objects in a third dataset where SlotAttention fails - all while
being 7-70 times faster to train.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-supervised Semantic Segmentation with Error Localization Network. (arXiv:2204.02078v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02078">
<div class="article-summary-box-inner">
<span><p>This paper studies semi-supervised learning of semantic segmentation, which
assumes that only a small portion of training images are labeled and the others
remain unlabeled. The unlabeled images are usually assigned pseudo labels to be
used in training, which however often causes the risk of performance
degradation due to the confirmation bias towards errors on the pseudo labels.
We present a novel method that resolves this chronic issue of pseudo labeling.
At the heart of our method lies error localization network (ELN), an auxiliary
module that takes an image and its segmentation prediction as input and
identifies pixels whose pseudo labels are likely to be wrong. ELN enables
semi-supervised learning to be robust against inaccurate pseudo labels by
disregarding label noises during training and can be naturally integrated with
self-training and contrastive learning. Moreover, we introduce a new learning
strategy for ELN that simulates plausible and diverse segmentation errors
during training of ELN to enhance its generalization. Our method is evaluated
on PASCAL VOC 2012 and Cityscapes, where it outperforms all existing methods in
every evaluation setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-time Online Multi-Object Tracking in Compressed Domain. (arXiv:2204.02081v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02081">
<div class="article-summary-box-inner">
<span><p>Recent online Multi-Object Tracking (MOT) methods have achieved desirable
tracking performance. However, the tracking speed of most existing methods is
rather slow. Inspired from the fact that the adjacent frames are highly
relevant and redundant, we divide the frames into key and non-key frames
respectively and track objects in the compressed domain. For the key frames,
the RGB images are restored for detection and data association. To make data
association more reliable, an appearance Convolutional Neural Network (CNN)
which can be jointly trained with the detector is proposed. For the non-key
frames, the objects are directly propagated by a tracking CNN based on the
motion information provided in the compressed domain. Compared with the
state-of-the-art online MOT methods,our tracker is about 6x faster while
maintaining a comparable tracking performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-time Hyperspectral Imaging in Hardware via Trained Metasurface Encoders. (arXiv:2204.02084v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02084">
<div class="article-summary-box-inner">
<span><p>Hyperspectral imaging has attracted significant attention to identify
spectral signatures for image classification and automated pattern recognition
in computer vision. State-of-the-art implementations of snapshot hyperspectral
imaging rely on bulky, non-integrated, and expensive optical elements,
including lenses, spectrometers, and filters. These macroscopic components do
not allow fast data processing for, e.g real-time and high-resolution videos.
This work introduces Hyplex, a new integrated architecture addressing the
limitations discussed above. Hyplex is a CMOS-compatible, fast hyperspectral
camera that replaces bulk optics with nanoscale metasurfaces inversely designed
through artificial intelligence. Hyplex does not require spectrometers but
makes use of conventional monochrome cameras, opening up the possibility for
real-time and high-resolution hyperspectral imaging at inexpensive costs.
Hyplex exploits a model-driven optimization, which connects the physical
metasurfaces layer with modern visual computing approaches based on end-to-end
training. We design and implement a prototype version of Hyplex and compare its
performance against the state-of-the-art for typical imaging tasks such as
spectral reconstruction and semantic segmentation. In all benchmarks, Hyplex
reports the smallest reconstruction error. We additionally present what is, to
the best of our knowledge, the largest publicly available labeled hyperspectral
dataset for semantic segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VocaLiST: An Audio-Visual Synchronisation Model for Lips and Voices. (arXiv:2204.02090v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02090">
<div class="article-summary-box-inner">
<span><p>In this paper, we address the problem of lip-voice synchronisation in videos
containing human face and voice. Our approach is based on determining if the
lips motion and the voice in a video are synchronised or not, depending on
their audio-visual correspondence score. We propose an audio-visual cross-modal
transformer-based model that outperforms several baseline models in the
audio-visual synchronisation task on the standard lip-reading speech benchmark
dataset LRS2. While the existing methods focus mainly on the lip
synchronisation in speech videos, we also consider the special case of singing
voice. Singing voice is a more challenging use case for synchronisation due to
sustained vowel sounds. We also investigate the relevance of lip
synchronisation models trained on speech datasets in the context of singing
voice. Finally, we use the frozen visual features learned by our lip
synchronisation model in the singing voice separation task to outperform a
baseline audio-visual model which was trained end-to-end. The demos, source
code, and the pre-trained model will be made available on
https://ipcv.github.io/VocaLiST/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">P3Depth: Monocular Depth Estimation with a Piecewise Planarity Prior. (arXiv:2204.02091v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02091">
<div class="article-summary-box-inner">
<span><p>Monocular depth estimation is vital for scene understanding and downstream
tasks. We focus on the supervised setup, in which ground-truth depth is
available only at training time. Based on knowledge about the high regularity
of real 3D scenes, we propose a method that learns to selectively leverage
information from coplanar pixels to improve the predicted depth. In particular,
we introduce a piecewise planarity prior which states that for each pixel,
there is a seed pixel which shares the same planar 3D surface with the former.
Motivated by this prior, we design a network with two heads. The first head
outputs pixel-level plane coefficients, while the second one outputs a dense
offset vector field that identifies the positions of seed pixels. The plane
coefficients of seed pixels are then used to predict depth at each position.
The resulting prediction is adaptively fused with the initial prediction from
the first head via a learned confidence to account for potential deviations
from precise local planarity. The entire architecture is trained end-to-end
thanks to the differentiability of the proposed modules and it learns to
predict regular depth maps, with sharp edges at occlusion boundaries. An
extensive evaluation of our method shows that we set the new state of the art
in supervised monocular depth estimation, surpassing prior methods on NYU
Depth-v2 and on the Garg split of KITTI. Our method delivers depth maps that
yield plausible 3D reconstructions of the input scenes. Code is available at:
https://github.com/SysCV/P3Depth
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Birds of A Feather Flock Together: Category-Divergence Guidance for Domain Adaptive Segmentation. (arXiv:2204.02111v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02111">
<div class="article-summary-box-inner">
<span><p>Unsupervised domain adaptation (UDA) aims to enhance the generalization
capability of a certain model from a source domain to a target domain. Present
UDA models focus on alleviating the domain shift by minimizing the feature
discrepancy between the source domain and the target domain but usually ignore
the class confusion problem. In this work, we propose an Inter-class Separation
and Intra-class Aggregation (ISIA) mechanism. It encourages the cross-domain
representative consistency between the same categories and differentiation
among diverse categories. In this way, the features belonging to the same
categories are aligned together and the confusable categories are separated. By
measuring the align complexity of each category, we design an Adaptive-weighted
Instance Matching (AIM) strategy to further optimize the instance-level
adaptation. Based on our proposed methods, we also raise a hierarchical
unsupervised domain adaptation framework for cross-domain semantic segmentation
task. Through performing the image-level, feature-level, category-level and
instance-level alignment, our method achieves a stronger generalization
performance of the model from the source domain to the target domain. In two
typical cross-domain semantic segmentation tasks, i.e., GTA5 to Cityscapes and
SYNTHIA to Cityscapes, our method achieves the state-of-the-art segmentation
accuracy. We also build two cross-domain semantic segmentation datasets based
on the publicly available data, i.e., remote sensing building segmentation and
road segmentation, for domain adaptive segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Overcoming Catastrophic Forgetting in Incremental Object Detection via Elastic Response Distillation. (arXiv:2204.02136v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02136">
<div class="article-summary-box-inner">
<span><p>Traditional object detectors are ill-equipped for incremental learning.
However, fine-tuning directly on a well-trained detection model with only new
data will lead to catastrophic forgetting. Knowledge distillation is a flexible
way to mitigate catastrophic forgetting. In Incremental Object Detection (IOD),
previous work mainly focuses on distilling for the combination of features and
responses. However, they under-explore the information that contains in
responses. In this paper, we propose a response-based incremental distillation
method, dubbed Elastic Response Distillation (ERD), which focuses on
elastically learning responses from the classification head and the regression
head. Firstly, our method transfers category knowledge while equipping student
detector with the ability to retain localization information during incremental
learning. In addition, we further evaluate the quality of all locations and
provide valuable responses by the Elastic Response Selection (ERS) strategy.
Finally, we elucidate that the knowledge from different responses should be
assigned with different importance during incremental distillation. Extensive
experiments conducted on MS COCO demonstrate our method achieves
state-of-the-art result, which substantially narrows the performance gap
towards full training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detector-Free Weakly Supervised Group Activity Recognition. (arXiv:2204.02139v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02139">
<div class="article-summary-box-inner">
<span><p>Group activity recognition is the task of understanding the activity
conducted by a group of people as a whole in a multi-person video. Existing
models for this task are often impractical in that they demand ground-truth
bounding box labels of actors even in testing or rely on off-the-shelf object
detectors. Motivated by this, we propose a novel model for group activity
recognition that depends neither on bounding box labels nor on object detector.
Our model based on Transformer localizes and encodes partial contexts of a
group activity by leveraging the attention mechanism, and represents a video
clip as a set of partial context embeddings. The embedding vectors are then
aggregated to form a single group representation that reflects the entire
context of an activity while capturing temporal evolution of each partial
context. Our method achieves outstanding performance on two benchmarks,
Volleyball and NBA datasets, surpassing not only the state of the art trained
with the same level of supervision, but also some of existing models relying on
stronger supervision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual-AI: Dual-path Action Interaction Learning for Group Activity Recognition. (arXiv:2204.02148v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02148">
<div class="article-summary-box-inner">
<span><p>Learning spatial-temporal relation among multiple actors is crucial for group
activity recognition. Different group activities often show the diversified
interactions between actors in the video. Hence, it is often difficult to model
complex group activities from a single view of spatial-temporal actor
evolution. To tackle this problem, we propose a distinct Dual-path Actor
Interaction (DualAI) framework, which flexibly arranges spatial and temporal
transformers in two complementary orders, enhancing actor relations by
integrating merits from different spatiotemporal paths. Moreover, we introduce
a novel Multi-scale Actor Contrastive Loss (MAC-Loss) between two interactive
paths of Dual-AI. Via self-supervised actor consistency in both frame and video
levels, MAC-Loss can effectively distinguish individual actor representations
to reduce action confusion among different actors. Consequently, our Dual-AI
can boost group activity recognition by fusing such discriminative features of
different actors. To evaluate the proposed approach, we conduct extensive
experiments on the widely used benchmarks, including Volleyball, Collective
Activity, and NBA datasets. The proposed Dual-AI achieves state-of-the-art
performance on all these datasets. It is worth noting the proposed Dual-AI with
50% training data outperforms a number of recent approaches with 100% training
data. This confirms the generalization power of Dual-AI for group activity
recognition, even under the challenging scenarios of limited supervision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Image Content Extraction: Operationalizing Machine Learning in Humanistic Photographic Studies of Large Visual Archives. (arXiv:2204.02149v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02149">
<div class="article-summary-box-inner">
<span><p>Applying machine learning tools to digitized image archives has a potential
to revolutionize quantitative research of visual studies in humanities and
social sciences. The ability to process a hundredfold greater number of photos
than has been traditionally possible and to analyze them with an extensive set
of variables will contribute to deeper insight into the material. Overall,
these changes will help to shift the workflow from simple manual tasks to more
demanding stages.
</p>
<p>In this paper, we introduce Automatic Image Content Extraction (AICE)
framework for machine learning-based search and analysis of large image
archives. We developed the framework in a multidisciplinary research project as
framework for future photographic studies by reformulating and expanding the
traditional visual content analysis methodologies to be compatible with the
current and emerging state-of-the-art machine learning tools and to cover the
novel machine learning opportunities for automatic content analysis. The
proposed framework can be applied in several domains in humanities and social
sciences, and it can be adjusted and scaled into various research settings. We
also provide information on the current state of different machine learning
techniques and show that there are already various publicly available methods
that are suitable to a wide-scale of visual content analysis tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Equivariant Features for Absolute Pose Regression. (arXiv:2204.02163v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02163">
<div class="article-summary-box-inner">
<span><p>While end-to-end approaches have achieved state-of-the-art performance in
many perception tasks, they are not yet able to compete with 3D geometry-based
methods in pose estimation. Moreover, absolute pose regression has been shown
to be more related to image retrieval. As a result, we hypothesize that the
statistical features learned by classical Convolutional Neural Networks do not
carry enough geometric information to reliably solve this inherently geometric
task. In this paper, we demonstrate how a translation and rotation equivariant
Convolutional Neural Network directly induces representations of camera motions
into the feature space. We then show that this geometric property allows for
implicitly augmenting the training data under a whole group of image
plane-preserving transformations. Therefore, we argue that directly learning
equivariant features is preferable than learning data-intensive intermediate
representations. Comprehensive experimental validation demonstrates that our
lightweight model outperforms existing ones on standard datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Learning of Feature Extraction and Cost Aggregation for Semantic Correspondence. (arXiv:2204.02164v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02164">
<div class="article-summary-box-inner">
<span><p>Establishing dense correspondences across semantically similar images is one
of the challenging tasks due to the significant intra-class variations and
background clutters. To solve these problems, numerous methods have been
proposed, focused on learning feature extractor or cost aggregation
independently, which yields sub-optimal performance. In this paper, we propose
a novel framework for jointly learning feature extraction and cost aggregation
for semantic correspondence. By exploiting the pseudo labels from each module,
the networks consisting of feature extraction and cost aggregation modules are
simultaneously learned in a boosting fashion. Moreover, to ignore unreliable
pseudo labels, we present a confidence-aware contrastive loss function for
learning the networks in a weakly-supervised manner. We demonstrate our
competitive results on standard benchmarks for semantic correspondence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-View Transformer for 3D Visual Grounding. (arXiv:2204.02174v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02174">
<div class="article-summary-box-inner">
<span><p>The 3D visual grounding task aims to ground a natural language description to
the targeted object in a 3D scene, which is usually represented in 3D point
clouds. Previous works studied visual grounding under specific views. The
vision-language correspondence learned by this way can easily fail once the
view changes. In this paper, we propose a Multi-View Transformer (MVT) for 3D
visual grounding. We project the 3D scene to a multi-view space, in which the
position information of the 3D scene under different views are modeled
simultaneously and aggregated together. The multi-view space enables the
network to learn a more robust multi-modal representation for 3D visual
grounding and eliminates the dependence on specific views. Extensive
experiments show that our approach significantly outperforms all
state-of-the-art methods. Specifically, on Nr3D and Sr3D datasets, our method
outperforms the best competitor by 11.2% and 7.1% and even surpasses recent
work with extra 2D assistance by 5.9% and 6.6%. Our code is available at
https://github.com/sega-hsj/MVT-3DVG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision Transformer Equipped with Neural Resizer on Facial Expression Recognition Task. (arXiv:2204.02181v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02181">
<div class="article-summary-box-inner">
<span><p>When it comes to wild conditions, Facial Expression Recognition is often
challenged with low-quality data and imbalanced, ambiguous labels. This field
has much benefited from CNN based approaches; however, CNN models have
structural limitation to see the facial regions in distant. As a remedy,
Transformer has been introduced to vision fields with global receptive field,
but requires adjusting input spatial size to the pretrained models to enjoy
their strong inductive bias at hands. We herein raise a question whether using
the deterministic interpolation method is enough to feed low-resolution data to
Transformer. In this work, we propose a novel training framework, Neural
Resizer, to support Transformer by compensating information and downscaling in
a data-driven manner trained with loss function balancing the noisiness and
imbalance. Experiments show our Neural Resizer with F-PDLS loss function
improves the performance with Transformer variants in general and nearly
achieves the state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SNUG: Self-Supervised Neural Dynamic Garments. (arXiv:2204.02219v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02219">
<div class="article-summary-box-inner">
<span><p>We present a self-supervised method to learn dynamic 3D deformations of
garments worn by parametric human bodies. State-of-the-art data-driven
approaches to model 3D garment deformations are trained using supervised
strategies that require large datasets, usually obtained by expensive
physics-based simulation methods or professional multi-camera capture setups.
In contrast, we propose a new training scheme that removes the need for
ground-truth samples, enabling self-supervised training of dynamic 3D garment
deformations. Our key contribution is to realize that physics-based deformation
models, traditionally solved in a frame-by-frame basis by implicit integrators,
can be recasted as an optimization problem. We leverage such optimization-based
scheme to formulate a set of physics-based loss terms that can be used to train
neural networks without precomputing ground-truth data. This allows us to learn
models for interactive garments, including dynamic deformations and fine
wrinkles, with two orders of magnitude speed up in training time compared to
state-of-the-art supervised methods
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When Sparsity Meets Dynamic Convolution. (arXiv:2204.02227v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02227">
<div class="article-summary-box-inner">
<span><p>Dynamic convolution achieves a substantial performance boost for efficient
CNNs at a cost of increased convolutional weights. Contrastively, mask-based
unstructured pruning obtains a lightweight network by removing redundancy in
the heavy network at risk of performance drop. In this paper, we propose a new
framework to coherently integrate these two paths so that they can complement
each other compensate for the disadvantages. We first design a binary mask
derived from a learnable threshold to prune static kernels, significantly
reducing the parameters and computational cost but achieving higher performance
in Imagenet-1K(0.6\% increase in top-1 accuracy with 0.67G fewer FLOPs). Based
on this learnable mask, we further propose a novel dynamic sparse network
incorporating the dynamic routine mechanism, which exerts much higher accuracy
than baselines ($2.63\%$ increase in top-1 accuracy for MobileNetV1 with $90\%$
sparsity). As a result, our method demonstrates a more efficient dynamic
convolution with sparsity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IRON: Inverse Rendering by Optimizing Neural SDFs and Materials from Photometric Images. (arXiv:2204.02232v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02232">
<div class="article-summary-box-inner">
<span><p>We propose a neural inverse rendering pipeline called IRON that operates on
photometric images and outputs high-quality 3D content in the format of
triangle meshes and material textures readily deployable in existing graphics
pipelines. Our method adopts neural representations for geometry as signed
distance fields (SDFs) and materials during optimization to enjoy their
flexibility and compactness, and features a hybrid optimization scheme for
neural SDFs: first, optimize using a volumetric radiance field approach to
recover correct topology, then optimize further using edgeaware physics-based
surface rendering for geometry refinement and disentanglement of materials and
lighting. In the second stage, we also draw inspiration from mesh-based
differentiable rendering, and design a novel edge sampling algorithm for neural
SDFs to further improve performance. We show that our IRON achieves
significantly better inverse rendering quality compared to prior works. Our
project page is here: https://kai-46.github.io/IRON-website/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RBGNet: Ray-based Grouping for 3D Object Detection. (arXiv:2204.02251v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02251">
<div class="article-summary-box-inner">
<span><p>As a fundamental problem in computer vision, 3D object detection is
experiencing rapid growth. To extract the point-wise features from the
irregularly and sparsely distributed points, previous methods usually take a
feature grouping module to aggregate the point features to an object candidate.
However, these methods have not yet leveraged the surface geometry of
foreground objects to enhance grouping and 3D box generation. In this paper, we
propose the RBGNet framework, a voting-based 3D detector for accurate 3D object
detection from point clouds. In order to learn better representations of object
shape to enhance cluster features for predicting 3D boxes, we propose a
ray-based feature grouping module, which aggregates the point-wise features on
object surfaces using a group of determined rays uniformly emitted from cluster
centers. Considering the fact that foreground points are more meaningful for
box estimation, we design a novel foreground biased sampling strategy in
downsample process to sample more points on object surfaces and further boost
the detection performance. Our model achieves state-of-the-art 3D detection
performance on ScanNet V2 and SUN RGB-D with remarkable performance gains. Code
will be available at https://github.com/Haiyang-W/RBGNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Probabilistic Normal Epipolar Constraint for Frame-To-Frame Rotation Optimization under Uncertain Feature Positions. (arXiv:2204.02256v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02256">
<div class="article-summary-box-inner">
<span><p>The estimation of the relative pose of two camera views is a fundamental
problem in computer vision. Kneip et al. proposed to solve this problem by
introducing the normal epipolar constraint (NEC). However, their approach does
not take into account uncertainties, so that the accuracy of the estimated
relative pose is highly dependent on accurate feature positions in the target
frame. In this work, we introduce the probabilistic normal epipolar constraint
(PNEC) that overcomes this limitation by accounting for anisotropic and
inhomogeneous uncertainties in the feature positions. To this end, we propose a
novel objective function, along with an efficient optimization scheme that
effectively minimizes our objective while maintaining real-time performance. In
experiments on synthetic data, we demonstrate that the novel PNEC yields more
accurate rotation estimates than the original NEC and several popular relative
rotation estimation algorithms. Furthermore, we integrate the proposed method
into a state-of-the-art monocular rotation-only odometry system and achieve
consistently improved results for the real-world KITTI dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Arbitrary-Scale Image Synthesis. (arXiv:2204.02273v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02273">
<div class="article-summary-box-inner">
<span><p>Positional encodings have enabled recent works to train a single adversarial
network that can generate images of different scales. However, these approaches
are either limited to a set of discrete scales or struggle to maintain good
perceptual quality at the scales for which the model is not trained explicitly.
We propose the design of scale-consistent positional encodings invariant to our
generator's layers transformations. This enables the generation of
arbitrary-scale images even at scales unseen during training. Moreover, we
incorporate novel inter-scale augmentations into our pipeline and partial
generation training to facilitate the synthesis of consistent images at
arbitrary scales. Lastly, we show competitive results for a continuum of scales
on various commonly used datasets for image synthesis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grounding of the Functional Object-Oriented Network in Industrial Tasks. (arXiv:2204.02274v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02274">
<div class="article-summary-box-inner">
<span><p>In this preliminary work, we propose to design an activity recognition system
that is suitable for Industrie 4.0 (I4.0) applications, especially focusing on
Learning from Demonstration (LfD) in collaborative robot tasks. More precisely,
we focus on the issue of data exchange between an activity recognition system
and a collaborative robotic system. We propose an activity recognition system
with linked data using functional object-oriented network (FOON) to facilitate
industrial use cases. Initially, we drafted a FOON for our use case.
Afterwards, an action is estimated by using object and hand recognition systems
coupled with a recurrent neural network, which refers to FOON objects and
states. Finally, the detected action is shared via a context broker using an
existing linked data model, thus enabling the robotic system to interpret the
action and execute it afterwards. Our initial results show that FOON can be
used for an industrial use case and that we can use existing linked data models
in LfD applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Clustering via Center-Oriented Margin Free-Triplet Loss for Skin Lesion Detection in Highly Imbalanced Datasets. (arXiv:2204.02275v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02275">
<div class="article-summary-box-inner">
<span><p>Melanoma is a fatal skin cancer that is curable and has dramatically
increasing survival rate when diagnosed at early stages. Learning-based methods
hold significant promise for the detection of melanoma from dermoscopic images.
However, since melanoma is a rare disease, existing databases of skin lesions
predominantly contain highly imbalanced numbers of benign versus malignant
samples. In turn, this imbalance introduces substantial bias in classification
models due to the statistical dominance of the majority class. To address this
issue, we introduce a deep clustering approach based on the latent-space
embedding of dermoscopic images. Clustering is achieved using a novel
center-oriented margin-free triplet loss (COM-Triplet) enforced on image
embeddings from a convolutional neural network backbone. The proposed method
aims to form maximally-separated cluster centers as opposed to minimizing
classification error, so it is less sensitive to class imbalance. To avoid the
need for labeled data, we further propose to implement COM-Triplet based on
pseudo-labels generated by a Gaussian mixture model. Comprehensive experiments
show that deep clustering with COM-Triplet loss outperforms clustering with
triplet loss, and competing classifiers in both supervised and unsupervised
settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lost in Latent Space: Disentangled Models and the Challenge of Combinatorial Generalisation. (arXiv:2204.02283v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02283">
<div class="article-summary-box-inner">
<span><p>Recent research has shown that generative models with highly disentangled
representations fail to generalise to unseen combination of generative factor
values. These findings contradict earlier research which showed improved
performance in out-of-training distribution settings when compared to entangled
representations. Additionally, it is not clear if the reported failures are due
to (a) encoders failing to map novel combinations to the proper regions of the
latent space or (b) novel combinations being mapped correctly but the
decoder/downstream process is unable to render the correct output for the
unseen combinations. We investigate these alternatives by testing several
models on a range of datasets and training settings. We find that (i) when
models fail, their encoders also fail to map unseen combinations to correct
regions of the latent space and (ii) when models succeed, it is either because
the test conditions do not exclude enough examples, or because excluded
generative factors determine independent parts of the output image. Based on
these results, we argue that to generalise properly, models not only need to
capture factors of variation, but also understand how to invert the generative
process that was used to generate the data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SwapMix: Diagnosing and Regularizing the Over-Reliance on Visual Context in Visual Question Answering. (arXiv:2204.02285v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02285">
<div class="article-summary-box-inner">
<span><p>While Visual Question Answering (VQA) has progressed rapidly, previous works
raise concerns about robustness of current VQA models. In this work, we study
the robustness of VQA models from a novel perspective: visual context. We
suggest that the models over-rely on the visual context, i.e., irrelevant
objects in the image, to make predictions. To diagnose the model's reliance on
visual context and measure their robustness, we propose a simple yet effective
perturbation technique, SwapMix. SwapMix perturbs the visual context by
swapping features of irrelevant context objects with features from other
objects in the dataset. Using SwapMix we are able to change answers to more
than 45 % of the questions for a representative VQA model. Additionally, we
train the models with perfect sight and find that the context over-reliance
highly depends on the quality of visual representations. In addition to
diagnosing, SwapMix can also be applied as a data augmentation strategy during
training in order to regularize the context over-reliance. By swapping the
context object features, the model reliance on context can be suppressed
effectively. Two representative VQA models are studied using SwapMix: a
co-attention model MCAN and a large-scale pretrained model LXMERT. Our
experiments on the popular GQA dataset show the effectiveness of SwapMix for
both diagnosing model robustness and regularizing the over-reliance on visual
context. The code for our method is available at
https://github.com/vipulgupta1011/swapmix
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Visual Geo-localization for Large-Scale Applications. (arXiv:2204.02287v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02287">
<div class="article-summary-box-inner">
<span><p>Visual Geo-localization (VG) is the task of estimating the position where a
given photo was taken by comparing it with a large database of images of known
locations. To investigate how existing techniques would perform on a real-world
city-wide VG application, we build San Francisco eXtra Large, a new dataset
covering a whole city and providing a wide range of challenging cases, with a
size 30x bigger than the previous largest dataset for visual geo-localization.
We find that current methods fail to scale to such large datasets, therefore we
design a new highly scalable training technique, called CosPlace, which casts
the training as a classification problem avoiding the expensive mining needed
by the commonly used contrastive learning. We achieve state-of-the-art
performance on a wide range of datasets and find that CosPlace is robust to
heavy domain changes. Moreover, we show that, compared to the previous
state-of-the-art, CosPlace requires roughly 80% less GPU memory at train time,
and it achieves better results with 8x smaller descriptors, paving the way for
city-wide real-world visual geo-localization. Dataset, code and trained models
are available for research purposes at https://github.com/gmberton/CosPlace.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Convolutional Surfaces. (arXiv:2204.02289v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02289">
<div class="article-summary-box-inner">
<span><p>This work is concerned with a representation of shapes that disentangles
fine, local and possibly repeating geometry, from global, coarse structures.
Achieving such disentanglement leads to two unrelated advantages: i) a
significant compression in the number of parameters required to represent a
given geometry; ii) the ability to manipulate either global geometry, or local
details, without harming the other. At the core of our approach lies a novel
pipeline and neural architecture, which are optimized to represent one specific
atlas, representing one 3D surface. Our pipeline and architecture are designed
so that disentanglement of global geometry from local details is accomplished
through optimization, in a completely unsupervised manner. We show that this
approach achieves better neural shape compression than the state of the art, as
well as enabling manipulation and transfer of shape details. Project page at
<a href="http://geometry.cs.ucl.ac.uk/projects/2022/cnnmaps/">this http URL</a> .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">iSDF: Real-Time Neural Signed Distance Fields for Robot Perception. (arXiv:2204.02296v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02296">
<div class="article-summary-box-inner">
<span><p>We present iSDF, a continual learning system for real-time signed distance
field (SDF) reconstruction. Given a stream of posed depth images from a moving
camera, it trains a randomly initialised neural network to map input 3D
coordinate to approximate signed distance. The model is self-supervised by
minimising a loss that bounds the predicted signed distance using the distance
to the closest sampled point in a batch of query points that are actively
sampled. In contrast to prior work based on voxel grids, our neural method is
able to provide adaptive levels of detail with plausible filling in of
partially observed regions and denoising of observations, all while having a
more compact representation. In evaluations against alternative methods on real
and synthetic datasets of indoor environments, we find that iSDF produces more
accurate reconstructions, and better approximations of collision costs and
gradients useful for downstream planners in domains from navigation to
manipulation. Code and video results can be found at our project page:
https://joeaortiz.github.io/iSDF/ .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Generalizable Dexterous Manipulation from Human Grasp Affordance. (arXiv:2204.02320v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02320">
<div class="article-summary-box-inner">
<span><p>Dexterous manipulation with a multi-finger hand is one of the most
challenging problems in robotics. While recent progress in imitation learning
has largely improved the sample efficiency compared to Reinforcement Learning,
the learned policy can hardly generalize to manipulate novel objects, given
limited expert demonstrations. In this paper, we propose to learn dexterous
manipulation using large-scale demonstrations with diverse 3D objects in a
category, which are generated from a human grasp affordance model. This
generalizes the policy to novel object instances within the same category. To
train the policy, we propose a novel imitation learning objective jointly with
a geometric representation learning objective using our demonstrations. By
experimenting with relocating diverse objects in simulation, we show that our
approach outperforms baselines with a large margin when manipulating novel
objects. We also ablate the importance on 3D object representation learning for
manipulation. We include videos, code, and additional information on the
project website - https://kristery.github.io/ILAD/ .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A lightweight and accurate YOLO-like network for small target detection in Aerial Imagery. (arXiv:2204.02325v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02325">
<div class="article-summary-box-inner">
<span><p>Despite the breakthrough deep learning performances achieved for automatic
object detection, small target detection is still a challenging problem,
especially when looking at fast and accurate solutions suitable for mobile or
edge applications. In this work we present YOLO-S, a simple, fast and efficient
network for small target detection. The architecture exploits a small feature
extractor based on Darknet20, as well as skip connection, via both bypass and
concatenation, and reshape-passthrough layer to alleviate the vanishing
gradient problem, promote feature reuse across network and combine low-level
positional information with more meaningful high-level information. To verify
the performances of YOLO-S, we build "AIRES", a novel dataset for cAr detectIon
fRom hElicopter imageS acquired in Europe, and set up experiments on both AIRES
and VEDAI datasets, benchmarking this architecture with four baseline
detectors. Furthermore, in order to handle efficiently the issue of data
insufficiency and domain gap when dealing with a transfer learning strategy, we
introduce a transitional learning task over a combined dataset based on DOTAv2
and VEDAI and demonstrate that can enhance the overall accuracy with respect to
more general features transferred from COCO data. YOLO-S is from 25% to 50%
faster than YOLOv3 and only 15-25% slower than Tiny-YOLOv3, outperforming also
YOLOv3 in terms of accuracy in a wide range of experiments. Further simulations
performed on SARD dataset demonstrate also its applicability to different
scenarios such as for search and rescue operations. Besides, YOLO-S has an 87%
decrease of parameter size and almost one half FLOPs of YOLOv3, making
practical the deployment for low-power industrial applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLEVR-X: A Visual Reasoning Dataset for Natural Language Explanations. (arXiv:2204.02380v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02380">
<div class="article-summary-box-inner">
<span><p>Providing explanations in the context of Visual Question Answering (VQA)
presents a fundamental problem in machine learning. To obtain detailed insights
into the process of generating natural language explanations for VQA, we
introduce the large-scale CLEVR-X dataset that extends the CLEVR dataset with
natural language explanations. For each image-question pair in the CLEVR
dataset, CLEVR-X contains multiple structured textual explanations which are
derived from the original scene graphs. By construction, the CLEVR-X
explanations are correct and describe the reasoning and visual information that
is necessary to answer a given question. We conducted a user study to confirm
that the ground-truth explanations in our proposed dataset are indeed complete
and relevant. We present baseline results for generating natural language
explanations in the context of VQA using two state-of-the-art frameworks on the
CLEVR-X dataset. Furthermore, we provide a detailed analysis of the explanation
generation quality for different question and answer types. Additionally, we
study the influence of using different numbers of ground-truth explanations on
the convergence of natural language generation (NLG) metrics. The CLEVR-X
dataset is publicly available at
\url{https://explainableml.github.io/CLEVR-X/}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pyramid Frequency Network with Spatial Attention Residual Refinement Module for Monocular Depth Estimation. (arXiv:2204.02386v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02386">
<div class="article-summary-box-inner">
<span><p>Deep-learning-based approaches to depth estimation are rapidly advancing,
offering superior performance over existing methods. To estimate the depth in
real-world scenarios, depth estimation models require the robustness of various
noise environments. In this work, a Pyramid Frequency Network(PFN) with Spatial
Attention Residual Refinement Module(SARRM) is proposed to deal with the weak
robustness of existing deep-learning methods. To reconstruct depth maps with
accurate details, the SARRM constructs a residual fusion method with an
attention mechanism to refine the blur depth. The frequency division strategy
is designed, and the frequency pyramid network is developed to extract features
from multiple frequency bands. With the frequency strategy, PFN achieves better
visual accuracy than state-of-the-art methods in both indoor and outdoor scenes
on Make3D, KITTI depth, and NYUv2 datasets. Additional experiments on the noisy
NYUv2 dataset demonstrate that PFN is more reliable than existing deep-learning
methods in high-noise scenes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ObjectFolder 2.0: A Multisensory Object Dataset for Sim2Real Transfer. (arXiv:2204.02389v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02389">
<div class="article-summary-box-inner">
<span><p>Objects play a crucial role in our everyday activities. Though multisensory
object-centric learning has shown great potential lately, the modeling of
objects in prior work is rather unrealistic. ObjectFolder 1.0 is a recent
dataset that introduces 100 virtualized objects with visual, acoustic, and
tactile sensory data. However, the dataset is small in scale and the
multisensory data is of limited quality, hampering generalization to real-world
scenarios. We present ObjectFolder 2.0, a large-scale, multisensory dataset of
common household objects in the form of implicit neural representations that
significantly enhances ObjectFolder 1.0 in three aspects. First, our dataset is
10 times larger in the amount of objects and orders of magnitude faster in
rendering time. Second, we significantly improve the multisensory rendering
quality for all three modalities. Third, we show that models learned from
virtual objects in our dataset successfully transfer to their real-world
counterparts in three challenging tasks: object scale estimation, contact
localization, and shape reconstruction. ObjectFolder 2.0 offers a new path and
testbed for multisensory learning in computer vision and robotics. The dataset
is available at https://github.com/rhgao/ObjectFolder.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Pneumatic Non-Prehensile Manipulation with a Mobile Blower. (arXiv:2204.02390v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02390">
<div class="article-summary-box-inner">
<span><p>We investigate pneumatic non-prehensile manipulation (i.e., blowing) as a
means of efficiently moving scattered objects into a target receptacle. Due to
the chaotic nature of aerodynamic forces, a blowing controller must (i)
continually adapt to unexpected changes from its actions, (ii) maintain
fine-grained control, since the slightest misstep can result in large
unintended consequences (e.g., scatter objects already in a pile), and (iii)
infer long-range plans (e.g., move the robot to strategic blowing locations).
We tackle these challenges in the context of deep reinforcement learning,
introducing a multi-frequency version of the spatial action maps framework.
This allows for efficient learning of vision-based policies that effectively
combine high-level planning and low-level closed-loop control for dynamic
mobile manipulation. Experiments show that our system learns efficient
behaviors for the task, demonstrating in particular that blowing achieves
better downstream performance than pushing, and that our policies improve
performance over baselines. Moreover, we show that our system naturally
encourages emergent specialization between the different subpolicies spanning
low-level fine-grained control and high-level planning. On a real mobile robot
equipped with a miniature air blower, we show that our simulation-trained
policies transfer well to a real environment and can generalize to novel
objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Action-Conditioned Contrastive Policy Pretraining. (arXiv:2204.02393v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02393">
<div class="article-summary-box-inner">
<span><p>Deep visuomotor policy learning achieves promising results in control tasks
such as robotic manipulation and autonomous driving, where the action is
generated from the visual input by the neural policy. However, it requires a
huge number of online interactions with the training environment, which limits
its real-world application. Compared to the popular unsupervised feature
learning for visual recognition, feature pretraining for visuomotor control
tasks is much less explored. In this work, we aim to pretrain policy
representations for driving tasks using hours-long uncurated YouTube videos. A
new contrastive policy pretraining method is developed to learn
action-conditioned features from video frames with action pseudo labels.
Experiments show that the resulting action-conditioned features bring
substantial improvements to the downstream reinforcement learning and imitation
learning tasks, outperforming the weights pretrained from previous unsupervised
learning methods. Code and models will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SE(3)-Equivariant Attention Networks for Shape Reconstruction in Function Space. (arXiv:2204.02394v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02394">
<div class="article-summary-box-inner">
<span><p>We propose the first SE(3)-equivariant coordinate-based network for learning
occupancy fields from point clouds. In contrast to previous shape
reconstruction methods that align the input to a regular grid, we operate
directly on the irregular, unoriented point cloud. We leverage attention
mechanisms in order to preserve the set structure (permutation equivariance and
variable length) of the input. At the same time, attention layers enable local
shape modelling, a crucial property for scalability to large scenes. In
contrast to architectures that create a global signature for the shape, we
operate on local tokens. Given an unoriented, sparse, noisy point cloud as
input, we produce equivariant features for each point. These serve as keys and
values for the subsequent equivariant cross-attention blocks that parametrize
the occupancy field. By querying an arbitrary point in space, we predict its
occupancy score. We show that our method outperforms previous SO(3)-equivariant
methods, as well as non-equivariant methods trained on SO(3)-augmented
datasets. More importantly, local modelling together with SE(3)-equivariance
create an ideal setting for SE(3) scene reconstruction. We show that by
training only on single objects and without any pre-segmentation, we can
reconstruct a novel scene with single-object performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SALISA: Saliency-based Input Sampling for Efficient Video Object Detection. (arXiv:2204.02397v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02397">
<div class="article-summary-box-inner">
<span><p>High-resolution images are widely adopted for high-performance object
detection in videos. However, processing high-resolution inputs comes with high
computation costs, and naive down-sampling of the input to reduce the
computation costs quickly degrades the detection performance. In this paper, we
propose SALISA, a novel non-uniform SALiency-based Input SAmpling technique for
video object detection that allows for heavy down-sampling of unimportant
background regions while preserving the fine-grained details of a
high-resolution image. The resulting image is spatially smaller, leading to
reduced computational costs while enabling a performance comparable to a
high-resolution input. To achieve this, we propose a differentiable resampling
module based on a thin plate spline spatial transformer network (TPS-STN). This
module is regularized by a novel loss to provide an explicit supervision signal
to learn to "magnify" salient regions. We report state-of-the-art results in
the low compute regime on the ImageNet-VID and UA-DETRAC video object detection
datasets. We demonstrate that on both datasets, the mAP of an EfficientDet-D1
(EfficientDet-D2) gets on par with EfficientDet-D2 (EfficientDet-D3) at a much
lower computational cost. We also show that SALISA significantly improves the
detection of small objects. In particular, SALISA with an EfficientDet-D1
detector improves the detection of small objects by $77\%$, and remarkably also
outperforms EfficientDetD3 baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VerSe: A Vertebrae Labelling and Segmentation Benchmark for Multi-detector CT Images. (arXiv:2001.09193v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2001.09193">
<div class="article-summary-box-inner">
<span><p>Vertebral labelling and segmentation are two fundamental tasks in an
automated spine processing pipeline. Reliable and accurate processing of spine
images is expected to benefit clinical decision-support systems for diagnosis,
surgery planning, and population-based analysis on spine and bone health.
However, designing automated algorithms for spine processing is challenging
predominantly due to considerable variations in anatomy and acquisition
protocols and due to a severe shortage of publicly available data. Addressing
these limitations, the Large Scale Vertebrae Segmentation Challenge (VerSe) was
organised in conjunction with the International Conference on Medical Image
Computing and Computer Assisted Intervention (MICCAI) in 2019 and 2020, with a
call for algorithms towards labelling and segmentation of vertebrae. Two
datasets containing a total of 374 multi-detector CT scans from 355 patients
were prepared and 4505 vertebrae have individually been annotated at
voxel-level by a human-machine hybrid algorithm (https://osf.io/nqjyw/,
https://osf.io/t98fz/). A total of 25 algorithms were benchmarked on these
datasets. In this work, we present the the results of this evaluation and
further investigate the performance-variation at vertebra-level, scan-level,
and at different fields-of-view. We also evaluate the generalisability of the
approaches to an implicit domain shift in data by evaluating the top performing
algorithms of one challenge iteration on data from the other iteration. The
principal takeaway from VerSe: the performance of an algorithm in labelling and
segmenting a spine scan hinges on its ability to correctly identify vertebrae
in cases of rare anatomical variations. The content and code concerning VerSe
can be accessed at: https://github.com/anjany/verse.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Generalization via Optimal Transport with Metric Similarity Learning. (arXiv:2007.10573v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.10573">
<div class="article-summary-box-inner">
<span><p>Generalizing knowledge to unseen domains, where data and labels are
unavailable, is crucial for machine learning models. We tackle the domain
generalization problem to learn from multiple source domains and generalize to
a target domain with unknown statistics. The crucial idea is to extract the
underlying invariant features across all the domains. Previous domain
generalization approaches mainly focused on learning invariant features and
stacking the learned features from each source domain to generalize to a new
target domain while ignoring the label information, which will lead to
indistinguishable features with an ambiguous classification boundary. For this,
one possible solution is to constrain the label-similarity when extracting the
invariant features and to take advantage of the label similarities for
class-specific cohesion and separation of features across domains. Therefore we
adopt optimal transport with Wasserstein distance, which could constrain the
class label similarity, for adversarial training and also further deploy a
metric learning objective to leverage the label information for achieving
distinguishable classification boundary. Empirical results show that our
proposed method could outperform most of the baselines. Furthermore, ablation
studies also demonstrate the effectiveness of each component of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Classification and Segmentation of Pulmonary Lesions in CT Images Using a Combined VGG-XGBoost Method, and an Integrated Fuzzy Clustering-Level Set Technique. (arXiv:2101.00948v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00948">
<div class="article-summary-box-inner">
<span><p>Given that lung cancer is one of the deadliest illnesses, early
identification and diagnosis are critical to preserving a patient's life.
However, lung illness diagnosis is time-intensive and requires the expertise of
a pulmonary disease specialist, subject to a significant rate of inaccuracy.
Our objective is to design a system capable of accurately detecting and
classifying lung lesions and segmenting them in CT-scan images. The suggested
technique extracts features automatically from the CT-scan image and then
classifies them using Ensemble Gradient Boosting methods. Finally, if a lesion
is detected in the CT-scan image, it is segmented using a hybrid approach based
on Fuzzy Clustering and Level Set. To train and test our models we gathered a
dataset that included CT images of patients residing in Mashhad, Iran. Finally,
the results indicate 96% accuracy within this dataset. This approach may assist
clinicians in diagnosing lung abnormalities and avoiding potential errors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Common Limitations of Image Processing Metrics: A Picture Story. (arXiv:2104.05642v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05642">
<div class="article-summary-box-inner">
<span><p>While the importance of automatic image analysis is continuously increasing,
recent meta-research revealed major flaws with respect to algorithm validation.
Performance metrics are particularly key for meaningful, objective, and
transparent performance assessment and validation of the used automatic
algorithms, but relatively little attention has been given to the practical
pitfalls when using specific metrics for a given image analysis task. These are
typically related to (1) the disregard of inherent metric properties, such as
the behaviour in the presence of class imbalance or small target structures,
(2) the disregard of inherent data set properties, such as the non-independence
of the test cases, and (3) the disregard of the actual biomedical domain
interest that the metrics should reflect. This living dynamically document has
the purpose to illustrate important limitations of performance metrics commonly
applied in the field of image analysis. In this context, it focuses on
biomedical image analysis problems that can be phrased as image-level
classification, semantic segmentation, instance segmentation, or object
detection task. The current version is based on a Delphi process on metrics
conducted by an international consortium of image analysis experts from more
than 60 institutions worldwide.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhanced Isotropy Maximization Loss: Seamless and High-Performance Out-of-Distribution Detection Simply Replacing the SoftMax Loss. (arXiv:2105.14399v10 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14399">
<div class="article-summary-box-inner">
<span><p>Current out-of-distribution detection approaches usually present special
requirements (e.g., collecting outlier data and hyperparameter validation) and
produce side effects (e.g., classification accuracy drop and slow/inefficient
inferences). Recently, entropic out-of-distribution detection has been proposed
as a seamless approach (i.e., a solution that avoids all previously mentioned
drawbacks). The entropic out-of-distribution detection solution uses the IsoMax
loss for training and the entropic score for out-of-distribution detection. The
IsoMax loss works as a drop-in replacement of the SoftMax loss (i.e., the
combination of the output linear layer, the SoftMax activation, and the
cross-entropy loss) because swapping the SoftMax loss with the IsoMax loss
requires no changes in the model's architecture or training
procedures/hyperparameters. In this paper, we perform what we call an
isometrization of the distances used in the IsoMax loss. Additionally, we
propose replacing the entropic score with the minimum distance score.
Experiments showed that these modifications significantly increase
out-of-distribution detection performance while keeping the solution seamless.
Besides being competitive with or outperforming all major current approaches,
the proposed solution avoids all their current limitations, in addition to
being much easier to use because only a simple loss replacement for training
the neural network is required. The code to replace the SoftMax loss with the
IsoMax+ loss and reproduce the results is available at
https://github.com/dlmacedo/entropic-out-of-distribution-detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Styleformer: Transformer based Generative Adversarial Networks with Style Vector. (arXiv:2106.07023v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07023">
<div class="article-summary-box-inner">
<span><p>We propose Styleformer, which is a style-based generator for GAN
architecture, but a convolution-free transformer-based generator. In our paper,
we explain how a transformer can generate high-quality images, overcoming the
disadvantage that convolution operations are difficult to capture global
features in an image. Furthermore, we change the demodulation of StyleGAN2 and
modify the existing transformer structure (e.g., residual connection, layer
normalization) to create a strong style-based generator with a convolution-free
structure. We also make Styleformer lighter by applying Linformer, enabling
Styleformer to generate higher resolution images and result in improvements in
terms of speed and memory. We experiment with the low-resolution image dataset
such as CIFAR-10, as well as the high-resolution image dataset like
LSUN-church. Styleformer records FID 2.82 and IS 9.94 on CIFAR-10, a benchmark
dataset, which is comparable performance to the current state-of-the-art and
outperforms all GAN-based generative models, including StyleGAN2-ADA with fewer
parameters on the unconditional setting. We also both achieve new
state-of-the-art with FID 15.17, IS 11.01, and FID 3.66, respectively on STL-10
and CelebA. We release our code at
https://github.com/Jeeseung-Park/Styleformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training of deep cross-modality conversion models with a small dataset, and their application in megavoltage CT to kilovoltage CT conversion. (arXiv:2107.05238v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.05238">
<div class="article-summary-box-inner">
<span><p>In recent years, deep-learning-based image processing has emerged as a
valuable tool for medical imaging owing to its high performance. However, the
quality of deep-learning-based methods heavily relies on the amount of training
data; the high cost of acquiring a large dataset is a limitation to their
utilization in medical fields. Herein, based on deep learning, we developed a
computed tomography (CT) modality conversion method requiring only a few
unsupervised images. The proposed method is based on CycleGAN with several
extensions tailored for CT images, which aims at preserving the structure in
the processed images and reducing the amount of training data. This method was
applied to realize the conversion of megavoltage computed tomography (MVCT) to
kilovoltage computed tomography (kVCT) images. Training was conducted using
several datasets acquired from patients with head and neck cancer. The size of
the datasets ranged from 16 slices (two patients) to 2745 slices (137 patients)
for MVCT and 2824 slices (98 patients) for kVCT. The required size of the
training data was found to be as small as a few hundred slices. By statistical
and visual evaluations, the quality improvement and structure preservation of
the MVCT images converted by the proposed model were investigated. As a
clinical benefit, it was observed by medical doctors that the converted images
enhanced the precision of contouring. We developed an MVCT to kVCT conversion
model based on deep learning, which can be trained using only a few hundred
unpaired images. The stability of the model against changes in data size was
demonstrated. This study promotes the reliable use of deep learning in clinical
medicine by partially answering commonly asked questions, such as "Is our data
sufficient?" and "How much data should we acquire?"
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Image-based Illumination Harmonization. (arXiv:2108.00150v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.00150">
<div class="article-summary-box-inner">
<span><p>Integrating a foreground object into a background scene with illumination
harmonization is an important but challenging task in computer vision and
augmented reality community. Existing methods mainly focus on foreground and
background appearance consistency or the foreground object shadow generation,
which rarely consider global appearance and illumination harmonization. In this
paper, we formulate seamless illumination harmonization as an illumination
exchange and aggregation problem. Specifically, we firstly apply a
physically-based rendering method to construct a large-scale, high-quality
dataset (named IH) for our task, which contains various types of foreground
objects and background scenes with different lighting conditions. Then, we
propose a deep image-based illumination harmonization GAN framework named
DIH-GAN, which makes full use of a multi-scale attention mechanism and
illumination exchange strategy to directly infer mapping relationship between
the inserted foreground object and the corresponding background scene.
Meanwhile, we also use adversarial learning strategy to further refine the
illumination harmonization result. Our method can not only achieve harmonious
appearance and illumination for the foreground object but also can generate
compelling shadow cast by the foreground object. Comprehensive experiments on
both our IH dataset and real-world images show that our proposed DIH-GAN
provides a practical and effective solution for image-based object illumination
harmonization editing, and validate the superiority of our method against
state-of-the-art methods. Our IH dataset is available at
https://github.com/zhongyunbao/Dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Blindly Assess Quality of In-the-Wild Videos via Quality-aware Pre-training and Motion Perception. (arXiv:2108.08505v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08505">
<div class="article-summary-box-inner">
<span><p>Perceptual quality assessment of the videos acquired in the wilds is of vital
importance for quality assurance of video services. The inaccessibility of
reference videos with pristine quality and the complexity of authentic
distortions pose great challenges for this kind of blind video quality
assessment (BVQA) task. Although model-based transfer learning is an effective
and efficient paradigm for the BVQA task, it remains to be a challenge to
explore what and how to bridge the domain shifts for better video
representation. In this work, we propose to transfer knowledge from image
quality assessment (IQA) databases with authentic distortions and large-scale
action recognition with rich motion patterns. We rely on both groups of data to
learn the feature extractor. We train the proposed model on the target VQA
databases using a mixed list-wise ranking loss function. Extensive experiments
on six databases demonstrate that our method performs very competitively under
both individual database and mixed database training settings. We also verify
the rationality of each component of the proposed method and explore a simple
manner for further improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Light Field-Based Underwater 3D Reconstruction Via Angular Resampling. (arXiv:2109.02116v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02116">
<div class="article-summary-box-inner">
<span><p>Recovering 3D geometry of underwater scenes is challenging because of
non-linear refraction of light at the water-air interface caused by the camera
housing. We present a light field-based approach that leverages properties of
angular samples for high-quality underwater 3D reconstruction from a single
viewpoint. Specifically, we resample the light field image to angular patches.
As underwater scenes exhibit weak view-dependent specularity, an angular patch
tends to have uniform intensity when sampled at the correct depth. We thus
impose this angular uniformity as a constraint for depth estimation. For
efficient angular resampling, we design a fast approximation algorithm based on
multivariate polynomial regression to approximate nonlinear refraction paths.
We further develop a light field calibration algorithm that estimates the
water-air interface geometry along with the camera parameters. Comprehensive
experiments on synthetic and real data show our method produces
state-of-the-art reconstruction on static and dynamic underwater scenes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatio-Temporal Recurrent Networks for Event-Based Optical Flow Estimation. (arXiv:2109.04871v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04871">
<div class="article-summary-box-inner">
<span><p>Event camera has offered promising alternative for visual perception,
especially in high speed and high dynamic range scenes. Recently, many deep
learning methods have shown great success in providing promising solutions to
many event-based problems, such as optical flow estimation. However, existing
deep learning methods did not address the importance of temporal information
well from the perspective of architecture design and cannot effectively extract
spatio-temporal features. Another line of research that utilizes Spiking Neural
Network suffers from training issues for deeper architecture.To address these
points, a novel input representation is proposed that captures the events'
temporal distribution for signal enhancement. Moreover, we introduce a
spatio-temporal recurrent encoding-decoding neural network architecture for
event-based optical flow estimation, which utilizes Convolutional Gated
Recurrent Units to extract feature maps from a series of event images. Besides,
our architecture allows some traditional frame-based core modules, such as
correlation layer and iterative residual refine scheme, to be incorporated. The
network is end-to-end trained with self-supervised learning on the
Multi-Vehicle Stereo Event Camera dataset. We have shown that it outperforms
all the existing state-of-the-art methods by a large margin. The code link is
https://github.com/ruizhao26/STE-FlowNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incremental Abstraction in Distributed Probabilistic SLAM Graphs. (arXiv:2109.06241v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06241">
<div class="article-summary-box-inner">
<span><p>Scene graphs represent the key components of a scene in a compact and
semantically rich way, but are difficult to build during incremental SLAM
operation because of the challenges of robustly identifying abstract scene
elements and optimising continually changing, complex graphs. We present a
distributed, graph-based SLAM framework for incrementally building scene graphs
based on two novel components. First, we propose an incremental abstraction
framework in which a neural network proposes abstract scene elements that are
incorporated into the factor graph of a feature-based monocular SLAM system.
Scene elements are confirmed or rejected through optimisation and incrementally
replace the points yielding a more dense, semantic and compact representation.
Second, enabled by our novel routing procedure, we use Gaussian Belief
Propagation (GBP) for distributed inference on a graph processor. The time per
iteration of GBP is structure-agnostic and we demonstrate the speed advantages
over direct methods for inference of heterogeneous factor graphs. We run our
system on real indoor datasets using planar abstractions and recover the major
planes with significant compression.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distract Your Attention: Multi-head Cross Attention Network for Facial Expression Recognition. (arXiv:2109.07270v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07270">
<div class="article-summary-box-inner">
<span><p>We present a novel facial expression recognition network, called Distract
your Attention Network (DAN). Our method is based on two key observations.
Firstly, multiple classes share inherently similar underlying facial
appearance, and their differences could be subtle. Secondly, facial expressions
exhibit themselves through multiple facial regions simultaneously, and the
recognition requires a holistic approach by encoding high-order interactions
among local features. To address these issues, we propose our DAN with three
key components: Feature Clustering Network (FCN), Multi-head cross Attention
Network (MAN), and Attention Fusion Network (AFN). The FCN extracts robust
features by adopting a large-margin learning objective to maximize class
separability. In addition, the MAN instantiates a number of attention heads to
simultaneously attend to multiple facial areas and build attention maps on
these regions. Further, the AFN distracts these attentions to multiple
locations before fusing the attention maps to a comprehensive one. Extensive
experiments on three public datasets (including AffectNet, RAF-DB, and SFEW
2.0) verified that the proposed method consistently achieves state-of-the-art
facial expression recognition performance. Code will be made available at
https://github.com/yaoing/DAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Markerless Suture Needle 6D Pose Tracking with Robust Uncertainty Estimation for Autonomous Minimally Invasive Robotic Surgery. (arXiv:2109.12722v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12722">
<div class="article-summary-box-inner">
<span><p>Suture needle localization is necessary for autonomous suturing. Previous
approaches in autonomous suturing often relied on fiducial markers rather than
markerless detection schemes for localizing a suture needle due to the
inconsistency of markerless detections. However, fiducial markers are not
practical for real-world applications and can often be occluded from
environmental factors in surgery (e.g., blood). Therefore in this work, we
present a robust tracking approach for estimating the 6D pose of a suture
needle when using inconsistent detections. We define observation models based
on suture needles' geometry that captures the uncertainty of the detections and
fuse them temporally in a probabilistic fashion. In our experiments, we compare
different permutations of the observation models in the suture needle
localization task to show their effectiveness. Our proposed method outperforms
previous approaches in localizing a suture needle. We also demonstrate the
proposed tracking method in an autonomous suture needle regrasping task and ex
vivo environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation. (arXiv:2110.02711v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02711">
<div class="article-summary-box-inner">
<span><p>Recently, GAN inversion methods combined with Contrastive Language-Image
Pretraining (CLIP) enables zero-shot image manipulation guided by text prompts.
However, their applications to diverse real images are still difficult due to
the limited GAN inversion capability. Specifically, these approaches often have
difficulties in reconstructing images with novel poses, views, and highly
variable contents compared to the training data, altering object identity, or
producing unwanted image artifacts. To mitigate these problems and enable
faithful manipulation of real images, we propose a novel method, dubbed
DiffusionCLIP, that performs text-driven image manipulation using diffusion
models. Based on full inversion capability and high-quality image generation
power of recent diffusion models, our method performs zero-shot image
manipulation successfully even between unseen domains and takes another step
towards general application by manipulating images from a widely varying
ImageNet dataset. Furthermore, we propose a novel noise combination method that
allows straightforward multi-attribute manipulation. Extensive experiments and
human evaluation confirmed robust and superior manipulation performance of our
methods compared to the existing baselines. Code is available at
https://github.com/gwang-kim/DiffusionCLIP.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transform2Act: Learning a Transform-and-Control Policy for Efficient Agent Design. (arXiv:2110.03659v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03659">
<div class="article-summary-box-inner">
<span><p>An agent's functionality is largely determined by its design, i.e., skeletal
structure and joint attributes (e.g., length, size, strength). However, finding
the optimal agent design for a given function is extremely challenging since
the problem is inherently combinatorial and the design space is prohibitively
large. Additionally, it can be costly to evaluate each candidate design which
requires solving for its optimal controller. To tackle these problems, our key
idea is to incorporate the design procedure of an agent into its
decision-making process. Specifically, we learn a conditional policy that, in
an episode, first applies a sequence of transform actions to modify an agent's
skeletal structure and joint attributes, and then applies control actions under
the new design. To handle a variable number of joints across designs, we use a
graph-based policy where each graph node represents a joint and uses message
passing with its neighbors to output joint-specific actions. Using policy
gradient methods, our approach enables joint optimization of agent design and
control as well as experience sharing across different designs, which improves
sample efficiency substantially. Experiments show that our approach,
Transform2Act, outperforms prior methods significantly in terms of convergence
speed and final performance. Notably, Transform2Act can automatically discover
plausible designs similar to giraffes, squids, and spiders. Code and videos are
available at https://sites.google.com/view/transform2act.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Representational Continuity for Unsupervised Continual Learning. (arXiv:2110.06976v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06976">
<div class="article-summary-box-inner">
<span><p>Continual learning (CL) aims to learn a sequence of tasks without forgetting
the previously acquired knowledge. However, recent CL advances are restricted
to supervised continual learning (SCL) scenarios. Consequently, they are not
scalable to real-world applications where the data distribution is often biased
and unannotated. In this work, we focus on unsupervised continual learning
(UCL), where we learn the feature representations on an unlabelled sequence of
tasks and show that reliance on annotated data is not necessary for continual
learning. We conduct a systematic study analyzing the learned feature
representations and show that unsupervised visual representations are
surprisingly more robust to catastrophic forgetting, consistently achieve
better performance, and generalize better to out-of-distribution tasks than
SCL. Furthermore, we find that UCL achieves a smoother loss landscape through
qualitative analysis of the learned representations and learns meaningful
feature representations. Additionally, we propose Lifelong Unsupervised Mixup
(LUMP), a simple yet effective technique that interpolates between the current
task and previous tasks' instances to alleviate catastrophic forgetting for
unsupervised representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Attention-guided Graph Clustering with Dual Self-supervision. (arXiv:2111.05548v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.05548">
<div class="article-summary-box-inner">
<span><p>Existing deep embedding clustering works only consider the deepest layer to
learn a feature embedding and thus fail to well utilize the available
discriminative information from cluster assignments, resulting performance
limitation. To this end, we propose a novel method, namely deep
attention-guided graph clustering with dual self-supervision (DAGC).
Specifically, DAGC first utilizes a heterogeneity-wise fusion module to
adaptively integrate the features of an auto-encoder and a graph convolutional
network in each layer and then uses a scale-wise fusion module to dynamically
concatenate the multi-scale features in different layers. Such modules are
capable of learning a discriminative feature embedding via an attention-based
mechanism. In addition, we design a distribution-wise fusion module that
leverages cluster assignments to acquire clustering results directly. To better
explore the discriminative information from the cluster assignments, we develop
a dual self-supervision solution consisting of a soft self-supervision strategy
with a triplet Kullback-Leibler divergence loss and a hard self-supervision
strategy with a pseudo supervision loss. Extensive experiments validate that
our method consistently outperforms state-of-the-art methods on six benchmark
datasets. Especially, our method improves the ARI by more than 18.14% over the
best baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multi-Stage model based on YOLOv3 for defect detection in PV panels based on IR and Visible Imaging by Unmanned Aerial Vehicle. (arXiv:2111.11709v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11709">
<div class="article-summary-box-inner">
<span><p>As solar capacity installed worldwide continues to grow, there is an
increasing awareness that advanced inspection systems are becoming of utmost
importance to schedule smart interventions and minimize downtime likelihood. In
this work we propose a novel automatic multi-stage model to detect panel
defects on aerial images captured by unmanned aerial vehicle by using the
YOLOv3 network and Computer Vision techniques. The model combines detections of
panels and defects to refine its accuracy and exhibits an average inference
time per image of 0.98 s. The main novelties are represented by its versatility
to process either thermographic or visible images and detect a large variety of
defects, to prescript recommended actions to O&amp;M crew to give a more efficient
data-driven maintenance strategy and its portability to both rooftop and
ground-mounted PV systems and different panel types. The proposed model has
been validated on two big PV plants in the south of Italy with an outstanding
AP@0.5 exceeding 98% for panel detection, a remarkable AP@0.4 (AP@0.5) of
roughly 88.3% (66.9%) for hotspots by means of infrared thermography and a
mAP@0.5 of almost 70% in the visible spectrum for detection of anomalies
including panel shading induced by soiling and bird dropping, delamination,
presence of puddles and raised rooftop panels. The model predicts also the
severity of hotspot areas based on the estimated temperature gradients, as well
as it computes the soiling coverage based on visual images. Finally an analysis
of the influence of the different YOLOv3's output scales on the detection is
discussed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Less is More: Generating Grounded Navigation Instructions from Landmarks. (arXiv:2111.12872v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12872">
<div class="article-summary-box-inner">
<span><p>We study the automatic generation of navigation instructions from 360-degree
images captured on indoor routes. Existing generators suffer from poor visual
grounding, causing them to rely on language priors and hallucinate objects. Our
MARKY-MT5 system addresses this by focusing on visual landmarks; it comprises a
first stage landmark detector and a second stage generator -- a multimodal,
multilingual, multitask encoder-decoder. To train it, we bootstrap grounded
landmark annotations on top of the Room-across-Room (RxR) dataset. Using text
parsers, weak supervision from RxR's pose traces, and a multilingual image-text
encoder trained on 1.8b images, we identify 971k English, Hindi and Telugu
landmark descriptions and ground them to specific regions in panoramas. On
Room-to-Room, human wayfinders obtain success rates (SR) of 71% following
MARKY-MT5's instructions, just shy of their 75% SR following human instructions
-- and well above SRs with other generators. Evaluations on RxR's longer,
diverse paths obtain 61-64% SRs on three languages. Generating such
high-quality navigation instructions in novel environments is a step towards
conversational navigation tools and could facilitate larger-scale training of
instruction-following agents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Robust and Adaptive Motion Forecasting: A Causal Representation Perspective. (arXiv:2111.14820v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14820">
<div class="article-summary-box-inner">
<span><p>Learning behavioral patterns from observational data has been a de-facto
approach to motion forecasting. Yet, the current paradigm suffers from two
shortcomings: brittle under distribution shifts and inefficient for knowledge
transfer. In this work, we propose to address these challenges from a causal
representation perspective. We first introduce a causal formalism of motion
forecasting, which casts the problem as a dynamic process with three groups of
latent variables, namely invariant variables, style confounders, and spurious
features. We then introduce a learning framework that treats each group
separately: (i) unlike the common practice mixing datasets collected from
different locations, we exploit their subtle distinctions by means of an
invariance loss encouraging the model to suppress spurious correlations; (ii)
we devise a modular architecture that factorizes the representations of
invariant mechanisms and style confounders to approximate a sparse causal
graph; (iii) we introduce a style contrastive loss that not only enforces the
structure of style representations but also serves as a self-supervisory signal
for test-time refinement on the fly. Experiments on synthetic and real datasets
show that our proposed method improves the robustness and reusability of
learned motion representations, significantly outperforming prior
state-of-the-art motion forecasting models for out-of-distribution
generalization and low-shot transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Norm Must Go On: Dynamic Unsupervised Domain Adaptation by Normalization. (arXiv:2112.00463v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00463">
<div class="article-summary-box-inner">
<span><p>Domain adaptation is crucial to adapt a learned model to new scenarios, such
as domain shifts or changing data distributions. Current approaches usually
require a large amount of labeled or unlabeled data from the shifted domain.
This can be a hurdle in fields which require continuous dynamic adaptation or
suffer from scarcity of data, e.g. autonomous driving in challenging weather
conditions. To address this problem of continuous adaptation to distribution
shifts, we propose Dynamic Unsupervised Adaptation (DUA). By continuously
adapting the statistics of the batch normalization layers we modify the feature
representations of the model. We show that by sequentially adapting a model
with only a fraction of unlabeled data, a strong performance gain can be
achieved. With even less than 1% of unlabeled data from the target domain, DUA
already achieves competitive results to strong baselines. In addition, the
computational overhead is minimal in contrast to previous approaches. Our
approach is simple, yet effective and can be applied to any architecture which
uses batch normalization as one of its components. We show the utility of DUA
by evaluating it on a variety of domain adaptation datasets and tasks including
object recognition, digit recognition and object detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GCA-Net : Utilizing Gated Context Attention for Improving Image Forgery Localization and Detection. (arXiv:2112.04298v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04298">
<div class="article-summary-box-inner">
<span><p>Forensic analysis of manipulated pixels requires the identification of
various hidden and subtle features from images. Conventional image recognition
models generally fail at this task because they are biased and more attentive
toward the dominant local and spatial features. In this paper, we propose a
novel Gated Context Attention Network (GCA-Net) that utilizes non-local
attention in conjunction with a gating mechanism in order to capture the finer
image discrepancies and better identify forged regions. The proposed framework
uses high dimensional embeddings to filter and aggregate the relevant context
from coarse feature maps at various stages of the decoding process. This
improves the network's understanding of global differences and reduces
false-positive localizations. Our evaluation on standard image forensic
benchmarks shows that GCA-Net can both compete against and improve over
state-of-the-art networks by an average of 4.7% AUC. Additional ablation
studies also demonstrate the method's robustness against attributions and
resilience to false-positive predictions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatio-temporal Relation Modeling for Few-shot Action Recognition. (arXiv:2112.05132v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05132">
<div class="article-summary-box-inner">
<span><p>We propose a novel few-shot action recognition framework, STRM, which
enhances class-specific feature discriminability while simultaneously learning
higher-order temporal representations. The focus of our approach is a novel
spatio-temporal enrichment module that aggregates spatial and temporal contexts
with dedicated local patch-level and global frame-level feature enrichment
sub-modules. Local patch-level enrichment captures the appearance-based
characteristics of actions. On the other hand, global frame-level enrichment
explicitly encodes the broad temporal context, thereby capturing the relevant
object features over time. The resulting spatio-temporally enriched
representations are then utilized to learn the relational matching between
query and support action sub-sequences. We further introduce a query-class
similarity classifier on the patch-level enriched features to enhance
class-specific feature discriminability by reinforcing the feature learning at
different stages in the proposed framework. Experiments are performed on four
few-shot action recognition benchmarks: Kinetics, SSv2, HMDB51 and UCF101. Our
extensive ablation study reveals the benefits of the proposed contributions.
Furthermore, our approach sets a new state-of-the-art on all four benchmarks.
On the challenging SSv2 benchmark, our approach achieves an absolute gain of
$3.5\%$ in classification accuracy, as compared to the best existing method in
the literature. Our code and models are available at
https://github.com/Anirudh257/strm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GAN-Supervised Dense Visual Alignment. (arXiv:2112.05143v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05143">
<div class="article-summary-box-inner">
<span><p>We propose GAN-Supervised Learning, a framework for learning discriminative
models and their GAN-generated training data jointly end-to-end. We apply our
framework to the dense visual alignment problem. Inspired by the classic
Congealing method, our GANgealing algorithm trains a Spatial Transformer to map
random samples from a GAN trained on unaligned data to a common,
jointly-learned target mode. We show results on eight datasets, all of which
demonstrate our method successfully aligns complex data and discovers dense
correspondences. GANgealing significantly outperforms past self-supervised
correspondence algorithms and performs on-par with (and sometimes exceeds)
state-of-the-art supervised correspondence algorithms on several datasets --
without making use of any correspondence supervision or data augmentation and
despite being trained exclusively on GAN-generated data. For precise
correspondence, we improve upon state-of-the-art supervised methods by as much
as $3\times$. We show applications of our method for augmented reality, image
editing and automated pre-processing of image datasets for downstream GAN
training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The CLEAR Benchmark: Continual LEArning on Real-World Imagery. (arXiv:2201.06289v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06289">
<div class="article-summary-box-inner">
<span><p>Continual learning (CL) is widely regarded as crucial challenge for lifelong
AI. However, existing CL benchmarks, e.g. Permuted-MNIST and Split-CIFAR, make
use of artificial temporal variation and do not align with or generalize to the
real-world. In this paper, we introduce CLEAR, the first continual image
classification benchmark dataset with a natural temporal evolution of visual
concepts in the real world that spans a decade (2004-2014). We build CLEAR from
existing large-scale image collections (YFCC100M) through a novel and scalable
low-cost approach to visio-linguistic dataset curation. Our pipeline makes use
of pretrained vision-language models (e.g. CLIP) to interactively build labeled
datasets, which are further validated with crowd-sourcing to remove errors and
even inappropriate images (hidden in original YFCC100M). The major strength of
CLEAR over prior CL benchmarks is the smooth temporal evolution of visual
concepts with real-world imagery, including both high-quality labeled data
along with abundant unlabeled samples per time period for continual
semi-supervised learning. We find that a simple unsupervised pre-training step
can already boost state-of-the-art CL algorithms that only utilize
fully-supervised data. Our analysis also reveals that mainstream CL evaluation
protocols that train and test on iid data artificially inflate performance of
CL system. To address this, we propose novel "streaming" protocols for CL that
always test on the (near) future. Interestingly, streaming protocols (a) can
simplify dataset curation since today's testset can be repurposed for
tomorrow's trainset and (b) can produce more generalizable models with more
accurate estimates of performance since all labeled data from each time-period
is used for both training and testing (unlike classic iid train-test splits).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">It's All in the Head: Representation Knowledge Distillation through Classifier Sharing. (arXiv:2201.06945v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06945">
<div class="article-summary-box-inner">
<span><p>Representation knowledge distillation aims at transferring rich information
from one model to another. Common approaches for representation distillation
mainly focus on the direct minimization of distance metrics between the models'
embedding vectors. Such direct methods may be limited in transferring
high-order dependencies embedded in the representation vectors, or in handling
the capacity gap between the teacher and student models. Moreover, in standard
knowledge distillation, the teacher is trained without awareness of the
student's characteristics and capacity. In this paper, we explore two
mechanisms for enhancing representation distillation using classifier sharing
between the teacher and student. We first investigate a simple scheme where the
teacher's classifier is connected to the student backbone, acting as an
additional classification head. Then, we propose a student-aware mechanism that
asks to tailor the teacher model to a student with limited capacity by training
the teacher with a temporary student's head. We analyze and compare these two
mechanisms and show their effectiveness on various datasets and tasks,
including image classification, fine-grained classification, and face
verification. In particular, we achieve state-of-the-art results for face
verification on the IJB-C dataset for a MobileFaceNet model:
TAR@(FAR=1e-5)=93.7\%. Code is available at
https://github.com/Alibaba-MIIL/HeadSharingKD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reliable Detection of Doppelg\"angers based on Deep Face Representations. (arXiv:2201.08831v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08831">
<div class="article-summary-box-inner">
<span><p>Doppelg\"angers (or lookalikes) usually yield an increased probability of
false matches in a facial recognition system, as opposed to random face image
pairs selected for non-mated comparison trials. In this work, we assess the
impact of doppelg\"angers on the HDA Doppelg\"anger and Disguised Faces in The
Wild databases using a state-of-the-art face recognition system. It is found
that doppelg\"anger image pairs yield very high similarity scores resulting in
a significant increase of false match rates. Further, we propose a
doppelg\"anger detection method which distinguishes doppelg\"angers from mated
comparison trials by analysing differences in deep representations obtained
from face image pairs. The proposed detection system employs a machine
learning-based classifier, which is trained with generated doppelg\"anger image
pairs utilising face morphing techniques. Experimental evaluations conducted on
the HDA Doppelg\"anger and Look-Alike Face databases reveal a detection equal
error rate of approximately 2.7% for the task of separating mated
authentication attempts from doppelg\"angers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SelfRecon: Self Reconstruction Your Digital Avatar from Monocular Video. (arXiv:2201.12792v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12792">
<div class="article-summary-box-inner">
<span><p>We propose SelfRecon, a clothed human body reconstruction method that
combines implicit and explicit representations to recover space-time coherent
geometries from a monocular self-rotating human video. Explicit methods require
a predefined template mesh for a given sequence, while the template is hard to
acquire for a specific subject. Meanwhile, the fixed topology limits the
reconstruction accuracy and clothing types. Implicit representation supports
arbitrary topology and can represent high-fidelity geometry shapes due to its
continuous nature. However, it is difficult to integrate multi-frame
information to produce a consistent registration sequence for downstream
applications. We propose to combine the advantages of both representations. We
utilize differential mask loss of the explicit mesh to obtain the coherent
overall shape, while the details on the implicit surface are refined with the
differentiable neural rendering. Meanwhile, the explicit mesh is updated
periodically to adjust its topology changes, and a consistency loss is designed
to match both representations. Compared with existing methods, SelfRecon can
produce high-fidelity surfaces for arbitrary clothed humans with
self-supervised optimization. Extensive experimental results demonstrate its
effectiveness on real captured monocular videos. The source code is available
at https://github.com/jby1993/SelfReconCode.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MedNeRF: Medical Neural Radiance Fields for Reconstructing 3D-aware CT-Projections from a Single X-ray. (arXiv:2202.01020v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01020">
<div class="article-summary-box-inner">
<span><p>Computed tomography (CT) is an effective medical imaging modality, widely
used in the field of clinical medicine for the diagnosis of various
pathologies. Advances in Multidetector CT imaging technology have enabled
additional functionalities, including generation of thin slice multiplanar
cross-sectional body imaging and 3D reconstructions. However, this involves
patients being exposed to a considerable dose of ionising radiation. Excessive
ionising radiation can lead to deterministic and harmful effects on the body.
This paper proposes a Deep Learning model that learns to reconstruct CT
projections from a few or even a single-view X-ray. This is based on a novel
architecture that builds from neural radiance fields, which learns a continuous
representation of CT scans by disentangling the shape and volumetric depth of
surface and internal anatomical structures from 2D images. Our model is trained
on chest and knee datasets, and we demonstrate qualitative and quantitative
high-fidelity renderings and compare our approach to other recent radiance
field-based methods. Our code and link to our datasets is available at
https://github.com/jonathanfrawley/mednerf
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Detection without Model Information. (arXiv:2202.04271v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04271">
<div class="article-summary-box-inner">
<span><p>Prior state-of-the-art adversarial detection works are classifier model
dependent, i.e., they require classifier model outputs and parameters for
training the detector or during adversarial detection. This makes their
detection approach classifier model specific. Furthermore, classifier model
outputs and parameters might not always be accessible. To this end, we propose
a classifier model independent adversarial detection method using a simple
energy function to distinguish between adversarial and natural inputs. We train
a standalone detector independent of the classifier model, with a layer-wise
energy separation (LES) training to increase the separation between natural and
adversarial energies. With this, we perform energy distribution-based
adversarial detection. Our method achieves comparable performance with
state-of-the-art detection works (ROC-AUC &gt; 0.9) across a wide range of
gradient, score and gaussian noise attacks on CIFAR10, CIFAR100 and
TinyImagenet datasets. Furthermore, compared to prior works, our detection
approach is light-weight, requires less amount of training data (40% of the
actual dataset) and is transferable across different datasets. For
reproducibility, we provide layer-wise energy separation training code at
https://github.com/Intelligent-Computing-Lab-Yale/Energy-Separation-Training
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Complementarity of Images and Text for the Expression of Emotions in Social Media. (arXiv:2202.07427v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07427">
<div class="article-summary-box-inner">
<span><p>Authors of posts in social media communicate their emotions and what causes
them with text and images. While there is work on emotion and stimulus
detection for each modality separately, it is yet unknown if the modalities
contain complementary emotion information in social media. We aim at filling
this research gap and contribute a novel, annotated corpus of English
multimodal Reddit posts. On this resource, we develop models to automatically
detect the relation between image and text, an emotion stimulus category and
the emotion class. We evaluate if these tasks require both modalities and find
for the image-text relations, that text alone is sufficient for most categories
(complementary, illustrative, opposing): the information in the text allows to
predict if an image is required for emotion understanding. The emotions of
anger and sadness are best predicted with a multimodal model, while text alone
is sufficient for disgust, joy, and surprise. Stimuli depicted by objects,
animals, food, or a person are best predicted by image-only models, while
multimodal models are most effective on art, events, memes, places, or
screenshots.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MSSNet: Multi-Scale-Stage Network for Single Image Deblurring. (arXiv:2202.09652v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09652">
<div class="article-summary-box-inner">
<span><p>Most of traditional single image deblurring methods before deep learning
adopt a coarse-to-fine scheme that estimates a sharp image at a coarse scale
and progressively refines it at finer scales. While this scheme has also been
adopted to several deep learning-based approaches, recently a number of
single-scale approaches have been introduced showing superior performance to
previous coarse-to-fine approaches both in quality and computation time. In
this paper, we revisit the coarse-to-fine scheme, and analyze defects of
previous coarse-to-fine approaches that degrade their performance. Based on the
analysis, we propose Multi-Scale-Stage Network (MSSNet), a novel deep
learning-based approach to single image deblurring that adopts our remedies to
the defects. Specifically, MSSNet adopts three novel technical components:
stage configuration reflecting blur scales, an inter-scale information
propagation scheme, and a pixel-shuffle-based multi-scale scheme. Our
experiments show that MSSNet achieves the state-of-the-art performance in terms
of quality, network size, and computation time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Protecting Celebrities from DeepFake with Identity Consistency Transformer. (arXiv:2203.01318v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01318">
<div class="article-summary-box-inner">
<span><p>In this work we propose Identity Consistency Transformer, a novel face
forgery detection method that focuses on high-level semantics, specifically
identity information, and detecting a suspect face by finding identity
inconsistency in inner and outer face regions. The Identity Consistency
Transformer incorporates a consistency loss for identity consistency
determination. We show that Identity Consistency Transformer exhibits superior
generalization ability not only across different datasets but also across
various types of image degradation forms found in real-world applications
including deepfake videos. The Identity Consistency Transformer can be easily
enhanced with additional identity information when such information is
available, and for this reason it is especially well-suited for detecting face
forgeries involving celebrities. Code will be released at
\url{https://github.com/LightDXY/ICT_DeepFake}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">S2F2: Self-Supervised High Fidelity Face Reconstruction from Monocular Image. (arXiv:2203.07732v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07732">
<div class="article-summary-box-inner">
<span><p>We present a novel face reconstruction method capable of reconstructing
detailed face geometry, spatially varying face reflectance from a single
monocular image. We build our work upon the recent advances of DNN-based
auto-encoders with differentiable ray tracing image formation, trained in
self-supervised manner. While providing the advantage of learning-based
approaches and real-time reconstruction, the latter methods lacked fidelity. In
this work, we achieve, for the first time, high fidelity face reconstruction
using self-supervised learning only. Our novel coarse-to-fine deep architecture
allows us to solve the challenging problem of decoupling face reflectance from
geometry using a single image, at high computational speed. Compared to
state-of-the-art methods, our method achieves more visually appealing
reconstruction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ImageNet Challenging Classification with the Raspberry Pi: An Incremental Local Stochastic Gradient Descent Algorithm. (arXiv:2203.11853v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11853">
<div class="article-summary-box-inner">
<span><p>With rising powerful, low-cost embedded devices, the edge computing has
become an increasingly popular choice. In this paper, we propose a new
incremental local stochastic gradient descent (SGD) tailored on the Raspberry
Pi to deal with large ImageNet ILSVRC 2010 dataset having 1,261,405 images with
1,000 classes. The local SGD splits the data block into $k$ partitions using
$k$means algorithm and then it learns in the parallel way SGD models in each
data partition to classify the data locally. The incremental local SGD
sequentially loads small data blocks of the training dataset to learn local SGD
models. The numerical test results on Imagenet dataset show that our
incremental local SGD algorithm with the Raspberry Pi 4 is faster and more
accurate than the state-of-the-art linear SVM run on a PC Intel(R) Core i7-4790
CPU, 3.6 GHz, 4 cores.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing Representation Forgetting in Supervised and Unsupervised Continual Learning. (arXiv:2203.13381v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13381">
<div class="article-summary-box-inner">
<span><p>Continual Learning research typically focuses on tackling the phenomenon of
catastrophic forgetting in neural networks. Catastrophic forgetting is
associated with an abrupt loss of knowledge previously learned by a model when
the task, or more broadly the data distribution, being trained on changes. In
supervised learning problems this forgetting, resulting from a change in the
model's representation, is typically measured or observed by evaluating the
decrease in old task performance. However, a model's representation can change
without losing knowledge about prior tasks. In this work we consider the
concept of representation forgetting, observed by using the difference in
performance of an optimal linear classifier before and after a new task is
introduced. Using this tool we revisit a number of standard continual learning
benchmarks and observe that, through this lens, model representations trained
without any explicit control for forgetting often experience small
representation forgetting and can sometimes be comparable to methods which
explicitly control for forgetting, especially in longer task sequences. We also
show that representation forgetting can lead to new insights on the effect of
model capacity and loss function used in continual learning. Based on our
results, we show that a simple yet competitive approach is to learn
representations continually with standard supervised contrastive learning while
constructing prototypes of class samples when queried on old samples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Intelligent Masking: Deep Q-Learning for Context Encoding in Medical Image Analysis. (arXiv:2203.13865v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13865">
<div class="article-summary-box-inner">
<span><p>The need for a large amount of labeled data in the supervised setting has led
recent studies to utilize self-supervised learning to pre-train deep neural
networks using unlabeled data. Many self-supervised training strategies have
been investigated especially for medical datasets to leverage the information
available in the much fewer unlabeled data. One of the fundamental strategies
in image-based self-supervision is context prediction. In this approach, a
model is trained to reconstruct the contents of an arbitrary missing region of
an image based on its surroundings. However, the existing methods adopt a
random and blind masking approach by focusing uniformly on all regions of the
images. This approach results in a lot of unnecessary network updates that
cause the model to forget the rich extracted features. In this work, we develop
a novel self-supervised approach that occludes targeted regions to improve the
pre-training procedure. To this end, we propose a reinforcement learning-based
agent which learns to intelligently mask input images through deep Q-learning.
We show that training the agent against the prediction model can significantly
improve the semantic features extracted for downstream classification tasks. We
perform our experiments on two public datasets for diagnosing breast cancer in
the ultrasound images and detecting lower-grade glioma with MR images. In our
experiments, we show that our novel masking strategy advances the learned
features according to the performance on the classification task in terms of
accuracy, macro F1, and AUROC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sylph: A Hypernetwork Framework for Incremental Few-shot Object Detection. (arXiv:2203.13903v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13903">
<div class="article-summary-box-inner">
<span><p>We study the challenging incremental few-shot object detection (iFSD)
setting. Recently, hypernetwork-based approaches have been studied in the
context of continuous and finetune-free iFSD with limited success. We take a
closer look at important design choices of such methods, leading to several key
improvements and resulting in a more accurate and flexible framework, which we
call Sylph. In particular, we demonstrate the effectiveness of decoupling
object classification from localization by leveraging a base detector that is
pretrained for class-agnostic localization on a large-scale dataset. Contrary
to what previous results have suggested, we show that with a carefully designed
class-conditional hypernetwork, finetune-free iFSD can be highly effective,
especially when a large number of base categories with abundant data are
available for meta-training, almost approaching alternatives that undergo
test-time-training. This result is even more significant considering its many
practical advantages: (1) incrementally learning new classes in sequence
without additional training, (2) detecting both novel and seen classes in a
single pass, and (3) no forgetting of previously seen classes. We benchmark our
model on both COCO and LVIS, reporting as high as 17% AP on the long-tail rare
classes on LVIS, indicating the promise of hypernetwork-based iFSD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Answer Questions in Dynamic Audio-Visual Scenarios. (arXiv:2203.14072v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14072">
<div class="article-summary-box-inner">
<span><p>In this paper, we focus on the Audio-Visual Question Answering (AVQA) task,
which aims to answer questions regarding different visual objects, sounds, and
their associations in videos. The problem requires comprehensive multimodal
understanding and spatio-temporal reasoning over audio-visual scenes. To
benchmark this task and facilitate our study, we introduce a large-scale
MUSIC-AVQA dataset, which contains more than 45K question-answer pairs covering
33 different question templates spanning over different modalities and question
types. We develop several baselines and introduce a spatio-temporal grounded
audio-visual network for the AVQA problem. Our results demonstrate that AVQA
benefits from multisensory perception and our model outperforms recent A-, V-,
and AVQA approaches. We believe that our built dataset has the potential to
serve as testbed for evaluating and promoting progress in audio-visual scene
understanding and spatio-temporal reasoning. Code and dataset:
<a href="http://gewu-lab.github.io/MUSIC-AVQA/">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Discriminative Representation: Multi-view Trajectory Contrastive Learning for Online Multi-object Tracking. (arXiv:2203.14208v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14208">
<div class="article-summary-box-inner">
<span><p>Discriminative representation is crucial for the association step in
multi-object tracking. Recent work mainly utilizes features in single or
neighboring frames for constructing metric loss and empowering networks to
extract representation of targets. Although this strategy is effective, it
fails to fully exploit the information contained in a whole trajectory. To this
end, we propose a strategy, namely multi-view trajectory contrastive learning,
in which each trajectory is represented as a center vector. By maintaining all
the vectors in a dynamically updated memory bank, a trajectory-level
contrastive loss is devised to explore the inter-frame information in the whole
trajectories. Besides, in this strategy, each target is represented as multiple
adaptively selected keypoints rather than a pre-defined anchor or center. This
design allows the network to generate richer representation from multiple views
of the same target, which can better characterize occluded objects.
Additionally, in the inference stage, a similarity-guided feature fusion
strategy is developed for further boosting the quality of the trajectory
representation. Extensive experiments have been conducted on MOTChallenge to
verify the effectiveness of the proposed techniques. The experimental results
indicate that our method has surpassed preceding trackers and established new
state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uni6D: A Unified CNN Framework without Projection Breakdown for 6D Pose Estimation. (arXiv:2203.14531v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14531">
<div class="article-summary-box-inner">
<span><p>As RGB-D sensors become more affordable, using RGB-D images to obtain
high-accuracy 6D pose estimation results becomes a better option.
State-of-the-art approaches typically use different backbones to extract
features for RGB and depth images. They use a 2D CNN for RGB images and a
per-pixel point cloud network for depth data, as well as a fusion network for
feature fusion. We find that the essential reason for using two independent
backbones is the "projection breakdown" problem. In the depth image plane, the
projected 3D structure of the physical world is preserved by the 1D depth value
and its built-in 2D pixel coordinate (UV). Any spatial transformation that
modifies UV, such as resize, flip, crop, or pooling operations in the CNN
pipeline, breaks the binding between the pixel value and UV coordinate. As a
consequence, the 3D structure is no longer preserved by a modified depth image
or feature. To address this issue, we propose a simple yet effective method
denoted as Uni6D that explicitly takes the extra UV data along with RGB-D
images as input. Our method has a Unified CNN framework for 6D pose estimation
with a single CNN backbone. In particular, the architecture of our method is
based on Mask R-CNN with two extra heads, one named RT head for directly
predicting 6D pose and the other named abc head for guiding the network to map
the visible points to their coordinates in the 3D model as an auxiliary module.
This end-to-end approach balances simplicity and accuracy, achieving comparable
accuracy with state of the arts and 7.2x faster inference speed on the
YCB-Video dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UNICON: Combating Label Noise Through Uniform Selection and Contrastive Learning. (arXiv:2203.14542v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14542">
<div class="article-summary-box-inner">
<span><p>Supervised deep learning methods require a large repository of annotated
data; hence, label noise is inevitable. Training with such noisy data
negatively impacts the generalization performance of deep neural networks. To
combat label noise, recent state-of-the-art methods employ some sort of sample
selection mechanism to select a possibly clean subset of data. Next, an
off-the-shelf semi-supervised learning method is used for training where
rejected samples are treated as unlabeled data. Our comprehensive analysis
shows that current selection methods disproportionately select samples from
easy (fast learnable) classes while rejecting those from relatively harder
ones. This creates class imbalance in the selected clean set and in turn,
deteriorates performance under high label noise. In this work, we propose
UNICON, a simple yet effective sample selection method which is robust to high
label noise. To address the disproportionate selection of easy and hard
samples, we introduce a Jensen-Shannon divergence based uniform selection
mechanism which does not require any probabilistic modeling and hyperparameter
tuning. We complement our selection method with contrastive learning to further
combat the memorization of noisy labels. Extensive experimentation on multiple
benchmark datasets demonstrates the effectiveness of UNICON; we obtain an 11.4%
improvement over the current state-of-the-art on CIFAR100 dataset with a 90%
noise rate. Our code is publicly available
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text2Pos: Text-to-Point-Cloud Cross-Modal Localization. (arXiv:2203.15125v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15125">
<div class="article-summary-box-inner">
<span><p>Natural language-based communication with mobile devices and home appliances
is becoming increasingly popular and has the potential to become natural for
communicating with mobile robots in the future. Towards this goal, we
investigate cross-modal text-to-point-cloud localization that will allow us to
specify, for example, a vehicle pick-up or goods delivery location. In
particular, we propose Text2Pos, a cross-modal localization module that learns
to align textual descriptions with localization cues in a coarse- to-fine
manner. Given a point cloud of the environment, Text2Pos locates a position
that is specified via a natural language-based description of the immediate
surroundings. To train Text2Pos and study its performance, we construct
KITTI360Pose, the first dataset for this task based on the recently introduced
KITTI360 dataset. Our experiments show that we can localize 65% of textual
queries within 15m distance to query locations for top-10 retrieved locations.
This is a starting point that we hope will spark future developments towards
language-based navigation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalizing Few-Shot NAS with Gradient Matching. (arXiv:2203.15207v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15207">
<div class="article-summary-box-inner">
<span><p>Efficient performance estimation of architectures drawn from large search
spaces is essential to Neural Architecture Search. One-Shot methods tackle this
challenge by training one supernet to approximate the performance of every
architecture in the search space via weight-sharing, thereby drastically
reducing the search cost. However, due to coupled optimization between child
architectures caused by weight-sharing, One-Shot supernet's performance
estimation could be inaccurate, leading to degraded search outcomes. To address
this issue, Few-Shot NAS reduces the level of weight-sharing by splitting the
One-Shot supernet into multiple separated sub-supernets via edge-wise
(layer-wise) exhaustive partitioning. Since each partition of the supernet is
not equally important, it necessitates the design of a more effective splitting
criterion. In this work, we propose a gradient matching score (GM) that
leverages gradient information at the shared weight for making informed
splitting decisions. Intuitively, gradients from different child models can be
used to identify whether they agree on how to update the shared modules, and
subsequently to decide if they should share the same weight. Compared with
exhaustive partitioning, the proposed criterion significantly reduces the
branching factor per edge. This allows us to split more edges (layers) for a
given budget, resulting in substantially improved performance as NAS search
spaces usually include dozens of edges (layers). Extensive empirical
evaluations of the proposed method on a wide range of search spaces
(NASBench-201, DARTS, MobileNet Space), datasets (cifar10, cifar100, ImageNet)
and search algorithms (DARTS, SNAS, RSPS, ProxylessNAS, OFA) demonstrate that
it significantly outperforms its Few-Shot counterparts while surpassing
previous comparable methods in terms of the accuracy of derived architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Supervised Learning of Semantic Correspondence with Pseudo-Labels. (arXiv:2203.16038v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16038">
<div class="article-summary-box-inner">
<span><p>Establishing dense correspondences across semantically similar images remains
a challenging task due to the significant intra-class variations and background
clutters. Traditionally, a supervised learning was used for training the
models, which required tremendous manually-labeled data, while some methods
suggested a self-supervised or weakly-supervised learning to mitigate the
reliance on the labeled data, but with limited performance. In this paper, we
present a simple, but effective solution for semantic correspondence that
learns the networks in a semi-supervised manner by supplementing few
ground-truth correspondences via utilization of a large amount of confident
correspondences as pseudo-labels, called SemiMatch. Specifically, our framework
generates the pseudo-labels using the model's prediction itself between source
and weakly-augmented target, and uses pseudo-labels to learn the model again
between source and strongly-augmented target, which improves the robustness of
the model. We also present a novel confidence measure for pseudo-labels and
data augmentation tailored for semantic correspondence. In experiments,
SemiMatch achieves state-of-the-art performance on various benchmarks,
especially on PF-Willow by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large-Scale Pre-training for Person Re-identification with Noisy Labels. (arXiv:2203.16533v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16533">
<div class="article-summary-box-inner">
<span><p>This paper aims to address the problem of pre-training for person
re-identification (Re-ID) with noisy labels. To setup the pre-training task, we
apply a simple online multi-object tracking system on raw videos of an existing
unlabeled Re-ID dataset "LUPerson" nd build the Noisy Labeled variant called
"LUPerson-NL". Since theses ID labels automatically derived from tracklets
inevitably contain noises, we develop a large-scale Pre-training framework
utilizing Noisy Labels (PNL), which consists of three learning modules:
supervised Re-ID learning, prototype-based contrastive learning, and
label-guided contrastive learning. In principle, joint learning of these three
modules not only clusters similar examples to one prototype, but also rectifies
noisy labels based on the prototype assignment. We demonstrate that learning
directly from raw videos is a promising alternative for pre-training, which
utilizes spatial and temporal correlations as weak supervision. This simple
pre-training task provides a scalable way to learn SOTA Re-ID representations
from scratch on "LUPerson-NL" without bells and whistles. For example, by
applying on the same supervised Re-ID method MGN, our pre-trained model
improves the mAP over the unsupervised pre-training counterpart by 5.7%, 2.2%,
2.3% on CUHK03, DukeMTMC, and MSMT17 respectively. Under the small-scale or
few-shot setting, the performance gain is even more significant, suggesting a
better transferability of the learned representation. Code is available at
https://github.com/DengpanFu/LUPerson-NL
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UNetFormer: A Unified Vision Transformer Model and Pre-Training Framework for 3D Medical Image Segmentation. (arXiv:2204.00631v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00631">
<div class="article-summary-box-inner">
<span><p>Vision Transformers (ViT)s have recently become popular due to their
outstanding modeling capabilities, in particular for capturing long-range
information, and scalability to dataset and model sizes which has led to
state-of-the-art performance in various computer vision and medical image
analysis tasks. In this work, we introduce a unified framework consisting of
two architectures, dubbed UNetFormer, with a 3D Swin Transformer-based encoder
and Convolutional Neural Network (CNN) and transformer-based decoders. In the
proposed model, the encoder is linked to the decoder via skip connections at
five different resolutions with deep supervision. The design of proposed
architecture allows for meeting a wide range of trade-off requirements between
accuracy and computational cost. In addition, we present a methodology for
self-supervised pre-training of the encoder backbone via learning to predict
randomly masked volumetric tokens using contextual information of visible
tokens. We pre-train our framework on a cohort of $5050$ CT images, gathered
from publicly available CT datasets, and present a systematic investigation of
various components such as masking ratio and patch size that affect the
representation learning capability and performance of downstream tasks. We
validate the effectiveness of our pre-training approach by fine-tuning and
testing our model on liver and liver tumor segmentation task using the Medical
Segmentation Decathlon (MSD) dataset and achieve state-of-the-art performance
in terms of various segmentation metrics. To demonstrate its generalizability,
we train and test the model on BraTS 21 dataset for brain tumor segmentation
using MRI images and outperform other methods in terms of Dice score. Code:
https://github.com/Project-MONAI/research-contributions
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Progressive Minimal Path Method with Embedded CNN. (arXiv:2204.00944v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00944">
<div class="article-summary-box-inner">
<span><p>We propose Path-CNN, a method for the segmentation of centerlines of tubular
structures by embedding convolutional neural networks (CNNs) into the
progressive minimal path method. Minimal path methods are widely used for
topology-aware centerline segmentation, but usually these methods rely on weak,
hand-tuned image features. In contrast, CNNs use strong image features which
are learned automatically from images. But CNNs usually do not take the
topology of the results into account, and often require a large amount of
annotations for training. We integrate CNNs into the minimal path method, so
that both techniques benefit from each other: CNNs employ learned image
features to improve the determination of minimal paths, while the minimal path
method ensures the correct topology of the segmented centerlines, provides
strong geometric priors to increase the performance of CNNs, and reduces the
amount of annotations for the training of CNNs significantly. Our method has
lower hardware requirements than many recent methods. Qualitative and
quantitative comparison with other methods shows that Path-CNN achieves better
performance, especially when dealing with tubular structures with complex
shapes in challenging environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adjusting for Bias with Procedural Data. (arXiv:2204.01108v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01108">
<div class="article-summary-box-inner">
<span><p>3D softwares are now capable of producing highly realistic images that look
nearly indistinguishable from the real images. This raises the question: can
real datasets be enhanced with 3D rendered data? We investigate this question.
In this paper we demonstrate the use of 3D rendered data, procedural, data for
the adjustment of bias in image datasets. We perform error analysis of images
of animals which shows that the misclassification of some animal breeds is
largely a data issue. We then create procedural images of the poorly classified
breeds and that model further trained on procedural data can better classify
poorly performing breeds on real data. We believe that this approach can be
used for the enhancement of visual data for any underrepresented group,
including rare diseases, or any data bias potentially improving the accuracy
and fairness of models. We find that the resulting representations rival or
even out-perform those learned directly from real data, but that good
performance requires care in the 3D rendered procedural data generation. 3D
image dataset can be viewed as a compressed and organized copy of a real
dataset, and we envision a future where more and more procedural data
proliferate while datasets become increasingly unwieldy, missing, or private.
This paper suggests several techniques for dealing with visual representation
learning in such a future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Change Detection Based on Image Reconstruction Loss. (arXiv:2204.01200v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01200">
<div class="article-summary-box-inner">
<span><p>To train the change detector, bi-temporal images taken at different times in
the same area are used. However, collecting labeled bi-temporal images is
expensive and time consuming. To solve this problem, various unsupervised
change detection methods have been proposed, but they still require unlabeled
bi-temporal images. In this paper, we propose unsupervised change detection
based on image reconstruction loss using only unlabeled single temporal single
image. The image reconstruction model is trained to reconstruct the original
source image by receiving the source image and the photometrically transformed
source image as a pair. During inference, the model receives bi-temporal images
as the input, and tries to reconstruct one of the inputs. The changed region
between bi-temporal images shows high reconstruction loss. Our change detector
showed significant performance in various change detection benchmark datasets
even though only a single temporal single source image was used. The code and
trained models will be publicly available for reproducibility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Document Image Dewarping by Grid Regularization. (arXiv:2203.16850v1 [eess.IV] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16850">
<div class="article-summary-box-inner">
<span><p>This paper addresses the problem of document image dewarping, which aims at
eliminating the geometric distortion in document images for document
digitization. Instead of designing a better neural network to approximate the
optical flow fields between the inputs and outputs, we pursue the best
readability by taking the text lines and the document boundaries into account
from a constrained optimization perspective. Specifically, our proposed method
first learns the boundary points and the pixels in the text lines and then
follows the most simple observation that the boundaries and text lines in both
horizontal and vertical directions should be kept after dewarping to introduce
a novel grid regularization scheme. To obtain the final forward mapping for
dewarping, we solve an optimization problem with our proposed grid
regularization. The experiments comprehensively demonstrate that our proposed
approach outperforms the prior arts by large margins in terms of readability
(with the metrics of Character Errors Rate and the Edit Distance) while
maintaining the best image quality on the publicly-available DocUNet benchmark.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-04-06 23:08:15.705720639 UTC">2022-04-06 23:08:15 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>