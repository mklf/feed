<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-01-07T01:30:00Z">01-07</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Does entity abstraction help generative Transformers reason?. (arXiv:2201.01787v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01787">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (LMs) often struggle to reason logically or
generalize in a compositional fashion. Recent work suggests that incorporating
external entity knowledge can improve LMs' abilities to reason and generalize.
However, the effect of explicitly providing entity abstraction remains unclear,
especially with recent studies suggesting that pre-trained LMs already encode
some of that knowledge in their parameters. We study the utility of
incorporating entity type abstractions into pre-trained Transformers and test
these methods on four NLP tasks requiring different forms of logical reasoning:
(1) compositional language understanding with text-based relational reasoning
(CLUTRR), (2) abductive reasoning (ProofWriter), (3) multi-hop question
answering (HotpotQA), and (4) conversational question answering (CoQA). We
propose and empirically explore three ways to add such abstraction: (i) as
additional input embeddings, (ii) as a separate sequence to encode, and (iii)
as an auxiliary prediction task for the model. Overall, our analysis
demonstrates that models with abstract entity knowledge performs better than
without it. However, our experiments also show that the benefits strongly
depend on the technique used and the task at hand. The best abstraction aware
models achieved an overall accuracy of 88.8% and 91.8% compared to the baseline
model achieving 62.3% and 89.8% on CLUTRR and ProofWriter respectively. In
addition, abstraction-aware models showed improved compositional generalization
in both interpolation and extrapolation settings. However, for HotpotQA and
CoQA, we find that F1 scores improve by only 0.5% on average. Our results
suggest that the benefit of explicit abstraction is significant in formally
defined logical reasoning settings requiring many reasoning hops, but point to
the notion that it is less beneficial for NLP tasks having less formal logical
structure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KUDO Interpreter Assist: Automated Real-time Support for Remote Interpretation. (arXiv:2201.01800v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01800">
<div class="article-summary-box-inner">
<span><p>High-quality human interpretation requires linguistic and factual preparation
as well as the ability to retrieve information in real-time. This situation
becomes particularly relevant in the context of remote simultaneous
interpreting (RSI) where time-to-event may be short, posing new challenges to
professional interpreters and their commitment to delivering high-quality
services. In order to mitigate these challenges, we present Interpreter Assist,
a computer-assisted interpreting tool specifically designed for the integration
in RSI scenarios. Interpreter Assist comprises two main feature sets: an
automatic glossary creation tool and a real-time suggestion system. In this
paper, we describe the overall design of our tool, its integration into the
typical RSI workflow, and the results achieved on benchmark tests both in terms
of quality and relevance of glossary creation as well as in precision and
recall of the real-time suggestion feature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Frame Shift Prediction. (arXiv:2201.01837v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01837">
<div class="article-summary-box-inner">
<span><p>Frame shift is a cross-linguistic phenomenon in translation which results in
corresponding pairs of linguistic material evoking different frames. The
ability to predict frame shifts enables automatic creation of multilingual
FrameNets through annotation projection. Here, we propose the Frame Shift
Prediction task and demonstrate that graph attention networks, combined with
auxiliary training, can learn cross-linguistic frame-to-frame correspondence
and predict frame shifts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data-driven Model Generalizability in Crosslinguistic Low-resource Morphological Segmentation. (arXiv:2201.01845v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01845">
<div class="article-summary-box-inner">
<span><p>Common designs of model evaluation typically focus on monolingual settings,
where different models are compared according to their performance on a single
data set that is assumed to be representative of all possible data for the task
at hand. While this may be reasonable for a large data set, this assumption is
difficult to maintain in low-resource scenarios, where artifacts of the data
collection can yield data sets that are outliers, potentially making
conclusions about model performance coincidental. To address these concerns, we
investigate model generalizability in crosslinguistic low-resource scenarios.
Using morphological segmentation as the test case, we compare three broad
classes of models with different parameterizations, taking data from 11
languages across 6 language families. In each experimental setting, we evaluate
all models on a first data set, then examine their performance consistency when
introducing new randomly sampled data sets with the same size and when applying
the trained models to unseen test sets of varying sizes. The results
demonstrate that the extent of model generalization depends on the
characteristics of the data set, and does not necessarily rely heavily on the
data set size. Among the characteristics that we studied, the ratio of morpheme
overlap and that of the average number of morphemes per word between the
training and test sets are the two most prominent factors. Our findings suggest
that future work should adopt random sampling to construct data sets with
different sizes in order to make more responsible claims about model
evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Related Work Generation: A Meta Study. (arXiv:2201.01880v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01880">
<div class="article-summary-box-inner">
<span><p>Academic research is an exploration activity to solve problems that have
never been resolved before. By this nature, each academic research work is
required to perform a literature review to distinguish its novelties that have
not been addressed by prior works. In natural language processing, this
literature review is usually conducted under the "Related Work" section. The
task of automatic related work generation aims to automatically generate the
"Related Work" section given the rest of the research paper and a list of cited
papers. Although this task was proposed over 10 years ago, it received little
attention until very recently, when it was cast as a variant of the scientific
multi-document summarization problem. However, even today, the problems of
automatic related work and citation text generation are not yet standardized.
In this survey, we conduct a meta-study to compare the existing literature on
related work generation from the perspectives of problem formulation, dataset
collection, methodological approach, performance evaluation, and future
prospects to provide the reader insight into the progress of the
state-of-the-art studies, as well as and how future studies can be conducted.
We also survey relevant fields of study that we suggest future work to consider
integrating.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incremental Object Grounding Using Scene Graphs. (arXiv:2201.01901v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01901">
<div class="article-summary-box-inner">
<span><p>Object grounding tasks aim to locate the target object in an image through
verbal communications. Understanding human command is an important process
needed for effective human-robot communication. However, this is challenging
because human commands can be ambiguous and erroneous. This paper aims to
disambiguate the human's referring expressions by allowing the agent to ask
relevant questions based on semantic data obtained from scene graphs. We test
if our agent can use relations between objects from a scene graph to ask
semantically relevant questions that can disambiguate the original user
command. In this paper, we present Incremental Grounding using Scene Graphs
(IGSG), a disambiguation model that uses semantic data from an image scene
graph and linguistic structures from a language scene graph to ground objects
based on human command. Compared to the baseline, IGSG shows promising results
in complex real-world scenes where there are multiple identical target objects.
IGSG can effectively disambiguate ambiguous or wrong referring expressions by
asking disambiguating questions back to the user.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HuSpaCy: an industrial-strength Hungarian natural language processing toolkit. (arXiv:2201.01956v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01956">
<div class="article-summary-box-inner">
<span><p>Although there are a couple of open-source language processing pipelines
available for Hungarian, none of them satisfies the requirements of today's NLP
applications. A language processing pipeline should consist of close to
state-of-the-art lemmatization, morphosyntactic analysis, entity recognition
and word embeddings. Industrial text processing applications have to satisfy
non-functional software quality requirements, what is more, frameworks
supporting multiple languages are more and more favored. This paper introduces
HuSpaCy, an industryready Hungarian language processing pipeline. The presented
tool provides components for the most important basic linguistic analysis
tasks. It is open-source and is available under a permissive license. Our
system is built upon spaCy's NLP components which means that it is fast, has a
rich ecosystem of NLP applications and extensions, comes with extensive
documentation and a well-known API. Besides the overview of the underlying
models, we also present rigorous evaluation on common benchmark datasets. Our
experiments confirm that HuSpaCy has high accuracy in all subtasks while
maintaining resource-efficient prediction capabilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compact Bidirectional Transformer for Image Captioning. (arXiv:2201.01984v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01984">
<div class="article-summary-box-inner">
<span><p>Most current image captioning models typically generate captions from left to
right. This unidirectional property makes them can only leverage past context
but not future context. Though recent refinement-based models can exploit both
past and future context by generating a new caption in the second stage based
on pre-retrieved or pre-generated captions in the first stage, the decoder of
these models generally consists of two networks~(i.e. a retriever or captioner
in the first stage and a refiner in the second stage), which can only be
executed sequentially. In this paper, we introduce a Compact Bidirectional
Transformer model for image captioning that can leverage bidirectional context
implicitly and explicitly while the decoder can be executed parallelly.
Specifically, it is implemented by tightly coupling left-to-right(L2R) and
right-to-left(R2L) flows into a single compact model~(i.e. implicitly) and
optionally allowing interaction of the two flows(i.e. explicitly), while the
final caption is chosen from either L2R or R2L flow in a sentence-level
ensemble manner. We conduct extensive ablation studies on the MSCOCO benchmark
and find that the compact architecture, which serves as a regularization for
implicitly exploiting bidirectional context, and the sentence-level ensemble
play more important roles than the explicit interaction mechanism. By combining
with word-level ensemble seamlessly, the effect of the sentence-level ensemble
is further enlarged. We further extend the conventional one-flow self-critical
training to the two-flows version under this architecture and achieve new
state-of-the-art results in comparison with non-vision-language-pretraining
models. Source code is available at
{\color{magenta}\url{https://github.com/YuanEZhou/CBTrans}}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Mandarin End-to-End Speech Recognition with Word N-gram Language Model. (arXiv:2201.01995v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01995">
<div class="article-summary-box-inner">
<span><p>Despite the rapid progress of end-to-end (E2E) automatic speech recognition
(ASR), it has been shown that incorporating external language models (LMs) into
the decoding can further improve the recognition performance of E2E ASR
systems. To align with the modeling units adopted in E2E ASR systems,
subword-level (e.g., characters, BPE) LMs are usually used to cooperate with
current E2E ASR systems. However, the use of subword-level LMs will ignore the
word-level information, which may limit the strength of the external LMs in E2E
ASR. Although several methods have been proposed to incorporate word-level
external LMs in E2E ASR, these methods are mainly designed for languages with
clear word boundaries such as English and cannot be directly applied to
languages like Mandarin, in which each character sequence can have multiple
corresponding word sequences. To this end, we propose a novel decoding
algorithm where a word-level lattice is constructed on-the-fly to consider all
possible word sequences for each partial hypothesis. Then, the LM score of the
hypothesis is obtained by intersecting the generated lattice with an external
word N-gram LM. The proposed method is examined on both Attention-based
Encoder-Decoder (AED) and Neural Transducer (NT) frameworks. Experiments
suggest that our method consistently outperforms subword-level LMs, including
N-gram LM and neural network LM. We achieve state-of-the-art results on both
Aishell-1 (CER 4.18%) and Aishell-2 (CER 5.06%) datasets and reduce CER by
14.8% relatively on a 21K-hour Mandarin dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An exploratory experiment on Hindi, Bengali hate-speech detection and transfer learning using neural networks. (arXiv:2201.01997v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01997">
<div class="article-summary-box-inner">
<span><p>This work presents our approach to train a neural network to detect
hate-speech texts in Hindi and Bengali. We also explore how transfer learning
can be applied to learning these languages, given that they have the same
origin and thus, are similar to some extend. Even though the whole experiment
was conducted with low computational power, the obtained result is comparable
to the results of other, more expensive, models. Furthermore, since the
training data in use is relatively small and the two languages are almost
entirely unknown to us, this work can be generalized as an effort to demystify
lost or alien languages that no human is capable of understanding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Phrase-level Adversarial Example Generation for Neural Machine Translation. (arXiv:2201.02009v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02009">
<div class="article-summary-box-inner">
<span><p>While end-to-end neural machine translation (NMT) has achieved impressive
progress, noisy input usually leads models to become fragile and unstable.
Generating adversarial examples as the augmented data is proved to be useful to
alleviate this problem. Existing methods for adversarial example generation
(AEG) are word-level or character-level. In this paper, we propose a
phrase-level adversarial example generation (PAEG) method to enhance the
robustness of the model. Our method leverages a gradient-based strategy to
substitute phrases of vulnerable positions in the source input. We verify our
method on three benchmarks, including LDC Chinese-English, IWSLT14
German-English, and WMT14 English-German tasks. Experimental results
demonstrate that our approach significantly improves performance compared to
previous methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Training Vision Language BERTs with a Unified Conditional Model. (arXiv:2201.02010v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02010">
<div class="article-summary-box-inner">
<span><p>Natural language BERTs are trained with language corpus in a self-supervised
manner. Unlike natural language BERTs, vision language BERTs need paired data
to train, which restricts the scale of VL-BERT pretraining. We propose a
self-training approach that allows training VL-BERTs from unlabeled image data.
The proposed method starts with our unified conditional model -- a vision
language BERT model that can perform zero-shot conditional generation. Given
different conditions, the unified conditional model can generate captions,
dense captions, and even questions. We use the labeled image data to train a
teacher model and use the trained model to generate pseudo captions on
unlabeled image data. We then combine the labeled data and pseudo labeled data
to train a student model. The process is iterated by putting the student model
as a new teacher. By using the proposed self-training approach and only 300k
unlabeled extra data, we are able to get competitive or even better
performances compared to the models of similar model size trained with 3
million extra image data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fortunately, Discourse Markers Can Enhance Language Models for Sentiment Analysis. (arXiv:2201.02026v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02026">
<div class="article-summary-box-inner">
<span><p>In recent years, pretrained language models have revolutionized the NLP
world, while achieving state of the art performance in various downstream
tasks. However, in many cases, these models do not perform well when labeled
data is scarce and the model is expected to perform in the zero or few shot
setting. Recently, several works have shown that continual pretraining or
performing a second phase of pretraining (inter-training) which is better
aligned with the downstream task, can lead to improved results, especially in
the scarce data setting. Here, we propose to leverage sentiment-carrying
discourse markers to generate large-scale weakly-labeled data, which in turn
can be used to adapt language models for sentiment analysis. Extensive
experimental results show the value of our approach on various benchmark
datasets, including the finance domain. Code, models and data are available at
https://github.com/ibm/tslm-discourse-markers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bayesian Regression Approach for Building and Stacking Predictive Models in Time Series Analytics. (arXiv:2201.02034v1 [stat.AP])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02034">
<div class="article-summary-box-inner">
<span><p>The paper describes the use of Bayesian regression for building time series
models and stacking different predictive models for time series. Using Bayesian
regression for time series modeling with nonlinear trend was analyzed. This
approach makes it possible to estimate an uncertainty of time series prediction
and calculate value at risk characteristics. A hierarchical model for time
series using Bayesian regression has been considered. In this approach, one set
of parameters is the same for all data samples, other parameters can be
different for different groups of data samples. Such an approach allows using
this model in the case of short historical data for specified time series, e.g.
in the case of new stores or new products in the sales prediction problem. In
the study of predictive models stacking, the models ARIMA, Neural Network,
Random Forest, Extra Tree were used for the prediction on the first level of
model ensemble. On the second level, time series predictions of these models on
the validation set were used for stacking by Bayesian regression. This approach
gives distributions for regression coefficients of these models. It makes it
possible to estimate the uncertainty contributed by each model to stacking
result. The information about these distributions allows us to select an
optimal set of stacking models, taking into account the domain knowledge. The
probabilistic approach for stacking predictive models allows us to make risk
assessment for the predictions that are important in a decision-making process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Forming Predictive Features of Tweets for Decision-Making Support. (arXiv:2201.02049v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02049">
<div class="article-summary-box-inner">
<span><p>The article describes the approaches for forming different predictive
features of tweet data sets and using them in the predictive analysis for
decision-making support. The graph theory as well as frequent itemsets and
association rules theory is used for forming and retrieving different features
from these datasests. The use of these approaches makes it possible to reveal a
semantic structure in tweets related to a specified entity. It is shown that
quantitative characteristics of semantic frequent itemsets can be used in
predictive regression models with specified target variables.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sales Time Series Analytics Using Deep Q-Learning. (arXiv:2201.02058v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02058">
<div class="article-summary-box-inner">
<span><p>The article describes the use of deep Q-learning models in the problems of
sales time series analytics. In contrast to supervised machine learning which
is a kind of passive learning using historical data, Q-learning is a kind of
active learning with goal to maximize a reward by optimal sequence of actions.
Model free Q-learning approach for optimal pricing strategies and supply-demand
problems was considered in the work. The main idea of the study is to show that
using deep Q-learning approach in time series analytics, the sequence of
actions can be optimized by maximizing the reward function when the environment
for learning agent interaction can be modeled using the parametric model and in
the case of using the model which is based on the historical data. In the
pricing optimizing case study environment was modeled using sales dependence on
extras price and randomly simulated demand. In the pricing optimizing case
study, the environment was modeled using sales dependence on extra price and
randomly simulated demand. In the supply-demand case study, it was proposed to
use historical demand time series for environment modeling, agent states were
represented by promo actions, previous demand values and weekly seasonality
features. Obtained results show that using deep Q-learning, we can optimize the
decision making process for price optimization and supply-demand problems.
Environment modeling using parametric models and historical data can be used
for the cold start of learning agent. On the next steps, after the cold start,
the trained agent can be used in real business environment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ASL-Skeleton3D and ASL-Phono: Two Novel Datasets for the American Sign Language. (arXiv:2201.02065v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02065">
<div class="article-summary-box-inner">
<span><p>Sign language is an essential resource enabling access to communication and
proper socioemotional development for individuals suffering from disabling
hearing loss. As this population is expected to reach 700 million by 2050, the
importance of the language becomes even more essential as it plays a critical
role to ensure the inclusion of such individuals in society. The Sign Language
Recognition field aims to bridge the gap between users and non-users of sign
languages. However, the scarcity in quantity and quality of datasets is one of
the main challenges limiting the exploration of novel approaches that could
lead to significant advancements in this research area. Thus, this paper
contributes by introducing two new datasets for the American Sign Language: the
first is composed of the three-dimensional representation of the signers and,
the second, by an unprecedented linguistics-based representation containing a
set of phonological attributes of the signs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERN2: an advanced neural biomedical named entity recognition and normalization tool. (arXiv:2201.02080v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02080">
<div class="article-summary-box-inner">
<span><p>In biomedical natural language processing, named entity recognition (NER) and
named entity normalization (NEN) are key tasks that enable the automatic
extraction of biomedical entities (e.g., diseases and chemicals) from the
ever-growing biomedical literature. In this paper, we present BERN2 (Advanced
Biomedical Entity Recognition and Normalization), a tool that improves the
previous neural network-based NER tool (Kim et al., 2019) by employing a
multi-task NER model and neural network-based NEN models to achieve much faster
and more accurate inference. We hope that our tool can help annotate
large-scale biomedical texts more accurately for various tasks such as
biomedical knowledge graph construction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ConTrip: Consensus Sentiment review Analysis and Platform ratings in a single score. (arXiv:2201.02113v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02113">
<div class="article-summary-box-inner">
<span><p>People unequivocally employ reviews to decide on purchasing an item or an
experience on the internet. In that regard, the growing significance and number
of opinions have led to the development of methods to assess their sentiment
content automatically. However, it is not straightforward for the models to
create a consensus value that embodies the agreement of the different reviews
and differentiates across equal ratings for an item. Based on the approach
proposed by Nguyen et al. in 2020, we derive a novel consensus value named
ConTrip that merges their consensus score and the overall rating of a platform
for an item. ConTrip lies in the rating range values, which makes it more
interpretable while maintaining the ability to differentiate across equally
rated experiences. ConTrip is implemented and freely available under MIT
license at https://github.com/pepebonet/contripscore
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Opinion Mining of Text in COVID-19 Issues along with Comparative Study in ML, BERT & RNN. (arXiv:2201.02119v1 [cs.NE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02119">
<div class="article-summary-box-inner">
<span><p>The global world is crossing a pandemic situation where this is a
catastrophic outbreak of Respiratory Syndrome recognized as COVID-19. This is a
global threat all over the 212 countries that people every day meet with mighty
situations. On the contrary, thousands of infected people live rich in
mountains. Mental health is also affected by this worldwide coronavirus
situation. Due to this situation online sources made a communicative place that
common people shares their opinion in any agenda. Such as affected news related
positive and negative, financial issues, country and family crisis, lack of
import and export earning system etc. different kinds of circumstances are
recent trendy news in anywhere. Thus, vast amounts of text are produced within
moments therefore, in subcontinent areas the same as situation in other
countries and peoples opinion of text and situation also same but the language
is different. This article has proposed some specific inputs along with Bangla
text comments from individual sources which can assure the goal of illustration
that machine learning outcome capable of building an assistive system. Opinion
mining assistive system can be impactful in all language preferences possible.
To the best of our knowledge, the article predicted the Bangla input text on
COVID-19 issues proposed ML algorithms and deep learning models analysis also
check the future reachability with a comparative analysis. Comparative analysis
states a report on text prediction accuracy is 91% along with ML algorithms and
79% along with Deep Learning Models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentiment Analysis and Sarcasm Detection of Indian General Election Tweets. (arXiv:2201.02127v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02127">
<div class="article-summary-box-inner">
<span><p>Social Media usage has increased to an all-time high level in today's digital
world. The majority of the population uses social media tools (like Twitter,
Facebook, YouTube, etc.) to share their thoughts and experiences with the
community. Analysing the sentiments and opinions of the common public is very
important for both the government and the business people. This is the reason
behind the activeness of many media agencies during the election time for
performing various kinds of opinion polls. In this paper, we have worked
towards analysing the sentiments of the people of India during the Lok Sabha
election of 2019 using the Twitter data of that duration. We have built an
automatic tweet analyser using the Transfer Learning technique to handle the
unsupervised nature of this problem. We have used the Linear Support Vector
Classifiers method in our Machine Learning model, also, the Term Frequency
Inverse Document Frequency (TF-IDF) methodology for handling the textual data
of tweets. Further, we have increased the capability of the model to address
the sarcastic tweets posted by some of the users, which has not been yet
considered by the researchers in this domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DialogLM: Pre-trained Model for Long Dialogue Understanding and Summarization. (arXiv:2109.02492v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02492">
<div class="article-summary-box-inner">
<span><p>Dialogue is an essential part of human communication and cooperation.
Existing research mainly focuses on short dialogue scenarios in a one-on-one
fashion. However, multi-person interactions in the real world, such as meetings
or interviews, are frequently over a few thousand words. There is still a lack
of corresponding research and powerful tools to understand and process such
long dialogues. Therefore, in this work, we present a pre-training framework
for long dialogue understanding and summarization. Considering the nature of
long conversations, we propose a window-based denoising approach for generative
pre-training. For a dialogue, it corrupts a window of text with
dialogue-inspired noise, and guides the model to reconstruct this window based
on the content of the remaining conversation. Furthermore, to process longer
input, we augment the model with sparse attention which is combined with
conventional attention in a hybrid manner. We conduct extensive experiments on
five datasets of long dialogues, covering tasks of dialogue summarization,
abstractive question answering and topic segmentation. Experimentally, we show
that our pre-trained model DialogLM significantly surpasses the
state-of-the-art models across datasets and tasks. Source code and all the
pre-trained models are available on our GitHub repository
(https://github.com/microsoft/DialogLM).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FST Morphological Analyser and Generator for Mapud\"ungun. (arXiv:2109.09176v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09176">
<div class="article-summary-box-inner">
<span><p>Following the Mapuche grammar by Smeets, this article describes the main
morphophonological aspects of Mapud\"ungun, explaining what triggers them and
the contexts where they arise. We present a computational approach producing a
finite state morphological analyser (and generator) capable of classifying and
appropriately tagging all the components (roots and suffixes) that interact in
a Mapuche word form. The bulk of the article focuses on presenting details
about the morphology of Mapud\"ungun verb and its formalisation using FOMA. A
system evaluation process and its results are also present in this article.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HydraText: Multi-objective Optimization for Adversarial Textual Attack. (arXiv:2111.01528v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.01528">
<div class="article-summary-box-inner">
<span><p>The field of adversarial textual attack has significantly grown over the last
few years, where the commonly considered objective is to craft adversarial
examples (AEs) that can successfully fool the target model. However, the
imperceptibility of attacks, which is also an essential objective for practical
attackers, is often left out by previous studies. In consequence, the crafted
AEs tend to have obvious structural and semantic differences from the original
human-written texts, making them easily perceptible. In this paper, we advocate
simultaneously considering both objectives of successful and imperceptible
attacks. Specifically, we formulate the problem of crafting AEs as a
multi-objective set maximization problem, and propose a novel evolutionary
algorithm (dubbed HydraText) to solve it. To the best of our knowledge,
HydraText is currently the only approach that can be effectively applied to
both score-based and decision-based attack settings. Exhaustive experiments
involving 44237 instances demonstrate that HydraText consistently achieves
higher attack success rates and better attack imperceptibility than the
state-of-the-art textual attack approaches. A human evaluation study also shows
that the AEs crafted by HydraText are more indistinguishable from human-written
texts. Finally, these AEs exhibit good transferability and can bring notable
robustness improvement to the target models by adversarial training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GALAXY: A Generative Pre-trained Model for Task-Oriented Dialog with Semi-Supervised Learning and Explicit Policy Injection. (arXiv:2111.14592v7 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14592">
<div class="article-summary-box-inner">
<span><p>Pre-trained models have proved to be powerful in enhancing task-oriented
dialog systems. However, current pre-training methods mainly focus on enhancing
dialog understanding and generation tasks while neglecting the exploitation of
dialog policy. In this paper, we propose GALAXY, a novel pre-trained dialog
model that explicitly learns dialog policy from limited labeled dialogs and
large-scale unlabeled dialog corpora via semi-supervised learning.
Specifically, we introduce a dialog act prediction task for policy optimization
during pre-training and employ a consistency regularization term to refine the
learned representation with the help of unlabeled dialogs. We also implement a
gating mechanism to weigh suitable unlabeled dialog samples. Empirical results
show that GALAXY substantially improves the performance of task-oriented dialog
systems, and achieves new state-of-the-art results on benchmark datasets:
In-Car, MultiWOZ2.0 and MultiWOZ2.1, improving their end-to-end combined scores
by 2.5, 5.3 and 5.5 points, respectively. We also show that GALAXY has a
stronger few-shot ability than existing models under various low-resource
settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">JABER and SABER: Junior and Senior Arabic BERt. (arXiv:2112.04329v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04329">
<div class="article-summary-box-inner">
<span><p>Language-specific pre-trained models have proven to be more accurate than
multilingual ones in a monolingual evaluation setting, Arabic is no exception.
However, we found that previously released Arabic BERT models were
significantly under-trained. In this technical report, we present JABER and
SABER, Junior and Senior Arabic BERt respectively, our pre-trained language
model prototypes dedicated for Arabic. We conduct an empirical study to
systematically evaluate the performance of models across a diverse set of
existing Arabic NLU tasks. Experimental results show that JABER and SABER
achieve state-of-the-art performances on ALUE, a new benchmark for Arabic
Language Understanding Evaluation, as well as on a well-established NER
benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text is no more Enough! A Benchmark for Profile-based Spoken Language Understanding. (arXiv:2112.11953v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11953">
<div class="article-summary-box-inner">
<span><p>Current researches on spoken language understanding (SLU) heavily are limited
to a simple setting: the plain text-based SLU that takes the user utterance as
input and generates its corresponding semantic frames (e.g., intent and slots).
Unfortunately, such a simple setting may fail to work in complex real-world
scenarios when an utterance is semantically ambiguous, which cannot be achieved
by the text-based SLU models. In this paper, we first introduce a new and
important task, Profile-based Spoken Language Understanding (ProSLU), which
requires the model that not only relies on the plain text but also the
supporting profile information to predict the correct intents and slots. To
this end, we further introduce a large-scale human-annotated Chinese dataset
with over 5K utterances and their corresponding supporting profile information
(Knowledge Graph (KG), User Profile (UP), Context Awareness (CA)). In addition,
we evaluate several state-of-the-art baseline models and explore a multi-level
knowledge adapter to effectively incorporate profile information. Experimental
results reveal that all existing text-based SLU models fail to work when the
utterances are semantically ambiguous and our proposed framework can
effectively fuse the supporting information for sentence-level intent detection
and token-level slot filling. Finally, we summarize key challenges and provide
new points for future directions, which hopes to facilitate the research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chinese Learners' Phonetic Transfer of /i/ from Mandarin Chinese to General American English: Evidence from Perception and Production Experiments. (arXiv:2112.13571v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13571">
<div class="article-summary-box-inner">
<span><p>Ever since the development of Contrastive Analysis (CA) in the 1950s, which
focuses on comparing and contrasting two language systems, linguists have
started to systematically explore the influence of the mother tongue on
acquiring a second language. This phenomenon is later defined as "language
transfer". The current paper concerns language transfer at the phonetic level
and concentrates on the transfer phenomenon existing in advanced-level Chinese
learners' acquisition of English vowels /i/ and its lax counterpart. By
determining whether advanced-level Chinese English-language learners (ELLs) can
accurately distinguish between /i/ and its lax counterpart, and pronounce them
in English words precisely, this paper serves as a reference for further
studying Chinese ELLs' language transfer. Two objectives were to be met:
firstly, learners' perceptual ability to distinguish between vowels /i/ and its
lax counterpart should be examined; and secondly, the effect of the phonetic
transfer should be determined. A perception test and a production test were
used to attain these two objectives. Both tests were completed by six
advanced-level Chinese ELLs, three males and three females. Results indicate
that both male and female participants could consciously distinguish between
/i/ and its lax counterpart. All participants have signs of experiencing
negative phonetic transfer in their pronunciation, except that the current data
do not decisively reflect an impact of the phonetic transfer on female ELLs'
acquisition of the high front lax vowel in English words.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mining Adverse Drug Reactions from Unstructured Mediums at Scale. (arXiv:2201.01405v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01405">
<div class="article-summary-box-inner">
<span><p>Adverse drug reactions / events (ADR/ADE) have a major impact on patient
health and health care costs. Detecting ADR's as early as possible and sharing
them with regulators, pharma companies, and healthcare providers can prevent
morbidity and save many lives. While most ADR's are not reported via formal
channels, they are often documented in a variety of unstructured conversations
such as social media posts by patients, customer support call transcripts, or
CRM notes of meetings between healthcare providers and pharma sales reps. In
this paper, we propose a natural language processing (NLP) solution that
detects ADR's in such unstructured free-text conversations, which improves on
previous work in three ways. First, a new Named Entity Recognition (NER) model
obtains new state-of-the-art accuracy for ADR and Drug entity extraction on the
ADE, CADEC, and SMM4H benchmark datasets (91.75%, 78.76%, and 83.41% F1 scores
respectively). Second, two new Relation Extraction (RE) models are introduced -
one based on BioBERT while the other utilizing crafted features over a Fully
Connected Neural Network (FCNN) - are shown to perform on par with existing
state-of-the-art models, and outperform them when trained with a supplementary
clinician-annotated RE dataset. Third, a new text classification model, for
deciding if a conversation includes an ADR, obtains new state-of-the-art
accuracy on the CADEC dataset (86.69% F1 score). The complete solution is
implemented as a unified NLP pipeline in a production-grade library built on
top of Apache Spark, making it natively scalable and able to process millions
of batch or streaming records on commodity clusters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">All You Need In Sign Language Production. (arXiv:2201.01609v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01609">
<div class="article-summary-box-inner">
<span><p>Sign Language is the dominant form of communication language used in the deaf
and hearing-impaired community. To make an easy and mutual communication
between the hearing-impaired and the hearing communities, building a robust
system capable of translating the spoken language into sign language and vice
versa is fundamental. To this end, sign language recognition and production are
two necessary parts for making such a two-way system. Sign language recognition
and production need to cope with some critical challenges. In this survey, we
review recent advances in Sign Language Production (SLP) and related areas
using deep learning. To have more realistic perspectives to sign language, we
present an introduction to the Deaf culture, Deaf centers, psychological
perspective of sign language, the main differences between spoken language and
sign language. Furthermore, we present the fundamental components of a
bi-directional sign language translation system, discussing the main challenges
in this area. Also, the backbone architectures and methods in SLP are briefly
introduced and the proposed taxonomy on SLP is presented. Finally, a general
framework for SLP and performance evaluation, and also a discussion on the
recent developments, advantages, and limitations in SLP, commenting on possible
lines for future research are presented.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantum Capsule Networks. (arXiv:2201.01778v1 [quant-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01778">
<div class="article-summary-box-inner">
<span><p>Capsule networks, which incorporate the paradigms of connectionism and
symbolism, have brought fresh insights into artificial intelligence. The
capsule, as the building block of capsule networks, is a group of neurons
represented by a vector to encode different features of an entity. The
information is extracted hierarchically through capsule layers via routing
algorithms. Here, we introduce a quantum capsule network (dubbed QCapsNet)
together with a quantum dynamic routing algorithm. Our model enjoys an
exponential speedup in the dynamic routing process and exhibits an enhanced
representation power. To benchmark the performance of the QCapsNet, we carry
out extensive numerical simulations on the classification of handwritten digits
and symmetry-protected topological phases, and show that the QCapsNet can
achieve the state-of-the-art accuracy and outperforms conventional quantum
classifiers evidently. We further unpack the output capsule state and find that
a particular subspace may correspond to a human-understandable feature of the
input data, which indicates the potential explainability of such networks. Our
work reveals an intriguing prospect of quantum capsule networks in quantum
machine learning, which may provide a valuable guide towards explainable
quantum artificial intelligence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Scoring of Graphical Open-Ended Responses Using Artificial Neural Networks. (arXiv:2201.01783v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01783">
<div class="article-summary-box-inner">
<span><p>Automated scoring of free drawings or images as responses has yet to be
utilized in large-scale assessments of student achievement. In this study, we
propose artificial neural networks to classify these types of graphical
responses from a computer based international mathematics and science
assessment. We are comparing classification accuracy of convolutional and
feedforward approaches. Our results show that convolutional neural networks
(CNNs) outperform feedforward neural networks in both loss and accuracy. The
CNN models classified up to 97.71% of the image responses into the appropriate
scoring category, which is comparable to, if not more accurate, than typical
human raters. These findings were further strengthened by the observation that
the most accurate CNN models correctly classified some image responses that had
been incorrectly scored by the human raters. As an additional innovation, we
outline a method to select human rated responses for the training sample based
on an application of the expected response function derived from item response
theory. This paper argues that CNN-based automated scoring of image responses
is a highly accurate procedure that could potentially replace the workload and
cost of second human raters for large scale assessments, while improving the
validity and comparability of scoring complex constructed-response items.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Deep Subspace Alignment for Unsupervised Domain Adaptation. (arXiv:2201.01806v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01806">
<div class="article-summary-box-inner">
<span><p>Unsupervised domain adaptation (UDA) aims to transfer and adapt knowledge
from a labeled source domain to an unlabeled target domain. Traditionally,
subspace-based methods form an important class of solutions to this problem.
Despite their mathematical elegance and tractability, these methods are often
found to be ineffective at producing domain-invariant features with complex,
real-world datasets. Motivated by the recent advances in representation
learning with deep networks, this paper revisits the use of subspace alignment
for UDA and proposes a novel adaptation algorithm that consistently leads to
improved generalization. In contrast to existing adversarial training-based DA
methods, our approach isolates feature learning and distribution alignment
steps, and utilizes a primary-auxiliary optimization strategy to effectively
balance the objectives of domain invariance and model fidelity. While providing
a significant reduction in target data and computational requirements, our
subspace-based DA performs competitively and sometimes even outperforms
state-of-the-art approaches on several standard UDA benchmarks. Furthermore,
subspace alignment leads to intrinsically well-regularized models that
demonstrate strong generalization even in the challenging partial DA setting.
Finally, the design of our UDA framework inherently supports progressive
adaptation to new target domains at test-time, without requiring retraining of
the model from scratch. In summary, powered by powerful feature learners and an
effective optimization strategy, we establish subspace-based DA as a highly
effective approach for visual recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Formal Analysis of Art: Proxy Learning of Visual Concepts from Style Through Language Models. (arXiv:2201.01819v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01819">
<div class="article-summary-box-inner">
<span><p>We present a machine learning system that can quantify fine art paintings
with a set of visual elements and principles of art. This formal analysis is
fundamental for understanding art, but developing such a system is challenging.
Paintings have high visual complexities, but it is also difficult to collect
enough training data with direct labels. To resolve these practical
limitations, we introduce a novel mechanism, called proxy learning, which
learns visual concepts in paintings though their general relation to styles.
This framework does not require any visual annotation, but only uses style
labels and a general relationship between visual concepts and style. In this
paper, we propose a novel proxy model and reformulate four pre-existing methods
in the context of proxy learning. Through quantitative and qualitative
comparison, we evaluate these methods and compare their effectiveness in
quantifying the artistic visual concepts, where the general relationship is
estimated by language models; GloVe or BERT. The language modeling is a
practical and scalable solution requiring no labeling, but it is inevitably
imperfect. We demonstrate how the new proxy model is robust to the
imperfection, while the other models are sensitively affected by it.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Semantic Ambiguities for Zero-Shot Learning. (arXiv:2201.01823v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01823">
<div class="article-summary-box-inner">
<span><p>Zero-shot learning (ZSL) aims at recognizing classes for which no visual
sample is available at training time. To address this issue, one can rely on a
semantic description of each class. A typical ZSL model learns a mapping
between the visual samples of seen classes and the corresponding semantic
descriptions, in order to do the same on unseen classes at test time. State of
the art approaches rely on generative models that synthesize visual features
from the prototype of a class, such that a classifier can then be learned in a
supervised manner. However, these approaches are usually biased towards seen
classes whose visual instances are the only one that can be matched to a given
class prototype. We propose a regularization method that can be applied to any
conditional generative-based ZSL method, by leveraging only the semantic class
prototypes. It learns to synthesize discriminative features for possible
semantic description that are not available at training time, that is the
unseen ones. The approach is evaluated for ZSL and GZSL on four datasets
commonly used in the literature, either in inductive and transductive settings,
with results on-par or above state of the art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">POCO: Point Convolution for Surface Reconstruction. (arXiv:2201.01831v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01831">
<div class="article-summary-box-inner">
<span><p>Implicit neural networks have been successfully used for surface
reconstruction from point clouds. However, many of them face scalability issues
as they encode the isosurface function of a whole object or scene into a single
latent vector. To overcome this limitation, a few approaches infer latent
vectors on a coarse regular 3D grid or on 3D patches, and interpolate them to
answer occupancy queries. In doing so, they loose the direct connection with
the input points sampled on the surface of objects, and they attach information
uniformly in space rather than where it matters the most, i.e., near the
surface. Besides, relying on fixed patch sizes may require discretization
tuning. To address these issues, we propose to use point cloud convolutions and
compute latent vectors at each input point. We then perform a learning-based
interpolation on nearest neighbors using inferred weights. Experiments on both
object and scene datasets show that our approach significantly outperforms
other methods on most classical metrics, producing finer details and better
reconstructing thinner volumes. The code is available at
https://github.com/valeoai/POCO.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiple Sclerosis Lesions Segmentation using Attention-Based CNNs in FLAIR Images. (arXiv:2201.01832v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01832">
<div class="article-summary-box-inner">
<span><p>Objective: Multiple Sclerosis (MS) is an autoimmune, and demyelinating
disease that leads to lesions in the central nervous system. This disease can
be tracked and diagnosed using Magnetic Resonance Imaging (MRI). Up to now a
multitude of multimodality automatic biomedical approaches is used to segment
lesions which are not beneficial for patients in terms of cost, time, and
usability. The authors of the present paper propose a method employing just one
modality (FLAIR image) to segment MS lesions accurately. Methods: A patch-based
Convolutional Neural Network (CNN) is designed, inspired by 3D-ResNet and
spatial-channel attention module, to segment MS lesions. The proposed method
consists of three stages: (1) the contrast-limited adaptive histogram
equalization (CLAHE) is applied to the original images and concatenated to the
extracted edges in order to create 4D images; (2) the patches of size 80 * 80 *
80 * 2 are randomly selected from the 4D images; and (3) the extracted patches
are passed into an attention-based CNN which is used to segment the lesions.
Finally, the proposed method was compared to previous studies of the same
dataset. Results: The current study evaluates the model, with a test set of
ISIB challenge data. Experimental results illustrate that the proposed approach
significantly surpasses existing methods in terms of Dice similarity and
Absolute Volume Difference while the proposed method use just one modality
(FLAIR) to segment the lesions. Conclusions: The authors have introduced an
automated approach to segment the lesions which is based on, at most, two
modalities as an input. The proposed architecture is composed of convolution,
deconvolution, and an SCA-VoxRes module as an attention module. The results
show, the proposed method outperforms well compare to other methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lumbar Bone Mineral Density Estimation from Chest X-ray Images: Anatomy-aware Attentive Multi-ROI Modeling. (arXiv:2201.01838v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01838">
<div class="article-summary-box-inner">
<span><p>Osteoporosis is a common chronic metabolic bone disease that is often
under-diagnosed and under-treated due to the limited access to bone mineral
density (BMD) examinations, e.g. via Dual-energy X-ray Absorptiometry (DXA). In
this paper, we propose a method to predict BMD from Chest X-ray (CXR), one of
the most commonly accessible and low-cost medical imaging examinations. Our
method first automatically detects Regions of Interest (ROIs) of local and
global bone structures from the CXR. Then a multi-ROI deep model with
transformer encoder is developed to exploit both local and global information
in the chest X-ray image for accurate BMD estimation. Our method is evaluated
on 13719 CXR patient cases with their ground truth BMD scores measured by
gold-standard DXA. The model predicted BMD has a strong correlation with the
ground truth (Pearson correlation coefficient 0.889 on lumbar 1). When applied
for osteoporosis screening, it achieves a high classification performance (AUC
0.963 on lumbar 1). As the first effort in the field using CXR scans to predict
the BMD, the proposed algorithm holds strong potential in early osteoporosis
screening and public health promotion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Real-World Adversarial Robustness of Real-Time Semantic Segmentation Models for Autonomous Driving. (arXiv:2201.01850v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01850">
<div class="article-summary-box-inner">
<span><p>The existence of real-world adversarial examples (commonly in the form of
patches) poses a serious threat for the use of deep learning models in
safety-critical computer vision tasks such as visual perception in autonomous
driving. This paper presents an extensive evaluation of the robustness of
semantic segmentation models when attacked with different types of adversarial
patches, including digital, simulated, and physical ones. A novel loss function
is proposed to improve the capabilities of attackers in inducing a
misclassification of pixels. Also, a novel attack strategy is presented to
improve the Expectation Over Transformation method for placing a patch in the
scene. Finally, a state-of-the-art method for detecting adversarial patch is
first extended to cope with semantic segmentation models, then improved to
obtain real-time performance, and eventually evaluated in real-world scenarios.
Experimental results reveal that, even though the adversarial effect is visible
with both digital and real-world attacks, its impact is often spatially
confined to areas of the image around the patch. This opens to further
questions about the spatial robustness of real-time semantic segmentation
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Grid Redundant Bounding Box Annotation for Accurate Object Detection. (arXiv:2201.01857v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01857">
<div class="article-summary-box-inner">
<span><p>Modern leading object detectors are either two-stage or one-stage networks
repurposed from a deep CNN-based backbone classifier network. YOLOv3 is one
such very-well known state-of-the-art one-shot detector that takes in an input
image and divides it into an equal-sized grid matrix. The grid cell having the
center of an object is the one responsible for detecting the particular object.
This paper presents a new mathematical approach that assigns multiple grids per
object for accurately tight-fit bounding box prediction. We also propose an
effective offline copy-paste data augmentation for object detection. Our
proposed method significantly outperforms some current state-of-the-art object
detectors with a prospect for further better performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards realistic symmetry-based completion of previously unseen point clouds. (arXiv:2201.01858v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01858">
<div class="article-summary-box-inner">
<span><p>3D scanning is a complex multistage process that generates a point cloud of
an object typically containing damaged parts due to occlusions, reflections,
shadows, scanner motion, specific properties of the object surface, imperfect
reconstruction algorithms, etc. Point cloud completion is specifically designed
to fill in the missing parts of the object and obtain its high-quality 3D
representation. The existing completion approaches perform well on the academic
datasets with a predefined set of object classes and very specific types of
defects; however, their performance drops significantly in the real-world
settings and degrades even further on previously unseen object classes.
</p>
<p>We propose a novel framework that performs well on symmetric objects, which
are ubiquitous in man-made environments. Unlike learning-based approaches, the
proposed framework does not require training data and is capable of completing
non-critical damages occurring in customer 3D scanning process using e.g.
Kinect, time-of-flight, or structured light scanners. With thorough
experiments, we demonstrate that the proposed framework achieves
state-of-the-art efficiency in point cloud completion of real-world customer
scans. We benchmark the framework performance on two types of datasets:
properly augmented existing academic dataset and the actual 3D scans of various
objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepMLS: Geometry-Aware Control Point Deformation. (arXiv:2201.01873v1 [cs.GR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01873">
<div class="article-summary-box-inner">
<span><p>We introduce DeepMLS, a space-based deformation technique, guided by a set of
displaced control points. We leverage the power of neural networks to inject
the underlying shape geometry into the deformation parameters. The goal of our
technique is to enable a realistic and intuitive shape deformation. Our method
is built upon moving least-squares (MLS), since it minimizes a weighted sum of
the given control point displacements. Traditionally, the influence of each
control point on every point in space (i.e., the weighting function) is defined
using inverse distance heuristics. In this work, we opt to learn the weighting
function, by training a neural network on the control points from a single
input shape, and exploit the innate smoothness of neural networks. Our
geometry-aware control point deformation is agnostic to the surface
representation and quality; it can be applied to point clouds or meshes,
including non-manifold and disconnected surface soups. We show that our
technique facilitates intuitive piecewise smooth deformations, which are well
suited for manufactured objects. We show the advantages of our approach
compared to existing surface and space-based deformation techniques, both
quantitatively and qualitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Memory-guided Image De-raining Using Time-Lapse Data. (arXiv:2201.01883v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01883">
<div class="article-summary-box-inner">
<span><p>This paper addresses the problem of single image de-raining, that is, the
task of recovering clean and rain-free background scenes from a single image
obscured by a rainy artifact. Although recent advances adopt real-world
time-lapse data to overcome the need for paired rain-clean images, they are
limited to fully exploit the time-lapse data. The main cause is that, in terms
of network architectures, they could not capture long-term rain streak
information in the time-lapse data during training owing to the lack of memory
components. To address this problem, we propose a novel network architecture
based on a memory network that explicitly helps to capture long-term rain
streak information in the time-lapse data. Our network comprises the
encoder-decoder networks and a memory network. The features extracted from the
encoder are read and updated in the memory network that contains several memory
items to store rain streak-aware feature representations. With the read/update
operation, the memory network retrieves relevant memory items in terms of the
queries, enabling the memory items to represent the various rain streaks
included in the time-lapse data. To boost the discriminative power of memory
features, we also present a novel background selective whitening (BSW) loss for
capturing only rain streak information in the memory network by erasing the
background information. Experimental results on standard benchmarks demonstrate
the effectiveness and superiority of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Flow-Guided Sparse Transformer for Video Deblurring. (arXiv:2201.01893v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01893">
<div class="article-summary-box-inner">
<span><p>Exploiting similar and sharper scene patches in spatio-temporal neighborhoods
is critical for video deblurring. However, CNN-based methods show limitations
in capturing long-range dependencies and modeling non-local self-similarity. In
this paper, we propose a novel framework, Flow-Guided Sparse Transformer
(FGST), for video deblurring. In FGST, we customize a self-attention module,
Flow-Guided Sparse Window-based Multi-head Self-Attention (FGSW-MSA). For each
$query$ element on the blurry reference frame, FGSW-MSA enjoys the guidance of
the estimated optical flow to globally sample spatially sparse yet highly
related $key$ elements corresponding to the same scene patch in neighboring
frames. Besides, we present a Recurrent Embedding (RE) mechanism to transfer
information from past frames and strengthen long-range temporal dependencies.
Comprehensive experiments demonstrate that our proposed FGST outperforms
state-of-the-art (SOTA) methods on both DVD and GOPRO datasets and even yields
more visually pleasing results in real video deblurring. Code and models will
be released to the public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incremental Object Grounding Using Scene Graphs. (arXiv:2201.01901v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01901">
<div class="article-summary-box-inner">
<span><p>Object grounding tasks aim to locate the target object in an image through
verbal communications. Understanding human command is an important process
needed for effective human-robot communication. However, this is challenging
because human commands can be ambiguous and erroneous. This paper aims to
disambiguate the human's referring expressions by allowing the agent to ask
relevant questions based on semantic data obtained from scene graphs. We test
if our agent can use relations between objects from a scene graph to ask
semantically relevant questions that can disambiguate the original user
command. In this paper, we present Incremental Grounding using Scene Graphs
(IGSG), a disambiguation model that uses semantic data from an image scene
graph and linguistic structures from a language scene graph to ground objects
based on human command. Compared to the baseline, IGSG shows promising results
in complex real-world scenes where there are multiple identical target objects.
IGSG can effectively disambiguate ambiguous or wrong referring expressions by
asking disambiguating questions back to the user.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Neighborhood Alignment. (arXiv:2201.01922v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01922">
<div class="article-summary-box-inner">
<span><p>We present Contrastive Neighborhood Alignment (CNA), a manifold learning
approach to maintain the topology of learned features whereby data points that
are mapped to nearby representations by the source (teacher) model are also
mapped to neighbors by the target (student) model. The target model aims to
mimic the local structure of the source representation space using a
contrastive loss. CNA is an unsupervised learning algorithm that does not
require ground-truth labels for the individual samples. CNA is illustrated in
three scenarios: manifold learning, where the model maintains the local
topology of the original data in a dimension-reduced space; model distillation,
where a small student model is trained to mimic a larger teacher; and legacy
model update, where an older model is replaced by a more powerful one.
Experiments show that CNA is able to capture the manifold in a high-dimensional
space and improves performance compared to the competing methods in their
domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Egocentric Deep Multi-Channel Audio-Visual Active Speaker Localization. (arXiv:2201.01928v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01928">
<div class="article-summary-box-inner">
<span><p>Augmented reality devices have the potential to enhance human perception and
enable other assistive functionalities in complex conversational environments.
Effectively capturing the audio-visual context necessary for understanding
these social interactions first requires detecting and localizing the voice
activities of the device wearer and the surrounding people. These tasks are
challenging due to their egocentric nature: the wearer's head motion may cause
motion blur, surrounding people may appear in difficult viewing angles, and
there may be occlusions, visual clutter, audio noise, and bad lighting. Under
these conditions, previous state-of-the-art active speaker detection methods do
not give satisfactory results. Instead, we tackle the problem from a new
setting using both video and multi-channel microphone array audio. We propose a
novel end-to-end deep learning approach that is able to give robust voice
activity detection and localization results. In contrast to previous methods,
our method localizes active speakers from all possible directions on the
sphere, even outside the camera's field of view, while simultaneously detecting
the device wearer's own voice activity. Our experiments show that the proposed
method gives superior results, can run in real time, and is robust against
noise and clutter.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decompose to Adapt: Cross-domain Object Detection via Feature Disentanglement. (arXiv:2201.01929v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01929">
<div class="article-summary-box-inner">
<span><p>Recent advances in unsupervised domain adaptation (UDA) techniques have
witnessed great success in cross-domain computer vision tasks, enhancing the
generalization ability of data-driven deep learning architectures by bridging
the domain distribution gaps. For the UDA-based cross-domain object detection
methods, the majority of them alleviate the domain bias by inducing the
domain-invariant feature generation via adversarial learning strategy. However,
their domain discriminators have limited classification ability due to the
unstable adversarial training process. Therefore, the extracted features
induced by them cannot be perfectly domain-invariant and still contain
domain-private factors, bringing obstacles to further alleviate the
cross-domain discrepancy. To tackle this issue, we design a Domain
Disentanglement Faster-RCNN (DDF) to eliminate the source-specific information
in the features for detection task learning. Our DDF method facilitates the
feature disentanglement at the global and local stages, with a Global Triplet
Disentanglement (GTD) module and an Instance Similarity Disentanglement (ISD)
module, respectively. By outperforming state-of-the-art methods on four
benchmark UDA object detection tasks, our DDF method is demonstrated to be
effective with wide applicability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Aerial Scene Parsing: From Tile-level Scene Classification to Pixel-wise Semantic Labeling. (arXiv:2201.01953v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01953">
<div class="article-summary-box-inner">
<span><p>Given an aerial image, aerial scene parsing (ASP) targets to interpret the
semantic structure of the image content, e.g., by assigning a semantic label to
every pixel of the image. With the popularization of data-driven methods, the
past decades have witnessed promising progress on ASP by approaching the
problem with the schemes of tile-level scene classification or
segmentation-based image analysis, when using high-resolution aerial images.
However, the former scheme often produces results with tile-wise boundaries,
while the latter one needs to handle the complex modeling process from pixels
to semantics, which often requires large-scale and well-annotated image samples
with pixel-wise semantic labels. In this paper, we address these issues in ASP,
with perspectives from tile-level scene classification to pixel-wise semantic
labeling. Specifically, we first revisit aerial image interpretation by a
literature review. We then present a large-scale scene classification dataset
that contains one million aerial images termed Million-AID. With the presented
dataset, we also report benchmarking experiments using classical convolutional
neural networks (CNNs). Finally, we perform ASP by unifying the tile-level
scene classification and object-based image analysis to achieve pixel-wise
semantic labeling. Intensive experiments show that Million-AID is a challenging
yet useful dataset, which can serve as a benchmark for evaluating newly
developed algorithms. When transferring knowledge from Million-AID, fine-tuning
CNN models pretrained on Million-AID perform consistently better than those
pretrained ImageNet for aerial scene classification. Moreover, our designed
hierarchical multi-task learning method achieves the state-of-the-art
pixel-wise classification on the challenging GID, bridging the tile-level scene
classification toward pixel-wise semantic labeling for aerial image
interpretation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Balancing Generalization and Specialization in Zero-shot Learning. (arXiv:2201.01961v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01961">
<div class="article-summary-box-inner">
<span><p>Zero-Shot Learning (ZSL) aims to transfer classification capability from seen
to unseen classes. Recent methods have proved that generalization and
specialization are two essential abilities to achieve good performance in ZSL.
However, they all focus on only one of the abilities, resulting in models that
are either too general with the degraded classifying ability or too specialized
to generalize to unseen classes. In this paper, we propose an end-to-end
network with balanced generalization and specialization abilities, termed as
BGSNet, to take advantage of both abilities, and balance them at instance- and
dataset-level. Specifically, BGSNet consists of two branches: the
Generalization Network (GNet), which applies episodic meta-learning to learn
generalized knowledge, and the Balanced Specialization Network (BSNet), which
adopts multiple attentive extractors to extract discriminative features and
fulfill the instance-level balance. A novel self-adjusting diversity loss is
designed to optimize BSNet with less redundancy and more diversity. We further
propose a differentiable dataset-level balance and update the weights in a
linear annealing schedule to simulate network pruning and thus obtain the
optimal structure for BSNet at a low cost with dataset-level balance achieved.
Experiments on four benchmark datasets demonstrate our model's effectiveness.
Sufficient component ablations prove the necessity of integrating
generalization and specialization abilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Label Classification on Remote-Sensing Images. (arXiv:2201.01971v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01971">
<div class="article-summary-box-inner">
<span><p>Acquiring information on large areas on the earth's surface through satellite
cameras allows us to see much more than we can see while standing on the
ground. This assists us in detecting and monitoring the physical
characteristics of an area like land-use patterns, atmospheric conditions,
forest cover, and many unlisted aspects. The obtained images not only keep
track of continuous natural phenomena but are also crucial in tackling the
global challenge of severe deforestation. Among which Amazon basin accounts for
the largest share every year. Proper data analysis would help limit detrimental
effects on the ecosystem and biodiversity with a sustainable healthy
atmosphere. This report aims to label the satellite image chips of the Amazon
rainforest with atmospheric and various classes of land cover or land use
through different machine learning and superior deep learning models.
Evaluation is done based on the F2 metric, while for loss function, we have
both sigmoid cross-entropy as well as softmax cross-entropy. Images are fed
indirectly to the machine learning classifiers after only features are
extracted using pre-trained ImageNet architectures. Whereas for deep learning
models, ensembles of fine-tuned ImageNet pre-trained models are used via
transfer learning. Our best score was achieved so far with the F2 metric is
0.927.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SASA: Semantics-Augmented Set Abstraction for Point-based 3D Object Detection. (arXiv:2201.01976v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01976">
<div class="article-summary-box-inner">
<span><p>Although point-based networks are demonstrated to be accurate for 3D point
cloud modeling, they are still falling behind their voxel-based competitors in
3D detection. We observe that the prevailing set abstraction design for
down-sampling points may maintain too much unimportant background information
that can affect feature learning for detecting objects. To tackle this issue,
we propose a novel set abstraction method named Semantics-Augmented Set
Abstraction (SASA). Technically, we first add a binary segmentation module as
the side output to help identify foreground points. Based on the estimated
point-wise foreground scores, we then propose a semantics-guided point sampling
algorithm to help retain more important foreground points during down-sampling.
In practice, SASA shows to be effective in identifying valuable points related
to foreground objects and improving feature learning for point-based 3D
detection. Additionally, it is an easy-to-plug-in module and able to boost
various point-based detectors, including single-stage and two-stage ones.
Extensive experiments on the popular KITTI and nuScenes datasets validate the
superiority of SASA, lifting point-based detection models to reach comparable
performance to state-of-the-art voxel-based methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Abstraction-Refinement Approach to Verifying Convolutional Neural Networks. (arXiv:2201.01978v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01978">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks have gained vast popularity due to their
excellent performance in the fields of computer vision, image processing, and
others. Unfortunately, it is now well known that convolutional networks often
produce erroneous results - for example, minor perturbations of the inputs of
these networks can result in severe classification errors. Numerous
verification approaches have been proposed in recent years to prove the absence
of such errors, but these are typically geared for fully connected networks and
suffer from exacerbated scalability issues when applied to convolutional
networks. To address this gap, we present here the Cnn-Abs framework, which is
particularly aimed at the verification of convolutional networks. The core of
Cnn-Abs is an abstraction-refinement technique, which simplifies the
verification problem through the removal of convolutional connections in a way
that soundly creates an over-approximation of the original problem; and which
restores these connections if the resulting problem becomes too abstract.
Cnn-Abs is designed to use existing verification engines as a backend, and our
evaluation demonstrates that it can significantly boost the performance of a
state-of-the-art DNN verification engine, reducing runtime by 15.7% on average.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Domain Joint Training for Person Re-Identification. (arXiv:2201.01983v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01983">
<div class="article-summary-box-inner">
<span><p>Deep learning-based person Re-IDentification (ReID) often requires a large
amount of training data to achieve good performance. Thus it appears that
collecting more training data from diverse environments tends to improve the
ReID performance. This paper re-examines this common belief and makes a somehow
surprising observation: using more samples, i.e., training with samples from
multiple datasets, does not necessarily lead to better performance by using the
popular ReID models. In some cases, training with more samples may even hurt
the performance of the evaluation is carried out in one of those datasets. We
postulate that this phenomenon is due to the incapability of the standard
network in adapting to diverse environments. To overcome this issue, we propose
an approach called Domain-Camera-Sample Dynamic network (DCSD) whose parameters
can be adaptive to various factors. Specifically, we consider the internal
domain-related factor that can be identified from the input features, and
external domain-related factors, such as domain information or camera
information. Our discovery is that training with such an adaptive model can
better benefit from more training samples. Experimental results show that our
DCSD can greatly boost the performance (up to 12.3%) while joint training in
multiple datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compact Bidirectional Transformer for Image Captioning. (arXiv:2201.01984v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01984">
<div class="article-summary-box-inner">
<span><p>Most current image captioning models typically generate captions from left to
right. This unidirectional property makes them can only leverage past context
but not future context. Though recent refinement-based models can exploit both
past and future context by generating a new caption in the second stage based
on pre-retrieved or pre-generated captions in the first stage, the decoder of
these models generally consists of two networks~(i.e. a retriever or captioner
in the first stage and a refiner in the second stage), which can only be
executed sequentially. In this paper, we introduce a Compact Bidirectional
Transformer model for image captioning that can leverage bidirectional context
implicitly and explicitly while the decoder can be executed parallelly.
Specifically, it is implemented by tightly coupling left-to-right(L2R) and
right-to-left(R2L) flows into a single compact model~(i.e. implicitly) and
optionally allowing interaction of the two flows(i.e. explicitly), while the
final caption is chosen from either L2R or R2L flow in a sentence-level
ensemble manner. We conduct extensive ablation studies on the MSCOCO benchmark
and find that the compact architecture, which serves as a regularization for
implicitly exploiting bidirectional context, and the sentence-level ensemble
play more important roles than the explicit interaction mechanism. By combining
with word-level ensemble seamlessly, the effect of the sentence-level ensemble
is further enlarged. We further extend the conventional one-flow self-critical
training to the two-flows version under this architecture and achieve new
state-of-the-art results in comparison with non-vision-language-pretraining
models. Source code is available at
{\color{magenta}\url{https://github.com/YuanEZhou/CBTrans}}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransVPR: Transformer-based place recognition with multi-level attention aggregation. (arXiv:2201.02001v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02001">
<div class="article-summary-box-inner">
<span><p>Visual place recognition is a challenging task for applications such as
autonomous driving navigation and mobile robot localization. Distracting
elements presenting in complex scenes often lead to deviations in the
perception of visual place. To address this problem, it is crucial to integrate
information from only task-relevant regions into image representations. In this
paper, we introduce a novel holistic place recognition model, TransVPR, based
on vision Transformers. It benefits from the desirable property of the
self-attention operation in Transformers which can naturally aggregate
task-relevant features. Attentions from multiple levels of the Transformer,
which focus on different regions of interest, are further combined to generate
a global image representation. In addition, the output tokens from Transformer
layers filtered by the fused attention mask are considered as key-patch
descriptors, which are used to perform spatial matching to re-rank the
candidates retrieved by the global image features. The whole model allows
end-to-end training with a single objective and image-level supervision.
TransVPR achieves state-of-the-art performance on several real-world benchmarks
while maintaining low computational time and storage requirements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Training Vision Language BERTs with a Unified Conditional Model. (arXiv:2201.02010v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02010">
<div class="article-summary-box-inner">
<span><p>Natural language BERTs are trained with language corpus in a self-supervised
manner. Unlike natural language BERTs, vision language BERTs need paired data
to train, which restricts the scale of VL-BERT pretraining. We propose a
self-training approach that allows training VL-BERTs from unlabeled image data.
The proposed method starts with our unified conditional model -- a vision
language BERT model that can perform zero-shot conditional generation. Given
different conditions, the unified conditional model can generate captions,
dense captions, and even questions. We use the labeled image data to train a
teacher model and use the trained model to generate pseudo captions on
unlabeled image data. We then combine the labeled data and pseudo labeled data
to train a student model. The process is iterated by putting the student model
as a new teacher. By using the proposed self-training approach and only 300k
unlabeled extra data, we are able to get competitive or even better
performances compared to the models of similar model size trained with 3
million extra image data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An unambiguous cloudiness index for nonwovens. (arXiv:2201.02011v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02011">
<div class="article-summary-box-inner">
<span><p>Cloudiness or formation is a concept routinely used in industry to address
deviations from homogeneity in nonwovens and papers. Measuring a cloudiness
index based on image data is a common task in industrial quality assurance. The
two most popular ways of quantifying cloudiness are based on power spectrum or
correlation function on the one hand or the Laplacian pyramid on the other
hand. Here, we recall the mathematical basis of the first approach
comprehensively, derive a cloudiness index, and demonstrate its practical
estimation. We prove that the Laplacian pyramid as well as other quantities
characterizing cloudiness like the range of interaction and the intensity of
small-angle scattering are very closely related to the power spectrum. Finally,
we show that the power spectrum is easy to be measured image analytically and
carries more information than the alternatives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Egocentric 3D Pose Estimation with Third Person Views. (arXiv:2201.02017v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02017">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel approach to enhance the 3D body pose
estimation of a person computed from videos captured from a single wearable
camera. The key idea is to leverage high-level features linking first- and
third-views in a joint embedding space. To learn such embedding space we
introduce First2Third-Pose, a new paired synchronized dataset of nearly 2,000
videos depicting human activities captured from both first- and third-view
perspectives. We explicitly consider spatial- and motion-domain features,
combined using a semi-Siamese architecture trained in a self-supervised
fashion. Experimental results demonstrate that the joint multi-view embedded
space learned with our dataset is useful to extract discriminatory features
from arbitrary single-view egocentric videos, without needing domain adaptation
or knowledge of camera parameters. We achieve significant improvement of
egocentric 3D body pose estimation performance on two unconstrained datasets,
over three supervised state-of-the-art approaches. Our dataset and code will be
available for research purposes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Light in the Dark: Deep Learning Practices for Industrial Computer Vision. (arXiv:2201.02028v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02028">
<div class="article-summary-box-inner">
<span><p>In recent years, large pre-trained deep neural networks (DNNs) have
revolutionized the field of computer vision (CV). Although these DNNs have been
shown to be very well suited for general image recognition tasks, application
in industry is often precluded for three reasons: 1) large pre-trained DNNs are
built on hundreds of millions of parameters, making deployment on many devices
impossible, 2) the underlying dataset for pre-training consists of general
objects, while industrial cases often consist of very specific objects, such as
structures on solar wafers, 3) potentially biased pre-trained DNNs raise legal
issues for companies. As a remedy, we study neural networks for CV that we
train from scratch. For this purpose, we use a real-world case from a solar
wafer manufacturer. We find that our neural networks achieve similar
performances as pre-trained DNNs, even though they consist of far fewer
parameters and do not rely on third-party datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unified Framework for Attention-Based Few-Shot Object Detection. (arXiv:2201.02052v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02052">
<div class="article-summary-box-inner">
<span><p>Few-Shot Object Detection (FSOD) is a rapidly growing field in computer
vision. It consists in finding all occurrences of a given set of classes with
only a few annotated examples for each class. Numerous methods have been
proposed to address this challenge and most of them are based on attention
mechanisms. However, the great variety of classic object detection frameworks
and training strategies makes performance comparison between methods difficult.
In particular, for attention-based FSOD methods, it is laborious to compare the
impact of the different attention mechanisms on performance. This paper aims at
filling this shortcoming. To do so, a flexible framework is proposed to allow
the implementation of most of the attention techniques available in the
literature. To properly introduce such a framework, a detailed review of the
existing FSOD methods is firstly provided. Some different attention mechanisms
are then reimplemented within the framework and compared with all other
parameters fixed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GLAN: A Graph-based Linear Assignment Network. (arXiv:2201.02057v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02057">
<div class="article-summary-box-inner">
<span><p>Differentiable solvers for the linear assignment problem (LAP) have attracted
much research attention in recent years, which are usually embedded into
learning frameworks as components. However, previous algorithms, with or
without learning strategies, usually suffer from the degradation of the
optimality with the increment of the problem size. In this paper, we propose a
learnable linear assignment solver based on deep graph networks. Specifically,
we first transform the cost matrix to a bipartite graph and convert the
assignment task to the problem of selecting reliable edges from the constructed
graph. Subsequently, a deep graph network is developed to aggregate and update
the features of nodes and edges. Finally, the network predicts a label for each
edge that indicates the assignment relationship. The experimental results on a
synthetic dataset reveal that our method outperforms state-of-the-art baselines
and achieves consistently high accuracy with the increment of the problem size.
Furthermore, we also embed the proposed solver, in comparison with
state-of-the-art baseline solvers, into a popular multi-object tracking (MOT)
framework to train the tracker in an end-to-end manner. The experimental
results on MOT benchmarks illustrate that the proposed LAP solver improves the
tracker by the largest margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ASL-Skeleton3D and ASL-Phono: Two Novel Datasets for the American Sign Language. (arXiv:2201.02065v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02065">
<div class="article-summary-box-inner">
<span><p>Sign language is an essential resource enabling access to communication and
proper socioemotional development for individuals suffering from disabling
hearing loss. As this population is expected to reach 700 million by 2050, the
importance of the language becomes even more essential as it plays a critical
role to ensure the inclusion of such individuals in society. The Sign Language
Recognition field aims to bridge the gap between users and non-users of sign
languages. However, the scarcity in quantity and quality of datasets is one of
the main challenges limiting the exploration of novel approaches that could
lead to significant advancements in this research area. Thus, this paper
contributes by introducing two new datasets for the American Sign Language: the
first is composed of the three-dimensional representation of the signers and,
the second, by an unprecedented linguistics-based representation containing a
set of phonological attributes of the signs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EM-driven unsupervised learning for efficient motion segmentation. (arXiv:2201.02074v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02074">
<div class="article-summary-box-inner">
<span><p>This paper presents a CNN-based fully unsupervised method for motion
segmentation from optical flow. We assume that the input optical flow can be
represented as a piecewise set of parametric motion models, typically, affine
or quadratic motion models.The core idea of this work is to leverage the
Expectation-Maximization (EM) framework. It enables us to design in a
well-founded manner the loss function and the training procedure of our motion
segmentation neural network. However, in contrast to the classical iterative
EM, once the network is trained, we can provide a segmentation for any unseen
optical flow field in a single inference step, with no dependence on the
initialization of the motion model parameters since they are not estimated in
the inference stage. Different loss functions have been investigated including
robust ones. We also propose a novel data augmentation technique on the optical
flow field with a noticeable impact on the performance. We tested our motion
segmentation network on the DAVIS2016 dataset. Our method outperforms
comparable unsupervised methods and is very efficient. Indeed, it can run at
125fps making it usable for real-time applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning Based Classification System For Recognizing Local Spinach. (arXiv:2201.02093v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02093">
<div class="article-summary-box-inner">
<span><p>A deep learning model gives an incredible result for image processing by
studying from the trained dataset. Spinach is a leaf vegetable that contains
vitamins and nutrients. In our research, a Deep learning method has been used
that can automatically identify spinach and this method has a dataset of a
total of five species of spinach that contains 3785 images. Four Convolutional
Neural Network (CNN) models were used to classify our spinach. These models
give more accurate results for image classification. Before applying these
models there is some preprocessing of the image data. For the preprocessing of
data, some methods need to happen. Those are RGB conversion, filtering, resize
&amp; rescaling, and categorization. After applying these methods image data are
pre-processed and ready to be used in the classifier algorithms. The accuracy
of these classifiers is in between 98.68% - 99.79%. Among those models, VGG16
achieved the highest accuracy of 99.79%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HyperionSolarNet: Solar Panel Detection from Aerial Images. (arXiv:2201.02107v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02107">
<div class="article-summary-box-inner">
<span><p>With the effects of global climate change impacting the world, collective
efforts are needed to reduce greenhouse gas emissions. The energy sector is the
single largest contributor to climate change and many efforts are focused on
reducing dependence on carbon-emitting power plants and moving to renewable
energy sources, such as solar power. A comprehensive database of the location
of solar panels is important to assist analysts and policymakers in defining
strategies for further expansion of solar energy. In this paper we focus on
creating a world map of solar panels. We identify locations and total surface
area of solar panels within a given geographic area. We use deep learning
methods for automated detection of solar panel locations and their surface area
using aerial imagery. The framework, which consists of a two-branch model using
an image classifier in tandem with a semantic segmentation model, is trained on
our created dataset of satellite images. Our work provides an efficient and
scalable method for detecting solar panels, achieving an accuracy of 0.96 for
classification and an IoU score of 0.82 for segmentation performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Eye Know You Too: A DenseNet Architecture for End-to-end Biometric Authentication via Eye Movements. (arXiv:2201.02110v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02110">
<div class="article-summary-box-inner">
<span><p>Plain convolutional neural networks (CNNs) have been used to achieve
state-of-the-art performance in various domains in the past years, including
biometric authentication via eye movements. There have been many relatively
recent improvements to plain CNNs, including residual networks (ResNets) and
densely connected convolutional networks (DenseNets). Although these networks
primarily target image processing domains, they can be easily modified to work
with time series data. We employ a DenseNet architecture for end-to-end
biometric authentication via eye movements. We compare our model against the
most relevant prior works including the current state-of-the-art. We find that
our model achieves state-of-the-art performance for all considered training
conditions and data sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bio-inspired Min-Nets Improve the Performance and Robustness of Deep Networks. (arXiv:2201.02149v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02149">
<div class="article-summary-box-inner">
<span><p>Min-Nets are inspired by end-stopped cortical cells with units that output
the minimum of two learned filters. We insert such Min-units into
state-of-the-art deep networks, such as the popular ResNet and DenseNet, and
show that the resulting Min-Nets perform better on the Cifar-10 benchmark.
Moreover, we show that Min-Nets are more robust against JPEG compression
artifacts. We argue that the minimum operation is the simplest way of
implementing an AND operation on pairs of filters and that such AND operations
introduce a bias that is appropriate given the statistics of natural images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction. (arXiv:2201.02184v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02184">
<div class="article-summary-box-inner">
<span><p>Video recordings of speech contain correlated audio and visual information,
providing a strong signal for speech representation learning from the speaker's
lip movements and the produced sound. We introduce Audio-Visual Hidden Unit
BERT (AV-HuBERT), a self-supervised representation learning framework for
audio-visual speech, which masks multi-stream video input and predicts
automatically discovered and iteratively refined multimodal hidden units.
AV-HuBERT learns powerful audio-visual speech representation benefiting both
lip-reading and automatic speech recognition. On the largest public lip-reading
benchmark LRS3 (433 hours), AV-HuBERT achieves 32.5% WER with only 30 hours of
labeled data, outperforming the former state-of-the-art approach (33.6%)
trained with a thousand times more transcribed video data (31K hours). The
lip-reading WER is further reduced to 26.9% when using all 433 hours of labeled
data from LRS3 and combined with self-training. Using our audio-visual
representation on the same benchmark for audio-only speech recognition leads to
a 40% relative WER reduction over the state-of-the-art performance (1.3% vs
2.3%). Our code and models are available at
https://github.com/facebookresearch/av_hubert
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Realistic Full-Body Anonymization with Surface-Guided GANs. (arXiv:2201.02193v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02193">
<div class="article-summary-box-inner">
<span><p>Recent work on image anonymization has shown that generative adversarial
networks (GANs) can generate near-photorealistic faces to anonymize
individuals. However, scaling these networks to the entire human body has
remained a challenging and yet unsolved task. We propose a new anonymization
method that generates close-to-photorealistic humans for in-the-wild images.A
key part of our design is to guide adversarial nets by dense pixel-to-surface
correspondences between an image and a canonical 3D surface.We introduce
Variational Surface-Adaptive Modulation (V-SAM) that embeds surface information
throughout the generator.Combining this with our novel discriminator surface
supervision loss, the generator can synthesize high quality humans with diverse
appearance in complex and varying scenes.We show that surface guidance
significantly improves image quality and diversity of samples, yielding a
highly practical generator.Finally, we demonstrate that surface-guided
anonymization preserves the usability of data for future computer vision
development
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Metric Structured Learning For Facial Expression Recognition. (arXiv:2001.06612v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2001.06612">
<div class="article-summary-box-inner">
<span><p>We propose a deep metric learning model to create embedded sub-spaces with a
well defined structure. A new loss function that imposes Gaussian structures on
the output space is introduced to create these sub-spaces thus shaping the
distribution of the data. Having a mixture of Gaussians solution space is
advantageous given its simplified and well established structure. It allows
fast discovering of classes within classes and the identification of mean
representatives at the centroids of individual classes. We also propose a new
semi-supervised method to create sub-classes. We illustrate our methods on the
facial expression recognition problem and validate results on the FER+,
AffectNet, Extended Cohn-Kanade (CK+), BU-3DFE, and JAFFE datasets. We
experimentally demonstrate that the learned embedding can be successfully used
for various applications including expression retrieval and emotion
recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Increased-confidence adversarial examples for deep learning counter-forensics. (arXiv:2005.06023v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.06023">
<div class="article-summary-box-inner">
<span><p>Transferability of adversarial examples is a key issue to apply this kind of
attacks against multimedia forensics (MMF) techniques based on Deep Learning
(DL) in a real-life setting. Adversarial example transferability, in fact,
would open the way to the deployment of successful counter forensics attacks
also in cases where the attacker does not have a full knowledge of the
to-be-attacked system. Some preliminary works have shown that adversarial
examples against CNN-based image forensics detectors are in general
non-transferrable, at least when the basic versions of the attacks implemented
in the most popular libraries are adopted. In this paper, we introduce a
general strategy to increase the strength of the attacks and evaluate their
transferability when such a strength varies. We experimentally show that, in
this way, attack transferability can be largely increased, at the expense of a
larger distortion. Our research confirms the security threats posed by the
existence of adversarial examples even in multimedia forensics scenarios, thus
calling for new defense strategies to improve the security of DL-based MMF
techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accelerated Zeroth-Order and First-Order Momentum Methods from Mini to Minimax Optimization. (arXiv:2008.08170v6 [math.OC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.08170">
<div class="article-summary-box-inner">
<span><p>In the paper, we propose a class of accelerated zeroth-order and first-order
momentum methods for both nonconvex mini-optimization and minimax-optimization.
Specifically, we propose a new accelerated zeroth-order momentum (Acc-ZOM)
method for black-box mini-optimization. Moreover, we prove that our Acc-ZOM
method achieves a lower query complexity of $\tilde{O}(d^{3/4}\epsilon^{-3})$
for finding an $\epsilon$-stationary point, which improves the best known
result by a factor of $O(d^{1/4})$ where $d$ denotes the variable dimension. In
particular, the Acc-ZOM does not require large batches required in the existing
zeroth-order stochastic algorithms. Meanwhile, we propose an accelerated
\textbf{zeroth-order} momentum descent ascent (Acc-ZOMDA) method for
\textbf{black-box} minimax-optimization, which obtains a query complexity of
$\tilde{O}((d_1+d_2)^{3/4}\kappa_y^{4.5}\epsilon^{-3})$ without large batches
for finding an $\epsilon$-stationary point, where $d_1$ and $d_2$ denote
variable dimensions and $\kappa_y$ is condition number. Moreover, we propose an
accelerated \textbf{first-order} momentum descent ascent (Acc-MDA) method for
\textbf{white-box} minimax optimization, which has a gradient complexity of
$\tilde{O}(\kappa_y^{4.5}\epsilon^{-3})$ without large batches for finding an
$\epsilon$-stationary point. In particular, our Acc-MDA can obtain a lower
gradient complexity of $\tilde{O}(\kappa_y^{2.5}\epsilon^{-3})$ with a batch
size $O(\kappa_y^4)$. Extensive experimental results on black-box adversarial
attack to deep neural networks and poisoning attack to logistic regression
demonstrate efficiency of our algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Example-based Color Transfer with Gaussian Mixture Modeling. (arXiv:2008.13626v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.13626">
<div class="article-summary-box-inner">
<span><p>Color transfer, which plays a key role in image editing, has attracted
noticeable attention recently. It has remained a challenge to date due to
various issues such as time-consuming manual adjustments and prior segmentation
issues. In this paper, we propose to model color transfer under a probability
framework and cast it as a parameter estimation problem. In particular, we
relate the transferred image with the example image under the Gaussian Mixture
Model (GMM) and regard the transferred image color as the GMM centroids. We
employ the Expectation-Maximization (EM) algorithm (E-step and M-step) for
optimization. To better preserve gradient information, we introduce a Laplacian
based regularization term to the objective function at the M-step which is
solved by deriving a gradient descent algorithm. Given the input of a source
image and an example image, our method is able to generate continuous color
transfer results with increasing EM iterations. Various experiments show that
our approach generally outperforms other competitive color transfer methods,
both visually and quantitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating and Mitigating Bias in Image Classifiers: A Causal Perspective Using Counterfactuals. (arXiv:2009.08270v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.08270">
<div class="article-summary-box-inner">
<span><p>Counterfactual examples for an input -- perturbations that change specific
features but not others -- have been shown to be useful for evaluating bias of
machine learning models, e.g., against specific demographic groups. However,
generating counterfactual examples for images is non-trivial due to the
underlying causal structure on the various features of an image. To be
meaningful, generated perturbations need to satisfy constraints implied by the
causal model. We present a method for generating counterfactuals by
incorporating a structural causal model (SCM) in an improved variant of
Adversarially Learned Inference (ALI), that generates counterfactuals in
accordance with the causal relationships between attributes of an image. Based
on the generated counterfactuals, we show how to explain a pre-trained machine
learning classifier, evaluate its bias, and mitigate the bias using a
counterfactual regularizer. On the Morpho-MNIST dataset, our method generates
counterfactuals comparable in quality to prior work on SCM-based
counterfactuals (DeepSCM), while on the more complex CelebA dataset our method
outperforms DeepSCM in generating high-quality valid counterfactuals. Moreover,
generated counterfactuals are indistinguishable from reconstructed images in a
human evaluation experiment and we subsequently use them to evaluate the
fairness of a standard classifier trained on CelebA data. We show that the
classifier is biased w.r.t. skin and hair color, and how counterfactual
regularization can remove those biases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PV-RCNN++: Point-Voxel Feature Set Abstraction With Local Vector Representation for 3D Object Detection. (arXiv:2102.00463v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.00463">
<div class="article-summary-box-inner">
<span><p>3D object detection is receiving increasing attention from both industry and
academia thanks to its wide applications in various fields. In this paper, we
propose the Point-Voxel Region-based Convolution Neural Networks (PV-RCNNs) for
3D object detection from point clouds. First, we propose a novel 3D detector,
PV-RCNN, which consists of two steps: the voxel-to-keypoint scene encoding and
keypoint-to-grid RoI feature abstraction. These two steps deeply integrate the
3D voxel CNN with the PointNet-based set abstraction for extracting
discriminative features. Second, we propose an advanced framework, PV-RCNN++,
for more efficient and accurate 3D object detection. It consists of two major
improvements: the sectorized proposal-centric strategy for efficiently
producing more representative keypoints, and the VectorPool aggregation for
better aggregating local point features with much less resource consumption.
With these two strategies, our PV-RCNN++ is more than 2x faster than PV-RCNN,
while also achieving better performance on the large-scale Waymo Open Dataset
with 150m * 150m detection range. Also, our proposed PV-RCNNs achieve
state-of-the-art 3D detection performance on both the Waymo Open Dataset and
the highly-competitive KITTI benchmark. The source code is available at
https://github.com/open-mmlab/OpenPCDet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SCPM-Net: An Anchor-free 3D Lung Nodule Detection Network using Sphere Representation and Center Points Matching. (arXiv:2104.05215v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05215">
<div class="article-summary-box-inner">
<span><p>Lung nodule detection from 3D Computed Tomography scans plays a vital role in
efficient lung cancer screening. Despite the SOTA performance obtained by
recent anchor-based detectors using CNNs for this task, they require
predetermined anchor parameters such as the size, number, and aspect ratio of
anchors, and have limited robustness when dealing with lung nodules with a
massive variety of sizes. To overcome these problems, we propose a 3D sphere
representation-based center-points matching detection network that is
anchor-free and automatically predicts the position, radius, and offset of
nodules without the manual design of nodule/anchor parameters. The SCPM-Net
consists of two novel components: sphere representation and center points
matching. First, to match the nodule annotation in clinical practice, we
replace the commonly used bounding box with our proposed bounding sphere to
represent nodules with the centroid, radius, and local offset in 3D space. A
compatible sphere-based intersection over-union loss function is introduced to
train the lung nodule detection network stably and efficiently. Second, we
empower the network anchor-free by designing a positive center-points selection
and matching process, which naturally discards pre-determined anchor boxes. An
online hard example mining and re-focal loss subsequently enable the CPM
process to be more robust, resulting in more accurate point assignment and
mitigation of class imbalance. In addition, to better capture spatial
information and 3D context for the detection, we propose to fuse multi-level
spatial coordinate maps with the feature extractor and combine them with 3D
squeeze-and-excitation attention modules. Experimental results on the LUNA16
dataset showed that our proposed framework achieves superior performance
compared with existing anchor-based and anchor-free methods for lung nodule
detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GRNN: Generative Regression Neural Network -- A Data Leakage Attack for Federated Learning. (arXiv:2105.00529v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.00529">
<div class="article-summary-box-inner">
<span><p>Data privacy has become an increasingly important issue in Machine Learning
(ML), where many approaches have been developed to tackle this challenge, e.g.
cryptography (Homomorphic Encryption (HE), Differential Privacy (DP), etc.) and
collaborative training (Secure Multi-Party Computation (MPC), Distributed
Learning and Federated Learning (FL)). These techniques have a particular focus
on data encryption or secure local computation. They transfer the intermediate
information to the third party to compute the final result. Gradient exchanging
is commonly considered to be a secure way of training a robust model
collaboratively in Deep Learning (DL). However, recent researches have
demonstrated that sensitive information can be recovered from the shared
gradient. Generative Adversarial Network (GAN), in particular, has shown to be
effective in recovering such information. However, GAN based techniques require
additional information, such as class labels which are generally unavailable
for privacy-preserved learning. In this paper, we show that, in the FL system,
image-based privacy data can be easily recovered in full from the shared
gradient only via our proposed Generative Regression Neural Network (GRNN). We
formulate the attack to be a regression problem and optimize two branches of
the generative model by minimizing the distance between gradients. We evaluate
our method on several image classification tasks. The results illustrate that
our proposed GRNN outperforms state-of-the-art methods with better stability,
stronger robustness, and higher accuracy. It also has no convergence
requirement to the global FL model. Moreover, we demonstrate information
leakage using face re-identification. Some defense strategies are also
discussed in this work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ExSinGAN: Learning an Explainable Generative Model from a Single Image. (arXiv:2105.07350v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.07350">
<div class="article-summary-box-inner">
<span><p>Generating images from a single sample, as a newly developing branch of image
synthesis, has attracted extensive attention. In this paper, we formulate this
problem as sampling from the conditional distribution of a single image, and
propose a hierarchical framework that simplifies the learning of the intricate
conditional distributions through the successive learning of the distributions
about structure, semantics and texture, making the process of learning and
generation comprehensible. On this basis, we design ExSinGAN composed of three
cascaded GANs for learning an explainable generative model from a given image,
where the cascaded GANs model the distributions about structure, semantics and
texture successively. ExSinGAN is learned not only from the internal patches of
the given image as the previous works did, but also from the external prior
obtained by the GAN inversion technique. Benefiting from the appropriate
combination of internal and external information, ExSinGAN has a more powerful
capability of generation and competitive generalization ability for the image
manipulation tasks compared with prior works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AUGCO: Augmentation Consistency-guided Self-training for Source-free Domain Adaptive Semantic Segmentation. (arXiv:2107.10140v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.10140">
<div class="article-summary-box-inner">
<span><p>Most modern approaches for domain adaptive semantic segmentation rely on
continued access to source data during adaptation, which may be infeasible due
to computational or privacy constraints. We focus on source-free domain
adaptation for semantic segmentation, wherein a source model must adapt itself
to a new target domain given only unlabeled target data. We propose
Augmentation Consistency-guided Self-training (AUGCO), a source-free adaptation
algorithm that uses the model's pixel-level predictive consistency across
diverse, automatically generated views of each target image along with model
confidence to identify reliable pixel predictions, and selectively self-trains
on those. AUGCO achieves state-of-the-art results for source-free adaptation on
3 standard benchmarks for semantic segmentation, all within a simple to
implement and fast to converge method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Classification of Abnormal Hand Movement for Aiding in Autism Detection: Machine Learning Study. (arXiv:2108.07917v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07917">
<div class="article-summary-box-inner">
<span><p>A formal autism diagnosis can be an inefficient and lengthy process. Families
may wait months or longer before receiving a diagnosis for their child despite
evidence that earlier intervention leads to better treatment outcomes. Digital
technologies which detect the presence of behaviors related to autism can scale
access to pediatric diagnoses. This work aims to demonstrate the feasibility of
deep learning technologies for detecting hand flapping from unstructured home
videos as a first step towards validating whether models and digital
technologies can be leveraged to aid with autism diagnoses. We used the
Self-Stimulatory Behavior Dataset (SSBD), which contains 75 videos of hand
flapping, head banging, and spinning exhibited by children. From all the hand
flapping videos, we extracted 100 positive and control videos of hand flapping,
each between 2 to 5 seconds in duration. Utilizing both
landmark-driven-approaches and MobileNet V2's pretrained convolutional layers,
our highest performing model achieved a testing F1 score of 84% (90% precision
and 80% recall) when evaluating with 5-fold cross validation 100 times. This
work provides the first step towards developing precise deep learning methods
for activity detection of autism-related behaviors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RGB-D Saliency Detection via Cascaded Mutual Information Minimization. (arXiv:2109.07246v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07246">
<div class="article-summary-box-inner">
<span><p>Existing RGB-D saliency detection models do not explicitly encourage RGB and
depth to achieve effective multi-modal learning. In this paper, we introduce a
novel multi-stage cascaded learning framework via mutual information
minimization to "explicitly" model the multi-modal information between RGB
image and depth data. Specifically, we first map the feature of each mode to a
lower dimensional feature vector, and adopt mutual information minimization as
a regularizer to reduce the redundancy between appearance features from RGB and
geometric features from depth. We then perform multi-stage cascaded learning to
impose the mutual information minimization constraint at every stage of the
network. Extensive experiments on benchmark RGB-D saliency datasets illustrate
the effectiveness of our framework. Further, to prosper the development of this
field, we contribute the largest (7x larger than NJU2K) dataset, which contains
15,625 image pairs with high quality
polygon-/scribble-/object-/instance-/rank-level annotations. Based on these
rich labels, we additionally construct four new benchmarks with strong
baselines and observe some interesting phenomena, which can motivate future
model design. Source code and dataset are available at
"https://github.com/JingZhang617/cascaded_rgbd_sod".
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Layout Generation Algorithm of Graphic Design Based on Transformer-CVAE. (arXiv:2110.06794v2 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06794">
<div class="article-summary-box-inner">
<span><p>Graphic design is ubiquitous in people's daily lives. For graphic design, the
most time-consuming task is laying out various components in the interface.
Repetitive manual layout design will waste a lot of time for professional
graphic designers. Existing templates are usually rudimentary and not suitable
for most designs, reducing efficiency and limiting creativity. This paper
implemented the Transformer model and conditional variational autoencoder
(CVAE) to the graphic design layout generation task. It proposed an end-to-end
graphic design layout generation model named LayoutT-CVAE. We also proposed
element disentanglement and feature-based disentanglement strategies and
introduce new graphic design principles and similarity metrics into the model,
which significantly increased the controllability and interpretability of the
deep model. Compared with the existing state-of-art models, the layout
generated by ours performs better on many metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Instance-Conditional Knowledge Distillation for Object Detection. (arXiv:2110.12724v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12724">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation has shown great success in classification, however, it
is still challenging for detection. In a typical image for detection,
representations from different locations may have different contributions to
detection targets, making the distillation hard to balance. In this paper, we
propose a conditional distillation framework to distill the desired knowledge,
namely knowledge that is beneficial in terms of both classification and
localization for every instance. The framework introduces a learnable
conditional decoding module, which retrieves information given each target
instance as query. Specifically, we encode the condition information as query
and use the teacher's representations as key. The attention between query and
key is used to measure the contribution of different features, guided by a
localization-recognition-sensitive auxiliary task. Extensive experiments
demonstrate the efficacy of our method: we observe impressive improvements
under various settings. Notably, we boost RetinaNet with ResNet-50 backbone
from 37.4 to 40.7 mAP (+3.3) under 1x schedule, that even surpasses the teacher
(40.4 mAP) with ResNet-101 backbone under 3x schedule. Code has been released
on https://github.com/megvii-research/ICD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PeCo: Perceptual Codebook for BERT Pre-training of Vision Transformers. (arXiv:2111.12710v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12710">
<div class="article-summary-box-inner">
<span><p>This paper explores a better codebook for BERT pre-training of vision
transformers. The recent work BEiT successfully transfers BERT pre-training
from NLP to the vision field. It directly adopts one simple discrete VAE as the
visual tokenizer, but has not considered the semantic level of the resulting
visual tokens. By contrast, the discrete tokens in NLP field are naturally
highly semantic. This difference motivates us to learn a perceptual codebook.
And we surprisingly find one simple yet effective idea: enforcing perceptual
similarity during the dVAE training. We demonstrate that the visual tokens
generated by the proposed perceptual codebook do exhibit better semantic
meanings, and subsequently help pre-training achieve superior transfer
performance in various downstream tasks. For example, we achieve 84.5% Top-1
accuracy on ImageNet-1K with ViT-B backbone, outperforming the competitive
method BEiT by +1.3 with the same pre-training epochs. It can also improve the
performance of object detection and segmentation tasks on COCO val by +1.3 box
AP and +1.0 mask AP, semantic segmentation on ADE20k by +1.0 mIoU. Equipped
with a larger backbone ViT-H, we achieve the state-of-the-art performance
(88.3% Top-1 accuracy) among the methods using only ImageNet-1K data. The code
and models will be available at https://github.com/microsoft/PeCo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dimensions of Motion: Learning to Predict a Subspace of Optical Flow from a Single Image. (arXiv:2112.01502v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01502">
<div class="article-summary-box-inner">
<span><p>We introduce the problem of predicting, from a single video frame, a
low-dimensional subspace of optical flow which includes the actual
instantaneous optical flow. We show how several natural scene assumptions allow
us to identify an appropriate flow subspace via a set of basis flow fields
parameterized by disparity and a representation of object instances. The flow
subspace, together with a novel loss function, can be used for the tasks of
predicting monocular depth or predicting depth plus an object instance
embedding. This provides a new approach to learning these tasks in an
unsupervised fashion using monocular input video without requiring camera
intrinsics or poses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HSPACE: Synthetic Parametric Humans Animated in Complex Environments. (arXiv:2112.12867v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.12867">
<div class="article-summary-box-inner">
<span><p>Advances in the state of the art for 3d human sensing are currently limited
by the lack of visual datasets with 3d ground truth, including multiple people,
in motion, operating in real-world environments, with complex illumination or
occlusion, and potentially observed by a moving camera. Sophisticated scene
understanding would require estimating human pose and shape as well as
gestures, towards representations that ultimately combine useful metric and
behavioral signals with free-viewpoint photo-realistic visualisation
capabilities. To sustain progress, we build a large-scale photo-realistic
dataset, Human-SPACE (HSPACE), of animated humans placed in complex synthetic
indoor and outdoor environments. We combine a hundred diverse individuals of
varying ages, gender, proportions, and ethnicity, with hundreds of motions and
scenes, as well as parametric variations in body shape (for a total of 1,600
different humans), in order to generate an initial dataset of over 1 million
frames. Human animations are obtained by fitting an expressive human body
model, GHUM, to single scans of people, followed by novel re-targeting and
positioning procedures that support the realistic animation of dressed humans,
statistical variation of body proportions, and jointly consistent scene
placement of multiple moving people. Assets are generated automatically, at
scale, and are compatible with existing real time rendering and game engines.
The dataset with evaluation server will be made available for research. Our
large-scale analysis of the impact of synthetic data, in connection with real
data and weak supervision, underlines the considerable potential for continuing
quality improvements and limiting the sim-to-real gap, in this practical
setting, in connection with increased model capacity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Fistful of Words: Learning Transferable Visual Models from Bag-of-Words Supervision. (arXiv:2112.13884v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13884">
<div class="article-summary-box-inner">
<span><p>Using natural language as a supervision for training visual recognition
models holds great promise. Recent works have shown that if such supervision is
used in the form of alignment between images and captions in large training
datasets, then the resulting aligned models perform well on zero-shot
classification as downstream tasks2. In this paper, we focus on teasing out
what parts of the language supervision are essential for training zero-shot
image classification models. Through extensive and careful experiments, we show
that: 1) A simple Bag-of-Words (BoW) caption could be used as a replacement for
most of the image captions in the dataset. Surprisingly, we observe that this
approach improves the zero-shot classification performance when combined with
word balancing. 2) Using a BoW pretrained model, we can obtain more training
data by generating pseudo-BoW captions on images that do not have a caption.
Models trained on images with real and pseudo-BoW captions achieve stronger
zero-shot performance. On ImageNet-1k zero-shot evaluation, our best model,
that uses only 3M image-caption pairs, performs on-par with a CLIP model
trained on 15M image-caption pairs (31.5% vs 31.3%).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Experience Replay: Continual Learning by Adaptively Tuning Task-wise Relationship. (arXiv:2112.15402v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15402">
<div class="article-summary-box-inner">
<span><p>Continual learning requires models to learn new tasks while maintaining
previously learned knowledge. Various algorithms have been proposed to address
this real challenge. Till now, rehearsal-based methods, such as experience
replay, have achieved state-of-the-art performance. These approaches save a
small part of the data of the past tasks as a memory buffer to prevent models
from forgetting previously learned knowledge. However, most of them treat every
new task equally, i.e., fixed the hyperparameters of the framework while
learning different new tasks. Such a setting lacks the consideration of the
relationship/similarity between past and new tasks. For example, the previous
knowledge/features learned from dogs are more beneficial for the identification
of cats (new task), compared to those learned from buses. In this regard, we
propose a meta learning algorithm based on bi-level optimization to adaptively
tune the relationship between the knowledge extracted from the past and new
tasks. Therefore, the model can find an appropriate direction of gradient
during continual learning and avoid the serious overfitting problem on memory
buffer. Extensive experiments are conducted on three publicly available
datasets (i.e., CIFAR-10, CIFAR-100, and Tiny ImageNet). The experimental
results demonstrate that the proposed method can consistently improve the
performance of all baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MoCoPnet: Exploring Local Motion and Contrast Priors for Infrared Small Target Super-Resolution. (arXiv:2201.01014v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01014">
<div class="article-summary-box-inner">
<span><p>Infrared small target super-resolution (SR) aims to recover reliable and
detailed high-resolution image with highcontrast targets from its
low-resolution counterparts. Since the infrared small target lacks color and
fine structure information, it is significant to exploit the supplementary
information among sequence images to enhance the target. In this paper, we
propose the first infrared small target SR method named local motion and
contrast prior driven deep network (MoCoPnet) to integrate the domain knowledge
of infrared small target into deep network, which can mitigate the intrinsic
feature scarcity of infrared small targets. Specifically, motivated by the
local motion prior in the spatio-temporal dimension, we propose a local
spatiotemporal attention module to perform implicit frame alignment and
incorporate the local spatio-temporal information to enhance the local features
(especially for small targets). Motivated by the local contrast prior in the
spatial dimension, we propose a central difference residual group to
incorporate the central difference convolution into the feature extraction
backbone, which can achieve center-oriented gradient-aware feature extraction
to further improve the target contrast. Extensive experiments have demonstrated
that our method can recover accurate spatial dependency and improve the target
contrast. Comparative results show that MoCoPnet can outperform the
state-of-the-art video SR and single image SR methods in terms of both SR
performance and target enhancement. Based on the SR results, we further
investigate the influence of SR on infrared small target detection and the
experimental results demonstrate that MoCoPnet promotes the detection
performance. The code is available at https://github.com/XinyiYing/MoCoPnet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">All You Need In Sign Language Production. (arXiv:2201.01609v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01609">
<div class="article-summary-box-inner">
<span><p>Sign Language is the dominant form of communication language used in the deaf
and hearing-impaired community. To make an easy and mutual communication
between the hearing-impaired and the hearing communities, building a robust
system capable of translating the spoken language into sign language and vice
versa is fundamental. To this end, sign language recognition and production are
two necessary parts for making such a two-way system. Sign language recognition
and production need to cope with some critical challenges. In this survey, we
review recent advances in Sign Language Production (SLP) and related areas
using deep learning. To have more realistic perspectives to sign language, we
present an introduction to the Deaf culture, Deaf centers, psychological
perspective of sign language, the main differences between spoken language and
sign language. Furthermore, we present the fundamental components of a
bi-directional sign language translation system, discussing the main challenges
in this area. Also, the backbone architectures and methods in SLP are briefly
introduced and the proposed taxonomy on SLP is presented. Finally, a general
framework for SLP and performance evaluation, and also a discussion on the
recent developments, advantages, and limitations in SLP, commenting on possible
lines for future research are presented.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-01-09 23:07:17.146508937 UTC">2022-01-09 23:07:17 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>