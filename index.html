<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-07-12T01:30:00Z">07-12</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph-based Multi-View Fusion and Local Adaptation: Mitigating Within-Household Confusability for Speaker Identification. (arXiv:2207.04081v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04081">
<div class="article-summary-box-inner">
<span><p>Speaker identification (SID) in the household scenario (e.g., for smart
speakers) is an important but challenging problem due to limited number of
labeled (enrollment) utterances, confusable voices, and demographic imbalances.
Conventional speaker recognition systems generalize from a large random sample
of speakers, causing the recognition to underperform for households drawn from
specific cohorts or otherwise exhibiting high confusability. In this work, we
propose a graph-based semi-supervised learning approach to improve
household-level SID accuracy and robustness with locally adapted graph
normalization and multi-signal fusion with multi-view graphs. Unlike other work
on household SID, fairness, and signal fusion, this work focuses on speaker
label inference (scoring) and provides a simple solution to realize
household-specific adaptation and multi-signal fusion without tuning the
embeddings or training a fusion network. Experiments on the VoxCeleb dataset
demonstrate that our approach consistently improves the performance across
households with different customer cohorts and degrees of confusability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Entity Disambiguation by Reasoning over a Knowledge Base. (arXiv:2207.04106v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04106">
<div class="article-summary-box-inner">
<span><p>Recent work in entity disambiguation (ED) has typically neglected structured
knowledge base (KB) facts, and instead relied on a limited subset of KB
information, such as entity descriptions or types. This limits the range of
contexts in which entities can be disambiguated. To allow the use of all KB
facts, as well as descriptions and types, we introduce an ED model which links
entities by reasoning over a symbolic knowledge base in a fully differentiable
fashion. Our model surpasses state-of-the-art baselines on six well-established
ED datasets by 1.3 F1 on average. By allowing access to all KB information, our
model is less reliant on popularity-based entity priors, and improves
performance on the challenging ShadowLink dataset (which emphasises infrequent
and ambiguous entities) by 12.7 F1.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReFinED: An Efficient Zero-shot-capable Approach to End-to-End Entity Linking. (arXiv:2207.04108v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04108">
<div class="article-summary-box-inner">
<span><p>We introduce ReFinED, an efficient end-to-end entity linking model which uses
fine-grained entity types and entity descriptions to perform linking. The model
performs mention detection, fine-grained entity typing, and entity
disambiguation for all mentions within a document in a single forward pass,
making it more than 60 times faster than competitive existing approaches.
ReFinED also surpasses state-of-the-art performance on standard entity linking
datasets by an average of 3.7 F1. The model is capable of generalising to
large-scale knowledge bases such as Wikidata (which has 15 times more entities
than Wikipedia) and of zero-shot entity linking. The combination of speed,
accuracy and scale makes ReFinED an effective and cost-efficient system for
extracting entities from web-scale datasets, for which the model has been
successfully deployed. Our code and pre-trained models are available at
https://github.com/alexa/ReFinED
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing Classifiers are Unreliable for Concept Removal and Detection. (arXiv:2207.04153v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04153">
<div class="article-summary-box-inner">
<span><p>Neural network models trained on text data have been found to encode
undesired linguistic or sensitive attributes in their representation. Removing
such attributes is non-trivial because of a complex relationship between the
attribute, text input, and the learnt representation. Recent work has proposed
post-hoc and adversarial methods to remove such unwanted attributes from a
model's representation. Through an extensive theoretical and empirical
analysis, we show that these methods can be counter-productive: they are unable
to remove the attributes entirely, and in the worst case may end up destroying
all task-relevant features. The reason is the methods' reliance on a probing
classifier as a proxy for the attribute. Even under the most favorable
conditions when an attribute's features in representation space can alone
provide 100% accuracy for learning the probing classifier, we prove that
post-hoc or adversarial methods will fail to remove the attribute correctly.
These theoretical implications are confirmed by empirical experiments on models
trained on synthetic, Multi-NLI, and Twitter datasets. For sensitive
applications of attribute removal such as fairness, we recommend caution
against using these methods and propose a spuriousness metric to gauge the
quality of the final classifier.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TalkToModel: Understanding Machine Learning Models With Open Ended Dialogues. (arXiv:2207.04154v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04154">
<div class="article-summary-box-inner">
<span><p>Machine Learning (ML) models are increasingly used to make critical decisions
in real-world applications, yet they have also become more complex, making them
harder to understand. To this end, several techniques to explain model
predictions have been proposed. However, practitioners struggle to leverage
explanations because they often do not know which to use, how to interpret the
results, and may have insufficient data science experience to obtain
explanations. In addition, most current works focus on generating one-shot
explanations and do not allow users to follow up and ask fine-grained questions
about the explanations, which can be frustrating. In this work, we address
these challenges by introducing TalkToModel: an open-ended dialogue system for
understanding machine learning models. Specifically, TalkToModel comprises
three key components: 1) a natural language interface for engaging in
dialogues, making understanding ML models highly accessible, 2) a dialogue
engine that adapts to any tabular model and dataset, interprets natural
language, maps it to appropriate operations (e.g., feature importance
explanations, counterfactual explanations, showing model errors), and generates
text responses, and 3) an execution component that run the operations and
ensures explanations are accurate. We carried out quantitative and human
subject evaluations of TalkToModel. We found the system understands user
questions on novel datasets and models with high accuracy, demonstrating the
system's capacity to generalize to new situations. In human evaluations, 73% of
healthcare workers (e.g., doctors and nurses) agreed they would use TalkToModel
over baseline point-and-click systems, and 84.6% of ML graduate students agreed
TalkToModel was easier to use.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Audio Captioning and Language-Based Audio Retrieval. (arXiv:2207.04156v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04156">
<div class="article-summary-box-inner">
<span><p>This project involved participation in the DCASE 2022 Competition (Task 6)
which had two subtasks: (1) Automated Audio Captioning and (2) Language-Based
Audio Retrieval. The first subtask involved the generation of a textual
description for audio samples, while the goal of the second was to find audio
samples within a fixed dataset that match a given description. For both
subtasks, the Clotho dataset was used. The models were evaluated on BLEU1,
BLEU2, BLEU3, ROUGEL, METEOR, CIDEr, SPICE, and SPIDEr scores for audio
captioning and R1, R5, R10 and mARP10 scores for audio retrieval. We have
conducted a handful of experiments that modify the baseline models for these
tasks. Our final architecture for Automated Audio Captioning is close to the
baseline performance, while our model for Language-Based Audio Retrieval has
surpassed its counterpart.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Internal Language Model Estimation based Language Model Fusion for Cross-Domain Code-Switching Speech Recognition. (arXiv:2207.04176v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04176">
<div class="article-summary-box-inner">
<span><p>Internal Language Model Estimation (ILME) based language model (LM) fusion
has been shown significantly improved recognition results over conventional
shallow fusion in both intra-domain and cross-domain speech recognition tasks.
In this paper, we attempt to apply our ILME method to cross-domain
code-switching speech recognition (CSSR) work. Specifically, our curiosity
comes from several aspects. First, we are curious about how effective the
ILME-based LM fusion is for both intra-domain and cross-domain CSSR tasks. We
verify this with or without merging two code-switching domains. More
importantly, we train an end-to-end (E2E) speech recognition model by means of
merging two monolingual data sets and observe the efficacy of the proposed
ILME-based LM fusion for CSSR. Experimental results on SEAME that is from
Southeast Asian and another Chinese Mainland CS data set demonstrate the
effectiveness of the proposed ILME-based LM fusion method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Study of Syntactic Multi-Modality in Non-Autoregressive Machine Translation. (arXiv:2207.04206v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04206">
<div class="article-summary-box-inner">
<span><p>It is difficult for non-autoregressive translation (NAT) models to capture
the multi-modal distribution of target translations due to their conditional
independence assumption, which is known as the "multi-modality problem",
including the lexical multi-modality and the syntactic multi-modality. While
the first one has been well studied, the syntactic multi-modality brings severe
challenge to the standard cross entropy (XE) loss in NAT and is under studied.
In this paper, we conduct a systematic study on the syntactic multi-modality
problem. Specifically, we decompose it into short- and long-range syntactic
multi-modalities and evaluate several recent NAT algorithms with advanced loss
functions on both carefully designed synthesized datasets and real datasets. We
find that the Connectionist Temporal Classification (CTC) loss and the
Order-Agnostic Cross Entropy (OAXE) loss can better handle short- and
long-range syntactic multi-modalities respectively. Furthermore, we take the
best of both and design a new loss function to better handle the complicated
syntactic multi-modality in real-world datasets. To facilitate practical usage,
we provide a guide to use different loss functions for different kinds of
syntactic multi-modality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explaining Chest X-ray Pathologies in Natural Language. (arXiv:2207.04343v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04343">
<div class="article-summary-box-inner">
<span><p>Most deep learning algorithms lack explanations for their predictions, which
limits their deployment in clinical practice. Approaches to improve
explainability, especially in medical imaging, have often been shown to convey
limited information, be overly reassuring, or lack robustness. In this work, we
introduce the task of generating natural language explanations (NLEs) to
justify predictions made on medical images. NLEs are human-friendly and
comprehensive, and enable the training of intrinsically explainable models. To
this goal, we introduce MIMIC-NLE, the first, large-scale, medical imaging
dataset with NLEs. It contains over 38,000 NLEs, which explain the presence of
various thoracic pathologies and chest X-ray findings. We propose a general
approach to solve the task and evaluate several architectures on this dataset,
including via clinician assessment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action. (arXiv:2207.04429v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04429">
<div class="article-summary-box-inner">
<span><p>Goal-conditioned policies for robotic navigation can be trained on large,
unannotated datasets, providing for good generalization to real-world settings.
However, particularly in vision-based settings where specifying goals requires
an image, this makes for an unnatural interface. Language provides a more
convenient modality for communication with robots, but contemporary methods
typically require expensive supervision, in the form of trajectories annotated
with language descriptions. We present a system, LM-Nav, for robotic navigation
that enjoys the benefits of training on unannotated large datasets of
trajectories, while still providing a high-level interface to the user. Instead
of utilizing a labeled instruction following dataset, we show that such a
system can be constructed entirely out of pre-trained models for navigation
(ViNG), image-language association (CLIP), and language modeling (GPT-3),
without requiring any fine-tuning or language-annotated robot data. We
instantiate LM-Nav on a real-world mobile robot and demonstrate long-horizon
navigation through complex, outdoor environments from natural language
instructions. For videos of our experiments, code release, and an interactive
Colab notebook that runs in your browser, please check out our project page
https://sites.google.com/view/lmnav
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human-Centric Research for NLP: Towards a Definition and Guiding Questions. (arXiv:2207.04447v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04447">
<div class="article-summary-box-inner">
<span><p>With Human-Centric Research (HCR) we can steer research activities so that
the research outcome is beneficial for human stakeholders, such as end users.
But what exactly makes research human-centric? We address this question by
providing a working definition and define how a research pipeline can be split
into different stages in which human-centric components can be added.
Additionally, we discuss existing NLP with HCR components and define a series
of guiding questions, which can serve as starting points for researchers
interested in exploring human-centric research approaches. We hope that this
work would inspire researchers to refine the proposed definition and to pose
other questions that might be meaningful for achieving HCR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Persuasion Detection: Video Games as an Invaluable Data Source for NLP. (arXiv:2207.04453v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04453">
<div class="article-summary-box-inner">
<span><p>Role-playing games (RPGs) have a considerable amount of text in video game
dialogues. Quite often this text is semi-annotated by the game developers. In
this paper, we extract a multilingual dataset of persuasive dialogue from
several RPGs. We show the viability of this data in building a persuasion
detection system using a natural language processing (NLP) model called BERT.
We believe that video games have a lot of unused potential as a datasource for
a variety of NLP tasks. The code and data described in this paper are available
on Zenodo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Myers-Briggs personality classification from social media text using pre-trained language models. (arXiv:2207.04476v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04476">
<div class="article-summary-box-inner">
<span><p>In Natural Language Processing, the use of pre-trained language models has
been shown to obtain state-of-the-art results in many downstream tasks such as
sentiment analysis, author identification and others. In this work, we address
the use of these methods for personality classification from text. Focusing on
the Myers-Briggs (MBTI) personality model, we describe a series of experiments
in which the well-known Bidirectional Encoder Representations from Transformers
(BERT) model is fine-tuned to perform MBTI classification. Our main findings
suggest that the current approach significantly outperforms well-known text
classification models based on bag-of-words and static word embeddings alike
across multiple evaluation scenarios, and generally outperforms previous work
in the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling up ML-based Black-box Planning with Partial STRIPS Models. (arXiv:2207.04479v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04479">
<div class="article-summary-box-inner">
<span><p>A popular approach for sequential decision-making is to perform
simulator-based search guided with Machine Learning (ML) methods like policy
learning. On the other hand, model-relaxation heuristics can guide the search
effectively if a full declarative model is available. In this work, we consider
how a practitioner can improve ML-based black-box planning on settings where a
complete symbolic model is not available. We show that specifying an incomplete
STRIPS model that describes only part of the problem enables the use of
relaxation heuristics. Our findings on several planning domains suggest that
this is an effective way to improve ML-based black-box planning beyond
collecting more data or tuning ML architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FairDistillation: Mitigating Stereotyping in Language Models. (arXiv:2207.04546v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04546">
<div class="article-summary-box-inner">
<span><p>Large pre-trained language models are successfully being used in a variety of
tasks, across many languages. With this ever-increasing usage, the risk of
harmful side effects also rises, for example by reproducing and reinforcing
stereotypes. However, detecting and mitigating these harms is difficult to do
in general and becomes computationally expensive when tackling multiple
languages or when considering different biases. To address this, we present
FairDistillation: a cross-lingual method based on knowledge distillation to
construct smaller language models while controlling for specific biases. We
found that our distillation method does not negatively affect the downstream
performance on most tasks and successfully mitigates stereotyping and
representational harms. We demonstrate that FairDistillation can create fairer
language models at a considerably lower cost than alternative approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Sentiment and Emotion Analysis for Computational Literary Studies. (arXiv:1808.03137v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1808.03137">
<div class="article-summary-box-inner">
<span><p>Emotions are a crucial part of compelling narratives: literature tells us
about people with goals, desires, passions, and intentions. Emotion analysis is
part of the broader and larger field of sentiment analysis, and receives
increasing attention in literary studies. In the past, the affective dimension
of literature was mainly studied in the context of literary hermeneutics.
However, with the emergence of the research field known as Digital Humanities
(DH), some studies of emotions in a literary context have taken a computational
turn. Given the fact that DH is still being formed as a field, this direction
of research can be rendered relatively new. In this survey, we offer an
overview of the existing body of research on emotion analysis as applied to
literature. The research under review deals with a variety of topics including
tracking dramatic changes of a plot development, network analysis of a literary
text, and understanding the emotionality of texts, among other topics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Baleen: Robust Multi-Hop Reasoning at Scale via Condensed Retrieval. (arXiv:2101.00436v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00436">
<div class="article-summary-box-inner">
<span><p>Multi-hop reasoning (i.e., reasoning across two or more documents) is a key
ingredient for NLP models that leverage large corpora to exhibit broad
knowledge. To retrieve evidence passages, multi-hop models must contend with a
fast-growing search space across the hops, represent complex queries that
combine multiple information needs, and resolve ambiguity about the best order
in which to hop between training passages. We tackle these problems via Baleen,
a system that improves the accuracy of multi-hop retrieval while learning
robustly from weak training signals in the many-hop setting. To tame the search
space, we propose condensed retrieval, a pipeline that summarizes the retrieved
passages after each hop into a single compact context. To model complex
queries, we introduce a focused late interaction retriever that allows
different parts of the same query representation to match disparate relevant
passages. Lastly, to infer the hopping dependencies among unordered training
passages, we devise latent hop ordering, a weak-supervision strategy in which
the trained retriever itself selects the sequence of hops. We evaluate Baleen
on retrieval for two-hop question answering and many-hop claim verification,
establishing state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MultiWOZ 2.4: A Multi-Domain Task-Oriented Dialogue Dataset with Essential Annotation Corrections to Improve State Tracking Evaluation. (arXiv:2104.00773v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.00773">
<div class="article-summary-box-inner">
<span><p>The MultiWOZ 2.0 dataset has greatly stimulated the research of task-oriented
dialogue systems. However, its state annotations contain substantial noise,
which hinders a proper evaluation of model performance. To address this issue,
massive efforts were devoted to correcting the annotations. Three improved
versions (i.e., MultiWOZ 2.1-2.3) have then been released. Nonetheless, there
are still plenty of incorrect and inconsistent annotations. This work
introduces MultiWOZ 2.4, which refines the annotations in the validation set
and test set of MultiWOZ 2.1. The annotations in the training set remain
unchanged (same as MultiWOZ 2.1) to elicit robust and noise-resilient model
training. We benchmark eight state-of-the-art dialogue state tracking models on
MultiWOZ 2.4. All of them demonstrate much higher performance than on MultiWOZ
2.1.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Crowdsourcing Protocols for Evaluating the Factual Consistency of Summaries. (arXiv:2109.09195v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09195">
<div class="article-summary-box-inner">
<span><p>Current pre-trained models applied to summarization are prone to factual
inconsistencies which either misrepresent the source text or introduce
extraneous information. Thus, comparing the factual consistency of summaries is
necessary as we develop improved models. However, the optimal human evaluation
setup for factual consistency has not been standardized. To address this issue,
we crowdsourced evaluations for factual consistency using the rating-based
Likert scale and ranking-based Best-Worst Scaling protocols, on 100 articles
from each of the CNN-Daily Mail and XSum datasets over four state-of-the-art
models, to determine the most reliable evaluation framework. We find that
ranking-based protocols offer a more reliable measure of summary quality across
datasets, while the reliability of Likert ratings depends on the target dataset
and the evaluation design. Our crowdsourcing templates and summary evaluations
will be publicly available to facilitate future research on factual consistency
in summarization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Task Learning with Sentiment, Emotion, and Target Detection to Recognize Hate Speech and Offensive Language. (arXiv:2109.10255v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10255">
<div class="article-summary-box-inner">
<span><p>The recognition of hate speech and offensive language (HOF) is commonly
formulated as a classification task to decide if a text contains HOF. We
investigate whether HOF detection can profit by taking into account the
relationships between HOF and similar concepts: (a) HOF is related to sentiment
analysis because hate speech is typically a negative statement and expresses a
negative opinion; (b) it is related to emotion analysis, as expressed hate
points to the author experiencing (or pretending to experience) anger while the
addressees experience (or are intended to experience) fear. (c) Finally, one
constituting element of HOF is the mention of a targeted person or group. On
this basis, we hypothesize that HOF detection shows improvements when being
modeled jointly with these concepts, in a multi-task learning setup. We base
our experiments on existing data sets for each of these concepts (sentiment,
emotion, target of HOF) and evaluate our models as a participant (as team
IMS-SINAI) in the HASOC FIRE 2021 English Subtask 1A. Based on model-selection
experiments in which we consider multiple available resources and submissions
to the shared task, we find that the combination of the CrowdFlower emotion
corpus, the SemEval 2016 Sentiment Corpus, and the OffensEval 2019 target
detection data leads to an F1 =.79 in a multi-head multi-task learning model
based on BERT, in comparison to .7895 of plain BERT. On the HASOC 2019 test
data, this result is more substantial with an increase by 2pp in F1 and a
considerable increase in recall. Across both data sets (2019, 2021), the recall
is particularly increased for the class of HOF (6pp for the 2019 data and 3pp
for the 2021 data), showing that MTL with emotion, sentiment, and target
identification is an appropriate approach for early warning systems that might
be deployed in social media platforms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is Attention always needed? A Case Study on Language Identification from Speech. (arXiv:2110.03427v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03427">
<div class="article-summary-box-inner">
<span><p>Language Identification (LID), a recommended initial step to Automatic Speech
Recognition (ASR), is used to detect a spoken language from audio specimens. In
state-of-the-art systems capable of multilingual speech processing, however,
users have to explicitly set one or more languages before using them. LID,
therefore, plays a very important role in situations where ASR based systems
cannot parse the uttered language in multilingual contexts causing failure in
speech recognition. We propose an attention based convolutional recurrent
neural network (CRNN with Attention) that works on Mel-frequency Cepstral
Coefficient (MFCC) features of audio specimens. Additionally, we reproduce some
state-of-the-art approaches, namely Convolutional Neural Network (CNN) and
Convolutional Recurrent Neural Network (CRNN), and compare them to our proposed
method. We performed extensive evaluation on thirteen different Indian
languages and our model achieves classification accuracy over 98%. Our LID
model is robust to noise and provides 91.2% accuracy in a noisy scenario. The
proposed model is easily extensible to new languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models Can Be Strong Differentially Private Learners. (arXiv:2110.05679v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05679">
<div class="article-summary-box-inner">
<span><p>Differentially Private (DP) learning has seen limited success for building
large deep learning models of text, and attempts at straightforwardly applying
Differentially Private Stochastic Gradient Descent (DP-SGD) to NLP tasks have
resulted in large performance drops and high computational overhead. We show
that this performance drop can be mitigated with (1) the use of large
pretrained models; (2) hyperparameters that suit DP optimization; and (3)
fine-tuning objectives aligned with the pretraining procedure. With these
factors set right, we obtain private NLP models that outperform
state-of-the-art private training approaches and strong non-private baselines
-- by directly fine-tuning pretrained models with DP optimization on
moderately-sized corpora. To address the computational challenge of running
DP-SGD with large Transformers, we propose a memory saving technique that
allows clipping in DP-SGD to run without instantiating per-example gradients
for any layer in the model. The technique enables privately training
Transformers with almost the same memory cost as non-private training at a
modest run-time overhead. Contrary to conventional wisdom that DP optimization
fails at learning high-dimensional models (due to noise that scales with
dimension) empirical results reveal that private learning with pretrained
models tends to not suffer from dimension-dependent performance degradation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoLLIE: Continual Learning of Language Grounding from Language-Image Embeddings. (arXiv:2111.07993v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.07993">
<div class="article-summary-box-inner">
<span><p>This paper presents CoLLIE: a simple, yet effective model for continual
learning of how language is grounded in vision. Given a pre-trained multimodal
embedding model, where language and images are projected in the same semantic
space (in this case CLIP by OpenAI), CoLLIE learns a transformation function
that adjusts the language embeddings when needed to accommodate new language
use. This is done by predicting the difference vector that needs to be applied,
as well as a scaling factor for this vector, so that the adjustment is only
applied when needed. Unlike traditional few-shot learning, the model does not
just learn new classes and labels, but can also generalize to similar language
use and leverage semantic compositionality. We verify the model's performance
on two different tasks of identifying the targets of referring expressions,
where it has to learn new language use. The results show that the model can
efficiently learn and generalize from only a few examples, with little
interference with the model's original zero-shot performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction. (arXiv:2112.01488v3 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01488">
<div class="article-summary-box-inner">
<span><p>Neural information retrieval (IR) has greatly advanced search and other
knowledge-intensive language tasks. While many neural IR methods encode queries
and documents into single-vector representations, late interaction models
produce multi-vector representations at the granularity of each token and
decompose relevance modeling into scalable token-level computations. This
decomposition has been shown to make late interaction more effective, but it
inflates the space footprint of these models by an order of magnitude. In this
work, we introduce ColBERTv2, a retriever that couples an aggressive residual
compression mechanism with a denoised supervision strategy to simultaneously
improve the quality and space footprint of late interaction. We evaluate
ColBERTv2 across a wide range of benchmarks, establishing state-of-the-art
quality within and outside the training domain while reducing the space
footprint of late interaction models by 6--10$\times$.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Use of External Data for Spoken Named Entity Recognition. (arXiv:2112.07648v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07648">
<div class="article-summary-box-inner">
<span><p>Spoken language understanding (SLU) tasks involve mapping from speech audio
signals to semantic labels. Given the complexity of such tasks, good
performance might be expected to require large labeled datasets, which are
difficult to collect for each new task and domain. However, recent advances in
self-supervised speech representations have made it feasible to consider
learning SLU models with limited labeled data. In this work we focus on
low-resource spoken named entity recognition (NER) and address the question:
Beyond self-supervised pre-training, how can we use external speech and/or text
data that are not annotated for the task? We draw on a variety of approaches,
including self-training, knowledge distillation, and transfer learning, and
consider their applicability to both end-to-end models and pipeline (speech
recognition followed by text NER model) approaches. We find that several of
these approaches improve performance in resource-constrained settings beyond
the benefits from pre-trained representations alone. Compared to prior work, we
find improved F1 scores of up to 16%. While the best baseline model is a
pipeline approach, the best performance when using external data is ultimately
achieved by an end-to-end model. We provide detailed comparisons and analyses,
showing for example that end-to-end models are able to focus on the more
NER-specific words.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Rich Representation of Keyphrases from Text. (arXiv:2112.08547v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08547">
<div class="article-summary-box-inner">
<span><p>In this work, we explore how to train task-specific language models aimed
towards learning rich representation of keyphrases from text documents. We
experiment with different masking strategies for pre-training transformer
language models (LMs) in discriminative as well as generative settings. In the
discriminative setting, we introduce a new pre-training objective - Keyphrase
Boundary Infilling with Replacement (KBIR), showing large gains in performance
(upto 8.16 points in F1) over SOTA, when the LM pre-trained using KBIR is
fine-tuned for the task of keyphrase extraction. In the generative setting, we
introduce a new pre-training setup for BART - KeyBART, that reproduces the
keyphrases related to the input text in the CatSeq format, instead of the
denoised original input. This also led to gains in performance (upto 4.33
points in F1@M) over SOTA for keyphrase generation. Additionally, we also
fine-tune the pre-trained language models on named entity recognition (NER),
question answering (QA), relation extraction (RE), abstractive summarization
and achieve comparable performance with that of the SOTA, showing that learning
rich representation of keyphrases is indeed beneficial for many other
fundamental NLP tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Isochrony-Aware Neural Machine Translation for Automatic Dubbing. (arXiv:2112.08548v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08548">
<div class="article-summary-box-inner">
<span><p>We introduce the task of isochrony-aware machine translation which aims at
generating translations suitable for dubbing. Dubbing of a spoken sentence
requires transferring the content as well as the speech-pause structure of the
source into the target language to achieve audiovisual coherence. Practically,
this implies correctly projecting pauses from the source to the target and
ensuring that target speech segments have roughly the same duration of the
corresponding source speech segments. In this work, we propose implicit and
explicit modeling approaches to integrate isochrony information into neural
machine translation. Experiments on English-German/French language pairs with
automatic metrics show that the simplest of the considered approaches works
best. Results are confirmed by human evaluations of translations and dubbed
videos.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CONFIT: Toward Faithful Dialogue Summarization with Linguistically-Informed Contrastive Fine-tuning. (arXiv:2112.08713v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08713">
<div class="article-summary-box-inner">
<span><p>Factual inconsistencies in generated summaries severely limit the practical
applications of abstractive dialogue summarization. Although significant
progress has been achieved by using pre-trained models, substantial amounts of
hallucinated content are found during the human evaluation. Pre-trained models
are most commonly fine-tuned with cross-entropy loss for text summarization,
which may not be an optimal strategy. In this work, we provide a typology of
factual errors with annotation data to highlight the types of errors and move
away from a binary understanding of factuality. We further propose a training
strategy that improves the factual consistency and overall quality of summaries
via a novel contrastive fine-tuning, called ConFiT. Based on our
linguistically-informed typology of errors, we design different modular
objectives that each target a specific type. Specifically, we utilize hard
negative samples with errors to reduce the generation of factual inconsistency.
In order to capture the key information between speakers, we also design a
dialogue-specific loss. Using human evaluation and automatic faithfulness
metrics, we show that our model significantly reduces all kinds of factual
errors on the dialogue summarization, SAMSum corpus. Moreover, our model could
be generalized to the meeting summarization, AMI corpus, and it produces
significantly higher scores than most of the baselines on both datasets
regarding word-overlap metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Imagined versus Remembered Stories: Quantifying Differences in Narrative Flow. (arXiv:2201.02662v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02662">
<div class="article-summary-box-inner">
<span><p>Lifelong experiences and learned knowledge lead to shared expectations about
how common situations tend to unfold. Such knowledge of narrative event flow
enables people to weave together a story. However, comparable computational
tools to evaluate the flow of events in narratives are limited. We quantify the
differences between autobiographical and imagined stories by introducing
sequentiality, a measure of narrative flow of events, drawing probabilistic
inferences from a cutting-edge large language model (GPT-3). Sequentiality
captures the flow of a narrative by comparing the probability of a sentence
with and without its preceding story context. We applied our measure to study
thousands of diary-like stories, collected from crowdworkers about either a
recent remembered experience or an imagined story on the same topic. The
results show that imagined stories have higher sequentiality than
autobiographical stories and that the sequentiality of autobiographical stories
increases when the memories are retold several months later. In pursuit of
deeper understandings of how sequentiality measures the flow of narratives, we
explore proportions of major and minor events in story sentences, as annotated
by crowdworkers. We find that lower sequentiality is associated with higher
proportions of major events. The methods and results highlight opportunities to
use cutting-edge computational analyses, such as sequentiality, on large
corpora of matched imagined and autobiographical stories to investigate the
influences of memory and reasoning on language generation processes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Linear Adversarial Concept Erasure. (arXiv:2201.12091v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12091">
<div class="article-summary-box-inner">
<span><p>Modern neural models trained on textual data rely on pre-trained
representations that emerge without direct supervision. As these
representations are increasingly being used in real-world applications, the
inability to \emph{control} their content becomes an increasingly important
problem.
</p>
<p>We formulate the problem of identifying and erasing a linear subspace that
corresponds to a given concept, in order to prevent linear predictors from
recovering the concept. We model this problem as a constrained, linear minimax
game, and show that existing solutions are generally not optimal for this task.
We derive a closed-form solution for certain objectives, and propose a convex
relaxation, R-LACE, that works well for others. When evaluated in the context
of binary gender removal, the method recovers a low-dimensional subspace whose
removal mitigates bias by intrinsic and extrinsic evaluation. We show that the
method -- despite being linear -- is highly expressive, effectively mitigating
bias in deep nonlinear classifiers while maintaining tractability and
interpretability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Embarrassingly Simple Performance Prediction for Abductive Natural Language Inference. (arXiv:2202.10408v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.10408">
<div class="article-summary-box-inner">
<span><p>The task of abductive natural language inference (\alpha{}nli), to decide
which hypothesis is the more likely explanation for a set of observations, is a
particularly difficult type of NLI. Instead of just determining a causal
relationship, it requires common sense to also evaluate how reasonable an
explanation is. All recent competitive systems build on top of contextualized
representations and make use of transformer architectures for learning an NLI
model. When somebody is faced with a particular NLI task, they need to select
the best model that is available. This is a time-consuming and resource-intense
endeavour. To solve this practical problem, we propose a simple method for
predicting the performance without actually fine-tuning the model. We do this
by testing how well the pre-trained models perform on the \alpha{}nli task when
just comparing sentence embeddings with cosine similarity to what the
performance that is achieved when training a classifier on top of these
embeddings. We show that the accuracy of the cosine similarity approach
correlates strongly with the accuracy of the classification approach with a
Pearson correlation coefficient of 0.65. Since the similarity computation is
orders of magnitude faster to compute on a given dataset (less than a minute
vs. hours), our method can lead to significant time savings in the process of
model selection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Identification of Toxic Code Reviews Using ToxiCR. (arXiv:2202.13056v2 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13056">
<div class="article-summary-box-inner">
<span><p>Toxic conversations during software development interactions may have serious
repercussions on a Free and Open Source Software (FOSS) development project.
For example, victims of toxic conversations may become afraid to express
themselves, therefore get demotivated, and may eventually leave the project.
Automated filtering of toxic conversations may help a FOSS community to
maintain healthy interactions among its members. However, off-the-shelf
toxicity detectors perform poorly on Software Engineering (SE) dataset, such as
one curated from code review comments. To encounter this challenge, we present
ToxiCR, a supervised learning-based toxicity identification tool for code
review interactions. ToxiCR includes a choice to select one of the ten
supervised learning algorithms, an option to select text vectorization
techniques, eight preprocessing steps, and a large scale labeled dataset of
19,571 code review comments. Two out of those eight preprocessing steps are SE
domain specific. With our rigorous evaluation of the models with various
combinations of preprocessing steps and vectorization techniques, we have
identified the best combination for our dataset that boosts 95.8% accuracy and
88.9% F1 score. ToxiCR significantly outperforms existing toxicity detectors on
our dataset. We have released our dataset, pretrained models, evaluation
results, and source code publicly available at:
https://github.com/WSU-SEAL/ToxiCR
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Recent Advances and Challenges in Reinforcement Learning Methods for Task-Oriented Dialogue Policy Learning. (arXiv:2202.13675v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13675">
<div class="article-summary-box-inner">
<span><p>Dialogue Policy Learning is a key component in a task-oriented dialogue
system (TDS) that decides the next action of the system given the dialogue
state at each turn. Reinforcement Learning (RL) is commonly chosen to learn the
dialogue policy, regarding the user as the environment and the system as the
agent. Many benchmark datasets and algorithms have been created to facilitate
the development and evaluation of dialogue policy based on RL. In this paper,
we survey recent advances and challenges in dialogue policy from the
prescriptive of RL. More specifically, we identify the major problems and
summarize corresponding solutions for RL-based dialogue policy learning.
Besides, we provide a comprehensive survey of applying RL to dialogue policy
learning by categorizing recent methods into basic elements in RL. We believe
this survey can shed a light on future research in dialogue management.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recent, rapid advancement in visual question answering architecture: a review. (arXiv:2203.01322v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01322">
<div class="article-summary-box-inner">
<span><p>Understanding visual question answering is going to be crucial for numerous
human activities. However, it presents major challenges at the heart of the
artificial intelligence endeavor. This paper presents an update on the rapid
advancements in visual question answering using images that have occurred in
the last couple of years. Tremendous growth in research on improving visual
question answering system architecture has been published recently, showing the
importance of multimodal architectures. Several points on the benefits of
visual question answering are mentioned in the review paper by Manmadhan et al.
(2020), on which the present article builds, including subsequent updates in
the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Discriminative Representations and Decision Boundaries for Open Intent Detection. (arXiv:2203.05823v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05823">
<div class="article-summary-box-inner">
<span><p>Open intent detection is a significant problem in natural language
understanding, which aims to detect the unseen open intent with the prior
knowledge of only known intents. Current methods have two core challenges in
this task. On the one hand, they have limitations in learning friendly
representations to detect the open intent. On the other hand, there lacks an
effective approach to obtaining specific and compact decision boundaries for
known intents. To address these issues, this paper introduces an original
framework, DA-ADB, which successively learns distance-aware intent
representations and adaptive decision boundaries for open intent detection.
Specifically, we first leverage distance information to enhance the
distinguishing capability of the intent representations. Then, we design a
novel loss function to obtain appropriate decision boundaries by balancing both
empirical and open space risks. Extensive experiments show the effectiveness of
distance-aware and boundary learning strategies. Compared with the
state-of-the-art methods, our method achieves substantial improvements on three
benchmark datasets. It also yields robust performance with different
proportions of labeled data and known categories. The full data and codes are
available at https://github.com/thuiar/TEXTOIR
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ELLE: Efficient Lifelong Pre-training for Emerging Data. (arXiv:2203.06311v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06311">
<div class="article-summary-box-inner">
<span><p>Current pre-trained language models (PLM) are typically trained with static
data, ignoring that in real-world scenarios, streaming data of various sources
may continuously grow. This requires PLMs to integrate the information from all
the sources in a lifelong manner. Although this goal could be achieved by
exhaustive pre-training on all the existing data, such a process is known to be
computationally expensive. To this end, we propose ELLE, aiming at efficient
lifelong pre-training for emerging data. Specifically, ELLE consists of (1)
function preserved model expansion, which flexibly expands an existing PLM's
width and depth to improve the efficiency of knowledge acquisition; and (2)
pre-trained domain prompts, which disentangle the versatile knowledge learned
during pre-training and stimulate the proper knowledge for downstream tasks. We
experiment ELLE with streaming data from 5 domains on BERT and GPT. The results
show the superiority of ELLE over various lifelong learning baselines in both
pre-training efficiency and downstream performances. The codes are publicly
available at https://github.com/thunlp/ELLE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SpeechPrompt: An Exploration of Prompt Tuning on Generative Spoken Language Model for Speech Processing Tasks. (arXiv:2203.16773v3 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16773">
<div class="article-summary-box-inner">
<span><p>Speech representations learned from Self-supervised learning (SSL) models can
benefit various speech processing tasks. However, utilizing SSL representations
usually requires fine-tuning the pre-trained models or designing task-specific
downstream models and loss functions, causing much memory usage and human
labor. Recently, prompting in Natural Language Processing (NLP) has been found
to be an efficient technique to leverage pre-trained language models (LMs).
Specifically, prompt tuning optimizes a limited number of task-specific
parameters with a fixed pre-trained model; as a result, only a small set of
parameters is needed to be stored for each task. Prompt tuning improves
computation and memory efficiency by leveraging the pre-trained LM's prediction
ability. Nevertheless, such a paradigm is little studied in the speech
community. We report in this paper the first exploration of the prompt tuning
paradigm for speech processing tasks based on Generative Spoken Language Model
(GSLM). Experiment results show that the prompt tuning technique achieves
competitive performance in speech classification tasks with fewer trainable
parameters than fine-tuning specialized downstream models. We further study the
technique in challenging sequence generation tasks. Prompt tuning also
demonstrates its potential, while the limitation and possible research
directions are discussed in this paper. The source code is available on
https://github.com/ga642381/SpeechPrompt.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WavThruVec: Latent speech representation as intermediate features for neural speech synthesis. (arXiv:2203.16930v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16930">
<div class="article-summary-box-inner">
<span><p>Recent advances in neural text-to-speech research have been dominated by
two-stage pipelines utilizing low-level intermediate speech representation such
as mel-spectrograms. However, such predetermined features are fundamentally
limited, because they do not allow to exploit the full potential of a
data-driven approach through learning hidden representations. For this reason,
several end-to-end methods have been proposed. However, such models are harder
to train and require a large number of high-quality recordings with
transcriptions. Here, we propose WavThruVec - a two-stage architecture that
resolves the bottleneck by using high-dimensional Wav2Vec 2.0 embeddings as
intermediate speech representation. Since these hidden activations provide
high-level linguistic features, they are more robust to noise. That allows us
to utilize annotated speech datasets of a lower quality to train the
first-stage module. At the same time, the second-stage component can be trained
on large-scale untranscribed audio corpora, as Wav2Vec 2.0 embeddings are
already time-aligned. This results in an increased generalization capability to
out-of-vocabulary words, as well as to a better generalization to unseen
speakers. We show that the proposed model not only matches the quality of
state-of-the-art neural models, but also presents useful properties enabling
tasks like voice conversion or zero-shot synthesis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge-aware Document Summarization: A Survey of Knowledge, Embedding Methods and Architectures. (arXiv:2204.11190v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.11190">
<div class="article-summary-box-inner">
<span><p>Knowledge-aware methods have boosted a range of natural language processing
applications over the last decades. With the gathered momentum, knowledge
recently has been pumped into enormous attention in document summarization, one
of natural language processing applications. Previous works reported that
knowledge-embedded document summarizers excel at generating superior digests,
especially in terms of informativeness, coherence, and fact consistency. This
paper pursues to present the first systematic survey for the state-of-the-art
methodologies that embed knowledge into document summarizers. Particularly, we
propose novel taxonomies to recapitulate knowledge and knowledge embeddings
under the document summarization view. We further explore how embeddings are
generated in embedding learning architectures of document summarization models,
especially of deep learning models. At last, we discuss the challenges of this
topic and future directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Linear Connectivity Reveals Generalization Strategies. (arXiv:2205.12411v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12411">
<div class="article-summary-box-inner">
<span><p>It is widely accepted in the mode connectivity literature that when two
neural networks are trained similarly on the same data, they are connected by a
path through parameter space over which test set accuracy is maintained. Under
some circumstances, including transfer learning from pretrained models, these
paths are presumed to be linear. In contrast to existing results, we find that
among text classifiers (trained on MNLI, QQP, and CoLA), some pairs of
finetuned models have large barriers of increasing loss on the linear paths
between them. On each task, we find distinct clusters of models which are
linearly connected on the test loss surface, but are disconnected from models
outside the cluster -- models that occupy separate basins on the surface. By
measuring performance on specially-crafted diagnostic datasets, we find that
these clusters correspond to different generalization strategies: one cluster
behaves like a bag of words model under domain shift, while another cluster
uses syntactic heuristics. Our work demonstrates how the geometry of the loss
surface can guide models towards different heuristic functions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Short Math Answer Grading via In-context Meta-learning. (arXiv:2205.15219v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15219">
<div class="article-summary-box-inner">
<span><p>Automatic short answer grading is an important research direction in the
exploration of how to use artificial intelligence (AI)-based tools to improve
education. Current state-of-the-art approaches use neural language models to
create vectorized representations of students responses, followed by
classifiers to predict the score. However, these approaches have several key
limitations, including i) they use pre-trained language models that are not
well-adapted to educational subject domains and/or student-generated text and
ii) they almost always train one model per question, ignoring the linkage
across a question and result in a significant model storage problem due to the
size of advanced language models. In this paper, we study the problem of
automatic short answer grading for students' responses to math questions and
propose a novel framework for this task. First, we use MathBERT, a variant of
the popular language model BERT adapted to mathematical content, as our base
model and fine-tune it for the downstream task of student response grading.
Second, we use an in-context learning approach that provides scoring examples
as input to the language model to provide additional context information and
promote generalization to previously unseen questions. We evaluate our
framework on a real-world dataset of student responses to open-ended math
questions and show that our framework (often significantly) outperforms
existing approaches, especially for new questions that are not seen during
training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Kwame for Science: An AI Teaching Assistant Based on Sentence-BERT for Science Education in West Africa. (arXiv:2206.13703v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.13703">
<div class="article-summary-box-inner">
<span><p>Africa has a high student-to-teacher ratio which limits students' access to
teachers. Consequently, students struggle to get answers to their questions. In
this work, we extended Kwame, our previous AI teaching assistant, adapted it
for science education, and deployed it as a web app. Kwame for Science answers
questions of students based on the Integrated Science subject of the West
African Senior Secondary Certificate Examination (WASSCE). Kwame for Science is
a Sentence-BERT-based question-answering web app that displays 3 paragraphs as
answers along with a confidence score in response to science questions.
Additionally, it displays the top 5 related past exam questions and their
answers in addition to the 3 paragraphs. Our preliminary evaluation of the
Kwame for Science with a 2.5-week real-world deployment showed a top 3 accuracy
of 87.5% (n=56) with 190 users across 11 countries. Kwame for Science will
enable the delivery of scalable, cost-effective, and quality remote education
to millions of people across Africa.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Language Understand Depth?. (arXiv:2207.01077v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01077">
<div class="article-summary-box-inner">
<span><p>Besides image classification, Contrastive Language-Image Pre-training (CLIP)
has accomplished extraordinary success for a wide range of vision tasks,
including object-level and 3D space understanding. However, it's still
challenging to transfer semantic knowledge learned from CLIP into more
intricate tasks of quantified targets, such as depth estimation with geometric
information. In this paper, we propose to apply CLIP for zero-shot monocular
depth estimation, named DepthCLIP. We found that the patches of the input image
could respond to a certain semantic distance token and then be projected to a
quantified depth bin for coarse estimation. Without any training, our DepthCLIP
surpasses existing unsupervised methods and even approaches the early
fully-supervised networks. To our best knowledge, we are the first to conduct
zero-shot adaptation from the semantic language knowledge to quantified
downstream tasks and perform zero-shot monocular depth estimation. We hope our
work could cast a light on future research. The code is available at
https://github.com/Adonis-galaxy/DepthCLIP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiSCoMaT: Distantly Supervised Composition Extraction from Tables in Materials Science Articles. (arXiv:2207.01079v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01079">
<div class="article-summary-box-inner">
<span><p>A crucial component in the curation of KB for a scientific domain is
information extraction from tables in the domain's published articles -- tables
carry important information (often numeric), which must be adequately extracted
for a comprehensive machine understanding of an article. Existing table
extractors assume prior knowledge of table structure and format, which may not
be known in scientific tables. We study a specific and challenging table
extraction problem: extracting compositions of materials (e.g., glasses,
alloys). We first observe that materials science researchers organize similar
compositions in a wide variety of table styles, necessitating an intelligent
model for table understanding and composition extraction. Consequently, we
define this novel task as a challenge for the ML community and create a
training dataset comprising 4,408 distantly supervised tables, along with 1,475
manually annotated dev and test tables. We also present DiSCoMaT, a strong
baseline geared towards this specific task, which combines multiple graph
neural networks with several task-specific regular expressions, features, and
constraints. We show that DiSCoMaT outperforms recent table processing
architectures by significant margins.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantum Circuit Compiler for a Shuttling-Based Trapped-Ion Quantum Computer. (arXiv:2207.01964v2 [quant-ph] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01964">
<div class="article-summary-box-inner">
<span><p>Increasing capabilities of quantum computing hardware and the challenge to
realize deep quantum circuits call for fully automated and efficient tools to
compile quantum circuits. To express arbitrary circuits in a sequence of native
gates pertaining to the specific quantum computer architecture is necessary to
make algorithms portable across the landscape of quantum hardware providers. In
this work, we present a compiler capable of transforming and optimizing a
quantum circuit, targeting a shuttling-based trapped-ion quantum processor. It
consists of custom algorithms set on top of the Cambridge Quantum Computer's
quantum circuit framework Pytket. The performance is evaluated for a wide range
of quantum circuits, showing that the gate counts can be reduced by a factor of
up to 3.6 compared to standard Pytket and up to 2.2 compared to standard Qiskit
compilation, while we achieve similar gate counts as compared to a Pytket
extension targeting the AQT linear-static trapped ion addressing-based
architecture.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">SInGE: Sparsity via Integrated Gradients Estimation of Neuron Relevance. (arXiv:2207.04089v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04089">
<div class="article-summary-box-inner">
<span><p>The leap in performance in state-of-the-art computer vision methods is
attributed to the development of deep neural networks. However it often comes
at a computational price which may hinder their deployment. To alleviate this
limitation, structured pruning is a well known technique which consists in
removing channels, neurons or filters, and is commonly applied in order to
produce more compact models. In most cases, the computations to remove are
selected based on a relative importance criterion. At the same time, the need
for explainable predictive models has risen tremendously and motivated the
development of robust attribution methods that highlight the relative
importance of pixels of an input image or feature map. In this work, we discuss
the limitations of existing pruning heuristics, among which magnitude and
gradient-based methods. We draw inspiration from attribution methods to design
a novel integrated gradient pruning criterion, in which the relevance of each
neuron is defined as the integral of the gradient variation on a path towards
this neuron removal. Furthermore, we propose an entwined DNN pruning and
fine-tuning flowchart to better preserve DNN accuracy while removing
parameters. We show through extensive validation on several datasets,
architectures as well as pruning scenarios that the proposed method, dubbed
SInGE, significantly outperforms existing state-of-the-art DNN pruning methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FAIVConf: Face enhancement for AI-based Video Conference with Low Bit-rate. (arXiv:2207.04090v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04090">
<div class="article-summary-box-inner">
<span><p>Recently, high-quality video conferencing with fewer transmission bits has
become a very hot and challenging problem. We propose FAIVConf, a specially
designed video compression framework for video conferencing, based on the
effective neural human face generation techniques. FAIVConf brings together
several designs to improve the system robustness in real video conference
scenarios: face-swapping to avoid artifacts in background animation; facial
blurring to decrease transmission bit-rate and maintain the quality of
extracted facial landmarks; and dynamic source update for face view
interpolation to accommodate a large range of head poses. Our method achieves a
significant bit-rate reduction in the video conference and gives much better
visual quality under the same bit-rate compared with H.264 and H.265 coding
schemes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StatMix: Data augmentation method that relies on image statistics in federated learning. (arXiv:2207.04103v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04103">
<div class="article-summary-box-inner">
<span><p>Availability of large amount of annotated data is one of the pillars of deep
learning success. Although numerous big datasets have been made available for
research, this is often not the case in real life applications (e.g. companies
are not able to share data due to GDPR or concerns related to intellectual
property rights protection). Federated learning (FL) is a potential solution to
this problem, as it enables training a global model on data scattered across
multiple nodes, without sharing local data itself. However, even FL methods
pose a threat to data privacy, if not handled properly. Therefore, we propose
StatMix, an augmentation approach that uses image statistics, to improve
results of FL scenario(s). StatMix is empirically tested on CIFAR-10 and
CIFAR-100, using two neural network architectures. In all FL experiments,
application of StatMix improves the average accuracy, compared to the baseline
training (with no use of StatMix). Some improvement can also be observed in
non-FL setups.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Systemic Error Detection Methods using Synthetic Images. (arXiv:2207.04104v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04104">
<div class="article-summary-box-inner">
<span><p>We introduce SpotCheck, a framework for generating synthetic datasets to use
for evaluating methods for discovering blindspots (i.e., systemic errors) in
image classifiers. We use SpotCheck to run controlled studies of how various
factors influence the performance of blindspot discovery methods. Our
experiments reveal several shortcomings of existing methods, such as relatively
poor performance in settings with multiple blindspots and sensitivity to
hyperparameters. Further, we find that a method based on dimensionality
reduction, PlaneSpot, is competitive with existing methods, which has promising
implications for the development of interactive tools.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Out of Distribution Detection via Neural Network Anchoring. (arXiv:2207.04125v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04125">
<div class="article-summary-box-inner">
<span><p>Our goal in this paper is to exploit heteroscedastic temperature scaling as a
calibration strategy for out of distribution (OOD) detection.
Heteroscedasticity here refers to the fact that the optimal temperature
parameter for each sample can be different, as opposed to conventional
approaches that use the same value for the entire distribution. To enable this,
we propose a new training strategy called anchoring that can estimate
appropriate temperature values for each sample, leading to state-of-the-art OOD
detection performance across several benchmarks. Using NTK theory, we show that
this temperature function estimate is closely linked to the epistemic
uncertainty of the classifier, which explains its behavior. In contrast to some
of the best-performing OOD detection approaches, our method does not require
exposure to additional outlier datasets, custom calibration objectives, or
model ensembling. Through empirical studies with different OOD detection
settings -- far OOD, near OOD, and semantically coherent OOD - we establish a
highly effective OOD detection approach. Code and models can be accessed here
-- https://github.com/rushilanirudh/AMP
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-view Attention for gestational age at birth prediction. (arXiv:2207.04130v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04130">
<div class="article-summary-box-inner">
<span><p>We present our method for gestational age at birth prediction for the SLCN
(surface learning for clinical neuroimaging) challenge. Our method is based on
a multi-view shape analysis technique that captures 2D renderings of a 3D
object from different viewpoints. We render the brain features on the surface
of the sphere and then the 2D images are analyzed via 2D CNNs and an attention
layer for the regression task. The regression task achieves a MAE of 1.637 +-
1.3 on the Native space and MAE of 1.38 +- 1.14 on the template space. The
source code for this project is available in our github repository
https://github.com/MathieuLeclercq/SLCN_challenge_UNC
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Attention Transformer for Video Interpolation. (arXiv:2207.04132v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04132">
<div class="article-summary-box-inner">
<span><p>We propose TAIN (Transformers and Attention for video INterpolation), a
residual neural network for video interpolation, which aims to interpolate an
intermediate frame given two consecutive image frames around it. We first
present a novel visual transformer module, named Cross-Similarity (CS), to
globally aggregate input image features with similar appearance as those of the
predicted interpolated frame. These CS features are then used to refine the
interpolated prediction. To account for occlusions in the CS features, we
propose an Image Attention (IA) module to allow the network to focus on CS
features from one frame over those of the other. Additionally, we augment our
training dataset with an occluder patch that moves across frames to improve the
network's robustness to occlusions and large motion. Because existing methods
yield smooth predictions especially near MBs, we use an additional training
loss based on image gradient to yield sharper predictions. TAIN outperforms
existing methods that do not require flow estimation and performs comparably to
flow-based methods while being computationally efficient in terms of inference
time on Vimeo90k, UCF101, and SNU-FILM benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">L$_0$onie: Compressing COINs with L$_0$-constraints. (arXiv:2207.04144v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04144">
<div class="article-summary-box-inner">
<span><p>Advances in Implicit Neural Representations (INR) have motivated research on
domain-agnostic compression techniques. These methods train a neural network to
approximate an object, and then store the weights of the trained model. For
example, given an image, a network is trained to learn the mapping from pixel
locations to RGB values. In this paper, we propose L$_0$onie, a
sparsity-constrained extension of the COIN compression method. Sparsity allows
to leverage the faster learning of overparameterized networks, while retaining
the desirable compression rate of smaller models. Moreover, our constrained
formulation ensures that the final model respects a pre-determined compression
rate, dispensing of the need for expensive architecture search.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Task-Based Machine Learning Content Extraction Services for VIDINT. (arXiv:2207.04158v1 [cs.ET])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04158">
<div class="article-summary-box-inner">
<span><p>This paper provides a comparison of current video content extraction tools
with a focus on comparing commercial task-based machine learning services.
Video intelligence (VIDINT) data has become a critical intelligence source in
the past decade. The need for AI-based analytics and automation tools to
extract and structure content from video has quickly become a priority for
organizations needing to search, analyze and exploit video at scale. With rapid
growth in machine learning technology, the maturity of machine transcription,
machine translation, topic tagging, and object recognition tasks are improving
at an exponential rate, breaking performance records in speed and accuracy as
new applications evolve. Each section of this paper reviews and compares
products, software resources and video analytics capabilities based on tasks
relevant to extracting information from video with machine learning techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few 'Zero Level Set'-Shot Learning of Shape Signed Distance Functions in Feature Space. (arXiv:2207.04161v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04161">
<div class="article-summary-box-inner">
<span><p>We explore a new idea for learning based shape reconstruction from a point
cloud, based on the recently popularized implicit neural shape representations.
We cast the problem as a few-shot learning of implicit neural signed distance
functions in feature space, that we approach using gradient based
meta-learning. We use a convolutional encoder to build a feature space given
the input point cloud. An implicit decoder learns to predict signed distance
values given points represented in this feature space. Setting the input point
cloud, i.e. samples from the target shape function's zero level set, as the
support (i.e. context) in few-shot learning terms, we train the decoder such
that it can adapt its weights to the underlying shape of this context with a
few (5) tuning steps. We thus combine two types of implicit neural network
conditioning mechanisms simultaneously for the first time, namely feature
encoding and meta-learning. Our numerical and qualitative evaluation shows that
in the context of implicit reconstruction from a sparse point cloud, our
proposed strategy, i.e. meta-learning in feature space, outperforms existing
alternatives, namely standard supervised learning in feature space, and
meta-learning in euclidean space, while still providing fast inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Multimodal Vision-Language Models Generating Non-Generic Text. (arXiv:2207.04174v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04174">
<div class="article-summary-box-inner">
<span><p>Vision-language models can assess visual context in an image and generate
descriptive text. While the generated text may be accurate and syntactically
correct, it is often overly general. To address this, recent work has used
optical character recognition to supplement visual information with text
extracted from an image. In this work, we contend that vision-language models
can benefit from additional information that can be extracted from an image,
but are not used by current models. We modify previous multimodal frameworks to
accept relevant information from any number of auxiliary classifiers. In
particular, we focus on person names as an additional set of tokens and create
a novel image-caption dataset to facilitate captioning with person names. The
dataset, Politicians and Athletes in Captions (PAC), consists of captioned
images of well-known people in context. By fine-tuning pretrained models with
this dataset, we demonstrate a model that can naturally integrate facial
recognition tokens into generated text by training on limited data. For the PAC
dataset, we provide a discussion on collection and baseline benchmark scores.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Direct Handheld Burst Imaging to Simulated Defocus. (arXiv:2207.04175v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04175">
<div class="article-summary-box-inner">
<span><p>A shallow depth-of-field image keeps the subject in focus, and the foreground
and background contexts blurred. This effect requires much larger lens
apertures than those of smartphone cameras. Conventional methods acquire RGB-D
images and blur image regions based on their depth. However, this approach is
not suitable for reflective or transparent surfaces, or finely detailed object
silhouettes, where the depth value is inaccurate or ambiguous.
</p>
<p>We present a learning-based method to synthesize the defocus blur in shallow
depth-of-field images from handheld bursts acquired with a single small
aperture lens. Our deep learning model directly produces the shallow
depth-of-field image, avoiding explicit depth-based blurring. The simulated
aperture diameter equals the camera translation during burst acquisition. Our
method does not suffer from artifacts due to inaccurate or ambiguous depth
estimation, and it is well-suited to portrait photography.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Robust Representation for Joint Grading of Ophthalmic Diseases via Adaptive Curriculum and Feature Disentanglement. (arXiv:2207.04183v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04183">
<div class="article-summary-box-inner">
<span><p>Diabetic retinopathy (DR) and diabetic macular edema (DME) are leading causes
of permanent blindness worldwide. Designing an automatic grading system with
good generalization ability for DR and DME is vital in clinical practice.
However, prior works either grade DR or DME independently, without considering
internal correlations between them, or grade them jointly by shared feature
representation, yet ignoring potential generalization issues caused by
difficult samples and data bias. Aiming to address these problems, we propose a
framework for joint grading with the dynamic difficulty-aware weighted loss
(DAW) and the dual-stream disentangled learning architecture (DETACH). Inspired
by curriculum learning, DAW learns from simple samples to difficult samples
dynamically via measuring difficulty adaptively. DETACH separates features of
grading tasks to avoid potential emphasis on the bias. With the addition of DAW
and DETACH, the model learns robust disentangled feature representations to
explore internal correlations between DR and DME and achieve better grading
performance. Experiments on three benchmarks show the effectiveness and
robustness of our framework under both the intra-dataset and cross-dataset
tests.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Alignment Meets Fully Test-Time Adaptation. (arXiv:2207.04185v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04185">
<div class="article-summary-box-inner">
<span><p>A foundational requirement of a deployed ML model is to generalize to data
drawn from a testing distribution that is different from training. A popular
solution to this problem is to adapt a pre-trained model to novel domains using
only unlabeled data. In this paper, we focus on a challenging variant of this
problem, where access to the original source data is restricted. While fully
test-time adaptation (FTTA) and unsupervised domain adaptation (UDA) are
closely related, the advances in UDA are not readily applicable to TTA, since
most UDA methods require access to the source data. Hence, we propose a new
approach, CATTAn, that bridges UDA and FTTA, by relaxing the need to access
entire source data, through a novel deep subspace alignment strategy. With a
minimal overhead of storing the subspace basis set for the source data, CATTAn
enables unsupervised alignment between source and target data during
adaptation. Through extensive experimental evaluation on multiple 2D and 3D
vision benchmarks (ImageNet-C, Office-31, OfficeHome, DomainNet, PointDA-10)
and model architectures, we demonstrate significant gains in FTTA performance.
Furthermore, we make a number of crucial findings on the utility of the
alignment objective even with inherently robust models, pre-trained ViT
representations and under low sample availability in the target domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Study on Self-Supervised Object Detection Pretraining. (arXiv:2207.04186v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04186">
<div class="article-summary-box-inner">
<span><p>In this work, we study different approaches to self-supervised pretraining of
object detection models. We first design a general framework to learn a
spatially consistent dense representation from an image, by randomly sampling
and projecting boxes to each augmented view and maximizing the similarity
between corresponding box features. We study existing design choices in the
literature, such as box generation, feature extraction strategies, and using
multiple views inspired by its success on instance-level image representation
learning techniques. Our results suggest that the method is robust to different
choices of hyperparameters, and using multiple views is not as effective as
shown for instance-level image representation learning. We also design two
auxiliary tasks to predict boxes in one view from their features in the other
view, by (1) predicting boxes from the sampled set by using a contrastive loss,
and (2) predicting box coordinates using a transformer, which potentially
benefits downstream object detection tasks. We found that these tasks do not
lead to better object detection performance when finetuning the pretrained
model on labeled data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Structured Representations of Visual Scenes. (arXiv:2207.04200v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04200">
<div class="article-summary-box-inner">
<span><p>As the intermediate-level representations bridging the two levels, structured
representations of visual scenes, such as visual relationships between pairwise
objects, have been shown to not only benefit compositional models in learning
to reason along with the structures but provide higher interpretability for
model decisions. Nevertheless, these representations receive much less
attention than traditional recognition tasks, leaving numerous open challenges
unsolved. In the thesis, we study how machines can describe the content of the
individual image or video with visual relationships as the structured
representations. Specifically, we explore how structured representations of
visual scenes can be effectively constructed and learned in both the
static-image and video settings, with improvements resulting from external
knowledge incorporation, bias-reducing mechanism, and enhanced representation
models. At the end of this thesis, we also discuss some open challenges and
limitations to shed light on future directions of structured representation
learning for visual scenes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Smart Multi-tenant Federated Learning. (arXiv:2207.04202v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04202">
<div class="article-summary-box-inner">
<span><p>Federated learning (FL) is an emerging distributed machine learning method
that empowers in-situ model training on decentralized edge devices. However,
multiple simultaneous training activities could overload resource-constrained
devices. In this work, we propose a smart multi-tenant FL system, MuFL, to
effectively coordinate and execute simultaneous training activities. We first
formalize the problem of multi-tenant FL, define multi-tenant FL scenarios, and
introduce a vanilla multi-tenant FL system that trains activities sequentially
to form baselines. Then, we propose two approaches to optimize multi-tenant FL:
1) activity consolidation merges training activities into one activity with a
multi-task architecture; 2) after training it for rounds, activity splitting
divides it into groups by employing affinities among activities such that
activities within a group have better synergy. Extensive experiments
demonstrate that MuFL outperforms other methods while consuming 40% less
energy. We hope this work will inspire the community to further study and
optimize multi-tenant FL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Variational Approach for Intensity Domain Multi-exposure Image Fusion. (arXiv:2207.04204v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04204">
<div class="article-summary-box-inner">
<span><p>Recent innovations shows that blending of details captured by single Low
Dynamic Range (LDR) sensor overcomes the limitations of standard digital
cameras to capture details from high dynamic range scene. We present a method
to produce well-exposed fused image that can be displayed directly on
conventional display devices. The ambition is to preserve details in poorly
illuminated and brightly illuminated regions. Proposed approach does not
require true radiance reconstruction and tone manipulation steps. The aforesaid
objective is achieved by taking into account local information measure that
select well-exposed regions across input exposures. In addition, Contrast
Limited Adaptive Histogram equalization (CLAHE) is introduced to improve
uniformity of input multi-exposure image prior to fusion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BOSS: Bottom-up Cross-modal Semantic Composition with Hybrid Counterfactual Training for Robust Content-based Image Retrieval. (arXiv:2207.04211v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04211">
<div class="article-summary-box-inner">
<span><p>Content-Based Image Retrieval (CIR) aims to search for a target image by
concurrently comprehending the composition of an example image and a
complementary text, which potentially impacts a wide variety of real-world
applications, such as internet search and fashion retrieval. In this scenario,
the input image serves as an intuitive context and background for the search,
while the corresponding language expressly requests new traits on how specific
characteristics of the query image should be modified in order to get the
intended target image. This task is challenging since it necessitates learning
and understanding the composite image-text representation by incorporating
cross-granular semantic updates. In this paper, we tackle this task by a novel
\underline{\textbf{B}}ottom-up cr\underline{\textbf{O}}ss-modal
\underline{\textbf{S}}emantic compo\underline{\textbf{S}}ition (\textbf{BOSS})
with Hybrid Counterfactual Training framework, which sheds new light on the CIR
task by studying it from two previously overlooked perspectives:
\emph{implicitly bottom-up composition of visiolinguistic representation} and
\emph{explicitly fine-grained correspondence of query-target construction}. On
the one hand, we leverage the implicit interaction and composition of
cross-modal embeddings from the bottom local characteristics to the top global
semantics, preserving and transforming the visual representation conditioned on
language semantics in several continuous steps for effective target image
search. On the other hand, we devise a hybrid counterfactual training strategy
that can reduce the model's ambiguity for similar queries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COVID-19 Disease Identification on Chest-CT images using CNN and VGG16. (arXiv:2207.04212v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04212">
<div class="article-summary-box-inner">
<span><p>A newly identified coronavirus disease called COVID-19 mainly affects the
human respiratory system. COVID-19 is an infectious disease caused by a virus
originating in Wuhan, China, in December 2019. Early diagnosis is the primary
challenge of health care providers. In the earlier stage, medical organizations
were dazzled because there were no proper health aids or medicine to detect a
COVID-19. A new diagnostic tool RT-PCR (Reverse Transcription Polymerase Chain
Reaction), was introduced. It collects swab specimens from the patient's nose
or throat, where the COVID-19 virus gathers. This method has some limitations
related to accuracy and testing time. Medical experts suggest an alternative
approach called CT (Computed Tomography) that can quickly diagnose the infected
lung areas and identify the COVID-19 in an earlier stage. Using chest CT
images, computer researchers developed several deep learning models identifying
the COVID-19 disease. This study presents a Convolutional Neural Network (CNN)
and VGG16-based model for automated COVID-19 identification on chest CT images.
The experimental results using a public dataset of 14320 CT images showed a
classification accuracy of 96.34% and 96.99% for CNN and VGG16, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual-path Attention is All You Need for Audio-Visual Speech Extraction. (arXiv:2207.04213v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04213">
<div class="article-summary-box-inner">
<span><p>Audio-visual target speech extraction, which aims to extract a certain
speaker's speech from the noisy mixture by looking at lip movements, has made
significant progress combining time-domain speech separation models and visual
feature extractors (CNN). One problem of fusing audio and video information is
that they have different time resolutions. Most current research upsamples the
visual features along the time dimension so that audio and video features are
able to align in time. However, we believe that lip movement should mostly
contain long-term, or phone-level information. Based on this assumption, we
propose a new way to fuse audio-visual features. We observe that for DPRNN
\cite{dprnn}, the interchunk dimension's time resolution could be very close to
the time resolution of video frames. Like \cite{sepformer}, the LSTM in DPRNN
is replaced by intra-chunk and inter-chunk self-attention, but in the proposed
algorithm, inter-chunk attention incorporates the visual features as an
additional feature stream. This prevents the upsampling of visual cues,
resulting in more efficient audio-visual fusion. The result shows we achieve
superior results compared with other time-domain based audio-visual fusion
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Persistent Homology for Visual Recognition. (arXiv:2207.04220v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04220">
<div class="article-summary-box-inner">
<span><p>Persistent topological properties of an image serve as an additional
descriptor providing an insight that might not be discovered by traditional
neural networks. The existing research in this area focuses primarily on
efficiently integrating topological properties of the data in the learning
process in order to enhance the performance. However, there is no existing
study to demonstrate all possible scenarios where introducing topological
properties can boost or harm the performance. This paper performs a detailed
analysis of the effectiveness of topological properties for image
classification in various training scenarios, defined by: the number of
training samples, the complexity of the training data and the complexity of the
backbone network. We identify the scenarios that benefit the most from
topological features, e.g., training simple networks on small datasets.
Additionally, we discuss the problem of topological consistency of the datasets
which is one of the major bottlenecks for using topological features for
classification. We further demonstrate how the topological inconsistency can
harm the performance for certain scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Register Unbalanced Point Pairs. (arXiv:2207.04221v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04221">
<div class="article-summary-box-inner">
<span><p>Recent 3D registration methods can effectively handle large-scale or
partially overlapping point pairs. However, despite its practicality, matching
the unbalanced pairs in terms of spatial scale and density has been overlooked.
We present a novel 3D registration method, called UPPNet, for the unbalanced
point pairs. We propose a hierarchical framework to find inlier correspondences
effectively by gradually reducing search space. Our method predicts the
subregions of the target points likely to be overlapped with the query points.
The following super-point matching module and fine-grained refinement module
estimate accurate inlier correspondences between two point clouds. Furthermore,
we apply geometric constraints to refine the correspondences that satisfy
spatial compatibility. Correspondence prediction is trained end-to-end, and our
approach can predict the proper rigid transformation with a single forward pass
given unbalanced point cloud pairs. To validate the efficacy of the proposed
method, we create a KITTI-UPP dataset by augmenting the KITTI LiDAR dataset.
Experiments on this dataset reveal that the proposed approach significantly
outperforms state-of-the-art pairwise point cloud registration methods by a
large margin, resulting in 78% improvement in Registration Recall when the
target point cloud is about 10$\times$ spatially larger and about 10$\times$
times denser than the query point cloud.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SiaTrans: Siamese Transformer Network for RGB-D Salient Object Detection with Depth Image Classification. (arXiv:2207.04224v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04224">
<div class="article-summary-box-inner">
<span><p>RGB-D SOD uses depth information to handle challenging scenes and obtain
high-quality saliency maps. Existing state-of-the-art RGB-D saliency detection
methods overwhelmingly rely on the strategy of directly fusing depth
information. Although these methods improve the accuracy of saliency prediction
through various cross-modality fusion strategies, misinformation provided by
some poor-quality depth images can affect the saliency prediction result. To
address this issue, a novel RGB-D salient object detection model (SiaTrans) is
proposed in this paper, which allows training on depth image quality
classification at the same time as training on SOD. In light of the common
information between RGB and depth images on salient objects, SiaTrans uses a
Siamese transformer network with shared weight parameters as the encoder and
extracts RGB and depth features concatenated on the batch dimension, saving
space resources without compromising performance. SiaTrans uses the Class token
in the backbone network (T2T-ViT) to classify the quality of depth images
without preventing the token sequence from going on with the saliency detection
task. Transformer-based cross-modality fusion module (CMF) can effectively fuse
RGB and depth information. And in the testing process, CMF can choose to fuse
cross-modality information or enhance RGB information according to the quality
classification signal of the depth image. The greatest benefit of our designed
CMF and decoder is that they maintain the consistency of RGB and RGB-D
information decoding: SiaTrans decodes RGB-D or RGB information under the same
model parameters according to the classification signal during testing.
Comprehensive experiments on nine RGB-D SOD benchmark datasets show that
SiaTrans has the best overall performance and the least computation compared
with recent state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Batch-efficient EigenDecomposition for Small and Medium Matrices. (arXiv:2207.04228v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04228">
<div class="article-summary-box-inner">
<span><p>EigenDecomposition (ED) is at the heart of many computer vision algorithms
and applications. One crucial bottleneck limiting its usage is the expensive
computation cost, particularly for a mini-batch of matrices in the deep neural
networks. In this paper, we propose a QR-based ED method dedicated to the
application scenarios of computer vision. Our proposed method performs the ED
entirely by batched matrix/vector multiplication, which processes all the
matrices simultaneously and thus fully utilizes the power of GPUs. Our
technique is based on the explicit QR iterations by Givens rotation with double
Wilkinson shifts. With several acceleration techniques, the time complexity of
QR iterations is reduced from $O{(}n^5{)}$ to $O{(}n^3{)}$. The numerical test
shows that for small and medium batched matrices (\emph{e.g.,} $dim{&lt;}32$) our
method can be much faster than the Pytorch SVD function. Experimental results
on visual recognition and image generation demonstrate that our methods also
achieve competitive performances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse Ellipsometry: Portable Acquisition of Polarimetric SVBRDF and Shape with Unstructured Flash Photography. (arXiv:2207.04236v1 [cs.GR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04236">
<div class="article-summary-box-inner">
<span><p>Ellipsometry techniques allow to measure polarization information of
materials, requiring precise rotations of optical components with different
configurations of lights and sensors. This results in cumbersome capture
devices, carefully calibrated in lab conditions, and in very long acquisition
times, usually in the order of a few days per object. Recent techniques allow
to capture polarimetric spatially-varying reflectance information, but limited
to a single view, or to cover all view directions, but limited to spherical
objects made of a single homogeneous material. We present sparse ellipsometry,
a portable polarimetric acquisition method that captures both polarimetric
SVBRDF and 3D shape simultaneously. Our handheld device consists of
off-the-shelf, fixed optical components. Instead of days, the total acquisition
time varies between twenty and thirty minutes per object. We develop a complete
polarimetric SVBRDF model that includes diffuse and specular components, as
well as single scattering, and devise a novel polarimetric inverse rendering
algorithm with data augmentation of specular reflection samples via generative
modeling. Our results show a strong agreement with a recent ground-truth
dataset of captured polarimetric BRDFs of real-world objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PI-Trans: Parallel-ConvMLP and Implicit-Transformation Based GAN for Cross-View Image Translation. (arXiv:2207.04242v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04242">
<div class="article-summary-box-inner">
<span><p>For semantic-guided cross-view image translation, it is crucial to learn
where to sample pixels from the source view image and where to reallocate them
guided by the target view semantic map, especially when there is little overlap
or drastic view difference between the source and target images. Hence, one not
only needs to encode the long-range dependencies among pixels in both the
source view image and target view the semantic map but also needs to translate
these learned dependencies. To this end, we propose a novel generative
adversarial network, PI-Trans, which mainly consists of a novel
Parallel-ConvMLP module and an Implicit Transformation module at multiple
semantic levels. Extensive experimental results show that the proposed PI-Trans
achieves the best qualitative and quantitative performance by a large margin
compared to the state-of-the-art methods on two challenging datasets. The code
will be made available at https://github.com/Amazingren/PI-Trans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving saliency models' predictions of the next fixation with humans' intrinsic cost of gaze shifts. (arXiv:2207.04250v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04250">
<div class="article-summary-box-inner">
<span><p>The human prioritization of image regions can be modeled in a time invariant
fashion with saliency maps or sequentially with scanpath models. However, while
both types of models have steadily improved on several benchmarks and datasets,
there is still a considerable gap in predicting human gaze. Here, we leverage
two recent developments to reduce this gap: theoretical analyses establishing a
principled framework for predicting the next gaze target and the empirical
measurement of the human cost for gaze switches independently of image content.
We introduce an algorithm in the framework of sequential decision making, which
converts any static saliency map into a sequence of dynamic history-dependent
value maps, which are recomputed after each gaze shift. These maps are based on
1) a saliency map provided by an arbitrary saliency model, 2) the recently
measured human cost function quantifying preferences in magnitude and direction
of eye movements, and 3) a sequential exploration bonus, which changes with
each subsequent gaze shift. The parameters of the spatial extent and temporal
decay of this exploration bonus are estimated from human gaze data. The
relative contributions of these three components were optimized on the MIT1003
dataset for the NSS score and are sufficient to significantly outperform
predictions of the next gaze target on NSS and AUC scores for five state of the
art saliency models on three image data sets. Thus, we provide an
implementation of human gaze preferences, which can be used to improve
arbitrary saliency models' predictions of humans' next gaze targets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rank-Enhanced Low-Dimensional Convolution Set for Hyperspectral Image Denoising. (arXiv:2207.04266v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04266">
<div class="article-summary-box-inner">
<span><p>This paper tackles the challenging problem of hyperspectral (HS) image
denoising. Unlike existing deep learning-based methods usually adopting
complicated network architectures or empirically stacking off-the-shelf modules
to pursue performance improvement, we focus on the efficient and effective
feature extraction manner for capturing the high-dimensional characteristics of
HS images. To be specific, based on the theoretical analysis that increasing
the rank of the matrix formed by the unfolded convolutional kernels can promote
feature diversity, we propose rank-enhanced low-dimensional convolution set
(Re-ConvSet), which separately performs 1-D convolution along the three
dimensions of an HS image side-by-side, and then aggregates the resulting
spatial-spectral embeddings via a learnable compression layer. Re-ConvSet not
only learns the diverse spatial-spectral features of HS images, but also
reduces the parameters and complexity of the network. We then incorporate
Re-ConvSet into the widely-used U-Net architecture to construct an HS image
denoising method. Surprisingly, we observe such a concise framework outperforms
the most recent method to a large extent in terms of quantitative metrics,
visual results, and efficiency. We believe our work may shed light on deep
learning-based HS image processing and analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explainable AI (XAI) in Biomedical Signal and Image Processing: Promises and Challenges. (arXiv:2207.04295v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04295">
<div class="article-summary-box-inner">
<span><p>Artificial intelligence has become pervasive across disciplines and fields,
and biomedical image and signal processing is no exception. The growing and
widespread interest on the topic has triggered a vast research activity that is
reflected in an exponential research effort. Through study of massive and
diverse biomedical data, machine and deep learning models have revolutionized
various tasks such as modeling, segmentation, registration, classification and
synthesis, outperforming traditional techniques. However, the difficulty in
translating the results into biologically/clinically interpretable information
is preventing their full exploitation in the field. Explainable AI (XAI)
attempts to fill this translational gap by providing means to make the models
interpretable and providing explanations. Different solutions have been
proposed so far and are gaining increasing interest from the community. This
paper aims at providing an overview on XAI in biomedical data processing and
points to an upcoming Special Issue on Deep Learning in Biomedical Image and
Signal Processing of the IEEE Signal Processing Magazine that is going to
appear in March 2022.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SHDM-NET: Heat Map Detail Guidance with Image Matting for Industrial Weld Semantic Segmentation Network. (arXiv:2207.04297v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04297">
<div class="article-summary-box-inner">
<span><p>In actual industrial production, the assessment of the steel plate welding
effect is an important task, and the segmentation of the weld section is the
basis of the assessment. This paper proposes an industrial weld segmentation
network based on a deep learning semantic segmentation algorithm fused with
heatmap detail guidance and Image Matting to solve the automatic segmentation
problem of weld regions. In the existing semantic segmentation networks, the
boundary information can be preserved by fusing the features of both high-level
and low-level layers. However, this method can lead to insufficient expression
of the spatial information in the low-level layer, resulting in inaccurate
segmentation boundary positioning. We propose a detailed guidance module based
on heatmaps to fully express the segmented region boundary information in the
low-level network to address this problem. Specifically, the expression of
boundary information can be enhanced by adding a detailed branch to predict
segmented boundary and then matching it with the boundary heat map generated by
mask labels to calculate the mean square error loss. In addition, although deep
learning has achieved great success in the field of semantic segmentation, the
precision of the segmentation boundary region is not high due to the loss of
detailed information caused by the classical segmentation network in the
process of encoding and decoding process. This paper introduces a matting
algorithm to calibrate the boundary of the segmentation region of the semantic
segmentation network to solve this problem. Through many experiments on
industrial weld data sets, the effectiveness of our method is demonstrated, and
the MIOU reaches 97.93%. It is worth noting that this performance is comparable
to human manual segmentation ( MIOU 97.96%).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QKVA grid: Attention in Image Perspective and Stacked DETR. (arXiv:2207.04313v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04313">
<div class="article-summary-box-inner">
<span><p>We present a new model named Stacked-DETR(SDETR), which inherits the main
ideas in canonical DETR. We improve DETR in two directions: simplifying the
cost of training and introducing the stacked architecture to enhance the
performance. To the former, we focus on the inside of the Attention block and
propose the QKVA grid, a new perspective to describe the process of attention.
By this, we can step further on how Attention works for image problems and the
effect of multi-head. These two ideas contribute the design of single-head
encoder-layer. To the latter, SDETR reaches great improvement(+1.1AP, +3.4APs)
to DETR. Especially to the performance on small objects, SDETR achieves better
results to the optimized Faster R-CNN baseline, which was a shortcoming in
DETR. Our changes are based on the code of DETR. Training code and pretrained
models are available at https://github.com/shengwenyuan/sdetr.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Diffusion Model Efficiency Through Patching. (arXiv:2207.04316v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04316">
<div class="article-summary-box-inner">
<span><p>Diffusion models are a powerful class of generative models that iteratively
denoise samples to produce data. While many works have focused on the number of
iterations in this sampling procedure, few have focused on the cost of each
iteration. We find that adding a simple ViT-style patching transformation can
considerably reduce a diffusion model's sampling time and memory usage. We
justify our approach both through an analysis of the diffusion model objective,
and through empirical experiments on LSUN Church, ImageNet 256, and FFHQ 1024.
We provide implementations in Tensorflow and Pytorch.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Snipper: A Spatiotemporal Transformer for Simultaneous Multi-Person 3D Pose Estimation Tracking and Forecasting on a Video Snippet. (arXiv:2207.04320v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04320">
<div class="article-summary-box-inner">
<span><p>Multi-person pose understanding from RGB videos includes three complex tasks:
pose estimation, tracking and motion forecasting. Among these three tasks, pose
estimation and tracking are correlated, and tracking is crucial to motion
forecasting. Most existing works either focus on a single task or employ
cascaded methods to solve each individual task separately. In this paper, we
propose Snipper, a framework to perform multi-person 3D pose estimation,
tracking and motion forecasting simultaneously in a single inference.
Specifically, we first propose a deformable attention mechanism to aggregate
spatiotemporal information from video snippets. Building upon this deformable
attention, a visual transformer is learned to encode the spatiotemporal
features from multi-frame images and to decode informative pose features to
update multi-person pose queries. Last, these queries are regressed to predict
multi-person pose trajectories and future motions in one forward pass. In the
experiments, we show the effectiveness of Snipper on three challenging public
datasets where a generic model rivals specialized state-of-art baselines for
pose estimation, tracking, and forecasting. Code is available at
\href{https://github.com/JimmyZou/Snipper}{https://github.com/JimmyZou/Snipper}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Coding Using Learned Latent GAN Compression. (arXiv:2207.04324v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04324">
<div class="article-summary-box-inner">
<span><p>We propose in this paper a new paradigm for facial video compression. We
leverage the generative capacity of GANs such as StyleGAN to represent and
compress a video, including intra and inter compression. Each frame is inverted
in the latent space of StyleGAN, from which the optimal compression is learned.
To do so, a diffeomorphic latent representation is learned using a normalizing
flows model, where an entropy model can be optimized for image coding. In
addition, we propose a new perceptual loss that is more efficient than other
counterparts. Finally, an entropy model for video inter coding with residual is
also learned in the previously constructed latent representation. Our method
(SGANC) is simple, faster to train, and achieves better results for image and
video coding compared to state-of-the-art codecs such as VTM, AV1, and recent
deep learning techniques. In particular, it drastically minimizes perceptual
distortion at low bit rates.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Joint Image Transfer and Uncertainty Quantification using Patch Invariant Networks. (arXiv:2207.04325v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04325">
<div class="article-summary-box-inner">
<span><p>Unsupervised image transfer enables intra- and inter-modality transfer for
medical applications where a large amount of paired training data is not
abundant. To ensure a structure-preserving mapping from the input to the target
domain, existing methods for unpaired medical image transfer are commonly based
on cycle-consistency, causing additional computation resources and instability
due to the learning of an inverse mapping. This paper presents a novel method
for uni-directional domain mapping where no paired data is needed throughout
the entire training process. A reasonable transfer is ensured by employing the
GAN architecture and a novel generator loss based on patch invariance. To be
more precise, generator outputs are evaluated and compared on different scales,
which brings increased attention to high-frequency details as well as implicit
data augmentation. This novel term also gives the opportunity to predict
aleatoric uncertainty by modeling an input-dependent scale map for the patch
residuals. The proposed method is comprehensively evaluated on three renowned
medical databases. Superior accuracy on these datasets compared to four
different state-of-the-art methods for unpaired image transfer suggests the
great potential of this approach for uncertainty-aware medical image
translation. Implementation of the proposed framework is released here:
https://github.com/anger-man/unsupervised-image-transfer-and-uq.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explaining Chest X-ray Pathologies in Natural Language. (arXiv:2207.04343v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04343">
<div class="article-summary-box-inner">
<span><p>Most deep learning algorithms lack explanations for their predictions, which
limits their deployment in clinical practice. Approaches to improve
explainability, especially in medical imaging, have often been shown to convey
limited information, be overly reassuring, or lack robustness. In this work, we
introduce the task of generating natural language explanations (NLEs) to
justify predictions made on medical images. NLEs are human-friendly and
comprehensive, and enable the training of intrinsically explainable models. To
this goal, we introduce MIMIC-NLE, the first, large-scale, medical imaging
dataset with NLEs. It contains over 38,000 NLEs, which explain the presence of
various thoracic pathologies and chest X-ray findings. We propose a general
approach to solve the task and evaluate several architectures on this dataset,
including via clinician assessment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Segmentation of Blood Vessels, Optic Disc Localization, Detection of Exudates and Diabetic Retinopathy Diagnosis from Digital Fundus Images. (arXiv:2207.04345v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04345">
<div class="article-summary-box-inner">
<span><p>Diabetic Retinopathy (DR) is a complication of long-standing, unchecked
diabetes and one of the leading causes of blindness in the world. This paper
focuses on improved and robust methods to extract some of the features of DR,
viz. Blood Vessels and Exudates. Blood vessels are segmented using multiple
morphological and thresholding operations. For the segmentation of exudates,
k-means clustering and contour detection on the original images are used.
Extensive noise reduction is performed to remove false positives from the
vessel segmentation algorithm's results. The localization of Optic Disc using
k-means clustering and template matching is also performed. Lastly, this paper
presents a Deep Convolutional Neural Network (DCNN) model with 14 Convolutional
Layers and 2 Fully Connected Layers, for the automatic, binary diagnosis of DR.
The vessel segmentation, optic disc localization and DCNN achieve accuracies of
95.93%, 98.77% and 75.73% respectively. The source code and pre-trained model
are available https://github.com/Sohambasu07/DR_2021
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Radiomics-Guided Global-Local Transformer for Weakly Supervised Pathology Localization in Chest X-Rays. (arXiv:2207.04394v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04394">
<div class="article-summary-box-inner">
<span><p>Before the recent success of deep learning methods for automated medical
image analysis, practitioners used handcrafted radiomic features to
quantitatively describe local patches of medical images. However, extracting
discriminative radiomic features relies on accurate pathology localization,
which is difficult to acquire in real-world settings. Despite advances in
disease classification and localization from chest X-rays, many approaches fail
to incorporate clinically-informed domain knowledge. For these reasons, we
propose a Radiomics-Guided Transformer (RGT) that fuses \textit{global} image
information with \textit{local} knowledge-guided radiomics information to
provide accurate cardiopulmonary pathology localization and classification
\textit{without any bounding box annotations}. RGT consists of an image
Transformer branch, a radiomics Transformer branch, and fusion layers that
aggregate image and radiomic information. Using the learned self-attention of
its image branch, RGT extracts a bounding box for which to compute radiomic
features, which are further processed by the radiomics branch; learned image
and radiomic features are then fused and mutually interact via cross-attention
layers. Thus, RGT utilizes a novel end-to-end feedback loop that can bootstrap
accurate pathology localization only using image-level disease labels.
Experiments on the NIH ChestXRay dataset demonstrate that RGT outperforms prior
works in weakly supervised disease localization (by an average margin of 3.6\%
over various intersection-over-union thresholds) and classification (by 1.1\%
in average area under the receiver operating characteristic curve). Code and
trained models will be released upon acceptance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">2DPASS: 2D Priors Assisted Semantic Segmentation on LiDAR Point Clouds. (arXiv:2207.04397v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04397">
<div class="article-summary-box-inner">
<span><p>As camera and LiDAR sensors capture complementary information used in
autonomous driving, great efforts have been made to develop semantic
segmentation algorithms through multi-modality data fusion. However,
fusion-based approaches require paired data, i.e., LiDAR point clouds and
camera images with strict point-to-pixel mappings, as the inputs in both
training and inference, which seriously hinders their application in practical
scenarios. Thus, in this work, we propose the 2D Priors Assisted Semantic
Segmentation (2DPASS), a general training scheme, to boost the representation
learning on point clouds, by fully taking advantage of 2D images with rich
appearance. In practice, by leveraging an auxiliary modal fusion and
multi-scale fusion-to-single knowledge distillation (MSFSKD), 2DPASS acquires
richer semantic and structural information from the multi-modal data, which are
then online distilled to the pure 3D network. As a result, equipped with
2DPASS, our baseline shows significant improvement with only point cloud
inputs. Specifically, it achieves the state-of-the-arts on two large-scale
benchmarks (i.e. SemanticKITTI and NuScenes), including top-1 results in both
single and multiple scan(s) competitions of SemanticKITTI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Learning with Local Contrastive Loss for Detection and Semantic Segmentation. (arXiv:2207.04398v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04398">
<div class="article-summary-box-inner">
<span><p>We present a self-supervised learning (SSL) method suitable for semi-global
tasks such as object detection and semantic segmentation. We enforce local
consistency between self-learned features, representing corresponding image
locations of transformed versions of the same image, by minimizing a
pixel-level local contrastive (LC) loss during training. LC-loss can be added
to existing self-supervised learning methods with minimal overhead. We evaluate
our SSL approach on two downstream tasks -- object detection and semantic
segmentation, using COCO, PASCAL VOC, and CityScapes datasets. Our method
outperforms the existing state-of-the-art SSL approaches by 1.9% on COCO object
detection, 1.4% on PASCAL VOC detection, and 0.6% on CityScapes segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Horizontal and Vertical Attention in Transformers. (arXiv:2207.04399v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04399">
<div class="article-summary-box-inner">
<span><p>Transformers are built upon multi-head scaled dot-product attention and
positional encoding, which aim to learn the feature representations and token
dependencies. In this work, we focus on enhancing the distinctive
representation by learning to augment the feature maps with the self-attention
mechanism in Transformers. Specifically, we propose the horizontal attention to
re-weight the multi-head output of the scaled dot-product attention before
dimensionality reduction, and propose the vertical attention to adaptively
re-calibrate channel-wise feature responses by explicitly modelling
inter-dependencies among different channels. We demonstrate the Transformer
models equipped with the two attentions have a high generalization capability
across different supervised learning tasks, with a very minor additional
computational cost overhead. The proposed horizontal and vertical attentions
are highly modular, which can be inserted into various Transformer models to
further improve the performance. Our code is available in the supplementary
material.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-attention on Multi-Shifted Windows for Scene Segmentation. (arXiv:2207.04403v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04403">
<div class="article-summary-box-inner">
<span><p>Scene segmentation in images is a fundamental yet challenging problem in
visual content understanding, which is to learn a model to assign every image
pixel to a categorical label. One of the challenges for this learning task is
to consider the spatial and semantic relationships to obtain descriptive
feature representations, so learning the feature maps from multiple scales is a
common practice in scene segmentation. In this paper, we explore the effective
use of self-attention within multi-scale image windows to learn descriptive
visual features, then propose three different strategies to aggregate these
feature maps to decode the feature representation for dense prediction. Our
design is based on the recently proposed Swin Transformer models, which totally
discards convolution operations. With the simple yet effective multi-scale
feature learning and aggregation, our models achieve very promising performance
on four public scene segmentation datasets, PASCAL VOC2012, COCO-Stuff 10K,
ADE20K and Cityscapes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoMER: Modeling Coverage for Transformer-based Handwritten Mathematical Expression Recognition. (arXiv:2207.04410v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04410">
<div class="article-summary-box-inner">
<span><p>The Transformer-based encoder-decoder architecture has recently made
significant advances in recognizing handwritten mathematical expressions.
However, the transformer model still suffers from the lack of coverage problem,
making its expression recognition rate (ExpRate) inferior to its RNN
counterpart. Coverage information, which records the alignment information of
the past steps, has proven effective in the RNN models. In this paper, we
propose CoMER, a model that adopts the coverage information in the transformer
decoder. Specifically, we propose a novel Attention Refinement Module (ARM) to
refine the attention weights with past alignment information without hurting
its parallelism. Furthermore, we take coverage information to the extreme by
proposing self-coverage and cross-coverage, which utilize the past alignment
information from the current and previous layers. Experiments show that CoMER
improves the ExpRate by 0.61%/2.09%/1.59% compared to the current
state-of-the-art model, and reaches 59.33%/59.81%/62.97% on the CROHME
2014/2016/2019 test sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SFNet: Faster, Accurate, and Domain Agnostic Semantic Segmentation via Semantic Flow. (arXiv:2207.04415v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04415">
<div class="article-summary-box-inner">
<span><p>In this paper, we focus on exploring effective methods for faster, accurate,
and domain agnostic semantic segmentation. Inspired by the Optical Flow for
motion alignment between adjacent video frames, we propose a Flow Alignment
Module (FAM) to learn \textit{Semantic Flow} between feature maps of adjacent
levels, and broadcast high-level features to high resolution features
effectively and efficiently. Furthermore, integrating our FAM to a common
feature pyramid structure exhibits superior performance over other real-time
methods even on light-weight backbone networks, such as ResNet-18 and DFNet.
Then to further speed up the inference procedure, we also present a novel Gated
Dual Flow Alignment Module to directly align high resolution feature maps and
low resolution feature maps where we term improved version network as
SFNet-Lite. Extensive experiments are conducted on several challenging
datasets, where results show the effectiveness of both SFNet and SFNet-Lite. In
particular, the proposed SFNet-Lite series achieve 80.1 mIoU while running at
60 FPS using ResNet-18 backbone and 78.8 mIoU while running at 120 FPS using
STDC backbone on RTX-3090. Moreover, we unify four challenging driving datasets
(i.e., Cityscapes, Mapillary, IDD and BDD) into one large dataset, which we
named Unified Driving Segmentation (UDS) dataset. It contains diverse domain
and style information. We benchmark several representative works on UDS. Both
SFNet and SFNet-Lite still achieve the best speed and accuracy trade-off on UDS
which serves as a strong baseline in such a new challenging setting. All the
code and models are publicly available at https://github.com/lxtGH/SFSegNets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual-Correction Adaptation Network for Noisy Knowledge Transfer. (arXiv:2207.04423v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04423">
<div class="article-summary-box-inner">
<span><p>Previous unsupervised domain adaptation (UDA) methods aim to promote target
learning via a single-directional knowledge transfer from label-rich source
domain to unlabeled target domain, while its reverse adaption from target to
source has not jointly been considered yet so far. In fact, in some real
teaching practice, a teacher helps students learn while also gets promotion
from students to some extent, which inspires us to explore a dual-directional
knowledge transfer between domains, and thus propose a Dual-Correction
Adaptation Network (DualCAN) in this paper. However, due to the asymmetrical
label knowledge across domains, transfer from unlabeled target to labeled
source poses a more difficult challenge than the common source-to-target
counterpart. First, the target pseudo-labels predicted by source commonly
involve noises due to model bias, hence in the reverse adaptation, they may
hurt the source performance and bring a negative target-to-source transfer.
Secondly, source domain usually contains innate noises, which will inevitably
aggravate the target noises, leading to noise amplification across domains. To
this end, we further introduce a Noise Identification and Correction (NIC)
module to correct and recycle noises in both domains. To our best knowledge,
this is the first naive attempt of dual-directional adaptation for noisy UDA,
and naturally applicable to noise-free UDA. A theory justification is given to
state the rationality of our intuition. Empirical results confirm the
effectiveness of DualCAN with remarkable performance gains over
state-of-the-arts, particularly for extreme noisy tasks (e.g., ~+ 15% on Pw-&gt;Pr
and Pr-&gt;Rw of Office-Home).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hiding Your Signals: A Security Analysis of PPG-based Biometric Authentication. (arXiv:2207.04434v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04434">
<div class="article-summary-box-inner">
<span><p>Recently, physiological signal-based biometric systems have received wide
attention. Unlike traditional biometric features, physiological signals can not
be easily compromised (usually unobservable to human eyes).
Photoplethysmography (PPG) signal is easy to measure, making it more attractive
than many other physiological signals for biometric authentication. However,
with the advent of remote PPG (rPPG), unobservability has been challenged when
the attacker can remotely steal the rPPG signals by monitoring the victim's
face, subsequently posing a threat to PPG-based biometrics. In PPG-based
biometric authentication, current attack approaches mandate the victim's PPG
signal, making rPPG-based attacks neglected. In this paper, we firstly analyze
the security of PPG-based biometrics, including user authentication and
communication protocols. We evaluate the signal waveforms, heart rate and
inter-pulse-interval information extracted by five rPPG methods, including four
traditional optical computing methods (CHROM, POS, LGI, PCA) and one deep
learning method (CL_rPPG). We conducted experiments on five datasets (PURE,
UBFC_rPPG, UBFC_Phys, LGI_PPGI, and COHFACE) to collect a comprehensive set of
results. Our empirical studies show that rPPG poses a serious threat to the
authentication system. The success rate of the rPPG signal spoofing attack in
the user authentication system reached 0.35. The bit hit rate is 0.6 in
inter-pulse-interval-based security protocols. Further, we propose an active
defence strategy to hide the physiological signals of the face to resist the
attack. It reduces the success rate of rPPG spoofing attacks in user
authentication to 0.05. The bit hit rate was reduced to 0.5, which is at the
level of a random guess. Our strategy effectively prevents the exposure of PPG
signals to protect users' sensitive physiological data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SRRT: Search Region Regulation Tracking. (arXiv:2207.04438v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04438">
<div class="article-summary-box-inner">
<span><p>Dominant trackers generate a fixed-size rectangular region based on the
previous prediction or initial bounding box as the model input, i.e., search
region. While this manner leads to improved tracking efficiency, a fixed-size
search region lacks flexibility and is likely to fail in cases, e.g., fast
motion and distractor interference. Trackers tend to lose the target object due
to the limited search region or be interfered by distractors due to excessive
search region. In this work, we propose a novel tracking paradigm, called
Search Region Regulation Tracking (SRRT), which applies a proposed search
region regulator to estimate an optimal search region dynamically for every
frame. To adapt the object's appearance variation during tracking, we further
propose a locking-state determined updating strategy for reference frame
updating. Our SRRT framework is very concise without fancy design, yet achieves
evident improvements on the baselines and competitive results with other
state-of-the-art trackers on seven challenging benchmarks. On the large-scale
LaSOT benchmark, our SRRT improves SiamRPN++ and TransT with the absolute gains
of 4.6% and 3.1% in terms of AUC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mix-Teaching: A Simple, Unified and Effective Semi-Supervised Learning Framework for Monocular 3D Object Detection. (arXiv:2207.04448v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04448">
<div class="article-summary-box-inner">
<span><p>Monocular 3D object detection is an essential perception task for autonomous
driving. However, the high reliance on large-scale labeled data make it costly
and time-consuming during model optimization. To reduce such over-reliance on
human annotations, we propose Mix-Teaching, an effective semi-supervised
learning framework applicable to employ both labeled and unlabeled images in
training stage. Mix-Teaching first generates pseudo-labels for unlabeled images
by self-training. The student model is then trained on the mixed images
possessing much more intensive and precise labeling by merging instance-level
image patches into empty backgrounds or labeled images. This is the first to
break the image-level limitation and put high-quality pseudo labels from multi
frames into one image for semi-supervised training. Besides, as a result of the
misalignment between confidence score and localization quality, it's hard to
discriminate high-quality pseudo-labels from noisy predictions using only
confidence-based criterion. To that end, we further introduce an
uncertainty-based filter to help select reliable pseudo boxes for the above
mixing operation. To the best of our knowledge, this is the first unified SSL
framework for monocular 3D object detection. Mix-Teaching consistently improves
MonoFlex and GUPNet by significant margins under various labeling ratios on
KITTI dataset. For example, our method achieves around +6.34% AP@0.7
improvement against the GUPNet baseline on validation set when using only 10%
labeled data. Besides, by leveraging full training set and the additional 48K
raw images of KITTI, it can further improve the MonoFlex by +4.65% improvement
on AP@0.7 for car detection, reaching 18.54% AP@0.7, which ranks the 1st place
among all monocular based methods on KITTI test leaderboard. The code and
pretrained models will be released at
https://github.com/yanglei18/Mix-Teaching.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Progressively-connected Light Field Network for Efficient View Synthesis. (arXiv:2207.04465v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04465">
<div class="article-summary-box-inner">
<span><p>This paper presents a Progressively-connected Light Field network (ProLiF),
for the novel view synthesis of complex forward-facing scenes. ProLiF encodes a
4D light field, which allows rendering a large batch of rays in one training
step for image- or patch-level losses. Directly learning a neural light field
from images has difficulty in rendering multi-view consistent images due to its
unawareness of the underlying 3D geometry. To address this problem, we propose
a progressive training scheme and regularization losses to infer the underlying
geometry during training, both of which enforce the multi-view consistency and
thus greatly improves the rendering quality. Experiments demonstrate that our
method is able to achieve significantly better rendering quality than the
vanilla neural light fields and comparable results to NeRF-like rendering
methods on the challenging LLFF dataset and Shiny Object dataset. Moreover, we
demonstrate better compatibility with LPIPS loss to achieve robustness to
varying light conditions and CLIP loss to control the rendering style of the
scene. Project page: https://totoro97.github.io/projects/prolif.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DPText-DETR: Towards Better Scene Text Detection with Dynamic Points in Transformer. (arXiv:2207.04491v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04491">
<div class="article-summary-box-inner">
<span><p>Recently, Transformer-based methods, which predict polygon points or Bezier
curve control points to localize texts, are quite popular in scene text
detection. However, the used point label form implies the reading order of
humans, which affects the robustness of Transformer model. As for the model
architecture, the formulation of queries used in decoder has not been fully
explored by previous methods. In this paper, we propose a concise dynamic point
scene text detection Transformer network termed DPText-DETR, which directly
uses point coordinates as queries and dynamically updates them between decoder
layers. We point out a simple yet effective positional point label form to
tackle the side effect of the original one. Moreover, an Enhanced Factorized
Self-Attention module is designed to explicitly model the circular shape of
polygon point sequences beyond non-local attention. Extensive experiments prove
the training efficiency, robustness, and state-of-the-art performance on
various arbitrary shape scene text benchmarks. Beyond detector, we observe that
existing end-to-end spotters struggle to recognize inverse-like texts. To
evaluate their performance objectively and facilitate future research, we
propose an Inverse-Text test set containing 500 manually labeled images. The
code and Inverse-Text test set will be available at
https://github.com/ymy-k/DPText-DETR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Adaptive Unknown Authentication for Universal Domain Adaptation by Classifier Paradox. (arXiv:2207.04494v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04494">
<div class="article-summary-box-inner">
<span><p>Universal domain adaptation (UniDA) is a general unsupervised domain
adaptation setting, which addresses both domain and label shifts in adaptation.
Its main challenge lies in how to identify target samples in unshared or
unknown classes. Previous methods commonly strive to depict sample "confidence"
along with a threshold for rejecting unknowns, and align feature distributions
of shared classes across domains. However, it is still hard to pre-specify a
"confidence" criterion and threshold which are adaptive to various real tasks,
and a mis-prediction of unknowns further incurs misalignment of features in
shared classes. In this paper, we propose a new UniDA method with adaptive
Unknown Authentication by Classifier Paradox (UACP), considering that samples
with paradoxical predictions are probably unknowns belonging to none of the
source classes. In UACP, a composite classifier is jointly designed with two
types of predictors. That is, a multi-class (MC) predictor classifies samples
to one of the multiple source classes, while a binary one-vs-all (OVA)
predictor further verifies the prediction by MC predictor. Samples with
verification failure or paradox are identified as unknowns. Further, instead of
feature alignment for shared classes, implicit domain alignment is conducted in
output space such that samples across domains share the same decision boundary,
though with feature discrepancy. Empirical results validate UACP under both
open-set and universal UDA settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning-based Monocular 3D Reconstruction of Birds: A Contemporary Survey. (arXiv:2207.04512v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04512">
<div class="article-summary-box-inner">
<span><p>In nature, the collective behavior of animals, such as flying birds is
dominated by the interactions between individuals of the same species. However,
the study of such behavior among the bird species is a complex process that
humans cannot perform using conventional visual observational techniques such
as focal sampling in nature. For social animals such as birds, the mechanism of
group formation can help ecologists understand the relationship between social
cues and their visual characteristics over time (e.g., pose and shape). But,
recovering the varying pose and shapes of flying birds is a highly challenging
problem. A widely-adopted solution to tackle this bottleneck is to extract the
pose and shape information from 2D image to 3D correspondence. Recent advances
in 3D vision have led to a number of impressive works on the 3D shape and pose
estimation, each with different pros and cons. To the best of our knowledge,
this work is the first attempt to provide an overview of recent advances in 3D
bird reconstruction based on monocular vision, give both computer vision and
biology researchers an overview of existing approaches, and compare their
characteristics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Facilitated machine learning for image-based fruit quality assessment in developing countries. (arXiv:2207.04523v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04523">
<div class="article-summary-box-inner">
<span><p>Automated image classification is a common task for supervised machine
learning in food science. An example is the image-based classification of the
fruit's external quality or ripeness. For this purpose, deep convolutional
neural networks (CNNs) are typically used. These models usually require a large
number of labeled training samples and enhanced computational resources. While
commercial fruit sorting lines readily meet these requirements, the use of
machine learning approaches can be hindered by these prerequisites, especially
for smallholder farmers in the developing world. We propose an alternative
method based on pre-trained vision transformers (ViTs) that is particularly
suitable for domains with low availability of data and limited computational
resources. It can be easily implemented with limited resources on a standard
device, which can democratize the use of these models for smartphone-based
image classification in developing countries. We demonstrate the
competitiveness of our method by benchmarking two different classification
tasks on domain data sets of banana and apple fruits with well-established CNN
approaches. Our method achieves a classification accuracy of less than one
percent below the best-performing CNN (0.950 vs. 0.958) on a training data set
of 3745 images. At the same time, our method is superior when only a small
number of labeled training samples is available. It requires three times less
data to achieve a 0.90 accuracy compared to CNNs. In addition, visualizations
of low-dimensional feature embeddings show that the model used in our study
extracts excellent features from unseen data without allocating labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Multi-Task RGB-D Scene Analysis for Indoor Environments. (arXiv:2207.04526v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04526">
<div class="article-summary-box-inner">
<span><p>Semantic scene understanding is essential for mobile agents acting in various
environments. Although semantic segmentation already provides a lot of
information, details about individual objects as well as the general scene are
missing but required for many real-world applications. However, solving
multiple tasks separately is expensive and cannot be accomplished in real time
given limited computing and battery capabilities on a mobile platform. In this
paper, we propose an efficient multi-task approach for RGB-D scene
analysis~(EMSANet) that simultaneously performs semantic and instance
segmentation~(panoptic segmentation), instance orientation estimation, and
scene classification. We show that all tasks can be accomplished using a single
neural network in real time on a mobile platform without diminishing
performance - by contrast, the individual tasks are able to benefit from each
other. In order to evaluate our multi-task approach, we extend the annotations
of the common RGB-D indoor datasets NYUv2 and SUNRGB-D for instance
segmentation and orientation estimation. To the best of our knowledge, we are
the first to provide results in such a comprehensive multi-task setting for
indoor scene analysis on NYUv2 and SUNRGB-D.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Open-Source Tool for Longitudinal Whole-Brain and White Matter Lesion Segmentation. (arXiv:2207.04534v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04534">
<div class="article-summary-box-inner">
<span><p>In this paper we describe and validate a longitudinal method for whole-brain
segmentation of longitudinal MRI scans. It builds upon an existing whole-brain
segmentation method that can handle multi-contrast data and robustly analyze
images with white matter lesions. This method is here extended with
subject-specific latent variables that encourage temporal consistency between
its segmentation results, enabling it to better track subtle morphological
changes in dozens of neuroanatomical structures and white matter lesions. We
validate the proposed method on multiple datasets of control subjects and
patients suffering from Alzheimer's disease and multiple sclerosis, and compare
its results against those obtained with its original cross-sectional
formulation and two benchmark longitudinal methods. The results indicate that
the method attains a higher test-retest reliability, while being more sensitive
to longitudinal disease effect differences between patient groups. An
implementation is publicly available as part of the open-source neuroimaging
package FreeSurfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Depthformer : Multiscale Vision Transformer For Monocular Depth Estimation With Local Global Information Fusion. (arXiv:2207.04535v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04535">
<div class="article-summary-box-inner">
<span><p>Attention-based models such as transformers have shown outstanding
performance on dense prediction tasks, such as semantic segmentation, owing to
their capability of capturing long-range dependency in an image. However, the
benefit of transformers for monocular depth prediction has seldom been explored
so far. This paper benchmarks various transformer-based models for the depth
estimation task on an indoor NYUV2 dataset and an outdoor KITTI dataset. We
propose a novel attention-based architecture, Depthformer for monocular depth
estimation that uses multi-head self-attention to produce the multiscale
feature maps, which are effectively combined by our proposed decoder network.
We also propose a Transbins module that divides the depth range into bins whose
center value is estimated adaptively per image. The final depth estimated is a
linear combination of bin centers for each pixel. Transbins module takes
advantage of the global receptive field using the transformer module in the
encoding stage. Experimental results on NYUV2 and KITTI depth estimation
benchmark demonstrate that our proposed method improves the state-of-the-art by
3.3%, and 3.3% respectively in terms of Root Mean Squared Error (RMSE).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Depth Perspective-aware Multiple Object Tracking. (arXiv:2207.04551v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04551">
<div class="article-summary-box-inner">
<span><p>This paper aims to tackle Multiple Object Tracking (MOT), an important
problem in computer vision but remains challenging due to many practical
issues, especially occlusions. Indeed, we propose a new real-time Depth
Perspective-aware Multiple Object Tracking (DP-MOT) approach to tackle the
occlusion problem in MOT. A simple yet efficient Subject-Ordered Depth
Estimation (SODE) is first proposed to automatically order the depth positions
of detected subjects in a 2D scene in an unsupervised manner. Using the output
from SODE, a new Active pseudo-3D Kalman filter, a simple but effective
extension of Kalman filter with dynamic control variables, is then proposed to
dynamically update the movement of objects. In addition, a new high-order
association approach is presented in the data association step to incorporate
first-order and second-order relationships between the detected objects. The
proposed approach consistently achieves state-of-the-art performance compared
to recent MOT methods on standard MOT benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GAN-based Virtual Re-Staining: A Promising Solution for Whole Slide Image Analysis. (arXiv:1901.04059v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1901.04059">
<div class="article-summary-box-inner">
<span><p>Histopathological cancer diagnosis is based on visual examination of stained
tissue slides. Hematoxylin and eosin (H\&amp;E) is a standard stain routinely
employed worldwide. It is easy to acquire and cost effective, but cells and
tissue components show low-contrast with varying tones of dark blue and pink,
which makes difficult visual assessments, digital image analysis, and
quantifications. These limitations can be overcome by IHC staining of target
proteins of the tissue slide. IHC provides a selective, high-contrast imaging
of cells and tissue components, but their use is largely limited by a
significantly more complex laboratory processing and high cost. We proposed a
conditional CycleGAN (cCGAN) network to transform the H\&amp;E stained images into
IHC stained images, facilitating virtual IHC staining on the same slide. This
data-driven method requires only a limited amount of labelled data but will
generate pixel level segmentation results. The proposed cCGAN model improves
the original network \cite{zhu_unpaired_2017} by adding category conditions and
introducing two structural loss functions, which realize a multi-subdomain
translation and improve the translation accuracy as well. % need to give
reasons here. Experiments demonstrate that the proposed model outperforms the
original method in unpaired image translation with multi-subdomains. We also
explore the potential of unpaired images to image translation method applied on
other histology images related tasks with different staining techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying and Compensating for Feature Deviation in Imbalanced Deep Learning. (arXiv:2001.01385v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2001.01385">
<div class="article-summary-box-inner">
<span><p>Classifiers trained with class-imbalanced data are known to perform poorly on
test data of the "minor" classes, of which we have insufficient training data.
In this paper, we investigate learning a ConvNet classifier under such a
scenario. We found that a ConvNet significantly over-fits the minor classes,
which is quite opposite to traditional machine learning algorithms that often
under-fit minor classes. We conducted a series of analysis and discovered the
feature deviation phenomenon -- the learned ConvNet generates deviated features
between the training and test data of minor classes -- which explains how
over-fitting happens. To compensate for the effect of feature deviation which
pushes test data toward low decision value regions, we propose to incorporate
class-dependent temperatures (CDT) in training a ConvNet. CDT simulates feature
deviation in the training phase, forcing the ConvNet to enlarge the decision
values for minor-class data so that it can overcome real feature deviation in
the test phase. We validate our approach on benchmark datasets and achieve
promising performance. We hope that our insights can inspire new ways of
thinking in resolving class-imbalanced deep learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Symmetry to Geometry: Tractable Nonconvex Problems. (arXiv:2007.06753v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.06753">
<div class="article-summary-box-inner">
<span><p>As science and engineering have become increasingly data-driven, the role of
optimization has expanded to touch almost every stage of the data analysis
pipeline, from signal and data acquisition to modeling and prediction. The
optimization problems encountered in practice are often nonconvex. While
challenges vary from problem to problem, one common source of nonconvexity is
nonlinearity in the data or measurement model. Nonlinear models often exhibit
symmetries, creating complicated, nonconvex objective landscapes, with multiple
equivalent solutions. Nevertheless, simple methods (e.g., gradient descent)
often perform surprisingly well in practice.
</p>
<p>The goal of this survey is to highlight a class of tractable nonconvex
problems, which can be understood through the lens of symmetries. These
problems exhibit a characteristic geometric structure: local minimizers are
symmetric copies of a single "ground truth" solution, while other critical
points occur at balanced superpositions of symmetric copies of the ground
truth, and exhibit negative curvature in directions that break the symmetry.
This structure enables efficient methods to obtain global minimizers. We
discuss examples of this phenomenon arising from a wide range of problems in
imaging, signal processing, and data analysis. We highlight the key role of
symmetry in shaping the objective landscape and discuss the different roles of
rotational and discrete symmetries. This area is rich with observed phenomena
and open problems; we close by highlighting directions for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards End-to-end Car License Plate Location and Recognition in Unconstrained Scenarios. (arXiv:2008.10916v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.10916">
<div class="article-summary-box-inner">
<span><p>Benefiting from the rapid development of convolutional neural networks, the
performance of car license plate detection and recognition has been largely
improved. Nonetheless, most existing methods solve detection and recognition
problems separately, and focus on specific scenarios, which hinders the
deployment for real-world applications. To overcome these challenges, we
present an efficient and accurate framework to solve the license plate
detection and recognition tasks simultaneously. It is a lightweight and unified
deep neural network, that can be optimized end-to-end and work in real-time.
Specifically, for unconstrained scenarios, an anchor-free method is adopted to
efficiently detect the bounding box and four corners of a license plate, which
are used to extract and rectify the target region features. Then, a novel
convolutional neural network branch is designed to further extract features of
characters without segmentation. Finally, the recognition task is treated as
sequence labeling problems, which are solved by Connectionist Temporal
Classification (CTC) directly. Several public datasets including images
collected from different scenarios under various conditions are chosen for
evaluation. Experimental results indicate that the proposed method
significantly outperforms the previous state-of-the-art methods in both speed
and precision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Amodal Segmentation through Out-of-Task and Out-of-Distribution Generalization with a Bayesian Model. (arXiv:2010.13175v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.13175">
<div class="article-summary-box-inner">
<span><p>Amodal completion is a visual task that humans perform easily but which is
difficult for computer vision algorithms. The aim is to segment those object
boundaries which are occluded and hence invisible. This task is particularly
challenging for deep neural networks because data is difficult to obtain and
annotate. Therefore, we formulate amodal segmentation as an out-of-task and
out-of-distribution generalization problem. Specifically, we replace the fully
connected classifier in neural networks with a Bayesian generative model of the
neural network features. The model is trained from non-occluded images using
bounding box annotations and class labels only, but is applied to generalize
out-of-task to object segmentation and to generalize out-of-distribution to
segment occluded objects. We demonstrate how such Bayesian models can naturally
generalize beyond the training task labels when they learn a prior that models
the object's background context and shape. Moreover, by leveraging an outlier
process, Bayesian models can further generalize out-of-distribution to segment
partially occluded objects and to predict their amodal object boundaries. Our
algorithm outperforms alternative methods that use the same supervision by a
large margin, and even outperforms methods where annotated amodal segmentations
are used during training, when the amount of occlusion is large. Code is
publicly available at https://github.com/YihongSun/Bayesian-Amodal.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Deep Learning-based Single Image Crowd Counting: Network Design, Loss Function and Supervisory Signal. (arXiv:2012.15685v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.15685">
<div class="article-summary-box-inner">
<span><p>Single image crowd counting is a challenging computer vision problem with
wide applications in public safety, city planning, traffic management, etc.
With the recent development of deep learning techniques, crowd counting has
aroused much attention and achieved great success in recent years. This survey
is to provide a comprehensive summary of recent advances on deep learning-based
crowd counting techniques via density map estimation by systematically
reviewing and summarizing more than 200 works in the area since 2015. Our goals
are to provide an up-to-date review of recent approaches, and educate new
researchers in this field the design principles and trade-offs. After
presenting publicly available datasets and evaluation metrics, we review the
recent advances with detailed comparisons on three major design modules for
crowd counting: deep neural network designs, loss functions, and supervisory
signals. We study and compare the approaches using the public datasets and
evaluation metrics. We conclude the survey with some future directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Interactive Image Segmentation: Feature Space Annotation. (arXiv:2101.04378v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.04378">
<div class="article-summary-box-inner">
<span><p>Despite the progress of interactive image segmentation methods, high-quality
pixel-level annotation is still time-consuming and laborious - a bottleneck for
several deep learning applications. We take a step back to propose interactive
and simultaneous segment annotation from multiple images guided by feature
space projection. This strategy is in stark contrast to existing interactive
segmentation methodologies, which perform annotation in the image domain. We
show that feature space annotation achieves competitive results with
state-of-the-art methods in foreground segmentation datasets: iCoSeg, DAVIS,
and Rooftop. Moreover, in the semantic segmentation context, it achieves 91.5%
accuracy in the Cityscapes dataset, being 74.75 times faster than the original
annotation procedure. Further, our contribution sheds light on a novel
direction for interactive image annotation that can be integrated with existing
methodologies. The supplementary material presents video demonstrations. Code
available at
https://github.com/LIDS-UNICAMP/rethinking-interactive-image-segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual explanation of black-box model: Similarity Difference and Uniqueness (SIDU) method. (arXiv:2101.10710v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.10710">
<div class="article-summary-box-inner">
<span><p>Explainable Artificial Intelligence (XAI) has in recent years become a
well-suited framework to generate human understandable explanations of
"black-box" models. In this paper, a novel XAI visual explanation algorithm
known as the Similarity Difference and Uniqueness (SIDU) method that can
effectively localize entire object regions responsible for prediction is
presented in full detail. The SIDU algorithm robustness and effectiveness is
analyzed through various computational and human subject experiments. In
particular, the SIDU algorithm is assessed using three different types of
evaluations (Application, Human and Functionally-Grounded) to demonstrate its
superior performance. The robustness of SIDU is further studied in the presence
of adversarial attack on "black-box" models to better understand its
performance. Our code is available at:
https://github.com/satyamahesh84/SIDU_XAI_CODE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learned Camera Gain and Exposure Control for Improved Visual Feature Detection and Matching. (arXiv:2102.04341v3 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.04341">
<div class="article-summary-box-inner">
<span><p>Successful visual navigation depends upon capturing images that contain
sufficient useful information. In this letter, we explore a data-driven
approach to account for environmental lighting changes, improving the quality
of images for use in visual odometry (VO) or visual simultaneous localization
and mapping (SLAM). We train a deep convolutional neural network model to
predictively adjust camera gain and exposure time parameters such that
consecutive images contain a maximal number of matchable features. The training
process is fully self-supervised: our training signal is derived from an
underlying VO or SLAM pipeline and, as a result, the model is optimized to
perform well with that specific pipeline. We demonstrate through extensive
real-world experiments that our network can anticipate and compensate for
dramatic lighting changes (e.g., transitions into and out of road tunnels),
maintaining a substantially higher number of inlier feature matches than
competing camera parameter control algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Purified Feature Representations from Task-irrelevant Labels. (arXiv:2102.10955v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.10955">
<div class="article-summary-box-inner">
<span><p>Learning an empirically effective model with generalization using limited
data is a challenging task for deep neural networks. In this paper, we propose
a novel learning framework called PurifiedLearning to exploit task-irrelevant
features extracted from task-irrelevant labels when training models on
small-scale datasets. Particularly, we purify feature representations by using
the expression of task-irrelevant information, thus facilitating the learning
process of classification. Our work is built on solid theoretical analysis and
extensive experiments, which demonstrate the effectiveness of PurifiedLearning.
According to the theory we proved, PurifiedLearning is model-agnostic and
doesn't have any restrictions on the model needed, so it can be combined with
any existing deep neural networks with ease to achieve better performance. The
source code of this paper will be available in the future for reproducibility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Playbacks in Unsupervised Domain Adaptation for 3D Object Detection. (arXiv:2103.14198v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.14198">
<div class="article-summary-box-inner">
<span><p>Self-driving cars must detect other vehicles and pedestrians in 3D to plan
safe routes and avoid collisions. State-of-the-art 3D object detectors, based
on deep learning, have shown promising accuracy but are prone to over-fit to
domain idiosyncrasies, making them fail in new environments -- a serious
problem if autonomous vehicles are meant to operate freely. In this paper, we
propose a novel learning approach that drastically reduces this gap by
fine-tuning the detector on pseudo-labels in the target domain, which our
method generates while the vehicle is parked, based on replays of previously
recorded driving sequences. In these replays, objects are tracked over time,
and detections are interpolated and extrapolated -- crucially, leveraging
future information to catch hard cases. We show, on five autonomous driving
datasets, that fine-tuning the object detector on these pseudo-labels
substantially reduces the domain gap to new driving environments, yielding
drastic improvements in accuracy and detection reliability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised object detection from audio-visual correspondence. (arXiv:2104.06401v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06401">
<div class="article-summary-box-inner">
<span><p>We tackle the problem of learning object detectors without supervision.
Differently from weakly-supervised object detection, we do not assume
image-level class labels. Instead, we extract a supervisory signal from
audio-visual data, using the audio component to "teach" the object detector.
While this problem is related to sound source localisation, it is considerably
harder because the detector must classify the objects by type, enumerate each
instance of the object, and do so even when the object is silent. We tackle
this problem by first designing a self-supervised framework with a contrastive
objective that jointly learns to classify and localise objects. Then, without
using any supervision, we simply use these self-supervised labels and boxes to
train an image-based object detector. With this, we outperform previous
unsupervised and weakly-supervised detectors for the task of object detection
and sound source localization. We also show that we can align this detector to
ground-truth classes with as little as one label per pseudo-class, and show how
our method can learn to detect generic objects that go beyond instruments, such
as airplanes and cats.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Exact Hypergraph Matching Algorithm for Nuclear Identification in Embryonic Caenorhabditis elegans. (arXiv:2104.10003v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.10003">
<div class="article-summary-box-inner">
<span><p>Finding an optimal correspondence between point sets is a common task in
computer vision. Existing techniques assume relatively simple relationships
among points and do not guarantee an optimal match. We introduce an algorithm
capable of exactly solving point set matching by modeling the task as
hypergraph matching. The algorithm extends the classical branch and bound
paradigm to select and aggregate vertices under a proposed decomposition of the
multilinear objective function. The methodology is motivated by Caenorhabditis
elegans, a model organism used frequently in developmental biology and
neurobiology. The embryonic C. elegans contains seam cells that can act as
fiducial markers allowing the identification of other nuclei during embryo
development. The proposed algorithm identifies seam cells more accurately than
established point-set matching methods, while providing a framework to approach
other similarly complex point set matching tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FourierNets enable the design of highly non-local optical encoders for computational imaging. (arXiv:2104.10611v5 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.10611">
<div class="article-summary-box-inner">
<span><p>Differentiable simulations of optical systems can be combined with deep
learning-based reconstruction networks to enable high performance computational
imaging via end-to-end (E2E) optimization of both the optical encoder and the
deep decoder. This has enabled imaging applications such as 3D localization
microscopy, depth estimation, and lensless photography via the optimization of
local optical encoders. More challenging computational imaging applications,
such as 3D snapshot microscopy which compresses 3D volumes into single 2D
images, require a highly non-local optical encoder. We show that existing deep
network decoders have a locality bias which prevents the optimization of such
highly non-local optical encoders. We address this with a decoder based on a
shallow neural network architecture using global kernel Fourier convolutional
neural networks (FourierNets). We show that FourierNets surpass existing deep
network based decoders at reconstructing photographs captured by the highly
non-local DiffuserCam optical encoder. Further, we show that FourierNets enable
E2E optimization of highly non-local optical encoders for 3D snapshot
microscopy. By combining FourierNets with a large-scale multi-GPU
differentiable optical simulation, we are able to optimize non-local optical
encoders 170$\times$ to 7372$\times$ larger than prior state of the art, and
demonstrate the potential for ROI-type specific optical encoding with a
programmable microscope.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stochastic Neural Networks for Automatic Cell Tracking in Microscopy Image Sequences of Bacterial Colonies. (arXiv:2104.13482v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.13482">
<div class="article-summary-box-inner">
<span><p>Our work targets automated analysis to quantify the growth dynamics of a
population of bacilliform bacteria. We propose an innovative approach to
frame-sequence tracking of deformable-cell motion by the automated minimization
of a new, specific cost functional. This minimization is implemented by
dedicated Boltzmann machines (stochastic recurrent neural networks). Automated
detection of cell divisions is handled similarly by successive minimizations of
two cost functions, alternating the identification of children pairs and parent
identification. We validate the proposed automatic cell tracking algorithm
using (i) recordings of simulated cell colonies that closely mimic the growth
dynamics of E. coli in microfluidic traps and (ii) real data. On a batch of
1100 simulated image frames, cell registration accuracies per frame ranged from
94.5% to 100%, with a high average. Our initial tests using experimental image
sequences (i.e., real data) of E. coli colonies also yield convincing results,
with a registration accuracy ranging from 90% to 100%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GAN Cocktail: mixing GANs without dataset access. (arXiv:2106.03847v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03847">
<div class="article-summary-box-inner">
<span><p>Today's generative models are capable of synthesizing high-fidelity images,
but each model specializes on a specific target domain. This raises the need
for model merging: combining two or more pretrained generative models into a
single unified one. In this work we tackle the problem of model merging, given
two constraints that often come up in the real world: (1) no access to the
original training data, and (2) without increasing the size of the neural
network. To the best of our knowledge, model merging under these constraints
has not been studied thus far. We propose a novel, two-stage solution. In the
first stage, we transform the weights of all the models to the same parameter
space by a technique we term model rooting. In the second stage, we merge the
rooted models by averaging their weights and fine-tuning them for each specific
domain, using only data generated by the original trained models. We
demonstrate that our approach is superior to baseline methods and to existing
transfer learning techniques, and investigate several applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Coupling of Depth and Egomotion Networks for Self-Supervised Structure from Motion. (arXiv:2106.04007v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04007">
<div class="article-summary-box-inner">
<span><p>Structure from motion (SfM) has recently been formulated as a self-supervised
learning problem, where neural network models of depth and egomotion are
learned jointly through view synthesis. Herein, we address the open problem of
how to best couple, or link, the depth and egomotion network components, so
that information such as a common scale factor can be shared between the
networks. Towards this end, we introduce several notions of coupling,
categorize existing approaches, and present a novel tightly-coupled approach
that leverages the interdependence of depth and egomotion at training time and
at test time. Our approach uses iterative view synthesis to recursively update
the egomotion network input, permitting contextual information to be passed
between the components. We demonstrate through substantial experiments that our
approach promotes consistency between the depth and egomotion predictions at
test time, improves generalization, and leads to state-of-the-art accuracy on
indoor and outdoor depth and egomotion evaluation benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inverting Adversarially Robust Networks for Image Synthesis. (arXiv:2106.06927v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06927">
<div class="article-summary-box-inner">
<span><p>Despite unconditional feature inversion being the foundation of many image
synthesis applications, training an inverter demands a high computational
budget, large decoding capacity and imposing conditions such as autoregressive
priors. To address these limitations, we propose the use of adversarially
robust representations as a perceptual primitive for feature inversion. We
train an adversarially robust encoder to extract disentangled and
perceptually-aligned image representations, making them easily invertible. By
training a simple generator with the mirror architecture of the encoder, we
achieve superior reconstruction quality and generalization over standard
models. Based on this, we propose an adversarially robust autoencoder and
demonstrate its improved performance on style transfer, image denoising and
anomaly detection tasks. Compared to recent ImageNet feature inversion methods,
our model attains improved performance with significantly less complexity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Making Images Real Again: A Comprehensive Survey on Deep Image Composition. (arXiv:2106.14490v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.14490">
<div class="article-summary-box-inner">
<span><p>As a common image editing operation, image composition aims to cut the
foreground from one image and paste it on another image, resulting in a
composite image. However, there are many issues that could make the composite
images unrealistic. These issues can be summarized as the inconsistency between
foreground and background, which includes appearance inconsistency (e.g.,
incompatible illumination), geometry inconsistency (e.g., unreasonable size),
and semantic inconsistency (e.g., mismatched semantic context). Previous works
divide image composition task into multiple sub-tasks, in which each sub-task
targets at one or more issues. Specifically, object placement aims to find
reasonable scale, location, and shape for the foreground. Image blending aims
to address the unnatural boundary between foreground and background. Image
harmonization aims to adjust the illumination statistics of foreground. Shadow
generation aims to generate plausible shadow for the foreground. By putting all
the abovementioned efforts together, we can acquire realistic composite images.
To the best of our knowledge, there is no previous survey on image composition.
In this paper, we conduct comprehensive survey over the sub-tasks of image
composition. For each sub-task, we summarize the traditional methods, deep
learning based methods, datasets and evaluation. We also point out the
limitations of existing methods in each sub-task and the problem of the whole
image composition task. Datasets and codes for image composition are summarized
at https://github.com/bcmi/Awesome-Image-Composition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How to Train Your MAML to Excel in Few-Shot Classification. (arXiv:2106.16245v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.16245">
<div class="article-summary-box-inner">
<span><p>Model-agnostic meta-learning (MAML) is arguably one of the most popular
meta-learning algorithms nowadays. Nevertheless, its performance on few-shot
classification is far behind many recent algorithms dedicated to the problem.
In this paper, we point out several key facets of how to train MAML to excel in
few-shot classification. First, we find that MAML needs a large number of
gradient steps in its inner loop update, which contradicts its common usage in
few-shot classification. Second, we find that MAML is sensitive to the class
label assignments during meta-testing. Concretely, MAML meta-trains the
initialization of an $N$-way classifier. These $N$ ways, during meta-testing,
then have "$N!$" different permutations to be paired with a few-shot task of
$N$ novel classes. We find that these permutations lead to a huge variance of
accuracy, making MAML unstable in few-shot classification. Third, we
investigate several approaches to make MAML permutation-invariant, among which
meta-training a single vector to initialize all the $N$ weight vectors in the
classification head performs the best. On benchmark datasets like MiniImageNet
and TieredImageNet, our approach, which we name UNICORN-MAML, performs on a par
with or even outperforms many recent few-shot classification algorithms,
without sacrificing MAML's simplicity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-Shot Learning with a Strong Teacher. (arXiv:2107.00197v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.00197">
<div class="article-summary-box-inner">
<span><p>Few-shot learning (FSL) aims to generate a classifier using limited labeled
examples. Many existing works take the meta-learning approach, constructing a
few-shot learner that can learn from few-shot examples to generate a
classifier. Typically, the few-shot learner is constructed or meta-trained by
sampling multiple few-shot tasks in turn and optimizing the few-shot learner's
performance in generating classifiers for those tasks. The performance is
measured by how well the resulting classifiers classify the test (i.e., query)
examples of those tasks. In this paper, we point out two potential weaknesses
of this approach. First, the sampled query examples may not provide sufficient
supervision for meta-training the few-shot learner. Second, the effectiveness
of meta-learning diminishes sharply with the increasing number of shots. To
resolve these issues, we propose a novel meta-training objective for the
few-shot learner, which is to encourage the few-shot learner to generate
classifiers that perform like strong classifiers. Concretely, we associate each
sampled few-shot task with a strong classifier, which is trained with ample
labeled examples. The strong classifiers can be seen as the target classifiers
that we hope the few-shot learner to generate given few-shot examples, and we
use the strong classifiers to supervise the few-shot learner. We present an
efficient way to construct the strong classifier, making our proposed objective
an easily plug-and-play term to existing meta-learning based FSL methods. We
validate our approach, LastShot, in combinations with many representative
meta-learning methods. On several benchmark datasets, our approach leads to a
notable improvement across a variety of tasks. More importantly, with our
approach, meta-learning based FSL methods can outperform non-meta-learning
based methods at different numbers of shots.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FINT: Field-aware INTeraction Neural Network For CTR Prediction. (arXiv:2107.01999v3 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.01999">
<div class="article-summary-box-inner">
<span><p>As a critical component for online advertising and marking, click-through
rate (CTR) prediction has draw lots of attentions from both industry and
academia field. Recently, the deep learning has become the mainstream
methodological choice for CTR. Despite of sustainable efforts have been made,
existing approaches still pose several challenges. On the one hand, high-order
interaction between the features is under-explored. On the other hand,
high-order interactions may neglect the semantic information from the low-order
fields. In this paper, we proposed a novel prediction method, named FINT, that
employs the Field-aware INTeraction layer which captures high-order feature
interactions while retaining the low-order field information. To empirically
investigate the effectiveness and robustness of the FINT, we perform extensive
experiments on the three realistic databases: KDD2012, Criteo and Avazu. The
obtained results demonstrate that the FINT can significantly improve the
performance compared to the existing methods, without increasing the amount of
computation required. Moreover, the proposed method brought about 2.72\%
increase to the advertising revenue of a big online video app through A/B
testing. To better promote the research in CTR field, we released our code as
well as reference implementation at: https://github.com/zhishan01/FINT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransAttUnet: Multi-level Attention-guided U-Net with Transformer for Medical Image Segmentation. (arXiv:2107.05274v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.05274">
<div class="article-summary-box-inner">
<span><p>Accurate segmentation of organs or lesions from medical images is crucial for
reliable diagnosis of diseases and organ morphometry. In recent years,
convolutional encoder-decoder solutions have achieved substantial progress in
the field of automatic medical image segmentation. Due to the inherent bias in
the convolution operations, prior models mainly focus on local visual cues
formed by the neighboring pixels, but fail to fully model the long-range
contextual dependencies. In this paper, we propose a novel Transformer-based
Attention Guided Network called TransAttUnet, in which the multi-level guided
attention and multi-scale skip connection are designed to jointly enhance the
performance of the semantical segmentation architecture. Inspired by
Transformer, the self-aware attention (SAA) module with Transformer Self
Attention (TSA) and Global Spatial Attention (GSA) is incorporated into
TransAttUnet to effectively learn the non-local interactions among encoder
features. Moreover, we also use additional multi-scale skip connections between
decoder blocks to aggregate the upsampled features with different semantic
scales. In this way, the representation ability of multi-scale context
information is strengthened to generate discriminative features. Benefitting
from these complementary components, the proposed TransAttUnet can effectively
alleviate the loss of fine details caused by the stacking of convolution layers
and the consecutive sampling operations, finally improving the segmentation
quality of medical images. Extensive experiments on multiple medical image
segmentation datasets from different imaging modalities demonstrate that the
proposed method consistently outperforms the state-of-the-art baselines. Our
code and pre-trained models are available at:
https://github.com/YishuLiu/TransAttUnet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Two New Low Rank Tensor Completion Methods Based on Sum Nuclear Norm. (arXiv:2108.03002v4 [math.NA] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03002">
<div class="article-summary-box-inner">
<span><p>The low rank tensor completion (LRTC) problem has attracted great attention
in computer vision and signal processing. How to acquire high quality image
recovery effect is still an urgent task to be solved at present. This paper
proposes a new tensor $L_{2,1}$ norm minimization model (TLNM) that integrates
sum nuclear norm (SNN) method, differing from the classical tensor nuclear norm
(TNN)-based tensor completion method, with $L_{2,1}$ norm and Qatar Riyal
decomposition for solving the LRTC problem. To improve the utilization rate of
the local prior information of the image, a total variation (TV) regularization
term is introduced, resulting in a new class of tensor $L_{2,1}$ norm
minimization with total variation model (TLNMTV). Both proposed models are
convex and therefore have global optimal solutions. Moreover, we adopt the
Alternating Direction Multiplier Method (ADMM) to obtain the closed-form
solution of each variable, thus ensuring the feasibility of the algorithm.
Numerical experiments show that the two proposed algorithms are convergent and
outperform compared methods. In particular, our method significantly
outperforms the contrastive methods when the sampling rate of hyperspectral
images is 2.5\%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tensor Full Feature Measure and Its Nonconvex Relaxation Applications to Tensor Recovery. (arXiv:2109.12257v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12257">
<div class="article-summary-box-inner">
<span><p>Tensor sparse modeling as a promising approach, in the whole of science and
engineering has been a huge success. As is known to all, various data in
practical application are often generated by multiple factors, so the use of
tensors to represent the data containing the internal structure of multiple
factors came into being. However, different from the matrix case, constructing
reasonable sparse measure of tensor is a relatively difficult and very
important task. Therefore, in this paper, we propose a new tensor sparsity
measure called Tensor Full Feature Measure (FFM). It can simultaneously
describe the feature information of each dimension of the tensor and the
related features between two dimensions, and connect the Tucker rank with the
tensor tube rank. This measurement method can describe the sparse features of
the tensor more comprehensively. On this basis, we establish its non-convex
relaxation, and apply FFM to low rank tensor completion (LRTC) and tensor
robust principal component analysis (TRPCA). LRTC and TRPCA models based on FFM
are proposed, and two efficient Alternating Direction Multiplier Method (ADMM)
algorithms are developed to solve the proposed model. A variety of real
numerical experiments substantiate the superiority of the proposed methods
beyond state-of-the-arts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CTRN: Class-Temporal Relational Network for Action Detection. (arXiv:2110.13473v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.13473">
<div class="article-summary-box-inner">
<span><p>Action detection is an essential and challenging task, especially for densely
labelled datasets of untrimmed videos. There are many real-world challenges in
those datasets, such as composite action, co-occurring action, and high
temporal variation of instance duration. For handling these challenges, we
propose to explore both the class and temporal relations of detected actions.
In this work, we introduce an end-to-end network: Class-Temporal Relational
Network (CTRN). It contains three key components: (1) The Representation
Transform Module filters the class-specific features from the mixed
representations to build graph-structured data. (2) The Class-Temporal Module
models the class and temporal relations in a sequential manner. (3)
G-classifier leverages the privileged knowledge of the snippet-wise
co-occurring action pairs to further improve the co-occurring action detection.
We evaluate CTRN on three challenging densely labelled datasets and achieve
state-of-the-art performance, reflecting the effectiveness and robustness of
our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust deep learning-based semantic organ segmentation in hyperspectral images. (arXiv:2111.05408v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.05408">
<div class="article-summary-box-inner">
<span><p>Semantic image segmentation is an important prerequisite for
context-awareness and autonomous robotics in surgery. The state of the art has
focused on conventional RGB video data acquired during minimally invasive
surgery, but full-scene semantic segmentation based on spectral imaging data
and obtained during open surgery has received almost no attention to date. To
address this gap in the literature, we are investigating the following research
questions based on hyperspectral imaging (HSI) data of pigs acquired in an open
surgery setting: (1) What is an adequate representation of HSI data for neural
network-based fully automated organ segmentation, especially with respect to
the spatial granularity of the data (pixels vs. superpixels vs. patches vs.
full images)? (2) Is there a benefit of using HSI data compared to other
modalities, namely RGB data and processed HSI data (e.g. tissue parameters like
oxygenation), when performing semantic organ segmentation? According to a
comprehensive validation study based on 506 HSI images from 20 pigs, annotated
with a total of 19 classes, deep learning-based segmentation performance
increases, consistently across modalities, with the spatial context of the
input data. Unprocessed HSI data offers an advantage over RGB data or processed
data from the camera provider, with the advantage increasing with decreasing
size of the input to the neural network. Maximum performance (HSI applied to
whole images) yielded a mean DSC of 0.90 ((standard deviation (SD)) 0.04),
which is in the range of the inter-rater variability (DSC of 0.89 ((standard
deviation (SD)) 0.07)). We conclude that HSI could become a powerful image
modality for fully-automatic surgical scene understanding with many advantages
over traditional imaging, including the ability to recover additional
functional tissue information. Code and pre-trained models:
https://github.com/IMSY-DKFZ/htc.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiple Hypothesis Hypergraph Tracking for Posture Identification in Embryonic Caenorhabditis elegans. (arXiv:2111.06425v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.06425">
<div class="article-summary-box-inner">
<span><p>Current methods in multiple object tracking (MOT) rely on independent object
trajectories undergoing predictable motion to effectively track large numbers
of objects. Adversarial conditions such as volatile object motion and imperfect
detections create a challenging tracking landscape in which established methods
may yield inadequate results. Multiple hypothesis hypergraph tracking (MHHT) is
developed to perform MOT among interdependent objects amid noisy detections.
The method extends traditional multiple hypothesis tracking (MHT) via
hypergraphs to model correlated object motion, allowing for robust tracking in
challenging scenarios. MHHT is applied to perform seam cell tracking during
late-stage embryogenesis in embryonic C. elegans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pedestrian Detection by Exemplar-Guided Contrastive Learning. (arXiv:2111.08974v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.08974">
<div class="article-summary-box-inner">
<span><p>Typical methods for pedestrian detection focus on either tackling mutual
occlusions between crowded pedestrians, or dealing with the various scales of
pedestrians. Detecting pedestrians with substantial appearance diversities such
as different pedestrian silhouettes, different viewpoints or different
dressing, remains a crucial challenge. Instead of learning each of these
diverse pedestrian appearance features individually as most existing methods
do, we propose to perform contrastive learning to guide the feature learning in
such a way that the semantic distance between pedestrians with different
appearances in the learned feature space is minimized to eliminate the
appearance diversities, whilst the distance between pedestrians and background
is maximized. To facilitate the efficiency and effectiveness of contrastive
learning, we construct an exemplar dictionary with representative pedestrian
appearances as prior knowledge to construct effective contrastive training
pairs and thus guide contrastive learning. Besides, the constructed exemplar
dictionary is further leveraged to evaluate the quality of pedestrian proposals
during inference by measuring the semantic distance between the proposal and
the exemplar dictionary. Extensive experiments on both daytime and nighttime
pedestrian detection validate the effectiveness of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning Based Automated COVID-19 Classification from Computed Tomography Images. (arXiv:2111.11191v5 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11191">
<div class="article-summary-box-inner">
<span><p>A method of a Convolutional Neural Networks (CNN) for image classification
with image preprocessing and hyperparameters tuning was proposed. The method
aims at increasing the predictive performance for COVID-19 diagnosis while more
complex model architecture. Firstly, the CNN model includes four similar
convolutional layers followed by a flattening and two dense layers. This work
proposes a less complex solution based on simply classifying 2D-slices of
Computed Tomography scans. Despite the simplicity in architecture, the proposed
CNN model showed improved quantitative results exceeding state-of-the-art when
predicting slice cases. The results were achieved on the annotated CT slices of
the COV-19-CT-DB dataset. Secondly, the original dataset was processed via
anatomy-relevant masking of slice, removing none-representative slices from the
CT volume, and hyperparameters tuning. For slice processing, a fixed-sized
rectangular area was used for cropping an anatomy-relevant region-of-interest
in the images, and a threshold based on the number of white pixels in binarized
slices was employed to remove none-representative slices from the 3D-CT scans.
The CNN model with a learning rate schedule and an exponential decay and slice
flipping techniques was deployed on the processed slices. The proposed method
was used to make predictions on the 2D slices and for final diagnosis at
patient level, majority voting was applied on the slices of each CT scan to
take the diagnosis. The macro F1 score of the proposed method well-exceeded the
baseline approach and other alternatives on the validation set as well as on a
test partition of previously unseen images from COV-19CT-DB dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Classification-Regression for Chart Comprehension. (arXiv:2111.14792v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14792">
<div class="article-summary-box-inner">
<span><p>Chart question answering (CQA) is a task used for assessing chart
comprehension, which is fundamentally different from understanding natural
images. CQA requires analyzing the relationships between the textual and the
visual components of a chart, in order to answer general questions or infer
numerical values. Most existing CQA datasets and models are based on
simplifying assumptions that often enable surpassing human performance. In this
work, we address this outcome and propose a new model that jointly learns
classification and regression. Our language-vision setup uses co-attention
transformers to capture the complex real-world interactions between the
question and the textual elements. We validate our design with extensive
experiments on the realistic PlotQA dataset, outperforming previous approaches
by a large margin, while showing competitive performance on FigureQA. Our model
is particularly well suited for realistic questions with out-of-vocabulary
answers that require regression.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeRF-SR: High-Quality Neural Radiance Fields using Super-Sampling. (arXiv:2112.01759v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01759">
<div class="article-summary-box-inner">
<span><p>We present NeRF-SR, a solution for high-resolution (HR) novel view synthesis
with mostly low-resolution (LR) inputs. Our method is built upon Neural
Radiance Fields (NeRF) that predicts per-point density and color with a
multi-layer perceptron. While producing images at arbitrary scales, NeRF
struggles with resolutions that go beyond observed images. Our key insight is
that NeRF benefits from 3D consistency, which means an observed pixel absorbs
information from nearby views. We first exploit it by a super-sampling strategy
that shoots multiple rays at each image pixel, which further enforces
multi-view constraint at a sub-pixel level. Then, we show that NeRF-SR can
further boost the performance of super-sampling by a refinement network that
leverages the estimated depth at hand to hallucinate details from related
patches on only one HR reference image. Experiment results demonstrate that
NeRF-SR generates high-quality results for novel view synthesis at HR on both
synthetic and real-world datasets without any external information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Classification of COVID-19 on chest X-Ray images using Deep Learning model with Histogram Equalization and Lungs Segmentation. (arXiv:2112.02478v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02478">
<div class="article-summary-box-inner">
<span><p>Background and Objective: Artificial intelligence (AI) methods coupled with
biomedical analysis has a critical role during pandemics as it helps to release
the overwhelming pressure from healthcare systems and physicians. As the
ongoing COVID-19 crisis worsens in countries having dense populations and
inadequate testing kits like Brazil and India, radiological imaging can act as
an important diagnostic tool to accurately classify covid-19 patients and
prescribe the necessary treatment in due time. With this motivation, we present
our study based on deep learning architecture for detecting covid-19 infected
lungs using chest X-rays. Dataset: We collected a total of 2470 images for
three different class labels, namely, healthy lungs, ordinary pneumonia, and
covid-19 infected pneumonia, out of which 470 X-ray images belong to the
covid-19 category. Methods: We first pre-process all the images using histogram
equalization techniques and segment them using U-net architecture. VGG-16
network is then used for feature extraction from the pre-processed images which
is further sampled by SMOTE oversampling technique to achieve a balanced
dataset. Finally, the class-balanced features are classified using a support
vector machine (SVM) classifier with 10-fold cross-validation and the accuracy
is evaluated. Result and Conclusion: Our novel approach combining well-known
pre-processing techniques, feature extraction methods, and dataset balancing
method, lead us to an outstanding rate of recognition of 98% for COVID-19
images over a dataset of 2470 X-ray images. Our model is therefore fit to be
utilized in healthcare facilities for screening purposes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DoodleFormer: Creative Sketch Drawing with Transformers. (arXiv:2112.03258v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03258">
<div class="article-summary-box-inner">
<span><p>Creative sketching or doodling is an expressive activity, where imaginative
and previously unseen depictions of everyday visual objects are drawn. Creative
sketch image generation is a challenging vision problem, where the task is to
generate diverse, yet realistic creative sketches possessing the unseen
composition of the visual-world objects. Here, we propose a novel
coarse-to-fine two-stage framework, DoodleFormer, that decomposes the creative
sketch generation problem into the creation of coarse sketch composition
followed by the incorporation of fine-details in the sketch. We introduce
graph-aware transformer encoders that effectively capture global dynamic as
well as local static structural relations among different body parts. To ensure
diversity of the generated creative sketches, we introduce a probabilistic
coarse sketch decoder that explicitly models the variations of each sketch body
part to be drawn. Experiments are performed on two creative sketch datasets:
Creative Birds and Creative Creatures. Our qualitative, quantitative and
human-based evaluations show that DoodleFormer outperforms the state-of-the-art
on both datasets, yielding realistic and diverse creative sketches. On Creative
Creatures, DoodleFormer achieves an absolute gain of 25 in terms of Fr`echet
inception distance (FID) over the state-of-the-art. We also demonstrate the
effectiveness of DoodleFormer for related applications of text to creative
sketch generation and sketch completion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fully Attentional Network for Semantic Segmentation. (arXiv:2112.04108v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04108">
<div class="article-summary-box-inner">
<span><p>Recent non-local self-attention methods have proven to be effective in
capturing long-range dependencies for semantic segmentation. These methods
usually form a similarity map of RC*C (by compressing spatial dimensions) or
RHW*HW (by compressing channels) to describe the feature relations along either
channel or spatial dimensions, where C is the number of channels, H and W are
the spatial dimensions of the input feature map. However, such practices tend
to condense feature dependencies along the other dimensions,hence causing
attention missing, which might lead to inferior results for small/thin
categories or inconsistent segmentation inside large objects. To address this
problem, we propose anew approach, namely Fully Attentional Network (FLANet),to
encode both spatial and channel attentions in a single similarity map while
maintaining high computational efficiency. Specifically, for each channel map,
our FLANet can harvest feature responses from all other channel maps, and the
associated spatial positions as well, through a novel fully attentional module.
Our new method has achieved state-of-the-art performance on three challenging
semantic segmentation datasets,i.e., 83.6%, 46.99%, and 88.5% on the Cityscapes
test set,the ADE20K validation set, and the PASCAL VOC test set,respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Looking Beyond Corners: Contrastive Learning of Visual Representations for Keypoint Detection and Description Extraction. (arXiv:2112.12002v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.12002">
<div class="article-summary-box-inner">
<span><p>Learnable keypoint detectors and descriptors are beginning to outperform
classical hand-crafted feature extraction methods. Recent studies on
self-supervised learning of visual representations have driven the increasing
performance of learnable models based on deep networks. By leveraging
traditional data augmentations and homography transformations, these networks
learn to detect corners under adverse conditions such as extreme illumination
changes. However, their generalization capabilities are limited to corner-like
features detected a priori by classical methods or synthetically generated
data.
</p>
<p>In this paper, we propose the Correspondence Network (CorrNet) that learns to
detect repeatable keypoints and to extract discriminative descriptions via
unsupervised contrastive learning under spatial constraints. Our experiments
show that CorrNet is not only able to detect low-level features such as
corners, but also high-level features that represent similar objects present in
a pair of input images through our proposed joint guided backpropagation of
their latent space. Our approach obtains competitive results under viewpoint
changes and achieves state-of-the-art performance under illumination changes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Free-Viewpoint RGB-D Human Performance Capture and Rendering. (arXiv:2112.13889v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13889">
<div class="article-summary-box-inner">
<span><p>Capturing and faithfully rendering photo-realistic humans from novel views is
a fundamental problem for AR/VR applications. While prior work has shown
impressive performance capture results in laboratory settings, it is
non-trivial to achieve casual free-viewpoint human capture and rendering for
unseen identities with high fidelity, especially for facial expressions, hands,
and clothes. To tackle these challenges we introduce a novel view synthesis
framework that generates realistic renders from unseen views of any human
captured from a single-view and sparse RGB-D sensor, similar to a low-cost
depth camera, and without actor-specific models. We propose an architecture to
create dense feature maps in novel views obtained by sphere-based neural
rendering, and create complete renders using a global context inpainting model.
Additionally, an enhancer network leverages the overall fidelity, even in
occluded areas from the original view, producing crisp renders with fine
details. We show that our method generates high-quality novel views of
synthetic and real human actors given a single-stream, sparse RGB-D input. It
generalizes to unseen identities, and new poses and faithfully reconstructs
facial expressions. Our approach outperforms prior view synthesis methods and
is robust to different levels of depth sparsity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tensor Recovery Based on Tensor Equivalent Minimax-Concave Penalty. (arXiv:2201.12709v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12709">
<div class="article-summary-box-inner">
<span><p>Tensor recovery is an important problem in computer vision and machine
learning. It usually uses the convex relaxation of tensor rank and $l_{0}$
norm, i.e., the nuclear norm and $l_{1}$ norm respectively, to solve such
problem. Convex approximations are known to produce biased estimators. To
overcome this problem, a corresponding non-convex regularizer is adopted and
designed. Inspired by the recently developed matrix equivalent Minimax-Concave
Penalty (EMCP) theorem, a theorem of tensor equivalent Minimax-Concave Penalty
(TEMCP) is established in this paper. Tensor equivalent MCP (TEMCP) as the
non-convex regularizer part and equivalent weighted tensor $\gamma$ norm
(EWTGN) as the low-rank part are constructed, both of which can achieve weight
adaptive. Meanwhile, we propose two corresponding adaptive models for two
classical tensor recovery problems, namely, low-rank tensor completion (LRTC)
and tensor robust principal component analysis (TRPCA), in which the
optimization algorithm is based on alternating direction multiplier (ADMM).
This novel iterative adaptive algorithm is devised, which can produce more
accurate tensor recovery effect. For the tensor completion model, multispectral
image (MSI), magnetic resonance imaging (MRI) and color video (CV) data are
considered, while for the tensor robust principal component analysis model,
hyperspectral image (HSI) denoising under gaussian noise plus salt and pepper
noise is considered. The proposed algorithm is superior to the state-of-arts
method, and the reduction and convergence of which are guaranteed through
experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Motion-Plane-Adaptive Inter Prediction in 360-Degree Video Coding. (arXiv:2202.03323v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03323">
<div class="article-summary-box-inner">
<span><p>Inter prediction is one of the key technologies enabling the high compression
efficiency of modern video coding standards. 360-degree video needs to be
mapped to the 2D image plane prior to coding in order to allow compression
using existing video coding standards. The distortions that inevitably occur
when mapping spherical data onto the 2D image plane, however, impair the
performance of classical inter prediction techniques. In this paper, we propose
a motion-plane-adaptive inter prediction technique (MPA) for 360-degree video
that takes the spherical characteristics of 360-degree video into account.
Based on the known projection format of the video, MPA allows to perform inter
prediction on different motion planes in 3D space instead of having to work on
the - in theory arbitrarily mapped - 2D image representation directly. We
furthermore derive a motion-plane-adaptive motion vector prediction technique
(MPA-MVP) that allows to translate motion information between different motion
planes and motion models. Our proposed integration of MPA together with MPA-MVP
into the state-of-the-art H.266/VVC video coding standard shows significant
Bjontegaard Delta rate savings of 1.72% with a peak of 3.97% based on PSNR and
1.56% with a peak of 3.40% based on WS-PSNR compared to the VTM-14.2 baseline
on average.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Motion Puzzle: Arbitrary Motion Style Transfer by Body Part. (arXiv:2202.05274v2 [cs.GR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05274">
<div class="article-summary-box-inner">
<span><p>This paper presents Motion Puzzle, a novel motion style transfer network that
advances the state-of-the-art in several important respects. The Motion Puzzle
is the first that can control the motion style of individual body parts,
allowing for local style editing and significantly increasing the range of
stylized motions. Designed to keep the human's kinematic structure, our
framework extracts style features from multiple style motions for different
body parts and transfers them locally to the target body parts. Another major
advantage is that it can transfer both global and local traits of motion style
by integrating the adaptive instance normalization and attention modules while
keeping the skeleton topology. Thus, it can capture styles exhibited by dynamic
movements, such as flapping and staggering, significantly better than previous
work. In addition, our framework allows for arbitrary motion style transfer
without datasets with style labeling or motion pairing, making many publicly
available motion datasets available for training. Our framework can be easily
integrated with motion generation frameworks to create many applications, such
as real-time motion transfer. We demonstrate the advantages of our framework
with a number of examples and comparisons with previous work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Depth-Cooperated Trimodal Network for Video Salient Object Detection. (arXiv:2202.06060v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06060">
<div class="article-summary-box-inner">
<span><p>Depth can provide useful geographical cues for salient object detection
(SOD), and has been proven helpful in recent RGB-D SOD methods. However,
existing video salient object detection (VSOD) methods only utilize
spatiotemporal information and seldom exploit depth information for detection.
In this paper, we propose a depth-cooperated trimodal network, called DCTNet
for VSOD, which is a pioneering work to incorporate depth information to assist
VSOD. To this end, we first generate depth from RGB frames, and then propose an
approach to treat the three modalities unequally. Specifically, a multi-modal
attention module (MAM) is designed to model multi-modal long-range dependencies
between the main modality (RGB) and the two auxiliary modalities (depth,
optical flow). We also introduce a refinement fusion module (RFM) to suppress
noises in each modality and select useful information dynamically for further
feature refinement. Lastly, a progressive fusion strategy is adopted after the
refined features to achieve final cross-modal fusion. Experiments on five
benchmark datasets demonstrate the superiority of our depth-cooperated model
against 12 state-of-the-art methods, and the necessity of depth is also
validated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Universal Adversarial Examples in Remote Sensing: Methodology and Benchmark. (arXiv:2202.07054v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07054">
<div class="article-summary-box-inner">
<span><p>Deep neural networks have achieved great success in many important remote
sensing tasks. Nevertheless, their vulnerability to adversarial examples should
not be neglected. In this study, we systematically analyze the universal
adversarial examples in remote sensing data for the first time, without any
knowledge from the victim model. Specifically, we propose a novel black-box
adversarial attack method, namely Mixup-Attack, and its simple variant
Mixcut-Attack, for remote sensing data. The key idea of the proposed methods is
to find common vulnerabilities among different networks by attacking the
features in the shallow layer of a given surrogate model. Despite their
simplicity, the proposed methods can generate transferable adversarial examples
that deceive most of the state-of-the-art deep neural networks in both scene
classification and semantic segmentation tasks with high success rates. We
further provide the generated universal adversarial examples in the dataset
named UAE-RS, which is the first dataset that provides black-box adversarial
samples in the remote sensing field. We hope UAE-RS may serve as a benchmark
that helps researchers to design deep neural networks with strong resistance
toward adversarial attacks in the remote sensing field. Codes and the UAE-RS
dataset are available online (https://github.com/YonghaoXu/UAE-RS).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MANet: Improving Video Denoising with a Multi-Alignment Network. (arXiv:2202.09704v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09704">
<div class="article-summary-box-inner">
<span><p>In video denoising, the adjacent frames often provide very useful
information, but accurate alignment is needed before such information can be
harnassed. In this work, we present a multi-alignment network, which generates
multiple flow proposals followed by attention-based averaging. It serves to
mimic the non-local mechanism, suppressing noise by averaging multiple
observations. Our approach can be applied to various state-of-the-art models
that are based on flow estimation. Experiments on a large-scale video dataset
demonstrate that our method improves the denoising baseline model by 0.2dB, and
further reduces the parameters by 47% with model distillation. Code is
available at https://github.com/IndigoPurple/MANet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Attention Network. (arXiv:2202.09741v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09741">
<div class="article-summary-box-inner">
<span><p>While originally designed for natural language processing tasks, the
self-attention mechanism has recently taken various computer vision areas by
storm. However, the 2D nature of images brings three challenges for applying
self-attention in computer vision. (1) Treating images as 1D sequences neglects
their 2D structures. (2) The quadratic complexity is too expensive for
high-resolution images. (3) It only captures spatial adaptability but ignores
channel adaptability. In this paper, we propose a novel linear attention named
large kernel attention (LKA) to enable self-adaptive and long-range
correlations in self-attention while avoiding its shortcomings. Furthermore, we
present a neural network based on LKA, namely Visual Attention Network (VAN).
While extremely simple, VAN surpasses similar size vision transformers(ViTs)
and convolutional neural networks(CNNs) in various tasks, including image
classification, object detection, semantic segmentation, panoptic segmentation,
pose estimation, etc. For example, VAN-B6 achieves 87.8% accuracy on ImageNet
benchmark and set new state-of-the-art performance (58.2 PQ) for panoptic
segmentation. Besides, VAN-B2 surpasses Swin-T 4% mIoU (50.1 vs. 46.1) for
semantic segmentation on ADE20K benchmark, 2.6% AP (48.8 vs. 46.2) for object
detection on COCO dataset. It provides a novel method and a simple yet strong
baseline for the community. Code is available at
https://github.com/Visual-Attention-Network.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep learning classification of large-scale point clouds: A case study on cuneiform tablets. (arXiv:2202.10851v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.10851">
<div class="article-summary-box-inner">
<span><p>This paper introduces a novel network architecture for the classification of
large-scale point clouds. The network is used to classify metadata from
cuneiform tablets. As more than half a million tablets remain unprocessed, this
can help create an overview of the tablets. The network is tested on a
comparison dataset and obtains state-of-the-art performance. We also introduce
new metadata classification tasks on which the network shows promising results.
Finally, we introduce the novel Maximum Attention visualization, demonstrating
that the trained network focuses on the intended features. Code available at
https://github.com/fhagelskjaer/dlc-cuneiform
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RelMobNet: End-to-end relative camera pose estimation using a robust two-stage training. (arXiv:2202.12838v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12838">
<div class="article-summary-box-inner">
<span><p>Relative camera pose estimation, i.e. estimating the translation and rotation
vectors using a pair of images taken in different locations, is an important
part of systems in augmented reality and robotics. In this paper, we present an
end-to-end relative camera pose estimation network using a siamese architecture
that is independent of camera parameters. The network is trained using the
Cambridge Landmarks data with four individual scene datasets and a dataset
combining the four scenes. To improve generalization, we propose a novel
two-stage training that alleviates the need of a hyperparameter to balance the
translation and rotation loss scale. The proposed method is compared with
one-stage training CNN-based methods such as RPNet and RCPNet and demonstrate
that the proposed model improves translation vector estimation by 16.11%,
28.88%, and 52.27% on the Kings College, Old Hospital, and St Marys Church
scenes, respectively. For proving texture invariance, we investigate the
generalization of the proposed method augmenting the datasets to different
scene styles, as ablation studies, using generative adversarial networks. Also,
we present a qualitative assessment of epipolar lines of our network
predictions and ground truth poses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Point Label Aware Superpixels for Multi-species Segmentation of Underwater Imagery. (arXiv:2202.13487v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13487">
<div class="article-summary-box-inner">
<span><p>Monitoring coral reefs using underwater vehicles increases the range of
marine surveys and availability of historical ecological data by collecting
significant quantities of images. Analysis of this imagery can be automated
using a model trained to perform semantic segmentation, however it is too
costly and time-consuming to densely label images for training supervised
models. In this letter, we leverage photo-quadrat imagery labeled by ecologists
with sparse point labels. We propose a point label aware method for propagating
labels within superpixel regions to obtain augmented ground truth for training
a semantic segmentation model. Our point label aware superpixel method utilizes
the sparse point labels, and clusters pixels using learned features to
accurately generate single-species segments in cluttered, complex coral images.
Our method outperforms prior methods on the UCSD Mosaics dataset by 3.62% for
pixel accuracy and 8.35% for mean IoU for the label propagation task, while
reducing computation time reported by previous approaches by 76%. We train a
DeepLabv3+ architecture and outperform state-of-the-art for semantic
segmentation by 2.91% for pixel accuracy and 9.65% for mean IoU on the UCSD
Mosaics dataset and by 4.19% for pixel accuracy and 14.32% mean IoU for the
Eilat dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual BatchNorm Adaptation (CBNA) for Semantic Segmentation. (arXiv:2203.01074v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01074">
<div class="article-summary-box-inner">
<span><p>Environment perception in autonomous driving vehicles often heavily relies on
deep neural networks (DNNs), which are subject to domain shifts, leading to a
significantly decreased performance during DNN deployment. Usually, this
problem is addressed by unsupervised domain adaptation (UDA) approaches trained
either simultaneously on source and target domain datasets or even source-free
only on target data in an offline fashion. In this work, we further expand a
source-free UDA approach to a continual and therefore online-capable UDA on a
single-image basis for semantic segmentation. Accordingly, our method only
requires the pre-trained model from the supplier (trained in the source domain)
and the current (unlabeled target domain) camera image. Our method Continual
BatchNorm Adaptation (CBNA) modifies the source domain statistics in the batch
normalization layers, using target domain images in an unsupervised fashion,
which yields consistent performance improvements during inference. Thereby, in
contrast to existing works, our approach can be applied to improve a DNN
continuously on a single-image basis during deployment without access to source
data, without algorithmic delay, and nearly without computational overhead. We
show the consistent effectiveness of our method across a wide variety of
source/target domain settings for semantic segmentation. Code is available at
https://github.com/ifnspaml/CBNA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recent, rapid advancement in visual question answering architecture: a review. (arXiv:2203.01322v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01322">
<div class="article-summary-box-inner">
<span><p>Understanding visual question answering is going to be crucial for numerous
human activities. However, it presents major challenges at the heart of the
artificial intelligence endeavor. This paper presents an update on the rapid
advancements in visual question answering using images that have occurred in
the last couple of years. Tremendous growth in research on improving visual
question answering system architecture has been published recently, showing the
importance of multimodal architectures. Several points on the benefits of
visual question answering are mentioned in the review paper by Manmadhan et al.
(2020), on which the present article builds, including subsequent updates in
the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatio-Temporal Gating-Adjacency GCN for Human Motion Prediction. (arXiv:2203.01474v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01474">
<div class="article-summary-box-inner">
<span><p>Predicting future motion based on historical motion sequence is a fundamental
problem in computer vision, and it has wide applications in autonomous driving
and robotics. Some recent works have shown that Graph Convolutional
Networks(GCN) are instrumental in modeling the relationship between different
joints. However, considering the variants and diverse action types in human
motion data, the cross-dependency of the spatio-temporal relationships will be
difficult to depict due to the decoupled modeling strategy, which may also
exacerbate the problem of insufficient generalization. Therefore, we propose
the Spatio-Temporal Gating-Adjacency GCN(GAGCN) to learn the complex
spatio-temporal dependencies over diverse action types. Specifically, we adopt
gating networks to enhance the generalization of GCN via the trainable adaptive
adjacency matrix obtained by blending the candidate spatio-temporal adjacency
matrices. Moreover, GAGCN addresses the cross-dependency of space and time by
balancing the weights of spatio-temporal modeling and fusing the decoupled
spatio-temporal features. Extensive experiments on Human 3.6M, AMASS, and 3DPW
demonstrate that GAGCN achieves state-of-the-art performance in both short-term
and long-term predictions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ensemble Knowledge Guided Sub-network Search and Fine-tuning for Filter Pruning. (arXiv:2203.02651v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02651">
<div class="article-summary-box-inner">
<span><p>Conventional NAS-based pruning algorithms aim to find the sub-network with
the best validation performance. However, validation performance does not
successfully represent test performance, i.e., potential performance. Also,
although fine-tuning the pruned network to restore the performance drop is an
inevitable process, few studies have handled this issue. This paper provides a
novel Ensemble Knowledge Guidance (EKG) to solve both problems at once. First,
we experimentally prove that the fluctuation of loss landscape can be an
effective metric to evaluate the potential performance. In order to search a
sub-network with the smoothest loss landscape at a low cost, we employ EKG as a
search reward. EKG utilized for the following search iteration is composed of
the ensemble knowledge of interim sub-networks, i.e., the by-products of the
sub-network evaluation. Next, we reuse EKG to provide a gentle and informative
guidance to the pruned network while fine-tuning the pruned network. Since EKG
is implemented as a memory bank in both phases, it requires a negligible cost.
For example, when pruning and training ResNet-50, just 315 GPU hours are
required to remove around 45.04% of FLOPS without any performance degradation,
which can operate even on a low-spec workstation. the implemented code is
available at https://github.com/sseung0703/EKG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic-Aware Latent Space Exploration for Face Image Restoration. (arXiv:2203.03005v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03005">
<div class="article-summary-box-inner">
<span><p>For image restoration, the majority of existing deep learning-based
algorithms have a tendency to overfit the training data, resulting in poor
performance when confronted with unseen degradations. To achieve more robust
restoration, generative adversarial network (GAN) prior based methods have been
proposed, demonstrating a promising capacity to restore photo-realistic and
high-quality results. However, these methods are susceptible to semantic
ambiguity, particularly with semantically relevant images such as facial
images. In this paper, we propose a semantic-aware latent space exploration
method for image restoration (SAIR). By explicitly modeling referenced
semantics information, SAIR is able to reliably restore severely degraded
images not only to high-resolution highly-realistic looks but also to correct
semantics. Quantitative and qualitative experiments collectively demonstrate
the effectiveness of the proposed SAIR. Our code can be found in
https://github.com/Liamkuo/SAIR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection. (arXiv:2203.03605v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03605">
<div class="article-summary-box-inner">
<span><p>We present DINO (\textbf{D}ETR with \textbf{I}mproved de\textbf{N}oising
anch\textbf{O}r boxes), a state-of-the-art end-to-end object detector. % in
this paper. DINO improves over previous DETR-like models in performance and
efficiency by using a contrastive way for denoising training, a mixed query
selection method for anchor initialization, and a look forward twice scheme for
box prediction. DINO achieves $49.4$AP in $12$ epochs and $51.3$AP in $24$
epochs on COCO with a ResNet-50 backbone and multi-scale features, yielding a
significant improvement of $\textbf{+6.0}$\textbf{AP} and
$\textbf{+2.7}$\textbf{AP}, respectively, compared to DN-DETR, the previous
best DETR-like model. DINO scales well in both model size and data size.
Without bells and whistles, after pre-training on the Objects365 dataset with a
SwinL backbone, DINO obtains the best results on both COCO \texttt{val2017}
($\textbf{63.2}$\textbf{AP}) and \texttt{test-dev}
(\textbf{$\textbf{63.3}$AP}). Compared to other models on the leaderboard, DINO
significantly reduces its model size and pre-training data size while achieving
better results. Our code will be available at
\url{https://github.com/IDEACVR/DINO}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EdgeFormer: Improving Light-weight ConvNets by Learning from Vision Transformers. (arXiv:2203.03952v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03952">
<div class="article-summary-box-inner">
<span><p>Recently, vision transformers started to show impressive results which
outperform large convolution based models significantly. However, in the area
of small models for mobile or resource constrained devices, ConvNet still has
its own advantages in both performance and model complexity. We propose
EdgeFormer, a pure ConvNet based backbone model that further strengthens these
advantages by fusing the merits of vision transformers into ConvNets.
Specifically, we propose global circular convolution (GCC) with position
embeddings, a light-weight convolution op which boasts a global receptive field
while producing location sensitive features as in local convolutions. We
combine the GCCs and squeeze-exictation ops to form a meta-former like model
block, which further has the attention mechanism like transformers. The
aforementioned block can be used in plug-and-play manner to replace relevant
blocks in ConvNets or transformers. Experiment results show that the proposed
EdgeFormer achieves better performance than popular light-weight ConvNets and
vision transformer based models in common vision tasks and datasets, while
having fewer parameters and faster inference speed. For classification on
ImageNet-1k, EdgeFormer achieves 78.6% top-1 accuracy with about 5.0 million
parameters, saving 11% parameters and 13% computational cost but gaining 0.2%
higher accuracy and 23% faster inference speed (on ARM based Rockchip RK3288)
compared with MobileViT, and uses only 0.5 times parameters but gaining 2.7%
accuracy compared with DeIT. On MS-COCO object detection and PASCAL VOC
segmentation tasks, EdgeFormer also shows better performance. Code is available
at https://github.com/hkzhang91/EdgeFormer
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Source-free Video Domain Adaptation by Learning Temporal Consistency for Action Recognition. (arXiv:2203.04559v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04559">
<div class="article-summary-box-inner">
<span><p>Video-based Unsupervised Domain Adaptation (VUDA) methods improve the
robustness of video models, enabling them to be applied to action recognition
tasks across different environments. However, these methods require constant
access to source data during the adaptation process. Yet in many real-world
applications, subjects and scenes in the source video domain should be
irrelevant to those in the target video domain. With the increasing emphasis on
data privacy, such methods that require source data access would raise serious
privacy issues. Therefore, to cope with such concern, a more practical domain
adaptation scenario is formulated as the Source-Free Video-based Domain
Adaptation (SFVDA). Though there are a few methods for Source-Free Domain
Adaptation (SFDA) on image data, these methods yield degenerating performance
in SFVDA due to the multi-modality nature of videos, with the existence of
additional temporal features. In this paper, we propose a novel Attentive
Temporal Consistent Network (ATCoN) to address SFVDA by learning temporal
consistency, guaranteed by two novel consistency objectives, namely feature
consistency and source prediction consistency, performed across local temporal
features. ATCoN further constructs effective overall temporal features by
attending to local temporal features based on prediction confidence. Empirical
results demonstrate the state-of-the-art performance of ATCoN across various
cross-domain action recognition benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Coarse-to-Fine Sparse Transformer for Hyperspectral Image Reconstruction. (arXiv:2203.04845v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04845">
<div class="article-summary-box-inner">
<span><p>Many algorithms have been developed to solve the inverse problem of coded
aperture snapshot spectral imaging (CASSI), i.e., recovering the 3D
hyperspectral images (HSIs) from a 2D compressive measurement. In recent years,
learning-based methods have demonstrated promising performance and dominated
the mainstream research direction. However, existing CNN-based methods show
limitations in capturing long-range dependencies and non-local self-similarity.
Previous Transformer-based methods densely sample tokens, some of which are
uninformative, and calculate the multi-head self-attention (MSA) between some
tokens that are unrelated in content. This does not fit the spatially sparse
nature of HSI signals and limits the model scalability. In this paper, we
propose a novel Transformer-based method, coarse-to-fine sparse Transformer
(CST), firstly embedding HSI sparsity into deep learning for HSI
reconstruction. In particular, CST uses our proposed spectra-aware screening
mechanism (SASM) for coarse patch selecting. Then the selected patches are fed
into our customized spectra-aggregation hashing multi-head self-attention
(SAH-MSA) for fine pixel clustering and self-similarity capturing.
Comprehensive experiments show that our CST significantly outperforms
state-of-the-art methods while requiring cheaper computational costs. The code
and models will be released at https://github.com/caiyuanhao1998/MST
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Curve Translator for High-Resolution Photorealistic Image Translation. (arXiv:2203.07756v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07756">
<div class="article-summary-box-inner">
<span><p>The dominant image-to-image translation methods are based on fully
convolutional networks, which extract and translate an image's features and
then reconstruct the image. However, they have unacceptable computational costs
when working with high-resolution images. To this end, we present the
Multi-Curve Translator (MCT), which not only predicts the translated pixels for
the corresponding input pixels but also for their neighboring pixels. And if a
high-resolution image is downsampled to its low-resolution version, the lost
pixels are the remaining pixels' neighboring pixels. So MCT makes it possible
to feed the network only the downsampled image to perform the mapping for the
full-resolution image, which can dramatically lower the computational cost.
Besides, MCT is a plug-in approach that utilizes existing base models and
requires only replacing their output layers. Experiments demonstrate that the
MCT variants can process 4K images in real-time and achieve comparable or even
better performance than the base models on various photorealistic
image-to-image translation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Multi-Domain Long-Tailed Recognition, Imbalanced Domain Generalization and Beyond. (arXiv:2203.09513v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09513">
<div class="article-summary-box-inner">
<span><p>Real-world data often exhibit imbalanced label distributions. Existing
studies on data imbalance focus on single-domain settings, i.e., samples are
from the same data distribution. However, natural data can originate from
distinct domains, where a minority class in one domain could have abundant
instances from other domains. We formalize the task of Multi-Domain Long-Tailed
Recognition (MDLT), which learns from multi-domain imbalanced data, addresses
label imbalance, domain shift, and divergent label distributions across
domains, and generalizes to all domain-class pairs. We first develop the
domain-class transferability graph, and show that such transferability governs
the success of learning in MDLT. We then propose BoDA, a theoretically grounded
learning strategy that tracks the upper bound of transferability statistics,
and ensures balanced alignment and calibration across imbalanced domain-class
distributions. We curate five MDLT benchmarks based on widely-used multi-domain
datasets, and compare BoDA to twenty algorithms that span different learning
strategies. Extensive and rigorous experiments verify the superior performance
of BoDA. Further, as a byproduct, BoDA establishes new state-of-the-art on
Domain Generalization benchmarks, highlighting the importance of addressing
data imbalance across domains, which can be crucial for improving
generalization to unseen domains. Code and data are available at:
https://github.com/YyzHarry/multi-domain-imbalance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unitail: Detecting, Reading, and Matching in Retail Scene. (arXiv:2204.00298v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00298">
<div class="article-summary-box-inner">
<span><p>To make full use of computer vision technology in stores, it is required to
consider the actual needs that fit the characteristics of the retail scene.
Pursuing this goal, we introduce the United Retail Datasets (Unitail), a
large-scale benchmark of basic visual tasks on products that challenges
algorithms for detecting, reading, and matching. With 1.8M quadrilateral-shaped
instances annotated, the Unitail offers a detection dataset to align product
appearance better. Furthermore, it provides a gallery-style OCR dataset
containing 1454 product categories, 30k text regions, and 21k transcriptions to
enable robust reading on products and motivate enhanced product matching.
Besides benchmarking the datasets using various state-of-the-arts, we customize
a new detector for product detection and provide a simple OCR-based matching
solution that verifies its effectiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Dempster-Shafer approach to trustworthy AI with application to fetal brain MRI segmentation. (arXiv:2204.02779v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02779">
<div class="article-summary-box-inner">
<span><p>Deep learning models for medical image segmentation can fail unexpectedly and
spectacularly for pathological cases and images acquired at different centers
than training images, with labeling errors that violate expert knowledge. Such
errors undermine the trustworthiness of deep learning models for medical image
segmentation. Mechanisms for detecting and correcting such failures are
essential for safely translating this technology into clinics and are likely to
be a requirement of future regulations on artificial intelligence (AI). In this
work, we propose a trustworthy AI theoretical framework and a practical system
that can augment any backbone AI system using a fallback method and a fail-safe
mechanism based on Dempster-Shafer theory. Our approach relies on an actionable
definition of trustworthy AI. Our method automatically discards the voxel-level
labeling predicted by the backbone AI that violate expert knowledge and relies
on a fallback for those voxels. We demonstrate the effectiveness of the
proposed trustworthy AI approach on the largest reported annotated dataset of
fetal MRI consisting of 540 manually annotated fetal brain 3D T2w MRIs from 13
centers. Our trustworthy AI method improves the robustness of a
state-of-the-art backbone AI for fetal brain MRIs acquired across various
centers and for fetuses with various brain abnormalities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fashionformer: A simple, Effective and Unified Baseline for Human Fashion Segmentation and Recognition. (arXiv:2204.04654v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04654">
<div class="article-summary-box-inner">
<span><p>Human fashion understanding is one crucial computer vision task since it has
comprehensive information for real-world applications. This focus on joint
human fashion segmentation and attribute recognition. Contrary to the previous
works that separately model each task as a multi-head prediction problem, our
insight is to bridge these two tasks with one unified model via vision
transformer modeling to benefit each task. In particular, we introduce the
object query for segmentation and the attribute query for attribute prediction.
Both queries and their corresponding features can be linked via mask
prediction. Then we adopt a two-stream query learning framework to learn the
decoupled query representations.We design a novel Multi-Layer Rendering module
for attribute stream to explore more fine-grained features. The decoder design
shares the same spirit as DETR. Thus we name the proposed method
\textit{Fahsionformer}. Extensive experiments on three human fashion datasets
illustrate the effectiveness of our approach. In particular, our method with
the same backbone achieve \textbf{relative 10\% improvements} than previous
works in case of \textit{a joint metric (AP$^{\text{mask}}_{\text{IoU+F}_1}$)
for both segmentation and attribute recognition}. To the best of our knowledge,
we are the first unified end-to-end vision transformer framework for human
fashion analysis. We hope this simple yet effective method can serve as a new
flexible baseline for fashion analysis. Code is available at
https://github.com/xushilin1/FashionFormer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Panoptic-PartFormer: Learning a Unified Model for Panoptic Part Segmentation. (arXiv:2204.04655v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04655">
<div class="article-summary-box-inner">
<span><p>Panoptic Part Segmentation (PPS) aims to unify panoptic segmentation and part
segmentation into one task. Previous work mainly utilizes separated approaches
to handle thing, stuff, and part predictions individually without performing
any shared computation and task association. In this work, we aim to unify
these tasks at the architectural level, designing the first end-to-end unified
method named Panoptic-PartFormer. In particular, motivated by the recent
progress in Vision Transformer, we model things, stuff, and part as object
queries and directly learn to optimize the all three predictions as unified
mask prediction and classification problem. We design a decoupled decoder to
generate part feature and thing/stuff feature respectively. Then we propose to
utilize all the queries and corresponding features to perform reasoning jointly
and iteratively. The final mask can be obtained via inner product between
queries and the corresponding features. The extensive ablation studies and
analysis prove the effectiveness of our framework. Our Panoptic-PartFormer
achieves the new state-of-the-art results on both Cityscapes PPS and Pascal
Context PPS datasets with at least 70% GFlops and 50% parameters decrease. In
particular, we get 3.4% relative improvements with ResNet50 backbone and 10%
improvements after adopting Swin Transformer on Pascal Context PPS dataset. To
the best of our knowledge, we are the first to solve the PPS problem via
\textit{a unified and end-to-end transformer model. Given its effectiveness and
conceptual simplicity, we hope our Panoptic-PartFormer can serve as a good
baseline and aid future unified research for PPS. Our code and models are
available at https://github.com/lxtGH/Panoptic-PartFormer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Calibrating Class Weights with Multi-Modal Information for Partial Video Domain Adaptation. (arXiv:2204.06187v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06187">
<div class="article-summary-box-inner">
<span><p>Assuming the source label space subsumes the target one, Partial Video Domain
Adaptation (PVDA) is a more general and practical scenario for cross-domain
video classification problems. The key challenge of PVDA is to mitigate the
negative transfer caused by the source-only outlier classes. To tackle this
challenge, a crucial step is to aggregate target predictions to assign class
weights by up-weighing target classes and down-weighing outlier classes.
However, the incorrect predictions of class weights can mislead the network and
lead to negative transfer. Previous works improve the class weight accuracy by
utilizing temporal features and attention mechanisms, but these methods may
fall short when trying to generate accurate class weight when domain shifts are
significant, as in most real-world scenarios. To deal with these challenges, we
propose the Multi-modality Cluster-calibrated partial Adversarial Network
(MCAN). MCAN enhances video feature extraction with multi-modal features from
multiple temporal scales to form more robust overall features. It utilizes a
novel class weight calibration method to alleviate the negative transfer caused
by incorrect class weights. The calibration method tries to identify and weigh
correct and incorrect predictions using distributional information implied by
unsupervised clustering. Extensive experiments are conducted on prevailing PVDA
benchmarks, and the proposed MCAN achieves significant improvements when
compared to state-of-the-art PVDA methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neighborhood Attention Transformer. (arXiv:2204.07143v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.07143">
<div class="article-summary-box-inner">
<span><p>We present Neighborhood Attention Transformer (NAT), an efficient, accurate
and scalable hierarchical transformer that works well on both image
classification and downstream vision tasks. It is built upon Neighborhood
Attention (NA), a simple and flexible attention mechanism that localizes the
receptive field for each query to its nearest neighboring pixels. NA is a
localization of self-attention, and approaches it as the receptive field size
increases. It is also equivalent in FLOPs and memory usage to Swin
Transformer's shifted-window attention given the same receptive field size,
while being less constrained. Furthermore, NA includes local inductive biases,
which eliminate the need for extra operations such as pixel shifts.
Experimental results on NAT are competitive; NAT-Tiny reaches 83.2% top-1
accuracy on ImageNet with only 4.3 GFLOPs and 28M parameters, 51.4% mAP on
MS-COCO and 48.4% mIoU on ADE20k. We open-sourced our checkpoints, code and
CUDA kernel at: https://github.com/SHI-Labs/Neighborhood-Attention-Transformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rotationally Equivariant 3D Object Detection. (arXiv:2204.13630v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13630">
<div class="article-summary-box-inner">
<span><p>Rotation equivariance has recently become a strongly desired property in the
3D deep learning community. Yet most existing methods focus on equivariance
regarding a global input rotation while ignoring the fact that rotation
symmetry has its own spatial support. Specifically, we consider the object
detection problem in 3D scenes, where an object bounding box should be
equivariant regarding the object pose, independent of the scene motion. This
suggests a new desired property we call object-level rotation equivariance. To
incorporate object-level rotation equivariance into 3D object detectors, we
need a mechanism to extract equivariant features with local object-level
spatial support while being able to model cross-object context information. To
this end, we propose Equivariant Object detection Network (EON) with a rotation
equivariance suspension design to achieve object-level equivariance. EON can be
applied to modern point cloud object detectors, such as VoteNet and PointRCNN,
enabling them to exploit object rotation symmetry in scene-scale inputs. Our
experiments on both indoor scene and autonomous driving datasets show that
significant improvements are obtained by plugging our EON design into existing
state-of-the-art 3D object detectors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Composition-aware Graphic Layout GAN for Visual-textual Presentation Designs. (arXiv:2205.00303v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.00303">
<div class="article-summary-box-inner">
<span><p>In this paper, we study the graphic layout generation problem of producing
high-quality visual-textual presentation designs for given images. We note that
image compositions, which contain not only global semantics but also spatial
information, would largely affect layout results. Hence, we propose a deep
generative model, dubbed as composition-aware graphic layout GAN (CGL-GAN), to
synthesize layouts based on the global and spatial visual contents of input
images. To obtain training images from images that already contain manually
designed graphic layout data, previous work suggests masking design elements
(e.g., texts and embellishments) as model inputs, which inevitably leaves hint
of the ground truth. We study the misalignment between the training inputs
(with hint masks) and test inputs (without masks), and design a novel domain
alignment module (DAM) to narrow this gap. For training, we built a large-scale
layout dataset which consists of 60,548 advertising posters with annotated
layout information. To evaluate the generated layouts, we propose three novel
metrics according to aesthetic intuitions. Through both quantitative and
qualitative evaluations, we demonstrate that the proposed model can synthesize
high-quality graphic layouts according to image compositions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lite Pose: Efficient Architecture Design for 2D Human Pose Estimation. (arXiv:2205.01271v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.01271">
<div class="article-summary-box-inner">
<span><p>Pose estimation plays a critical role in human-centered vision applications.
However, it is difficult to deploy state-of-the-art HRNet-based pose estimation
models on resource-constrained edge devices due to the high computational cost
(more than 150 GMACs per frame). In this paper, we study efficient architecture
design for real-time multi-person pose estimation on edge. We reveal that
HRNet's high-resolution branches are redundant for models at the
low-computation region via our gradual shrinking experiments. Removing them
improves both efficiency and performance. Inspired by this finding, we design
LitePose, an efficient single-branch architecture for pose estimation, and
introduce two simple approaches to enhance the capacity of LitePose, including
Fusion Deconv Head and Large Kernel Convs. Fusion Deconv Head removes the
redundancy in high-resolution branches, allowing scale-aware feature fusion
with low overhead. Large Kernel Convs significantly improve the model's
capacity and receptive field while maintaining a low computational cost. With
only 25% computation increment, 7x7 kernels achieve +14.0 mAP better than 3x3
kernels on the CrowdPose dataset. On mobile platforms, LitePose reduces the
latency by up to 5.0x without sacrificing performance, compared with prior
state-of-the-art efficient pose estimation models, pushing the frontier of
real-time multi-person pose estimation on edge. Our code and pre-trained models
are released at https://github.com/mit-han-lab/litepose.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generate and Edit Your Own Character in a Canonical View. (arXiv:2205.02974v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.02974">
<div class="article-summary-box-inner">
<span><p>Recently, synthesizing personalized characters from a single user-given
portrait has received remarkable attention as a drastic popularization of
social media and the metaverse. The input image is not always in frontal view,
thus it is important to acquire or predict canonical view for 3D modeling or
other applications. Although the progress of generative models enables the
stylization of a portrait, obtaining the stylized image in canonical view is
still a challenging task. There have been several studies on face
frontalization but their performance significantly decreases when input is not
in the real image domain, e.g., cartoon or painting. Stylizing after
frontalization also results in degenerated output. In this paper, we propose a
novel and unified framework which generates stylized portraits in canonical
view. With a proposed latent mapper, we analyze and discover frontalization
mapping in a latent space of StyleGAN to stylize and frontalize at once. In
addition, our model can be trained with unlabelled 2D image sets, without any
3D supervision. The effectiveness of our method is demonstrated by experimental
results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FlowBot3D: Learning 3D Articulation Flow to Manipulate Articulated Objects. (arXiv:2205.04382v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.04382">
<div class="article-summary-box-inner">
<span><p>We explore a novel method to perceive and manipulate 3D articulated objects
that generalizes to enable a robot to articulate unseen classes of objects. We
propose a vision-based system that learns to predict the potential motions of
the parts of a variety of articulated objects to guide downstream motion
planning of the system to articulate the objects. To predict the object
motions, we train a neural network to output a dense vector field representing
the point-wise motion direction of the points in the point cloud under
articulation. We then deploy an analytical motion planner based on this vector
field to achieve a policy that yields maximum articulation. We train the vision
system entirely in simulation, and we demonstrate the capability of our system
to generalize to unseen object instances and novel categories in both
simulation and the real world, deploying our policy on a Sawyer robot with no
finetuning. Results show that our system achieves state-of-the-art performance
in both simulated and real-world experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Distillation for Multi-Target Domain Adaptation in Real-Time Person Re-Identification. (arXiv:2205.06237v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.06237">
<div class="article-summary-box-inner">
<span><p>Despite the recent success of deep learning architectures, person
re-identification (ReID) remains a challenging problem in real-word
applications. Several unsupervised single-target domain adaptation (STDA)
methods have recently been proposed to limit the decline in ReID accuracy
caused by the domain shift that typically occurs between source and target
video data. Given the multimodal nature of person ReID data (due to variations
across camera viewpoints and capture conditions), training a common CNN
backbone to address domain shifts across multiple target domains, can provide
an efficient solution for real-time ReID applications. Although multi-target
domain adaptation (MTDA) has not been widely addressed in the ReID literature,
a straightforward approach consists in blending different target datasets, and
performing STDA on the mixture to train a common CNN. However, this approach
may lead to poor generalization, especially when blending a growing number of
distinct target domains to train a smaller CNN.
</p>
<p>To alleviate this problem, we introduce a new MTDA method based on knowledge
distillation (KD-ReID) that is suitable for real-time person ReID applications.
Our method adapts a common lightweight student backbone CNN over the target
domains by alternatively distilling from multiple specialized teacher CNNs,
each one adapted on data from a specific target domain. Extensive experiments
conducted on several challenging person ReID datasets indicate that our
approach outperforms state-of-art methods for MTDA, including blending methods,
particularly when training a compact CNN backbone like OSNet. Results suggest
that our flexible MTDA approach can be employed to design cost-effective ReID
systems for real-time video surveillance applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VLCDoC: Vision-Language Contrastive Pre-Training Model for Cross-Modal Document Classification. (arXiv:2205.12029v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12029">
<div class="article-summary-box-inner">
<span><p>Multimodal learning from document data has achieved great success lately as
it allows to pre-train semantically meaningful features as a prior into a
learnable downstream approach. In this paper, we approach the document
classification problem by learning cross-modal representations through language
and vision cues, considering intra- and inter-modality relationships. Instead
of merging features from different modalities into a common representation
space, the proposed method exploits high-level interactions and learns relevant
semantic information from effective attention flows within and across
modalities. The proposed learning objective is devised between intra- and
inter-modality alignment tasks, where the similarity distribution per task is
computed by contracting positive sample pairs while simultaneously contrasting
negative ones in the common feature representation space}. Extensive
experiments on public document classification datasets demonstrate the
effectiveness and the generalization capacity of our model on both low-scale
and large-scale datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sim-To-Real Transfer of Visual Grounding for Human-Aided Ambiguity Resolution. (arXiv:2205.12089v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.12089">
<div class="article-summary-box-inner">
<span><p>Service robots should be able to interact naturally with non-expert human
users, not only to help them in various tasks but also to receive guidance in
order to resolve ambiguities that might be present in the instruction. We
consider the task of visual grounding, where the agent segments an object from
a crowded scene given a natural language description. Modern holistic
approaches to visual grounding usually ignore language structure and struggle
to cover generic domains, therefore relying heavily on large datasets.
Additionally, their transfer performance in RGB-D datasets suffers due to high
visual discrepancy between the benchmark and the target domains. Modular
approaches marry learning with domain modeling and exploit the compositional
nature of language to decouple visual representation from language parsing, but
either rely on external parsers or are trained in an end-to-end fashion due to
the lack of strong supervision. In this work, we seek to tackle these
limitations by introducing a fully decoupled modular framework for
compositional visual grounding of entities, attributes, and spatial relations.
We exploit rich scene graph annotations generated in a synthetic domain and
train each module independently. Our approach is evaluated both in simulation
and in two real RGB-D scene datasets. Experimental results show that the
decoupled nature of our framework allows for easy integration with domain
adaptation approaches for Sim-To-Real visual recognition, offering a
data-efficient, robust, and interpretable solution to visual grounding in
robotic applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-Shot Adaptation of Pre-Trained Networks for Domain Shift. (arXiv:2205.15234v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15234">
<div class="article-summary-box-inner">
<span><p>Deep networks are prone to performance degradation when there is a domain
shift between the source (training) data and target (test) data. Recent
test-time adaptation methods update batch normalization layers of pre-trained
source models deployed in new target environments with streaming data to
mitigate such performance degradation. Although such methods can adapt
on-the-fly without first collecting a large target domain dataset, their
performance is dependent on streaming conditions such as mini-batch size and
class-distribution, which can be unpredictable in practice. In this work, we
propose a framework for few-shot domain adaptation to address the practical
challenges of data-efficient adaptation. Specifically, we propose a constrained
optimization of feature normalization statistics in pre-trained source models
supervised by a small support set from the target domain. Our method is easy to
implement and improves source model performance with as few as one sample per
class for classification tasks. Extensive experiments on 5 cross-domain
classification and 4 semantic segmentation datasets show that our method
achieves more accurate and reliable performance than test-time adaptation,
while not being constrained by streaming conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SDQ: Stochastic Differentiable Quantization with Mixed Precision. (arXiv:2206.04459v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.04459">
<div class="article-summary-box-inner">
<span><p>In order to deploy deep models in a computationally efficient manner, model
quantization approaches have been frequently used. In addition, as new hardware
that supports mixed bitwidth arithmetic operations, recent research on mixed
precision quantization (MPQ) begins to fully leverage the capacity of
representation by searching optimized bitwidths for different layers and
modules in a network. However, previous studies mainly search the MPQ strategy
in a costly scheme using reinforcement learning, neural architecture search,
etc., or simply utilize partial prior knowledge for bitwidth assignment, which
might be biased and sub-optimal. In this work, we present a novel Stochastic
Differentiable Quantization (SDQ) method that can automatically learn the MPQ
strategy in a more flexible and globally-optimized space with smoother gradient
approximation. Particularly, Differentiable Bitwidth Parameters (DBPs) are
employed as the probability factors in stochastic quantization between adjacent
bitwidth choices. After the optimal MPQ strategy is acquired, we further train
our network with entropy-aware bin regularization and knowledge distillation.
We extensively evaluate our method for several networks on different hardware
(GPUs and FPGA) and datasets. SDQ outperforms all state-of-the-art mixed or
single precision quantization with a lower bitwidth and is even better than the
full-precision counterparts across various ResNet and MobileNet families,
demonstrating the effectiveness and superiority of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learned reconstruction methods with convergence guarantees. (arXiv:2206.05431v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.05431">
<div class="article-summary-box-inner">
<span><p>In recent years, deep learning has achieved remarkable empirical success for
image reconstruction. This has catalyzed an ongoing quest for precise
characterization of correctness and reliability of data-driven methods in
critical use-cases, for instance in medical imaging. Notwithstanding the
excellent performance and efficacy of deep learning-based methods, concerns
have been raised regarding their stability, or lack thereof, with serious
practical implications. Significant advances have been made in recent years to
unravel the inner workings of data-driven image recovery methods, challenging
their widely perceived black-box nature. In this article, we will specify
relevant notions of convergence for data-driven image reconstruction, which
will form the basis of a survey of learned methods with mathematically rigorous
reconstruction guarantees. An example that is highlighted is the role of ICNN,
offering the possibility to combine the power of deep learning with classical
convex regularization theory for devising methods that are provably convergent.
</p>
<p>This survey article is aimed at both methodological researchers seeking to
advance the frontiers of our understanding of data-driven image reconstruction
methods as well as practitioners, by providing an accessible description of
useful convergence concepts and by placing some of the existing empirical
practices on a solid mathematical foundation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Med-DANet: Dynamic Architecture Network for Efficient Medical Volumetric Segmentation. (arXiv:2206.06575v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.06575">
<div class="article-summary-box-inner">
<span><p>For 3D medical image (e.g. CT and MRI) segmentation, the difficulty of
segmenting each slice in a clinical case varies greatly. Previous research on
volumetric medical image segmentation in a slice-by-slice manner conventionally
use the identical 2D deep neural network to segment all the slices of the same
case, ignoring the data heterogeneity among image slices. In this paper, we
focus on multi-modal 3D MRI brain tumor segmentation and propose a dynamic
architecture network named Med-DANet based on adaptive model selection to
achieve effective accuracy and efficiency trade-off. For each slice of the
input 3D MRI volume, our proposed method learns a slice-specific decision by
the Decision Network to dynamically select a suitable model from the predefined
Model Bank for the subsequent 2D segmentation task. Extensive experimental
results on both BraTS 2019 and 2020 datasets show that our proposed method
achieves comparable or better results than previous state-of-the-art methods
for 3D MRI brain tumor segmentation with much less model complexity. Compared
with the state-of-the-art 3D method TransBTS, the proposed framework improves
the model efficiency by up to 3.5x without sacrificing the accuracy. Our code
will be publicly available soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervision on Images and Text Reduces Reliance on Visual Shortcut Features. (arXiv:2206.07155v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07155">
<div class="article-summary-box-inner">
<span><p>Deep learning models trained in a fully supervised manner have been shown to
rely on so-called "shortcut" features. Shortcut features are inputs that are
associated with the outcome of interest in the training data, but are either no
longer associated or not present in testing or deployment settings. Here we
provide experiments that show recent self-supervised models trained on images
and text provide more robust image representations and reduce the model's
reliance on visual shortcut features on a realistic medical imaging example.
Additionally, we find that these self-supervised models "forget" shortcut
features more quickly than fully supervised ones when fine-tuned on labeled
data. Though not a complete solution, our experiments provide compelling
evidence that self-supervised models trained on images and text provide some
resilience to visual shortcut features.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real3D-Aug: Point Cloud Augmentation by Placing Real Objects with Occlusion Handling for 3D Detection and Segmentation. (arXiv:2206.07634v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07634">
<div class="article-summary-box-inner">
<span><p>Object detection and semantic segmentation with the 3D lidar point cloud data
require expensive annotation. We propose a data augmentation method that takes
advantage of already annotated data multiple times. We propose an augmentation
framework that reuses real data, automatically finds suitable placements in the
scene to be augmented, and handles occlusions explicitly. Due to the usage of
the real data, the scan points of newly inserted objects in augmentation
sustain the physical characteristics of the lidar, such as intensity and
raydrop. The pipeline proves competitive in training top-performing models for
3D object detection and semantic segmentation. The new augmentation provides a
significant performance gain in rare and essential classes, notably 6.65%
average precision gain for "Hard" pedestrian class in KITTI object detection or
2.14 mean IoU gain in the SemanticKITTI segmentation challenge over the state
of the art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lessons learned from the NeurIPS 2021 MetaDL challenge: Backbone fine-tuning without episodic meta-learning dominates for few-shot learning image classification. (arXiv:2206.08138v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.08138">
<div class="article-summary-box-inner">
<span><p>Although deep neural networks are capable of achieving performance superior
to humans on various tasks, they are notorious for requiring large amounts of
data and computing resources, restricting their success to domains where such
resources are available. Metalearning methods can address this problem by
transferring knowledge from related tasks, thus reducing the amount of data and
computing resources needed to learn new tasks. We organize the MetaDL
competition series, which provide opportunities for research groups all over
the world to create and experimentally assess new meta-(deep)learning solutions
for real problems. In this paper, authored collaboratively between the
competition organizers and the top-ranked participants, we describe the design
of the competition, the datasets, the best experimental results, as well as the
top-ranked methods in the NeurIPS 2021 challenge, which attracted 15 active
teams who made it to the final phase (by outperforming the baseline), making
over 100 code submissions during the feedback phase. The solutions of the top
participants have been open-sourced. The lessons learned include that learning
good representations is essential for effective transfer learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Audio-visual Synchronization for Active Speaker Detection. (arXiv:2206.10421v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10421">
<div class="article-summary-box-inner">
<span><p>Active speaker detection (ASD) systems are important modules for analyzing
multi-talker conversations. They aim to detect which speakers or none are
talking in a visual scene at any given time. Existing research on ASD does not
agree on the definition of active speakers. We clarify the definition in this
work and require synchronization between the audio and visual speaking
activities. This clarification of definition is motivated by our extensive
experiments, through which we discover that existing ASD methods fail in
modeling the audio-visual synchronization and often classify unsynchronized
videos as active speaking. To address this problem, we propose a cross-modal
contrastive learning strategy and apply positional encoding in attention
modules for supervised ASD models to leverage the synchronization cue.
Experimental results suggest that our model can successfully detect
unsynchronized speaking as not speaking, addressing the limitation of current
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision Transformer for Contrastive Clustering. (arXiv:2206.12925v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12925">
<div class="article-summary-box-inner">
<span><p>Vision Transformer (ViT) has shown its advantages over the convolutional
neural network (CNN) with its ability to capture global long-range dependencies
for visual representation learning. Besides ViT, contrastive learning is
another popular research topic recently. While previous contrastive learning
works are mostly based on CNNs, some recent studies have attempted to combine
ViT and contrastive learning for enhanced self-supervised learning. Despite the
considerable progress, these combinations of ViT and contrastive learning
mostly focus on the instance-level contrastiveness, which often overlook the
global contrastiveness and also lack the ability to directly learn the
clustering result (e.g., for images). In view of this, this paper presents a
novel deep clustering approach termed Vision Transformer for Contrastive
Clustering (VTCC), which for the first time, to our knowledge, unifies the
Transformer and the contrastive learning for the image clustering task.
Specifically, with two random augmentations performed on each image, we utilize
a ViT encoder with two weight-sharing views as the backbone. To remedy the
potential instability of the ViT, we incorporate a convolutional stem to split
each augmented sample into a sequence of patches, which uses multiple stacked
small convolutions instead of a big convolution in the patch projection layer.
By learning the feature representations for the sequences of patches via the
backbone, an instance projector and a cluster projector are further utilized to
perform the instance-level contrastive learning and the global clustering
structure learning, respectively. Experiments on eight image datasets
demonstrate the stability (during the training-from-scratch) and the
superiority (in clustering performance) of our VTCC approach over the
state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robustness Implies Generalization via Data-Dependent Generalization Bounds. (arXiv:2206.13497v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.13497">
<div class="article-summary-box-inner">
<span><p>This paper proves that robustness implies generalization via data-dependent
generalization bounds. As a result, robustness and generalization are shown to
be connected closely in a data-dependent manner. Our bounds improve previous
bounds in two directions, to solve an open problem that has seen little
development since 2010. The first is to reduce the dependence on the covering
number. The second is to remove the dependence on the hypothesis space. We
present several examples, including ones for lasso and deep learning, in which
our bounds are provably preferable. The experiments on real-world data and
theoretical models demonstrate near-exponential improvements in various
situations. To achieve these improvements, we do not require additional
assumptions on the unknown distribution; instead, we only incorporate an
observable and computable property of the training samples. A key technical
innovation is an improved concentration bound for multinomial random variables
that is of independent interest beyond robustness and generalization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PolarFormer: Multi-camera 3D Object Detection with Polar Transformers. (arXiv:2206.15398v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.15398">
<div class="article-summary-box-inner">
<span><p>3D object detection in autonomous driving aims to reason "what" and "where"
the objects of interest present in a 3D world. Following the conventional
wisdom of previous 2D object detection, existing methods often adopt the
canonical Cartesian coordinate system with perpendicular axis. However, we
conjugate that this does not fit the nature of the ego car's perspective, as
each onboard camera perceives the world in shape of wedge intrinsic to the
imaging geometry with radical (non-perpendicular) axis. Hence, in this paper we
advocate the exploitation of the Polar coordinate system and propose a new
Polar Transformer (PolarFormer) for more accurate 3D object detection in the
bird's-eye-view (BEV) taking as input only multi-camera 2D images.
Specifically, we design a cross attention based Polar detection head without
restriction to the shape of input structure to deal with irregular Polar grids.
For tackling the unconstrained object scale variations along Polar's distance
dimension, we further introduce a multi-scalePolar representation learning
strategy. As a result, our model can make best use of the Polar representation
rasterized via attending to the corresponding image observation in a
sequence-to-sequence fashion subject to the geometric constraints. Thorough
experiments on the nuScenes dataset demonstrate that our PolarFormer
outperforms significantly state-of-the-art 3D object detection alternatives, as
well as yielding competitive performance on BEV semantic segmentation task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Far Can I Go ? : A Self-Supervised Approach for Deterministic Video Depth Forecasting. (arXiv:2207.00506v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00506">
<div class="article-summary-box-inner">
<span><p>In this paper we present a novel self-supervised method to anticipate the
depth estimate for a future, unobserved real-world urban scene. This work is
the first to explore self-supervised learning for estimation of monocular depth
of future unobserved frames of a video. Existing works rely on a large number
of annotated samples to generate the probabilistic prediction of depth for
unseen frames. However, this makes it unrealistic due to its requirement for
large amount of annotated depth samples of video. In addition, the
probabilistic nature of the case, where one past can have multiple future
outcomes often leads to incorrect depth estimates. Unlike previous methods, we
model the depth estimation of the unobserved frame as a view-synthesis problem,
which treats the depth estimate of the unseen video frame as an auxiliary task
while synthesizing back the views using learned pose. This approach is not only
cost effective - we do not use any ground truth depth for training (hence
practical) but also deterministic (a sequence of past frames map to an
immediate future). To address this task we first develop a novel depth
forecasting network DeFNet which estimates depth of unobserved future by
forecasting latent features. Second, we develop a channel-attention based pose
estimation network that estimates the pose of the unobserved frame. Using this
learned pose, estimated depth map is reconstructed back into the image domain,
thus forming a self-supervised solution. Our proposed approach shows
significant improvements in Abs Rel metric compared to state-of-the-art
alternatives on both short and mid-term forecasting setting, benchmarked on
KITTI and Cityscapes. Code is available at
https://github.com/sauradip/depthForecasting
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Test-time Adaptation with Calibration of Medical Image Classification Nets for Label Distribution Shift. (arXiv:2207.00769v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.00769">
<div class="article-summary-box-inner">
<span><p>Class distribution plays an important role in learning deep classifiers. When
the proportion of each class in the test set differs from the training set, the
performance of classification nets usually degrades. Such a label distribution
shift problem is common in medical diagnosis since the prevalence of disease
vary over location and time. In this paper, we propose the first method to
tackle label shift for medical image classification, which effectively adapt
the model learned from a single training label distribution to arbitrary
unknown test label distribution. Our approach innovates distribution
calibration to learn multiple representative classifiers, which are capable of
handling different one-dominating-class distributions. When given a test image,
the diverse classifiers are dynamically aggregated via the consistency-driven
test-time adaptation, to deal with the unknown test label distribution. We
validate our method on two important medical image classification tasks
including liver fibrosis staging and COVID-19 severity prediction. Our
experiments clearly show the decreased model performance under label shift.
With our method, model performance significantly improves on all the test
datasets with different label shifts for both medical image diagnosis tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Language Understand Depth?. (arXiv:2207.01077v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01077">
<div class="article-summary-box-inner">
<span><p>Besides image classification, Contrastive Language-Image Pre-training (CLIP)
has accomplished extraordinary success for a wide range of vision tasks,
including object-level and 3D space understanding. However, it's still
challenging to transfer semantic knowledge learned from CLIP into more
intricate tasks of quantified targets, such as depth estimation with geometric
information. In this paper, we propose to apply CLIP for zero-shot monocular
depth estimation, named DepthCLIP. We found that the patches of the input image
could respond to a certain semantic distance token and then be projected to a
quantified depth bin for coarse estimation. Without any training, our DepthCLIP
surpasses existing unsupervised methods and even approaches the early
fully-supervised networks. To our best knowledge, we are the first to conduct
zero-shot adaptation from the semantic language knowledge to quantified
downstream tasks and perform zero-shot monocular depth estimation. We hope our
work could cast a light on future research. The code is available at
https://github.com/Adonis-galaxy/DepthCLIP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DecisioNet: A Binary-Tree Structured Neural Network. (arXiv:2207.01127v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01127">
<div class="article-summary-box-inner">
<span><p>Deep neural networks (DNNs) and decision trees (DTs) are both
state-of-the-art classifiers. DNNs perform well due to their representational
learning capabilities, while DTs are computationally efficient as they perform
inference along one route (root-to-leaf) that is dependent on the input data.
In this paper, we present DecisioNet (DN), a binary-tree structured neural
network. We propose a systematic way to convert an existing DNN into a DN to
create a lightweight version of the original model. DecisioNet takes the best
of both worlds - it uses neural modules to perform representational learning
and utilizes its tree structure to perform only a portion of the computations.
We evaluate various DN architectures, along with their corresponding baseline
models on the FashionMNIST, CIFAR10, and CIFAR100 datasets. We show that the DN
variants achieve similar accuracy while significantly reducing the
computational cost of the original network.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detection of ADHD based on Eye Movements during Natural Viewing. (arXiv:2207.01377v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01377">
<div class="article-summary-box-inner">
<span><p>Attention-deficit/hyperactivity disorder (ADHD) is a neurodevelopmental
disorder that is highly prevalent and requires clinical specialists to
diagnose. It is known that an individual's viewing behavior, reflected in their
eye movements, is directly related to attentional mechanisms and higher-order
cognitive processes. We therefore explore whether ADHD can be detected based on
recorded eye movements together with information about the video stimulus in a
free-viewing task. To this end, we develop an end-to-end deep learning-based
sequence model which we pre-train on a related task for which more data are
available. We find that the method is in fact able to detect ADHD and
outperforms relevant baselines. We investigate the relevance of the input
features in an ablation study. Interestingly, we find that the model's
performance is closely related to the content of the video, which provides
insights for future experimental designs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Memory Efficient Patch-based Training for INR-based GANs. (arXiv:2207.01395v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01395">
<div class="article-summary-box-inner">
<span><p>Recent studies have shown remarkable progress in GANs based on implicit
neural representation (INR) - an MLP that produces an RGB value given its (x,
y) coordinate. They represent an image as a continuous version of the
underlying 2D signal instead of a 2D array of pixels, which opens new horizons
for GAN applications (e.g., zero-shot super-resolution, image outpainting).
However, training existing approaches require a heavy computational cost
proportional to the image resolution, since they compute an MLP operation for
every (x, y) coordinate. To alleviate this issue, we propose a multi-stage
patch-based training, a novel and scalable approach that can train INR-based
GANs with a flexible computational cost regardless of the image resolution.
Specifically, our method allows to generate and discriminate by patch to learn
the local details of the image and learn global structural information by a
novel reconstruction loss to enable efficient GAN training. We conduct
experiments on several benchmark datasets to demonstrate that our approach
enhances baseline models in GPU memory while maintaining FIDs at a reasonable
level.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiview Detection with Cardboard Human Modeling. (arXiv:2207.02013v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02013">
<div class="article-summary-box-inner">
<span><p>Multiview detection uses multiple calibrated cameras with overlapping fields
of views to locate occluded pedestrians. In this field, existing methods
typically adopt a "human modeling - aggregation" strategy. To find robust
pedestrian representations, some intuitively use locations of detected 2D
bounding boxes, while others use entire frame features projected to the ground
plane. However, the former does not consider human appearance and leads to many
ambiguities, and the latter suffers from projection errors due to the lack of
accurate height of the human torso and head. In this paper, we propose a new
pedestrian representation scheme based on human point clouds modeling.
Specifically, using ray tracing for holistic human depth estimation, we model
pedestrians as upright, thin cardboard point clouds on the ground. Then, we
aggregate the point clouds of the pedestrian cardboard across multiple views
for a final decision. Compared with existing representations, the proposed
method explicitly leverages human appearance and reduces projection errors
significantly by relatively accurate height estimation. On two standard
evaluation benchmarks, the proposed method achieves very competitive results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TractoFormer: A Novel Fiber-level Whole Brain Tractography Analysis Framework Using Spectral Embedding and Vision Transformers. (arXiv:2207.02327v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02327">
<div class="article-summary-box-inner">
<span><p>Diffusion MRI tractography is an advanced imaging technique for quantitative
mapping of the brain's structural connectivity. Whole brain tractography (WBT)
data contains over hundreds of thousands of individual fiber streamlines
(estimated brain connections), and this data is usually parcellated to create
compact representations for data analysis applications such as disease
classification. In this paper, we propose a novel parcellation-free WBT
analysis framework, TractoFormer, that leverages tractography information at
the level of individual fiber streamlines and provides a natural mechanism for
interpretation of results using the attention mechanism of transformers.
TractoFormer includes two main contributions. First, we propose a novel and
simple 2D image representation of WBT, TractoEmbedding, to encode 3D fiber
spatial relationships and any feature of interest that can be computed from
individual fibers (such as FA or MD). Second, we design a network based on
vision transformers (ViTs) that includes: 1) data augmentation to overcome
model overfitting on small datasets, 2) identification of discriminative fibers
for interpretation of results, and 3) ensemble learning to leverage fiber
information from different brain regions. In a synthetic data experiment,
TractoFormer successfully identifies discriminative fibers with simulated group
differences. In a disease classification experiment comparing several methods,
TractoFormer achieves the highest accuracy in classifying schizophrenia vs
control. Discriminative fibers are identified in left hemispheric frontal and
parietal superficial white matter regions, which have previously been shown to
be affected in schizophrenia patients.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Two-stage Decision Improves Open-Set Panoptic Segmentation. (arXiv:2207.02504v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02504">
<div class="article-summary-box-inner">
<span><p>Open-set panoptic segmentation (OPS) problem is a new research direction
aiming to perform segmentation for both \known classes and \unknown classes,
i.e., the objects ("things") that are never annotated in the training set. The
main challenges of OPS are twofold: (1) the infinite possibility of the
\unknown object appearances makes it difficult to model them from a limited
number of training data. (2) at training time, we are only provided with the
"void" category, which essentially mixes the "unknown thing" and "background"
classes. We empirically find that directly using "void" category to supervise
\known class or "background" without screening will not lead to a satisfied OPS
result. In this paper, we propose a divide-and-conquer scheme to develop a
two-stage decision process for OPS. We show that by properly combining a \known
class discriminator with an additional class-agnostic object prediction head,
the OPS performance can be significantly improved. Specifically, we first
propose to create a classifier with only \known categories and let the "void"
class proposals achieve low prediction probability from those categories. Then
we distinguish the "unknown things" from the background by using the additional
object prediction head. To further boost performance, we introduce "unknown
things" pseudo-labels generated from up-to-date models and a heuristic rule to
enrich the training set. Our extensive experimental evaluation shows that our
approach significantly improves \unknown class panoptic quality, with more than
30\% relative improvements than the existing best-performed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PIC 4th Challenge: Semantic-Assisted Multi-Feature Encoding and Multi-Head Decoding for Dense Video Captioning. (arXiv:2207.02583v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02583">
<div class="article-summary-box-inner">
<span><p>The task of Dense Video Captioning (DVC) aims to generate captions with
timestamps for multiple events in one video. Semantic information plays an
important role for both localization and description of DVC. We present a
semantic-assisted dense video captioning model based on the encoding-decoding
framework. In the encoding stage, we design a concept detector to extract
semantic information, which is then fused with multi-modal visual features to
sufficiently represent the input video. In the decoding stage, we design a
classification head, paralleled with the localization and captioning heads, to
provide semantic supervision. Our method achieves significant improvements on
the YouMakeup dataset under DVC evaluation metrics and achieves high
performance in the Makeup Dense Video Captioning (MDVC) task of PIC 4th
Challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-supervised Human Pose Estimation in Art-historical Images. (arXiv:2207.02976v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02976">
<div class="article-summary-box-inner">
<span><p>Gesture as language of non-verbal communication has been theoretically
established since the 17th century. However, its relevance for the visual arts
has been expressed only sporadically. This may be primarily due to the sheer
overwhelming amount of data that traditionally had to be processed by hand.
With the steady progress of digitization, though, a growing number of
historical artifacts have been indexed and made available to the public,
creating a need for automatic retrieval of art-historical motifs with similar
body constellations or poses. Since the domain of art differs significantly
from existing real-world data sets for human pose estimation due to its style
variance, this presents new challenges. In this paper, we propose a novel
approach to estimate human poses in art-historical images. In contrast to
previous work that attempts to bridge the domain gap with pre-trained models or
through style transfer, we suggest semi-supervised learning for both object and
keypoint detection. Furthermore, we introduce a novel domain-specific art data
set that includes both bounding box and keypoint annotations of human figures.
Our approach achieves significantly better results than methods that use
pre-trained models or style transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Design of Human Machine Interface through vision-based low-cost Hand Gesture Recognition system based on deep CNN. (arXiv:2207.03112v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03112">
<div class="article-summary-box-inner">
<span><p>In this work, a real-time hand gesture recognition system-based
human-computer interface (HCI) is presented. The system consists of six stages:
(1) hand detection, (2) gesture segmentation, (3) use of six pre-trained CNN
models by using the transfer-learning method, (4) building an interactive
human-machine interface, (5) development of a gesture-controlled virtual mouse,
(6) use of Kalman filter to estimate the hand position, based on that the
smoothness of the motion of pointer is improved. Six pre-trained convolutional
neural network (CNN) models (VGG16, VGG19, ResNet50, ResNet101, Inception-V1,
and MobileNet-V1) have been used to classify hand gesture images. Three
multi-class datasets (two publicly and one custom) have been used to evaluate
the model performances. Considering the models' performances, it has been
observed that Inception-V1 has significantly shown a better classification
performance compared to the other five pre-trained models in terms of accuracy,
precision, recall, and F-score values. The gesture recognition system is
expanded and used to control multimedia applications (like VLC player, audio
player, file management, playing 2D Super-Mario-Bros game, etc.) with different
customized gesture commands in real-time scenarios. The average speed of this
system has reached 35 fps (frame per seconds), which meets the requirements for
the real-time scenario.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting Zero-shot Learning via Contrastive Optimization of Attribute Representations. (arXiv:2207.03824v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03824">
<div class="article-summary-box-inner">
<span><p>Zero-shot learning (ZSL) aims to recognize classes that do not have samples
in the training set. One representative solution is to directly learn an
embedding function associating visual features with corresponding class
semantics for recognizing new classes. Many methods extend upon this solution,
and recent ones are especially keen on extracting rich features from images,
e.g. attribute features. These attribute features are normally extracted within
each individual image; however, the common traits for features across images
yet belonging to the same attribute are not emphasized. In this paper, we
propose a new framework to boost ZSL by explicitly learning attribute
prototypes beyond images and contrastively optimizing them with attribute-level
features within images. Besides the novel architecture, two elements are
highlighted for attribute representations: a new prototype generation module is
designed to generate attribute prototypes from attribute semantics; a hard
example-based contrastive optimization scheme is introduced to reinforce
attribute-level features in the embedding space. We explore two alternative
backbones, CNN-based and transformer-based, to build our framework and conduct
experiments on three standard benchmarks, CUB, SUN, AwA2. Results on these
benchmarks demonstrate that our method improves the state of the art by a
considerable margin. Our codes will be available at
https://github.com/dyabel/CoAR-ZSL.git
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Event Collapse in Contrast Maximization Frameworks. (arXiv:2207.04007v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04007">
<div class="article-summary-box-inner">
<span><p>Contrast maximization (CMax) is a framework that provides state-of-the-art
results on several event-based computer vision tasks, such as ego-motion or
optical flow estimation. However, it may suffer from a problem called event
collapse, which is an undesired solution where events are warped into too few
pixels. As prior works have largely ignored the issue or proposed workarounds,
it is imperative to analyze this phenomenon in detail. Our work demonstrates
event collapse in its simplest form and proposes collapse metrics by using
first principles of space-time deformation based on differential geometry and
physics. We experimentally show on publicly available datasets that the
proposed metrics mitigate event collapse and do not harm well-posed warps. To
the best of our knowledge, regularizers based on the proposed metrics are the
only effective solution against event collapse in the experimental settings
considered, compared with other methods. We hope that this work inspires
further research to tackle more complex warp models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pruning Early Exit Networks. (arXiv:2207.03644v1 [cs.LG] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03644">
<div class="article-summary-box-inner">
<span><p>Deep learning models that perform well often have high computational costs.
In this paper, we combine two approaches that try to reduce the computational
cost while keeping the model performance high: pruning and early exit networks.
We evaluate two approaches of pruning early exit networks: (1) pruning the
entire network at once, (2) pruning the base network and additional linear
classifiers in an ordered fashion. Experimental results show that pruning the
entire network at once is a better strategy in general. However, at high
accuracy rates, the two approaches have a similar performance, which implies
that the processes of pruning and early exit can be separated without loss of
optimality.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-07-12 23:08:28.330831886 UTC">2022-07-12 23:08:28 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>