<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-04-07T01:30:00Z">04-07</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Combining Spectral and Self-Supervised Features for Low Resource Speech Recognition and Translation. (arXiv:2204.02470v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02470">
<div class="article-summary-box-inner">
<span><p>Self-Supervised Learning (SSL) models have been successfully applied in
various deep learning-based speech tasks, particularly those with a limited
amount of data. However, the quality of SSL representations depends highly on
the relatedness between the SSL training domain(s) and the target data domain.
On the contrary, spectral feature (SF) extractors such as log Mel-filterbanks
are hand-crafted non-learnable components, and could be more robust to domain
shifts. The present work examines the assumption that combining non-learnable
SF extractors to SSL models is an effective approach to low resource speech
tasks. We propose a learnable and interpretable framework to combine SF and SSL
representations. The proposed framework outperforms significantly both baseline
and SSL models on Automatic Speech Recognition (ASR) and Speech Translation
(ST) tasks on three low resource datasets. We additionally design a mixture of
experts based combination model. This last model reveals that the relative
contribution of SSL models over conventional SF extractors is very small in
case of domain mismatch between SSL training set and the target language data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Considerations for Multilingual Wikipedia Research. (arXiv:2204.02483v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02483">
<div class="article-summary-box-inner">
<span><p>English Wikipedia has long been an important data source for much research
and natural language machine learning modeling. The growth of non-English
language editions of Wikipedia, greater computational resources, and calls for
equity in the performance of language and multimodal models have led to the
inclusion of many more language editions of Wikipedia in datasets and models.
Building better multilingual and multimodal models requires more than just
access to expanded datasets; it also requires a better understanding of what is
in the data and how this content was generated. This paper seeks to provide
some background to help researchers think about what differences might arise
between different language editions of Wikipedia and how that might affect
their models. It details three major ways in which content differences between
language editions arise (local context, community and governance, and
technology) and recommendations for good practices when using multilingual and
multimodal data for research and modeling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards End-to-end Unsupervised Speech Recognition. (arXiv:2204.02492v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02492">
<div class="article-summary-box-inner">
<span><p>Unsupervised speech recognition has shown great potential to make Automatic
Speech Recognition (ASR) systems accessible to every language. However,
existing methods still heavily rely on hand-crafted pre-processing. Similar to
the trend of making supervised speech recognition end-to-end, we introduce
\wvu~which does away with all audio-side pre-processing and improves accuracy
through better architecture. In addition, we introduce an auxiliary
self-supervised objective that ties model predictions back to the input.
Experiments show that \wvu~improves unsupervised recognition results across
different languages while being conceptually simpler.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inferring Rewards from Language in Context. (arXiv:2204.02515v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02515">
<div class="article-summary-box-inner">
<span><p>In classic instruction following, language like "I'd like the JetBlue flight"
maps to actions (e.g., selecting that flight). However, language also conveys
information about a user's underlying reward function (e.g., a general
preference for JetBlue), which can allow a model to carry out desirable actions
in new contexts. We present a model that infers rewards from language
pragmatically: reasoning about how speakers choose utterances not only to
elicit desired actions, but also to reveal information about their preferences.
On a new interactive flight-booking task with natural language, our model more
accurately infers rewards and predicts optimal actions in unseen environments,
in comparison to past work that first maps language to actions (instruction
following) and then maps actions to rewards (inverse reinforcement learning).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simple and Effective Unsupervised Speech Synthesis. (arXiv:2204.02524v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02524">
<div class="article-summary-box-inner">
<span><p>We introduce the first unsupervised speech synthesis system based on a
simple, yet effective recipe. The framework leverages recent work in
unsupervised speech recognition as well as existing neural-based speech
synthesis. Using only unlabeled speech audio and unlabeled text as well as a
lexicon, our method enables speech synthesis without the need for a
human-labeled corpus. Experiments demonstrate the unsupervised system can
synthesize speech similar to a supervised counterpart in terms of naturalness
and intelligibility measured by human evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prosodic Alignment for off-screen automatic dubbing. (arXiv:2204.02530v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02530">
<div class="article-summary-box-inner">
<span><p>The goal of automatic dubbing is to perform speech-to-speech translation
while achieving audiovisual coherence. This entails isochrony, i.e.,
translating the original speech by also matching its prosodic structure into
phrases and pauses, especially when the speaker's mouth is visible. In previous
work, we introduced a prosodic alignment model to address isochrone or
on-screen dubbing. In this work, we extend the prosodic alignment model to also
address off-screen dubbing that requires less stringent synchronization
constraints. We conduct experiments on four dubbing directions - English to
French, Italian, German and Spanish - on a publicly available collection of TED
Talks and on publicly available YouTube videos. Empirical results show that
compared to our previous work the extended prosodic alignment model provides
significantly better subjective viewing experience on videos in which on-screen
and off-screen automatic dubbing is applied for sentences with speakers mouth
visible and not visible, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Zero-Shot Event Extraction via Sentence Simplification. (arXiv:2204.02531v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02531">
<div class="article-summary-box-inner">
<span><p>The success of sites such as ACLED and Our World in Data have demonstrated
the massive utility of extracting events in structured formats from large
volumes of textual data in the form of news, social media, blogs and discussion
forums. Event extraction can provide a window into ongoing geopolitical crises
and yield actionable intelligence. With the proliferation of large pretrained
language models, Machine Reading Comprehension (MRC) has emerged as a new
paradigm for event extraction in recent times. In this approach, event argument
extraction is framed as an extractive question-answering task. One of the key
advantages of the MRC-based approach is its ability to perform zero-shot
extraction. However, the problem of long-range dependencies, i.e., large
lexical distance between trigger and argument words and the difficulty of
processing syntactically complex sentences plague MRC-based approaches. In this
paper, we present a general approach to improve the performance of MRC-based
event extraction by performing unsupervised sentence simplification guided by
the MRC model itself. We evaluate our approach on the ICEWS geopolitical event
extraction dataset, with specific attention to `Actor' and `Target' argument
roles. We show how such context simplification can improve the performance of
MRC-based event extraction by more than 5% for actor extraction and more than
10% for target extraction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quick Starting Dialog Systems with Paraphrase Generation. (arXiv:2204.02546v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02546">
<div class="article-summary-box-inner">
<span><p>Acquiring training data to improve the robustness of dialog systems can be a
painstakingly long process. In this work, we propose a method to reduce the
cost and effort of creating new conversational agents by artificially
generating more data from existing examples, using paraphrase generation. Our
proposed approach can kick-start a dialog system with little human effort, and
brings its performance to a level satisfactory enough for allowing actual
interactions with real end-users. We experimented with two neural paraphrasing
approaches, namely Neural Machine Translation and a Transformer-based seq2seq
model. We present the results obtained with two datasets in English and in
French:~a crowd-sourced public intent classification dataset and our own
corporate dialog system dataset. We show that our proposed approach increased
the generalization capabilities of the intent classification model on both
datasets, reducing the effort required to initialize a new dialog system and
helping to deploy this technology at scale within an organization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Motion with Multi-Modal Features for Text-Based Video Segmentation. (arXiv:2204.02547v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02547">
<div class="article-summary-box-inner">
<span><p>Text-based video segmentation aims to segment the target object in a video
based on a describing sentence. Incorporating motion information from optical
flow maps with appearance and linguistic modalities is crucial yet has been
largely ignored by previous work. In this paper, we design a method to fuse and
align appearance, motion, and linguistic features to achieve accurate
segmentation. Specifically, we propose a multi-modal video transformer, which
can fuse and aggregate multi-modal and temporal features between frames.
Furthermore, we design a language-guided feature fusion module to progressively
fuse appearance and motion features in each feature level with guidance from
linguistic features. Finally, a multi-modal alignment loss is proposed to
alleviate the semantic gap between features from different modalities.
Extensive experiments on A2D Sentences and J-HMDB Sentences verify the
performance and the generalization ability of our method compared to the
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">C3KG: A Chinese Commonsense Conversation Knowledge Graph. (arXiv:2204.02549v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02549">
<div class="article-summary-box-inner">
<span><p>Existing commonsense knowledge bases often organize tuples in an isolated
manner, which is deficient for commonsense conversational models to plan the
next steps. To fill the gap, we curate a large-scale multi-turn human-written
conversation corpus, and create the first Chinese commonsense conversation
knowledge graph which incorporates both social commonsense knowledge and dialog
flow information. To show the potential of our graph, we develop a
graph-conversation matching approach, and benchmark two graph-grounded
conversational tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Temporal-Modal Entity Graph for Procedural Multimodal Machine Comprehension. (arXiv:2204.02566v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02566">
<div class="article-summary-box-inner">
<span><p>Procedural Multimodal Documents (PMDs) organize textual instructions and
corresponding images step by step. Comprehending PMDs and inducing their
representations for the downstream reasoning tasks is designated as Procedural
MultiModal Machine Comprehension (M3C). In this study, we approach Procedural
M3C at a fine-grained level (compared with existing explorations at a document
or sentence level), that is, entity. With delicate consideration, we model
entity both in its temporal and cross-modal relation and propose a novel
Temporal-Modal Entity Graph (TMEG). Specifically, graph structure is formulated
to capture textual and visual entities and trace their temporal-modal
evolution. In addition, a graph aggregation module is introduced to conduct
graph encoding and reasoning. Comprehensive experiments across three Procedural
M3C tasks are conducted on a traditional dataset RecipeQA and our new dataset
CraftQA, which can better evaluate the generalization of TMEG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing Structured Pruning on Multilingual Pre-trained Models: Settings, Algorithms, and Efficiency. (arXiv:2204.02601v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02601">
<div class="article-summary-box-inner">
<span><p>Structured pruning has been extensively studied on monolingual pre-trained
language models and is yet to be fully evaluated on their multilingual
counterparts. This work investigates three aspects of structured pruning on
multilingual pre-trained language models: settings, algorithms, and efficiency.
Experiments on nine downstream tasks show several counter-intuitive phenomena:
for settings, individually pruning for each language does not induce a better
result; for algorithms, the simplest method performs the best; for efficiency,
a fast model does not imply that it is also small. To facilitate the comparison
on all sparsity levels, we present Dynamic Sparsification, a simple approach
that allows training the model once and adapting to different model sizes at
inference. We hope this work fills the gap in the study of structured pruning
on multilingual pre-trained models and sheds light on future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distributed Transition Systems with Tags for Privacy Analysis. (arXiv:2204.02602v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02602">
<div class="article-summary-box-inner">
<span><p>We present a logical framework that formally models how a given private
information P stored on a given database D, can get captured progressively, by
an agent/adversary querying the database repeatedly.Named DLTTS (Distributed
Labeled Tagged Transition System), the frame-work borrows ideas from several
domains: Probabilistic Automata of Segala, Probabilistic Concurrent Systems,
and Probabilistic labelled transition systems. To every node on a DLTTS is
attached a tag that represents the 'current' knowledge of the adversary,
acquired from the responses of the answering mechanism of the DBMS to his/her
queries, at the nodes traversed earlier, along any given run; this knowledge is
completed at the same node, with further relational deductions, possibly in
combination with 'public' information from other databases given in advance. A
'blackbox' mechanism is also part of a DLTTS, and it is meant as an oracle; its
role is to tell if the private information has been deduced by the adversary at
the current node, and if so terminate the run. An additional special feature is
that the blackbox also gives information on how 'close',or how 'far', the
knowledge of the adversary is, from the private information P , at the current
node. A metric is defined for that purpose, on the set of all 'type compatible'
tuples from the given database, the data themselves being typed with the
headers of the base. Despite the transition systems flavor of our framework,
this metric is not 'behavioral' in the sense presented in some other works. It
is exclusively database oriented,and allows to define new notions of adjacency
and of -indistinguishabilty between databases, more generally than those
usually based on the Hamming metric (and a restricted notion of adjacency).
Examples are given all along to illustrate how our framework works.
Keywords:Database, Privacy, Transition System, Probability, Distribution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">There Are a Thousand Hamlets in a Thousand People's Eyes: Enhancing Knowledge-grounded Dialogue with Personal Memory. (arXiv:2204.02624v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02624">
<div class="article-summary-box-inner">
<span><p>Knowledge-grounded conversation (KGC) shows great potential in building an
engaging and knowledgeable chatbot, and knowledge selection is a key ingredient
in it. However, previous methods for knowledge selection only concentrate on
the relevance between knowledge and dialogue context, ignoring the fact that
age, hobby, education and life experience of an interlocutor have a major
effect on his or her personal preference over external knowledge. Without
taking the personalization issue into account, it is difficult to select the
proper knowledge and generate persona-consistent responses. In this work, we
introduce personal memory into knowledge selection in KGC to address the
personalization issue. We propose a variational method to model the underlying
relationship between one's personal memory and his or her selection of
knowledge, and devise a learning scheme in which the forward mapping from
personal memory to knowledge and its inverse mapping is included in a closed
loop so that they could teach each other. Experiment results show that our
method outperforms existing KGC methods significantly on both automatic
evaluation and human evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Weakly Supervised Propagation Model for Rumor Verification and Stance Detection with Multiple Instance Learning. (arXiv:2204.02626v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02626">
<div class="article-summary-box-inner">
<span><p>The diffusion of rumors on microblogs generally follows a propagation tree
structure, that provides valuable clues on how an original message is
transmitted and responded by users over time. Recent studies reveal that rumor
detection and stance detection are two different but relevant tasks which can
jointly enhance each other, e.g., rumors can be debunked by cross-checking the
stances conveyed by their relevant microblog posts, and stances are also
conditioned on the nature of the rumor. However, most stance detection methods
require enormous post-level stance labels for training, which are
labor-intensive given a large number of posts. Enlightened by Multiple Instance
Learning (MIL) scheme, we first represent the diffusion of claims with
bottom-up and top-down trees, then propose two tree-structured weakly
supervised frameworks to jointly classify rumors and stances, where only the
bag-level labels concerning claim's veracity are needed. Specifically, we
convert the multi-class problem into a multiple MIL-based binary classification
problem where each binary model focuses on differentiating a target stance or
rumor type and other types. Finally, we propose a hierarchical attention
mechanism to aggregate the binary predictions, including (1) a bottom-up or
top-down tree attention layer to aggregate binary stances into binary veracity;
and (2) a discriminative attention layer to aggregate the binary class into
finer-grained classes. Extensive experiments conducted on three Twitter-based
datasets demonstrate promising performance of our model on both claim-level
rumor detection and post-level stance classification compared with
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DAGAM: Data Augmentation with Generation And Modification. (arXiv:2204.02633v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02633">
<div class="article-summary-box-inner">
<span><p>Text classification is a representative downstream task of natural language
processing, and has exhibited excellent performance since the advent of
pre-trained language models based on Transformer architecture. However, in
pre-trained language models, under-fitting often occurs due to the size of the
model being very large compared to the amount of available training data. Along
with significant importance of data collection in modern machine learning
paradigm, studies have been actively conducted for natural language data
augmentation. In light of this, we introduce three data augmentation schemes
that help reduce underfitting problems of large-scale language models.
Primarily we use a generation model for data augmentation, which is defined as
Data Augmentation with Generation (DAG). Next, we augment data using text
modification techniques such as corruption and word order change (Data
Augmentation with Modification, DAM). Finally, we propose Data Augmentation
with Generation And Modification (DAGAM), which combines DAG and DAM techniques
for a boosted performance. We conduct data augmentation for six benchmark
datasets of text classification task, and verify the usefulness of DAG, DAM,
and DAGAM through BERT-based fine-tuning and evaluation, deriving better
results compared to the performance with original datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Synthetic Data for Conversational Response Generation in Low-resource Settings. (arXiv:2204.02653v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02653">
<div class="article-summary-box-inner">
<span><p>Response generation is a task in natural language processing (NLP) where a
model is trained to respond to human statements. Conversational response
generators take this one step further with the ability to respond within the
context of previous responses. While there are existing techniques for training
such models, they all require an abundance of conversational data which are not
always available for low-resource languages. In this research, we make three
contributions. First, we released the first Filipino conversational dataset
collected from a popular Philippine online forum, which we named the PEx
Conversations Dataset. Second, we introduce a data augmentation (DA)
methodology for Filipino data by employing a Tagalog RoBERTa model to increase
the size of the existing corpora. Lastly, we published the first Filipino
conversational response generator capable of generating responses related to
the previous 3 responses. With the supplementary synthetic data, we were able
to improve the performance of the response generator by up to 12.2% in
BERTScore, 10.7% in perplexity, and 11.7% in content word usage as compared to
training with zero synthetic data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Yunshan Cup 2020: Overview of the Part-of-Speech Tagging Task for Low-resourced Languages. (arXiv:2204.02658v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02658">
<div class="article-summary-box-inner">
<span><p>The Yunshan Cup 2020 track focused on creating a framework for evaluating
different methods of part-of-speech (POS). There were two tasks for this track:
(1) POS tagging for the Indonesian language, and (2) POS tagging for the Lao
tagging. The Indonesian dataset is comprised of 10000 sentences from Indonesian
news within 29 tags. And the Lao dataset consists of 8000 sentences within 27
tags. 25 teams registered for the task. The methods of participants ranged from
feature-based to neural networks using either classical machine learning
techniques or ensemble methods. The best performing results achieve an accuracy
of 95.82% for Indonesian and 93.03%, showing that neural sequence labeling
models significantly outperform classic feature-based methods and rule-based
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Model for Text Analytic in Cybersecurity. (arXiv:2204.02685v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02685">
<div class="article-summary-box-inner">
<span><p>NLP is a form of artificial intelligence and machine learning concerned with
a computer or machine's ability to understand and interpret human language.
Language models are crucial in text analytics and NLP since they allow
computers to interpret qualitative input and convert it to quantitative data
that they can use in other tasks. In essence, in the context of transfer
learning, language models are typically trained on a large generic corpus,
referred to as the pre-training stage, and then fine-tuned to a specific
underlying task. As a result, pre-trained language models are mostly used as a
baseline model that incorporates a broad grasp of the context and may be
further customized to be used in a new NLP task.
</p>
<p>The majority of pre-trained models are trained on corpora from general
domains, such as Twitter, newswire, Wikipedia, and Web. Such off-the-shelf NLP
models trained on general text may be inefficient and inaccurate in specialized
fields. In this paper, we propose a cybersecurity language model called
SecureBERT, which is able to capture the text connotations in the cybersecurity
domain, and therefore could further be used in automation for many important
cybersecurity tasks that would otherwise rely on human expertise and tedious
manual efforts. SecureBERT is trained on a large corpus of cybersecurity text
collected and preprocessed by us from a variety of sources in cybersecurity and
the general computing domain. Using our proposed methods for tokenization and
model weights adjustment, SecureBERT is not only able to preserve the
understanding of general English as most pre-trained language models can do,
but also effective when applied to text that has cybersecurity implications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mix-and-Match: Scalable Dialog Response Retrieval using Gaussian Mixture Embeddings. (arXiv:2204.02710v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02710">
<div class="article-summary-box-inner">
<span><p>Embedding-based approaches for dialog response retrieval embed the
context-response pairs as points in the embedding space. These approaches are
scalable, but fail to account for the complex, many-to-many relationships that
exist between context-response pairs. On the other end of the spectrum, there
are approaches that feed the context-response pairs jointly through multiple
layers of neural networks. These approaches can model the complex relationships
between context-response pairs, but fail to scale when the set of responses is
moderately large (&gt;100). In this paper, we combine the best of both worlds by
proposing a scalable model that can learn complex relationships between
context-response pairs. Specifically, the model maps the contexts as well as
responses to probability distributions over the embedding space. We train the
models by optimizing the Kullback-Leibler divergence between the distributions
induced by context-response pairs in the training data. We show that the
resultant model achieves better performance as compared to other
embedding-based approaches on publicly available conversation data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A New Dataset for Topic-Based Paragraph Classification in Genocide-Related Court Transcripts. (arXiv:2204.02712v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02712">
<div class="article-summary-box-inner">
<span><p>Recent progress in natural language processing has been impressive in many
different areas with transformer-based approaches setting new benchmarks for a
wide range of applications. This development has also lowered the barriers for
people outside the NLP community to tap into the tools and resources applied to
a variety of domain-specific applications. The bottleneck however still remains
the lack of annotated gold-standard collections as soon as one's research or
professional interest falls outside the scope of what is readily available. One
such area is genocide-related research (also including the work of experts who
have a professional interest in accessing, exploring and searching large-scale
document collections on the topic, such as lawyers). We present GTC (Genocide
Transcript Corpus), the first annotated corpus of genocide-related court
transcripts which serves three purposes: (1) to provide a first reference
corpus for the community, (2) to establish benchmark performances (using
state-of-the-art transformer-based approaches) for the new classification task
of paragraph identification of violence-related witness statements, (3) to
explore first steps towards transfer learning within the domain. We consider
our contribution to be addressing in particular this year's hot topic on
Language Technology for All.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Annotation-Scheme Reconstruction for "Fake News" and Japanese Fake News Dataset. (arXiv:2204.02718v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02718">
<div class="article-summary-box-inner">
<span><p>Fake news provokes many societal problems; therefore, there has been
extensive research on fake news detection tasks to counter it. Many fake news
datasets were constructed as resources to facilitate this task. Contemporary
research focuses almost exclusively on the factuality aspect of the news.
However, this aspect alone is insufficient to explain "fake news," which is a
complex phenomenon that involves a wide range of issues. To fully understand
the nature of each instance of fake news, it is important to observe it from
various perspectives, such as the intention of the false news disseminator, the
harmfulness of the news to our society, and the target of the news. We propose
a novel annotation scheme with fine-grained labeling based on detailed
investigations of existing fake news datasets to capture these various aspects
of fake news. Using the annotation scheme, we construct and publish the first
Japanese fake news dataset. The annotation scheme is expected to provide an
in-depth understanding of fake news. We plan to build datasets for both
Japanese and other languages using our scheme. Our Japanese dataset is
published at https://hkefka385.github.io/dataset/fakenews-japanese/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Multi-task Generalization Ability for Neural Text Matching via Prompt Learning. (arXiv:2204.02725v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02725">
<div class="article-summary-box-inner">
<span><p>Text matching is a fundamental technique in both information retrieval and
natural language processing. Text matching tasks share the same paradigm that
determines the relationship between two given texts. Evidently, the
relationships vary from task to task, e.g. relevance in document retrieval,
semantic alignment in paraphrase identification and answerable judgment in
question answering. However, the essential signals for text matching remain in
a finite scope, i.e. exact matching, semantic matching, and inference matching.
Recent state-of-the-art neural text matching models, e.g. pre-trained language
models (PLMs), are hard to generalize to different tasks. It is because the
end-to-end supervised learning on task-specific dataset makes model
overemphasize the data sample bias and task-specific signals instead of the
essential matching signals, which ruins the generalization of model to
different tasks. To overcome this problem, we adopt a
specialization-generalization training strategy and refer to it as
Match-Prompt. In specialization stage, descriptions of different matching tasks
are mapped to only a few prompt tokens. In generalization stage, text matching
model explores the essential matching signals by being trained on diverse
multiple matching tasks. High diverse matching tasks avoid model fitting the
data sample bias on a specific task, so that model can focus on learning the
essential matching signals. Meanwhile, the prompt tokens obtained in the first
step are added to the corresponding tasks to help the model distinguish
different task-specific matching signals. Experimental results on eighteen
public datasets show that Match-Prompt can significantly improve multi-task
generalization capability of PLMs in text matching, and yield better in-domain
multi-task, out-of-domain multi-task and new task adaptation performance than
task-specific model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Global Readiness of Language Technology for Healthcare: What would it Take to Combat the Next Pandemic?. (arXiv:2204.02790v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02790">
<div class="article-summary-box-inner">
<span><p>The COVID-19 pandemic has brought out both the best and worst of language
technology (LT). On one hand, conversational agents for information
dissemination and basic diagnosis have seen widespread use, and arguably, had
an important role in combating the pandemic. On the other hand, it has also
become clear that such technologies are readily available for a handful of
languages, and the vast majority of the global south is completely bereft of
these benefits. What is the state of LT, especially conversational agents, for
healthcare across the world's languages? And, what would it take to ensure
global readiness of LT before the next pandemic? In this paper, we try to
answer these questions through survey of existing literature and resources, as
well as through a rapid chatbot building exercise for 15 Asian and African
languages with varying amount of resource-availability. The study confirms the
pitiful state of LT even for languages with large speaker bases, such as
Sinhala and Hausa, and identifies the gaps that could help us prioritize
research and investment strategies in LT for healthcare.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Transformer-Based Contrastive Learning Approach for Few-Shot Sign Language Recognition. (arXiv:2204.02803v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02803">
<div class="article-summary-box-inner">
<span><p>Sign language recognition from sequences of monocular images or 2D poses is a
challenging field, not only due to the difficulty to infer 3D information from
2D data, but also due to the temporal relationship between the sequences of
information. Additionally, the wide variety of signs and the constant need to
add new ones on production environments makes it infeasible to use traditional
classification techniques. We propose a novel Contrastive Transformer-based
model, which demonstrate to learn rich representations from body key points
sequences, allowing better comparison between vector embedding. This allows us
to apply these techniques to perform one-shot or few-shot tasks, such as
classification and translation. The experiments showed that the model could
generalize well and achieved competitive results for sign classes never seen in
the training process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Algebraic Approach to Learning and Grounding. (arXiv:2204.02813v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02813">
<div class="article-summary-box-inner">
<span><p>We consider the problem of learning the semantics of composite algebraic
expressions from examples. The outcome is a versatile framework for studying
learning tasks that can be put into the following abstract form: The input is a
partial algebra A and a finite set of samples ({\phi}1, O1), ({\phi}2, O2),
..., each consisting of an algebraic term {\phi}i and a set of objects Oi. The
objective is to simultaneously fill in the missing algebraic operations in A
and ground the variables of every {\phi}i in Oi, so that the combined value of
the terms is optimised. We demonstrate the applicability of this framework
through case studies in grammatical inference, picture-language learning, and
the grounding of logic scene descriptions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Aggression in Hindi and English Speech: Acoustic Correlates and Automatic Identification. (arXiv:2204.02814v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02814">
<div class="article-summary-box-inner">
<span><p>In the present paper, we will present the results of an acoustic analysis of
political discourse in Hindi and discuss some of the conventionalised acoustic
features of aggressive speech regularly employed by the speakers of Hindi and
English. The study is based on a corpus of slightly over 10 hours of political
discourse and includes debates on news channel and political speeches. Using
this study, we develop two automatic classification systems for identifying
aggression in English and Hindi speech, based solely on an acoustic model. The
Hindi classifier, trained using 50 hours of annotated speech, and English
classifier, trained using 40 hours of annotated speech, achieve a respectable
accuracy of over 73% and 66% respectively. In this paper, we discuss the
development of this annotated dataset, the experiments for developing the
classifier and discuss the errors that it makes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">drsphelps at SemEval-2022 Task 2: Learning idiom representations using BERTRAM. (arXiv:2204.02821v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02821">
<div class="article-summary-box-inner">
<span><p>This paper describes our system for SemEval-2022 Task 2 Multilingual
Idiomaticity Detection and Sentence Embedding sub-task B. We modify a standard
BERT sentence transformer by adding embeddings for each idioms, which are
created using BERTRAM and a small number of contexts. We show that this
technique increases the quality of idiom representations and leads to better
performance on the task. We also perform analysis on our final results and show
that the quality of the produced idiom embeddings is highly sensitive to the
quality of the input contexts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Resources and Technologies for Non-Scheduled and Endangered Indian Languages. (arXiv:2204.02822v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02822">
<div class="article-summary-box-inner">
<span><p>In the present paper, we will present a survey of the language resources and
technologies available for the non-scheduled and endangered languages of India.
While there have been different estimates from different sources about the
number of languages in India, it could be assumed that there are more than
1,000 languages currently being spoken in India. However barring some of the 22
languages included in the 8th Schedule of the Indian Constitution (called the
scheduled languages), there is hardly any substantial resource or technology
available for the rest of the languages. Nonetheless there have been some
individual attempts at developing resources and technologies for the different
languages across the country. Of late, some financial support has also become
available for the endangered languages. In this paper, we give a summary of the
resources and technologies for those Indian languages which are not included in
the 8th schedule of the Indian Constitution and/or which are endangered.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KNN-Diffusion: Image Generation via Large-Scale Retrieval. (arXiv:2204.02849v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02849">
<div class="article-summary-box-inner">
<span><p>While the availability of massive Text-Image datasets is shown to be
extremely useful in training large-scale generative models (e.g. DDPMs,
Transformers), their output typically depends on the quality of both the input
text, as well as the training dataset. In this work, we show how large-scale
retrieval methods, in particular efficient K-Nearest-Neighbors (KNN) search,
can be used in order to train a model to adapt to new samples. Learning to
adapt enables several new capabilities. Sifting through billions of records at
inference time is extremely efficient and can alleviate the need to train or
memorize an adequately large generative model. Additionally, fine-tuning
trained models to new samples can be achieved by simply adding them to the
table. Rare concepts, even without any presence in the training set, can be
then leveraged during test time without any modification to the generative
model. Our diffusion-based model trains on images only, by leveraging a joint
Text-Image multi-modal metric. Compared to baseline methods, our generations
achieve state of the art results both in human evaluations as well as with
perceptual scores when tested on a public multimodal dataset of natural images,
as well as on a collected dataset of 400 million Stickers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ECLIPSE: Efficient Long-range Video Retrieval using Sight and Sound. (arXiv:2204.02874v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02874">
<div class="article-summary-box-inner">
<span><p>We introduce an audiovisual method for long-range text-to-video retrieval.
Unlike previous approaches designed for short video retrieval (e.g., 5-15
seconds in duration), our approach aims to retrieve minute-long videos that
capture complex human actions. One challenge of standard video-only approaches
is the large computational cost associated with processing hundreds of densely
extracted frames from such long videos. To address this issue, we propose to
replace parts of the video with compact audio cues that succinctly summarize
dynamic audio events and are cheap to process. Our method, named ECLIPSE
(Efficient CLIP with Sound Encoding), adapts the popular CLIP model to an
audiovisual video setting, by adding a unified audiovisual transformer block
that captures complementary cues from the video and audio streams. In addition
to being 2.92x faster and 2.34x memory-efficient than long-range video-only
approaches, our method also achieves better text-to-video retrieval accuracy on
several diverse long-range video datasets such as ActivityNet, QVHighlights,
YouCook2, DiDeMo and Charades.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sub-Task Decomposition Enables Learning in Sequence to Sequence Tasks. (arXiv:2204.02892v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02892">
<div class="article-summary-box-inner">
<span><p>The field of Natural Language Processing (NLP) has experienced a dramatic
leap in capabilities with the recent introduction of huge Language Models
(LMs). Despite this success, natural language problems that involve several
compounded steps are still practically unlearnable, even by the largest LMs.
This complies with experimental failures for end-to-end learning of composite
problems that were demonstrated in a variety of domains. A known mitigation is
to introduce intermediate supervision for solving sub-tasks of the compounded
problem. Recently, several works have demonstrated high gains by taking a
straightforward approach for incorporating intermediate supervision in
compounded natural language problems: the sequence-to-sequence LM is fed with
an augmented input, in which the decomposed tasks' labels are simply
concatenated to the original input. In this paper, we prove a positive learning
result that motivates these recent efforts. We show that when concatenating
intermediate supervision to the input and training a sequence-to-sequence model
on this modified input, an unlearnable composite problem becomes learnable. We
prove this for the notoriously unlearnable composite task of bit-subset parity,
with the intermediate supervision being parity results of increasingly large
bit-subsets. Beyond motivating contemporary empirical efforts for incorporating
intermediate supervision in sequence-to-sequence language models, our positive
theoretical result is the first of its kind in the landscape of results on the
benefits of intermediate supervision: Until now, all theoretical results on the
subject are negative, i.e., show cases where learning is impossible without
intermediate supervision, while our result is positive, showing a case where
learning is facilitated in the presence of intermediate supervision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EMMT: A simultaneous eye-tracking, 4-electrode EEG and audio corpus for multi-modal reading and translation scenarios. (arXiv:2204.02905v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02905">
<div class="article-summary-box-inner">
<span><p>We present the Eyetracked Multi-Modal Translation (EMMT) corpus, a dataset
containing monocular eye movement recordings, audio and 4-electrode
electroencephalogram (EEG) data of 43 participants. The objective was to
collect cognitive signals as responses of participants engaged in a number of
language intensive tasks involving different text-image stimuli settings when
translating from English to Czech.
</p>
<p>Each participant was exposed to 32 text-image stimuli pairs and asked to (1)
read the English sentence, (2) translate it into Czech, (3) consult the image,
(4) translate again, either updating or repeating the previous translation. The
text stimuli consisted of 200 unique sentences with 616 unique words coupled
with 200 unique images as the visual stimuli.
</p>
<p>The recordings were collected over a two week period and all the participants
included in the study were Czech natives with strong English skills. Due to the
nature of the tasks involved in the study and the relatively large number of
participants involved, the corpus is well suited for research in Translation
Process Studies, Cognitive Sciences among other disciplines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Base Index Compression via Dimensionality and Precision Reduction. (arXiv:2204.02906v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02906">
<div class="article-summary-box-inner">
<span><p>Recently neural network based approaches to knowledge-intensive NLP tasks,
such as question answering, started to rely heavily on the combination of
neural retrievers and readers. Retrieval is typically performed over a large
textual knowledge base (KB) which requires significant memory and compute
resources, especially when scaled up. On HotpotQA we systematically investigate
reducing the size of the KB index by means of dimensionality (sparse random
projections, PCA, autoencoders) and numerical precision reduction.
</p>
<p>Our results show that PCA is an easy solution that requires very little data
and is only slightly worse than autoencoders, which are less stable. All
methods are sensitive to pre- and post-processing and data should always be
centered and normalized both before and after dimension reduction. Finally, we
show that it is possible to combine PCA with using 1bit per dimension. Overall
we achieve (1) 100$\times$ compression with 75%, and (2) 24$\times$ compression
with 92% original retrieval performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Question Generation for Reading Comprehension Assessment by Modeling How and What to Ask. (arXiv:2204.02908v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02908">
<div class="article-summary-box-inner">
<span><p>Reading is integral to everyday life, and yet learning to read is a struggle
for many young learners. During lessons, teachers can use comprehension
questions to increase engagement, test reading skills, and improve retention.
Historically such questions were written by skilled teachers, but recently
language models have been used to generate comprehension questions. However,
many existing Question Generation (QG) systems focus on generating literal
questions from the text, and have no way to control the type of the generated
question. In this paper, we study QG for reading comprehension where
inferential questions are critical and extractive techniques cannot be used. We
propose a two-step model (HTA-WTA) that takes advantage of previous datasets,
and can generate questions for a specific targeted comprehension skill. We
propose a new reading comprehension dataset that contains questions annotated
with story-based reading comprehension skills (SBRCS), allowing for a more
complete reader assessment. Across several experiments, our results show that
HTA-WTA outperforms multiple strong baselines on this new dataset. We show that
the HTA-WTA model tests for strong SCRS by asking deep inferential questions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Paying More Attention to Self-attention: Improving Pre-trained Language Models via Attention Guiding. (arXiv:2204.02922v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02922">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (PLM) have demonstrated their effectiveness for a
broad range of information retrieval and natural language processing tasks. As
the core part of PLM, multi-head self-attention is appealing for its ability to
jointly attend to information from different positions. However, researchers
have found that PLM always exhibits fixed attention patterns regardless of the
input (e.g., excessively paying attention to [CLS] or [SEP]), which we argue
might neglect important information in the other positions. In this work, we
propose a simple yet effective attention guiding mechanism to improve the
performance of PLM by encouraging attention towards the established goals.
Specifically, we propose two kinds of attention guiding methods, i.e., map
discrimination guiding (MDG) and attention pattern decorrelation guiding (PDG).
The former definitely encourages the diversity among multiple self-attention
heads to jointly attend to information from different representation subspaces,
while the latter encourages self-attention to attend to as many different
positions of the input as possible. We conduct experiments with multiple
general pre-trained models (i.e., BERT, ALBERT, and Roberta) and
domain-specific pre-trained models (i.e., BioBERT, ClinicalBERT, BlueBert, and
SciBERT) on three benchmark datasets (i.e., MultiNLI, MedNLI, and
Cross-genre-IR). Extensive experimental results demonstrate that our proposed
MDG and PDG bring stable performance improvements on all datasets with high
efficiency and low cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inducing Positive Perspectives with Text Reframing. (arXiv:2204.02952v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02952">
<div class="article-summary-box-inner">
<span><p>Sentiment transfer is one popular example of a text style transfer task,
where the goal is to reverse the sentiment polarity of a text. With a sentiment
reversal comes also a reversal in meaning. We introduce a different but related
task called positive reframing in which we neutralize a negative point of view
and generate a more positive perspective for the author without contradicting
the original meaning. Our insistence on meaning preservation makes positive
reframing a challenging and semantically rich task. To facilitate rapid
progress, we introduce a large-scale benchmark, Positive Psychology Frames,
with 8,349 sentence pairs and 12,755 structured annotations to explain positive
reframing in terms of six theoretically-motivated reframing strategies. Then we
evaluate a set of state-of-the-art text style transfer models, and conclude by
discussing key challenges and directions for future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhanced Direct Speech-to-Speech Translation Using Self-supervised Pre-training and Data Augmentation. (arXiv:2204.02967v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02967">
<div class="article-summary-box-inner">
<span><p>Direct speech-to-speech translation (S2ST) models suffer from data scarcity
issues as there exists little parallel S2ST data, compared to the amount of
data available for conventional cascaded systems that consist of automatic
speech recognition (ASR), machine translation (MT), and text-to-speech (TTS)
synthesis. In this work, we explore self-supervised pre-training with unlabeled
speech data and data augmentation to tackle this issue. We take advantage of a
recently proposed speech-to-unit translation (S2UT) framework that encodes
target speech into discrete representations, and transfer pre-training and
efficient partial finetuning techniques that work well for speech-to-text
translation (S2T) to the S2UT domain by studying both speech encoder and
discrete unit decoder pre-training. Our experiments show that self-supervised
pre-training consistently improves model performance compared with multitask
learning with a BLEU gain of 4.3-12.0 under various data setups, and it can be
further combined with data augmentation techniques that apply MT to create
weakly supervised training data. Audio samples are available at:
https://facebookresearch.github.io/speech_translation/enhanced_direct_s2st_units/index.html .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Hierarchy-Aware Knowledge Graph Embeddings for Link Prediction. (arXiv:1911.09419v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1911.09419">
<div class="article-summary-box-inner">
<span><p>Knowledge graph embedding, which aims to represent entities and relations as
low dimensional vectors (or matrices, tensors, etc.), has been shown to be a
powerful technique for predicting missing links in knowledge graphs. Existing
knowledge graph embedding models mainly focus on modeling relation patterns
such as symmetry/antisymmetry, inversion, and composition. However, many
existing approaches fail to model semantic hierarchies, which are common in
real-world applications. To address this challenge, we propose a novel
knowledge graph embedding model -- namely, Hierarchy-Aware Knowledge Graph
Embedding (HAKE) -- which maps entities into the polar coordinate system. HAKE
is inspired by the fact that concentric circles in the polar coordinate system
can naturally reflect the hierarchy. Specifically, the radial coordinate aims
to model entities at different levels of the hierarchy, and entities with
smaller radii are expected to be at higher levels; the angular coordinate aims
to distinguish entities at the same level of the hierarchy, and these entities
are expected to have roughly the same radii but different angles. Experiments
demonstrate that HAKE can effectively model the semantic hierarchies in
knowledge graphs, and significantly outperforms existing state-of-the-art
methods on benchmark datasets for the link prediction task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ktrain: A Low-Code Library for Augmented Machine Learning. (arXiv:2004.10703v5 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.10703">
<div class="article-summary-box-inner">
<span><p>We present ktrain, a low-code Python library that makes machine learning more
accessible and easier to apply. As a wrapper to TensorFlow and many other
libraries (e.g., transformers, scikit-learn, stellargraph), it is designed to
make sophisticated, state-of-the-art machine learning models simple to build,
train, inspect, and apply by both beginners and experienced practitioners.
Featuring modules that support text data (e.g., text classification, sequence
tagging, open-domain question-answering), vision data (e.g., image
classification), graph data (e.g., node classification, link prediction), and
tabular data, ktrain presents a simple unified interface enabling one to
quickly solve a wide range of tasks in as little as three or four "commands" or
lines of code.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Federated Learning for Personalized Humor Recognition. (arXiv:2012.01675v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.01675">
<div class="article-summary-box-inner">
<span><p>Computational understanding of humor is an important topic under creative
language understanding and modeling. It can play a key role in complex human-AI
interactions. The challenge here is that human perception of humorous content
is highly subjective. The same joke may receive different funniness ratings
from different readers. This makes it highly challenging for humor recognition
models to achieve personalization in practical scenarios. Existing approaches
are generally designed based on the assumption that users have a consensus on
whether a given text is humorous or not. Thus, they cannot handle diverse humor
preferences well. In this paper, we propose the FedHumor approach for the
recognition of humorous content in a personalized manner through Federated
Learning (FL). Extending a pre-trained language model, FedHumor guides the
fine-tuning process by considering diverse distributions of humor preferences
from individuals. It incorporates a diversity adaptation strategy into the FL
paradigm to train a personalized humor recognition model. To the best of our
knowledge, FedHumor is the first text-based personalized humor recognition
model through federated learning. Extensive experiments demonstrate the
advantage of FedHumor in recognizing humorous texts compared to nine
state-of-the-art humor recognition approaches with superior capability for
handling the diversity in humor labels produced by users with diverse
preferences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Linguistic Characterization of Divisive Topics Online: Case Studies on Contentiousness in Abortion, Climate Change, and Gun Control. (arXiv:2108.13556v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13556">
<div class="article-summary-box-inner">
<span><p>As public discourse continues to move and grow online, conversations about
divisive topics on social media platforms have also increased. These divisive
topics prompt both contentious and non-contentious conversations. Although what
distinguishes these conversations, often framed as what makes these
conversations contentious, is known in broad strokes, much less is known about
the linguistic signature of these conversations. Prior work has shown that
contentious content and structure can be a predictor for this task, however,
most of them have been focused on conversation in general, very specific
events, or complex structural analysis. Additionally, many models used in prior
work have lacked interpret-ability, a key factor in online moderation. Our work
fills these gaps by focusing on conversations from highly divisive topics
(abortion, climate change, and gun control), operationalizing a set of novel
linguistic and conversational characteristics and user factors, and
incorporating them to build interpretable models. We demonstrate that such
characteristics can largely improve the performance of prediction on this task,
and also enable nuanced interpretability. Our case studies on these three
contentious topics suggest that certain generic linguistic characteristics are
highly correlated with contentiousness in conversations while others
demonstrate significant contextual influences on specific divisive topics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Grammar-Learning Trajectories of Neural Language Models. (arXiv:2109.06096v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06096">
<div class="article-summary-box-inner">
<span><p>The learning trajectories of linguistic phenomena in humans provide insight
into linguistic representation, beyond what can be gleaned from inspecting the
behavior of an adult speaker. To apply a similar approach to analyze neural
language models (NLM), it is first necessary to establish that different models
are similar enough in the generalizations they make. In this paper, we show
that NLMs with different initialization, architecture, and training data
acquire linguistic phenomena in a similar order, despite their different end
performance. These findings suggest that there is some mutual inductive bias
that underlies these models' learning of linguistic phenomena. Taking
inspiration from psycholinguistics, we argue that studying this inductive bias
is an opportunity to study the linguistic representation implicit in NLMs.
</p>
<p>Leveraging these findings, we compare the relative performance on different
phenomena at varying learning stages with simpler reference models. Results
suggest that NLMs exhibit consistent "developmental" stages. Moreover, we find
the learning trajectory to be approximately one-dimensional: given an NLM with
a certain overall performance, it is possible to predict what linguistic
generalizations it has already acquired. Initial analysis of these stages
presents phenomena clusters (notably morphological ones), whose performance
progresses in unison, suggesting a potential link between the generalizations
behind them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adapting TTS models For New Speakers using Transfer Learning. (arXiv:2110.05798v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05798">
<div class="article-summary-box-inner">
<span><p>Training neural text-to-speech (TTS) models for a new speaker typically
requires several hours of high quality speech data. Prior works on voice
cloning attempt to address this challenge by adapting pre-trained multi-speaker
TTS models for a new voice, using a few minutes of speech data of the new
speaker. However, publicly available large multi-speaker datasets are often
noisy, thereby resulting in TTS models that are not suitable for use in
products. We address this challenge by proposing transfer-learning guidelines
for adapting high quality single-speaker TTS models for a new speaker, using
only a few minutes of speech data. We conduct an extensive study using
different amounts of data for a new speaker and evaluate the synthesized speech
in terms of naturalness and voice/style similarity to the target speaker. We
find that fine-tuning a single-speaker TTS model on just 30 minutes of data,
can yield comparable performance to a model trained from scratch on more than
27 hours of data for both male and female target speakers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta-Knowledge Transfer for Inductive Knowledge Graph Embedding. (arXiv:2110.14170v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14170">
<div class="article-summary-box-inner">
<span><p>Knowledge graphs (KGs) which consist of a large number of triples have become
widespread recently, and many knowledge graph embedding (KGE) methods are
proposed to embed entities and relations of a KG into continuous vector spaces.
Such embedding methods aim at simplifying the operations of conducting various
in-KG tasks (e.g., link prediction) and out-of-KG tasks (e.g., question
answering), and can be viewed as general solutions for representing KGs.
However, existing KGE methods are not applicable to inductive settings, where a
model trained on source KGs will be tested on target KGs with entities unseen
during model training. Existing works focusing on KGs in inductive settings can
only solve the inductive relation prediction task and can not handle other
out-of-KG tasks as general as KGE methods, since they don't produce embeddings
for entities. In this paper, to achieve inductive knowledge graph embedding, we
propose a model MorsE, which doesn't learn embeddings for entities, while
learning transferable meta-knowledge that can be used to produce entity
embeddings. Such meta-knowledge is modeled by entity-independent modules and
learned by meta-learning. Experimental results show that our model
significantly outperforms corresponding baselines for in-KG and out-of-KG tasks
in inductive settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">L-Verse: Bidirectional Generation Between Image and Text. (arXiv:2111.11133v11 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11133">
<div class="article-summary-box-inner">
<span><p>Far beyond learning long-range interactions of natural language, transformers
are becoming the de-facto standard for many vision tasks with their power and
scalability. Especially with cross-modal tasks between image and text, vector
quantized variational autoencoders (VQ-VAEs) are widely used to make a raw RGB
image into a sequence of feature vectors. To better leverage the correlation
between image and text, we propose L-Verse, a novel architecture consisting of
feature-augmented variational autoencoder (AugVAE) and bidirectional
auto-regressive transformer (BiART) for image-to-text and text-to-image
generation. Our AugVAE shows the state-of-the-art reconstruction performance on
ImageNet1K validation set, along with the robustness to unseen images in the
wild. Unlike other models, BiART can distinguish between image (or text) as a
conditional reference and a generation target. L-Verse can be directly used for
image-to-text or text-to-image generation without any finetuning or extra
object detection framework. In quantitative and qualitative experiments,
L-Verse shows impressive results against previous methods in both image-to-text
and text-to-image generation on MS-COCO Captions. We furthermore assess the
scalability of L-Verse architecture on Conceptual Captions and present the
initial result of bidirectional vision-language representation learning on
general domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LAVT: Language-Aware Vision Transformer for Referring Image Segmentation. (arXiv:2112.02244v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02244">
<div class="article-summary-box-inner">
<span><p>Referring image segmentation is a fundamental vision-language task that aims
to segment out an object referred to by a natural language expression from an
image. One of the key challenges behind this task is leveraging the referring
expression for highlighting relevant positions in the image. A paradigm for
tackling this problem is to leverage a powerful vision-language ("cross-modal")
decoder to fuse features independently extracted from a vision encoder and a
language encoder. Recent methods have made remarkable advancements in this
paradigm by exploiting Transformers as cross-modal decoders, concurrent to the
Transformer's overwhelming success in many other vision-language tasks.
Adopting a different approach in this work, we show that significantly better
cross-modal alignments can be achieved through the early fusion of linguistic
and visual features in intermediate layers of a vision Transformer encoder
network. By conducting cross-modal feature fusion in the visual feature
encoding stage, we can leverage the well-proven correlation modeling power of a
Transformer encoder for excavating helpful multi-modal context. This way,
accurate segmentation results are readily harvested with a light-weight mask
predictor. Without bells and whistles, our method surpasses the previous
state-of-the-art methods on RefCOCO, RefCOCO+, and G-Ref by large margins.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explainable Natural Language Processing with Matrix Product States. (arXiv:2112.08628v2 [cond-mat.dis-nn] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08628">
<div class="article-summary-box-inner">
<span><p>Despite empirical successes of recurrent neural networks (RNNs) in natural
language processing (NLP), theoretical understanding of RNNs is still limited
due to intrinsically complex non-linear computations. We systematically analyze
RNNs' behaviors in a ubiquitous NLP task, the sentiment analysis of movie
reviews, via the mapping between a class of RNNs called recurrent arithmetic
circuits (RACs) and a matrix product state (MPS). Using the von-Neumann
entanglement entropy (EE) as a proxy for information propagation, we show that
single-layer RACs possess a maximum information propagation capacity, reflected
by the saturation of the EE. Enlarging the bond dimension beyond the EE
saturation threshold does not increase model prediction accuracies, so a
minimal model that best estimates the data statistics can be inferred. Although
the saturated EE is smaller than the maximum EE allowed by the area law, our
minimal model still achieves ~99% training accuracies in realistic sentiment
analysis data sets. Thus, low EE is not a warrant against the adoption of
single-layer RACs for NLP. Contrary to a common belief that long-range
information propagation is the main source of RNNs' successes, we show that
single-layer RACs harness high expressiveness from the subtle interplay between
the information propagation and the word vector embeddings. Our work sheds
light on the phenomenology of learning in RACs, and more generally on the
explainability of RNNs for NLP, using tools from many-body quantum physics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal data matters: language model pre-training over structured and unstructured electronic health records. (arXiv:2201.10113v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10113">
<div class="article-summary-box-inner">
<span><p>The massive amount of electronic health records (EHRs) has created enormous
potential for improving healthcare, among which clinical codes (structured
data) and clinical narratives (unstructured data) are two important textual
modalities. Most existing EHR-oriented studies, however, either only focus on a
particular modality or integrate data from different modalities in a shallow
manner, which ignores the intrinsic interactions between them. To address these
issues, we proposed a Medical Multimodal Pre-trained Language Model, named
MedM-PLM, to learn enhanced EHR representations over structured and
unstructured data. In MedM-PLM, two Transformer-based neural networks
components are firstly adopted to learn representative characteristics from
each modality. A cross-modal module is then introduced to model their
interactions. We pre-trained MedM-PLM on the MIMIC-III dataset and verified the
effectiveness of the model on three downstream clinical tasks, i.e., medication
recommendation, 30-day readmission, and ICD coding. Extensive experiments
demonstrate the power of MedM-PLM compared with state-of-the-art methods.
Further analyses and visualizations show the robustness of our model which
could potentially provide more comprehensive interpretations for clinical
decision-making.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chain of Thought Prompting Elicits Reasoning in Large Language Models. (arXiv:2201.11903v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11903">
<div class="article-summary-box-inner">
<span><p>Although scaling up language model size has reliably improved performance on
a range of NLP tasks, even the largest models currently struggle with certain
reasoning tasks such as math word problems, symbolic manipulation, and
commonsense reasoning. This paper explores the ability of language models to
generate a coherent chain of thought -- a series of short sentences that mimic
the reasoning process a person might have when responding to a question.
Experiments show that inducing a chain of thought via prompting can enable
sufficiently large language models to better perform reasoning tasks that
otherwise have flat scaling curves. When combined with the 540B parameter PaLM
model, chain of thought prompting achieves new state of the art of 58.1\% on
the GSM8K benchmark of math word problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Legal Argument Mining with Domain Pre-training and Neural Networks. (arXiv:2202.13457v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13457">
<div class="article-summary-box-inner">
<span><p>The contextual word embedding model, BERT, has proved its ability on
downstream tasks with limited quantities of annotated data. BERT and its
variants help to reduce the burden of complex annotation work in many
interdisciplinary research areas, for example, legal argument mining in digital
humanities. Argument mining aims to develop text analysis tools that can
automatically retrieve arguments and identify relationships between
argumentation clauses. Since argumentation is one of the key aspects of case
law, argument mining tools for legal texts are applicable to both academic and
non-academic legal research. Domain-specific BERT variants (pre-trained with
corpora from a particular background) have also achieved strong performance in
many tasks. To our knowledge, previous machine learning studies of argument
mining on judicial case law still heavily rely on statistical models. In this
paper, we provide a broad study of both classic and contextual embedding models
and their performance on practical case law from the European Court of Human
Rights (ECHR). During our study, we also explore a number of neural networks
when being combined with different embeddings. Our experiments provide a
comprehensive overview of a variety of approaches to the legal argument mining
task. We conclude that domain pre-trained transformer models have great
potential in this area, although traditional embeddings can also achieve strong
performance when combined with additional neural network layers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ERNIE-GeoL: A Geography-and-Language Pre-trained Model and its Applications in Baidu Maps. (arXiv:2203.09127v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09127">
<div class="article-summary-box-inner">
<span><p>Pre-trained models (PTMs) have become a fundamental backbone for downstream
tasks in natural language processing and computer vision. Despite initial gains
that were obtained by applying generic PTMs to geo-related tasks at Baidu Maps,
a clear performance plateau over time was observed. One of the main reasons for
this plateau is the lack of readily available geographic knowledge in generic
PTMs. To address this problem, in this paper, we present ERNIE-GeoL, which is a
geography-and-language pre-trained model designed and developed for improving
the geo-related tasks at Baidu Maps. ERNIE-GeoL is elaborately designed to
learn a universal representation of geography-language by pre-training on
large-scale data generated from a heterogeneous graph that contains abundant
geographic knowledge. Extensive quantitative and qualitative experiments
conducted on large-scale real-world datasets demonstrate the superiority and
effectiveness of ERNIE-GeoL. ERNIE-GeoL has already been deployed in production
at Baidu Maps since April 2021, which significantly benefits the performance of
a wide range of downstream tasks. This demonstrates that ERNIE-GeoL can serve
as a fundamental backbone for geo-related tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Expanding Pretrained Models to Thousands More Languages via Lexicon-based Adaptation. (arXiv:2203.09435v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09435">
<div class="article-summary-box-inner">
<span><p>The performance of multilingual pretrained models is highly dependent on the
availability of monolingual or parallel text present in a target language.
Thus, the majority of the world's languages cannot benefit from recent progress
in NLP as they have no or limited textual data. To expand possibilities of
using NLP technology in these under-represented languages, we systematically
study strategies that relax the reliance on conventional language resources
through the use of bilingual lexicons, an alternative resource with much better
language coverage. We analyze different strategies to synthesize textual or
labeled data using lexicons, and how this data can be combined with monolingual
or parallel text when available. For 19 under-represented languages across 3
tasks, our methods lead to consistent improvements of up to 5 and 15 points
with and without extra monolingual text respectively. Overall, our study
highlights how NLP methods can be adapted to thousands more languages that are
under-served by current technology
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Consistency Improves Chain of Thought Reasoning in Language Models. (arXiv:2203.11171v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11171">
<div class="article-summary-box-inner">
<span><p>We explore a simple ensemble strategy, self-consistency, that significantly
improves the reasoning accuracy of large language models. The idea is to sample
a diverse set of reasoning paths from a language model via chain of thought
prompting then return the most consistent final answer in the set. We evaluate
self-consistency on a range of arithmetic and commonsense reasoning benchmarks,
and find that it robustly improves accuracy across a variety of language models
and model scales without the need for additional training or auxiliary models.
When combined with a recent large language model, PaLM-540B, self-consistency
increases performance to state-of-the-art levels across several benchmark
reasoning tasks, including GSM8K (56.5% -&gt; 74.4%), SVAMP (79.0% -&gt; 86.6%), AQuA
(35.8% -&gt; 48.3%), StrategyQA (75.3% -&gt; 81.6%) and ARC-challenge (85.2% -&gt;
88.7%).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Expressive Speaking Style Modelling with Hierarchical Context Information for Mandarin Speech Synthesis. (arXiv:2203.12201v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12201">
<div class="article-summary-box-inner">
<span><p>Previous works on expressive speech synthesis mainly focus on current
sentence. The context in adjacent sentences is neglected, resulting in
inflexible speaking style for the same text, which lacks speech variations. In
this paper, we propose a hierarchical framework to model speaking style from
context. A hierarchical context encoder is proposed to explore a wider range of
contextual information considering structural relationship in context,
including inter-phrase and inter-sentence relations. Moreover, to encourage
this encoder to learn style representation better, we introduce a novel
training strategy with knowledge distillation, which provides the target for
encoder training. Both objective and subjective evaluations on a Mandarin
lecture dataset demonstrate that the proposed method can significantly improve
the naturalness and expressiveness of the synthesized speech.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Duality-Induced Regularizer for Semantic Matching Knowledge Graph Embeddings. (arXiv:2203.12949v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12949">
<div class="article-summary-box-inner">
<span><p>Semantic matching models -- which assume that entities with similar semantics
have similar embeddings -- have shown great power in knowledge graph embeddings
(KGE). Many existing semantic matching models use inner products in embedding
spaces to measure the plausibility of triples and quadruples in static and
temporal knowledge graphs. However, vectors that have the same inner products
with another vector can still be orthogonal to each other, which implies that
entities with similar semantics may have dissimilar embeddings. This property
of inner products significantly limits the performance of semantic matching
models. To address this challenge, we propose a novel regularizer -- namely,
DUality-induced RegulArizer (DURA) -- which effectively encourages the entities
with similar semantics to have similar embeddings. The major novelty of DURA is
based on the observation that, for an existing semantic matching KGE model
(primal), there is often another distance based KGE model (dual) closely
associated with it, which can be used as effective constraints for entity
embeddings. Experiments demonstrate that DURA consistently and significantly
improves the performance of state-of-the-art semantic matching models on both
static and temporal knowledge graph benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Baseline Readability Model for Cebuano. (arXiv:2203.17225v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.17225">
<div class="article-summary-box-inner">
<span><p>In this study, we developed the first baseline readability model for the
Cebuano language. Cebuano is the second most-used native language in the
Philippines with about 27.5 million speakers. As the baseline, we extracted
traditional or surface-based features, syllable patterns based from Cebuano's
documented orthography, and neural embeddings from the multilingual BERT model.
Results show that the use of the first two handcrafted linguistic features
obtained the best performance trained on an optimized Random Forest model with
approximately 87% across all metrics. The feature sets and algorithm used also
is similar to previous results in readability assessment for the Filipino
language showing potential of crosslingual application. To encourage more work
for readability assessment in Philippine languages such as Cebuano, we
open-sourced both code and data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Metaphorical User Simulators for Evaluating Task-oriented Dialogue Systems. (arXiv:2204.00763v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00763">
<div class="article-summary-box-inner">
<span><p>Task-oriented dialogue systems (TDSs) are assessed mainly in an offline
setting or through human evaluation. The evaluation is often limited to
single-turn or very time-intensive. As an alternative, user simulators that
mimic user behavior allow us to consider a broad set of user goals to generate
human-like conversations for simulated evaluation. Employing existing user
simulators to evaluate TDSs is challenging as user simulators are primarily
designed to optimize dialogue policies for TDSs and have limited evaluation
capability. Moreover, the evaluation of user simulators is an open challenge.
In this work, we proposes a metaphorical user simulator for endto-end TDS
evaluation. We also propose a tester-based evaluation framework to generate
variants, i.e., dialogue systems with different capabilities. Our user
simulator constructs a metaphorical user model that assists the simulator in
reasoning by referring to prior knowledge when encountering new items. We
estimate the quality of simulators by checking the simulated interactions
between simulators and variants. Our experiments are conducted using three TDS
datasets. The metaphorical user simulator demonstrates better consistency with
manual evaluation than Agenda-based simulator and Seq2seq model on three
datasets; our tester framework demonstrates efficiency, and our approach
demonstrates better generalization and scalability.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Modal Hypergraph Diffusion Network with Dual Prior for Alzheimer Classification. (arXiv:2204.02399v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02399">
<div class="article-summary-box-inner">
<span><p>The automatic early diagnosis of prodromal stages of Alzheimer's disease is
of great relevance for patient treatment to improve quality of life. We address
this problem as a multi-modal classification task. Multi-modal data provides
richer and complementary information. However, existing techniques only
consider either lower order relations between the data and single/multi-modal
imaging data. In this work, we introduce a novel semi-supervised hypergraph
learning framework for Alzheimer's disease diagnosis. Our framework allows for
higher-order relations among multi-modal imaging and non-imaging data whilst
requiring a tiny labelled set. Firstly, we introduce a dual embedding strategy
for constructing a robust hypergraph that preserves the data semantics. We
achieve this by enforcing perturbation invariance at the image and graph levels
using a contrastive based mechanism. Secondly, we present a dynamically
adjusted hypergraph diffusion model, via a semi-explicit flow, to improve the
predictive uncertainty. We demonstrate, through our experiments, that our
framework is able to outperform current techniques for Alzheimer's disease
diagnosis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explainable Deep Learning Algorithm for Distinguishing Incomplete Kawasaki Disease by Coronary Artery Lesions on Echocardiographic Imaging. (arXiv:2204.02403v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02403">
<div class="article-summary-box-inner">
<span><p>Background and Objective: Incomplete Kawasaki disease (KD) has often been
misdiagnosed due to a lack of the clinical manifestations of classic KD.
However, it is associated with a markedly higher prevalence of coronary artery
lesions. Identifying coronary artery lesions by echocardiography is important
for the timely diagnosis of and favorable outcomes in KD. Moreover, similar to
KD, coronavirus disease 2019, currently causing a worldwide pandemic, also
manifests with fever; therefore, it is crucial at this moment that KD should be
distinguished clearly among the febrile diseases in children. In this study, we
aimed to validate a deep learning algorithm for classification of KD and other
acute febrile diseases.
</p>
<p>Methods: We obtained coronary artery images by echocardiography of children
(n = 88 for KD; n = 65 for pneumonia). We trained six deep learning networks
(VGG19, Xception, ResNet50, ResNext50, SE-ResNet50, and SE-ResNext50) using the
collected data.
</p>
<p>Results: SE-ResNext50 showed the best performance in terms of accuracy,
specificity, and precision in the classification. SE-ResNext50 offered a
precision of 76.35%, a sensitivity of 82.64%, and a specificity of 58.12%.
</p>
<p>Conclusions: The results of our study suggested that deep learning algorithms
have similar performance to an experienced cardiologist in detecting coronary
artery lesions to facilitate the diagnosis of KD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hospital-Agnostic Image Representation Learning in Digital Pathology. (arXiv:2204.02404v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02404">
<div class="article-summary-box-inner">
<span><p>Whole Slide Images (WSIs) in digital pathology are used to diagnose cancer
subtypes. The difference in procedures to acquire WSIs at various trial sites
gives rise to variability in the histopathology images, thus making consistent
diagnosis challenging. These differences may stem from variability in image
acquisition through multi-vendor scanners, variable acquisition parameters, and
differences in staining procedure; as well, patient demographics may bias the
glass slide batches before image acquisition. These variabilities are assumed
to cause a domain shift in the images of different hospitals. It is crucial to
overcome this domain shift because an ideal machine-learning model must be able
to work on the diverse sources of images, independent of the acquisition
center. A domain generalization technique is leveraged in this study to improve
the generalization capability of a Deep Neural Network (DNN), to an unseen
histopathology image set (i.e., from an unseen hospital/trial site) in the
presence of domain shift. According to experimental results, the conventional
supervised-learning regime generalizes poorly to data collected from different
hospitals. However, the proposed hospital-agnostic learning can improve the
generalization considering the low-dimensional latent space representation
visualization, and classification accuracy results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-shot Blind Image Denoising via Implicit Neural Representations. (arXiv:2204.02405v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02405">
<div class="article-summary-box-inner">
<span><p>Recent denoising algorithms based on the "blind-spot" strategy show
impressive blind image denoising performances, without utilizing any external
dataset. While the methods excel in recovering highly contaminated images, we
observe that such algorithms are often less effective under a low-noise or real
noise regime. To address this gap, we propose an alternative denoising strategy
that leverages the architectural inductive bias of implicit neural
representations (INRs), based on our two findings: (1) INR tends to fit the
low-frequency clean image signal faster than the high-frequency noise, and (2)
INR layers that are closer to the output play more critical roles in fitting
higher-frequency parts. Building on these observations, we propose a denoising
algorithm that maximizes the innate denoising capability of INRs by penalizing
the growth of deeper layer weights. We show that our method outperforms
existing zero-shot denoising methods under an extensive set of low-noise or
real-noise scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A deep learning framework for the detection and quantification of drusen and reticular pseudodrusen on optical coherence tomography. (arXiv:2204.02406v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02406">
<div class="article-summary-box-inner">
<span><p>Purpose - To develop and validate a deep learning (DL) framework for the
detection and quantification of drusen and reticular pseudodrusen (RPD) on
optical coherence tomography scans.
</p>
<p>Design - Development and validation of deep learning models for
classification and feature segmentation.
</p>
<p>Methods - A DL framework was developed consisting of a classification model
and an out-of-distribution (OOD) detection model for the identification of
ungradable scans; a classification model to identify scans with drusen or RPD;
and an image segmentation model to independently segment lesions as RPD or
drusen. Data were obtained from 1284 participants in the UK Biobank (UKBB) with
a self-reported diagnosis of age-related macular degeneration (AMD) and 250
UKBB controls. Drusen and RPD were manually delineated by five retina
specialists. The main outcome measures were sensitivity, specificity, area
under the ROC curve (AUC), kappa, accuracy and intraclass correlation
coefficient (ICC).
</p>
<p>Results - The classification models performed strongly at their respective
tasks (0.95, 0.93, and 0.99 AUC, respectively, for the ungradable scans
classifier, the OOD model, and the drusen and RPD classification model). The
mean ICC for drusen and RPD area vs. graders was 0.74 and 0.61, respectively,
compared with 0.69 and 0.68 for intergrader agreement. FROC curves showed that
the model's sensitivity was close to human performance.
</p>
<p>Conclusions - The models achieved high classification and segmentation
performance, similar to human performance. Application of this robust framework
will further our understanding of RPD as a separate entity from drusen in both
research and clinical settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Texturify: Generating Textures on 3D Shape Surfaces. (arXiv:2204.02411v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02411">
<div class="article-summary-box-inner">
<span><p>Texture cues on 3D objects are key to compelling visual representations, with
the possibility to create high visual fidelity with inherent spatial
consistency across different views. Since the availability of textured 3D
shapes remains very limited, learning a 3D-supervised data-driven method that
predicts a texture based on the 3D input is very challenging. We thus propose
Texturify, a GAN-based method that leverages a 3D shape dataset of an object
class and learns to reproduce the distribution of appearances observed in real
images by generating high-quality textures. In particular, our method does not
require any 3D color supervision or correspondence between shape geometry and
images to learn the texturing of 3D objects. Texturify operates directly on the
surface of the 3D objects by introducing face convolutional operators on a
hierarchical 4-RoSy parametrization to generate plausible object-specific
textures. Employing differentiable rendering and adversarial losses that
critique individual views and consistency across views, we effectively learn
the high-quality surface texturing distribution from real-world images.
Experiments on car and chair shape collections show that our approach
outperforms state of the art by an average of 22% in FID score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CHORE: Contact, Human and Object REconstruction from a single RGB image. (arXiv:2204.02445v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02445">
<div class="article-summary-box-inner">
<span><p>While most works in computer vision and learning have focused on perceiving
3D humans from single images in isolation, in this work we focus on capturing
3D humans interacting with objects. The problem is extremely challenging due to
heavy occlusions between human and object, diverse interaction types and depth
ambiguity. In this paper, we introduce CHORE, a novel method that learns to
jointly reconstruct human and object from a single image. CHORE takes
inspiration from recent advances in implicit surface learning and classical
model-based fitting. We compute a neural reconstruction of human and object
represented implicitly with two unsigned distance fields, and additionally
predict a correspondence field to a parametric body as well as an object pose
field. This allows us to robustly fit a parametric body model and a 3D object
template, while reasoning about interactions. Furthermore, prior pixel-aligned
implicit learning methods use synthetic data and make assumptions that are not
met in real data. We propose a simple yet effective depth-aware scaling that
allows more efficient shape learning on real data. Our experiments show that
our joint reconstruction learned with the proposed strategy significantly
outperforms the SOTA. Our code and models will be released to foster future
research in this direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Cloud-Based Phishing Attacks by Combining Deep Learning Models. (arXiv:2204.02446v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02446">
<div class="article-summary-box-inner">
<span><p>Web-based phishing attacks nowadays exploit popular cloud web hosting
services and apps such as Google Sites and Typeform for hosting their attacks.
Since these attacks originate from reputable domains and IP addresses of the
cloud services, traditional phishing detection methods such as IP reputation
monitoring and blacklisting are not very effective. Here we investigate the
effectiveness of deep learning models in detecting this class of cloud-based
phishing attacks. Specifically, we evaluate deep learning models for three
phishing detection methods--LSTM model for URL analysis, YOLOv2 model for logo
analysis, and triplet network model for visual similarity analysis. We train
the models using well-known datasets and test their performance on phishing
attacks in the wild. Our results qualitatively explain why the models succeed
or fail. Furthermore, our results highlight how combining results from the
individual models can improve the effectiveness of detecting cloud-based
phishing attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting and Explaining Mobile UI Tappability with Vision Modeling and Saliency Analysis. (arXiv:2204.02448v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02448">
<div class="article-summary-box-inner">
<span><p>We use a deep learning based approach to predict whether a selected element
in a mobile UI screenshot will be perceived by users as tappable, based on
pixels only instead of view hierarchies required by previous work. To help
designers better understand model predictions and to provide more actionable
design feedback than predictions alone, we additionally use ML interpretability
techniques to help explain the output of our model. We use XRAI to highlight
areas in the input screenshot that most strongly influence the tappability
prediction for the selected region, and use k-Nearest Neighbors to present the
most similar mobile UIs from the dataset with opposing influences on
tappability perception.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Federated Cross Learning for Medical Image Segmentation. (arXiv:2204.02450v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02450">
<div class="article-summary-box-inner">
<span><p>Federated learning (FL) can collaboratively train deep learning models using
isolated patient data owned by different hospitals for various clinical
applications, including medical image segmentation. However, a major problem of
FL is its performance degradation when dealing with the data that are not
independently and identically distributed (non-iid), which is often the case in
medical images. In this paper, we first conduct a theoretical analysis on the
FL algorithm to reveal the problem of model aggregation during training on
non-iid data. With the insights gained through the analysis, we propose a
simple and yet effective method, federated cross learning (FedCross), to tackle
this challenging problem. Unlike the conventional FL methods that combine
multiple individually trained local models on a server node, our FedCross
sequentially trains the global model across different clients in a round-robin
manner, and thus the entire training procedure does not involve any model
aggregation steps. To further improve its performance to be comparable with the
centralized learning method, we combine the FedCross with an ensemble learning
mechanism to compose a federated cross ensemble learning (FedCrossEns) method.
Finally, we conduct extensive experiments using a set of public datasets. The
experimental results show that the proposed FedCross training strategy
outperforms the mainstream FL methods on non-iid data. In addition to improving
the segmentation performance, our FedCrossEns can further provide a
quantitative estimation of the model uncertainty, demonstrating the
effectiveness and clinical significance of our designs. Source code will be
made publicly available after paper publication.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Optimal K-space Acquisition and Reconstruction using Physics-Informed Neural Networks. (arXiv:2204.02480v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02480">
<div class="article-summary-box-inner">
<span><p>The inherent slow imaging speed of Magnetic Resonance Image (MRI) has spurred
the development of various acceleration methods, typically through
heuristically undersampling the MRI measurement domain known as k-space.
Recently, deep neural networks have been applied to reconstruct undersampled
k-space data and have shown improved reconstruction performance. While most of
these methods focus on designing novel reconstruction networks or new training
strategies for a given undersampling pattern, \textit{e.g.}, Cartesian
undersampling or Non-Cartesian sampling, to date, there is limited research
aiming to learn and optimize k-space sampling strategies using deep neural
networks. This work proposes a novel optimization framework to learn k-space
sampling trajectories by considering it as an Ordinary Differential Equation
(ODE) problem that can be solved using neural ODE. In particular, the sampling
of k-space data is framed as a dynamic system, in which neural ODE is
formulated to approximate the system with additional constraints on MRI
physics. In addition, we have also demonstrated that trajectory optimization
and image reconstruction can be learned collaboratively for improved imaging
efficiency and reconstruction performance. Experiments were conducted on
different in-vivo datasets (\textit{e.g.}, brain and knee images) acquired with
different sequences. Initial results have shown that our proposed method can
generate better image quality in accelerated MRI than conventional
undersampling schemes in Cartesian and Non-Cartesian acquisitions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Robustness through the Lens of Convolutional Filters. (arXiv:2204.02481v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02481">
<div class="article-summary-box-inner">
<span><p>Deep learning models are intrinsically sensitive to distribution shifts in
the input data. In particular, small, barely perceivable perturbations to the
input data can force models to make wrong predictions with high confidence. An
common defense mechanism is regularization through adversarial training which
injects worst-case perturbations back into training to strengthen the decision
boundaries, and to reduce overfitting. In this context, we perform an
investigation of 3x3 convolution filters that form in adversarially-trained
models. Filters are extracted from 71 public models of the linf-RobustBench
CIFAR-10/100 and ImageNet1k leaderboard and compared to filters extracted from
models built on the same architectures but trained without robust
regularization. We observe that adversarially-robust models appear to form more
diverse, less sparse, and more orthogonal convolution filters than their normal
counterparts. The largest differences between robust and normal models are
found in the deepest layers, and the very first convolution layer, which
consistently and predominantly forms filters that can partially eliminate
perturbations, irrespective of the architecture. Data &amp; Project website:
https://github.com/paulgavrikov/cvpr22w_RobustnessThroughTheLens
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training-Free Robust Multimodal Learning via Sample-Wise Jacobian Regularization. (arXiv:2204.02485v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02485">
<div class="article-summary-box-inner">
<span><p>Multimodal fusion emerges as an appealing technique to improve model
performances on many tasks. Nevertheless, the robustness of such fusion methods
is rarely involved in the present literature. In this paper, we propose a
training-free robust late-fusion method by exploiting conditional independence
assumption and Jacobian regularization. Our key is to minimize the Frobenius
norm of a Jacobian matrix, where the resulting optimization problem is relaxed
to a tractable Sylvester equation. Furthermore, we provide a theoretical error
bound of our method and some insights about the function of the extra modality.
Several numerical experiments on AV-MNIST, RAVDESS, and VGGsound demonstrate
the efficacy of our method under both adversarial attacks and random
corruptions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text2LIVE: Text-Driven Layered Image and Video Editing. (arXiv:2204.02491v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02491">
<div class="article-summary-box-inner">
<span><p>We present a method for zero-shot, text-driven appearance manipulation in
natural images and videos. Given an input image or video and a target text
prompt, our goal is to edit the appearance of existing objects (e.g., object's
texture) or augment the scene with visual effects (e.g., smoke, fire) in a
semantically meaningful manner. We train a generator using an internal dataset
of training examples, extracted from a single input (image or video and target
text prompt), while leveraging an external pre-trained CLIP model to establish
our losses. Rather than directly generating the edited output, our key idea is
to generate an edit layer (color+opacity) that is composited over the original
input. This allows us to constrain the generation process and maintain high
fidelity to the original input via novel text-driven losses that are applied
directly to the edit layer. Our method neither relies on a pre-trained
generator nor requires user-provided edit masks. We demonstrate localized,
semantic edits on high-resolution natural images and videos across a variety of
objects and scenes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Disentangled Representations to Improve Vision-Based Keystroke Inference Attacks Under Low Data. (arXiv:2204.02494v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02494">
<div class="article-summary-box-inner">
<span><p>Keystroke inference attacks are a form of side-channel attacks in which an
attacker leverages various techniques to recover a user's keystrokes as she
inputs information into some display (e.g., while sending a text message or
entering her pin). Typically, these attacks leverage machine learning
approaches, but assessing the realism of the threat space has lagged behind the
pace of machine learning advancements, due in-part, to the challenges in
curating large real-life datasets. We aim to overcome the challenge of having
limited number of real data by introducing a video domain adaptation technique
that is able to leverage synthetic data through supervised disentangled
learning. Specifically, for a given domain, we decompose the observed data into
two factors of variation: Style and Content. Doing so provides four learned
representations: real-life style, synthetic style, real-life content and
synthetic content. Then, we combine them into feature representations from all
combinations of style-content pairings across domains, and train a model on
these combined representations to classify the content (i.e., labels) of a
given datapoint in the style of another domain. We evaluate our method on
real-life data using a variety of metrics to quantify the amount of information
an attacker is able to recover. We show that our method prevents our model from
overfitting to a small real-life training set, indicating that our method is an
effective form of data augmentation, thereby making keystroke inference attacks
more practical.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Depth-Guided Sparse Structure-from-Motion for Movies and TV Shows. (arXiv:2204.02509v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02509">
<div class="article-summary-box-inner">
<span><p>Existing approaches for Structure from Motion (SfM) produce impressive 3-D
reconstruction results especially when using imagery captured with large
parallax. However, to create engaging video-content in movies and TV shows, the
amount by which a camera can be moved while filming a particular shot is often
limited. The resulting small-motion parallax between video frames makes
standard geometry-based SfM approaches not as effective for movies and TV
shows. To address this challenge, we propose a simple yet effective approach
that uses single-frame depth-prior obtained from a pretrained network to
significantly improve geometry-based SfM for our small-parallax setting. To
this end, we first use the depth-estimates of the detected keypoints to
reconstruct the point cloud and camera-pose for initial two-view
reconstruction. We then perform depth-regularized optimization to register new
images and triangulate the new points during incremental reconstruction. To
comprehensively evaluate our approach, we introduce a new dataset (StudioSfM)
consisting of 130 shots with 21K frames from 15 studio-produced videos that are
manually annotated by a professional CG studio. We demonstrate that our
approach: (a) significantly improves the quality of 3-D reconstruction for our
small-parallax setting, (b) does not cause any degradation for data with
large-parallax, and (c) maintains the generalizability and scalability of
geometry-based sparse SfM. Our dataset can be obtained at
https://github.com/amazon-research/small-baseline-camera-tracking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emphasis on the Minimization of False Negatives or False Positives in Binary Classification. (arXiv:2204.02526v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02526">
<div class="article-summary-box-inner">
<span><p>The minimization of specific cases in binary classification, such as false
negatives or false positives, grows increasingly important as humans begin to
implement more machine learning into current products. While there are a few
methods to put a bias towards the reduction of specific cases, these methods
aren't very effective, hence their minimal use in models. To this end, a new
method is introduced to reduce the False Negatives or False positives without
drastically changing the overall performance or F1 score of the model. This
method involving the careful change to the real value of the input after
pre-training the model. Presenting the results of this method being applied on
various datasets, some being more complex than others. Through experimentation
on multiple model architectures on these datasets, the best model was found. In
all the models, an increase in the recall or precision, minimization of False
Negatives or False Positives respectively, was shown without a large drop in F1
score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Motion with Multi-Modal Features for Text-Based Video Segmentation. (arXiv:2204.02547v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02547">
<div class="article-summary-box-inner">
<span><p>Text-based video segmentation aims to segment the target object in a video
based on a describing sentence. Incorporating motion information from optical
flow maps with appearance and linguistic modalities is crucial yet has been
largely ignored by previous work. In this paper, we design a method to fuse and
align appearance, motion, and linguistic features to achieve accurate
segmentation. Specifically, we propose a multi-modal video transformer, which
can fuse and aggregate multi-modal and temporal features between frames.
Furthermore, we design a language-guided feature fusion module to progressively
fuse appearance and motion features in each feature level with guidance from
linguistic features. Finally, a multi-modal alignment loss is proposed to
alleviate the semantic gap between features from different modalities.
Extensive experiments on A2D Sentences and J-HMDB Sentences verify the
performance and the generalization ability of our method compared to the
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Style-Hallucinated Dual Consistency Learning for Domain Generalized Semantic Segmentation. (arXiv:2204.02548v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02548">
<div class="article-summary-box-inner">
<span><p>In this paper, we study the task of synthetic-to-real domain generalized
semantic segmentation, which aims to learn a model that is robust to unseen
real-world scenes using only synthetic data. The large domain shift between
synthetic and real-world data, including the limited source environmental
variations and the large distribution gap between synthetic and real-world
data, significantly hinders the model performance on unseen real-world scenes.
In this work, we propose the Style-HAllucinated Dual consistEncy learning
(SHADE) framework to handle such domain shift. Specifically, SHADE is
constructed based on two consistency constraints, Style Consistency (SC) and
Retrospection Consistency (RC). SC enriches the source situations and
encourages the model to learn consistent representation across
style-diversified samples. RC leverages real-world knowledge to prevent the
model from overfitting to synthetic data and thus largely keeps the
representation consistent between the synthetic and real-world models.
Furthermore, we present a novel style hallucination module (SHM) to generate
style-diversified samples that are essential to consistency learning. SHM
selects basis styles from the source distribution, enabling the model to
dynamically generate diverse and realistic samples during training. Experiments
show that our SHADE yields significant improvement and outperforms
state-of-the-art methods by 5.07% and 8.35% on the average mIoU of three
real-world datasets on single- and multi-source settings respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RODD: A Self-Supervised Approach for Robust Out-of-Distribution Detection. (arXiv:2204.02553v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02553">
<div class="article-summary-box-inner">
<span><p>Recent studies have addressed the concern of detecting and rejecting the
out-of-distribution (OOD) samples as a major challenge in the safe deployment
of deep learning (DL) models. It is desired that the DL model should only be
confident about the in-distribution (ID) data which reinforces the driving
principle of the OOD detection. In this paper, we propose a simple yet
effective generalized OOD detection method independent of out-of-distribution
datasets. Our approach relies on self-supervised feature learning of the
training samples, where the embeddings lie on a compact low-dimensional space.
Motivated by the recent studies that show self-supervised adversarial
contrastive learning helps robustify the model, we empirically show that a
pre-trained model with self-supervised contrastive learning yields a better
model for uni-dimensional feature learning in the latent space. The method
proposed in this work referred to as \texttt{RODD}, outperforms SOTA detection
performance on an extensive suite of benchmark datasets on OOD detection tasks.
On the CIFAR-100 benchmarks, \texttt{RODD} achieves a 26.97 $\%$ lower
false-positive rate (FPR@95) compared to SOTA methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MixFormer: Mixing Features across Windows and Dimensions. (arXiv:2204.02557v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02557">
<div class="article-summary-box-inner">
<span><p>While local-window self-attention performs notably in vision tasks, it
suffers from limited receptive field and weak modeling capability issues. This
is mainly because it performs self-attention within non-overlapped windows and
shares weights on the channel dimension. We propose MixFormer to find a
solution. First, we combine local-window self-attention with depth-wise
convolution in a parallel design, modeling cross-window connections to enlarge
the receptive fields. Second, we propose bi-directional interactions across
branches to provide complementary clues in the channel and spatial dimensions.
These two designs are integrated to achieve efficient feature mixing among
windows and dimensions. Our MixFormer provides competitive results on image
classification with EfficientNet and shows better results than RegNet and Swin
Transformer. Performance in downstream tasks outperforms its alternatives by
significant margins with less computational costs in 5 dense prediction tasks
on MS COCO, ADE20k, and LVIS. Code is available at
\url{https://github.com/PaddlePaddle/PaddleClas}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gait Recognition in the Wild with Dense 3D Representations and A Benchmark. (arXiv:2204.02569v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02569">
<div class="article-summary-box-inner">
<span><p>Existing studies for gait recognition are dominated by 2D representations
like the silhouette or skeleton of the human body in constrained scenes.
However, humans live and walk in the unconstrained 3D space, so projecting the
3D human body onto the 2D plane will discard a lot of crucial information like
the viewpoint, shape, and dynamics for gait recognition. Therefore, this paper
aims to explore dense 3D representations for gait recognition in the wild,
which is a practical yet neglected problem. In particular, we propose a novel
framework to explore the 3D Skinned Multi-Person Linear (SMPL) model of the
human body for gait recognition, named SMPLGait. Our framework has two
elaborately-designed branches of which one extracts appearance features from
silhouettes, the other learns knowledge of 3D viewpoints and shapes from the 3D
SMPL model. In addition, due to the lack of suitable datasets, we build the
first large-scale 3D representation-based gait recognition dataset, named
Gait3D. It contains 4,000 subjects and over 25,000 sequences extracted from 39
cameras in an unconstrained indoor scene. More importantly, it provides 3D SMPL
models recovered from video frames which can provide dense 3D information of
body shape, viewpoint, and dynamics. Based on Gait3D, we comprehensively
compare our method with existing gait recognition approaches, which reflects
the superior performance of our framework and the potential of 3D
representations for gait recognition in the wild. The code and dataset are
available at https://gait3d.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting key Soccer match events to create highlights using Computer Vision. (arXiv:2204.02573v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02573">
<div class="article-summary-box-inner">
<span><p>The research and data science community has been fascinated with the
development of automatic systems for the detection of key events in a video.
Special attention in this field is given to sports video analytics which could
help in identifying key events during a match and help in preparing a strategy
for the games going forward. For this paper, we have chosen Football (soccer)
as a sport where we would want to create highlights for a given match video,
through a computer vision model that aims to identify important events in a
Soccer match to create highlights of the match. We built the models based on
Faster RCNN and YoloV5 architectures and noticed that for the amount of data we
used for training Faster RCNN did better than YoloV5 in detecting the events in
the match though it was much slower. Within Faster RCNN using ResNet50 as a
base model gave a better class accuracy of 95.5% as compared to 92% with VGG16
as base model completely outperforming YoloV5 for our training dataset. We
tested with an original video of size 23 minutes and our model could reduce it
to 4:50 minutes of highlights capturing almost all important events in the
match.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FocalClick: Towards Practical Interactive Image Segmentation. (arXiv:2204.02574v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02574">
<div class="article-summary-box-inner">
<span><p>Interactive segmentation allows users to extract target masks by making
positive/negative clicks. Although explored by many previous works, there is
still a gap between academic approaches and industrial needs: first, existing
models are not efficient enough to work on low power devices; second, they
perform poorly when used to refine preexisting masks as they could not avoid
destroying the correct part. FocalClick solves both issues at once by
predicting and updating the mask in localized areas. For higher efficiency, we
decompose the slow prediction on the entire image into two fast inferences on
small crops: a coarse segmentation on the Target Crop, and a local refinement
on the Focus Crop. To make the model work with preexisting masks, we formulate
a sub-task termed Interactive Mask Correction, and propose Progressive Merge as
the solution. Progressive Merge exploits morphological information to decide
where to preserve and where to update, enabling users to refine any preexisting
mask effectively. FocalClick achieves competitive results against SOTA methods
with significantly smaller FLOPs. It also shows significant superiority when
making corrections on preexisting masks. Code and data will be released at
github.com/XavierCHEN34/ClickSEG
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Banana Sub-Family Classification and Quality Prediction using Computer Vision. (arXiv:2204.02581v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02581">
<div class="article-summary-box-inner">
<span><p>India is the second largest producer of fruits and vegetables in the world,
and one of the largest consumers of fruits like Banana, Papaya and Mangoes
through retail and ecommerce giants like BigBasket, Grofers and Amazon Fresh.
However, adoption of technology in supply chain and retail stores is still low
and there is a great potential to adopt computer-vision based technology for
identification and classification of fruits. We have chosen banana fruit to
build a computer vision based model to carry out the following three use-cases
(a) Identify Banana from a given image (b) Determine sub-family or variety of
Banana (c) Determine the quality of Banana. Successful execution of these
use-cases using computer-vision model would greatly help with overall inventory
management automation, quality control, quick and efficient weighing and
billing which all are manual labor intensive currently. In this work, we
suggest a machine learning pipeline that combines the ideas of CNNs, transfer
learning, and data augmentation towards improving Banana fruit sub family and
quality image classification. We have built a basic CNN and then went on to
tune a MobileNet Banana classification model using a combination of
self-curated and publicly-available dataset of 3064 images. The results show an
overall 93.4% and 100% accuracy for sub-family/variety and for quality test
classifications respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SqueezeNeRF: Further factorized FastNeRF for memory-efficient inference. (arXiv:2204.02585v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02585">
<div class="article-summary-box-inner">
<span><p>Neural Radiance Fields (NeRF) has emerged as the state-of-the-art method for
novel view generation of complex scenes, but is very slow during inference.
Recently, there have been multiple works on speeding up NeRF inference, but the
state of the art methods for real-time NeRF inference rely on caching the
neural network output, which occupies several giga-bytes of disk space that
limits their real-world applicability. As caching the neural network of
original NeRF network is not feasible, Garbin et.al. proposed "FastNeRF" which
factorizes the problem into 2 sub-networks - one which depends only on the 3D
coordinate of a sample point and one which depends only on the 2D camera
viewing direction. Although this factorization enables them to reduce the cache
size and perform inference at over 200 frames per second, the memory overhead
is still substantial. In this work, we propose SqueezeNeRF, which is more than
60 times memory-efficient than the sparse cache of FastNeRF and is still able
to render at more than 190 frames per second on a high spec GPU during
inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Anticipate Future with Dynamic Context Removal. (arXiv:2204.02587v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02587">
<div class="article-summary-box-inner">
<span><p>Anticipating future events is an essential feature for intelligent systems
and embodied AI. However, compared to the traditional recognition task, the
uncertainty of future and reasoning ability requirement make the anticipation
task very challenging and far beyond solved. In this filed, previous methods
usually care more about the model architecture design or but few attention has
been put on how to train an anticipation model with a proper learning policy.
To this end, in this work, we propose a novel training scheme called Dynamic
Context Removal (DCR), which dynamically schedules the visibility of observed
future in the learning procedure. It follows the human-like curriculum learning
process, i.e., gradually removing the event context to increase the
anticipation difficulty till satisfying the final anticipation target. Our
learning scheme is plug-and-play and easy to integrate any reasoning model
including transformer and LSTM, with advantages in both effectiveness and
efficiency. In extensive experiments, the proposed method achieves
state-of-the-art on four widely-used benchmarks. Our code and models are
publicly released at https://github.com/AllenXuuu/DCR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextual Attention Mechanism, SRGAN Based Inpainting System for Eliminating Interruptions from Images. (arXiv:2204.02591v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02591">
<div class="article-summary-box-inner">
<span><p>The new alternative is to use deep learning to inpaint any image by utilizing
image classification and computer vision techniques. In general, image
inpainting is a task of recreating or reconstructing any broken image which
could be a photograph or oil/acrylic painting. With the advancement in the
field of Artificial Intelligence, this topic has become popular among AI
enthusiasts. With our approach, we propose an initial end-to-end pipeline for
inpainting images using a complete Machine Learning approach instead of a
conventional application-based approach. We first use the YOLO model to
automatically identify and localize the object we wish to remove from the
image. Using the result obtained from the model we can generate a mask for the
same. After this, we provide the masked image and original image to the GAN
model which uses the Contextual Attention method to fill in the region. It
consists of two generator networks and two discriminator networks and is also
called a coarse-to-fine network structure. The two generators use fully
convolutional networks while the global discriminator gets hold of the entire
image as input while the local discriminator gets the grip of the filled region
as input. The contextual Attention mechanism is proposed to effectively borrow
the neighbor information from distant spatial locations for reconstructing the
missing pixels. The third part of our implementation uses SRGAN to resolve the
inpainted image back to its original size. Our work is inspired by the paper
Free-Form Image Inpainting with Gated Convolution and Generative Image
Inpainting with Contextual Attention.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-Grained Predicates Learning for Scene Graph Generation. (arXiv:2204.02597v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02597">
<div class="article-summary-box-inner">
<span><p>The performance of current Scene Graph Generation models is severely hampered
by some hard-to-distinguish predicates, e.g., "woman-on/standing on/walking
on-beach" or "woman-near/looking at/in front of-child". While general SGG
models are prone to predict head predicates and existing re-balancing
strategies prefer tail categories, none of them can appropriately handle these
hard-to-distinguish predicates. To tackle this issue, inspired by fine-grained
image classification, which focuses on differentiating among
hard-to-distinguish object classes, we propose a method named Fine-Grained
Predicates Learning (FGPL) which aims at differentiating among
hard-to-distinguish predicates for Scene Graph Generation task. Specifically,
we first introduce a Predicate Lattice that helps SGG models to figure out
fine-grained predicate pairs. Then, utilizing the Predicate Lattice, we propose
a Category Discriminating Loss and an Entity Discriminating Loss, which both
contribute to distinguishing fine-grained predicates while maintaining learned
discriminatory power over recognizable ones. The proposed model-agnostic
strategy significantly boosts the performances of three benchmark models
(Transformer, VCTree, and Motif) by 22.8\%, 24.1\% and 21.7\% of Mean Recall
(mR@100) on the Predicate Classification sub-task, respectively. Our model also
outperforms state-of-the-art methods by a large margin (i.e., 6.1\%, 4.6\%, and
3.2\% of Mean Recall (mR@100)) on the Visual Genome dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Face recognition in a transformed domain. (arXiv:2204.02608v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02608">
<div class="article-summary-box-inner">
<span><p>This paper proposes the use of a discrete cosine transform (DCT) instead of
the eigenfaces method (Karhunen-Loeve Transform) for biometric identification
based on frontal face images. Experimental results show better recognition
accuracies and reduced computational burden. This paper includes results with
different classifiers and a combination of them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cloning Outfits from Real-World Images to 3D Characters for Generalizable Person Re-Identification. (arXiv:2204.02611v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02611">
<div class="article-summary-box-inner">
<span><p>Recently, large-scale synthetic datasets are shown to be very useful for
generalizable person re-identification. However, synthesized persons in
existing datasets are mostly cartoon-like and in random dress collocation,
which limits their performance. To address this, in this work, an automatic
approach is proposed to directly clone the whole outfits from real-world person
images to virtual 3D characters, such that any virtual person thus created will
appear very similar to its real-world counterpart. Specifically, based on UV
texture mapping, two cloning methods are designed, namely registered clothes
mapping and homogeneous cloth expansion. Given clothes keypoints detected on
person images and labeled on regular UV maps with clear clothes structures,
registered mapping applies perspective homography to warp real-world clothes to
the counterparts on the UV map. As for invisible clothes parts and irregular UV
maps, homogeneous expansion segments a homogeneous area on clothes as a
realistic cloth pattern or cell, and expand the cell to fill the UV map.
Furthermore, a similarity-diversity expansion strategy is proposed, by
clustering person images, sampling images per cluster, and cloning outfits for
3D character generation. This way, virtual persons can be scaled up densely in
visual similarity to challenge model learning, and diversely in population to
enrich sample distribution. Finally, by rendering the cloned characters in
Unity3D scenes, a more realistic virtual dataset called ClonedPerson is
created, with 5,621 identities and 887,766 images. Experimental results show
that the model trained on ClonedPerson has a better generalization performance,
superior to that trained on other popular real-world and synthetic person
re-identification datasets. The ClonedPerson project is available at
https://github.com/Yanan-Wang-cs/ClonedPerson.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Robust Adaptive Object Detection under Noisy Annotations. (arXiv:2204.02620v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02620">
<div class="article-summary-box-inner">
<span><p>Domain Adaptive Object Detection (DAOD) models a joint distribution of images
and labels from an annotated source domain and learns a domain-invariant
transformation to estimate the target labels with the given target domain
images. Existing methods assume that the source domain labels are completely
clean, yet large-scale datasets often contain error-prone annotations due to
instance ambiguity, which may lead to a biased source distribution and severely
degrade the performance of the domain adaptive detector de facto. In this
paper, we represent the first effort to formulate noisy DAOD and propose a
Noise Latent Transferability Exploration (NLTE) framework to address this
issue. It is featured with 1) Potential Instance Mining (PIM), which leverages
eligible proposals to recapture the miss-annotated instances from the
background; 2) Morphable Graph Relation Module (MGRM), which models the
adaptation feasibility and transition probability of noisy samples with
relation matrices; 3) Entropy-Aware Gradient Reconcilement (EAGR), which
incorporates the semantic information into the discrimination process and
enforces the gradients provided by noisy and clean samples to be consistent
towards learning domain-invariant representations. A thorough evaluation on
benchmark DAOD datasets with noisy source annotations validates the
effectiveness of NLTE. In particular, NLTE improves the mAP by 8.4\% under 60\%
corrupted annotations and even approaches the ideal upper bound of training on
a clean source dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IterVM: Iterative Vision Modeling Module for Scene Text Recognition. (arXiv:2204.02630v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02630">
<div class="article-summary-box-inner">
<span><p>Scene text recognition (STR) is a challenging problem due to the imperfect
imagery conditions in natural images. State-of-the-art methods utilize both
visual cues and linguistic knowledge to tackle this challenging problem.
Specifically, they propose iterative language modeling module (IterLM) to
repeatedly refine the output sequence from the visual modeling module (VM).
Though achieving promising results, the vision modeling module has become the
performance bottleneck of these methods. In this paper, we newly propose
iterative vision modeling module (IterVM) to further improve the STR accuracy.
Specifically, the first VM directly extracts multi-level features from the
input image, and the following VMs re-extract multi-level features from the
input image and fuse them with the high-level (i.e., the most semantic one)
feature extracted by the previous VM. By combining the proposed IterVM with
iterative language modeling module, we further propose a powerful scene text
recognizer called IterNet. Extensive experiments demonstrate that the proposed
IterVM can significantly improve the scene text recognition accuracy,
especially on low-quality scene text images. Moreover, the proposed scene text
recognizer IterNet achieves new state-of-the-art results on several public
benchmarks. Codes will be available at https://github.com/VDIGPKU/IterNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Super-resolved multi-temporal segmentation with deep permutation-invariant networks. (arXiv:2204.02631v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02631">
<div class="article-summary-box-inner">
<span><p>Multi-image super-resolution from multi-temporal satellite acquisitions of a
scene has recently enjoyed great success thanks to new deep learning models. In
this paper, we go beyond classic image reconstruction at a higher resolution by
studying a super-resolved inference problem, namely semantic segmentation at a
spatial resolution higher than the one of sensing platform. We expand upon
recently proposed models exploiting temporal permutation invariance with a
multi-resolution fusion module able to infer the rich semantic information
needed by the segmentation task. The model presented in this paper has recently
won the AI4EO challenge on Enhanced Sentinel 2 Agriculture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Swiss Army Knife for Image-to-Image Translation: Multi-Task Diffusion Models. (arXiv:2204.02641v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02641">
<div class="article-summary-box-inner">
<span><p>Recently, diffusion models were applied to a wide range of image analysis
tasks. We build on a method for image-to-image translation using denoising
diffusion implicit models and include a regression problem and a segmentation
problem for guiding the image generation to the desired output. The main
advantage of our approach is that the guidance during the denoising process is
done by an external gradient. Consequently, the diffusion model does not need
to be retrained for the different tasks on the same dataset. We apply our
method to simulate the aging process on facial photos using a regression task,
as well as on a brain magnetic resonance (MR) imaging dataset for the
simulation of brain tumor growth. Furthermore, we use a segmentation model to
inpaint tumors at the desired location in healthy slices of brain MR images. We
achieve convincing results for all problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CAIPI in Practice: Towards Explainable Interactive Medical Image Classification. (arXiv:2204.02661v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02661">
<div class="article-summary-box-inner">
<span><p>Would you trust physicians if they cannot explain their decisions to you?
Medical diagnostics using machine learning gained enormously in importance
within the last decade. However, without further enhancements many
state-of-the-art machine learning methods are not suitable for medical
application. The most important reasons are insufficient data set quality and
the black-box behavior of machine learning algorithms such as Deep Learning
models. Consequently, end-users cannot correct the model's decisions and the
corresponding explanations. The latter is crucial for the trustworthiness of
machine learning in the medical domain. The research field explainable
interactive machine learning searches for methods that address both
shortcomings. This paper extends the explainable and interactive CAIPI
algorithm and provides an interface to simplify human-in-the-loop approaches
for image classification. The interface enables the end-user (1) to investigate
and (2) to correct the model's prediction and explanation, and (3) to influence
the data set quality. After CAIPI optimization with only a single
counterexample per iteration, the model achieves an accuracy of $97.48\%$ on
the Medical MNIST and $95.02\%$ on the Fashion MNIST. This accuracy is
approximately equal to state-of-the-art Deep Learning optimization procedures.
Besides, CAIPI reduces the labeling effort by approximately $80\%$.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards An End-to-End Framework for Flow-Guided Video Inpainting. (arXiv:2204.02663v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02663">
<div class="article-summary-box-inner">
<span><p>Optical flow, which captures motion information across frames, is exploited
in recent video inpainting methods through propagating pixels along its
trajectories. However, the hand-crafted flow-based processes in these methods
are applied separately to form the whole inpainting pipeline. Thus, these
methods are less efficient and rely heavily on the intermediate results from
earlier stages. In this paper, we propose an End-to-End framework for
Flow-Guided Video Inpainting (E$^2$FGVI) through elaborately designed three
trainable modules, namely, flow completion, feature propagation, and content
hallucination modules. The three modules correspond with the three stages of
previous flow-based methods but can be jointly optimized, leading to a more
efficient and effective inpainting process. Experimental results demonstrate
that the proposed method outperforms state-of-the-art methods both
qualitatively and quantitatively and shows promising efficiency. The code is
available at https://github.com/MCG-NKU/E2FGVI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Faster-TAD: Towards Temporal Action Detection with Proposal Generation and Classification in a Unified Network. (arXiv:2204.02674v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02674">
<div class="article-summary-box-inner">
<span><p>Temporal action detection (TAD) aims to detect the semantic labels and
boundaries of action instances in untrimmed videos. Current mainstream
approaches are multi-step solutions, which fall short in efficiency and
flexibility. In this paper, we propose a unified network for TAD, termed
Faster-TAD, by re-purposing a Faster-RCNN like architecture. To tackle the
unique difficulty in TAD, we make important improvements over the original
framework. We propose a new Context-Adaptive Proposal Module and an innovative
Fake-Proposal Generation Block. What's more, we use atomic action features to
improve the performance. Faster-TAD simplifies the pipeline of TAD and gets
remarkable performance on lots of benchmarks, i.e., ActivityNet-1.3 (40.01%
mAP), HACS Segments (38.39% mAP), SoccerNet-Action Spotting (54.09% mAP). It
outperforms existing single-network detector by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rolling Colors: Adversarial Laser Exploits against Traffic Light Recognition. (arXiv:2204.02675v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02675">
<div class="article-summary-box-inner">
<span><p>Traffic light recognition is essential for fully autonomous driving in urban
areas. In this paper, we investigate the feasibility of fooling traffic light
recognition mechanisms by shedding laser interference on the camera. By
exploiting the rolling shutter of CMOS sensors, we manage to inject a color
stripe overlapped on the traffic light in the image, which can cause a red
light to be recognized as a green light or vice versa. To increase the success
rate, we design an optimization method to search for effective laser parameters
based on empirical models of laser interference. Our evaluation in emulated and
real-world setups on 2 state-of-the-art recognition systems and 5 cameras
reports a maximum success rate of 30% and 86.25% for Red-to-Green and
Green-to-Red attacks. We observe that the attack is effective in continuous
frames from more than 40 meters away against a moving vehicle, which may cause
end-to-end impacts on self-driving such as running a red light or emergency
stop. To mitigate the threat, we propose redesigning the rolling shutter
mechanism.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PP-LiteSeg: A Superior Real-Time Semantic Segmentation Model. (arXiv:2204.02681v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02681">
<div class="article-summary-box-inner">
<span><p>Real-world applications have high demands for semantic segmentation methods.
Although semantic segmentation has made remarkable leap-forwards with deep
learning, the performance of real-time methods is not satisfactory. In this
work, we propose PP-LiteSeg, a novel lightweight model for the real-time
semantic segmentation task. Specifically, we present a Flexible and Lightweight
Decoder (FLD) to reduce computation overhead of previous decoder. To strengthen
feature representations, we propose a Unified Attention Fusion Module (UAFM),
which takes advantage of spatial and channel attention to produce a weight and
then fuses the input features with the weight. Moreover, a Simple Pyramid
Pooling Module (SPPM) is proposed to aggregate global context with low
computation cost. Extensive evaluations demonstrate that PP-LiteSeg achieves a
superior trade-off between accuracy and speed compared to other methods. On the
Cityscapes test set, PP-LiteSeg achieves 72.0% mIoU/273.6 FPS and 77.5%
mIoU/102.6 FPS on NVIDIA GTX 1080Ti. Source code and models are available at
PaddleSeg: https://github.com/PaddlePaddle/PaddleSeg.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain-Agnostic Prior for Transfer Semantic Segmentation. (arXiv:2204.02684v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02684">
<div class="article-summary-box-inner">
<span><p>Unsupervised domain adaptation (UDA) is an important topic in the computer
vision community. The key difficulty lies in defining a common property between
the source and target domains so that the source-domain features can align with
the target-domain semantics. In this paper, we present a simple and effective
mechanism that regularizes cross-domain representation learning with a
domain-agnostic prior (DAP) that constrains the features extracted from source
and target domains to align with a domain-agnostic space. In practice, this is
easily implemented as an extra loss term that requires a little extra costs. In
the standard evaluation protocol of transferring synthesized data to real data,
we validate the effectiveness of different types of DAP, especially that
borrowed from a text embedding model that shows favorable performance beyond
the state-of-the-art UDA approaches in terms of segmentation accuracy. Our
research reveals that UDA benefits much from better proxies, possibly from
other data modalities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SEAL: A Large-scale Video Dataset of Multi-grained Spatio-temporally Action Localization. (arXiv:2204.02688v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02688">
<div class="article-summary-box-inner">
<span><p>In spite of many dataset efforts for human action recognition, current
computer vision algorithms are still limited to coarse-grained spatial and
temporal annotations among human daily life. In this paper, we introduce a
novel large-scale video dataset dubbed SEAL for multi-grained Spatio-tEmporal
Action Localization. SEAL consists of two kinds of annotations, SEAL Tubes and
SEAL Clips. We observe that atomic actions can be combined into many complex
activities. SEAL Tubes provide both atomic action and complex activity
annotations in tubelet level, producing 49.6k atomic actions spanning 172
action categories and 17.7k complex activities spanning 200 activity
categories. SEAL Clips localizes atomic actions in space during two-second
clips, producing 510.4k action labels with multiple labels per person.
Extensive experimental results show that SEAL significantly helps to advance
video understanding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Aesthetic Text Logo Synthesis via Content-aware Layout Inferring. (arXiv:2204.02701v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02701">
<div class="article-summary-box-inner">
<span><p>Text logo design heavily relies on the creativity and expertise of
professional designers, in which arranging element layouts is one of the most
important procedures. However, few attention has been paid to this task which
needs to take many factors (e.g., fonts, linguistics, topics, etc.) into
consideration. In this paper, we propose a content-aware layout generation
network which takes glyph images and their corresponding text as input and
synthesizes aesthetic layouts for them automatically. Specifically, we develop
a dual-discriminator module, including a sequence discriminator and an image
discriminator, to evaluate both the character placing trajectories and rendered
shapes of synthesized text logos, respectively. Furthermore, we fuse the
information of linguistics from texts and visual semantics from glyphs to guide
layout prediction, which both play important roles in professional layout
design. To train and evaluate our approach, we construct a dataset named as
TextLogo3K, consisting of about 3,500 text logo images and their pixel-level
annotations. Experimental studies on this dataset demonstrate the effectiveness
of our approach for synthesizing visually-pleasing text logos and verify its
superiority against the state of the art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Georeferencing of Photovoltaic Modules from Aerial Infrared Videos using Structure-from-Motion. (arXiv:2204.02733v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02733">
<div class="article-summary-box-inner">
<span><p>To identify abnormal photovoltaic (PV) modules in large-scale PV plants
economically, drone-mounted infrared (IR) cameras and automated video
processing algorithms are frequently used. While most related works focus on
the detection of abnormal modules, little has been done to automatically
localize those modules within the plant. In this work, we use incremental
structure-from-motion to automatically obtain geocoordinates of all PV modules
in a plant based on visual cues and the measured GPS trajectory of the drone.
In addition, we extract multiple IR images of each PV module. Using our method,
we successfully map 99.3 % of the 35084 modules in four large-scale and one
rooftop plant and extract over 2.2 million module images. As compared to our
previous work, extraction misses 18 times less modules (one in 140 modules as
compared to one in eight). Furthermore, two or three plant rows can be
processed simultaneously, increasing module throughput and reducing flight
duration by a factor of 2.1 and 3.7, respectively. Comparison with an accurate
orthophoto of one of the large-scale plants yields a root mean square error of
the estimated module geocoordinates of 5.87 m and a relative error within each
plant row of 0.22 m to 0.82 m. Finally, we use the module geocoordinates and
extracted IR images to visualize distributions of module temperatures and
anomaly predictions of a deep learning classifier on a map. While the
temperature distribution helps to identify disconnected strings, we also find
that its detection accuracy for module anomalies reaches, or even exceeds, that
of a deep learning classifier for seven out of ten common anomaly types. The
software is published at https://github.com/LukasBommes/PV-Hawk.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Masking Adversarial Damage: Finding Adversarial Saliency for Robust and Sparse Network. (arXiv:2204.02738v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02738">
<div class="article-summary-box-inner">
<span><p>Adversarial examples provoke weak reliability and potential security issues
in deep neural networks. Although adversarial training has been widely studied
to improve adversarial robustness, it works in an over-parameterized regime and
requires high computations and large memory budgets. To bridge adversarial
robustness and model compression, we propose a novel adversarial pruning
method, Masking Adversarial Damage (MAD) that employs second-order information
of adversarial loss. By using it, we can accurately estimate adversarial
saliency for model parameters and determine which parameters can be pruned
without weakening adversarial robustness. Furthermore, we reveal that model
parameters of initial layer are highly sensitive to the adversarial examples
and show that compressed feature representation retains semantic information
for the target objects. Through extensive experiments on three public datasets,
we demonstrate that MAD effectively prunes adversarially trained networks
without loosing adversarial robustness and shows better performance than
previous adversarial pruning methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Universal Representations: A Unified Look at Multiple Task and Domain Learning. (arXiv:2204.02744v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02744">
<div class="article-summary-box-inner">
<span><p>We propose a unified look at jointly learning multiple vision tasks and
visual domains through universal representations, a single deep neural network.
Learning multiple problems simultaneously involves minimizing a weighted sum of
multiple loss functions with different magnitudes and characteristics and thus
results in unbalanced state of one loss dominating the optimization and poor
results compared to learning a separate model for each problem. To this end, we
propose distilling knowledge of multiple task/domain-specific networks into a
single deep neural network after aligning its representations with the
task/domain-specific ones through small capacity adapters. We rigorously show
that universal representations achieve state-of-the-art performances in
learning of multiple dense prediction problems in NYU-v2 and Cityscapes,
multiple image classification problems from diverse domains in Visual Decathlon
Dataset and cross-domain few-shot learning in MetaDataset. Finally we also
conduct multiple analysis through ablation and qualitative studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BFRnet: A deep learning-based MR background field removal method for QSM of the brain containing significant pathological susceptibility sources. (arXiv:2204.02760v1 [q-bio.QM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02760">
<div class="article-summary-box-inner">
<span><p>Introduction: Background field removal (BFR) is a critical step required for
successful quantitative susceptibility mapping (QSM). However, eliminating the
background field in brains containing significant susceptibility sources, such
as intracranial hemorrhages, is challenging due to the relatively large scale
of the field induced by these pathological susceptibility sources. Method: This
study proposes a new deep learning-based method, BFRnet, to remove background
field in healthy and hemorrhagic subjects. The network is built with the
dual-frequency octave convolutions on the U-net architecture, trained with
synthetic field maps containing significant susceptibility sources. The BFRnet
method is compared with three conventional BFR methods and one previous deep
learning method using simulated and in vivo brains from 4 healthy and 2
hemorrhagic subjects. Robustness against acquisition field-of-view (FOV)
orientation and brain masking are also investigated. Results: For both
simulation and in vivo experiments, BFRnet led to the best visually appealing
results in the local field and QSM results with the minimum contrast loss and
the most accurate hemorrhage susceptibility measurements among all five
methods. In addition, BFRnet produced the most consistent local field and
susceptibility maps between different sizes of brain masks, while conventional
methods depend drastically on precise brain extraction and further brain edge
erosions. It is also observed that BFRnet performed the best among all BFR
methods for acquisition FOVs oblique to the main magnetic field. Conclusion:
The proposed BFRnet improved the accuracy of local field reconstruction in the
hemorrhagic subjects compared with conventional BFR algorithms. The BFRnet
method was effective for acquisitions of titled orientations and retained whole
brains without edge erosion as often required by traditional BFR methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-DRDNet Semi-supervised Detail-recovery Image Deraining Network via Unpaired Contrastive Learning. (arXiv:2204.02772v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02772">
<div class="article-summary-box-inner">
<span><p>The intricacy of rainy image contents often leads cutting-edge deraining
models to image degradation including remnant rain, wrongly-removed details,
and distorted appearance. Such degradation is further exacerbated when applying
the models trained on synthetic data to real-world rainy images. We raise an
intriguing question -- if leveraging both accessible unpaired clean/rainy yet
real-world images and additional detail repair guidance, can improve the
generalization ability of a deraining model? To answer it, we propose a
semi-supervised detail-recovery image deraining network (termed as
Semi-DRDNet). Semi-DRDNet consists of three branches: 1) for removing rain
streaks without remnants, we present a \textit{squeeze-and-excitation}
(SE)-based rain residual network; 2) for encouraging the lost details to
return, we construct a \textit{structure detail context aggregation}
(SDCAB)-based detail repair network; to our knowledge, this is the first time;
and 3) for bridging the domain gap, we develop a novel contrastive
regularization network to learn from unpaired positive (clean) and negative
(rainy) yet real-world images. As a semi-supervised learning paradigm,
Semi-DRDNet operates smoothly on both synthetic and real-world rainy data in
terms of deraining robustness and detail accuracy. Comparisons on four datasets
show clear visual and numerical improvements of our Semi-DRDNet over thirteen
state-of-the-arts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D face reconstruction with dense landmarks. (arXiv:2204.02776v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02776">
<div class="article-summary-box-inner">
<span><p>Landmarks often play a key role in face analysis, but many aspects of
identity or expression cannot be represented by sparse landmarks alone. Thus,
in order to reconstruct faces more accurately, landmarks are often combined
with additional signals like depth images or techniques like differentiable
rendering. Can we keep things simple by just using more landmarks? In answer,
we present the first method that accurately predicts 10x as many landmarks as
usual, covering the whole head, including the eyes and teeth. This is
accomplished using synthetic training data, which guarantees perfect landmark
annotations. By fitting a morphable model to these dense landmarks, we achieve
state-of-the-art results for monocular 3D face reconstruction in the wild. We
show that dense landmarks are an ideal signal for integrating face shape
information across frames by demonstrating accurate and expressive facial
performance capture in both monocular and multi-view scenarios. This approach
is also highly efficient: we can predict dense landmarks and fit our 3D face
model at over 150FPS on a single CPU thread.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Dempster-Shafer approach to trustworthy AI with application to fetal brain MRI segmentation. (arXiv:2204.02779v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02779">
<div class="article-summary-box-inner">
<span><p>Deep learning models for medical image segmentation can fail unexpectedly and
spectacularly for pathological cases and for images acquired at different
centers than those used for training, with labeling errors that violate expert
knowledge about the anatomy and the intensity distribution of the regions to be
segmented. Such errors undermine the trustworthiness of deep learning models
developed for medical image segmentation. Mechanisms with a fallback method for
detecting and correcting such failures are essential for safely translating
this technology into clinics and are likely to be a requirement of future
regulations on artificial intelligence (AI). Here, we propose a principled
trustworthy AI theoretical framework and a practical system that can augment
any backbone AI system using a fallback method and a fail-safe mechanism based
on Dempster-Shafer theory. Our approach relies on an actionable definition of
trustworthy AI. Our method automatically discards the voxel-level labeling
predicted by the backbone AI that are likely to violate expert knowledge and
relies on a fallback atlas-based segmentation method for those voxels. We
demonstrate the effectiveness of the proposed trustworthy AI approach on the
largest reported annotated dataset of fetal T2w MRI consisting of 540 manually
annotated fetal brain 3D MRIs with neurotypical or abnormal brain development
and acquired from 13 sources of data across 6 countries. We show that our
trustworthy AI method improves the robustness of a state-of-the-art backbone AI
for fetal brain MRI segmentation on MRIs acquired across various centers and
for fetuses with various brain abnormalities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Implicit Motion-Compensated Network for Unsupervised Video Object Segmentation. (arXiv:2204.02791v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02791">
<div class="article-summary-box-inner">
<span><p>Unsupervised video object segmentation (UVOS) aims at automatically
separating the primary foreground object(s) from the background in a video
sequence. Existing UVOS methods either lack robustness when there are visually
similar surroundings (appearance-based) or suffer from deterioration in the
quality of their predictions because of dynamic background and inaccurate flow
(flow-based). To overcome the limitations, we propose an implicit
motion-compensated network (IMCNet) combining complementary cues
($\textit{i.e.}$, appearance and motion) with aligned motion information from
the adjacent frames to the current frame at the feature level without
estimating optical flows. The proposed IMCNet consists of an affinity computing
module (ACM), an attention propagation module (APM), and a motion compensation
module (MCM). The light-weight ACM extracts commonality between neighboring
input frames based on appearance features. The APM then transmits global
correlation in a top-down manner. Through coarse-to-fine iterative inspiring,
the APM will refine object regions from multiple resolutions so as to
efficiently avoid losing details. Finally, the MCM aligns motion information
from temporally adjacent frames to the current frame which achieves implicit
motion compensation at the feature level. We perform extensive experiments on
$\textit{DAVIS}_{\textit{16}}$ and $\textit{YouTube-Objects}$. Our network
achieves favorable performance while running at a faster speed compared to the
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Transformer-Based Contrastive Learning Approach for Few-Shot Sign Language Recognition. (arXiv:2204.02803v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02803">
<div class="article-summary-box-inner">
<span><p>Sign language recognition from sequences of monocular images or 2D poses is a
challenging field, not only due to the difficulty to infer 3D information from
2D data, but also due to the temporal relationship between the sequences of
information. Additionally, the wide variety of signs and the constant need to
add new ones on production environments makes it infeasible to use traditional
classification techniques. We propose a novel Contrastive Transformer-based
model, which demonstrate to learn rich representations from body key points
sequences, allowing better comparison between vector embedding. This allows us
to apply these techniques to perform one-shot or few-shot tasks, such as
classification and translation. The experiments showed that the model could
generalize well and achieved competitive results for sign classes never seen in
the training process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Expression-preserving face frontalization improves visually assisted speech processing. (arXiv:2204.02810v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02810">
<div class="article-summary-box-inner">
<span><p>Face frontalization consists of synthesizing a frontally-viewed face from an
arbitrarily-viewed one. The main contribution of this paper is a frontalization
methodology that preserves non-rigid facial deformations in order to boost the
performance of visually assisted speech communication. The method alternates
between the estimation of (i)~the rigid transformation (scale, rotation, and
translation) and (ii)~the non-rigid deformation between an arbitrarily-viewed
face and a face model. The method has two important merits: it can deal with
non-Gaussian errors in the data and it incorporates a dynamical face
deformation model. For that purpose, we use the generalized Student
t-distribution in combination with a linear dynamic system in order to account
for both rigid head motions and time-varying facial deformations caused by
speech production. We propose to use the zero-mean normalized cross-correlation
(ZNCC) score to evaluate the ability of the method to preserve facial
expressions. The method is thoroughly evaluated and compared with several state
of the art methods, either based on traditional geometric models or on deep
learning. Moreover, we show that the method, when incorporated into deep
learning pipelines, namely lip reading and speech enhancement, improves word
recognition and speech intelligibilty scores by a considerable margin.
Supplemental material is accessible at
https://team.inria.fr/robotlearn/research/facefrontalization-benchmark/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BMD: A General Class-balanced Multicentric Dynamic Prototype Strategy for Source-free Domain Adaptation. (arXiv:2204.02811v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02811">
<div class="article-summary-box-inner">
<span><p>Source-free Domain Adaptation (SFDA) aims to adapt a pre-trained source model
to the unlabeled target domain without accessing the well-labeled source data,
which is a much more practical setting due to the data privacy, security, and
transmission issues. To make up for the absence of source data, most existing
methods introduced feature prototype based pseudo-labeling strategies to
realize self-training model adaptation. However, feature prototypes are
obtained by instance-level predictions based feature clustering, which is
category-biased and tends to result in noisy labels since the visual domain
gaps between source and target are usually different between categories. In
addition, we found that a monocentric feature prototype may be ineffective to
represent each category and introduce negative transfer, especially for those
hard-transfer data. To address these issues, we propose a general
class-Balanced Multicentric Dynamic prototype (BMD) strategy for the SFDA task.
Specifically, for each target category, we first introduce a global inter-class
balanced sampling strategy to aggregate potential representative target
samples. Then, we design an intra-class multicentric clustering strategy to
achieve more robust and representative prototypes generation. In contrast to
existing strategies that update the pseudo label at a fixed training period, we
further introduce a dynamic pseudo labeling strategy to incorporate network
update information during model adaptation. Extensive experiments show that the
proposed model-agnostic BMD strategy significantly improves representative SFDA
methods to yield new state-of-the-art results, e.g., improving SHOT from 82.9\%
to 85.8\% on VisDA-C and NRC from 52.6\% to 57.0\% on PointDA. The code is
available at https://github.com/ispc-lab/BMD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ShowFace: Coordinated Face Inpainting with Memory-Disentangled Refinement Networks. (arXiv:2204.02824v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02824">
<div class="article-summary-box-inner">
<span><p>Face inpainting aims to complete the corrupted regions of the face images,
which requires coordination between the completed areas and the non-corrupted
areas. Recently, memory-oriented methods illustrate great prospects in the
generation related tasks by introducing an external memory module to improve
image coordination. However, such methods still have limitations in restoring
the consistency and continuity for specificfacial semantic parts. In this
paper, we propose the coarse-to-fine Memory-Disentangled Refinement Networks
(MDRNets) for coordinated face inpainting, in which two collaborative modules
are integrated, Disentangled Memory Module (DMM) and Mask-Region Enhanced
Module (MREM). Specifically, the DMM establishes a group of disentangled memory
blocks to store the semantic-decoupled face representations, which could
provide the most relevant information to refine the semantic-level
coordination. The MREM involves a masked correlation mining mechanism to
enhance the feature relationships into the corrupted regions, which could also
make up for the correlation loss caused by memory disentanglement. Furthermore,
to better improve the inter-coordination between the corrupted and
non-corrupted regions and enhance the intra-coordination in corrupted regions,
we design InCo2 Loss, a pair of similarity based losses to constrain the
feature consistency. Eventually, extensive experiments conducted on CelebA-HQ
and FFHQ datasets demonstrate the superiority of our MDRNets compared with
previous State-Of-The-Art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Study of Remote Sensing Pretraining. (arXiv:2204.02825v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02825">
<div class="article-summary-box-inner">
<span><p>Deep learning has largely reshaped remote sensing research for aerial image
understanding. Nevertheless, most of existing deep models are initialized with
ImageNet pretrained weights, where the natural images inevitably presents a
large domain gap relative to the aerial images, probably limiting the
finetuning performance on downstream aerial scene tasks. This issue motivates
us to conduct an empirical study of remote sensing pretraining (RSP). To this
end, we train different networks from scratch with the help of the largest
remote sensing scene recognition dataset up to now-MillionAID, to obtain the
remote sensing pretrained backbones, including both convolutional neural
networks (CNN) and vision transformers such as Swin and ViTAE, which have shown
promising performance on computer vision tasks. Then, we investigate the impact
of ImageNet pretraining (IMP) and RSP on a series of downstream tasks including
scene recognition, semantic segmentation, object detection, and change
detection using the CNN and vision transformers backbones. We have some
empirical findings as follows. First, vision transformers generally outperforms
CNN backbones, where ViTAE achieves the best performance, owing to its strong
representation capacity by introducing intrinsic inductive bias from
convolutions to transformers. Second, both IMP and RSP help deliver better
performance, where IMP enjoys a versatility by learning more universal
representations from diverse images belonging to much more categories while RSP
is distinctive in perceiving remote sensing related semantics. Third, RSP
mitigates the data discrepancy of IMP for remote sensing but may still suffer
from the task discrepancy, where downstream tasks require different
representations from the scene recognition task. These findings call for
further research efforts on both large-scale pretraining datasets and effective
pretraining methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CCAT-NET: A Novel Transformer Based Semi-supervised Framework for Covid-19 Lung Lesion Segmentation. (arXiv:2204.02839v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02839">
<div class="article-summary-box-inner">
<span><p>The spread of the novel coronavirus disease 2019 (COVID-19) has claimed
millions of lives. Automatic segmentation of lesions from CT images can assist
doctors with screening, treatment, and monitoring. However, accurate
segmentation of lesions from CT images can be very challenging due to data and
model limitations. Recently, Transformer-based networks have attracted a lot of
attention in the area of computer vision, as Transformer outperforms CNN at a
bunch of tasks. In this work, we propose a novel network structure that
combines CNN and Transformer for the segmentation of COVID-19 lesions. We
further propose an efficient semi-supervised learning framework to address the
shortage of labeled data. Extensive experiments showed that our proposed
network outperforms most existing networks and the semi-supervised learning
framework can outperform the base network by 3.0% and 8.2% in terms of Dice
coefficient and sensitivity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open-Source Tools for Behavioral Video Analysis: Setup, Methods, and Development. (arXiv:2204.02842v1 [q-bio.QM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02842">
<div class="article-summary-box-inner">
<span><p>Recently developed methods for video analysis, especially models for pose
estimation and behavior classification, are transforming behavioral
quantification to be more precise, scalable, and reproducible in fields such as
neuroscience and ethology. These tools overcome long-standing limitations of
manual scoring of video frames and traditional "center of mass" tracking
algorithms to enable video analysis at scale. The expansion of open-source
tools for video acquisition and analysis has led to new experimental approaches
to understand behavior. Here, we review currently available open source tools
for video analysis, how to set them up in a lab that is new to video recording
methods, and some issues that should be addressed by developers and advanced
users, including the need to openly share datasets and code, how to compare
algorithms and their parameters, and the need for documentation and
community-wide standards. We hope to encourage more widespread use and
continued development of the tools. They have tremendous potential for
accelerating scientific progress for understanding the brain and behavior.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Generate Realistic Noisy Images via Pixel-level Noise-aware Adversarial Training. (arXiv:2204.02844v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02844">
<div class="article-summary-box-inner">
<span><p>Existing deep learning real denoising methods require a large amount of
noisy-clean image pairs for supervision. Nonetheless, capturing a real
noisy-clean dataset is an unacceptable expensive and cumbersome procedure. To
alleviate this problem, this work investigates how to generate realistic noisy
images. Firstly, we formulate a simple yet reasonable noise model that treats
each real noisy pixel as a random variable. This model splits the noisy image
generation problem into two sub-problems: image domain alignment and noise
domain alignment. Subsequently, we propose a novel framework, namely
Pixel-level Noise-aware Generative Adversarial Network (PNGAN). PNGAN employs a
pre-trained real denoiser to map the fake and real noisy images into a nearly
noise-free solution space to perform image domain alignment. Simultaneously,
PNGAN establishes a pixel-level adversarial training to conduct noise domain
alignment. Additionally, for better noise fitting, we present an efficient
architecture Simple Multi-scale Network (SMNet) as the generator. Qualitative
validation shows that noise generated by PNGAN is highly similar to real noise
in terms of intensity and distribution. Quantitative experiments demonstrate
that a series of denoisers trained with the generated noisy images achieve
state-of-the-art (SOTA) results on four real denoising benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KNN-Diffusion: Image Generation via Large-Scale Retrieval. (arXiv:2204.02849v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02849">
<div class="article-summary-box-inner">
<span><p>While the availability of massive Text-Image datasets is shown to be
extremely useful in training large-scale generative models (e.g. DDPMs,
Transformers), their output typically depends on the quality of both the input
text, as well as the training dataset. In this work, we show how large-scale
retrieval methods, in particular efficient K-Nearest-Neighbors (KNN) search,
can be used in order to train a model to adapt to new samples. Learning to
adapt enables several new capabilities. Sifting through billions of records at
inference time is extremely efficient and can alleviate the need to train or
memorize an adequately large generative model. Additionally, fine-tuning
trained models to new samples can be achieved by simply adding them to the
table. Rare concepts, even without any presence in the training set, can be
then leveraged during test time without any modification to the generative
model. Our diffusion-based model trains on images only, by leveraging a joint
Text-Image multi-modal metric. Compared to baseline methods, our generations
achieve state of the art results both in human evaluations as well as with
perceptual scores when tested on a public multimodal dataset of natural images,
as well as on a collected dataset of 400 million Stickers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Influence of Color Spaces for Deep Learning Image Colorization. (arXiv:2204.02850v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02850">
<div class="article-summary-box-inner">
<span><p>Colorization is a process that converts a grayscale image into a color one
that looks as natural as possible. Over the years this task has received a lot
of attention. Existing colorization methods rely on different color spaces:
RGB, YUV, Lab, etc. In this chapter, we aim to study their influence on the
results obtained by training a deep neural network, to answer the question: "Is
it crucial to correctly choose the right color space in deep-learning based
colorization?". First, we briefly summarize the literature and, in particular,
deep learning-based methods. We then compare the results obtained with the same
deep neural network architecture with RGB, YUV and Lab color spaces.
Qualitative and quantitative analysis do not conclude similarly on which color
space is better. We then show the importance of carefully designing the
architecture and evaluation protocols depending on the types of images that are
being processed and their specificities: strong/small contours, few/many
objects, recent/archive images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieval-based Spatially Adaptive Normalization for Semantic Image Synthesis. (arXiv:2204.02854v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02854">
<div class="article-summary-box-inner">
<span><p>Semantic image synthesis is a challenging task with many practical
applications. Albeit remarkable progress has been made in semantic image
synthesis with spatially-adaptive normalization and existing methods normalize
the feature activations under the coarse-level guidance (e.g., semantic class).
However, different parts of a semantic object (e.g., wheel and window of car)
are quite different in structures and textures, making blurry synthesis results
usually inevitable due to the missing of fine-grained guidance. In this paper,
we propose a novel normalization module, termed as REtrieval-based Spatially
AdaptIve normaLization (RESAIL), for introducing pixel level fine-grained
guidance to the normalization architecture. Specifically, we first present a
retrieval paradigm by finding a content patch of the same semantic class from
training set with the most similar shape to each test semantic mask. Then,
RESAIL is presented to use the retrieved patch for guiding the feature
normalization of corresponding region, and can provide pixel level fine-grained
guidance, thereby greatly mitigating blurry synthesis results. Moreover,
distorted ground-truth images are also utilized as alternatives of
retrieval-based guidance for feature normalization, further benefiting model
training and improving visual quality of generated images. Experiments on
several challenging datasets show that our RESAIL performs favorably against
state-of-the-arts in terms of quantitative metrics, visual quality, and
subjective evaluation. The source code and pre-trained models will be publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Demonstrate Once, Imitate Immediately (DOME): Learning Visual Servoing for One-Shot Imitation Learning. (arXiv:2204.02863v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02863">
<div class="article-summary-box-inner">
<span><p>We present DOME, a novel method for one-shot imitation learning, where a task
can be learned from just a single demonstration and then be deployed
immediately, without any further data collection or training. DOME does not
require prior task or object knowledge, and can perform the task in novel
object configurations and with distractors. At its core, DOME uses an
image-conditioned object segmentation network followed by a learned visual
servoing network, to move the robot's end-effector to the same relative pose to
the object as during the demonstration, after which the task can be completed
by replaying the demonstration's end-effector velocities. We show that DOME
achieves near 100% success rate on 7 real-world everyday tasks, and we perform
several studies to thoroughly understand each individual component of DOME.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ECLIPSE: Efficient Long-range Video Retrieval using Sight and Sound. (arXiv:2204.02874v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02874">
<div class="article-summary-box-inner">
<span><p>We introduce an audiovisual method for long-range text-to-video retrieval.
Unlike previous approaches designed for short video retrieval (e.g., 5-15
seconds in duration), our approach aims to retrieve minute-long videos that
capture complex human actions. One challenge of standard video-only approaches
is the large computational cost associated with processing hundreds of densely
extracted frames from such long videos. To address this issue, we propose to
replace parts of the video with compact audio cues that succinctly summarize
dynamic audio events and are cheap to process. Our method, named ECLIPSE
(Efficient CLIP with Sound Encoding), adapts the popular CLIP model to an
audiovisual video setting, by adding a unified audiovisual transformer block
that captures complementary cues from the video and audio streams. In addition
to being 2.92x faster and 2.34x memory-efficient than long-range video-only
approaches, our method also achieves better text-to-video retrieval accuracy on
several diverse long-range video datasets such as ActivityNet, QVHighlights,
YouCook2, DiDeMo and Charades.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sampling-based Fast Gradient Rescaling Method for Highly Transferable Adversarial Attacks. (arXiv:2204.02887v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02887">
<div class="article-summary-box-inner">
<span><p>Deep neural networks have shown to be very vulnerable to adversarial examples
crafted by adding human-imperceptible perturbations to benign inputs. After
achieving impressive attack success rates in the white-box setting, more focus
is shifted to black-box attacks. In either case, the common gradient-based
approaches generally use the $sign$ function to generate perturbations at the
end of the process. However, only a few works pay attention to the limitation
of the $sign$ function. Deviation between the original gradient and the
generated noises may lead to inaccurate gradient update estimation and
suboptimal solutions for adversarial transferability, which is crucial for
black-box attacks. To address this issue, we propose a Sampling-based Fast
Gradient Rescaling Method (S-FGRM) to improve the transferability of the
crafted adversarial examples. Specifically, we use data rescaling to substitute
the inefficient $sign$ function in gradient-based attacks without extra
computational cost. We also propose a Depth First Sampling method to eliminate
the fluctuation of rescaling and stabilize the gradient update. Our method can
be used in any gradient-based optimizations and is extensible to be integrated
with various input transformation or ensemble methods for further improving the
adversarial transferability. Extensive experiments on the standard ImageNet
dataset show that our S-FGRM could significantly boost the transferability of
gradient-based attacks and outperform the state-of-the-art baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DBF: Dynamic Belief Fusion for Combining Multiple Object Detectors. (arXiv:2204.02890v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02890">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel and highly practical score-level fusion
approach called dynamic belief fusion (DBF) that directly integrates inference
scores of individual detections from multiple object detection methods. To
effectively integrate the individual outputs of multiple detectors, the level
of ambiguity in each detection score is estimated using a confidence model
built on a precision-recall relationship of the corresponding detector. For
each detector output, DBF then calculates the probabilities of three hypotheses
(target, non-target, and intermediate state (target or non-target)) based on
the confidence level of the detection score conditioned on the prior confidence
model of individual detectors, which is referred to as basic probability
assignment. The probability distributions over three hypotheses of all the
detectors are optimally fused via the Dempster's combination rule. Experiments
on the ARL, PASCAL VOC 07, and 12 datasets show that the detection accuracy of
the DBF is significantly higher than any of the baseline fusion approaches as
well as individual detectors used for the fusion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Instance Edge Detection. (arXiv:2204.02898v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02898">
<div class="article-summary-box-inner">
<span><p>Edge detection has long been an important problem in the field of computer
vision. Previous works have explored category-agnostic or category-aware edge
detection. In this paper, we explore edge detection in the context of object
instances. Although object boundaries could be easily derived from segmentation
masks, in practice, instance segmentation models are trained to maximize IoU to
the ground-truth mask, which means that segmentation boundaries are not
enforced to precisely align with ground-truth edge boundaries. Thus, the task
of instance edge detection itself is different and critical. Since precise edge
detection requires high resolution feature maps, we design a novel transformer
architecture that efficiently combines a FPN and a transformer decoder to
enable cross attention on multi-scale high resolution feature maps within a
reasonable computation budget. Further, we propose a light weight dense
prediction head that is applicable to both instance edge and mask detection.
Finally, we use a penalty reduced focal loss to effectively train the model
with point supervision on instance edges, which can reduce annotation costs. We
demonstrate highly competitive instance edge detection performance compared to
state-of-the-art baselines, and also show that the proposed task and loss are
complementary to instance segmentation and object detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Study of End-to-End Temporal Action Detection. (arXiv:2204.02932v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02932">
<div class="article-summary-box-inner">
<span><p>Temporal action detection (TAD) is an important yet challenging task in video
understanding. It aims to simultaneously predict the semantic label and the
temporal interval of every action instance in an untrimmed video. Rather than
end-to-end learning, most existing methods adopt a head-only learning paradigm,
where the video encoder is pre-trained for action classification, and only the
detection head upon the encoder is optimized for TAD. The effect of end-to-end
learning is not systematically evaluated. Besides, there lacks an in-depth
study on the efficiency-accuracy trade-off in end-to-end TAD. In this paper, we
present an empirical study of end-to-end temporal action detection. We validate
the advantage of end-to-end learning over head-only learning and observe up to
11\% performance improvement. Besides, we study the effects of multiple design
choices that affect the TAD performance and speed, including detection head,
video encoder, and resolution of input videos. Based on the findings, we build
a mid-resolution baseline detector, which achieves the state-of-the-art
performance of end-to-end methods while running more than 4$\times$ faster. We
hope that this paper can serve as a guide for end-to-end learning and inspire
future research in this field. Code and models are available at
\url{https://github.com/xlliu7/E2E-TAD}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations. (arXiv:2204.02937v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02937">
<div class="article-summary-box-inner">
<span><p>Neural network classifiers can largely rely on simple spurious features, such
as backgrounds, to make predictions. However, even in these cases, we show that
they still often learn core features associated with the desired attributes of
the data, contrary to recent findings. Inspired by this insight, we demonstrate
that simple last layer retraining can match or outperform state-of-the-art
approaches on spurious correlation benchmarks, but with profoundly lower
complexity and computational expenses. Moreover, we show that last layer
retraining on large ImageNet-trained models can also significantly reduce
reliance on background and texture information, improving robustness to
covariate shift, after only minutes of training on a single GPU.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">S-R2F2U-Net: A single-stage model for teeth segmentation. (arXiv:2204.02939v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02939">
<div class="article-summary-box-inner">
<span><p>Precision tooth segmentation is crucial in the oral sector because it
provides location information for orthodontic therapy, clinical diagnosis, and
surgical treatments. In this paper, we investigate residual, recurrent, and
attention networks to segment teeth from panoramic dental images. Based on our
findings, we suggest three single-stage models: Single Recurrent R2U-Net
(S-R2U-Net), Single Recurrent Filter Double R2U-Net (S-R2F2U-Net), and Single
Recurrent Attention Enabled Filter Double (S-R2F2-Attn-U-Net). Particularly,
S-R2F2U-Net outperforms state-of-the-art models in terms of accuracy and dice
score. A hybrid loss function combining the cross-entropy loss and dice loss is
used to train the model. In addition, it reduces around 45% of model parameters
compared to the R2U-Net model. Models are trained and evaluated on a benchmark
dataset containing 1500 dental panoramic X-ray images. S-R2F2U-Net achieves
97.31% of accuracy and 93.26% of dice score, showing superiority over the
state-of-the-art methods. Codes are available at
https://github.com/mrinal054/teethSeg_sr2f2u-net.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Intervertebral Disc Labeling With Learning Shape Information, A Look Once Approach. (arXiv:2204.02943v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02943">
<div class="article-summary-box-inner">
<span><p>Accurate and automatic segmentation of intervertebral discs from medical
images is a critical task for the assessment of spine-related diseases such as
osteoporosis, vertebral fractures, and intervertebral disc herniation. To date,
various approaches have been developed in the literature which routinely relies
on detecting the discs as the primary step. A disadvantage of many cohort
studies is that the localization algorithm also yields false-positive
detections. In this study, we aim to alleviate this problem by proposing a
novel U-Net-based structure to predict a set of candidates for intervertebral
disc locations. In our design, we integrate the image shape information (image
gradients) to encourage the model to learn rich and generic geometrical
information. This additional signal guides the model to selectively emphasize
the contextual representation and suppress the less discriminative features. On
the post-processing side, to further decrease the false positive rate, we
propose a permutation invariant 'look once' model, which accelerates the
candidate recovery procedure. In comparison with previous studies, our proposed
approach does not need to perform the selection in an iterative fashion. The
proposed method was evaluated on the spine generic public multi-center dataset
and demonstrated superior performance compared to previous work. We have
provided the implementation code in
https://github.com/rezazad68/intervertebral-lookonce
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"The Pedestrian next to the Lamppost" Adaptive Object Graphs for Better Instantaneous Mapping. (arXiv:2204.02944v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02944">
<div class="article-summary-box-inner">
<span><p>Estimating a semantically segmented bird's-eye-view (BEV) map from a single
image has become a popular technique for autonomous control and navigation.
However, they show an increase in localization error with distance from the
camera. While such an increase in error is entirely expected - localization is
harder at distance - much of the drop in performance can be attributed to the
cues used by current texture-based models, in particular, they make heavy use
of object-ground intersections (such as shadows), which become increasingly
sparse and uncertain for distant objects. In this work, we address these
shortcomings in BEV-mapping by learning the spatial relationship between
objects in a scene. We propose a graph neural network which predicts BEV
objects from a monocular image by spatially reasoning about an object within
the context of other objects. Our approach sets a new state-of-the-art in BEV
estimation from monocular images across three large-scale datasets, including a
50% relative improvement for objects on nuScenes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Demoireing with Relation-Based Temporal Consistency. (arXiv:2204.02957v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02957">
<div class="article-summary-box-inner">
<span><p>Moire patterns, appearing as color distortions, severely degrade image and
video qualities when filming a screen with digital cameras. Considering the
increasing demands for capturing videos, we study how to remove such
undesirable moire patterns in videos, namely video demoireing. To this end, we
introduce the first hand-held video demoireing dataset with a dedicated data
collection pipeline to ensure spatial and temporal alignments of captured data.
Further, a baseline video demoireing model with implicit feature space
alignment and selective feature aggregation is developed to leverage
complementary information from nearby frames to improve frame-level video
demoireing. More importantly, we propose a relation-based temporal consistency
loss to encourage the model to learn temporal consistency priors directly from
ground-truth reference videos, which facilitates producing temporally
consistent predictions and effectively maintains frame-level qualities.
Extensive experiments manifest the superiority of our model. Code is available
at \url{https://daipengwa.github.io/VDmoire_ProjectPage/}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LEAD: Self-Supervised Landmark Estimation by Aligning Distributions of Feature Similarity. (arXiv:2204.02958v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02958">
<div class="article-summary-box-inner">
<span><p>In this work, we introduce LEAD, an approach to discover landmarks from an
unannotated collection of category-specific images. Existing works in
self-supervised landmark detection are based on learning dense (pixel-level)
feature representations from an image, which are further used to learn
landmarks in a semi-supervised manner. While there have been advances in
self-supervised learning of image features for instance-level tasks like
classification, these methods do not ensure dense equivariant representations.
The property of equivariance is of interest for dense prediction tasks like
landmark estimation. In this work, we introduce an approach to enhance the
learning of dense equivariant representations in a self-supervised fashion. We
follow a two-stage training approach: first, we train a network using the BYOL
objective which operates at an instance level. The correspondences obtained
through this network are further used to train a dense and compact
representation of the image using a lightweight network. We show that having
such a prior in the feature extractor helps in landmark detection, even under
drastically limited number of annotations while also improving generalization
across scale variations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simple and Effective Synthesis of Indoor 3D Scenes. (arXiv:2204.02960v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02960">
<div class="article-summary-box-inner">
<span><p>We study the problem of synthesizing immersive 3D indoor scenes from one or
more images. Our aim is to generate high-resolution images and videos from
novel viewpoints, including viewpoints that extrapolate far beyond the input
images while maintaining 3D consistency. Existing approaches are highly
complex, with many separately trained stages and components. We propose a
simple alternative: an image-to-image GAN that maps directly from reprojections
of incomplete point clouds to full high-resolution RGB-D images. On the
Matterport3D and RealEstate10K datasets, our approach significantly outperforms
prior work when evaluated by humans, as well as on FID scores. Further, we show
that our model is useful for generative data augmentation. A
vision-and-language navigation (VLN) agent trained with trajectories
spatially-perturbed by our model improves success rate by up to 1.5% over a
state of the art baseline on the R2R benchmark. Our code will be made available
to facilitate generative data augmentation and applications to downstream
robotics and embodied AI tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SMU-Net: Style matching U-Net for brain tumor segmentation with missing modalities. (arXiv:2204.02961v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02961">
<div class="article-summary-box-inner">
<span><p>Gliomas are one of the most prevalent types of primary brain tumours,
accounting for more than 30\% of all cases and they develop from the glial stem
or progenitor cells. In theory, the majority of brain tumours could well be
identified exclusively by the use of Magnetic Resonance Imaging (MRI). Each MRI
modality delivers distinct information on the soft tissue of the human brain
and integrating all of them would provide comprehensive data for the accurate
segmentation of the glioma, which is crucial for the patient's prognosis,
diagnosis, and determining the best follow-up treatment. Unfortunately, MRI is
prone to artifacts for a variety of reasons, which might result in missing one
or more MRI modalities. Various strategies have been proposed over the years to
synthesize the missing modality or compensate for the influence it has on
automated segmentation models. However, these methods usually fail to model the
underlying missing information. In this paper, we propose a style matching
U-Net (SMU-Net) for brain tumour segmentation on MRI images. Our co-training
approach utilizes a content and style-matching mechanism to distill the
informative features from the full-modality network into a missing modality
network. To do so, we encode both full-modality and missing-modality data into
a latent space, then we decompose the representation space into a style and
content representation. Our style matching module adaptively recalibrates the
representation space by learning a matching function to transfer the
informative and textural features from a full-modality path into a
missing-modality path. Moreover, by modelling the mutual information, our
content module surpasses the less informative features and re-calibrates the
representation space based on discriminative semantic features. The evaluation
process on the BraTS 2018 dataset shows a significant results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unleashing Vanilla Vision Transformer with Masked Image Modeling for Object Detection. (arXiv:2204.02964v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02964">
<div class="article-summary-box-inner">
<span><p>We present an approach to efficiently and effectively adapt a masked image
modeling (MIM) pre-trained vanilla Vision Transformer (ViT) for object
detection, which is based on our two novel observations: (i) A MIM pre-trained
vanilla ViT can work surprisingly well in the challenging object-level
recognition scenario even with random sampled partial observations, e.g., only
25% ~ 50% of the input sequence. (ii) In order to construct multi-scale
representations for object detection, a random initialized compact
convolutional stem supplants the pre-trained large kernel patchify stem, and
its intermediate features can naturally serve as the higher resolution inputs
of a feature pyramid without upsampling. While the pre-trained ViT is only
regarded as the third-stage of our detector's backbone instead of the whole
feature extractor, resulting in a ConvNet-ViT hybrid architecture. The proposed
detector, named MIMDet, enables a MIM pre-trained vanilla ViT to outperform
hierarchical Swin Transformer by 2.3 box AP and 2.5 mask AP on COCO, and
achieve even better results compared with other adapted vanilla ViT using a
more modest fine-tuning recipe while converging 2.8x faster. Code and
pre-trained models are available at \url{https://github.com/hustvl/MIMDet}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LilNetX: Lightweight Networks with EXtreme Model Compression and Structured Sparsification. (arXiv:2204.02965v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02965">
<div class="article-summary-box-inner">
<span><p>We introduce LilNetX, an end-to-end trainable technique for neural networks
that enables learning models with specified accuracy-rate-computation
trade-off. Prior works approach these problems one at a time and often require
post-processing or multistage training which become less practical and do not
scale very well for large datasets or architectures. Our method constructs a
joint training objective that penalizes the self-information of network
parameters in a reparameterized latent space to encourage small model size
while also introducing priors to increase structured sparsity in the parameter
space to reduce computation. We achieve up to 50% smaller model size and 98%
model sparsity on ResNet-20 while retaining the same accuracy on the CIFAR-10
dataset as well as 35% smaller model size and 42% structured sparsity on
ResNet-50 trained on ImageNet, when compared to existing state-of-the-art model
compression methods. Code is available at
https://github.com/Sharath-girish/LilNetX.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Temporal Alignment Networks for Long-term Video. (arXiv:2204.02968v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02968">
<div class="article-summary-box-inner">
<span><p>The objective of this paper is a temporal alignment network that ingests long
term video sequences, and associated text sentences, in order to: (1) determine
if a sentence is alignable with the video; and (2) if it is alignable, then
determine its alignment. The challenge is to train such networks from
large-scale datasets, such as HowTo100M, where the associated text sentences
have significant noise, and are only weakly aligned when relevant. Apart from
proposing the alignment network, we also make four contributions: (i) we
describe a novel co-training method that enables to denoise and train on raw
instructional videos without using manual annotation, despite the considerable
noise; (ii) to benchmark the alignment performance, we manually curate a
10-hour subset of HowTo100M, totalling 80 videos, with sparse temporal
descriptions. Our proposed model, trained on HowTo100M, outperforms strong
baselines (CLIP, MIL-NCE) on this alignment dataset by a significant margin;
(iii) we apply the trained model in the zero-shot settings to multiple
downstream video understanding tasks and achieve state-of-the-art results,
including text-video retrieval on YouCook2, and weakly supervised video action
segmentation on Breakfast-Action; (iv) we use the automatically aligned
HowTo100M annotations for end-to-end finetuning of the backbone model, and
obtain improved performance on downstream action recognition tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ME R-CNN: Multi-Expert R-CNN for Object Detection. (arXiv:1704.01069v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1704.01069">
<div class="article-summary-box-inner">
<span><p>We introduce Multi-Expert Region-based Convolutional Neural Network (ME
R-CNN) which is equipped with multiple experts (ME) where each expert is
learned to process a certain type of regions of interest (RoIs). This
architecture better captures the appearance variations of the RoIs caused by
different shapes, poses, and viewing angles. In order to direct each RoI to the
appropriate expert, we devise a novel "learnable" network, which we call,
expert assignment network (EAN). EAN automatically learns the optimal
RoI-expert relationship even without any supervision of expert assignment. As
the major components of ME R-CNN, ME and EAN, are mutually affecting each other
while tied to a shared network, neither an alternating nor a naive end-to-end
optimization is likely to fail. To address this problem, we introduce a
practical training strategy which is tailored to optimize ME, EAN, and the
shared network in an end-to-end fashion. We show that both of the architectures
provide considerable performance increase over the baselines on PASCAL VOC 07,
12, and MS COCO datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning V1 Simple Cells with Vector Representation of Local Content and Matrix Representation of Local Motion. (arXiv:1902.03871v5 [cs.NE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1902.03871">
<div class="article-summary-box-inner">
<span><p>This paper proposes a representational model for image pairs such as
consecutive video frames that are related by local pixel displacements, in the
hope that the model may shed light on motion perception in primary visual
cortex (V1). The model couples the following two components: (1) the vector
representations of local contents of images and (2) the matrix representations
of local pixel displacements caused by the relative motions between the agent
and the objects in the 3D scene. When the image frame undergoes changes due to
local pixel displacements, the vectors are multiplied by the matrices that
represent the local displacements. Thus the vector representation is
equivariant as it varies according to the local displacements. Our experiments
show that our model can learn Gabor-like filter pairs of quadrature phases. The
profiles of the learned filters match those of simple cells in Macaque V1.
Moreover, we demonstrate that the model can learn to infer local motions in
either a supervised or unsupervised manner. With such a simple model, we
achieve competitive results on optical flow estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ktrain: A Low-Code Library for Augmented Machine Learning. (arXiv:2004.10703v5 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.10703">
<div class="article-summary-box-inner">
<span><p>We present ktrain, a low-code Python library that makes machine learning more
accessible and easier to apply. As a wrapper to TensorFlow and many other
libraries (e.g., transformers, scikit-learn, stellargraph), it is designed to
make sophisticated, state-of-the-art machine learning models simple to build,
train, inspect, and apply by both beginners and experienced practitioners.
Featuring modules that support text data (e.g., text classification, sequence
tagging, open-domain question-answering), vision data (e.g., image
classification), graph data (e.g., node classification, link prediction), and
tabular data, ktrain presents a simple unified interface enabling one to
quickly solve a wide range of tasks in as little as three or four "commands" or
lines of code.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantically Adversarial Learnable Filters. (arXiv:2008.06069v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.06069">
<div class="article-summary-box-inner">
<span><p>We present an adversarial framework to craft perturbations that mislead
classifiers by accounting for the image content and the semantics of the
labels. The proposed framework combines a structure loss and a semantic
adversarial loss in a multi-task objective function to train a fully
convolutional neural network. The structure loss helps generate perturbations
whose type and magnitude are defined by a target image processing filter. The
semantic adversarial loss considers groups of (semantic) labels to craft
perturbations that prevent the filtered image {from} being classified with a
label in the same group. We validate our framework with three different target
filters, namely detail enhancement, log transformation and gamma correction
filters; and evaluate the adversarially filtered images against three
classifiers, ResNet50, ResNet18 and AlexNet, pre-trained on ImageNet. We show
that the proposed framework generates filtered images with a high success rate,
robustness, and transferability to unseen classifiers. We also discuss
objective and subjective evaluations of the adversarial perturbations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video action recognition for lane-change classification and prediction of surrounding vehicles. (arXiv:2101.05043v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.05043">
<div class="article-summary-box-inner">
<span><p>In highway scenarios, an alert human driver will typically anticipate early
cut-in/cut-out maneuvers of surrounding vehicles using visual cues mainly.
Autonomous vehicles must anticipate these situations at an early stage too, to
increase their safety and efficiency. In this work, lane-change recognition and
prediction tasks are posed as video action recognition problems. Up to four
different two-stream-based approaches, that have been successfully applied to
address human action recognition, are adapted here by stacking visual cues from
forward-looking video cameras to recognize and anticipate lane-changes of
target vehicles. We study the influence of context and observation horizons on
performance, and different prediction horizons are analyzed. The different
models are trained and evaluated using the PREVENTION dataset. The obtained
results clearly demonstrate the potential of these methodologies to serve as
robust predictors of future lane-changes of surrounding vehicles proving an
accuracy higher than 90% in time horizons of between 1-2 seconds.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeRF--: Neural Radiance Fields Without Known Camera Parameters. (arXiv:2102.07064v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.07064">
<div class="article-summary-box-inner">
<span><p>Considering the problem of novel view synthesis (NVS) from only a set of 2D
images, we simplify the training process of Neural Radiance Field (NeRF) on
forward-facing scenes by removing the requirement of known or pre-computed
camera parameters, including both intrinsics and 6DoF poses. To this end, we
propose NeRF$--$, with three contributions: First, we show that the camera
parameters can be jointly optimised as learnable parameters with NeRF training,
through a photometric reconstruction; Second, to benchmark the camera parameter
estimation and the quality of novel view renderings, we introduce a new dataset
of path-traced synthetic scenes, termed as Blender Forward-Facing Dataset
(BLEFF); Third, we conduct extensive analyses to understand the training
behaviours under various camera motions, and show that in most scenarios, the
joint optimisation pipeline can recover accurate camera parameters and achieve
comparable novel view synthesis quality as those trained with COLMAP
pre-computed camera parameters. Our code and data are available at
https://nerfmm.active.vision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Calibrated-Guidance for Object Detection in Aerial Images. (arXiv:2103.11399v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.11399">
<div class="article-summary-box-inner">
<span><p>Object detection is one of the most fundamental yet challenging research
topics in the domain of computer vision. Recently, the study on this topic in
aerial images has made tremendous progress. However, complex background and
worse imaging quality are obvious problems in aerial object detection. Most
state-of-the-art approaches tend to develop elaborate attention mechanisms for
the space-time feature calibrations with arduous computational complexity,
while surprisingly ignoring the importance of feature calibrations in
channel-wise. In this work, we propose a simple yet effective
Calibrated-Guidance (CG) scheme to enhance channel communications in a feature
transformer fashion, which can adaptively determine the calibration weights for
each channel based on the global feature affinity correlations. Specifically,
for a given set of feature maps, CG first computes the feature similarity
between each channel and the remaining channels as the intermediary calibration
guidance. Then, re-representing each channel by aggregating all the channels
weighted together via the guidance operation. Our CG is a general module that
can be plugged into any deep neural networks, which is named as CG-Net. To
demonstrate its effectiveness and efficiency, extensive experiments are carried
out on both oriented object detection task and horizontal object detection task
in aerial images. Experimental results on two challenging benchmarks (DOTA and
HRSC2016) demonstrate that our CG-Net can achieve the new state-of-the-art
performance in accuracy with a fair computational overhead. The source code has
been open sourced at https://github.com/WeiZongqi/CG-Net
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph Sampling Based Deep Metric Learning for Generalizable Person Re-Identification. (arXiv:2104.01546v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01546">
<div class="article-summary-box-inner">
<span><p>Recent studies show that, both explicit deep feature matching as well as
large-scale and diverse training data can significantly improve the
generalization of person re-identification. However, the efficiency of learning
deep matchers on large-scale data has not yet been adequately studied. Though
learning with classification parameters or class memory is a popular way, it
incurs large memory and computational costs. In contrast, pairwise deep metric
learning within mini batches would be a better choice. However, the most
popular random sampling method, the well-known PK sampler, is not informative
and efficient for deep metric learning. Though online hard example mining has
improved the learning efficiency to some extent, the mining in mini batches
after random sampling is still limited. This inspires us to explore the use of
hard example mining earlier, in the data sampling stage. To do so, in this
paper, we propose an efficient mini-batch sampling method, called graph
sampling (GS), for large-scale deep metric learning. The basic idea is to build
a nearest neighbor relationship graph for all classes at the beginning of each
epoch. Then, each mini batch is composed of a randomly selected class and its
nearest neighboring classes so as to provide informative and challenging
examples for learning. Together with an adapted competitive baseline, we
improve the state of the art in generalizable person re-identification
significantly, by 25.1% in Rank-1 on MSMT17 when trained on RandPerson.
Besides, the proposed method also outperforms the competitive baseline, by 6.8%
in Rank-1 on CUHK03-NP when trained on MSMT17. Meanwhile, the training time is
significantly reduced, from 25.4 hours to 2 hours when trained on RandPerson
with 8,000 identities. Code is available at
https://github.com/ShengcaiLiao/QAConv.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Common Limitations of Image Processing Metrics: A Picture Story. (arXiv:2104.05642v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05642">
<div class="article-summary-box-inner">
<span><p>While the importance of automatic image analysis is continuously increasing,
recent meta-research revealed major flaws with respect to algorithm validation.
Performance metrics are particularly key for meaningful, objective, and
transparent performance assessment and validation of the used automatic
algorithms, but relatively little attention has been given to the practical
pitfalls when using specific metrics for a given image analysis task. These are
typically related to (1) the disregard of inherent metric properties, such as
the behaviour in the presence of class imbalance or small target structures,
(2) the disregard of inherent data set properties, such as the non-independence
of the test cases, and (3) the disregard of the actual biomedical domain
interest that the metrics should reflect. This living dynamically document has
the purpose to illustrate important limitations of performance metrics commonly
applied in the field of image analysis. In this context, it focuses on
biomedical image analysis problems that can be phrased as image-level
classification, semantic segmentation, instance segmentation, or object
detection task. The current version is based on a Delphi process on metrics
conducted by an international consortium of image analysis experts from more
than 60 institutions worldwide.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FINet: Dual Branches Feature Interaction for Partial-to-Partial Point Cloud Registration. (arXiv:2106.03479v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03479">
<div class="article-summary-box-inner">
<span><p>Data association is important in the point cloud registration. In this work,
we propose to solve the partial-to-partial registration from a new perspective,
by introducing multi-level feature interactions between the source and the
reference clouds at the feature extraction stage, such that the registration
can be realized without the attentions or explicit mask estimation for the
overlapping detection as adopted previously. Specifically, we present FINet, a
feature interaction-based structure with the capability to enable and
strengthen the information associating between the inputs at multiple stages.
To achieve this, we first split the features into two components, one for
rotation and one for translation, based on the fact that they belong to
different solution spaces, yielding a dual branches structure. Second, we
insert several interaction modules at the feature extractor for the data
association. Third, we propose a transformation sensitivity loss to obtain
rotation-attentive and translation-attentive features. Experiments demonstrate
that our method performs higher precision and robustness compared to the
state-of-the-art traditional and learning-based methods. Code is available at
https://github.com/megvii-research/FINet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Connection between Local Attention and Dynamic Depth-wise Convolution. (arXiv:2106.04263v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04263">
<div class="article-summary-box-inner">
<span><p>Vision Transformer (ViT) attains state-of-the-art performance in visual
recognition, and the variant, Local Vision Transformer, makes further
improvements. The major component in Local Vision Transformer, local attention,
performs the attention separately over small local windows. We rephrase local
attention as a channel-wise locally-connected layer and analyze it from two
network regularization manners, sparse connectivity and weight sharing, as well
as weight computation. Sparse connectivity: there is no connection across
channels, and each position is connected to the positions within a small local
window. Weight sharing: the connection weights for one position are shared
across channels or within each group of channels. Dynamic weight: the
connection weights are dynamically predicted according to each image instance.
We point out that local attention resembles depth-wise convolution and its
dynamic version in sparse connectivity. The main difference lies in weight
sharing - depth-wise convolution shares connection weights (kernel weights)
across spatial positions. We empirically observe that the models based on
depth-wise convolution and the dynamic variant with lower computation
complexity perform on-par with or sometimes slightly better than Swin
Transformer, an instance of Local Vision Transformer, for ImageNet
classification, COCO object detection and ADE semantic segmentation. These
observations suggest that Local Vision Transformer takes advantage of two
regularization forms and dynamic weight to increase the network capacity. Code
is available at https://github.com/Atten4Vis/DemystifyLocalViT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Dynamics of Nonlinear Representation Learning and Its Application. (arXiv:2106.14836v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.14836">
<div class="article-summary-box-inner">
<span><p>Representations of the world environment play a crucial role in artificial
intelligence. It is often inefficient to conduct reasoning and inference
directly in the space of raw sensory representations, such as pixel values of
images. Representation learning allows us to automatically discover suitable
representations from raw sensory data. For example, given raw sensory data, a
deep neural network learns nonlinear representations at its hidden layers,
which are subsequently used for classification at its output layer. This
happens implicitly during training through minimizing a supervised or
unsupervised loss. In this paper, we study the dynamics of such implicit
nonlinear representation learning. We identify a pair of a new assumption and a
novel condition, called the common model structure assumption and the
data-architecture alignment condition. Under the common model structure
assumption, the data-architecture alignment condition is shown to be sufficient
for the global convergence and necessary for the global optimality. Moreover,
our theory explains how and when increasing the network size does and does not
improve the training behaviors in the practical regime. Our results provide
practical guidance for designing a model structure: e.g., the common model
structure assumption can be used as a justification for using a particular
model structure instead of others. We also derive a new training framework,
which satisfies the data-architecture alignment condition by automatically
modifying any given training algorithm. Given a standard training algorithm,
the framework running its modified version is empirically shown to maintain
competitive test performances while providing global convergence guarantees for
deep residual neural networks with convolutions, skip connections, and batch
normalization with datasets, including MNIST, CIFAR-10, CIFAR-100, Semeion,
KMNIST and SVHN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Long-Short Ensemble Network for Bipolar Manic-Euthymic State Recognition Based on Wrist-worn Sensors. (arXiv:2107.00710v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.00710">
<div class="article-summary-box-inner">
<span><p>Manic episodes of bipolar disorder can lead to uncritical behaviour and
delusional psychosis, often with destructive consequences for those affected
and their surroundings. Early detection and intervention of a manic episode are
crucial to prevent escalation, hospital admission and premature death. However,
people with bipolar disorder may not recognize that they are experiencing a
manic episode and symptoms such as euphoria and increased productivity can also
deter affected individuals from seeking help. This work proposes to perform
user-independent, automatic mood-state detection based on actigraphy and
electrodermal activity acquired from a wrist-worn device during mania and after
recovery (euthymia). This paper proposes a new deep learning-based ensemble
method leveraging long (20h) and short (5 minutes) time-intervals to
discriminate between the mood-states. When tested on 47 bipolar patients, the
proposed classification scheme achieves an average accuracy of 91.59% in
euthymic/manic mood-state recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NTIRE 2021 Multi-modal Aerial View Object Classification Challenge. (arXiv:2107.01189v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.01189">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce the first Challenge on Multi-modal Aerial View
Object Classification (MAVOC) in conjunction with the NTIRE 2021 workshop at
CVPR. This challenge is composed of two different tracks using EO andSAR
imagery. Both EO and SAR sensors possess different advantages and drawbacks.
The purpose of this competition is to analyze how to use both sets of sensory
information in complementary ways. We discuss the top methods submitted for
this competition and evaluate their results on our blind test set. Our
challenge results show significant improvement of more than 15% accuracy from
our current baselines for each track of the competition
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detect and Locate: Exposing Face Manipulation by Semantic- and Noise-level Telltales. (arXiv:2107.05821v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.05821">
<div class="article-summary-box-inner">
<span><p>The technological advancements of deep learning have enabled sophisticated
face manipulation schemes, raising severe trust issues and security concerns in
modern society. Generally speaking, detecting manipulated faces and locating
the potentially altered regions are challenging tasks. Herein, we propose a
conceptually simple but effective method to efficiently detect forged faces in
an image while simultaneously locating the manipulated regions. The proposed
scheme relies on a segmentation map that delivers meaningful high-level
semantic information clues about the image. Furthermore, a noise map is
estimated, playing a complementary role in capturing low-level clues and
subsequently empowering decision-making. Finally, the features from these two
modules are combined to distinguish fake faces. Extensive experiments show that
the proposed model achieves state-of-the-art detection accuracy and remarkable
localization performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accelerating Video Object Segmentation with Compressed Video. (arXiv:2107.12192v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.12192">
<div class="article-summary-box-inner">
<span><p>We propose an efficient plug-and-play acceleration framework for
semi-supervised video object segmentation by exploiting the temporal
redundancies in videos presented by the compressed bitstream. Specifically, we
propose a motion vector-based warping method for propagating segmentation masks
from keyframes to other frames in a bi-directional and multi-hop manner.
Additionally, we introduce a residual-based correction module that can fix
wrongly propagated segmentation masks from noisy or erroneous motion vectors.
Our approach is flexible and can be added on top of several existing video
object segmentation algorithms. We achieved highly competitive results on
DAVIS17 and YouTube-VOS on various base models with substantial speed-ups of up
to 3.5X with minor drops in accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WORD: A large scale dataset, benchmark and clinical applicable study for abdominal organ segmentation from CT image. (arXiv:2111.02403v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02403">
<div class="article-summary-box-inner">
<span><p>Whole abdominal organ segmentation plays an important role in diagnosing
abdomen lesions, radiotherapy, and follow-up. However, oncologists delineating
all abdominal organs is time-consuming and very expensive. Recently, deep
learning-based medical image segmentation has shown the potential to reduce
manual delineation efforts, but it still requires a large-scale fine annotated
dataset for training. Although many efforts in this task, there are still few
large image datasets covering the whole abdomen region with accurate and
detailed annotations for the whole abdominal organ segmentation. In this work,
we establish a large-scale \textit{W}hole abdominal \textit{OR}gan
\textit{D}ataset (\textit{WORD}) for algorithms research and clinical
applications development. This dataset contains 150 abdominal CT volumes (30495
slices). Each volume has 16 organs with fine pixel-level annotations and
scribble-based sparse annotation, which may be the largest dataset with whole
abdominal organ annotation. Several state-of-the-art segmentation methods are
evaluated on this dataset. And we also invited three clinical oncologists to
revise the model predictions to measure the gap between the deep learning
method and three oncologists. Afterwards, we investigate the
inference-efficiently learning on the WORD dataset, as the high-resolution
image requires large GPU memory and inference time in the test stage. We
further evaluate the scribble-based annotation-efficient learning on this
dataset, as the pixel-wise manual annotation is time-consuming and expensive.
The work provided a new benchmark for the abdominal multi-organ segmentation
task, and these experiments can serve as the baseline for future research and
clinical application development. The codebase and dataset is released at:
\url{https://github.com/HiLab-git/WORD}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">It's About Time: Analog Clock Reading in the Wild. (arXiv:2111.09162v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.09162">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a framework for reading analog clocks in natural
images or videos. Specifically, we make the following contributions: First, we
create a scalable pipeline for generating synthetic clocks, significantly
reducing the requirements for the labour-intensive annotations; Second, we
introduce a clock recognition architecture based on spatial transformer
networks (STN), which is trained end-to-end for clock alignment and
recognition. We show that the model trained on the proposed synthetic dataset
generalises towards real clocks with good accuracy, advocating a Sim2Real
training regime; Third, to further reduce the gap between simulation and real
data, we leverage the special property of "time", i.e.uniformity, to generate
reliable pseudo-labels on real unlabelled clock videos, and show that training
on these videos offers further improvements while still requiring zero manual
annotations. Lastly, we introduce three benchmark datasets based on COCO, Open
Images, and The Clock movie, with full annotations for time, accurate to the
minute.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">L-Verse: Bidirectional Generation Between Image and Text. (arXiv:2111.11133v11 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11133">
<div class="article-summary-box-inner">
<span><p>Far beyond learning long-range interactions of natural language, transformers
are becoming the de-facto standard for many vision tasks with their power and
scalability. Especially with cross-modal tasks between image and text, vector
quantized variational autoencoders (VQ-VAEs) are widely used to make a raw RGB
image into a sequence of feature vectors. To better leverage the correlation
between image and text, we propose L-Verse, a novel architecture consisting of
feature-augmented variational autoencoder (AugVAE) and bidirectional
auto-regressive transformer (BiART) for image-to-text and text-to-image
generation. Our AugVAE shows the state-of-the-art reconstruction performance on
ImageNet1K validation set, along with the robustness to unseen images in the
wild. Unlike other models, BiART can distinguish between image (or text) as a
conditional reference and a generation target. L-Verse can be directly used for
image-to-text or text-to-image generation without any finetuning or extra
object detection framework. In quantitative and qualitative experiments,
L-Verse shows impressive results against previous methods in both image-to-text
and text-to-image generation on MS-COCO Captions. We furthermore assess the
scalability of L-Verse architecture on Conceptual Captions and present the
initial result of bidirectional vision-language representation learning on
general domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Fields in Visual Computing and Beyond. (arXiv:2111.11426v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11426">
<div class="article-summary-box-inner">
<span><p>Recent advances in machine learning have created increasing interest in
solving visual computing problems using a class of coordinate-based neural
networks that parametrize physical properties of scenes or objects across space
and time. These methods, which we call neural fields, have seen successful
application in the synthesis of 3D shapes and image, animation of human bodies,
3D reconstruction, and pose estimation. However, due to rapid progress in a
short time, many papers exist but a comprehensive review and formulation of the
problem has not yet emerged. In this report, we address this limitation by
providing context, mathematical grounding, and an extensive review of
literature on neural fields. This report covers research along two dimensions.
In Part I, we focus on techniques in neural fields by identifying common
components of neural field methods, including different representations,
architectures, forward mapping, and generalization methods. In Part II, we
focus on applications of neural fields to different problems in visual
computing, and beyond (e.g., robotics, audio). Our review shows the breadth of
topics already covered in visual computing, both historically and in current
incarnations, demonstrating the improved quality, flexibility, and capability
brought by neural fields methods. Finally, we present a companion website that
contributes a living version of this review that can be continually updated by
the community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-invasive hemodynamic analysis for aortic regurgitation using computational fluid dynamics and deep learning. (arXiv:2111.11660v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11660">
<div class="article-summary-box-inner">
<span><p>Changes in cardiovascular hemodynamics are closely related to the development
of aortic regurgitation, a type of valvular heart disease. Metrics derived from
blood flows are used to indicate aortic regurgitation onset and evaluate its
severity. These metrics can be non-invasively obtained using four-dimensional
(4D) flow magnetic resonance imaging (MRI), where accuracy is primarily
dependent on spatial resolution. However, insufficient resolution often results
from limitations in 4D flow MRI and complex aortic regurgitation hemodynamics.
To address this, computational fluid dynamics simulations were transformed into
synthetic 4D flow MRI data and used to train a variety of neural networks.
These networks generated super resolution, full-field phase images with an
upsample factor of 4. Results showed decreased velocity error, high structural
similarity scores, and improved learning capabilities from previous work.
Further validation was performed on two sets of in-vivo 4D flow MRI data and
demonstrated success in de-noising flow images. This approach presents an
opportunity to comprehensively analyse aortic regurgitation hemodynamics in a
non-invasive manner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Image Patch is a Wave: Phase-Aware Vision MLP. (arXiv:2111.12294v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12294">
<div class="article-summary-box-inner">
<span><p>In the field of computer vision, recent works show that a pure MLP
architecture mainly stacked by fully-connected layers can achieve competing
performance with CNN and transformer. An input image of vision MLP is usually
split into multiple tokens (patches), while the existing MLP models directly
aggregate them with fixed weights, neglecting the varying semantic information
of tokens from different images. To dynamically aggregate tokens, we propose to
represent each token as a wave function with two parts, amplitude and phase.
Amplitude is the original feature and the phase term is a complex value
changing according to the semantic contents of input images. Introducing the
phase term can dynamically modulate the relationship between tokens and fixed
weights in MLP. Based on the wave-like token representation, we establish a
novel Wave-MLP architecture for vision tasks. Extensive experiments demonstrate
that the proposed Wave-MLP is superior to the state-of-the-art MLP
architectures on various vision tasks such as image classification, object
detection and semantic segmentation. The source code is available at
https://github.com/huawei-noah/CV-Backbones/tree/master/wavemlp_pytorch and
https://gitee.com/mindspore/models/tree/master/research/cv/wave_mlp.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UDA-COPE: Unsupervised Domain Adaptation for Category-level Object Pose Estimation. (arXiv:2111.12580v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12580">
<div class="article-summary-box-inner">
<span><p>Learning to estimate object pose often requires ground-truth (GT) labels,
such as CAD model and absolute-scale object pose, which is expensive and
laborious to obtain in the real world. To tackle this problem, we propose an
unsupervised domain adaptation (UDA) for category-level object pose estimation,
called UDA-COPE. Inspired by recent multi-modal UDA techniques, the proposed
method exploits a teacher-student self-supervised learning scheme to train a
pose estimation network without using target domain pose labels. We also
introduce a bidirectional filtering method between the predicted normalized
object coordinate space (NOCS) map and observed point cloud, to not only make
our teacher network more robust to the target domain but also to provide more
reliable pseudo labels for the student network training. Extensive experimental
results demonstrate the effectiveness of our proposed method both
quantitatively and qualitatively. Notably, without leveraging target-domain GT
labels, our proposed method achieved comparable or sometimes superior
performance to existing methods that depend on the GT labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeRFReN: Neural Radiance Fields with Reflections. (arXiv:2111.15234v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.15234">
<div class="article-summary-box-inner">
<span><p>Neural Radiance Fields (NeRF) has achieved unprecedented view synthesis
quality using coordinate-based neural scene representations. However, NeRF's
view dependency can only handle simple reflections like highlights but cannot
deal with complex reflections such as those from glass and mirrors. In these
scenarios, NeRF models the virtual image as real geometries which leads to
inaccurate depth estimation, and produces blurry renderings when the multi-view
consistency is violated as the reflected objects may only be seen under some of
the viewpoints. To overcome these issues, we introduce NeRFReN, which is built
upon NeRF to model scenes with reflections. Specifically, we propose to split a
scene into transmitted and reflected components, and model the two components
with separate neural radiance fields. Considering that this decomposition is
highly under-constrained, we exploit geometric priors and apply
carefully-designed training strategies to achieve reasonable decomposition
results. Experiments on various self-captured scenes show that our method
achieves high-quality novel view synthesis and physically sound depth
estimation results while enabling scene editing applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PolyWorld: Polygonal Building Extraction with Graph Neural Networks in Satellite Images. (arXiv:2111.15491v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.15491">
<div class="article-summary-box-inner">
<span><p>While most state-of-the-art instance segmentation methods produce binary
segmentation masks, geographic and cartographic applications typically require
precise vector polygons of extracted objects instead of rasterized output. This
paper introduces PolyWorld, a neural network that directly extracts building
vertices from an image and connects them correctly to create precise polygons.
The model predicts the connection strength between each pair of vertices using
a graph neural network and estimates the assignments by solving a
differentiable optimal transport problem. Moreover, the vertex positions are
optimized by minimizing a combined segmentation and polygonal angle difference
loss. PolyWorld significantly outperforms the state of the art in building
polygonization and achieves not only notable quantitative results, but also
produces visually pleasing building polygons. Code and trained weights are
publicly available at https://github.com/zorzi-s/PolyWorldPretrainedNetwork.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LAVT: Language-Aware Vision Transformer for Referring Image Segmentation. (arXiv:2112.02244v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02244">
<div class="article-summary-box-inner">
<span><p>Referring image segmentation is a fundamental vision-language task that aims
to segment out an object referred to by a natural language expression from an
image. One of the key challenges behind this task is leveraging the referring
expression for highlighting relevant positions in the image. A paradigm for
tackling this problem is to leverage a powerful vision-language ("cross-modal")
decoder to fuse features independently extracted from a vision encoder and a
language encoder. Recent methods have made remarkable advancements in this
paradigm by exploiting Transformers as cross-modal decoders, concurrent to the
Transformer's overwhelming success in many other vision-language tasks.
Adopting a different approach in this work, we show that significantly better
cross-modal alignments can be achieved through the early fusion of linguistic
and visual features in intermediate layers of a vision Transformer encoder
network. By conducting cross-modal feature fusion in the visual feature
encoding stage, we can leverage the well-proven correlation modeling power of a
Transformer encoder for excavating helpful multi-modal context. This way,
accurate segmentation results are readily harvested with a light-weight mask
predictor. Without bells and whistles, our method surpasses the previous
state-of-the-art methods on RefCOCO, RefCOCO+, and G-Ref by large margins.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Watch It Move: Unsupervised Discovery of 3D Joints for Re-Posing of Articulated Objects. (arXiv:2112.11347v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11347">
<div class="article-summary-box-inner">
<span><p>Rendering articulated objects while controlling their poses is critical to
applications such as virtual reality or animation for movies. Manipulating the
pose of an object, however, requires the understanding of its underlying
structure, that is, its joints and how they interact with each other.
Unfortunately, assuming the structure to be known, as existing methods do,
precludes the ability to work on new object categories. We propose to learn
both the appearance and the structure of previously unseen articulated objects
by observing them move from multiple views, with no joints annotation
supervision, or information about the structure. We observe that 3D points that
are static relative to one another should belong to the same part, and that
adjacent parts that move relative to each other must be connected by a joint.
To leverage this insight, we model the object parts in 3D as ellipsoids, which
allows us to identify joints. We combine this explicit representation with an
implicit one that compensates for the approximation introduced. We show that
our method works for different structures, from quadrupeds, to single-arm
robots, to humans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross Modal Retrieval with Querybank Normalisation. (arXiv:2112.12777v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.12777">
<div class="article-summary-box-inner">
<span><p>Profiting from large-scale training datasets, advances in neural architecture
design and efficient inference, joint embeddings have become the dominant
approach for tackling cross-modal retrieval. In this work we first show that,
despite their effectiveness, state-of-the-art joint embeddings suffer
significantly from the longstanding "hubness problem" in which a small number
of gallery embeddings form the nearest neighbours of many queries. Drawing
inspiration from the NLP literature, we formulate a simple but effective
framework called Querybank Normalisation (QB-Norm) that re-normalises query
similarities to account for hubs in the embedding space. QB-Norm improves
retrieval performance without requiring retraining. Differently from prior
work, we show that QB-Norm works effectively without concurrent access to any
test set queries. Within the QB-Norm framework, we also propose a novel
similarity normalisation method, the Dynamic Inverted Softmax, that is
significantly more robust than existing approaches. We showcase QB-Norm across
a range of cross modal retrieval models and benchmarks where it consistently
enhances strong baselines beyond the state of the art. Code is available at
https://vladbogo.github.io/QB-Norm/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Deep Image Matting via Local Smoothness Assumption. (arXiv:2112.13809v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13809">
<div class="article-summary-box-inner">
<span><p>Natural image matting is a fundamental and challenging computer vision task.
Conventionally, the problem is formulated as an underconstrained problem. Since
the problem is ill-posed, further assumptions on the data distribution are
required to make the problem well-posed. For classical matting methods, a
commonly adopted assumption is the local smoothness assumption on foreground
and background colors. However, the use of such assumptions was not
systematically considered for deep learning based matting methods. In this
work, we consider two local smoothness assumptions which can help improving
deep image matting models. Based on the local smoothness assumptions, we
propose three techniques, i.e., training set refinement, color augmentation and
backpropagating refinement, which can improve the performance of the deep image
matting model significantly. We conduct experiments to examine the
effectiveness of the proposed algorithm. The experimental results show that the
proposed method has favorable performance compared with existing matting
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal registration of FISH and nanoSIMS images using convolutional neural network models. (arXiv:2201.05545v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05545">
<div class="article-summary-box-inner">
<span><p>Nanoscale secondary ion mass spectrometry (nanoSIMS) and fluorescence in situ
hybridization (FISH) microscopy provide high-resolution, multimodal image
representations of the identity and cell activity respectively of targeted
microbial communities in microbiological research. Despite its importance to
microbiologists, multimodal registration of FISH and nanoSIMS images is
challenging given the morphological distortion and background noise in both
images. In this study, we use convolutional neural networks (CNNs) for
multiscale feature extraction, shape context for computation of the minimum
transformation cost feature matching and the thin-plate spline (TPS) model for
multimodal registration of the FISH and nanoSIMS images. Registration accuracy
was quantitatively assessed against manually registered images, at both, the
pixel and structural levels using standard metrics. Although all six tested CNN
models performed well, ResNet18 was observed to outperform VGG16, VGG19,
GoogLeNet and ShuffleNet and ResNet101 based on most metrics. This study
demonstrates the utility of CNNs in the registration of multimodal images with
significant background noise and morphology distortion. We also show aggregate
shape, preserved by binarization, to be a robust feature for registering
multimodal microbiology-related images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">vCLIMB: A Novel Video Class Incremental Learning Benchmark. (arXiv:2201.09381v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09381">
<div class="article-summary-box-inner">
<span><p>Continual learning (CL) is under-explored in the video domain. The few
existing works contain splits with imbalanced class distributions over the
tasks, or study the problem in unsuitable datasets. We introduce vCLIMB, a
novel video continual learning benchmark. vCLIMB is a standardized test-bed to
analyze catastrophic forgetting of deep models in video continual learning. In
contrast to previous work, we focus on class incremental continual learning
with models trained on a sequence of disjoint tasks, and distribute the number
of classes uniformly across the tasks. We perform in-depth evaluations of
existing CL methods in vCLIMB, and observe two unique challenges in video data.
The selection of instances to store in episodic memory is performed at the
frame level. Second, untrimmed training data influences the effectiveness of
frame sampling strategies. We address these two challenges by proposing a
temporal consistency regularization that can be applied on top of memory-based
continual learning methods. Our approach significantly improves the baseline,
by up to 24% on the untrimmed continual learning task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Recognition and Digital Documentation of Cultural Heritage Hemispherical Domes using Images. (arXiv:2201.10015v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10015">
<div class="article-summary-box-inner">
<span><p>Advancements in optical metrology has enabled documentation of dense 3D point
clouds of cultural heritage sites. For large scale and continuous digital
documentation, processing of dense 3D point clouds becomes computationally
cumbersome, and often requires additional hardware for data management,
increasing the time cost, and complexity of projects. To this end, this
manuscript presents an original approach to generate fast and reliable semantic
digital models of heritage hemispherical domes using only two images. New
closed formulations were derived to establish the relationships between spheres
and their projected ellipses onto images, which fostered the development of a
new automatic framework for as-built generation of spheres. The effectiveness
of the proposed method was evaluated under both laboratory and real-world
datasets. The results revealed that the proposed method achieved as-built
modeling accuracy of around 6mm, while improving the computation time by a
factor of 7, when compared to established point cloud processing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast Online Video Super-Resolution with Deformable Attention Pyramid. (arXiv:2202.01731v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01731">
<div class="article-summary-box-inner">
<span><p>Video super-resolution (VSR) has many applications that pose strict causal,
real-time, and latency constraints, including video streaming and TV. We
address the VSR problem under these settings, which poses additional important
challenges since information from future frames is unavailable. Importantly,
designing efficient, yet effective frame alignment and fusion modules remain
central problems. In this work, we propose a recurrent VSR architecture based
on a deformable attention pyramid (DAP). Our DAP aligns and integrates
information from the recurrent state into the current frame prediction. To
circumvent the computational cost of traditional attention-based methods, we
only attend to a limited number of spatial locations, which are dynamically
predicted by the DAP. Comprehensive experiments and analysis of the proposed
key innovations show the effectiveness of our approach. We significantly reduce
processing time and computational complexity in comparison to state-of-the-art
methods, while maintaining a high performance. We surpass state-of-the-art
method EDVR-M on two standard benchmarks with a speed-up of over $3\times$.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-direction and Multi-scale Pyramid in Transformer for Video-based Pedestrian Retrieval. (arXiv:2202.06014v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06014">
<div class="article-summary-box-inner">
<span><p>In video surveillance, pedestrian retrieval (also called person
re-identification) is a critical task. This task aims to retrieve the
pedestrian of interest from non-overlapping cameras. Recently,
transformer-based models have achieved significant progress for this task.
However, these models still suffer from ignoring fine-grained, part-informed
information. This paper proposes a multi-direction and multi-scale Pyramid in
Transformer (PiT) to solve this problem. In transformer-based architecture,
each pedestrian image is split into many patches. Then, these patches are fed
to transformer layers to obtain the feature representation of this image. To
explore the fine-grained information, this paper proposes to apply vertical
division and horizontal division on these patches to generate
different-direction human parts. These parts provide more fine-grained
information. To fuse multi-scale feature representation, this paper presents a
pyramid structure containing global-level information and many pieces of
local-level information from different scales. The feature pyramids of all the
pedestrian images from the same video are fused to form the final
multi-direction and multi-scale feature representation. Experimental results on
two challenging video-based benchmarks, MARS and iLIDS-VID, show the proposed
PiT achieves state-of-the-art performance. Extensive ablation studies
demonstrate the superiority of the proposed pyramid structure. The code is
available at https://git.openi.org.cn/zangxh/PiT.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">X-Trans2Cap: Cross-Modal Knowledge Transfer using Transformer for 3D Dense Captioning. (arXiv:2203.00843v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00843">
<div class="article-summary-box-inner">
<span><p>3D dense captioning aims to describe individual objects by natural language
in 3D scenes, where 3D scenes are usually represented as RGB-D scans or point
clouds. However, only exploiting single modal information, e.g., point cloud,
previous approaches fail to produce faithful descriptions. Though aggregating
2D features into point clouds may be beneficial, it introduces an extra
computational burden, especially in inference phases. In this study, we
investigate a cross-modal knowledge transfer using Transformer for 3D dense
captioning, X-Trans2Cap, to effectively boost the performance of single-modal
3D caption through knowledge distillation using a teacher-student framework. In
practice, during the training phase, the teacher network exploits auxiliary 2D
modality and guides the student network that only takes point clouds as input
through the feature consistency constraints. Owing to the well-designed
cross-modal feature fusion module and the feature alignment in the training
phase, X-Trans2Cap acquires rich appearance information embedded in 2D images
with ease. Thus, a more faithful caption can be generated only using point
clouds during the inference. Qualitative and quantitative results confirm that
X-Trans2Cap outperforms previous state-of-the-art by a large margin, i.e.,
about +21 and about +16 absolute CIDEr score on ScanRefer and Nr3D datasets,
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SkinningNet: Two-Stream Graph Convolutional Neural Network for Skinning Prediction of Synthetic Characters. (arXiv:2203.04746v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04746">
<div class="article-summary-box-inner">
<span><p>This work presents SkinningNet, an end-to-end Two-Stream Graph Neural Network
architecture that computes skinning weights from an input mesh and its
associated skeleton, without making any assumptions on shape class and
structure of the provided mesh. Whereas previous methods pre-compute
handcrafted features that relate the mesh and the skeleton or assume a fixed
topology of the skeleton, the proposed method extracts this information in an
end-to-end learnable fashion by jointly learning the best relationship between
mesh vertices and skeleton joints. The proposed method exploits the benefits of
the novel Multi-Aggregator Graph Convolution that combines the results of
different aggregators during the summarizing step of the Message-Passing
scheme, helping the operation to generalize for unseen topologies. Experimental
results demonstrate the effectiveness of the contributions of our novel
architecture, with SkinningNet outperforming current state-of-the-art
alternatives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Patch-Fool: Are Vision Transformers Always Robust Against Adversarial Perturbations?. (arXiv:2203.08392v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08392">
<div class="article-summary-box-inner">
<span><p>Vision transformers (ViTs) have recently set off a new wave in neural
architecture design thanks to their record-breaking performance in various
vision tasks. In parallel, to fulfill the goal of deploying ViTs into
real-world vision applications, their robustness against potential malicious
attacks has gained increasing attention. In particular, recent works show that
ViTs are more robust against adversarial attacks as compared with convolutional
neural networks (CNNs), and conjecture that this is because ViTs focus more on
capturing global interactions among different input/feature patches, leading to
their improved robustness to local perturbations imposed by adversarial
attacks. In this work, we ask an intriguing question: "Under what kinds of
perturbations do ViTs become more vulnerable learners compared to CNNs?" Driven
by this question, we first conduct a comprehensive experiment regarding the
robustness of both ViTs and CNNs under various existing adversarial attacks to
understand the underlying reason favoring their robustness. Based on the drawn
insights, we then propose a dedicated attack framework, dubbed Patch-Fool, that
fools the self-attention mechanism by attacking its basic component (i.e., a
single patch) with a series of attention-aware optimization techniques.
Interestingly, our Patch-Fool framework shows for the first time that ViTs are
not necessarily more robust than CNNs against adversarial perturbations. In
particular, we find that ViTs are more vulnerable learners compared with CNNs
against our Patch-Fool attack which is consistent across extensive experiments,
and the observations from Sparse/Mild Patch-Fool, two variants of Patch-Fool,
indicate an intriguing insight that the perturbation density and strength on
each patch seem to be the key factors that influence the robustness ranking
between ViTs and CNNs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Medium Transmission Map Matters for Learning to Restore Real-World Underwater Images. (arXiv:2203.09414v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09414">
<div class="article-summary-box-inner">
<span><p>Underwater visual perception is essentially important for underwater
exploration, archeology, ecosystem and so on. The low illumination, light
reflections, scattering, absorption and suspended particles inevitably lead to
the critically degraded underwater image quality, which causes great challenges
on recognizing the objects from the underwater images. The existing underwater
enhancement methods that aim to promote the underwater visibility, heavily
suffer from the poor image restoration performance and generalization ability.
To reduce the difficulty of underwater image enhancement, we introduce the
media transmission map as guidance to assist in image enhancement. We formulate
the interaction between the underwater visual images and the transmission map
to obtain better enhancement results. Even with simple and lightweight network
configuration, the proposed method can achieve advanced results of 22.6 dB on
the challenging Test-R90 with an impressive 30 times faster than the existing
models. Comprehensive experimental results have demonstrated the superiority
and potential on underwater perception. Paper's code is offered on:
https://github.com/GroupG-yk/MTUR-Net.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Expression Classification using Concatenation of Deep Neural Network for the 3rd ABAW3 Competition. (arXiv:2203.12899v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12899">
<div class="article-summary-box-inner">
<span><p>For computers to recognize human emotions, expression classification is an
equally important problem in the human-computer interaction area. In the 3rd
Affective Behavior Analysis In-The-Wild competition, the task of expression
classification includes eight classes with six basic expressions of human faces
from videos. In this paper, we employ a transformer mechanism to encode the
robust representation from the backbone. Fusion of the robust representations
plays an important role in the expression classification task. Our approach
achieves 30.35\% and 28.60\% for the $F_1$ score on the validation set and the
test set, respectively. This result shows the effectiveness of the proposed
architecture based on the Aff-Wild2 dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CenterLoc3D: Monocular 3D Vehicle Localization Network for Roadside Surveillance Cameras. (arXiv:2203.14550v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14550">
<div class="article-summary-box-inner">
<span><p>Monocular 3D vehicle localization is an important task in Intelligent
Transportation System (ITS) and Cooperative Vehicle Infrastructure System
(CVIS), which is usually achieved by monocular 3D vehicle detection. However,
depth information cannot be obtained directly by monocular cameras due to the
inherent imaging mechanism, resulting in more challenging monocular 3D tasks.
Most of the current monocular 3D vehicle detection methods leverage 2D
detectors and additional geometric modules, which reduces the efficiency. In
this paper, we propose a 3D vehicle localization network CenterLoc3D for
roadside monocular cameras, which directly predicts centroid and eight vertexes
in image space, and the dimension of 3D bounding boxes without 2D detectors. To
improve the precision of 3D vehicle localization, we propose a weighted-fusion
module and a loss with spatial constraints embedded in CenterLoc3D. Firstly,
the transformation matrix between 2D image space and 3D world space is solved
by camera calibration. Secondly, vehicle type, centroid, eight vertexes, and
the dimension of 3D vehicle bounding boxes are obtained by CenterLoc3D.
Finally, centroid in 3D world space can be obtained by camera calibration and
CenterLoc3D for 3D vehicle localization. To the best of our knowledge, this is
the first application of 3D vehicle localization for roadside monocular
cameras. Hence, we also propose a benchmark for this application including a
dataset (SVLD-3D), an annotation tool (LabelImg-3D), and evaluation metrics.
Through experimental validation, the proposed method achieves high accuracy and
real-time performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Mechanisms Inspired Efficient Transformers for Image and Video Quality Assessment. (arXiv:2203.14557v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.14557">
<div class="article-summary-box-inner">
<span><p>Visual (image, video) quality assessments can be modelled by visual features
in different domains, e.g., spatial, frequency, and temporal domains.
Perceptual mechanisms in the human visual system (HVS) play a crucial role in
generation of quality perception. This paper proposes a general framework for
no-reference visual quality assessment using efficient windowed transformer
architectures. A lightweight module for multi-stage channel attention is
integrated into Swin (shifted window) Transformer. Such module can represent
appropriate perceptual mechanisms in image quality assessment (IQA) to build an
accurate IQA model. Meanwhile, representative features for image quality
perception in the spatial and frequency domains can also be derived from the
IQA model, which are then fed into another windowed transformer architecture
for video quality assessment (VQA). The VQA model efficiently reuses attention
information across local windows to tackle the issue of expensive time and
memory complexities of original transformer. Experimental results on both
large-scale IQA and VQA databases demonstrate that the proposed quality
assessment models outperform other state-of-the-art models by large margins.
The complete source code will be published on Github.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Classification of Hyperspectral Images Using SVM with Shape-adaptive Reconstruction and Smoothed Total Variation. (arXiv:2203.15619v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15619">
<div class="article-summary-box-inner">
<span><p>In this work, a novel algorithm called SVM with Shape-adaptive Reconstruction
and Smoothed Total Variation (SaR-SVM-STV) is introduced to classify
hyperspectral images, which makes full use of spatial and spectral information.
The Shape-adaptive Reconstruction (SaR) is introduced to preprocess each pixel
based on the Pearson Correlation between pixels in its shape-adaptive (SA)
region. Support Vector Machines (SVMs) are trained to estimate the pixel-wise
probability maps of each class. Then the Smoothed Total Variation (STV) model
is applied to denoise and generate the final classification map. Experiments
show that SaR-SVM-STV outperforms the SVM-STV method with a few training
labels, demonstrating the significance of reconstructing hyperspectral images
before classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unified Framework for Domain Adaptive Pose Estimation. (arXiv:2204.00172v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.00172">
<div class="article-summary-box-inner">
<span><p>While pose estimation is an important computer vision task, it requires
expensive annotation and suffers from domain shift. In this paper, we
investigate the problem of domain adaptive 2D pose estimation that transfers
knowledge learned on a synthetic source domain to a target domain without
supervision. While several domain adaptive pose estimation models have been
proposed recently, they are not generic but only focus on either human pose or
animal pose estimation, and thus their effectiveness is somewhat limited to
specific scenarios. In this work, we propose a unified framework that
generalizes well on various domain adaptive pose estimation problems. We
propose to align representations using both input-level and output-level cues
(pixels and pose labels, respectively), which facilitates the knowledge
transfer from the source domain to the unlabeled target domain. Our experiments
show that our method achieves state-of-the-art performance under various domain
shifts. Our method outperforms existing baselines on human pose estimation by
up to 4.5 percent points (pp), hand pose estimation by up to 7.4 pp, and animal
pose estimation by up to 4.8 pp for dogs and 3.3 pp for sheep. These results
suggest that our method is able to mitigate domain shift on diverse tasks and
even unseen domains and objects (e.g., trained on horse and tested on dog).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MRI-based Multi-task Decoupling Learning for Alzheimer's Disease Detection and MMSE Score Prediction: A Multi-site Validation. (arXiv:2204.01708v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01708">
<div class="article-summary-box-inner">
<span><p>Accurately detecting Alzheimer's disease (AD) and predicting mini-mental
state examination (MMSE) score are important tasks in elderly health by
magnetic resonance imaging (MRI). Most of the previous methods on these two
tasks are based on single-task learning and rarely consider the correlation
between them. Since the MMSE score, which is an important basis for AD
diagnosis, can also reflect the progress of cognitive impairment, some studies
have begun to apply multi-task learning methods to these two tasks. However,
how to exploit feature correlation remains a challenging problem for these
methods. To comprehensively address this challenge, we propose a MRI-based
multi-task decoupled learning method for AD detection and MMSE score
prediction. First, a multi-task learning network is proposed to implement AD
detection and MMSE score prediction, which exploits feature correlation by
adding three multi-task interaction layers between the backbones of the two
tasks. Each multi-task interaction layer contains two feature decoupling
modules and one feature interaction module. Furthermore, to enhance the
generalization between tasks of the features selected by the feature decoupling
module, we propose the feature consistency loss constrained feature decoupling
module. Finally, in order to exploit the specific distribution information of
MMSE score in different groups, a distribution loss is proposed to further
enhance the model performance. We evaluate our proposed method on multi-site
datasets. Experimental results show that our proposed multi-task decoupled
representation learning method achieves good performance, outperforming
single-task learning and other existing state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Explaining Multimodal Hateful Meme Detection Models. (arXiv:2204.01734v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01734">
<div class="article-summary-box-inner">
<span><p>Hateful meme detection is a new multimodal task that has gained significant
traction in academic and industry research communities. Recently, researchers
have applied pre-trained visual-linguistic models to perform the multimodal
classification task, and some of these solutions have yielded promising
results. However, what these visual-linguistic models learn for the hateful
meme classification task remains unclear. For instance, it is unclear if these
models are able to capture the derogatory or slurs references in multimodality
(i.e., image and text) of the hateful memes. To fill this research gap, this
paper propose three research questions to improve our understanding of these
visual-linguistic models performing the hateful meme classification task. We
found that the image modality contributes more to the hateful meme
classification task, and the visual-linguistic models are able to perform
visual-text slurs grounding to a certain extent. Our error analysis also shows
that the visual-linguistic models have acquired biases, which resulted in
false-positive predictions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified Implicit Neural Stylization. (arXiv:2204.01943v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01943">
<div class="article-summary-box-inner">
<span><p>Representing visual signals by implicit representation (e.g., a coordinate
based deep network) has prevailed among many vision tasks. This work explores a
new intriguing direction: training a stylized implicit representation, using a
generalized approach that can apply to various 2D and 3D scenarios. We conduct
a pilot study on a variety of implicit functions, including 2D coordinate-based
representation, neural radiance field, and signed distance function. Our
solution is a Unified Implicit Neural Stylization framework, dubbed INS. In
contrary to vanilla implicit representation, INS decouples the ordinary
implicit function into a style implicit module and a content implicit module,
in order to separately encode the representations from the style image and
input scenes. An amalgamation module is then applied to aggregate these
information and synthesize the stylized output. To regularize the geometry in
3D scenes, we propose a novel self-distillation geometry consistency loss which
preserves the geometry fidelity of the stylized scenes. Comprehensive
experiments are conducted on multiple task settings, including novel view
synthesis of complex scenes, stylization for implicit surfaces, and fitting
images using MLPs. We further demonstrate that the learned representation is
continuous not only spatially but also style-wise, leading to effortlessly
interpolating between different styles and generating images with new mixed
styles. Please refer to the video on our project page for more view synthesis
results: https://zhiwenfan.github.io/INS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-Local Latent Relation Distillation for Self-Adaptive 3D Human Pose Estimation. (arXiv:2204.01971v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01971">
<div class="article-summary-box-inner">
<span><p>Available 3D human pose estimation approaches leverage different forms of
strong (2D/3D pose) or weak (multi-view or depth) paired supervision. Barring
synthetic or in-studio domains, acquiring such supervision for each new target
environment is highly inconvenient. To this end, we cast 3D pose learning as a
self-supervised adaptation problem that aims to transfer the task knowledge
from a labeled source domain to a completely unpaired target. We propose to
infer image-to-pose via two explicit mappings viz. image-to-latent and
latent-to-pose where the latter is a pre-learned decoder obtained from a
prior-enforcing generative adversarial auto-encoder. Next, we introduce
relation distillation as a means to align the unpaired cross-modal samples i.e.
the unpaired target videos and unpaired 3D pose sequences. To this end, we
propose a new set of non-local relations in order to characterize long-range
latent pose interactions unlike general contrastive relations where positive
couplings are limited to a local neighborhood structure. Further, we provide an
objective way to quantify non-localness in order to select the most effective
relation set. We evaluate different self-adaptation settings and demonstrate
state-of-the-art 3D human pose estimation performance on standard benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual-AI: Dual-path Actor Interaction Learning for Group Activity Recognition. (arXiv:2204.02148v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02148">
<div class="article-summary-box-inner">
<span><p>Learning spatial-temporal relation among multiple actors is crucial for group
activity recognition. Different group activities often show the diversified
interactions between actors in the video. Hence, it is often difficult to model
complex group activities from a single view of spatial-temporal actor
evolution. To tackle this problem, we propose a distinct Dual-path Actor
Interaction (DualAI) framework, which flexibly arranges spatial and temporal
transformers in two complementary orders, enhancing actor relations by
integrating merits from different spatiotemporal paths. Moreover, we introduce
a novel Multi-scale Actor Contrastive Loss (MAC-Loss) between two interactive
paths of Dual-AI. Via self-supervised actor consistency in both frame and video
levels, MAC-Loss can effectively distinguish individual actor representations
to reduce action confusion among different actors. Consequently, our Dual-AI
can boost group activity recognition by fusing such discriminative features of
different actors. To evaluate the proposed approach, we conduct extensive
experiments on the widely used benchmarks, including Volleyball, Collective
Activity, and NBA datasets. The proposed Dual-AI achieves state-of-the-art
performance on all these datasets. It is worth noting the proposed Dual-AI with
50% training data outperforms a number of recent approaches with 100% training
data. This confirms the generalization power of Dual-AI for group activity
recognition, even under the challenging scenarios of limited supervision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CNN Filter DB: An Empirical Investigation of Trained Convolutional Filters. (arXiv:2203.15331v1 [cs.CV] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.15331">
<div class="article-summary-box-inner">
<span><p>Currently, many theoretical as well as practically relevant questions towards
the transferability and robustness of Convolutional Neural Networks (CNNs)
remain unsolved. While ongoing research efforts are engaging these problems
from various angles, in most computer vision related cases these approaches can
be generalized to investigations of the effects of distribution shifts in image
data. In this context, we propose to study the shifts in the learned weights of
trained CNN models. Here we focus on the properties of the distributions of
dominantly used 3x3 convolution filter kernels. We collected and publicly
provide a dataset with over 1.4 billion filters from hundreds of trained CNNs,
using a wide range of datasets, architectures, and vision tasks. In a first use
case of the proposed dataset, we can show highly relevant properties of many
publicly available pre-trained models for practical applications: I) We analyze
distribution shifts (or the lack thereof) between trained filters along
different axes of meta-parameters, like visual category of the dataset, task,
architecture, or layer depth. Based on these results, we conclude that model
pre-training can succeed on arbitrary datasets if they meet size and variance
conditions. II) We show that many pre-trained models contain degenerated
filters which make them less robust and less suitable for fine-tuning on target
applications.
</p>
<p>Data &amp; Project website: https://github.com/paulgavrikov/cnn-filter-db
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-04-07 23:08:25.371866767 UTC">2022-04-07 23:08:25 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>