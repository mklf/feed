<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-10-20T01:30:00Z">10-20</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">A Data Bootstrapping Recipe for Low Resource Multilingual Relation Classification. (arXiv:2110.09570v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09570">
<div class="article-summary-box-inner">
<span><p>Relation classification (sometimes called 'extraction') requires trustworthy
datasets for fine-tuning large language models, as well as for evaluation. Data
collection is challenging for Indian languages, because they are syntactically
and morphologically diverse, as well as different from resource-rich languages
like English. Despite recent interest in deep generative models for Indian
languages, relation classification is still not well served by public data
sets. In response, we present IndoRE, a dataset with 21K entity and relation
tagged gold sentences in three Indian languages, plus English. We start with a
multilingual BERT (mBERT) based system that captures entity span positions and
type information and provides competitive monolingual relation classification.
Using this system, we explore and compare transfer mechanisms between
languages. In particular, we study the accuracy efficiency tradeoff between
expensive gold instances vs. translated and aligned 'silver' instances. We
release the dataset for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Domain Adaptation for NMT: Decoupling Language and Domain Information with Adapters. (arXiv:2110.09574v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09574">
<div class="article-summary-box-inner">
<span><p>Adapter layers are lightweight, learnable units inserted between transformer
layers. Recent work explores using such layers for neural machine translation
(NMT), to adapt pre-trained models to new domains or language pairs, training
only a small set of parameters for each new setting (language pair or domain).
In this work we study the compositionality of language and domain adapters in
the context of Machine Translation. We aim to study, 1) parameter-efficient
adaptation to multiple domains and languages simultaneously (full-resource
scenario) and 2) cross-lingual transfer in domains where parallel data is
unavailable for certain language pairs (partial-resource scenario). We find
that in the partial resource scenario a naive combination of domain-specific
and language-specific adapters often results in `catastrophic forgetting' of
the missing languages. We study other ways to combine the adapters to alleviate
this issue and maximize cross-lingual transfer. With our best adapter
combinations, we obtain improvements of 3-4 BLEU on average for source
languages that do not have in-domain data. For target languages without
in-domain data, we achieve a similar improvement by combining adapters with
back-translation. Supplementary material is available at
https://tinyurl.com/r66stbxj
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Label-Descriptive Patterns and their Application to Characterizing Classification Errors. (arXiv:2110.09599v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09599">
<div class="article-summary-box-inner">
<span><p>State-of-the-art deep learning methods achieve human-like performance on many
tasks, but make errors nevertheless. Characterizing these errors in easily
interpretable terms gives insight into whether a model is prone to making
systematic errors, but also gives a way to act and improve the model. In this
paper we propose a method that allows us to do so for arbitrary classifiers by
mining a small set of patterns that together succinctly describe the input data
that is partitioned according to correctness of prediction. We show this is an
instance of the more general label description problem, which we formulate in
terms of the Minimum Description Length principle. To discover good pattern
sets we propose the efficient and hyperparameter-free Premise algorithm, which
through an extensive set of experiments we show on both synthetic and
real-world data performs very well in practice; unlike existing solutions it
ably recovers ground truth patterns, even on highly imbalanced data over many
unique items, or where patterns are only weakly associated to labels. Through
two real-world case studies we confirm that Premise gives clear and actionable
insight into the systematic errors made by modern NLP classifiers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Monotonic Simultaneous Translation with Chunk-wise Reordering and Refinement. (arXiv:2110.09646v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09646">
<div class="article-summary-box-inner">
<span><p>Recent work in simultaneous machine translation is often trained with
conventional full sentence translation corpora, leading to either excessive
latency or necessity to anticipate as-yet-unarrived words, when dealing with a
language pair whose word orders significantly differ. This is unlike human
simultaneous interpreters who produce largely monotonic translations at the
expense of the grammaticality of a sentence being translated. In this paper, we
thus propose an algorithm to reorder and refine the target side of a full
sentence translation corpus, so that the words/phrases between the source and
target sentences are aligned largely monotonically, using word alignment and
non-autoregressive neural machine translation. We then train a widely used
wait-k simultaneous translation model on this reordered-and-refined corpus. The
proposed approach improves BLEU scores and resulting translations exhibit
enhanced monotonicity with source sentences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ensemble ALBERT on SQuAD 2.0. (arXiv:2110.09665v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09665">
<div class="article-summary-box-inner">
<span><p>Machine question answering is an essential yet challenging task in natural
language processing. Recently, Pre-trained Contextual Embeddings (PCE) models
like Bidirectional Encoder Representations from Transformers (BERT) and A Lite
BERT (ALBERT) have attracted lots of attention due to their great performance
in a wide range of NLP tasks. In our Paper, we utilized the fine-tuned ALBERT
models and implemented combinations of additional layers (e.g. attention layer,
RNN layer) on top of them to improve model performance on Stanford Question
Answering Dataset (SQuAD 2.0). We implemented four different models with
different layers on top of ALBERT-base model, and two other models based on
ALBERT-xlarge and ALBERT-xxlarge. We compared their performance to our baseline
model ALBERT-base-v2 + ALBERT-SQuAD-out with details. Our best-performing
individual model is ALBERT-xxlarge + ALBERT-SQuAD-out, which achieved an F1
score of 88.435 on the dev set. Furthermore, we have implemented three
different ensemble algorithms to boost overall performance. By passing in
several best-performing models' results into our weighted voting ensemble
algorithm, our final result ranks first on the Stanford CS224N Test PCE SQuAD
Leaderboard with F1 = 90.123.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Lexicon Reader: Reduce Pronunciation Errors in End-to-end TTS by Leveraging External Textual Knowledge. (arXiv:2110.09698v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09698">
<div class="article-summary-box-inner">
<span><p>End-to-end TTS suffers from high data requirements as it is difficult for
both costly speech corpora to cover all necessary knowledge and neural models
to learn the knowledge, hence additional knowledge needs to be injected
manually. For example, to capture pronunciation knowledge on languages without
regular orthography, a complicated grapheme-to-phoneme pipeline needs to be
built based on a structured, large pronunciation lexicon, leading to extra,
sometimes high, costs to extend neural TTS to such languages. In this paper, we
propose a framework to learn to extract knowledge from unstructured external
resources using Token2Knowledge attention modules. The framework is applied to
build a novel end-to-end TTS model named Neural Lexicon Reader that extracts
pronunciations from raw lexicon texts. Experiments support the potential of our
framework that the model significantly reduces pronunciation errors in
low-resource, end-to-end Chinese TTS, and the lexicon-reading capability can be
transferred to other languages with a smaller amount of data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A non-hierarchical attention network with modality dropout for textual response generation in multimodal dialogue systems. (arXiv:2110.09702v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09702">
<div class="article-summary-box-inner">
<span><p>Existing text- and image-based multimodal dialogue systems use the
traditional Hierarchical Recurrent Encoder-Decoder (HRED) framework, which has
an utterance-level encoder to model utterance representation and a
context-level encoder to model context representation. Although pioneer efforts
have shown promising performances, they still suffer from the following
challenges: (1) the interaction between textual features and visual features is
not fine-grained enough. (2) the context representation can not provide a
complete representation for the context. To address the issues mentioned above,
we propose a non-hierarchical attention network with modality dropout, which
abandons the HRED framework and utilizes attention modules to encode each
utterance and model the context representation. To evaluate our proposed model,
we conduct comprehensive experiments on a public multimodal dialogue dataset.
Automatic and human evaluation demonstrate that our proposed model outperforms
the existing methods and achieves state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inter-Sense: An Investigation of Sensory Blending in Fiction. (arXiv:2110.09710v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09710">
<div class="article-summary-box-inner">
<span><p>This study reports on the semantic organization of English sensory
descriptors of the five basic senses of sight, hearing, touch, taste, and smell
in a large corpus of over 8,000 fiction books. We introduce a large-scale text
data-driven approach based on distributional-semantic word embeddings to
identify and extract these descriptors as well as analyze their mixing
interconnections in the resulting conceptual and sensory space. The findings
are relevant for research on concept acquisition and representation, as well as
for applications that can benefit from a better understanding of perceptual
spaces of sensory experiences, in fiction, in particular, and in language in
general.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Sensory Spaces of English Perceptual Verbs in Natural Language Data. (arXiv:2110.09721v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09721">
<div class="article-summary-box-inner">
<span><p>In this study, we explore how language captures the meaning of words, in
particular meaning related to sensory experiences learned from statistical
distributions across texts. We focus on the most frequent perception verbs of
English analyzed from an and Agentive vs. Experiential distinction across the
five basic sensory modalities: Visual (to look vs. to see), Auditory (to listen
vs. to hear), Tactile (to touch vs. to feel), Olfactory (to smell), and
Gustatory (to taste). In this study we report on a data-driven approach based
on distributional-semantic word embeddings and clustering models to identify
and uncover the descriptor sensory spaces of the perception verbs. In the
analysis, we identified differences and similarities of the generated
descriptors based on qualitative and quantitative differences of the perceptual
experience they denote. For instance, our results show that while the
perceptual spaces of the experiential verbs like to see, to hear show a more
detached, logical way of knowing and learning, their agentive counterparts (to
look, listen) provide a more intentional as well as more intimate and intuitive
way of discovering and interacting with the world around us. We believe that
such an approach has a high potential to expand our understanding and the
applicability of such sensory spaces to different fields of social and cultural
analysis. Research on the semantic organization of sensory spaces for various
applications might benefit from an the Agentive/Experiential account to address
the complexity of multiple senses wired with each other in still unexplored
ways.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Trajectory Prediction with Linguistic Representations. (arXiv:2110.09741v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09741">
<div class="article-summary-box-inner">
<span><p>Language allows humans to build mental models that interpret what is
happening around them resulting in more accurate long-term predictions. We
present a novel trajectory prediction model that uses linguistic intermediate
representations to forecast trajectories, and is trained using trajectory
samples with partially annotated captions. The model learns the meaning of each
of the words without direct per-word supervision. At inference time, it
generates a linguistic description of trajectories which captures maneuvers and
interactions over an extended time interval. This generated description is used
to refine predictions of the trajectories of multiple agents. We train and
validate our model on the Argoverse dataset, and demonstrate improved accuracy
results in trajectory prediction. In addition, our model is more interpretable:
it presents part of its reasoning in plain language as captions, which can aid
model development and can aid in building confidence in the model before
deploying it.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Importance Estimation from Multiple Perspectives for Keyphrase Extraction. (arXiv:2110.09749v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09749">
<div class="article-summary-box-inner">
<span><p>Keyphrase extraction is a fundamental task in Natural Language Processing,
which usually contains two main parts: candidate keyphrase extraction and
keyphrase importance estimation. From the view of human understanding
documents, we typically measure the importance of phrase according to its
syntactic accuracy, information saliency, and concept consistency
simultaneously. However, most existing keyphrase extraction approaches only
focus on the part of them, which leads to biased results. In this paper, we
propose a new approach to estimate the importance of keyphrase from multiple
perspectives (called as \textit{KIEMP}) and further improve the performance of
keyphrase extraction. Specifically, \textit{KIEMP} estimates the importance of
phrase with three modules: a chunking module to measure its syntactic accuracy,
a ranking module to check its information saliency, and a matching module to
judge the concept (i.e., topic) consistency between phrase and the whole
document. These three modules are seamlessly jointed together via an end-to-end
multi-task learning model, which is helpful for three parts to enhance each
other and balance the effects of three perspectives. Experimental results on
six benchmark datasets show that \textit{KIEMP} outperforms the existing
state-of-the-art keyphrase extraction approaches in most cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unifying Multimodal Transformer for Bi-directional Image and Text Generation. (arXiv:2110.09753v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09753">
<div class="article-summary-box-inner">
<span><p>We study the joint learning of image-to-text and text-to-image generations,
which are naturally bi-directional tasks. Typical existing works design two
separate task-specific models for each task, which impose expensive design
efforts. In this work, we propose a unified image-and-text generative framework
based on a single multimodal model to jointly study the bi-directional tasks.
We adopt Transformer as our unified architecture for its strong performance and
task-agnostic design. Specifically, we formulate both tasks as sequence
generation tasks, where we represent images and text as unified sequences of
tokens, and the Transformer learns multimodal interactions to generate
sequences. We further propose two-level granularity feature representations and
sequence-level training to improve the Transformer-based unified framework.
Experiments show that our approach significantly improves previous
Transformer-based model X-LXMERT's FID from 37.0 to 29.9 (lower is better) for
text-to-image generation, and improves CIDEr-D score from 100.9% to 122.6% for
fine-tuned image-to-text generation on the MS-COCO dataset. Our code is
available online.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Picture is Worth a Thousand Words: A Unified System for Diverse Captions and Rich Images Generation. (arXiv:2110.09756v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09756">
<div class="article-summary-box-inner">
<span><p>A creative image-and-text generative AI system mimics humans' extraordinary
abilities to provide users with diverse and comprehensive caption suggestions,
as well as rich image creations. In this work, we demonstrate such an AI
creation system to produce both diverse captions and rich images. When users
imagine an image and associate it with multiple captions, our system paints a
rich image to reflect all captions faithfully. Likewise, when users upload an
image, our system depicts it with multiple diverse captions. We propose a
unified multi-modal framework to achieve this goal. Specifically, our framework
jointly models image-and-text representations with a Transformer network, which
supports rich image creation by accepting multiple captions as input. We
consider the relations among input captions to encourage diversity in training
and adopt a non-autoregressive decoding strategy to enable real-time inference.
Based on these, our system supports both diverse captions and rich images
generations. Our code is available online.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open-domain clarification question generation without question examples. (arXiv:2110.09779v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09779">
<div class="article-summary-box-inner">
<span><p>An overarching goal of natural language processing is to enable machines to
communicate seamlessly with humans. However, natural language can be ambiguous
or unclear. In cases of uncertainty, humans engage in an interactive process
known as repair: asking questions and seeking clarification until their
uncertainty is resolved. We propose a framework for building a visually
grounded question-asking model capable of producing polar (yes-no)
clarification questions to resolve misunderstandings in dialogue. Our model
uses an expected information gain objective to derive informative questions
from an off-the-shelf image captioner without requiring any supervised
question-answer data. We demonstrate our model's ability to pose questions that
improve communicative success in a goal-oriented 20 questions game with
synthetic and human answerers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speech Pattern based Black-box Model Watermarking for Automatic Speech Recognition. (arXiv:2110.09814v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09814">
<div class="article-summary-box-inner">
<span><p>As an effective method for intellectual property (IP) protection, model
watermarking technology has been applied on a wide variety of deep neural
networks (DNN), including speech classification models. However, how to design
a black-box watermarking scheme for automatic speech recognition (ASR) models
is still an unsolved problem, which is a significant demand for protecting
remote ASR Application Programming Interface (API) deployed in cloud servers.
Due to conditional independence assumption and label-detection-based evasion
attack risk of ASR models, the black-box model watermarking scheme for speech
classification models cannot apply to ASR models. In this paper, we propose the
first black-box model watermarking framework for protecting the IP of ASR
models. Specifically, we synthesize trigger audios by spreading the speech
clips of model owners over the entire input audios and labeling the trigger
audios with the stego texts, which hides the authorship information with
linguistic steganography. Experiments on the state-of-the-art open-source ASR
system DeepSpeech demonstrate the feasibility of the proposed watermarking
scheme, which is robust against five kinds of attacks and has little impact on
accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AequeVox: Automated Fairness Testing of Speech Recognition Systems. (arXiv:2110.09843v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09843">
<div class="article-summary-box-inner">
<span><p>Automatic Speech Recognition (ASR) systems have become ubiquitous. They can
be found in a variety of form factors and are increasingly important in our
daily lives. As such, ensuring that these systems are equitable to different
subgroups of the population is crucial. In this paper, we introduce, AequeVox,
an automated testing framework for evaluating the fairness of ASR systems.
AequeVox simulates different environments to assess the effectiveness of ASR
systems for different populations. In addition, we investigate whether the
chosen simulations are comprehensible to humans. We further propose a fault
localization technique capable of identifying words that are not robust to
these varying environments. Both components of AequeVox are able to operate in
the absence of ground truth data.
</p>
<p>We evaluated AequeVox on speech from four different datasets using three
different commercial ASRs. Our experiments reveal that non-native English,
female and Nigerian English speakers generate 109%, 528.5% and 156.9% more
errors, on average than native English, male and UK Midlands speakers,
respectively. Our user study also reveals that 82.9% of the simulations
(employed through speech transformations) had a comprehensibility rating above
seven (out of ten), with the lowest rating being 6.78. This further validates
the fairness violations discovered by AequeVox. Finally, we show that the
non-robust words, as predicted by the fault localization technique embodied in
AequeVox, show 223.8% more errors than the predicted robust words across all
ASRs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Two-stage Voice Application Recommender System for Unhandled Utterances in Intelligent Personal Assistant. (arXiv:2110.09877v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09877">
<div class="article-summary-box-inner">
<span><p>Intelligent personal assistants (IPA) enable voice applications that
facilitate people's daily tasks. However, due to the complexity and ambiguity
of voice requests, some requests may not be handled properly by the standard
natural language understanding (NLU) component. In such cases, a simple reply
like "Sorry, I don't know" hurts the user's experience and limits the
functionality of IPA. In this paper, we propose a two-stage
shortlister-reranker recommender system to match third-party voice applications
(skills) to unhandled utterances. In this approach, a skill shortlister is
proposed to retrieve candidate skills from the skill catalog by calculating
both lexical and semantic similarity between skills and user requests. We also
illustrate how to build a new system by using observed data collected from a
baseline rule-based system, and how the exposure biases can generate
discrepancy between offline and human metrics. Lastly, we present two
relabeling methods that can handle the incomplete ground truth, and mitigate
exposure bias. We demonstrate the effectiveness of our proposed system through
extensive offline experiments. Furthermore, we present online A/B testing
results that show a significant boost on user experience satisfaction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Entity Relation Extraction as Dependency Parsing in Visually Rich Documents. (arXiv:2110.09915v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09915">
<div class="article-summary-box-inner">
<span><p>Previous works on key information extraction from visually rich documents
(VRDs) mainly focus on labeling the text within each bounding box (i.e.,
semantic entity), while the relations in-between are largely unexplored. In
this paper, we adapt the popular dependency parsing model, the biaffine parser,
to this entity relation extraction task. Being different from the original
dependency parsing model which recognizes dependency relations between words,
we identify relations between groups of words with layout information instead.
We have compared different representations of the semantic entity, different
VRD encoders, and different relation decoders. The results demonstrate that our
proposed model achieves 65.96% F1 score on the FUNSD dataset. As for the
real-world application, our model has been applied to the in-house customs
data, achieving reliable performance in the production setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speech Representation Learning Through Self-supervised Pretraining And Multi-task Finetuning. (arXiv:2110.09930v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09930">
<div class="article-summary-box-inner">
<span><p>Speech representation learning plays a vital role in speech processing. Among
them, self-supervised learning (SSL) has become an important research
direction. It has been shown that an SSL pretraining model can achieve
excellent performance in various downstream tasks of speech processing. On the
other hand, supervised multi-task learning (MTL) is another representation
learning paradigm, which has been proven effective in computer vision (CV) and
natural language processing (NLP). However, there is no systematic research on
the general representation learning model trained by supervised MTL in speech
processing. In this paper, we show that MTL finetuning can further improve SSL
pretraining. We analyze the generalizability of supervised MTL finetuning to
examine if the speech representation learned by MTL finetuning can generalize
to unseen new tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DEEPAG\'E: Answering Questions in Portuguese about the Brazilian Environment. (arXiv:2110.10015v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10015">
<div class="article-summary-box-inner">
<span><p>The challenge of climate change and biome conservation is one of the most
pressing issues of our time - particularly in Brazil, where key environmental
reserves are located. Given the availability of large textual databases on
ecological themes, it is natural to resort to question answering (QA) systems
to increase social awareness and understanding about these topics. In this
work, we introduce multiple QA systems that combine in novel ways the BM25
algorithm, a sparse retrieval technique, with PTT5, a pre-trained
state-of-the-art language model. Our QA systems focus on the Portuguese
language, thus offering resources not found elsewhere in the literature. As
training data, we collected questions from open-domain datasets, as well as
content from the Portuguese Wikipedia and news from the press. We thus
contribute with innovative architectures and novel applications, attaining an
F1-score of 36.2 with our best model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Private Language Model Adaptation for Speech Recognition. (arXiv:2110.10026v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10026">
<div class="article-summary-box-inner">
<span><p>Speech model adaptation is crucial to handle the discrepancy between
server-side proxy training data and actual data received on users' local
devices. With the use of federated learning (FL), we introduce an efficient
approach on continuously adapting neural network language models (NNLMs) on
private devices with applications on automatic speech recognition (ASR). To
address the potential speech transcription errors in the on-device training
corpus, we perform empirical studies on comparing various strategies of
leveraging token-level confidence scores to improve the NNLM quality in the FL
settings. Experiments show that compared with no model adaptation, the proposed
method achieves relative 2.6% and 10.8% word error rate (WER) reductions on two
speech evaluation datasets, respectively. We also provide analysis in
evaluating privacy guarantees of our presented procedure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Clinical Trial Information Extraction with BERT. (arXiv:2110.10027v1 [q-bio.QM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10027">
<div class="article-summary-box-inner">
<span><p>Natural language processing (NLP) of clinical trial documents can be useful
in new trial design. Here we identify entity types relevant to clinical trial
design and propose a framework called CT-BERT for information extraction from
clinical trial text. We trained named entity recognition (NER) models to
extract eligibility criteria entities by fine-tuning a set of pre-trained BERT
models. We then compared the performance of CT-BERT with recent baseline
methods including attention-based BiLSTM and Criteria2Query. The results
demonstrate the superiority of CT-BERT in clinical trial NLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Idiomatic Expression Identification using Semantic Compatibility. (arXiv:2110.10064v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10064">
<div class="article-summary-box-inner">
<span><p>Idiomatic expressions are an integral part of natural language and constantly
being added to a language. Owing to their non-compositionality and their
ability to take on a figurative or literal meaning depending on the sentential
context, they have been a classical challenge for NLP systems. To address this
challenge, we study the task of detecting whether a sentence has an idiomatic
expression and localizing it. Prior art for this task had studied specific
classes of idiomatic expressions offering limited views of their
generalizability to new idioms. We propose a multi-stage neural architecture
with the attention flow mechanism for identifying these expressions. The
network effectively fuses contextual and lexical information at different
levels using word and sub-word representations. Empirical evaluations on three
of the largest benchmark datasets with idiomatic expressions of varied
syntactic patterns and degrees of non-compositionality show that our proposed
model achieves new state-of-the-art results. A salient feature of the model is
its ability to identify idioms unseen during training with gains from 1.4% to
30.8% over competitive baselines on the largest dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compositional Networks Enable Systematic Generalization for Grounded Language Understanding. (arXiv:2008.02742v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.02742">
<div class="article-summary-box-inner">
<span><p>Humans are remarkably flexible when understanding new sentences that include
combinations of concepts they have never encountered before. Recent work has
shown that while deep networks can mimic some human language abilities when
presented with novel sentences, systematic variation uncovers the limitations
in the language-understanding abilities of networks. We demonstrate that these
limitations can be overcome by addressing the generalization challenges in the
gSCAN dataset, which explicitly measures how well an agent is able to interpret
novel linguistic commands grounded in vision, e.g., novel pairings of
adjectives and nouns. The key principle we employ is compositionality: that the
compositional structure of networks should reflect the compositional structure
of the problem domain they address, while allowing other parameters to be
learned end-to-end. We build a general-purpose mechanism that enables agents to
generalize their language understanding to compositional domains. Crucially,
our network has the same state-of-the-art performance as prior work while
generalizing its knowledge when prior work does not. Our network also provides
a level of interpretability that enables users to inspect what each part of
networks learns. Robust grounded language understanding without dramatic
failures and without corner cases is critical to building safe and fair robots;
we demonstrate the significant role that compositionality can play in achieving
that goal.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simulated Chats for Building Dialog Systems: Learning to Generate Conversations from Instructions. (arXiv:2010.10216v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.10216">
<div class="article-summary-box-inner">
<span><p>Popular dialog datasets such as MultiWOZ are created by providing crowd
workers an instruction, expressed in natural language, that describes the task
to be accomplished. Crowd workers play the role of a user and an agent to
generate dialogs to accomplish tasks involving booking restaurant tables,
calling a taxi etc. In this paper, we present a data creation strategy that
uses the pre-trained language model, GPT2, to simulate the interaction between
crowd workers by creating a user bot and an agent bot. We train the simulators
using a smaller percentage of actual crowd-generated conversations and their
corresponding instructions. We demonstrate that by using the simulated data, we
achieve significant improvements in low-resource settings on two publicly
available datasets - the MultiWOZ dataset and the Persona chat dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HALO 1.0: A Hardware-agnostic Accelerator Orchestration Framework for Enabling Hardware-agnostic Programming with True Performance Portability for Heterogeneous HPC. (arXiv:2011.10896v4 [cs.DC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.10896">
<div class="article-summary-box-inner">
<span><p>This paper presents HALO 1.0, an open-ended extensible multi-agent software
framework that implements a set of proposed hardware-agnostic accelerator
orchestration (HALO) principles. HALO implements a novel compute-centric
message passing interface (C^2MPI) specification for enabling the
performance-portable execution of a hardware-agnostic host application across
heterogeneous accelerators. The experiment results of evaluating eight widely
used HPC subroutines based on Intel Xeon E5-2620 CPUs, Intel Arria 10 GX FPGAs,
and NVIDIA GeForce RTX 2080 Ti GPUs show that HALO 1.0 allows for a unified
control flow for host programs to run across all the computing devices with a
consistently top performance portability score, which is up to five orders of
magnitude higher than the OpenCL-based solution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VidLanKD: Improving Language Understanding via Video-Distilled Knowledge Transfer. (arXiv:2107.02681v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.02681">
<div class="article-summary-box-inner">
<span><p>Since visual perception can give rich information beyond text descriptions
for world understanding, there has been increasing interest in leveraging
visual grounding for language learning. Recently, vokenization (Tan and Bansal,
2020) has attracted attention by using the predictions of a text-to-image
retrieval model as labels for language model supervision. Despite its success,
the method suffers from approximation error of using finite image labels and
the lack of vocabulary diversity of a small image-text dataset. To overcome
these limitations, we present VidLanKD, a video-language knowledge distillation
method for improving language understanding. We train a multi-modal teacher
model on a video-text dataset, and then transfer its knowledge to a student
language model with a text dataset. To avoid approximation error, we propose to
use different knowledge distillation objectives. In addition, the use of a
large-scale video-text dataset helps learn diverse and richer vocabularies. In
our experiments, VidLanKD achieves consistent improvements over text-only
language models and vokenization models, on several downstream language
understanding tasks including GLUE, SQuAD, and SWAG. We also demonstrate the
improved world knowledge, physical reasoning, and temporal reasoning
capabilities of our model by evaluating on the GLUE-diagnostics, PIQA, and
TRACIE datasets. Lastly, we present comprehensive ablation studies as well as
visualizations of the learned text-to-video grounding results of our teacher
and student language models. Our code and models are available at:
https://github.com/zinengtang/VidLanKD
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Persistent Spatial Semantic Representation for High-level Natural Language Instruction Execution. (arXiv:2107.05612v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.05612">
<div class="article-summary-box-inner">
<span><p>Natural language provides an accessible and expressive interface to specify
long-term tasks for robotic agents. However, non-experts are likely to specify
such tasks with high-level instructions, which abstract over specific robot
actions through several layers of abstraction. We propose that key to bridging
this gap between language and robot actions over long execution horizons are
persistent representations. We propose a persistent spatial semantic
representation method, and show how it enables building an agent that performs
hierarchical reasoning to effectively execute long-term tasks. We evaluate our
approach on the ALFRED benchmark and achieve state-of-the-art results, despite
completely avoiding the commonly used step-by-step instructions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Break, Perturb, Build: Automatic Perturbation of Reasoning Paths Through Question Decomposition. (arXiv:2107.13935v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.13935">
<div class="article-summary-box-inner">
<span><p>Recent efforts to create challenge benchmarks that test the abilities of
natural language understanding models have largely depended on human
annotations. In this work, we introduce the "Break, Perturb, Build" (BPB)
framework for automatic reasoning-oriented perturbation of question-answer
pairs. BPB represents a question by decomposing it into the reasoning steps
that are required to answer it, symbolically perturbs the decomposition, and
then generates new question-answer pairs. We demonstrate the effectiveness of
BPB by creating evaluation sets for three reading comprehension (RC)
benchmarks, generating thousands of high-quality examples without human
intervention. We evaluate a range of RC models on our evaluation sets, which
reveals large performance gaps on generated examples compared to the original
data. Moreover, symbolic perturbations enable fine-grained analysis of the
strengths and limitations of models. Last, augmenting the training data with
examples generated by BPB helps close the performance gaps, without any drop on
the original data distribution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-modal Retrieval of Tables and Texts Using Tri-encoder Models. (arXiv:2108.04049v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04049">
<div class="article-summary-box-inner">
<span><p>Open-domain extractive question answering works well on textual data by first
retrieving candidate texts and then extracting the answer from those
candidates. However, some questions cannot be answered by text alone but
require information stored in tables. In this paper, we present an approach for
retrieving both texts and tables relevant to a question by jointly encoding
texts, tables and questions into a single vector space. To this end, we create
a new multi-modal dataset based on text and table datasets from related work
and compare the retrieval performance of different encoding schemata. We find
that dense vector embeddings of transformer models outperform sparse embeddings
on four out of six evaluation datasets. Comparing different dense embedding
models, tri-encoders with one encoder for each question, text and table,
increase retrieval performance compared to bi-encoders with one encoder for the
question and one for both text and tables. We release the newly created
multi-modal dataset to the community so that it can be used for training and
evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Generalization Ability of Pretrained Language Models on Arithmetic and Logical Reasoning. (arXiv:2108.06743v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06743">
<div class="article-summary-box-inner">
<span><p>To quantitatively and intuitively explore the generalization ability of
pre-trained language models (PLMs), we have designed several tasks of
arithmetic and logical reasoning. We both analyse how well PLMs generalize when
the test data is in the same distribution as the train data and when it is
different, for the latter analysis, we have also designed a cross-distribution
test set other than the in-distribution test set. We conduct experiments on one
of the most advanced and publicly released generative PLM - BART. Our research
finds that the PLMs can easily generalize when the distribution is the same,
however, it is still difficult for them to generalize out of the distribution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond NED: Fast and Effective Search Space Reduction for Complex Question Answering over Knowledge Bases. (arXiv:2108.08597v3 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08597">
<div class="article-summary-box-inner">
<span><p>Answering complex questions over knowledge bases (KB-QA) faces huge input
data with billions of facts, involving millions of entities and thousands of
predicates. For efficiency, QA systems first reduce the answer search space by
identifying a set of facts that is likely to contain all answers and relevant
cues. The most common technique or doing this is to apply named entity
disambiguation (NED) systems to the question, and retrieve KB facts for the
disambiguated entities. This work presents CLOCQ, an efficient method that
prunes irrelevant parts of the search space using KB-aware signals. CLOCQ uses
a top-k query processor over score-ordered lists of KB items that combine
signals about lexical matching, relevance to the question, coherence among
candidate items, and connectivity in the KB graph. Experiments with two recent
QA benchmarks for complex questions demonstrate the superiority of CLOCQ over
state-of-the-art baselines with respect to answer presence, size of the search
space, and runtimes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LAViTeR: Learning Aligned Visual and Textual Representations Assisted by Image and Caption Generation. (arXiv:2109.04993v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04993">
<div class="article-summary-box-inner">
<span><p>Pre-training visual and textual representations from large-scale image-text
pairs is becoming a standard approach for many downstream vision-language
tasks. The transformer-based models learn inter and intra-modal attention
through a list of self-supervised learning tasks. This paper proposes LAViTeR,
a novel architecture for visual and textual representation learning. The main
module, Visual Textual Alignment (VTA) will be assisted by two auxiliary tasks,
GAN-based image synthesis and Image Captioning. We also propose a new
evaluation metric measuring the similarity between the learnt visual and
textual embedding. The experimental results on two public datasets, CUB and
MS-COCO, demonstrate superior visual and textual representation alignment in
the joint feature embedding space
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Causal Direction of Data Collection Matters: Implications of Causal and Anticausal Learning for NLP. (arXiv:2110.03618v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03618">
<div class="article-summary-box-inner">
<span><p>The principle of independent causal mechanisms (ICM) states that generative
processes of real world data consist of independent modules which do not
influence or inform each other. While this idea has led to fruitful
developments in the field of causal inference, it is not widely-known in the
NLP community. In this work, we argue that the causal direction of the data
collection process bears nontrivial implications that can explain a number of
published NLP findings, such as differences in semi-supervised learning (SSL)
and domain adaptation (DA) performance across different settings. We categorize
common NLP tasks according to their causal direction and empirically assay the
validity of the ICM principle for text data using minimum description length.
We conduct an extensive meta-analysis of over 100 published SSL and 30 DA
studies, and find that the results are consistent with our expectations based
on causal insights. This work presents the first attempt to analyze the ICM
principle in NLP, and provides constructive suggestions for future modeling
choices. Code available at https://github.com/zhijing-jin/icm4nlp
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Word Order Does Not Matter For Speech Recognition. (arXiv:2110.05994v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05994">
<div class="article-summary-box-inner">
<span><p>In this paper, we study training of automatic speech recognition system in a
weakly supervised setting where the order of words in transcript labels of the
audio training data is not known. We train a word-level acoustic model which
aggregates the distribution of all output frames using LogSumExp operation and
uses a cross-entropy loss to match with the ground-truth words distribution.
Using the pseudo-labels generated from this model on the training set, we then
train a letter-based acoustic model using Connectionist Temporal Classification
loss. Our system achieves 2.3%/4.6% on test-clean/test-other subsets of
LibriSpeech, which closely matches with the supervised baseline's performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FILM: Following Instructions in Language with Modular Methods. (arXiv:2110.07342v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07342">
<div class="article-summary-box-inner">
<span><p>Recent methods for embodied instruction following are typically trained
end-to-end using imitation learning. This requires the use of expert
trajectories and low-level language instructions. Such approaches assume
learned hidden states will simultaneously integrate semantics from the language
and vision to perform state tracking, spatial memory, exploration, and
long-term planning. In contrast, we propose a modular method with structured
representations that (1) builds a semantic map of the scene, and (2) performs
exploration with a semantic search policy, to achieve the natural language
goal. Our modular method achieves SOTA performance (24.46%) with a substantial
(8.17 % absolute) gap from previous work while using less data by eschewing
both expert trajectories and low-level instructions. Leveraging low-level
language, however, can further increase our performance (26.49%). Our findings
suggest that an explicit spatial memory and a semantic search policy can
provide a stronger and more general representation for state-tracking and
guidance, even in the absence of expert trajectories or low-level instructions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm. (arXiv:2110.08190v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08190">
<div class="article-summary-box-inner">
<span><p>Various pruning approaches have been proposed to reduce the footprint
requirements of Transformer-based language models. Conventional wisdom is that
pruning reduces the model expressiveness and thus is more likely to underfit
than overfit compared to the original model. However, under the trending
pretrain-and-finetune paradigm, we argue that pruning increases the risk of
overfitting if pruning was performed at the fine-tuning phase, as it increases
the amount of information a model needs to learn from the downstream task,
resulting in relative data deficiency. In this paper, we aim to address the
overfitting issue under the pretrain-and-finetune paradigm to improve pruning
performance via progressive knowledge distillation (KD) and sparse pruning.
Furthermore, to mitigate the interference between different strategies of
learning rate, pruning and distillation, we propose a three-stage learning
framework. We show for the first time that reducing the risk of overfitting can
help the effectiveness of pruning under the pretrain-and-finetune paradigm.
Experiments on multiple datasets of GLUE benchmark show that our method
achieves highly competitive pruning performance over the state-of-the-art
competitors across different pruning ratio constraints.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Multimodal to Unimodal Attention in Transformers using Knowledge Distillation. (arXiv:2110.08270v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08270">
<div class="article-summary-box-inner">
<span><p>Multimodal Deep Learning has garnered much interest, and transformers have
triggered novel approaches, thanks to the cross-attention mechanism. Here we
propose an approach to deal with two key existing challenges: the high
computational resource demanded and the issue of missing modalities. We
introduce for the first time the concept of knowledge distillation in
transformers to use only one modality at inference time. We report a full study
analyzing multiple student-teacher configurations, levels at which distillation
is applied, and different methodologies. With the best configuration, we
improved the state-of-the-art accuracy by 3%, we reduced the number of
parameters by 2.5 times and the inference time by 22%. Such
performance-computation tradeoff can be exploited in many applications and we
aim at opening a new research area where the deployment of complex models with
limited resources is demanded.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ViraPart: A Text Refinement Framework for ASR and NLP Tasks in Persian. (arXiv:2110.09086v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09086">
<div class="article-summary-box-inner">
<span><p>The Persian language is an inflectional SOV language. This fact makes Persian
a more uncertain language. However, using techniques such as ZWNJ recognition,
punctuation restoration, and Persian Ezafe construction will lead us to a more
understandable and precise language. In most of the works in Persian, these
techniques are addressed individually. Despite that, we believe that for text
refinement in Persian, all of these tasks are necessary. In this work, we
proposed a ViraPart framework that uses embedded ParsBERT in its core for text
clarifications. First, used the BERT variant for Persian following by a
classifier layer for classification procedures. Next, we combined models
outputs to output cleartext. In the end, the proposed model for ZWNJ
recognition, punctuation restoration, and Persian Ezafe construction performs
the averaged F1 macro scores of 96.90%, 92.13%, and 98.50%, respectively.
Experimental results show that our proposed approach is very effective in text
refinement for the Persian language.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">TransFusion: Cross-view Fusion with Transformer for 3D Human Pose Estimation. (arXiv:2110.09554v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09554">
<div class="article-summary-box-inner">
<span><p>Estimating the 2D human poses in each view is typically the first step in
calibrated multi-view 3D pose estimation. But the performance of 2D pose
detectors suffers from challenging situations such as occlusions and oblique
viewing angles. To address these challenges, previous works derive
point-to-point correspondences between different views from epipolar geometry
and utilize the correspondences to merge prediction heatmaps or feature
representations. Instead of post-prediction merge/calibration, here we
introduce a transformer framework for multi-view 3D pose estimation, aiming at
directly improving individual 2D predictors by integrating information from
different views. Inspired by previous multi-modal transformers, we design a
unified transformer architecture, named TransFusion, to fuse cues from both
current views and neighboring views. Moreover, we propose the concept of
epipolar field to encode 3D positional information into the transformer model.
The 3D position encoding guided by the epipolar field provides an efficient way
of encoding correspondences between pixels of different views. Experiments on
Human 3.6M and Ski-Pose show that our method is more efficient and has
consistent improvements compared to other fusion methods. Specifically, we
achieve 25.8 mm MPJPE on Human 3.6M with only 5M parameters on 256 x 256
resolution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BGaitR-Net: Occluded Gait Sequence reconstructionwith temporally constrained model for gait recognition. (arXiv:2110.09564v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09564">
<div class="article-summary-box-inner">
<span><p>Recent advancements in computational resources and Deep Learning
methodologies has significantly benefited development of intelligent
vision-based surveillance applications. Gait recognition in the presence of
occlusion is one of the challenging research topics in this area, and the
solutions proposed by researchers to date lack in robustness and also dependent
of several unrealistic constraints, which limits their practical applicability.
We improve the state-of-the-art by developing novel deep learning-based
algorithms to identify the occluded frames in an input sequence and next
reconstruct these occluded frames by exploiting the spatio-temporal information
present in the gait sequence. The multi-stage pipeline adopted in this work
consists of key pose mapping, occlusion detection and reconstruction, and
finally gait recognition. While the key pose mapping and occlusion detection
phases are done %using Constrained KMeans Clustering and via a graph sorting
algorithm, reconstruction of occluded frames is done by fusing the key
pose-specific information derived in the previous step along with the
spatio-temporal information contained in a gait sequence using a Bi-Directional
Long Short Time Memory. This occlusion reconstruction model has been trained
using synthetically occluded CASIA-B and OU-ISIR data, and the trained model is
termed as Bidirectional Gait Reconstruction Network BGait-R-Net. Our LSTM-based
model reconstructs occlusion and generates frames that are temporally
consistent with the periodic pattern of a gait cycle, while simultaneously
preserving the body structure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hands Off: A Handshake Interaction Detection and Localization Model for COVID-19 Threat Control. (arXiv:2110.09571v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09571">
<div class="article-summary-box-inner">
<span><p>The COVID-19 outbreak has affected millions of people across the globe and is
continuing to spread at a drastic scale. Out of the numerous steps taken to
control the spread of the virus, social distancing has been a crucial and
effective practice. However, recent reports of social distancing violations
suggest the need for non-intrusive detection techniques to ensure safety in
public spaces. In this paper, a real-time detection model is proposed to
identify handshake interactions in a range of realistic scenarios with multiple
people in the scene and also detect multiple interactions in a single frame.
This is the first work that performs dyadic interaction localization in a
multi-person setting. The efficacy of the proposed model was evaluated across
two different datasets on more than 3200 frames, thus enabling a robust
localization model in different environments. The proposed model is the first
dyadic interaction localizer in a multi-person setting, which enables it to be
used in public spaces to identify handshake interactions and thereby identify
and mitigate COVID-19 transmission.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Feature Alignment for Semi-supervised Domain Adaptation. (arXiv:2110.09641v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09641">
<div class="article-summary-box-inner">
<span><p>Most research on domain adaptation has focused on the purely unsupervised
setting, where no labeled examples in the target domain are available. However,
in many real-world scenarios, a small amount of labeled target data is
available and can be used to improve adaptation. We address this
semi-supervised setting and propose to use dynamic feature alignment to address
both inter- and intra-domain discrepancy. Unlike previous approaches, which
attempt to align source and target features within a mini-batch, we propose to
align the target features to a set of dynamically updated class prototypes,
which we use both for minimizing divergence and pseudo-labeling. By updating
based on class prototypes, we avoid problems that arise in previous approaches
due to class imbalances. Our approach, which doesn't require extensive tuning
or adversarial training, significantly improves the state of the art for
semi-supervised domain adaptation. We provide a quantitative evaluation on two
standard datasets, DomainNet and Office-Home, and performance analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Osteoporosis Prescreening using Panoramic Radiographs through a Deep Convolutional Neural Network with Attention Mechanism. (arXiv:2110.09662v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09662">
<div class="article-summary-box-inner">
<span><p>Objectives. The aim of this study was to investigate whether a deep
convolutional neural network (CNN) with an attention module can detect
osteoporosis on panoramic radiographs.
</p>
<p>Study Design. A dataset of 70 panoramic radiographs (PRs) from 70 different
subjects of age between 49 to 60 was used, including 49 subjects with
osteoporosis and 21 normal subjects. We utilized the leave-one-out
cross-validation approach to generate 70 training and test splits.
Specifically, for each split, one image was used for testing and the remaining
69 images were used for training. A deep convolutional neural network (CNN)
using the Siamese architecture was implemented through a fine-tuning process to
classify an PR image using patches extracted from eight representative
trabecula bone areas (Figure 1). In order to automatically learn the importance
of different PR patches, an attention module was integrated into the deep CNN.
Three metrics, including osteoporosis accuracy (OPA), non-osteoporosis accuracy
(NOPA) and overall accuracy (OA), were utilized for performance evaluation.
</p>
<p>Results. The proposed baseline CNN approach achieved the OPA, NOPA and OA
scores of 0.667, 0.878 and 0.814, respectively. With the help of the attention
module, the OPA, NOPA and OA scores were further improved to 0.714, 0.939 and
0.871, respectively.
</p>
<p>Conclusions. The proposed method obtained promising results using deep CNN
with an attention module, which might be applied to osteoporosis prescreening.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Distillation: Aggregating Knowledge from Multiple Paths for Efficient Distillation. (arXiv:2110.09674v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09674">
<div class="article-summary-box-inner">
<span><p>Knowledge Distillation is becoming one of the primary trends among neural
network compression algorithms to improve the generalization performance of a
smaller student model with guidance from a larger teacher model. This momentous
rise in applications of knowledge distillation is accompanied by the
introduction of numerous algorithms for distilling the knowledge such as soft
targets and hint layers. Despite this advancement in different techniques for
distilling the knowledge, the aggregation of different paths for distillation
has not been studied comprehensively. This is of particular significance, not
only because different paths have different importance, but also due to the
fact that some paths might have negative effects on the generalization
performance of the student model. Hence, we need to adaptively adjust the
importance of each path to maximize the impact of distillation on the student
model. In this paper, we explore different approaches for aggregating these
different paths and introduce our proposed adaptive approach based on multitask
learning methods. We empirically demonstrate the effectiveness of the proposed
approach over other baselines on the applications of knowledge distillation in
classification, semantic segmentation, and object detection tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Vendor CT Image Data Harmonization Using CVH-CT. (arXiv:2110.09693v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09693">
<div class="article-summary-box-inner">
<span><p>While remarkable advances have been made in Computed Tomography (CT), most of
the existing efforts focus on imaging enhancement while reducing radiation
dose. How to harmonize CT image data captured using different scanners is vital
in cross-center large-scale radiomics studies but remains the boundary to
explore. Furthermore, the lack of paired training image problem makes it
computationally challenging to adopt existing deep learning models. %developed
for CT image standardization. %this problem more challenging. We propose a
novel deep learning approach called CVH-CT for harmonizing CT images captured
using scanners from different vendors. The generator of CVH-CT uses a
self-attention mechanism to learn the scanner-related information. We also
propose a VGG feature-based domain loss to effectively extract texture
properties from unpaired image data to learn the scanner-based texture
distributions. The experimental results show that CVH-CT is clearly better than
the baselines because of the use of the proposed domain loss, and CVH-CT can
effectively reduce the scanner-related variability in terms of radiomic
features.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Quality Assessment in the Modern Age. (arXiv:2110.09699v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09699">
<div class="article-summary-box-inner">
<span><p>This tutorial provides the audience with the basic theories, methodologies,
and current progresses of image quality assessment (IQA). From an actionable
perspective, we will first revisit several subjective quality assessment
methodologies, with emphasis on how to properly select visual stimuli. We will
then present in detail the design principles of objective quality assessment
models, supplemented by an in-depth analysis of their advantages and
disadvantages. Both hand-engineered and (deep) learning-based methods will be
covered. Moreover, the limitations with the conventional model comparison
methodology for objective quality models will be pointed out, and novel
comparison methodologies such as those based on the theory of "analysis by
synthesis" will be introduced. We will last discuss the real-world multimedia
applications of IQA, and give a list of open challenging problems, in the hope
of encouraging more and more talented researchers and engineers devoting to
this exciting and rewarding research field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mask-aware IoU for Anchor Assignment in Real-time Instance Segmentation. (arXiv:2110.09734v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09734">
<div class="article-summary-box-inner">
<span><p>This paper presents Mask-aware Intersection-over-Union (maIoU) for assigning
anchor boxes as positives and negatives during training of instance
segmentation methods. Unlike conventional IoU or its variants, which only
considers the proximity of two boxes; maIoU consistently measures the proximity
of an anchor box with not only a ground truth box but also its associated
ground truth mask. Thus, additionally considering the mask, which, in fact,
represents the shape of the object, maIoU enables a more accurate supervision
during training. We present the effectiveness of maIoU on a state-of-the-art
(SOTA) assigner, ATSS, by replacing IoU operation by our maIoU and training
YOLACT, a SOTA real-time instance segmentation method. Using ATSS with maIoU
consistently outperforms (i) ATSS with IoU by $\sim 1$ mask AP, (ii) baseline
YOLACT with fixed IoU threshold assigner by $\sim 2$ mask AP over different
image sizes and (iii) decreases the inference time by $25 \%$ owing to using
less anchors. Then, exploiting this efficiency, we devise maYOLACT, a faster
and $+6$ AP more accurate detector than YOLACT. Our best model achieves $37.7$
mask AP at $25$ fps on COCO test-dev establishing a new state-of-the-art for
real-time instance segmentation. Code is available at
https://github.com/kemaloksuz/Mask-aware-IoU
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Not to Reconstruct Anomalies. (arXiv:2110.09742v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09742">
<div class="article-summary-box-inner">
<span><p>Video anomaly detection is often seen as one-class classification (OCC)
problem due to the limited availability of anomaly examples. Typically, to
tackle this problem, an autoencoder (AE) is trained to reconstruct the input
with training set consisting only of normal data. At test time, the AE is then
expected to well reconstruct the normal data while poorly reconstructing the
anomalous data. However, several studies have shown that, even with only normal
data training, AEs can often start reconstructing anomalies as well which
depletes the anomaly detection performance. To mitigate this problem, we
propose a novel methodology to train AEs with the objective of reconstructing
only normal data, regardless of the input (i.e., normal or abnormal). Since no
real anomalies are available in the OCC settings, the training is assisted by
pseudo anomalies that are generated by manipulating normal data to simulate the
out-of-normal-data distribution. We additionally propose two ways to generate
pseudo anomalies: patch and skip frame based. Extensive experiments on three
challenging video anomaly datasets demonstrate the effectiveness of our method
in improving conventional AEs, achieving state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spectral Variability Augmented Sparse Unmixing of Hyperspectral Images. (arXiv:2110.09744v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09744">
<div class="article-summary-box-inner">
<span><p>Spectral unmixing (SU) expresses the mixed pixels existed in hyperspectral
images as the product of endmember and abundance, which has been widely used in
hyperspectral imagery analysis. However, the influence of light, acquisition
conditions and the inherent properties of materials, results in that the
identified endmembers can vary spectrally within a given image (construed as
spectral variability). To address this issue, recent methods usually use a
priori obtained spectral library to represent multiple characteristic spectra
of the same object, but few of them extracted the spectral variability
explicitly. In this paper, a spectral variability augmented sparse unmixing
model (SVASU) is proposed, in which the spectral variability is extracted for
the first time. The variable spectra are divided into two parts of intrinsic
spectrum and spectral variability for spectral reconstruction, and modeled
synchronously in the SU model adding the regular terms restricting the sparsity
of abundance and the generalization of the variability coefficient. It is noted
that the spectral variability library and the intrinsic spectral library are
all constructed from the In-situ observed image. Experimental results over both
synthetic and real-world data sets demonstrate that the augmented decomposition
by spectral variability significantly improves the unmixing performance than
the decomposition only by spectral library, as well as compared to
state-of-the-art algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unifying Multimodal Transformer for Bi-directional Image and Text Generation. (arXiv:2110.09753v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09753">
<div class="article-summary-box-inner">
<span><p>We study the joint learning of image-to-text and text-to-image generations,
which are naturally bi-directional tasks. Typical existing works design two
separate task-specific models for each task, which impose expensive design
efforts. In this work, we propose a unified image-and-text generative framework
based on a single multimodal model to jointly study the bi-directional tasks.
We adopt Transformer as our unified architecture for its strong performance and
task-agnostic design. Specifically, we formulate both tasks as sequence
generation tasks, where we represent images and text as unified sequences of
tokens, and the Transformer learns multimodal interactions to generate
sequences. We further propose two-level granularity feature representations and
sequence-level training to improve the Transformer-based unified framework.
Experiments show that our approach significantly improves previous
Transformer-based model X-LXMERT's FID from 37.0 to 29.9 (lower is better) for
text-to-image generation, and improves CIDEr-D score from 100.9% to 122.6% for
fine-tuned image-to-text generation on the MS-COCO dataset. Our code is
available online.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Picture is Worth a Thousand Words: A Unified System for Diverse Captions and Rich Images Generation. (arXiv:2110.09756v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09756">
<div class="article-summary-box-inner">
<span><p>A creative image-and-text generative AI system mimics humans' extraordinary
abilities to provide users with diverse and comprehensive caption suggestions,
as well as rich image creations. In this work, we demonstrate such an AI
creation system to produce both diverse captions and rich images. When users
imagine an image and associate it with multiple captions, our system paints a
rich image to reflect all captions faithfully. Likewise, when users upload an
image, our system depicts it with multiple diverse captions. We propose a
unified multi-modal framework to achieve this goal. Specifically, our framework
jointly models image-and-text representations with a Transformer network, which
supports rich image creation by accepting multiple captions as input. We
consider the relations among input captions to encourage diversity in training
and adopt a non-autoregressive decoding strategy to enable real-time inference.
Based on these, our system supports both diverse captions and rich images
generations. Our code is available online.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Regularization Method to Improve Adversarial Robustness of Neural Networks for ECG Signal Classification. (arXiv:2110.09759v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09759">
<div class="article-summary-box-inner">
<span><p>Electrocardiogram (ECG) is the most widely used diagnostic tool to monitor
the condition of the human heart. By using deep neural networks (DNNs),
interpretation of ECG signals can be fully automated for the identification of
potential abnormalities in a patient's heart in a fraction of a second. Studies
have shown that given a sufficiently large amount of training data, DNN
accuracy for ECG classification could reach human-expert cardiologist level.
However, despite of the excellent performance in classification accuracy, DNNs
are highly vulnerable to adversarial noises that are subtle changes in the
input of a DNN and may lead to a wrong class-label prediction. It is
challenging and essential to improve robustness of DNNs against adversarial
noises, which are a threat to life-critical applications. In this work, we
proposed a regularization method to improve DNN robustness from the perspective
of noise-to-signal ratio (NSR) for the application of ECG signal
classification. We evaluated our method on PhysioNet MIT-BIH dataset and
CPSC2018 ECG dataset, and the results show that our method can substantially
enhance DNN robustness against adversarial noises generated from adversarial
attacks, with a minimal change in accuracy on clean data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Blurred Ground-based Sky/Cloud Images. (arXiv:2110.09764v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09764">
<div class="article-summary-box-inner">
<span><p>Ground-based whole sky imagers (WSIs) are being used by researchers in
various fields to study the atmospheric events. These ground-based sky cameras
capture visible-light images of the sky at regular intervals of time. Owing to
the atmospheric interference and camera sensor noise, the captured images often
exhibit noise and blur. This may pose a problem in subsequent image processing
stages. Therefore, it is important to accurately identify the blurred images.
This is a difficult task, as clouds have varying shapes, textures, and soft
edges whereas the sky acts as a homogeneous and uniform background. In this
paper, we propose an efficient framework that can identify the blurred
sky/cloud images. Using a static external marker, our proposed methodology has
a detection accuracy of 94\%. To the best of our knowledge, our approach is the
first of its kind in the automatic identification of blurred images for
ground-based sky/cloud images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Memory-Augmented Deep Unfolding Network for Compressive Sensing. (arXiv:2110.09766v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09766">
<div class="article-summary-box-inner">
<span><p>Mapping a truncated optimization method into a deep neural network, deep
unfolding network (DUN) has attracted growing attention in compressive sensing
(CS) due to its good interpretability and high performance. Each stage in DUNs
corresponds to one iteration in optimization. By understanding DUNs from the
perspective of the human brain's memory processing, we find there exists two
issues in existing DUNs. One is the information between every two adjacent
stages, which can be regarded as short-term memory, is usually lost seriously.
The other is no explicit mechanism to ensure that the previous stages affect
the current stage, which means memory is easily forgotten. To solve these
issues, in this paper, a novel DUN with persistent memory for CS is proposed,
dubbed Memory-Augmented Deep Unfolding Network (MADUN). We design a
memory-augmented proximal mapping module (MAPMM) by combining two types of
memory augmentation mechanisms, namely High-throughput Short-term Memory (HSM)
and Cross-stage Long-term Memory (CLM). HSM is exploited to allow DUNs to
transmit multi-channel short-term memory, which greatly reduces information
loss between adjacent stages. CLM is utilized to develop the dependency of deep
information across cascading stages, which greatly enhances network
representation capability. Extensive CS experiments on natural and MR images
show that with the strong ability to maintain and balance information our MADUN
outperforms existing state-of-the-art methods by a large margin. The source
code is available at https://github.com/jianzhangcs/MADUN/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synthetic Temporal Anomaly Guided End-to-End Video Anomaly Detection. (arXiv:2110.09768v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09768">
<div class="article-summary-box-inner">
<span><p>Due to the limited availability of anomaly examples, video anomaly detection
is often seen as one-class classification (OCC) problem. A popular way to
tackle this problem is by utilizing an autoencoder (AE) trained only on normal
data. At test time, the AE is then expected to reconstruct the normal input
well while reconstructing the anomalies poorly. However, several studies show
that, even with normal data only training, AEs can often start reconstructing
anomalies as well which depletes their anomaly detection performance. To
mitigate this, we propose a temporal pseudo anomaly synthesizer that generates
fake-anomalies using only normal data. An AE is then trained to maximize the
reconstruction loss on pseudo anomalies while minimizing this loss on normal
data. This way, the AE is encouraged to produce distinguishable reconstructions
for normal and anomalous frames. Extensive experiments and analysis on three
challenging video anomaly datasets demonstrate the effectiveness of our
approach to improve the basic AEs in achieving superiority against several
existing state-of-the-art models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synergy between 3DMM and 3D Landmarks for Accurate 3D Facial Geometry. (arXiv:2110.09772v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09772">
<div class="article-summary-box-inner">
<span><p>This work studies learning from a synergy process of 3D Morphable Models
(3DMM) and 3D facial landmarks to predict complete 3D facial geometry,
including 3D alignment, face orientation, and 3D face modeling. Our synergy
process leverages a representation cycle for 3DMM parameters and 3D landmarks.
3D landmarks can be extracted and refined from face meshes built by 3DMM
parameters. We next reverse the representation direction and show that
predicting 3DMM parameters from sparse 3D landmarks improves the information
flow. Together we create a synergy process that utilizes the relation between
3D landmarks and 3DMM parameters, and they collaboratively contribute to better
performance. We extensively validate our contribution on full tasks of facial
geometry prediction and show our superior and robust performance on these tasks
for various scenarios. Particularly, we adopt only simple and widely-used
network operations to attain fast and accurate facial geometry prediction.
Codes and data: https://choyingw.github.io/works/SynergyNet/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Aesthetic Photo Collage with Deep Reinforcement Learning. (arXiv:2110.09775v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09775">
<div class="article-summary-box-inner">
<span><p>Photo collage aims to automatically arrange multiple photos on a given canvas
with high aesthetic quality. Existing methods are based mainly on handcrafted
feature optimization, which cannot adequately capture high-level human
aesthetic senses. Deep learning provides a promising way, but owing to the
complexity of collage and lack of training data, a solution has yet to be
found. In this paper, we propose a novel pipeline for automatic generation of
aspect ratio specified collage and the reinforcement learning technique is
introduced in collage for the first time. Inspired by manual collages, we model
the collage generation as sequential decision process to adjust spatial
positions, orientation angles, placement order and the global layout. To
instruct the agent to improve both the overall layout and local details, the
reward function is specially designed for collage, considering subjective and
objective factors. To overcome the lack of training data, we pretrain our deep
aesthetic network on a large scale image aesthetic dataset (CPC) for general
aesthetic feature extraction and propose an attention fusion module for
structural collage feature representation. We test our model against competing
methods on two movie datasets and our results outperform others in aesthetic
quality evaluation. Further user study is also conducted to demonstrate the
effectiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Toxic and Narcotic Medication Detection with Rotated Object Detector. (arXiv:2110.09777v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09777">
<div class="article-summary-box-inner">
<span><p>Recent years have witnessed the advancement of deep learning vision
technologies and applications in the medical industry. Intelligent devices for
special medication management are in great need of, which requires more precise
detection algorithms to identify the specifications and locations. In this
work, YOLO (You only look once) based object detectors are tailored for toxic
and narcotic medications detection tasks. Specifically, a more flexible
annotation with rotated degree ranging from $0^\circ$ to $90^\circ$ and a
mask-mapping-based non-maximum suppression method are proposed to achieve a
feasible and efficient medication detector aiming at arbitrarily oriented
bounding boxes. Extensive experiments demonstrate that the rotated YOLO
detectors are more suitable for identifying densely arranged drugs. The best
shot mean average precision of the proposed network reaches 0.811 while the
inference time is less than 300ms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatial-Temporal Transformer for 3D Point Cloud Sequences. (arXiv:2110.09783v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09783">
<div class="article-summary-box-inner">
<span><p>Effective learning of spatial-temporal information within a point cloud
sequence is highly important for many down-stream tasks such as 4D semantic
segmentation and 3D action recognition. In this paper, we propose a novel
framework named Point Spatial-Temporal Transformer (PST2) to learn
spatial-temporal representations from dynamic 3D point cloud sequences. Our
PST2 consists of two major modules: a Spatio-Temporal Self-Attention (STSA)
module and a Resolution Embedding (RE) module. Our STSA module is introduced to
capture the spatial-temporal context information across adjacent frames, while
the RE module is proposed to aggregate features across neighbors to enhance the
resolution of feature maps. We test the effectiveness our PST2 with two
different tasks on point cloud sequences, i.e., 4D semantic segmentation and 3D
action recognition. Extensive experiments on three benchmarks show that our
PST2 outperforms existing methods on all datasets. The effectiveness of our
STSA and RE modules have also been justified with ablation experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CIPS-3D: A 3D-Aware Generator of GANs Based on Conditionally-Independent Pixel Synthesis. (arXiv:2110.09788v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09788">
<div class="article-summary-box-inner">
<span><p>The style-based GAN (StyleGAN) architecture achieved state-of-the-art results
for generating high-quality images, but it lacks explicit and precise control
over camera poses. The recently proposed NeRF-based GANs made great progress
towards 3D-aware generators, but they are unable to generate high-quality
images yet. This paper presents CIPS-3D, a style-based, 3D-aware generator that
is composed of a shallow NeRF network and a deep implicit neural representation
(INR) network. The generator synthesizes each pixel value independently without
any spatial convolution or upsampling operation. In addition, we diagnose the
problem of mirror symmetry that implies a suboptimal solution and solve it by
introducing an auxiliary discriminator. Trained on raw, single-view images,
CIPS-3D sets new records for 3D-aware image synthesis with an impressive FID of
6.97 for images at the $256\times256$ resolution on FFHQ. We also demonstrate
several interesting directions for CIPS-3D such as transfer learning and
3D-aware face stylization. The synthesis results are best viewed as videos, so
we recommend the readers to check our github project at
https://github.com/PeterouZh/CIPS-3D
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Geo-DefakeHop: High-Performance Geographic Fake Image Detection. (arXiv:2110.09795v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09795">
<div class="article-summary-box-inner">
<span><p>A robust fake satellite image detection method, called Geo-DefakeHop, is
proposed in this work. Geo-DefakeHop is developed based on the parallel
subspace learning (PSL) methodology. PSL maps the input image space into
several feature subspaces using multiple filter banks. By exploring response
differences of different channels between real and fake images for a filter
bank, Geo-DefakeHop learns the most discriminant channels and uses their soft
decision scores as features. Then, Geo-DefakeHop selects a few discriminant
features from each filter bank and ensemble them to make a final binary
decision. Geo-DefakeHop offers a light-weight high-performance solution to fake
satellite images detection. Its model size is analyzed, which ranges from 0.8
to 62K parameters. Furthermore, it is shown by experimental results that it
achieves an F1-score higher than 95\% under various common image manipulations
such as resizing, compression and noise corruption.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Latent reweighting, an almost free improvement for GANs. (arXiv:2110.09803v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09803">
<div class="article-summary-box-inner">
<span><p>Standard formulations of GANs, where a continuous function deforms a
connected latent space, have been shown to be misspecified when fitting
different classes of images. In particular, the generator will necessarily
sample some low-quality images in between the classes. Rather than modifying
the architecture, a line of works aims at improving the sampling quality from
pre-trained generators at the expense of increased computational cost. Building
on this, we introduce an additional network to predict latent importance
weights and two associated sampling methods to avoid the poorest samples. This
idea has several advantages: 1) it provides a way to inject disconnectedness
into any GAN architecture, 2) since the rejection happens in the latent space,
it avoids going through both the generator and the discriminator, saving
computation time, 3) this importance weights formulation provides a principled
way to reduce the Wasserstein's distance to the target distribution. We
demonstrate the effectiveness of our method on several datasets, both synthetic
and high-dimensional.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Microstructure reconstruction via artificial neural networks: A combination of causal and non-causal approach. (arXiv:2110.09815v1 [cond-mat.mtrl-sci])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09815">
<div class="article-summary-box-inner">
<span><p>We investigate the applicability of artificial neural networks (ANNs) in
reconstructing a sample image of a sponge-like microstructure. We propose to
reconstruct the image by predicting the phase of the current pixel based on its
causal neighbourhood, and subsequently, use a non-causal ANN model to smooth
out the reconstructed image as a form of post-processing. We also consider the
impacts of different configurations of the ANN model (e.g. number of densely
connected layers, number of neurons in each layer, the size of both the causal
and non-causal neighbourhood) on the models' predictive abilities quantified by
the discrepancy between the spatial statistics of the reference and the
reconstructed sample.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LSTC: Boosting Atomic Action Detection with Long-Short-Term Context. (arXiv:2110.09819v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09819">
<div class="article-summary-box-inner">
<span><p>In this paper, we place the atomic action detection problem into a Long-Short
Term Context (LSTC) to analyze how the temporal reliance among video signals
affect the action detection results. To do this, we decompose the action
recognition pipeline into short-term and long-term reliance, in terms of the
hypothesis that the two kinds of context are conditionally independent given
the objective action instance. Within our design, a local aggregation branch is
utilized to gather dense and informative short-term cues, while a high order
long-term inference branch is designed to reason the objective action class
from high-order interaction between actor and other person or person pairs.
Both branches independently predict the context-specific actions and the
results are merged in the end. We demonstrate that both temporal grains are
beneficial to atomic action recognition. On the mainstream benchmarks of atomic
action detection, our design can bring significant performance gain from the
existing state-of-the-art pipeline. The code of this project can be found at
[this url](https://github.com/TencentYoutuResearch/ActionDetection-LSTC)
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring Hidden Bias within Face Recognition via Racial Phenotypes. (arXiv:2110.09839v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09839">
<div class="article-summary-box-inner">
<span><p>Recent work reports disparate performance for intersectional racial groups
across face recognition tasks: face verification and identification. However,
the definition of those racial groups has a significant impact on the
underlying findings of such racial bias analysis. Previous studies define these
groups based on either demographic information (e.g. African, Asian etc.) or
skin tone (e.g. lighter or darker skins). The use of such sensitive or broad
group definitions has disadvantages for bias investigation and subsequent
counter-bias solutions design. By contrast, this study introduces an
alternative racial bias analysis methodology via facial phenotype attributes
for face recognition. We use the set of observable characteristics of an
individual face where a race-related facial phenotype is hence specific to the
human face and correlated to the racial profile of the subject. We propose
categorical test cases to investigate the individual influence of those
attributes on bias within face recognition tasks. We compare our
phenotype-based grouping methodology with previous grouping strategies and show
that phenotype-based groupings uncover hidden bias without reliance upon any
potentially protected attributes or ill-defined grouping strategies.
Furthermore, we contribute corresponding phenotype attribute category labels
for two face recognition tasks: RFW for face verification and VGGFace2 (test
set) for face identification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cutting Voxel Projector a New Approach to Construct 3D Cone Beam CT Operator. (arXiv:2110.09841v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09841">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce a new class of projectors for 3D cone beam
tomographic reconstruction. We find analytical formulas for the relationship
between the voxel volume projected onto a given detector pixel and its
contribution to the extinction value detected on that pixel. Using this
approach, we construct a near-exact projector and backprojector that can be
used especially for algebraic reconstruction techniques. We have implemented
this cutting voxel projector and a less accurate, speed-optimized version of it
together with two established projectors, a ray tracing projector based on
Siddon's algorithm and a TT footprint projector. We show that the cutting voxel
projector achieves, especially for large cone beam angles, noticeably higher
accuracy than the TT projector. Moreover, our implementation of the relaxed
version of the cutting voxel projector is significantly faster than current
footprint projector implementations. We further show that Siddon's algorithm
with comparable accuracy would be much slower than the cutting voxel projector.
All algorithms are implemented within an open source framework for algebraic
reconstruction in OpenCL 1.2 and C++ and are optimized for GPU computation.
They are published as open-source software under the GNU GPL 3 license, see
https://github.com/kulvait/KCT_cbct.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Object Detection via Generative Image Synthesis. (arXiv:2110.09848v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09848">
<div class="article-summary-box-inner">
<span><p>We present SSOD, the first end-to-end analysis-by synthesis framework with
controllable GANs for the task of self-supervised object detection. We use
collections of real world images without bounding box annotations to learn to
synthesize and detect objects. We leverage controllable GANs to synthesize
images with pre-defined object properties and use them to train object
detectors. We propose a tight end-to-end coupling of the synthesis and
detection networks to optimally train our system. Finally, we also propose a
method to optimally adapt SSOD to an intended target data without requiring
labels for it. For the task of car detection, on the challenging KITTI and
Cityscapes datasets, we show that SSOD outperforms the prior state-of-the-art
purely image-based self-supervised object detection method Wetectron. Even
without requiring any 3D CAD assets, it also surpasses the state-of-the-art
rendering based method Meta-Sim2. Our work advances the field of
self-supervised object detection by introducing a successful new paradigm of
using controllable GAN-based image synthesis for it and by significantly
improving the baseline accuracy of the task. We open-source our code at
https://github.com/NVlabs/SSOD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bilateral-ViT for Robust Fovea Localization. (arXiv:2110.09860v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09860">
<div class="article-summary-box-inner">
<span><p>The fovea is an important anatomical landmark of the retina. Detecting the
location of the fovea is essential for the analysis of many retinal diseases.
However, robust fovea localization remains a challenging problem, as the fovea
region often appears fuzzy, and retina diseases may further obscure its
appearance. This paper proposes a novel vision transformer (ViT) approach that
integrates information both inside and outside the fovea region to achieve
robust fovea localization. Our proposed network named
Bilateral-Vision-Transformer (Bilateral-ViT) consists of two network branches:
a transformer-based main network branch for integrating global context across
the entire fundus image and a vessel branch for explicitly incorporating the
structure of blood vessels. The encoded features from both network branches are
subsequently merged with a customized multi-scale feature fusion (MFF) module.
Our comprehensive experiments demonstrate that the proposed approach is
significantly more robust for diseased images and establishes the new state of
the arts on both Messidor and PALM datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning a self-supervised tone mapping operator via feature contrast masking loss. (arXiv:2110.09866v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09866">
<div class="article-summary-box-inner">
<span><p>High Dynamic Range (HDR) content is becoming ubiquitous due to the rapid
development of capture technologies. Nevertheless, the dynamic range of common
display devices is still limited, therefore tone mapping (TM) remains a key
challenge for image visualization. Recent work has demonstrated that neural
networks can achieve remarkable performance in this task when compared to
traditional methods, however, the quality of the results of these
learning-based methods is limited by the training data. Most existing works use
as training set a curated selection of best-performing results from existing
traditional tone mapping operators (often guided by a quality metric),
therefore, the quality of newly generated results is fundamentally limited by
the performance of such operators. This quality might be even further limited
by the pool of HDR content that is used for training. In this work we propose a
learning-based self-supervised tone mapping operator that is trained at test
time specifically for each HDR image and does not need any data labeling. The
key novelty of our approach is a carefully designed loss function built upon
fundamental knowledge on contrast perception that allows for directly comparing
the content in the HDR and tone mapped images. We achieve this goal by
reformulating classic VGG feature maps into feature contrast maps that
normalize local feature differences by their average magnitude in a local
neighborhood, allowing our loss to account for contrast masking effects. We
perform extensive ablation studies and exploration of parameters and
demonstrate that our solution outperforms existing approaches with a single set
of fixed parameters, as confirmed by both objective and subjective metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HM-Net: A Regression Network for Object Center Detection and Tracking on Wide Area Motion Imagery. (arXiv:2110.09881v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09881">
<div class="article-summary-box-inner">
<span><p>Wide Area Motion Imagery (WAMI) yields high resolution images with a large
number of extremely small objects. Target objects have large spatial
displacements throughout consecutive frames. This nature of WAMI images makes
object tracking and detection challenging. In this paper, we present our deep
neural network-based combined object detection and tracking model, namely, Heat
Map Network (HM-Net). HM-Net is significantly faster than state-of-the-art
frame differencing and background subtraction-based methods, without
compromising detection and tracking performances. HM-Net follows object
center-based joint detection and tracking paradigm. Simple heat map-based
predictions support unlimited number of simultaneous detections. The proposed
method uses two consecutive frames and the object detection heat map obtained
from the previous frame as input, which helps HM-Net monitor spatio-temporal
changes between frames and keeps track of previously predicted objects.
Although reuse of prior object detection heat map acts as a vital
feedback-based memory element, it can lead to unintended surge of false
positive detections. To increase robustness of the method against false
positives and to eliminate low confidence detections, HM-Net employs novel
feedback filters and advanced data augmentations. HM-Net outperforms
state-of-the-art WAMI moving object detection and tracking methods on WPAFB
dataset with its 96.2% F1 and 94.4% mAP detection scores, while achieving a
61.8% mAP tracking score on the same dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unrestricted Adversarial Attacks on ImageNet Competition. (arXiv:2110.09903v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09903">
<div class="article-summary-box-inner">
<span><p>Many works have investigated the adversarial attacks or defenses under the
settings where a bounded and imperceptible perturbation can be added to the
input. However in the real-world, the attacker does not need to comply with
this restriction. In fact, more threats to the deep model come from
unrestricted adversarial examples, that is, the attacker makes large and
visible modifications on the image, which causes the model classifying
mistakenly, but does not affect the normal observation in human perspective.
Unrestricted adversarial attack is a popular and practical direction but has
not been studied thoroughly. We organize this competition with the purpose of
exploring more effective unrestricted adversarial attack algorithm, so as to
accelerate the academical research on the model robustness under stronger
unbounded attacks. The competition is held on the TianChi platform
(\url{https://tianchi.aliyun.com/competition/entrance/531853/introduction}) as
one of the series of AI Security Challengers Program.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conditional De-Identification of 3D Magnetic Resonance Images. (arXiv:2110.09927v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09927">
<div class="article-summary-box-inner">
<span><p>Privacy protection of medical image data is challenging. Even if metadata is
removed, brain scans are vulnerable to attacks that match renderings of the
face to facial image databases. Solutions have been developed to de-identify
diagnostic scans by obfuscating or removing parts of the face. However, these
solutions either fail to reliably hide the patient's identity or are so
aggressive that they impair further analyses. We propose a new class of
de-identification techniques that, instead of removing facial features,
remodels them. Our solution relies on a conditional multi-scale GAN
architecture. It takes a patient's MRI scan as input and generates a 3D volume
conditioned on the patient's brain, which is preserved exactly, but where the
face has been de-identified through remodeling. We demonstrate that our
approach preserves privacy far better than existing techniques, without
compromising downstream medical analyses. Analyses were run on the OASIS-3 and
ADNI corpora.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeuralDiff: Segmenting 3D objects that move in egocentric videos. (arXiv:2110.09936v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09936">
<div class="article-summary-box-inner">
<span><p>Given a raw video sequence taken from a freely-moving camera, we study the
problem of decomposing the observed 3D scene into a static background and a
dynamic foreground containing the objects that move in the video sequence. This
task is reminiscent of the classic background subtraction problem, but is
significantly harder because all parts of the scene, static and dynamic,
generate a large apparent motion due to the camera large viewpoint change. In
particular, we consider egocentric videos and further separate the dynamic
component into objects and the actor that observes and moves them. We achieve
this factorization by reconstructing the video via a triple-stream neural
rendering network that explains the different motions based on corresponding
inductive biases. We demonstrate that our method can successfully separate the
different types of motion, outperforming recent neural rendering baselines at
this task, and can accurately segment moving objects. We do so by assessing the
method empirically on challenging videos from the EPIC-KITCHENS dataset which
we augment with appropriate annotations to create a new benchmark for the task
of dynamic object segmentation on unconstrained video sequences, for complex 3D
environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Talking Head Generation with Audio and Speech Related Facial Action Units. (arXiv:2110.09951v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09951">
<div class="article-summary-box-inner">
<span><p>The task of talking head generation is to synthesize a lip synchronized
talking head video by inputting an arbitrary face image and audio clips. Most
existing methods ignore the local driving information of the mouth muscles. In
this paper, we propose a novel recurrent generative network that uses both
audio and speech-related facial action units (AUs) as the driving information.
AU information related to the mouth can guide the movement of the mouth more
accurately. Since speech is highly correlated with speech-related AUs, we
propose an Audio-to-AU module in our system to predict the speech-related AU
information from speech. In addition, we use AU classifier to ensure that the
generated images contain correct AU information. Frame discriminator is also
constructed for adversarial training to improve the realism of the generated
face. We verify the effectiveness of our model on the GRID dataset and
TCD-TIMIT dataset. We also conduct an ablation study to verify the contribution
of each component in our model. Quantitative and qualitative experiments
demonstrate that our method outperforms existing methods in both image quality
and lip-sync accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Positional-Spectral-Temporal Attention in 3D Convolutional Neural Networks for EEG Emotion Recognition. (arXiv:2110.09955v1 [eess.SP])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09955">
<div class="article-summary-box-inner">
<span><p>Recognizing the feelings of human beings plays a critical role in our daily
communication. Neuroscience has demonstrated that different emotion states
present different degrees of activation in different brain regions, EEG
frequency bands and temporal stamps. In this paper, we propose a novel
structure to explore the informative EEG features for emotion recognition. The
proposed module, denoted by PST-Attention, consists of Positional, Spectral and
Temporal Attention modules to explore more discriminative EEG features.
Specifically, the Positional Attention module is to capture the activate
regions stimulated by different emotions in the spatial dimension. The Spectral
and Temporal Attention modules assign the weights of different frequency bands
and temporal slices respectively. Our method is adaptive as well as efficient
which can be fit into 3D Convolutional Neural Networks (3D-CNN) as a plug-in
module. We conduct experiments on two real-world datasets. 3D-CNN combined with
our module achieves promising results and demonstrate that the PST-Attention is
able to capture stable patterns for emotion recognition from EEG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fully Three-dimensional Radial Visualization. (arXiv:2110.09971v1 [stat.ME])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09971">
<div class="article-summary-box-inner">
<span><p>We develop methodology for three-dimensional (3D) radial visualization
(RadViz) of multidimensional datasets. The classical two-dimensional (2D)
RadViz visualizes multivariate data in the 2D plane by mapping every
observation to a point inside the unit circle. Our tool, RadViz3D, distributes
anchor points uniformly on the 3D unit sphere. We show that this uniform
distribution provides the best visualization with minimal artificial visual
correlation for data with uncorrelated variables. However, anchor points can be
placed exactly equi-distant from each other only for the five Platonic solids,
so we provide equi-distant anchor points for these five settings, and
approximately equi-distant anchor points via a Fibonacci grid for the other
cases. Our methodology, implemented in the R package $radviz3d$, makes fully 3D
RadViz possible and is shown to improve the ability of this nonlinear technique
in more faithfully displaying simulated data as well as the crabs, olive oils
and wine datasets. Additionally, because radial visualization is naturally
suited for compositional data, we use RadViz3D to illustrate (i) the chemical
composition of Longquan celadon ceramics and their Jingdezhen imitation over
centuries, and (ii) US regional SARS-Cov-2 variants' prevalence in the Covid-19
pandemic during the summer 2021 surge of the Delta variant.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Optimal Correlational Object Search. (arXiv:2110.09991v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09991">
<div class="article-summary-box-inner">
<span><p>In realistic applications of object search, robots will need to locate target
objects in complex environments while coping with unreliable sensors,
especially for small or hard-to-detect objects. In such settings, correlational
information can be valuable for planning efficiently: when looking for a fork,
the robot could start by locating the easier-to-detect refrigerator, since
forks would probably be found nearby. Previous approaches to object search with
correlational information typically resort to ad-hoc or greedy search
strategies. In this paper, we propose the Correlational Object Search POMDP
(COS-POMDP), which can be solved to produce search strategies that use
correlational information. COS-POMDPs contain a correlation-based observation
model that allows us to avoid the exponential blow-up of maintaining a joint
belief about all objects, while preserving the optimal solution to this naive,
exponential POMDP formulation. We propose a hierarchical planning algorithm to
scale up COS-POMDP for practical domains. We conduct experiments using
AI2-THOR, a realistic simulator of household environments, as well as YOLOv5, a
widely-used object detector. Our results show that, particularly for
hard-to-detect objects, such as scrub brush and remote control, our method
offers the most robust performance compared to baselines that ignore
correlations as well as a greedy, next-best view approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ERQA: Edge-Restoration Quality Assessment for Video Super-Resolution. (arXiv:2110.09992v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09992">
<div class="article-summary-box-inner">
<span><p>Despite the growing popularity of video super-resolution (VSR), there is
still no good way to assess the quality of the restored details in upscaled
frames. Some SR methods may produce the wrong digit or an entirely different
face. Whether a method's results are trustworthy depends on how well it
restores truthful details. Image super-resolution can use natural distributions
to produce a high-resolution image that is only somewhat similar to the real
one. VSR enables exploration of additional information in neighboring frames to
restore details from the original scene. The ERQA metric, which we propose in
this paper, aims to estimate a model's ability to restore real details using
VSR. On the assumption that edges are significant for detail and character
recognition, we chose edge fidelity as the foundation for this metric.
Experimental validation of our work is based on the MSU Video Super-Resolution
Benchmark, which includes the most difficult patterns for detail restoration
and verifies the fidelity of details from the original frame. Code for the
proposed metric is publicly available at
https://github.com/msu-video-group/ERQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DPFM: Deep Partial Functional Maps. (arXiv:2110.09994v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09994">
<div class="article-summary-box-inner">
<span><p>We consider the problem of computing dense correspondences between non-rigid
shapes with potentially significant partiality. Existing formulations tackle
this problem through heavy manifold optimization in the spectral domain, given
hand-crafted shape descriptors. In this paper, we propose the first learning
method aimed directly at partial non-rigid shape correspondence. Our approach
uses the functional map framework, can be trained in a supervised or
unsupervised manner, and learns descriptors directly from the data, thus both
improving robustness and accuracy in challenging cases. Furthermore, unlike
existing techniques, our method is also applicable to partial-to-partial
non-rigid matching, in which the common regions on both shapes are unknown a
priori. We demonstrate that the resulting method is data-efficient, and
achieves state-of-the-art results on several benchmark datasets. Our code and
data can be found online: https://github.com/pvnieo/DPFM
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data-driven and Automatic Surface Texture Analysis Using Persistent Homology. (arXiv:2110.10005v1 [eess.SP])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10005">
<div class="article-summary-box-inner">
<span><p>Surface roughness plays an important role in analyzing engineering surfaces.
It quantifies the surface topography and can be used to determine whether the
resulting surface finish is acceptable or not. Nevertheless, while several
existing tools and standards are available for computing surface roughness,
these methods rely heavily on user input thus slowing down the analysis and
increasing manufacturing costs. Therefore, fast and automatic determination of
the roughness level is essential to avoid costs resulting from surfaces with
unacceptable finish, and user-intensive analysis. In this study, we propose a
Topological Data Analysis (TDA) based approach to classify the roughness level
of synthetic surfaces using both their areal images and profiles. We utilize
persistent homology from TDA to generate persistence diagrams that encapsulate
information on the shape of the surface. We then obtain feature matrices for
each surface or profile using Carlsson coordinates, persistence images, and
template functions. We compare our results to two widely used methods in the
literature: Fast Fourier Transform (FFT) and Gaussian filtering. The results
show that our approach yields mean accuracies as high as 97%. We also show
that, in contrast to existing surface analysis tools, our TDA-based approach is
fully automatable and provides adaptive feature extraction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Online Continual Learning on Class Incremental Blurry Task Configuration with Anytime Inference. (arXiv:2110.10031v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10031">
<div class="article-summary-box-inner">
<span><p>Despite rapid advances in continual learning, a large body of research is
devoted to improving performance in the existing setups. While a handful of
work do propose new continual learning setups, they still lack practicality in
certain aspects. For better practicality, we first propose a novel continual
learning setup that is online, task-free, class-incremental, of blurry task
boundaries and subject to inference queries at any moment. We additionally
propose a new metric to better measure the performance of the continual
learning methods subject to inference queries at any moment. To address the
challenging setup and evaluation protocol, we propose an effective method that
employs a new memory management scheme and novel learning techniques. Our
empirical validation demonstrates that the proposed method outperforms prior
arts by large margins.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Tail-Class Representation with Centroid Contrastive Learning. (arXiv:2110.10048v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10048">
<div class="article-summary-box-inner">
<span><p>In vision domain, large-scale natural datasets typically exhibit long-tailed
distribution which has large class imbalance between head and tail classes.
This distribution poses difficulty in learning good representations for tail
classes. Recent developments have shown good long-tailed model can be learnt by
decoupling the training into representation learning and classifier balancing.
However, these works pay insufficient consideration on the long-tailed effect
on representation learning. In this work, we propose interpolative centroid
contrastive learning (ICCL) to improve long-tailed representation learning.
ICCL interpolates two images from a class-agnostic sampler and a class-aware
sampler, and trains the model such that the representation of the interpolative
image can be used to retrieve the centroids for both source classes. We
demonstrate the effectiveness of our approach on multiple long-tailed image
classification benchmarks. Our result shows a significant accuracy gain of 2.8%
on the iNaturalist 2018 dataset with a real-world long-tailed distribution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stochastic Primal-Dual Deep Unrolling Networks for Imaging Inverse Problems. (arXiv:2110.10093v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10093">
<div class="article-summary-box-inner">
<span><p>In this work we present a new type of efficient deep-unrolling networks for
solving imaging inverse problems. Classical deep-unrolling methods require full
forward operator and its adjoint across each layer, and hence can be
computationally more expensive than other end-to-end methods such as
FBP-ConvNet, especially in 3D image reconstruction tasks. We propose a
stochastic (ordered-subsets) extension of the Learned Primal-Dual (LPD) which
is the state-of-the-art unrolling network. In our unrolling network, we only
use a subset of the forward and adjoint operator, to achieve computational
efficiency. We consider 3 ways of training the proposed network to cope with
different scenarios of the availability of the training data, including (1)
supervised training on paired data, (2) unsupervised adversarial training which
enable us to train the network without paired ground-truth data, (3)
equivariant self-supervised training approach, which utilizes equivariant
structure which is prevalent in many imaging applications, and only requires
measurement data. Our numerical results demonstrate the effectiveness of our
approach in X-ray CT imaging task, showing that our networks achieve similar
reconstruction accuracies as the full-batch LPD, while require only a fraction
of the computation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Generalization through Audio-Visual Relative Norm Alignment in First Person Action Recognition. (arXiv:2110.10101v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10101">
<div class="article-summary-box-inner">
<span><p>First person action recognition is becoming an increasingly researched area
thanks to the rising popularity of wearable cameras. This is bringing to light
cross-domain issues that are yet to be addressed in this context. Indeed, the
information extracted from learned representations suffers from an intrinsic
"environmental bias". This strongly affects the ability to generalize to unseen
scenarios, limiting the application of current methods to real settings where
labeled data are not available during training. In this work, we introduce the
first domain generalization approach for egocentric activity recognition, by
proposing a new audio-visual loss, called Relative Norm Alignment loss. It
re-balances the contributions from the two modalities during training, over
different domains, by aligning their feature norm representations. Our approach
leads to strong results in domain generalization on both EPIC-Kitchens-55 and
EPIC-Kitchens-100, as demonstrated by extensive experiments, and can be
extended to work also on domain adaptation settings with competitive results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BUSIS: A Benchmark for Breast Ultrasound Image Segmentation. (arXiv:1801.03182v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1801.03182">
<div class="article-summary-box-inner">
<span><p>Breast ultrasound (BUS) image segmentation is challenging and critical for
BUS Comput-er-Aided Diagnosis (CAD) systems. Many BUS segmentation approaches
have been studied in the last two decades, but the performances of most
approaches have been assessed using relatively small private datasets with
different quantitative metrics, which results in a discrepancy in performance
comparison. Therefore, there is a pressing need for building a benchmark to
compare existing methods using a public dataset objectively, to determine the
performance of the best breast tumor segmentation algorithm available today,
and to investigate what segmentation strategies are valuable in clinical
practice and theoretical study. In this work, a benchmark for B-mode breast
ultrasound image segmentation is presented. In the benchmark, 1) we collected
562 breast ultrasound images, prepared a software tool, and involved four
radiologists in obtaining accurate annotations through standardized procedures;
2) we extensively compared the performance of sixteen state-of-the-art
segmentation methods and discussed their advantages and disadvantages; 3) we
proposed a set of valuable quantitative metrics to evaluate both semi-automatic
and fully automatic segmentation approaches; and 4) the successful segmentation
strategies and possible future improvements are discussed in details.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoScale: Learning to Scale for Crowd Counting and Localization. (arXiv:1912.09632v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1912.09632">
<div class="article-summary-box-inner">
<span><p>Recent works on crowd counting mainly leverage CNNs to count by regressing
density maps, and have achieved great progress. In the density map, each person
is represented by a Gaussian blob, and the final count is obtained from the
integration of the whole map. However, it is difficult to accurately predict
the density map on dense regions. A major issue is that the density map on
dense regions usually accumulates density values from a number of nearby
Gaussian blobs, yielding different large density values on a small set of
pixels. This makes the density map present variant patterns with significant
pattern shifts and brings a long-tailed distribution of pixel-wise density
values. We propose a simple and effective Learning to Scale (L2S) module, which
automatically scales dense regions into reasonable closeness levels (reflecting
image-plane distance between neighboring people). L2S directly normalizes the
closeness in different patches such that it dynamically separates the
overlapped blobs, decomposes the accumulated values in the ground-truth density
map, and thus alleviates the pattern shifts and long-tailed distribution of
density values. This helps the model to better learn the density map. We also
explore the effectiveness of L2S in localizing people by finding the local
minima of the quantized distance (w.r.t. person location map). To the best of
our knowledge, such a localization method is also novel in localization-based
crowd counting. We further introduce a customized dynamic cross-entropy loss,
significantly improving the localization-based model optimization. Extensive
experiments demonstrate that the proposed framework termed AutoScale improves
upon some state-of-the-art methods in both regression and localization
benchmarks on three crowded datasets and achieves very competitive performance
on two sparse datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DriverMHG: A Multi-Modal Dataset for Dynamic Recognition of Driver Micro Hand Gestures and a Real-Time Recognition Framework. (arXiv:2003.00951v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.00951">
<div class="article-summary-box-inner">
<span><p>The use of hand gestures provides a natural alternative to cumbersome
interface devices for Human-Computer Interaction (HCI) systems. However,
real-time recognition of dynamic micro hand gestures from video streams is
challenging for in-vehicle scenarios since (i) the gestures should be performed
naturally without distracting the driver, (ii) micro hand gestures occur within
very short time intervals at spatially constrained areas, (iii) the performed
gesture should be recognized only once, and (iv) the entire architecture should
be designed lightweight as it will be deployed to an embedded system. In this
work, we propose an HCI system for dynamic recognition of driver micro hand
gestures, which can have a crucial impact in automotive sector especially for
safety related issues. For this purpose, we initially collected a dataset named
Driver Micro Hand Gestures (DriverMHG), which consists of RGB, depth and
infrared modalities. The challenges for dynamic recognition of micro hand
gestures have been addressed by proposing a lightweight convolutional neural
network (CNN) based architecture which operates online efficiently with a
sliding window approach. For the CNN model, several 3-dimensional resource
efficient networks are applied and their performances are analyzed. Online
recognition of gestures has been performed with 3D-MobileNetV2, which provided
the best offline accuracy among the applied networks with similar computational
complexities. The final architecture is deployed on a driver simulator
operating in real-time. We make DriverMHG dataset and our source code publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FairMOT: On the Fairness of Detection and Re-Identification in Multiple Object Tracking. (arXiv:2004.01888v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.01888">
<div class="article-summary-box-inner">
<span><p>Multi-object tracking (MOT) is an important problem in computer vision which
has a wide range of applications. Formulating MOT as multi-task learning of
object detection and re-ID in a single network is appealing since it allows
joint optimization of the two tasks and enjoys high computation efficiency.
However, we find that the two tasks tend to compete with each other which need
to be carefully addressed. In particular, previous works usually treat re-ID as
a secondary task whose accuracy is heavily affected by the primary detection
task. As a result, the network is biased to the primary detection task which is
not fair to the re-ID task. To solve the problem, we present a simple yet
effective approach termed as FairMOT based on the anchor-free object detection
architecture CenterNet. Note that it is not a naive combination of CenterNet
and re-ID. Instead, we present a bunch of detailed designs which are critical
to achieve good tracking results by thorough empirical studies. The resulting
approach achieves high accuracy for both detection and tracking. The approach
outperforms the state-of-the-art methods by a large margin on several public
datasets. The source code and pre-trained models are released at
https://github.com/ifzhang/FairMOT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reintroducing Straight-Through Estimators as Principled Methods for Stochastic Binary Networks. (arXiv:2006.06880v4 [stat.ML] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.06880">
<div class="article-summary-box-inner">
<span><p>Training neural networks with binary weights and activations is a challenging
problem due to the lack of gradients and difficulty of optimization over
discrete weights. Many successful experimental results have been achieved with
empirical straight-through (ST) approaches, proposing a variety of ad-hoc rules
for propagating gradients through non-differentiable activations and updating
discrete weights. At the same time, ST methods can be truly derived as
estimators in the stochastic binary network (SBN) model with Bernoulli weights.
We advance these derivations to a more complete and systematic study. We
analyze properties, estimation accuracy, obtain different forms of correct ST
estimators for activations and weights, explain existing empirical approaches
and their shortcomings, explain how latent weights arise from the mirror
descent method when optimizing over probabilities. This allows to reintroduce
ST methods, long known empirically, as sound approximations, apply them with
clarity and develop further improvements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-View Fusion of Sensor Data for Improved Perception and Prediction in Autonomous Driving. (arXiv:2008.11901v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.11901">
<div class="article-summary-box-inner">
<span><p>We present an end-to-end method for object detection and trajectory
prediction utilizing multi-view representations of LiDAR returns and camera
images. In this work, we recognize the strengths and weaknesses of different
view representations, and we propose an efficient and generic fusing method
that aggregates benefits from all views. Our model builds on a state-of-the-art
Bird's-Eye View (BEV) network that fuses voxelized features from a sequence of
historical LiDAR data as well as rasterized high-definition map to perform
detection and prediction tasks. We extend this model with additional LiDAR
Range-View (RV) features that use the raw LiDAR information in its native,
non-quantized representation. The RV feature map is projected into BEV and
fused with the BEV features computed from LiDAR and high-definition map. The
fused features are then further processed to output the final detections and
trajectories, within a single end-to-end trainable network. In addition, the RV
fusion of LiDAR and camera is performed in a straightforward and
computationally efficient manner using this framework. The proposed multi-view
fusion approach improves the state-of-the-art on proprietary large-scale
real-world data collected by a fleet of self-driving vehicles, as well as on
the public nuScenes data set with minimal increases on the computational cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep-LIBRA: Artificial intelligence method for robust quantification of breast density with independent validation in breast cancer risk assessment. (arXiv:2011.08001v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.08001">
<div class="article-summary-box-inner">
<span><p>Breast density is an important risk factor for breast cancer that also
affects the specificity and sensitivity of screening mammography. Current
federal legislation mandates reporting of breast density for all women
undergoing breast screening. Clinically, breast density is assessed visually
using the American College of Radiology Breast Imaging Reporting And Data
System (BI-RADS) scale. Here, we introduce an artificial intelligence (AI)
method to estimate breast percentage density (PD) from digital mammograms. Our
method leverages deep learning (DL) using two convolutional neural network
architectures to accurately segment the breast area. A machine-learning
algorithm combining superpixel generation, texture feature analysis, and
support vector machine is then applied to differentiate dense from non-dense
tissue regions, from which PD is estimated. Our method has been trained and
validated on a multi-ethnic, multi-institutional dataset of 15,661 images
(4,437 women), and then tested on an independent dataset of 6,368 digital
mammograms (1,702 women; cases=414) for both PD estimation and discrimination
of breast cancer. On the independent dataset, PD estimates from Deep-LIBRA and
an expert reader were strongly correlated (Spearman correlation coefficient =
0.90). Moreover, Deep-LIBRA yielded a higher breast cancer discrimination
performance (area under the ROC curve, AUC = 0.611 [95% confidence interval
(CI): 0.583, 0.639]) compared to four other widely-used research and commercial
PD assessment methods (AUCs = 0.528 to 0.588). Our results suggest a strong
agreement of PD estimates between Deep-LIBRA and gold-standard assessment by an
expert reader, as well as improved performance in breast cancer risk assessment
over state-of-the-art open-source and commercial methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Cats and Dogs: Semi-supervised Classification of fuzzy labels with overclustering. (arXiv:2012.01768v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.01768">
<div class="article-summary-box-inner">
<span><p>A long-standing issue with deep learning is the need for large and
consistently labeled datasets. Although the current research in semi-supervised
learning can decrease the required amount of annotated data by a factor of 10
or even more, this line of research still uses distinct classes like cats and
dogs. However, in the real-world we often encounter problems where different
experts have different opinions, thus producing fuzzy labels. We propose a
novel framework for handling semi-supervised classifications of such fuzzy
labels. Our framework is based on the idea of overclustering to detect
substructures in these fuzzy labels. We propose a novel loss to improve the
overclustering capability of our framework and show on the common image
classification dataset STL-10 that it is faster and has better overclustering
performance than previous work. On a real-world plankton dataset, we illustrate
the benefit of overclustering for fuzzy labels and show that we beat previous
state-of-the-art semisupervised methods. Moreover, we acquire 5 to 10% more
consistent predictions of substructures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Models as Distributions of Functions. (arXiv:2102.04776v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.04776">
<div class="article-summary-box-inner">
<span><p>Generative models are typically trained on grid-like data such as images. As
a result, the size of these models usually scales directly with the underlying
grid resolution. In this paper, we abandon discretized grids and instead
parameterize individual data points by continuous functions. We then build
generative models by learning distributions over such functions. By treating
data points as functions, we can abstract away from the specific type of data
we train on and construct models that are agnostic to discretization. To train
our model, we use an adversarial approach with a discriminator that acts on
continuous signals. Through experiments on a wide variety of data modalities
including images, 3D shapes and climate data, we demonstrate that our model can
learn rich distributions of functions independently of data type and
resolution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Fully Convolutional Neural Networks with Intersection Over Union Loss for Crop Mapping from Multi-Temporal Satellite Images. (arXiv:2102.07280v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.07280">
<div class="article-summary-box-inner">
<span><p>Information on cultivated crops is relevant for a large number of food
security studies. Different scientific efforts are dedicated to generating this
information from remote sensing images by means of machine learning methods.
Unfortunately, these methods do not take account of the spatial-temporal
relationships inherent in remote sensing images. In our paper, we explore the
capability of a 3D Fully Convolutional Neural Network (FCN) to map crop types
from multi-temporal images. In addition, we propose the Intersection Over Union
(IOU) loss function for increasing the overlap between the predicted classes
and ground reference data. The proposed method was applied to identify soybean
and corn from a study area situated in the US corn belt using multi-temporal
Landsat images. The study shows that our method outperforms related methods,
obtaining a Kappa coefficient of 91.8%. We conclude that using the IOU loss
function provides a superior choice to learn individual crop types.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Axiomatic Explanations for Visual Search, Retrieval, and Similarity Learning. (arXiv:2103.00370v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.00370">
<div class="article-summary-box-inner">
<span><p>Visual search, recommendation, and contrastive similarity learning power a
wide breadth of technologies that impact billions of users across the world.
The best-performing approaches are often complex and difficult to interpret,
and there are several competing techniques one can use to explain a search
engine's behavior. We show that the theory of fair credit assignment provides a
unique axiomatic solution that generalizes several existing recommendation- and
metric-explainability techniques in the literature. Using this formalism, we
are able to determine in what regimes existing approaches fall short of
fairness and provide variations that are fair in more situations and handle
counterfactual information. More specifically, we show existing approaches
implicitly approximate second-order Shapley-Taylor indices and use this
perspective to extend CAM, GradCAM, LIME, SHAP, SBSM, and other methods to
search engines. These extensions can extract pairwise correspondences between
images from trained black-box models. We also introduce a fast kernel-based
method for estimating Shapley-Taylor indices that require orders of magnitude
fewer function evaluations to converge. Finally, we evaluate these methods and
show that these game-theoretic measures yield more consistent explanations for
image similarity architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Application of Image-to-Image Translation: Chromosome Straightening Framework by Learning from a Single Image. (arXiv:2103.02835v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.02835">
<div class="article-summary-box-inner">
<span><p>In medical imaging, chromosome straightening plays a significant role in the
pathological study of chromosomes and in the development of cytogenetic maps.
Whereas different approaches exist for the straightening task, typically
geometric algorithms are used whose outputs are characterized by jagged edges
or fragments with discontinued banding patterns. To address the flaws in the
geometric algorithms, we propose a novel framework based on image-to-image
translation to learn a pertinent mapping dependence for synthesizing
straightened chromosomes with uninterrupted banding patterns and preserved
details. In addition, to avoid the pitfall of deficient input chromosomes, we
construct an augmented dataset using only one single curved chromosome image
for training models. Based on this framework, we apply two popular
image-to-image translation architectures, U-shape networks and conditional
generative adversarial networks, to assess its efficacy. Experiments on a
dataset comprised of 642 real-world chromosomes demonstrate the superiority of
our framework, as compared to the geometric method in straightening
performance, by rendering realistic and continued chromosome details.
Furthermore, our straightened results improve the chromosome classification by
0.98%-1.39% mean accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual Attention-in-Attention Model for Joint Rain Streak and Raindrop Removal. (arXiv:2103.07051v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.07051">
<div class="article-summary-box-inner">
<span><p>Rain streaks and rain drops are two natural phenomena, which degrade image
capture in different ways. Currently, most existing deep deraining networks
take them as two distinct problems and individually address one, and thus
cannot deal adequately with both simultaneously. To address this, we propose a
Dual Attention-in-Attention Model (DAiAM) which includes two DAMs for removing
both rain streaks and raindrops. Inside the DAM, there are two attentive maps -
each of which attends to the heavy and light rainy regions, respectively, to
guide the deraining process differently for applicable regions. In addition, to
further refine the result, a Differential-driven Dual Attention-in-Attention
Model (D-DAiAM) is proposed with a "heavy-to-light" scheme to remove rain via
addressing the unsatisfying deraining regions. Extensive experiments on one
public raindrop dataset, one public rain streak and our synthesized joint rain
streak and raindrop (JRSRD) dataset have demonstrated that the proposed method
not only is capable of removing rain streaks and raindrops simultaneously, but
also achieves the state-of-the-art performance on both tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Information Maximization Clustering via Multi-View Self-Labelling. (arXiv:2103.07368v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.07368">
<div class="article-summary-box-inner">
<span><p>Image clustering is a particularly challenging computer vision task, which
aims to generate annotations without human supervision. Recent advances focus
on the use of self-supervised learning strategies in image clustering, by first
learning valuable semantics and then clustering the image representations.
These multiple-phase algorithms, however, increase the computational time and
their final performance is reliant on the first stage. By extending the
self-supervised approach, we propose a novel single-phase clustering method
that simultaneously learns meaningful representations and assigns the
corresponding annotations. This is achieved by integrating a discrete
representation into the self-supervised paradigm through a classifier net.
Specifically, the proposed clustering objective employs mutual information, and
maximizes the dependency between the integrated discrete representation and a
discrete probability distribution. The discrete probability distribution is
derived though the self-supervised process by comparing the learnt latent
representation with a set of trainable prototypes. To enhance the learning
performance of the classifier, we jointly apply the mutual information across
multi-crop views. Our empirical results show that the proposed framework
outperforms state-of-the-art techniques with the average accuracy of 89.1% and
49.0%, respectively, on CIFAR-10 and CIFAR-100/20 datasets. Finally, the
proposed method also demonstrates attractive robustness to parameter settings,
making it ready to be applicable to other datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FakeMix Augmentation Improves Transparent Object Detection. (arXiv:2103.13279v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.13279">
<div class="article-summary-box-inner">
<span><p>Detecting transparent objects in natural scenes is challenging due to the low
contrast in texture, brightness and colors. Recent deep-learning-based works
reveal that it is effective to leverage boundaries for transparent object
detection (TOD). However, these methods usually encounter boundary-related
imbalance problem, leading to limited generation capability. Detailly, a kind
of boundaries in the background, which share the same characteristics with
boundaries of transparent objects but have much smaller amounts, usually hurt
the performance. To conquer the boundary-related imbalance problem, we propose
a novel content-dependent data augmentation method termed FakeMix. Considering
collecting these trouble-maker boundaries in the background is hard without
corresponding annotations, we elaborately generate them by appending the
boundaries of transparent objects from other samples into the current image
during training, which adjusts the data space and improves the generalization
of the models. Further, we present AdaptiveASPP, an enhanced version of ASPP,
that can capture multi-scale and cross-modality features dynamically. Extensive
experiments demonstrate that our methods clearly outperform the
state-of-the-art methods. We also show that our approach can also transfer well
on related tasks, in which the model meets similar troubles, such as mirror
detection, glass detection, and camouflaged object detection. Code will be made
publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Novel Scene Compositions from Single Images and Videos. (arXiv:2103.13389v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.13389">
<div class="article-summary-box-inner">
<span><p>Given a large dataset for training, GANs can achieve remarkable performance
for the image synthesis task. However, training GANs in extremely low data
regimes remains a challenge, as overfitting often occurs, leading to
memorization or training divergence. In this work, we introduce SIV-GAN, an
unconditional generative model that can generate new scene compositions from a
single training image or a single video clip. We propose a two-branch
discriminator architecture, with content and layout branches designed to judge
internal content and scene layout realism separately from each other. This
discriminator design enables synthesis of visually plausible, novel
compositions of a scene, with varying content and layout, while preserving the
context of the original sample. Compared to previous single-image GANs, our
model generates more diverse, higher quality images, while not being restricted
to a single image setting. We show that SIV-GAN successfully deals with a new
challenging task of learning from a single video, for which prior GAN models
fail to achieve synthesis of both high quality and diversity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Broaden Your Views for Self-Supervised Video Learning. (arXiv:2103.16559v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.16559">
<div class="article-summary-box-inner">
<span><p>Most successful self-supervised learning methods are trained to align the
representations of two independent views from the data. State-of-the-art
methods in video are inspired by image techniques, where these two views are
similarly extracted by cropping and augmenting the resulting crop. However,
these methods miss a crucial element in the video domain: time. We introduce
BraVe, a self-supervised learning framework for video. In BraVe, one of the
views has access to a narrow temporal window of the video while the other view
has a broad access to the video content. Our models learn to generalise from
the narrow view to the general content of the video. Furthermore, BraVe
processes the views with different backbones, enabling the use of alternative
augmentations or modalities into the broad view such as optical flow, randomly
convolved RGB frames, audio or their combinations. We demonstrate that BraVe
achieves state-of-the-art results in self-supervised representation learning on
standard video and audio classification benchmarks including UCF101, HMDB51,
Kinetics, ESC-50 and AudioSet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image-Level or Object-Level? A Tale of Two Resampling Strategies for Long-Tailed Detection. (arXiv:2104.05702v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05702">
<div class="article-summary-box-inner">
<span><p>Training on datasets with long-tailed distributions has been challenging for
major recognition tasks such as classification and detection. To deal with this
challenge, image resampling is typically introduced as a simple but effective
approach. However, we observe that long-tailed detection differs from
classification since multiple classes may be present in one image. As a result,
image resampling alone is not enough to yield a sufficiently balanced
distribution at the object level. We address object-level resampling by
introducing an object-centric memory replay strategy based on dynamic, episodic
memory banks. Our proposed strategy has two benefits: 1) convenient
object-level resampling without significant extra computation, and 2) implicit
feature-level augmentation from model updates. We show that image-level and
object-level resamplings are both important, and thus unify them with a joint
resampling strategy (RIO). Our method outperforms state-of-the-art long-tailed
detection and segmentation methods on LVIS v0.5 across various backbones. Code
is available at https://github.com/NVlabs/RIO.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Permutation Equivariant Structure from Motion. (arXiv:2104.06703v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06703">
<div class="article-summary-box-inner">
<span><p>Existing deep methods produce highly accurate 3D reconstructions in stereo
and multiview stereo settings, i.e., when cameras are both internally and
externally calibrated. Nevertheless, the challenge of simultaneous recovery of
camera poses and 3D scene structure in multiview settings with deep networks is
still outstanding. Inspired by projective factorization for Structure from
Motion (SFM) and by deep matrix completion techniques, we propose a neural
network architecture that, given a set of point tracks in multiple images of a
static scene, recovers both the camera parameters and a (sparse) scene
structure by minimizing an unsupervised reprojection loss. Our network
architecture is designed to respect the structure of the problem: the sought
output is equivariant to permutations of both cameras and scene points.
Notably, our method does not require initialization of camera parameters or 3D
point locations. We test our architecture in two setups: (1) single scene
reconstruction and (2) learning from multiple scenes. Our experiments,
conducted on a variety of datasets in both internally calibrated and
uncalibrated settings, indicate that our method accurately recovers pose and
structure, on par with classical state of the art methods. Additionally, we
show that a pre-trained network can be used to reconstruct novel scenes using
inexpensive fine-tuning with no loss of accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data-Driven 3D Reconstruction of Dressed Humans From Sparse Views. (arXiv:2104.08013v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08013">
<div class="article-summary-box-inner">
<span><p>Recently, data-driven single-view reconstruction methods have shown great
progress in modeling 3D dressed humans. However, such methods suffer heavily
from depth ambiguities and occlusions inherent to single view inputs. In this
paper, we tackle this problem by considering a small set of input views and
investigate the best strategy to suitably exploit information from these views.
We propose a data-driven end-to-end approach that reconstructs an implicit 3D
representation of dressed humans from sparse camera views. Specifically, we
introduce three key components: first a spatially consistent reconstruction
that allows for arbitrary placement of the person in the input views using a
perspective camera model; second an attention-based fusion layer that learns to
aggregate visual information from several viewpoints; and third a mechanism
that encodes local 3D patterns under the multi-view context. In the
experiments, we show the proposed approach outperforms the state of the art on
standard data both quantitatively and qualitatively. To demonstrate the
spatially consistent reconstruction, we apply our approach to dynamic scenes.
Additionally, we apply our method on real data acquired with a multi-camera
platform and demonstrate our approach can obtain results comparable to
multi-view stereo with dramatically less views.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SeaDronesSee: A Maritime Benchmark for Detecting Humans in Open Water. (arXiv:2105.01922v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.01922">
<div class="article-summary-box-inner">
<span><p>Unmanned Aerial Vehicles (UAVs) are of crucial importance in search and
rescue missions in maritime environments due to their flexible and fast
operation capabilities. Modern computer vision algorithms are of great interest
in aiding such missions. However, they are dependent on large amounts of
real-case training data from UAVs, which is only available for traffic
scenarios on land. Moreover, current object detection and tracking data sets
only provide limited environmental information or none at all, neglecting a
valuable source of information. Therefore, this paper introduces a large-scaled
visual object detection and tracking benchmark (SeaDronesSee) aiming to bridge
the gap from land-based vision systems to sea-based ones. We collect and
annotate over 54,000 frames with 400,000 instances captured from various
altitudes and viewing angles ranging from 5 to 260 meters and 0 to 90 degrees
while providing the respective meta information for altitude, viewing angle and
other meta data. We evaluate multiple state-of-the-art computer vision
algorithms on this newly established benchmark serving as baselines. We provide
an evaluation server where researchers can upload their prediction and compare
their results on a central leaderboard
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Iterative Human and Automated Identification of Wildlife Images. (arXiv:2105.02320v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.02320">
<div class="article-summary-box-inner">
<span><p>Camera trapping is increasingly used to monitor wildlife, but this technology
typically requires extensive data annotation. Recently, deep learning has
significantly advanced automatic wildlife recognition. However, current methods
are hampered by a dependence on large static data sets when wildlife data is
intrinsically dynamic and involves long-tailed distributions. These two
drawbacks can be overcome through a hybrid combination of machine learning and
humans in the loop. Our proposed iterative human and automated identification
approach is capable of learning from wildlife imagery data with a long-tailed
distribution. Additionally, it includes self-updating learning that facilitates
capturing the community dynamics of rapidly changing natural systems. Extensive
experiments show that our approach can achieve a ~90% accuracy employing only
~20% of the human annotations of existing approaches. Our synergistic
collaboration of humans and machines transforms deep learning from a relatively
inefficient post-annotation tool to a collaborative on-going annotation tool
that vastly relieves the burden of human annotation and enables efficient and
constant model updates.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Brain Inspired Face Recognition: A Computational Framework. (arXiv:2105.07237v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.07237">
<div class="article-summary-box-inner">
<span><p>This paper presents a new proposal of an efficient computational model of
face recognition which uses cues from the distributed face recognition
mechanism of the brain, and by gathering engineering equivalent of these cues
from existing literature. Three distinct and widely used features: Histogram of
Oriented Gradients (HOG), Local Binary Patterns (LBP), and Principal components
(PCs) extracted from target images are used in a manner which is simple, and
yet effective. The HOG and LBP features further undergo principal component
analysis for dimensionality reduction. Our model uses multi-layer perceptrons
(MLP) to classify these three features and fuse them at the decision level
using sum rule. A computational theory is first developed by using concepts
from the information processing mechanism of the brain. Extensive experiments
are carried out using ten publicly available datasets to validate our proposed
model's performance in recognizing faces with extreme variation of
illumination, pose angle, expression, and background. Results obtained are
extremely promising when compared with other face recognition algorithms
including CNN and deep learning-based methods. This highlights that simple
computational processes, if clubbed properly, can produce competing performance
with best algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A parameter refinement method for Ptychography based on Deep Learning concepts. (arXiv:2105.08058v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.08058">
<div class="article-summary-box-inner">
<span><p>X-ray Ptychography is an advanced computational microscopy technique which is
delivering exceptionally detailed quantitative imaging of biological and
nanotechnology specimens. However coarse parametrisation in propagation
distance, position errors and partial coherence frequently menaces the
experiment viability. In this work we formally introduced these actors, solving
the whole reconstruction as an optimisation problem. A modern Deep Learning
framework is used to correct autonomously the setup incoherences, thus
improving the quality of a ptychography reconstruction. Automatic procedures
are indeed crucial to reduce the time for a reliable analysis, which has a
significant impact on all the fields that use this kind of microscopy. We
implemented our algorithm in our software framework, SciComPty, releasing it as
open-source. We tested our system on both synthetic datasets and also on real
data acquired at the TwinMic beamline of the Elettra synchrotron facility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Blind Motion Deblurring Super-Resolution: When Dynamic Spatio-Temporal Learning Meets Static Image Understanding. (arXiv:2105.13077v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.13077">
<div class="article-summary-box-inner">
<span><p>Single-image super-resolution (SR) and multi-frame SR are two ways to super
resolve low-resolution images. Single-Image SR generally handles each image
independently, but ignores the temporal information implied in continuing
frames. Multi-frame SR is able to model the temporal dependency via capturing
motion information. However, it relies on neighbouring frames which are not
always available in the real world. Meanwhile, slight camera shake easily
causes heavy motion blur on long-distance-shot low-resolution images. To
address these problems, a Blind Motion Deblurring Super-Reslution Networks,
BMDSRNet, is proposed to learn dynamic spatio-temporal information from single
static motion-blurred images. Motion-blurred images are the accumulation over
time during the exposure of cameras, while the proposed BMDSRNet learns the
reverse process and uses three-streams to learn Bidirectional spatio-temporal
information based on well designed reconstruction loss functions to recover
clean high-resolution images. Extensive experiments demonstrate that the
proposed BMDSRNet outperforms recent state-of-the-art methods, and has the
ability to simultaneously deal with image deblurring and SR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ZeroWaste Dataset: Towards Deformable Object Segmentation in Extreme Clutter. (arXiv:2106.02740v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02740">
<div class="article-summary-box-inner">
<span><p>Less than 35% of recyclable waste is being actually recycled in the US, which
leads to increased soil and sea pollution and is one of the major concerns of
environmental researchers as well as the common public. At the heart of the
problem are the inefficiencies of the waste sorting process (separating paper,
plastic, metal, glass, etc.) due to the extremely complex and cluttered nature
of the waste stream. Automated waste detection has great potential to enable
more efficient, reliable, and safe waste sorting practices, but it requires
label-efficient detection of deformable objects in extremely cluttered scenes.
This challenging computer vision task currently lacks suitable datasets or
methods in the available literature. In this paper, we take a step towards
computer-aided waste detection and present the first in-the-wild
industrial-grade waste detection and segmentation dataset, ZeroWaste. This
dataset contains over 1800 fully segmented video frames collected from a real
waste sorting plant along with waste material labels for training and
evaluation of the segmentation methods, as well as over 6000 unlabeled frames
that can be further used for semi-supervised and self-supervised learning
techniques, as well as frames of the conveyor belt before and after the sorting
process, comprising a novel setup that can be used for weakly-supervised
segmentation. Our experimental results demonstrate that state-of-the-art
segmentation methods struggle to correctly detect and classify target objects
which suggests the challenging nature of our proposed real-world task of
fine-grained object detection in cluttered scenes. We believe that ZeroWaste
will catalyze research in object detection and semantic segmentation in extreme
clutter as well as applications in the recycling domain.
</p>
<p>Our project page can be found at <a href="http://ai.bu.edu/zerowaste/.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Not Escape From the Manifold: Discovering the Local Coordinates on the Latent Space of GANs. (arXiv:2106.06959v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06959">
<div class="article-summary-box-inner">
<span><p>The discovery of the disentanglement properties of the latent space in GANs
motivated a lot of research to find the semantically meaningful directions on
it. In this paper, we suggest that the disentanglement property is closely
related to the geometry of the latent space. In this regard, we propose an
unsupervised method for finding the semantic-factorizing directions on the
intermediate latent space of GANs based on the local geometry. Intuitively, our
proposed method, called Local Basis, finds the principal variation of the
latent space in the neighborhood of the base latent variable. Experimental
results show that the local principal variation corresponds to the semantic
factorization and traversing along it provides strong robustness to image
traversal. Moreover, we suggest an explanation for the limited success in
finding the global traversal directions in the latent space, especially W-space
of StyleGAN2. We show that W-space is warped globally by comparing the local
geometry, discovered from Local Basis, through the metric on Grassmannian
Manifold. The global warpage implies that the latent space is not well-aligned
globally and therefore the global traversal directions are bound to show
limited success on it.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Super Resolution be used to improve Human Pose Estimation in Low Resolution Scenarios?. (arXiv:2107.02108v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.02108">
<div class="article-summary-box-inner">
<span><p>The results obtained from state of the art human pose estimation (HPE) models
degrade rapidly when evaluating people of a low resolution, but can super
resolution (SR) be used to help mitigate this effect? By using various SR
approaches we enhanced two low resolution datasets and evaluated the change in
performance of both an object and keypoint detector as well as end-to-end HPE
results. We remark the following observations. First we find that for people
who were originally depicted at a low resolution (segmentation area in pixels),
their keypoint detection performance would improve once SR was applied. Second,
the keypoint detection performance gained is dependent on that persons pixel
count in the original image prior to any application of SR; keypoint detection
performance was improved when SR was applied to people with a small initial
segmentation area, but degrades as this becomes larger. To address this we
introduced a novel Mask-RCNN approach, utilising a segmentation area threshold
to decide when to use SR during the keypoint detection step. This approach
achieved the best results on our low resolution datasets for each HPE
performance metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VidLanKD: Improving Language Understanding via Video-Distilled Knowledge Transfer. (arXiv:2107.02681v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.02681">
<div class="article-summary-box-inner">
<span><p>Since visual perception can give rich information beyond text descriptions
for world understanding, there has been increasing interest in leveraging
visual grounding for language learning. Recently, vokenization (Tan and Bansal,
2020) has attracted attention by using the predictions of a text-to-image
retrieval model as labels for language model supervision. Despite its success,
the method suffers from approximation error of using finite image labels and
the lack of vocabulary diversity of a small image-text dataset. To overcome
these limitations, we present VidLanKD, a video-language knowledge distillation
method for improving language understanding. We train a multi-modal teacher
model on a video-text dataset, and then transfer its knowledge to a student
language model with a text dataset. To avoid approximation error, we propose to
use different knowledge distillation objectives. In addition, the use of a
large-scale video-text dataset helps learn diverse and richer vocabularies. In
our experiments, VidLanKD achieves consistent improvements over text-only
language models and vokenization models, on several downstream language
understanding tasks including GLUE, SQuAD, and SWAG. We also demonstrate the
improved world knowledge, physical reasoning, and temporal reasoning
capabilities of our model by evaluating on the GLUE-diagnostics, PIQA, and
TRACIE datasets. Lastly, we present comprehensive ablation studies as well as
visualizations of the learned text-to-video grounding results of our teacher
and student language models. Our code and models are available at:
https://github.com/zinengtang/VidLanKD
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EasyCom: An Augmented Reality Dataset to Support Algorithms for Easy Communication in Noisy Environments. (arXiv:2107.04174v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.04174">
<div class="article-summary-box-inner">
<span><p>Augmented Reality (AR) as a platform has the potential to facilitate the
reduction of the cocktail party effect. Future AR headsets could potentially
leverage information from an array of sensors spanning many different
modalities. Training and testing signal processing and machine learning
algorithms on tasks such as beam-forming and speech enhancement require high
quality representative data. To the best of the author's knowledge, as of
publication there are no available datasets that contain synchronized
egocentric multi-channel audio and video with dynamic movement and
conversations in a noisy environment. In this work, we describe, evaluate and
release a dataset that contains over 5 hours of multi-modal data useful for
training and testing algorithms for the application of improving conversations
for an AR glasses wearer. We provide speech intelligibility, quality and
signal-to-noise ratio improvement results for a baseline method and show
improvements across all tested metrics. The dataset we are releasing contains
AR glasses egocentric multi-channel microphone array audio, wide field-of-view
RGB video, speech source pose, headset microphone audio, annotated voice
activity, speech transcriptions, head bounding boxes, target of speech and
source identification labels. We have created and are releasing this dataset to
facilitate research in multi-modal AR solutions to the cocktail party problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Persistent Spatial Semantic Representation for High-level Natural Language Instruction Execution. (arXiv:2107.05612v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.05612">
<div class="article-summary-box-inner">
<span><p>Natural language provides an accessible and expressive interface to specify
long-term tasks for robotic agents. However, non-experts are likely to specify
such tasks with high-level instructions, which abstract over specific robot
actions through several layers of abstraction. We propose that key to bridging
this gap between language and robot actions over long execution horizons are
persistent representations. We propose a persistent spatial semantic
representation method, and show how it enables building an agent that performs
hierarchical reasoning to effectively execute long-term tasks. We evaluate our
approach on the ALFRED benchmark and achieve state-of-the-art results, despite
completely avoiding the commonly used step-by-step instructions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fairness Properties of Face Recognition and Obfuscation Systems. (arXiv:2108.02707v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02707">
<div class="article-summary-box-inner">
<span><p>The proliferation of automated face recognition in various commercial and
government sectors has caused significant privacy concerns for individuals. A
recent, popular approach to address these privacy concerns is to employ evasion
attacks against the metric embedding networks powering face recognition
systems. Face obfuscation systems generate imperceptible perturbations, when
added to an image, cause the face recognition system to misidentify the user.
The key to these approaches is the generation of perturbations using a
pre-trained metric embedding network followed by their application to an online
system, whose model might be proprietary. This dependence of face obfuscation
on metric embedding networks, which are known to be unfair in the context of
face recognition, surfaces the question of demographic fairness -- \textit{are
there demographic disparities in the performance of face obfuscation systems?}
To address this question, we perform an analytical and empirical exploration of
the performance of recent face obfuscation systems that rely on deep embedding
networks. We find that metric embedding networks are demographically aware;
they cluster faces in the embedding space based on their demographic
attributes. We observe that this effect carries through to face obfuscation
systems: faces belonging to minority groups incur reduced utility compared to
those from majority groups. For example, the disparity in average obfuscation
success rate on the online Face++ API can reach up to 20 percentage points. We
present an intuitive analytical model to provide insights into these phenomena.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Zero-Shot Learning for Semantic Segmentation of 3D Point Cloud. (arXiv:2108.06230v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06230">
<div class="article-summary-box-inner">
<span><p>While there has been a number of studies on Zero-Shot Learning (ZSL) for 2D
images, its application to 3D data is still recent and scarce, with just a few
methods limited to classification. We present the first generative approach for
both ZSL and Generalized ZSL (GZSL) on 3D data, that can handle both
classification and, for the first time, semantic segmentation. We show that it
reaches or outperforms the state of the art on ModelNet40 classification for
both inductive ZSL and inductive GZSL. For semantic segmentation, we created
three benchmarks for evaluating this new ZSL task, using S3DIS, ScanNet and
SemanticKITTI. Our experiments show that our method outperforms strong
baselines, which we additionally propose for this task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WikiChurches: A Fine-Grained Dataset of Architectural Styles with Real-World Challenges. (arXiv:2108.06959v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06959">
<div class="article-summary-box-inner">
<span><p>We introduce a novel dataset for architectural style classification,
consisting of 9,485 images of church buildings. Both images and style labels
were sourced from Wikipedia. The dataset can serve as a benchmark for various
research fields, as it combines numerous real-world challenges: fine-grained
distinctions between classes based on subtle visual features, a comparatively
small sample size, a highly imbalanced class distribution, a high variance of
viewpoints, and a hierarchical organization of labels, where only some images
are labeled at the most precise level. In addition, we provide 631 bounding box
annotations of characteristic visual features for 139 churches from four major
categories. These annotations can, for example, be useful for research on
fine-grained classification, where additional expert knowledge about
distinctive object parts is often available. Images and annotations are
available at: https://doi.org/10.5281/zenodo.5166987
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sk-Unet Model with Fourier Domain for Mitosis Detection. (arXiv:2109.00957v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00957">
<div class="article-summary-box-inner">
<span><p>Mitotic count is the most important morphological feature of breast cancer
grading. Many deep learning-based methods have been proposed but suffer from
domain shift. In this work, we construct a Fourier-based segmentation model for
mitosis detection to address the problem. Swapping the low-frequency spectrum
of source and target images is shown effective to alleviate the discrepancy
between different scanners. Our Fourier-based segmentation method can achieve
F1 with 0.7456 on the preliminary test set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Power of Points for Modeling Humans in Clothing. (arXiv:2109.01137v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01137">
<div class="article-summary-box-inner">
<span><p>Currently it requires an artist to create 3D human avatars with realistic
clothing that can move naturally. Despite progress on 3D scanning and modeling
of human bodies, there is still no technology that can easily turn a static
scan into an animatable avatar. Automating the creation of such avatars would
enable many applications in games, social networking, animation, and AR/VR to
name a few. The key problem is one of representation. Standard 3D meshes are
widely used in modeling the minimally-clothed body but do not readily capture
the complex topology of clothing. Recent interest has shifted to implicit
surface models for this task but they are computationally heavy and lack
compatibility with existing 3D tools. What is needed is a 3D representation
that can capture varied topology at high resolution and that can be learned
from data. We argue that this representation has been with us all along -- the
point cloud. Point clouds have properties of both implicit and explicit
representations that we exploit to model 3D garment geometry on a human body.
We train a neural network with a novel local clothing geometric feature to
represent the shape of different outfits. The network is trained from 3D point
clouds of many types of clothing, on many bodies, in many poses, and learns to
model pose-dependent clothing deformations. The geometry feature can be
optimized to fit a previously unseen scan of a person in clothing, enabling the
scan to be reposed realistically. Our model demonstrates superior quantitative
and qualitative results in both multi-outfit modeling and unseen outfit
animation. The code is available for research purposes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A realistic approach to generate masked faces applied on two novel masked face recognition data sets. (arXiv:2109.01745v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01745">
<div class="article-summary-box-inner">
<span><p>The COVID-19 pandemic raises the problem of adapting face recognition systems
to the new reality, where people may wear surgical masks to cover their noses
and mouths. Traditional data sets (e.g., CelebA, CASIA-WebFace) used for
training these systems were released before the pandemic, so they now seem
unsuited due to the lack of examples of people wearing masks. We propose a
method for enhancing data sets containing faces without masks by creating
synthetic masks and overlaying them on faces in the original images. Our method
relies on SparkAR Studio, a developer program made by Facebook that is used to
create Instagram face filters. In our approach, we use 9 masks of different
colors, shapes and fabrics. We employ our method to generate a number of
445,446 (90%) samples of masks for the CASIA-WebFace data set and 196,254
(96.8%) masks for the CelebA data set, releasing the mask images at
https://github.com/securifai/masked_faces. We show that our method produces
significantly more realistic training examples of masks overlaid on faces by
asking volunteers to qualitatively compare it to other methods or data sets
designed for the same task. We also demonstrate the usefulness of our method by
evaluating state-of-the-art face recognition systems (FaceNet, VGG-face,
ArcFace) trained on our enhanced data sets and showing that they outperform
equivalent systems trained on original data sets (containing faces without
masks) or competing data sets (containing masks generated by related methods),
when the test benchmarks contain masked faces.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LAViTeR: Learning Aligned Visual and Textual Representations Assisted by Image and Caption Generation. (arXiv:2109.04993v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04993">
<div class="article-summary-box-inner">
<span><p>Pre-training visual and textual representations from large-scale image-text
pairs is becoming a standard approach for many downstream vision-language
tasks. The transformer-based models learn inter and intra-modal attention
through a list of self-supervised learning tasks. This paper proposes LAViTeR,
a novel architecture for visual and textual representation learning. The main
module, Visual Textual Alignment (VTA) will be assisted by two auxiliary tasks,
GAN-based image synthesis and Image Captioning. We also propose a new
evaluation metric measuring the similarity between the learnt visual and
textual embedding. The experimental results on two public datasets, CUB and
MS-COCO, demonstrate superior visual and textual representation alignment in
the joint feature embedding space
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Context-Aware Network for Abdominal Multi-organ Segmentation. (arXiv:2109.10601v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10601">
<div class="article-summary-box-inner">
<span><p>The contextual information, presented in abdominal CT scan, is relative
consistent. In order to make full use of the overall 3D context, we develop a
whole-volume-based coarse-to-fine framework for efficient and effective
abdominal multi-organ segmentation. We propose a new efficientSegNet network,
which is composed of encoder,decoder and context block. For the decoder
module,anisotropic convolution with a k*k*1 intra-slice convolution and a 1*1*k
inter-slice convolution, is designed to reduce the computation burden. For the
context block, we propose strip pooling module to capture anisotropic and
long-range contextual information, which exists in abdominal scene.
Quantitative evaluation on the FLARE2021 validation cases, this method achieves
the average dice similarity coefficient (DSC) of 0.895 and average normalized
surface distance (NSD) of 0.775. This method won the 1st place on the
2021-MICCAI-FLARE challenge. Codes and models are available at
https://github.com/Shanghai-Aitrox-Technology/EfficientSegmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Training of 3D Seismic Image Fault Segmentation Network under Sparse Labels by Weakening Anomaly Annotation. (arXiv:2110.05319v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05319">
<div class="article-summary-box-inner">
<span><p>Seismic data fault detection has recently been regarded as a 3D image
segmentation task. The nature of fault structures in seismic image makes it
difficult to manually label faults. Manual labeling often has many false
negative labels (abnormal annotations), which will seriously jeopardize the
training process. In this work, we find that region-based loss significantly
outperforms distribution-based loss when dealing with false negative labels,
therefore we proposed Mask Dice loss (MD loss), which is the first reported
region-based loss function for training 3D image segmentation networks using
sparse 2D slice labels. In addition, fault is an edge feature, and the current
network widely used for fault segmentation downsamples the features multiple
times, which is not conducive to edge representation and thus requires many
parameters and computational effort to preserve the features. We proposed
Fault-Net, which uses a high-resolution and shallow structure to propagate
multi-scale features in parallel, fully preserving edge features. Meanwhile, in
order to efficiently fuse multiscale features, we decouple the convolution
process into feature selection and channel fusion, and proposed a lightweight
feature fusion block, Multi-Scale Compression Fusion (MCF). Because the
Fault-Net always keeps the edge features during propagation, only few
parameters and computation are required. Experimental results show that MD loss
can clearly weaken the effect of false negative labels. The Fault-Net parameter
is only 0.42MB, support up to 528^3(1.5x10^8, Float32) size cuboid inference on
16GB video ram, its inference speed on CPU and GPU is significantly faster than
other networks. It works well on most of the open data seismic images, and the
result of our approach is state-ofthe-art in FORCE fault identification
competition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling Laws for the Few-Shot Adaptation of Pre-trained Image Classifiers. (arXiv:2110.06990v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06990">
<div class="article-summary-box-inner">
<span><p>Empirical science of neural scaling laws is a rapidly growing area of
significant importance to the future of machine learning, particularly in the
light of recent breakthroughs achieved by large-scale pre-trained models such
as GPT-3, CLIP and DALL-e. Accurately predicting the neural network performance
with increasing resources such as data, compute and model size provides a more
comprehensive evaluation of different approaches across multiple scales, as
opposed to traditional point-wise comparisons of fixed-size models on
fixed-size benchmarks, and, most importantly, allows for focus on the
best-scaling, and thus most promising in the future, approaches. In this work,
we consider a challenging problem of few-shot learning in image classification,
especially when the target data distribution in the few-shot phase is different
from the source, training, data distribution, in a sense that it includes new
image classes not encountered during training. Our current main goal is to
investigate how the amount of pre-training data affects the few-shot
generalization performance of standard image classifiers. Our key observations
are that (1) such performance improvements are well-approximated by power laws
(linear log-log plots) as the training set size increases, (2) this applies to
both cases of target data coming from either the same or from a different
domain (i.e., new classes) as the training data, and (3) few-shot performance
on new classes converges at a faster rate than the standard classification
performance on previously seen classes. Our findings shed new light on the
relationship between scale and generalization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SAGAN: Adversarial Spatial-asymmetric Attention for Noisy Nona-Bayer Reconstruction. (arXiv:2110.08619v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08619">
<div class="article-summary-box-inner">
<span><p>Nona-Bayer colour filter array (CFA) pattern is considered one of the most
viable alternatives to traditional Bayer patterns. Despite the substantial
advantages, such non-Bayer CFA patterns are susceptible to produce visual
artefacts while reconstructing RGB images from noisy sensor data. This study
addresses the challenges of learning RGB image reconstruction from noisy
Nona-Bayer CFA comprehensively. We propose a novel spatial-asymmetric attention
module to jointly learn bi-direction transformation and large-kernel global
attention to reduce the visual artefacts. We combine our proposed module with
adversarial learning to produce plausible images from Nona-Bayer CFA. The
feasibility of the proposed method has been verified and compared with the
state-of-the-art image reconstruction method. The experiments reveal that the
proposed method can reconstruct RGB images from noisy Nona-Bayer CFA without
producing any visually disturbing artefacts. Also, it can outperform the
state-of-the-art image reconstruction method in both qualitative and
quantitative comparison. Code available:
https://github.com/sharif-apu/SAGAN_BMVC21.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LoveDA: A Remote Sensing Land-Cover Dataset for Domain Adaptive Semantic Segmentation. (arXiv:2110.08733v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08733">
<div class="article-summary-box-inner">
<span><p>Deep learning approaches have shown promising results in remote sensing high
spatial resolution (HSR) land-cover mapping. However, urban and rural scenes
can show completely different geographical landscapes, and the inadequate
generalizability of these algorithms hinders city-level or national-level
mapping. Most of the existing HSR land-cover datasets mainly promote the
research of learning semantic representation, thereby ignoring the model
transferability. In this paper, we introduce the Land-cOVEr Domain Adaptive
semantic segmentation (LoveDA) dataset to advance semantic and transferable
learning. The LoveDA dataset contains 5927 HSR images with 166768 annotated
objects from three different cities. Compared to the existing datasets, the
LoveDA dataset encompasses two domains (urban and rural), which brings
considerable challenges due to the: 1) multi-scale objects; 2) complex
background samples; and 3) inconsistent class distributions. The LoveDA dataset
is suitable for both land-cover semantic segmentation and unsupervised domain
adaptation (UDA) tasks. Accordingly, we benchmarked the LoveDA dataset on
eleven semantic segmentation methods and eight UDA methods. Some exploratory
studies including multi-scale architectures and strategies, additional
background supervision, and pseudo-label analysis were also carried out to
address these challenges. The code are available at
https://github.com/Junjue-Wang/LoveDA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Alleviating Noisy-label Effects in Image Classification via Probability Transition Matrix. (arXiv:2110.08866v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08866">
<div class="article-summary-box-inner">
<span><p>Deep-learning-based image classification frameworks often suffer from the
noisy label problem caused by the inter-observer variation. Recent studies
employed learning-to-learn paradigms (e.g., Co-teaching and JoCoR) to filter
the samples with noisy labels from the training set. However, most of them use
a simple cross-entropy loss as the criterion for noisy label identification.
The hard samples, which are beneficial for classifier learning, are often
mistakenly treated as noises in such a setting since both the hard samples and
ones with noisy labels lead to a relatively larger loss value than the easy
cases. In this paper, we propose a plugin module, namely noise ignoring block
(NIB), consisting of a probability transition matrix and an inter-class
correlation (IC) loss, to separate the hard samples from the mislabeled ones,
and further boost the accuracy of image classification network trained with
noisy labels. Concretely, our IC loss is calculated as Kullback-Leibler
divergence between the network prediction and the accumulative soft label
generated by the probability transition matrix. Such that, with the lower value
of IC loss, the hard cases can be easily distinguished from mislabeled cases.
Extensive experiments are conducted on natural and medical image datasets
(CIFAR-10 and ISIC 2019). The experimental results show that our NIB module
consistently improves the performances of the state-of-the-art robust training
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Salt and pepper noise removal method based on stationary Framelet transform with non-convex sparsity regularization. (arXiv:2110.09113v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09113">
<div class="article-summary-box-inner">
<span><p>Salt and pepper noise removal is a common inverse problem in image
processing, and it aims to restore image information with high quality.
Traditional salt and pepper denoising methods have two limitations. First,
noise characteristics are often not described accurately. For example, the
noise location information is often ignored and the sparsity of the salt and
pepper noise is often described by L1 norm, which cannot illustrate the sparse
variables clearly. Second, conventional methods separate the contaminated image
into a recovered image and a noise part, thus resulting in recovering an image
with unsatisfied smooth parts and detail parts. In this study, we introduce a
noise detection strategy to determine the position of the noise, and a
non-convex sparsity regularization depicted by Lp quasi-norm is employed to
describe the sparsity of the noise, thereby addressing the first limitation.
The morphological component analysis framework with stationary Framelet
transform is adopted to decompose the processed image into cartoon, texture,
and noise parts to resolve the second limitation. In this framework, the
stationary Framelet regularizations with different parameters control the
restoration of the cartoon and texture parts. In this way, the two parts are
recovered separately to avoid mutual interference. Then, the alternating
direction method of multipliers (ADMM) is employed to solve the proposed model.
Finally, experiments are conducted to verify the proposed method and compare it
with some current state-of-the-art denoising methods. The experimental results
show that the proposed method can remove salt and pepper noise while preserving
the details of the processed image.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning multiplane images from single views with self-supervision. (arXiv:2110.09380v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09380">
<div class="article-summary-box-inner">
<span><p>Generating static novel views from an already captured image is a hard task
in computer vision and graphics, in particular when the single input image has
dynamic parts such as persons or moving objects. In this paper, we tackle this
problem by proposing a new framework, called CycleMPI, that is capable of
learning a multiplane image representation from single images through a cyclic
training strategy for self-supervision. Our framework does not require stereo
data for training, therefore it can be trained with massive visual data from
the Internet, resulting in a better generalization capability even for very
challenging cases. Although our method does not require stereo data for
supervision, it reaches results on stereo datasets comparable to the state of
the art in a zero-shot scenario. We evaluated our method on RealEstate10K and
Mannequin Challenge datasets for view synthesis and presented qualitative
results on Places II dataset.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-10-20 23:02:32.498203518 UTC">2021-10-20 23:02:32 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.4</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>