<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-07-19T01:30:00Z">07-19</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing Semantic Grounding in Language Models of Code with Representational Similarity Analysis. (arXiv:2207.07706v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07706">
<div class="article-summary-box-inner">
<span><p>Representational Similarity Analysis is a method from cognitive neuroscience,
which helps in comparing representations from two different sources of data. In
this paper, we propose using Representational Similarity Analysis to probe the
semantic grounding in language models of code. We probe representations from
the CodeBERT model for semantic grounding by using the data from the IBM
CodeNet dataset. Through our experiments, we show that current pre-training
methods do not induce semantic grounding in language models of code, and
instead focus on optimizing form-based patterns. We also show that even a
little amount of fine-tuning on semantically relevant tasks increases the
semantic grounding in CodeBERT significantly. Our ablations with the input
modality to the CodeBERT model show that using bimodal inputs (code and natural
language) over unimodal inputs (only code) gives better semantic grounding and
sample efficiency during semantic fine-tuning. Finally, our experiments with
semantic perturbations in code reveal that CodeBERT is able to robustly
distinguish between semantically correct and incorrect code.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sotto Voce: Federated Speech Recognition with Differential Privacy Guarantees. (arXiv:2207.07816v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07816">
<div class="article-summary-box-inner">
<span><p>Speech data is expensive to collect, and incredibly sensitive to its sources.
It is often the case that organizations independently collect small datasets
for their own use, but often these are not performant for the demands of
machine learning. Organizations could pool these datasets together and jointly
build a strong ASR system; sharing data in the clear, however, comes with
tremendous risk, in terms of intellectual property loss as well as loss of
privacy of the individuals who exist in the dataset. In this paper, we offer a
potential solution for learning an ML model across multiple organizations where
we can provide mathematical guarantees limiting privacy loss. We use a
Federated Learning approach built on a strong foundation of Differential
Privacy techniques. We apply these to a senone classification prototype and
demonstrate that the model improves with the addition of private data while
still respecting privacy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Dialog Systems with Dual Knowledge-enhanced Generative Pretrained Language Model. (arXiv:2207.07934v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07934">
<div class="article-summary-box-inner">
<span><p>Text response generation for multimodal task-oriented dialog systems, which
aims to generate the proper text response given the multimodal context, is an
essential yet challenging task. Although existing efforts have achieved
compelling success, they still suffer from two pivotal limitations: 1) overlook
the benefit of generative pre-training, and 2) ignore the textual context
related knowledge. To address these limitations, we propose a novel dual
knowledge-enhanced generative pretrained language model for multimodal
task-oriented dialog systems (DKMD), consisting of three key components: dual
knowledge selection, dual knowledge-enhanced context learning, and
knowledge-enhanced response generation. To be specific, the dual knowledge
selection component aims to select the related knowledge according to both
textual and visual modalities of the given context. Thereafter, the dual
knowledge-enhanced context learning component targets seamlessly integrating
the selected knowledge into the multimodal context learning from both global
and local perspectives, where the cross-modal semantic relation is also
explored. Moreover, the knowledge-enhanced response generation component
comprises a revised BART decoder, where an additional dot-product
knowledge-decoder attention sub-layer is introduced for explicitly utilizing
the knowledge to advance the text response generation. Extensive experiments on
a public dataset verify the superiority of the proposed DKMD over
state-of-the-art competitors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta-Referential Games to Learn Compositional Learning Behaviours. (arXiv:2207.08012v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08012">
<div class="article-summary-box-inner">
<span><p>Human beings use compositionality to generalise from past experiences to
actual or fictive, novel experiences. To do so, we separate our experiences
into fundamental atomic components. These atomic components can then be
recombined in novel ways to support our ability to imagine and engage with
novel experiences. We frame this as the ability to learn to generalise
compositionally. And, we will refer to behaviours making use of this ability as
compositional learning behaviours (CLBs).
</p>
<p>A central problem to learning CLBs is the resolution of a binding problem
(BP) (by learning to, firstly, segregate the supportive stimulus components
from the observation of multiple stimuli, and then, combine them in a single
episodic experience). While it is another feat of intelligence that human
beings perform with ease, it is not the case for state-of-the-art artificial
agents.
</p>
<p>Thus, in order to build artificial agents able to collaborate with human
beings, we propose to develop a novel benchmark to investigate agents'
abilities to exhibit CLBs by solving a domain-agnostic version of the BP. We
take inspiration from the language emergence and grounding framework of
referential games and propose a meta-learning extension of referential games,
entitled Meta-Referential Games, and use this framework to build our benchmark,
that we name Symbolic Behaviour Benchmark (S2B).
</p>
<p>While it has the potential to test for more symbolic behaviours, rather than
solely CLBs, in the present paper, though, we solely focus on the single-agent
language grounding task that tests for CLBs. We provide baseline results for
it, using state-of-the-art RL agents, and show that our proposed benchmark is a
compelling challenge that we hope will spur the research community towards
developing more capable artificial agents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Explainability in NLP: Analyzing and Calculating Word Saliency through Word Properties. (arXiv:2207.08083v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08083">
<div class="article-summary-box-inner">
<span><p>The wide use of black-box models in natural language processing brings great
challenges to the understanding of the decision basis, the trustworthiness of
the prediction results, and the improvement of the model performance. The words
in text samples have properties that reflect their semantics and contextual
information, such as the part of speech, the position, etc. These properties
may have certain relationships with the word saliency, which is of great help
for studying the explainability of the model predictions. In this paper, we
explore the relationships between the word saliency and the word properties.
According to the analysis results, we further establish a mapping model,
Seq2Saliency, from the words in a text sample and their properties to the
saliency values based on the idea of sequence tagging. In addition, we
establish a new dataset called PrSalM, which contains each word in the text
samples, the word properties, and the word saliency values. The experimental
evaluations are conducted to analyze the saliency of words with different
properties. The effectiveness of the Seq2Saliency model is verified.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Context Pattern Generation for Entity Set Expansion. (arXiv:2207.08087v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08087">
<div class="article-summary-box-inner">
<span><p>Entity Set Expansion (ESE) is a valuable task that aims to find entities of
the target semantic class described by given seed entities. Various NLP and IR
downstream applications have benefited from ESE due to its ability to discover
knowledge. Although existing bootstrapping methods have achieved great
progress, most of them still rely on manually pre-defined context patterns. A
non-negligible shortcoming of the pre-defined context patterns is that they
cannot be flexibly generalized to all kinds of semantic classes, and we call
this phenomenon as "semantic sensitivity". To address this problem, we devise a
context pattern generation module that utilizes autoregressive language models
(e.g., GPT-2) to automatically generate high-quality context patterns for
entities. In addition, we propose the GAPA, a novel ESE framework that
leverages the aforementioned GenerAted PAtterns to expand target entities.
Extensive experiments and detailed analyses on three widely used datasets
demonstrate the effectiveness of our method. All the codes of our experiments
will be available for reproducibility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Aspect-specific Context Modeling for Aspect-based Sentiment Analysis. (arXiv:2207.08099v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08099">
<div class="article-summary-box-inner">
<span><p>Aspect-based sentiment analysis (ABSA) aims at predicting sentiment polarity
(SC) or extracting opinion span (OE) expressed towards a given aspect. Previous
work in ABSA mostly relies on rather complicated aspect-specific feature
induction. Recently, pretrained language models (PLMs), e.g., BERT, have been
used as context modeling layers to simplify the feature induction structures
and achieve state-of-the-art performance. However, such PLM-based context
modeling can be not that aspect-specific. Therefore, a key question is left
under-explored: how the aspect-specific context can be better modeled through
PLMs? To answer the question, we attempt to enhance aspect-specific context
modeling with PLM in a non-intrusive manner. We propose three aspect-specific
input transformations, namely aspect companion, aspect prompt, and aspect
marker. Informed by these transformations, non-intrusive aspect-specific PLMs
can be achieved to promote the PLM to pay more attention to the aspect-specific
context in a sentence. Additionally, we craft an adversarial benchmark for ABSA
(advABSA) to see how aspect-specific modeling can impact model robustness.
Extensive experimental results on standard and adversarial benchmarks for SC
and OE demonstrate the effectiveness and robustness of the proposed method,
yielding new state-of-the-art performance on OE and competitive performance on
SC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multibias-mitigated and Sentiment Knowledge Enriched Transformer for Debiasing in Multimodal Conversational Emotion Recognition. (arXiv:2207.08104v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08104">
<div class="article-summary-box-inner">
<span><p>Multimodal emotion recognition in conversations (mERC) is an active research
topic in natural language processing (NLP), which aims to predict human's
emotional states in communications of multiple modalities, e,g., natural
language and facial gestures. Innumerable implicit prejudices and
preconceptions fill human language and conversations, leading to the question
of whether the current data-driven mERC approaches produce a biased error. For
example, such approaches may offer higher emotional scores on the utterances by
females than males. In addition, the existing debias models mainly focus on
gender or race, where multibias mitigation is still an unexplored task in mERC.
In this work, we take the first step to solve these issues by proposing a
series of approaches to mitigate five typical kinds of bias in textual
utterances (i.e., gender, age, race, religion and LGBTQ+) and visual
representations (i.e, gender and age), followed by a Multibias-Mitigated and
sentiment Knowledge Enriched bi-modal Transformer (MMKET). Comprehensive
experimental results show the effectiveness of the proposed model and prove
that the debias operation has a great impact on the classification performance
for mERC. We hope our study will benefit the development of bias mitigation in
mERC and related emotion studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">United States Politicians' Tone Became More Negative with 2016 Primary Campaigns. (arXiv:2207.08112v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08112">
<div class="article-summary-box-inner">
<span><p>There is a widespread belief that the tone of US political language has
become more negative recently, in particular when Donald Trump entered
politics. At the same time, there is disagreement as to whether Trump changed
or merely continued previous trends. To date, data-driven evidence regarding
these questions is scarce, partly due to the difficulty of obtaining a
comprehensive, longitudinal record of politicians' utterances. Here we apply
psycholinguistic tools to a novel, comprehensive corpus of 24 million quotes
from online news attributed to 18,627 US politicians in order to analyze how
the tone of US politicians' language evolved between 2008 and 2020. We show
that, whereas the frequency of negative emotion words had decreased
continuously during Obama's tenure, it suddenly and lastingly increased with
the 2016 primary campaigns, by 1.6 pre-campaign standard deviations, or 8% of
the pre-campaign mean, in a pattern that emerges across parties. The effect
size drops by 40% when omitting Trump's quotes, and by 50% when averaging over
speakers rather than quotes, implying that prominent speakers, and Trump in
particular, have disproportionately, though not exclusively, contributed to the
rise in negative language. This work provides the first large-scale data-driven
evidence of a drastic shift toward a more negative political tone following
Trump's campaign start as a catalyst, with important implications for the
debate about the state of US politics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting Span-based Joint Entity and Relation Extraction via Squence Tagging Mechanism. (arXiv:2105.10080v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.10080">
<div class="article-summary-box-inner">
<span><p>Span-based joint extraction simultaneously conducts named entity recognition
(NER) and relation extraction (RE) in text span form. Recent studies have shown
that token labels can convey crucial task-specific information and enrich token
semantics. However, as far as we know, due to completely abstain from sequence
tagging mechanism, all prior span-based work fails to use token label
in-formation. To solve this problem, we pro-pose Sequence Tagging enhanced
Span-based Network (STSN), a span-based joint extrac-tion network that is
enhanced by token BIO label information derived from sequence tag-ging based
NER. By stacking multiple atten-tion layers in depth, we design a deep neu-ral
architecture to build STSN, and each atten-tion layer consists of three basic
attention units. The deep neural architecture first learns seman-tic
representations for token labels and span-based joint extraction, and then
constructs in-formation interactions between them, which also realizes
bidirectional information interac-tions between span-based NER and RE.
Fur-thermore, we extend the BIO tagging scheme to make STSN can extract
overlapping en-tity. Experiments on three benchmark datasets show that our
model consistently outperforms previous optimal models by a large margin,
creating new state-of-the-art results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Selective Differential Privacy for Language Modeling. (arXiv:2108.12944v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12944">
<div class="article-summary-box-inner">
<span><p>With the increasing applications of language models, it has become crucial to
protect these models from leaking private information. Previous work has
attempted to tackle this challenge by training RNN-based language models with
differential privacy guarantees. However, applying classical differential
privacy to language models leads to poor model performance as the underlying
privacy notion is over-pessimistic and provides undifferentiated protection for
all tokens in the data. Given that the private information in natural language
is sparse (for example, the bulk of an email might not carry personally
identifiable information), we propose a new privacy notion, selective
differential privacy, to provide rigorous privacy guarantees on the sensitive
portion of the data to improve model utility. To realize such a new notion, we
develop a corresponding privacy mechanism, Selective-DPSGD, for RNN-based
language models. Besides language modeling, we also apply the method to a more
concrete application--dialog systems. Experiments on both language modeling and
dialog system building show that the proposed privacy-preserving mechanism
achieves better utilities while remaining safe under various privacy attacks
compared to the baselines. The data and code are released at
https://github.com/wyshi/lm_privacy to facilitate future research .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CPT: A Pre-Trained Unbalanced Transformer for Both Chinese Language Understanding and Generation. (arXiv:2109.05729v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05729">
<div class="article-summary-box-inner">
<span><p>In this paper, we take the advantage of previous pre-trained models (PTMs)
and propose a novel Chinese Pre-trained Unbalanced Transformer (CPT). Different
from previous Chinese PTMs, CPT is designed to utilize the shared knowledge
between natural language understanding (NLU) and natural language generation
(NLG) to boost the performance. CPT consists of three parts: a shared encoder,
an understanding decoder, and a generation decoder. Two specific decoders with
a shared encoder are pre-trained with masked language modeling (MLM) and
denoising auto-encoding (DAE) tasks, respectively. With the partially shared
architecture and multi-task pre-training, CPT can (1) learn specific knowledge
of both NLU or NLG tasks with two decoders and (2) be fine-tuned flexibly that
fully exploits the potential of the model. Moreover, the unbalanced Transformer
saves the computational and storage cost, which makes CPT competitive and
greatly accelerates the inference of text generation. Experimental results on a
wide range of Chinese NLU and NLG tasks show the effectiveness of CPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models Can Be Strong Differentially Private Learners. (arXiv:2110.05679v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05679">
<div class="article-summary-box-inner">
<span><p>Differentially Private (DP) learning has seen limited success for building
large deep learning models of text, and attempts at straightforwardly applying
Differentially Private Stochastic Gradient Descent (DP-SGD) to NLP tasks have
resulted in large performance drops and high computational overhead. We show
that this performance drop can be mitigated with (1) the use of large
pretrained models; (2) hyperparameters that suit DP optimization; and (3)
fine-tuning objectives aligned with the pretraining procedure. With these
factors set right, we obtain private NLP models that outperform
state-of-the-art private training approaches and strong non-private baselines
-- by directly fine-tuning pretrained models with DP optimization on
moderately-sized corpora. To address the computational challenge of running
DP-SGD with large Transformers, we propose a memory saving technique that
allows clipping in DP-SGD to run without instantiating per-example gradients
for any layer in the model. The technique enables privately training
Transformers with almost the same memory cost as non-private training at a
modest run-time overhead. Contrary to conventional wisdom that DP optimization
fails at learning high-dimensional models (due to noise that scales with
dimension) empirical results reveal that private learning with pretrained
models tends to not suffer from dimension-dependent performance degradation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Amortized Noisy Channel Neural Machine Translation. (arXiv:2112.08670v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08670">
<div class="article-summary-box-inner">
<span><p>Noisy channel models have been especially effective in neural machine
translation (NMT). However, recent approaches like "beam search and rerank"
(BSR) incur significant computation overhead during inference, making
real-world application infeasible. We aim to study if it is possible to build
an amortized noisy channel NMT model such that when we do greedy decoding
during inference, the translation accuracy matches that of BSR in terms of
reward (based on the source-to-target log probability and the target-to-source
log probability) and quality (based on BLEU and BLEURT). We attempt three
approaches to train the new model: knowledge distillation, one-step-deviation
imitation learning, and Q learning. The first approach obtains the noisy
channel signal from a pseudo-corpus, and the latter two approaches aim to
optimize toward a noisy-channel MT reward directly. For all three approaches,
the generated translations fail to achieve rewards comparable to BSR, but the
translation quality approximated by BLEU and BLEURT is similar to the quality
of BSR-produced translations. Additionally, all three approaches speed up
inference by 1-2 orders of magnitude.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emotion Intensity and its Control for Emotional Voice Conversion. (arXiv:2201.03967v3 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03967">
<div class="article-summary-box-inner">
<span><p>Emotional voice conversion (EVC) seeks to convert the emotional state of an
utterance while preserving the linguistic content and speaker identity. In EVC,
emotions are usually treated as discrete categories overlooking the fact that
speech also conveys emotions with various intensity levels that the listener
can perceive. In this paper, we aim to explicitly characterize and control the
intensity of emotion. We propose to disentangle the speaker style from
linguistic content and encode the speaker style into a style embedding in a
continuous space that forms the prototype of emotion embedding. We further
learn the actual emotion encoder from an emotion-labelled database and study
the use of relative attributes to represent fine-grained emotion intensity. To
ensure emotional intelligibility, we incorporate emotion classification loss
and emotion embedding similarity loss into the training of the EVC network. As
desired, the proposed network controls the fine-grained emotion intensity in
the output speech. Through both objective and subjective evaluations, we
validate the effectiveness of the proposed network for emotional expressiveness
and emotion intensity control.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages. (arXiv:2201.11732v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11732">
<div class="article-summary-box-inner">
<span><p>Reliable evaluation benchmarks designed for replicability and
comprehensiveness have driven progress in machine learning. Due to the lack of
a multilingual benchmark, however, vision-and-language research has mostly
focused on English language tasks. To fill this gap, we introduce the
Image-Grounded Language Understanding Evaluation benchmark. IGLUE brings
together - by both aggregating pre-existing datasets and creating new ones -
visual question answering, cross-modal retrieval, grounded reasoning, and
grounded entailment tasks across 20 diverse languages. Our benchmark enables
the evaluation of multilingual multimodal models for transfer learning, not
only in a zero-shot setting, but also in newly defined few-shot learning
setups. Based on the evaluation of the available state-of-the-art models, we
find that translate-test transfer is superior to zero-shot transfer and that
few-shot learning is hard to harness for many tasks. Moreover, downstream
performance is partially explained by the amount of available unlabelled
textual data for pretraining, and only weakly by the typological distance of
target-source languages. We hope to encourage future research efforts in this
area by releasing the benchmark to the community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Vision-Language Pre-Trained Models. (arXiv:2202.10936v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.10936">
<div class="article-summary-box-inner">
<span><p>As transformer evolves, pre-trained models have advanced at a breakneck pace
in recent years. They have dominated the mainstream techniques in natural
language processing (NLP) and computer vision (CV). How to adapt pre-training
to the field of Vision-and-Language (V-L) learning and improve downstream task
performance becomes a focus of multimodal learning. In this paper, we review
the recent progress in Vision-Language Pre-Trained Models (VL-PTMs). As the
core content, we first briefly introduce several ways to encode raw images and
texts to single-modal embeddings before pre-training. Then, we dive into the
mainstream architectures of VL-PTMs in modeling the interaction between text
and image representations. We further present widely-used pre-training tasks,
and then we introduce some common downstream tasks. We finally conclude this
paper and present some promising research directions. Our survey aims to
provide researchers with synthesis and pointer to related research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PALI-NLP at SemEval-2022 Task 4: Discriminative Fine-tuning of Transformers for Patronizing and Condescending Language Detection. (arXiv:2203.04616v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04616">
<div class="article-summary-box-inner">
<span><p>Patronizing and condescending language (PCL) has a large harmful impact and
is difficult to detect, both for human judges and existing NLP systems. At
SemEval-2022 Task 4, we propose a novel Transformer-based model and its
ensembles to accurately understand such language context for PCL detection. To
facilitate comprehension of the subtle and subjective nature of PCL, two
fine-tuning strategies are applied to capture discriminative features from
diverse linguistic behaviour and categorical distribution. The system achieves
remarkable results on the official ranking, including 1st in Subtask 1 and 5th
in Subtask 2. Extensive experiments on the task demonstrate the effectiveness
of our system and its strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compression of Generative Pre-trained Language Models via Quantization. (arXiv:2203.10705v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10705">
<div class="article-summary-box-inner">
<span><p>The increasing size of generative Pre-trained Language Models (PLMs) has
greatly increased the demand for model compression. Despite various methods to
compress BERT or its variants, there are few attempts to compress generative
PLMs, and the underlying difficulty remains unclear. In this paper, we compress
generative PLMs by quantization. We find that previous quantization methods
fail on generative tasks due to the \textit{homogeneous word embeddings} caused
by reduced capacity, and \textit{varied distribution of weights}.
Correspondingly, we propose a token-level contrastive distillation to learn
distinguishable word embeddings, and a module-wise dynamic scaling to make
quantizers adaptive to different modules. Empirical results on various tasks
show that our proposed method outperforms the state-of-the-art compression
methods on generative PLMs by a clear margin. With comparable performance with
the full-precision models, we achieve 14.4x and 13.4x compression rates on
GPT-2 and BART, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IAM: A Comprehensive and Large-Scale Dataset for Integrated Argument Mining Tasks. (arXiv:2203.12257v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12257">
<div class="article-summary-box-inner">
<span><p>Traditionally, a debate usually requires a manual preparation process,
including reading plenty of articles, selecting the claims, identifying the
stances of the claims, seeking the evidence for the claims, etc. As the AI
debate attracts more attention these years, it is worth exploring the methods
to automate the tedious process involved in the debating system. In this work,
we introduce a comprehensive and large dataset named IAM, which can be applied
to a series of argument mining tasks, including claim extraction, stance
classification, evidence extraction, etc. Our dataset is collected from over 1k
articles related to 123 topics. Near 70k sentences in the dataset are fully
annotated based on their argument properties (e.g., claims, stances, evidence,
etc.). We further propose two new integrated argument mining tasks associated
with the debate preparation process: (1) claim extraction with stance
classification (CESC) and (2) claim-evidence pair extraction (CEPE). We adopt a
pipeline approach and an end-to-end method for each integrated task separately.
Promising experimental results are reported to show the values and challenges
of our proposed tasks, and motivate future research on argument mining.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Event Linking to Wikidata. (arXiv:2204.06535v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06535">
<div class="article-summary-box-inner">
<span><p>We present a task of multilingual linking of events to a knowledge base. We
automatically compile a large-scale dataset for this task, comprising of 1.8M
mentions across 44 languages referring to over 10.9K events from Wikidata. We
propose two variants of the event linking task: 1) multilingual, where event
descriptions are from the same language as the mention, and 2) crosslingual,
where all event descriptions are in English. On the two proposed tasks, we
compare multiple event linking systems including BM25+ (Lv and Zhai, 2011) and
multilingual adaptations of the biencoder and crossencoder architectures from
BLINK (Wu et al., 2020). In our experiments on the two task variants, we find
both biencoder and crossencoder models significantly outperform the BM25+
baseline. Our results also indicate that the crosslingual task is in general
more challenging than the multilingual task. To test the out-of-domain
generalization of the proposed linking systems, we additionally create a
Wikinews-based evaluation set. We present qualitative analysis highlighting
various aspects captured by the proposed dataset, including the need for
temporal reasoning over context and tackling diverse event descriptions across
languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Retrieve Videos by Asking Questions. (arXiv:2205.05739v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05739">
<div class="article-summary-box-inner">
<span><p>The majority of traditional text-to-video retrieval systems operate in static
environments, i.e., there is no interaction between the user and the agent
beyond the initial textual query provided by the user. This can be sub-optimal
if the initial query has ambiguities, which would lead to many falsely
retrieved videos. To overcome this limitation, we propose a novel framework for
Video Retrieval using Dialog (ViReD), which enables the user to interact with
an AI agent via multiple rounds of dialog, where the user refines retrieved
results by answering questions generated by an AI agent. Our novel multimodal
question generator learns to ask questions that maximize the subsequent video
retrieval performance using (i) the video candidates retrieved during the last
round of interaction with the user and (ii) the text-based dialog history
documenting all previous interactions, to generate questions that incorporate
both visual and linguistic cues relevant to video retrieval. Furthermore, to
generate maximally informative questions, we propose an Information-Guided
Supervision (IGS), which guides the question generator to ask questions that
would boost subsequent video retrieval accuracy. We validate the effectiveness
of our interactive ViReD framework on the AVSD dataset, showing that our
interactive method performs significantly better than traditional
non-interactive video retrieval systems. We also demonstrate that our proposed
approach generalizes to the real-world settings that involve interactions with
real humans, thus, demonstrating the robustness and generality of our framework
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Augmentation for Dementia Detection in Spoken Language. (arXiv:2206.12879v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.12879">
<div class="article-summary-box-inner">
<span><p>Dementia is a growing problem as our society ages, and detection methods are
often invasive and expensive. Recent deep-learning techniques can offer a
faster diagnosis and have shown promising results. However, they require large
amounts of labelled data which is not easily available for the task of dementia
detection. One effective solution to sparse data problems is data augmentation,
though the exact methods need to be selected carefully. To date, there has been
no empirical study of data augmentation on Alzheimer's disease (AD) datasets
for NLP and speech processing. In this work, we investigate data augmentation
techniques for the task of AD detection and perform an empirical evaluation of
the different approaches on two kinds of models for both the text and audio
domains. We use a transformer-based model for both domains, and SVM and Random
Forest models for the text and audio domains, respectively. We generate
additional samples using traditional as well as deep learning based methods and
show that data augmentation improves performance for both the text- and
audio-based models and that such results are comparable to state-of-the-art
results on the popular ADReSS set, with carefully crafted architectures and
features.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents. (arXiv:2207.01206v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01206">
<div class="article-summary-box-inner">
<span><p>Existing benchmarks for grounding language in interactive environments either
lack real-world linguistic elements, or prove difficult to scale up due to
substantial human involvement in the collection of data or feedback signals. To
bridge this gap, we develop WebShop -- a simulated e-commerce website
environment with $1.18$ million real-world products and $12,087$ crowd-sourced
text instructions. Given a text instruction specifying a product requirement,
an agent needs to navigate multiple types of webpages and issue diverse actions
to find, customize, and purchase an item. WebShop provides several challenges
for language grounding including understanding compositional instructions,
query (re-)formulation, comprehending and acting on noisy text in webpages, and
performing strategic exploration. We collect over $1,600$ human demonstrations
for the task, and train and evaluate a diverse range of agents using
reinforcement learning, imitation learning, and pre-trained image and language
models. Our best model achieves a task success rate of $29\%$, which
outperforms rule-based heuristics ($9.6\%$) but is far lower than human expert
performance ($59\%$). We also analyze agent and human trajectories and ablate
various model components to provide insights for developing future agents with
stronger language understanding and decision making abilities. Finally, we show
that agents trained on WebShop exhibit non-trivial sim-to-real transfer when
evaluated on amazon.com and ebay.com, indicating the potential value of WebShop
in developing practical web-based agents that can operate in the wild.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MIA 2022 Shared Task Submission: Leveraging Entity Representations, Dense-Sparse Hybrids, and Fusion-in-Decoder for Cross-Lingual Question Answering. (arXiv:2207.01940v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01940">
<div class="article-summary-box-inner">
<span><p>We describe our two-stage system for the Multilingual Information Access
(MIA) 2022 Shared Task on Cross-Lingual Open-Retrieval Question Answering. The
first stage consists of multilingual passage retrieval with a hybrid dense and
sparse retrieval strategy. The second stage consists of a reader which outputs
the answer from the top passages returned by the first stage. We show the
efficacy of using a multilingual language model with entity representations in
pretraining, sparse retrieval signals to help dense retrieval, and
Fusion-in-Decoder. On the development set, we obtain 43.46 F1 on XOR-TyDi QA
and 21.99 F1 on MKQA, for an average F1 score of 32.73. On the test set, we
obtain 40.93 F1 on XOR-TyDi QA and 22.29 F1 on MKQA, for an average F1 score of
31.61. We improve over the official baseline by over 4 F1 points on both the
development and test sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking the Value of Gazetteer in Chinese Named Entity Recognition. (arXiv:2207.02802v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02802">
<div class="article-summary-box-inner">
<span><p>Gazetteer is widely used in Chinese named entity recognition (NER) to enhance
span boundary detection and type classification. However, to further understand
the generalizability and effectiveness of gazetteers, the NLP community still
lacks a systematic analysis of the gazetteer-enhanced NER model. In this paper,
we first re-examine the effectiveness several common practices of the
gazetteer-enhanced NER models and carry out a series of detailed analysis to
evaluate the relationship between the model performance and the gazetteer
characteristics, which can guide us to build a more suitable gazetteer. The
findings of this paper are as follows: (1) the gazetteer improves most of the
situations that the traditional NER model datasets are difficult to learn. (2)
the performance of model greatly benefits from the high-quality pre-trained
lexeme embeddings. (3) a good gazetteer should cover more entities that can be
matched in both the training set and testing set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Win-Win Cooperation: Bundling Sequence and Span Models for Named Entity Recognition. (arXiv:2207.03300v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03300">
<div class="article-summary-box-inner">
<span><p>For Named Entity Recognition (NER), sequence labeling-based and span-based
paradigms are quite different. Previous research has demonstrated that the two
paradigms have clear complementary advantages, but few models have attempted to
leverage these advantages in a single NER model as far as we know. In our
previous work, we proposed a paradigm known as Bundling Learning (BL) to
address the above problem. The BL paradigm bundles the two NER paradigms,
enabling NER models to jointly tune their parameters by weighted summing each
paradigm's training loss. However, three critical issues remain unresolved:
When does BL work? Why does BL work? Can BL enhance the existing
state-of-the-art (SOTA) NER models? To address the first two issues, we
implement three NER models, involving a sequence labeling-based model--SeqNER,
a span-based NER model--SpanNER, and BL-NER that bundles SeqNER and SpanNER
together. We draw two conclusions regarding the two issues based on the
experimental results on eleven NER datasets from five domains. We then apply BL
to five existing SOTA NER models to investigate the third issue, consisting of
three sequence labeling-based models and two span-based models. Experimental
results indicate that BL consistently enhances their performance, suggesting
that it is possible to construct a new SOTA NER system by incorporating BL into
the current SOTA system. Moreover, we find that BL reduces both entity boundary
and type prediction errors. In addition, we compare two commonly used labeling
tagging methods as well as three types of span semantic representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying public values and spatial conflicts in urban planning. (arXiv:2207.04719v2 [cs.CY] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04719">
<div class="article-summary-box-inner">
<span><p>Identifying the diverse and often competing values of citizens, and resolving
the consequent public value conflicts, are of significant importance for
inclusive and integrated urban development. Scholars have highlighted that
relational, value-laden urban space gives rise to many diverse conflicts that
vary both spatially and temporally. Although notions of public value conflicts
have been conceived in theory, there are very few empirical studies that
identify such values and their conflicts in urban space. Building on public
value theory and using a case-study mixed-methods approach, this paper proposes
a new approach to empirically investigate public value conflicts in urban
space. Using unstructured participatory data of 4,528 citizen contributions
from a Public Participation Geographic Information Systems in Hamburg, Germany,
natural language processing and spatial clustering techniques are used to
identify areas of potential value conflicts. Four expert workshops assess and
interpret these quantitative findings. Integrating both quantitative and
qualitative results, 19 general public values and a total of 9 archetypical
conflicts are identified. On the basis of these results, this paper proposes a
new conceptual tool of Public Value Spheres that extends the theoretical notion
of public-value conflicts and helps to further account for the value-laden
nature of urban space.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Graph Induction enabling Recommending and Trend Analysis: A Corporate Research Community Use Case. (arXiv:2207.05188v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05188">
<div class="article-summary-box-inner">
<span><p>A research division plays an important role of driving innovation in an
organization. Drawing insights, following trends, keeping abreast of new
research, and formulating strategies are increasingly becoming more challenging
for both researchers and executives as the amount of information grows in both
velocity and volume. In this paper we present a use case of how a corporate
research community, IBM Research, utilizes Semantic Web technologies to induce
a unified Knowledge Graph from both structured and textual data obtained by
integrating various applications used by the community related to research
projects, academic papers, datasets, achievements and recognition. In order to
make the Knowledge Graph more accessible to application developers, we
identified a set of common patterns for exploiting the induced knowledge and
exposed them as APIs. Those patterns were born out of user research which
identified the most valuable use cases or user pain points to be alleviated. We
outline two distinct scenarios: recommendation and analytics for business use.
We will discuss these scenarios in detail and provide an empirical evaluation
on entity recommendation specifically. The methodology used and the lessons
learned from this work can be applied to other organizations facing similar
challenges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models (Mostly) Know What They Know. (arXiv:2207.05221v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05221">
<div class="article-summary-box-inner">
<span><p>We study whether language models can evaluate the validity of their own
claims and predict which questions they will be able to answer correctly. We
first show that larger models are well-calibrated on diverse multiple choice
and true/false questions when they are provided in the right format. Thus we
can approach self-evaluation on open-ended sampling tasks by asking models to
first propose answers, and then to evaluate the probability "P(True)" that
their answers are correct. We find encouraging performance, calibration, and
scaling for P(True) on a diverse array of tasks. Performance at self-evaluation
further improves when we allow models to consider many of their own samples
before predicting the validity of one specific possibility. Next, we
investigate whether models can be trained to predict "P(IK)", the probability
that "I know" the answer to a question, without reference to any particular
proposed answer. Models perform well at predicting P(IK) and partially
generalize across tasks, though they struggle with calibration of P(IK) on new
tasks. The predicted P(IK) probabilities also increase appropriately in the
presence of relevant source materials in the context, and in the presence of
hints towards the solution of mathematical word problems. We hope these
observations lay the groundwork for training more honest models, and for
investigating how honesty generalizes to cases where models are trained on
objectives other than the imitation of human writing.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Localisation And Imaging Methods for Moving Target Ghost Imaging Radar Based On Correlation Intensity Weighting. (arXiv:2207.07649v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07649">
<div class="article-summary-box-inner">
<span><p>Ghost imaging radar is a new system of gaze imaging radar with high detection
sensitivity, super-resolution and better anti-interference performance, but the
relative motion between the radar system and the target will make the target
imaging deteriorate. This paper proposes to perform absolute position
localisation of a single target in the field of view by weighting the
correlation strength of a single frame image of rough target, and to compensate
translation of the reference arm speckle according to the localisation and
tracking trajectory to accumulate the rough image into a high quality image.
The proposed correlation intensity weighted localization and tracking imaging
method has been verified by simulation to be able to locate and image targets
in the field of view well.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">POET: Training Neural Networks on Tiny Devices with Integrated Rematerialization and Paging. (arXiv:2207.07697v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07697">
<div class="article-summary-box-inner">
<span><p>Fine-tuning models on edge devices like mobile phones would enable
privacy-preserving personalization over sensitive data. However, edge training
has historically been limited to relatively small models with simple
architectures because training is both memory and energy intensive. We present
POET, an algorithm to enable training large neural networks on memory-scarce
battery-operated edge devices. POET jointly optimizes the integrated search
search spaces of rematerialization and paging, two algorithms to reduce the
memory consumption of backpropagation. Given a memory budget and a run-time
constraint, we formulate a mixed-integer linear program (MILP) for
energy-optimal training. Our approach enables training significantly larger
models on embedded devices while reducing energy consumption while not
modifying mathematical correctness of backpropagation. We demonstrate that it
is possible to fine-tune both ResNet-18 and BERT within the memory constraints
of a Cortex-M class embedded device while outperforming current edge training
methods in energy efficiency. POET is an open-source project available at
https://github.com/ShishirPatil/poet
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Untrained, physics-informed neural networks for structured illumination microscopy. (arXiv:2207.07705v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07705">
<div class="article-summary-box-inner">
<span><p>In recent years there has been great interest in using deep neural networks
(DNN) for super-resolution image reconstruction including for structured
illumination microscopy (SIM). While these methods have shown very promising
results, they all rely on data-driven, supervised training strategies that need
a large number of ground truth images, which is experimentally difficult to
realize. For SIM imaging, there exists a need for a flexible, general, and
open-source reconstruction method that can be readily adapted to different
forms of structured illumination. We demonstrate that we can combine a deep
neural network with the forward model of the structured illumination process to
reconstruct sub-diffraction images without training data. The resulting
physics-informed neural network (PINN) can be optimized on a single set of
diffraction limited sub-images and thus doesn't require any training set. We
show with simulated and experimental data that this PINN can be applied to a
wide variety of SIM methods by simply changing the known illumination patterns
used in the loss function and can achieve resolution improvements that match
well with theoretical expectations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Focal Loss: Asking Your Discriminator for Hard Examples. (arXiv:2207.07739v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07739">
<div class="article-summary-box-inner">
<span><p>Focal Loss has reached incredible popularity as it uses a simple technique to
identify and utilize hard examples to achieve better performance on
classification. However, this method does not easily generalize outside of
classification tasks, such as in keypoint detection. In this paper, we propose
a novel adaptation of Focal Loss for keypoint detection tasks, called
Adversarial Focal Loss (AFL). AFL not only is semantically analogous to Focal
loss, but also works as a plug-and-chug upgrade for arbitrary loss functions.
While Focal Loss requires output from a classifier, AFL leverages a separate
adversarial network to produce a difficulty score for each input. This
difficulty score can then be used to dynamically prioritize learning on hard
examples, even in absence of a classifier. In this work, we show AFL's
effectiveness in enhancing existing methods in keypoint detection and verify
its capability to re-weigh examples based on difficulty.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human keypoint detection for close proximity human-robot interaction. (arXiv:2207.07742v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07742">
<div class="article-summary-box-inner">
<span><p>We study the performance of state-of-the-art human keypoint detectors in the
context of close proximity human-robot interaction. The detection in this
scenario is specific in that only a subset of body parts such as hands and
torso are in the field of view. In particular, (i) we survey existing datasets
with human pose annotation from the perspective of close proximity images and
prepare and make publicly available a new Human in Close Proximity (HiCP)
dataset; (ii) we quantitatively and qualitatively compare state-of-the-art
human whole-body 2D keypoint detection methods (OpenPose, MMPose, AlphaPose,
Detectron2) on this dataset; (iii) since accurate detection of hands and
fingers is critical in applications with handovers, we evaluate the performance
of the MediaPipe hand detector; (iv) we deploy the algorithms on a humanoid
robot with an RGB-D camera on its head and evaluate the performance in 3D human
keypoint detection. A motion capture system is used as reference.
</p>
<p>The best performing whole-body keypoint detectors in close proximity were
MMPose and AlphaPose, but both had difficulty with finger detection. Thus, we
propose a combination of MMPose or AlphaPose for the body and MediaPipe for the
hands in a single framework providing the most accurate and robust detection.
We also analyse the failure modes of individual detectors -- for example, to
what extent the absence of the head of the person in the image degrades
performance. Finally, we demonstrate the framework in a scenario where a
humanoid robot interacting with a person uses the detected 3D keypoints for
whole-body avoidance maneuvers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HOME: High-Order Mixed-Moment-based Embedding for Representation Learning. (arXiv:2207.07743v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07743">
<div class="article-summary-box-inner">
<span><p>Minimum redundancy among different elements of an embedding in a latent space
is a fundamental requirement or major preference in representation learning to
capture intrinsic informational structures. Current self-supervised learning
methods minimize a pair-wise covariance matrix to reduce the feature redundancy
and produce promising results. However, such representation features of
multiple variables may contain the redundancy among more than two feature
variables that cannot be minimized via the pairwise regularization. Here we
propose the High-Order Mixed-Moment-based Embedding (HOME) strategy to reduce
the redundancy between any sets of feature variables, which is to our best
knowledge the first attempt to utilize high-order statistics/information in
this context. Multivariate mutual information is minimum if and only if
multiple variables are mutually independent, which suggests the necessary
conditions of factorized mixed moments among multiple variables. Based on these
statistical and information theoretic principles, our general HOME framework is
presented for self-supervised representation learning. Our initial experiments
show that a simple version in the form of a three-order HOME scheme already
significantly outperforms the current two-order baseline method (i.e., Barlow
Twins) in terms of the linear evaluation on representation features.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ESFPNet: efficient deep learning architecture for real-time lesion segmentation in autofluorescence bronchoscopic video. (arXiv:2207.07759v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07759">
<div class="article-summary-box-inner">
<span><p>Lung cancer tends to be detected at an advanced stage, resulting in a high
patient mortality rate. Thus, recent research has focused on early disease
detection. Lung cancer generally first appears as lesions developing within the
bronchial epithelium of the airway walls. Bronchoscopy is the procedure of
choice for effective noninvasive bronchial lesion detection. In particular,
autofluorescence bronchoscopy (AFB) discriminates the autofluorescence
properties of normal and diseased tissue, whereby lesions appear reddish brown
in AFB video frames, while normal tissue appears green. Because recent studies
show AFB's ability for high lesion sensitivity, it has become a potentially
pivotal method during the standard bronchoscopic airway exam for early-stage
lung cancer detection. Unfortunately, manual inspection of AFB video is
extremely tedious and error-prone, while limited effort has been expended
toward potentially more robust automatic AFB lesion detection and segmentation.
We propose a real-time deep learning architecture ESFPNet for robust detection
and segmentation of bronchial lesions from an AFB video stream. The
architecture features an encoder structure that exploits pretrained Mix
Transformer (MiT) encoders and a stage-wise feature pyramid (ESFP) decoder
structure. Results from AFB videos derived from lung cancer patient airway
exams indicate that our approach gives mean Dice index and IOU values of 0.782
and 0.658, respectively, while having a processing throughput of 27 frames/sec.
These values are superior to results achieved by other competing architectures
that use Mix transformers or CNN-based encoders. Moreover, the superior
performance on the ETIS-LaribPolypDB dataset demonstrates its potential
applicability to other domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Long-Term Spatial-Temporal Graphs for Active Speaker Detection. (arXiv:2207.07783v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07783">
<div class="article-summary-box-inner">
<span><p>Active speaker detection (ASD) in videos with multiple speakers is a
challenging task as it requires learning effective audiovisual features and
spatial-temporal correlations over long temporal windows. In this paper, we
present SPELL, a novel spatial-temporal graph learning framework that can solve
complex tasks such as ASD. To this end, each person in a video frame is first
encoded in a unique node for that frame. Nodes corresponding to a single person
across frames are connected to encode their temporal dynamics. Nodes within a
frame are also connected to encode inter-person relationships. Thus, SPELL
reduces ASD to a node classification task. Importantly, SPELL is able to reason
over long temporal contexts for all nodes without relying on computationally
expensive fully connected graph neural networks. Through extensive experiments
on the AVA-ActiveSpeaker dataset, we demonstrate that learning graph-based
representations can significantly improve the active speaker detection
performance owing to its explicit spatial and temporal structure. SPELL
outperforms all previous state-of-the-art approaches while requiring
significantly lower memory and computational resources. Our code is publicly
available at https://github.com/SRA2/SPELL
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards the Desirable Decision Boundary by Moderate-Margin Adversarial Training. (arXiv:2207.07793v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07793">
<div class="article-summary-box-inner">
<span><p>Adversarial training, as one of the most effective defense methods against
adversarial attacks, tends to learn an inclusive decision boundary to increase
the robustness of deep learning models. However, due to the large and
unnecessary increase in the margin along adversarial directions, adversarial
training causes heavy cross-over between natural examples and adversarial
examples, which is not conducive to balancing the trade-off between robustness
and natural accuracy. In this paper, we propose a novel adversarial training
scheme to achieve a better trade-off between robustness and natural accuracy.
It aims to learn a moderate-inclusive decision boundary, which means that the
margins of natural examples under the decision boundary are moderate. We call
this scheme Moderate-Margin Adversarial Training (MMAT), which generates
finer-grained adversarial examples to mitigate the cross-over problem. We also
take advantage of logits from a teacher model that has been well-trained to
guide the learning of our model. Finally, MMAT achieves high natural accuracy
and robustness under both black-box and white-box attacks. On SVHN, for
example, state-of-the-art robustness and natural accuracy are achieved.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RCRN: Real-world Character Image Restoration Network via Skeleton Extraction. (arXiv:2207.07795v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07795">
<div class="article-summary-box-inner">
<span><p>Constructing high-quality character image datasets is challenging because
real-world images are often affected by image degradation. There are
limitations when applying current image restoration methods to such real-world
character images, since (i) the categories of noise in character images are
different from those in general images; (ii) real-world character images
usually contain more complex image degradation, e.g., mixed noise at different
noise levels. To address these problems, we propose a real-world character
restoration network (RCRN) to effectively restore degraded character images,
where character skeleton information and scale-ensemble feature extraction are
utilized to obtain better restoration performance. The proposed method consists
of a skeleton extractor (SENet) and a character image restorer (CiRNet). SENet
aims to preserve the structural consistency of the character and normalize
complex noise. Then, CiRNet reconstructs clean images from degraded character
images and their skeletons. Due to the lack of benchmarks for real-world
character image restoration, we constructed a dataset containing 1,606
character images with real-world degradation to evaluate the validity of the
proposed method. The experimental results demonstrate that RCRN outperforms
state-of-the-art methods quantitatively and qualitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CARBEN: Composite Adversarial Robustness Benchmark. (arXiv:2207.07797v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07797">
<div class="article-summary-box-inner">
<span><p>Prior literature on adversarial attack methods has mainly focused on
attacking with and defending against a single threat model, e.g., perturbations
bounded in Lp ball. However, multiple threat models can be combined into
composite perturbations. One such approach, composite adversarial attack (CAA),
not only expands the perturbable space of the image, but also may be overlooked
by current modes of robustness evaluation. This paper demonstrates how CAA's
attack order affects the resulting image, and provides real-time inferences of
different models, which will facilitate users' configuration of the parameters
of the attack level and their rapid evaluation of model prediction. A
leaderboard to benchmark adversarial robustness against CAA is also introduced.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CharFormer: A Glyph Fusion based Attentive Framework for High-precision Character Image Denoising. (arXiv:2207.07798v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07798">
<div class="article-summary-box-inner">
<span><p>Degraded images commonly exist in the general sources of character images,
leading to unsatisfactory character recognition results. Existing methods have
dedicated efforts to restoring degraded character images. However, the
denoising results obtained by these methods do not appear to improve character
recognition performance. This is mainly because current methods only focus on
pixel-level information and ignore critical features of a character, such as
its glyph, resulting in character-glyph damage during the denoising process. In
this paper, we introduce a novel generic framework based on glyph fusion and
attention mechanisms, i.e., CharFormer, for precisely recovering character
images without changing their inherent glyphs. Unlike existing frameworks,
CharFormer introduces a parallel target task for capturing additional
information and injecting it into the image denoising backbone, which will
maintain the consistency of character glyphs during character image denoising.
Moreover, we utilize attention-based networks for global-local feature
interaction, which will help to deal with blind denoising and enhance denoising
performance. We compare CharFormer with state-of-the-art methods on multiple
datasets. The experimental results show the superiority of CharFormer
quantitatively and qualitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Granularity-Unified Representations for Text-to-Image Person Re-identification. (arXiv:2207.07802v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07802">
<div class="article-summary-box-inner">
<span><p>Text-to-image person re-identification (ReID) aims to search for pedestrian
images of an interested identity via textual descriptions. It is challenging
due to both rich intra-modal variations and significant inter-modal gaps.
Existing works usually ignore the difference in feature granularity between the
two modalities, i.e., the visual features are usually fine-grained while
textual features are coarse, which is mainly responsible for the large
inter-modal gaps. In this paper, we propose an end-to-end framework based on
transformers to learn granularity-unified representations for both modalities,
denoted as LGUR. LGUR framework contains two modules: a Dictionary-based
Granularity Alignment (DGA) module and a Prototype-based Granularity
Unification (PGU) module. In DGA, in order to align the granularities of two
modalities, we introduce a Multi-modality Shared Dictionary (MSD) to
reconstruct both visual and textual features. Besides, DGA has two important
factors, i.e., the cross-modality guidance and the foreground-centric
reconstruction, to facilitate the optimization of MSD. In PGU, we adopt a set
of shared and learnable prototypes as the queries to extract diverse and
semantically aligned features for both modalities in the granularity-unified
feature space, which further promotes the ReID performance. Comprehensive
experiments show that our LGUR consistently outperforms state-of-the-arts by
large margins on both CUHK-PEDES and ICFG-PEDES datasets. Code will be released
at https://github.com/ZhiyinShao-H/LGUR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Masked Spatial-Spectral Autoencoders Are Excellent Hyperspectral Defenders. (arXiv:2207.07803v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07803">
<div class="article-summary-box-inner">
<span><p>Deep learning methodology contributes a lot to the development of
hyperspectral image (HSI) analysis community. However, it also makes HSI
analysis systems vulnerable to adversarial attacks. To this end, we propose a
masked spatial-spectral autoencoder (MSSA) in this paper under self-supervised
learning theory, for enhancing the robustness of HSI analysis systems. First, a
masked sequence attention learning module is conducted to promote the inherent
robustness of HSI analysis systems along spectral channel. Then, we develop a
graph convolutional network with learnable graph structure to establish global
pixel-wise combinations.In this way, the attack effect would be dispersed by
all the related pixels among each combination, and a better defense performance
is achievable in spatial aspect.Finally, to improve the defense transferability
and address the problem of limited labelled samples, MSSA employs spectra
reconstruction as a pretext task and fits the datasets in a self-supervised
manner.Comprehensive experiments over three benchmarks verify the effectiveness
of MSSA in comparison with the state-of-the-art hyperspectral classification
methods and representative adversarial defense strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-calibrating Photometric Stereo by Neural Inverse Rendering. (arXiv:2207.07815v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07815">
<div class="article-summary-box-inner">
<span><p>This paper tackles the task of uncalibrated photometric stereo for 3D object
reconstruction, where both the object shape, object reflectance, and lighting
directions are unknown. This is an extremely difficult task, and the challenge
is further compounded with the existence of the well-known generalized
bas-relief (GBR) ambiguity in photometric stereo. Previous methods to resolve
this ambiguity either rely on an overly simplified reflectance model, or assume
special light distribution. We propose a new method that jointly optimizes
object shape, light directions, and light intensities, all under general
surfaces and lights assumptions. The specularities are used explicitly to solve
uncalibrated photometric stereo via a neural inverse rendering process. We
gradually fit specularities from shiny to rough using novel progressive
specular bases. Our method leverages a physically based rendering equation by
minimizing the reconstruction error on a per-object-basis. Our method
demonstrates state-of-the-art accuracy in light estimation and shape recovery
on real-world datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bagging Regional Classification Activation Maps for Weakly Supervised Object Localization. (arXiv:2207.07818v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07818">
<div class="article-summary-box-inner">
<span><p>Classification activation map (CAM), utilizing the classification structure
to generate pixel-wise localization maps, is a crucial mechanism for weakly
supervised object localization (WSOL). However, CAM directly uses the
classifier trained on image-level features to locate objects, making it prefers
to discern global discriminative factors rather than regional object cues. Thus
only the discriminative locations are activated when feeding pixel-level
features into this classifier. To solve this issue, this paper elaborates a
plug-and-play mechanism called BagCAMs to better project a well-trained
classifier for the localization task without refining or re-training the
baseline structure. Our BagCAMs adopts a proposed regional localizer generation
(RLG) strategy to define a set of regional localizers and then derive them from
a well-trained classifier. These regional localizers can be viewed as the base
learner that only discerns region-wise object factors for localization tasks,
and their results can be effectively weighted by our BagCAMs to form the final
localization map. Experiments indicate that adopting our proposed BagCAMs can
improve the performance of baseline WSOL methods to a great extent and obtains
state-of-the-art performance on three WSOL benchmarks. Code are released at
https://github.com/zh460045050/BagCAMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Domain Cross-Set Few-Shot Learning via Learning Compact and Aligned Representations. (arXiv:2207.07826v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07826">
<div class="article-summary-box-inner">
<span><p>Few-shot learning (FSL) aims to recognize novel queries with only a few
support samples through leveraging prior knowledge from a base dataset. In this
paper, we consider the domain shift problem in FSL and aim to address the
domain gap between the support set and the query set. Different from previous
cross-domain FSL work (CD-FSL) that considers the domain shift between base and
novel classes, the new problem, termed cross-domain cross-set FSL (CDSC-FSL),
requires few-shot learners not only to adapt to the new domain, but also to be
consistent between different domains within each novel class. To this end, we
propose a novel approach, namely stabPA, to learn prototypical compact and
cross-domain aligned representations, so that the domain shift and few-shot
learning can be addressed simultaneously. We evaluate our approach on two new
CDCS-FSL benchmarks built from the DomainNet and Office-Home datasets
respectively. Remarkably, our approach outperforms multiple elaborated
baselines by a large margin, e.g., improving 5-shot accuracy by 6.0 points on
average on DomainNet. Code is available at
https://github.com/WentaoChen0813/CDCS-FSL
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalizable Memory-driven Transformer for Multivariate Long Sequence Time-series Forecasting. (arXiv:2207.07827v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07827">
<div class="article-summary-box-inner">
<span><p>Multivariate long sequence time-series forecasting (M-LSTF) is a practical
but challenging problem. Unlike traditional timer-series forecasting tasks,
M-LSTF tasks are more challenging from two aspects: 1) M-LSTF models need to
learn time-series patterns both within and between multiple time features; 2)
Under the rolling forecasting setting, the similarity between two consecutive
training samples increases with the increasing prediction length, which makes
models more prone to overfitting. In this paper, we propose a generalizable
memory-driven Transformer to target M-LSTF problems. Specifically, we first
propose a global-level memory component to drive the forecasting procedure by
integrating multiple time-series features. In addition, we adopt a progressive
fashion to train our model to increase its generalizability, in which we
gradually introduce Bernoulli noises to training samples. Extensive experiments
have been performed on five different datasets across multiple fields.
Experimental results demonstrate that our approach can be seamlessly plugged
into varying Transformer-based models to improve their performances up to
roughly 30%. Particularly, this is the first work to specifically focus on the
M-LSTF tasks to the best of our knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structural Prior Guided Generative Adversarial Transformers for Low-Light Image Enhancement. (arXiv:2207.07828v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07828">
<div class="article-summary-box-inner">
<span><p>We propose an effective Structural Prior guided Generative Adversarial
Transformer (SPGAT) to solve low-light image enhancement. Our SPGAT mainly
contains a generator with two discriminators and a structural prior estimator
(SPE). The generator is based on a U-shaped Transformer which is used to
explore non-local information for better clear image restoration. The SPE is
used to explore useful structures from images to guide the generator for better
structural detail estimation. To generate more realistic images, we develop a
new structural prior guided adversarial learning method by building the skip
connections between the generator and discriminators so that the discriminators
can better discriminate between real and fake features. Finally, we propose a
parallel windows-based Swin Transformer block to aggregate different level
hierarchical features for high-quality image restoration. Experimental results
demonstrate that the proposed SPGAT performs favorably against recent
state-of-the-art methods on both synthetic and real-world datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TS2-Net: Token Shift and Selection Transformer for Text-Video Retrieval. (arXiv:2207.07852v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07852">
<div class="article-summary-box-inner">
<span><p>Text-Video retrieval is a task of great practical value and has received
increasing attention, among which learning spatial-temporal video
representation is one of the research hotspots. The video encoders in the
state-of-the-art video retrieval models usually directly adopt the pre-trained
vision backbones with the network structure fixed, they therefore can not be
further improved to produce the fine-grained spatial-temporal video
representation. In this paper, we propose Token Shift and Selection Network
(TS2-Net), a novel token shift and selection transformer architecture, which
dynamically adjusts the token sequence and selects informative tokens in both
temporal and spatial dimensions from input video samples. The token shift
module temporally shifts the whole token features back-and-forth across
adjacent frames, to preserve the complete token representation and capture
subtle movements. Then the token selection module selects tokens that
contribute most to local spatial semantics. Based on thorough experiments, the
proposed TS2-Net achieves state-of-the-art performance on major text-video
retrieval benchmarks, including new records on MSRVTT, VATEX, LSMDC,
ActivityNet, and DiDeMo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Lottery Ticket Hypothesis for Self-attention in Convolutional Neural Network. (arXiv:2207.07858v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07858">
<div class="article-summary-box-inner">
<span><p>Recently many plug-and-play self-attention modules (SAMs) are proposed to
enhance the model generalization by exploiting the internal information of deep
convolutional neural networks (CNNs). In general, previous works ignore where
to plug in the SAMs since they connect the SAMs individually with each block of
the entire CNN backbone for granted, leading to incremental computational cost
and the number of parameters with the growth of network depth. However, we
empirically find and verify some counterintuitive phenomena that: (a)
Connecting the SAMs to all the blocks may not always bring the largest
performance boost, and connecting to partial blocks would be even better; (b)
Adding the SAMs to a CNN may not always bring a performance boost, and instead
it may even harm the performance of the original CNN backbone. Therefore, we
articulate and demonstrate the Lottery Ticket Hypothesis for Self-attention
Networks: a full self-attention network contains a subnetwork with sparse
self-attention connections that can (1) accelerate inference, (2) reduce extra
parameter increment, and (3) maintain accuracy. In addition to the empirical
evidence, this hypothesis is also supported by our theoretical evidence.
Furthermore, we propose a simple yet effective reinforcement-learning-based
method to search the ticket, i.e., the connection scheme that satisfies the
three above-mentioned conditions. Extensive experiments on widely-used
benchmark datasets and popular self-attention networks show the effectiveness
of our method. Besides, our experiments illustrate that our searched ticket has
the capacity of transferring to some vision tasks, e.g., crowd counting and
segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransGrasp: Grasp Pose Estimation of a Category of Objects by Transferring Grasps from Only One Labeled Instance. (arXiv:2207.07861v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07861">
<div class="article-summary-box-inner">
<span><p>Grasp pose estimation is an important issue for robots to interact with the
real world. However, most of existing methods require exact 3D object models
available beforehand or a large amount of grasp annotations for training. To
avoid these problems, we propose TransGrasp, a category-level grasp pose
estimation method that predicts grasp poses of a category of objects by
labeling only one object instance. Specifically, we perform grasp pose transfer
across a category of objects based on their shape correspondences and propose a
grasp pose refinement module to further fine-tune grasp pose of grippers so as
to ensure successful grasps. Experiments demonstrate the effectiveness of our
method on achieving high-quality grasps with the transferred grasp poses. Our
code is available at https://github.com/yanjh97/TransGrasp.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic dataset generation for specific object detection. (arXiv:2207.07867v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07867">
<div class="article-summary-box-inner">
<span><p>In the past decade, object detection tasks are defined mostly by large public
datasets. However, building object detection datasets is not scalable due to
inefficient image collecting and labeling. Furthermore, most labels are still
in the form of bounding boxes, which provide much less information than the
real human visual system. In this paper, we present a method to synthesize
object-in-scene images, which can preserve the objects' detailed features
without bringing irrelevant information. In brief, given a set of images
containing a target object, our algorithm first trains a model to find an
approximate center of the object as an anchor, then makes an outline regression
to estimate its boundary, and finally blends the object into a new scene. Our
result shows that in the synthesized image, the boundaries of objects blend
very well with the background. Experiments also show that SOTA segmentation
models work well with our synthesized data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLOSE: Curriculum Learning On the Sharing Extent Towards Better One-shot NAS. (arXiv:2207.07868v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07868">
<div class="article-summary-box-inner">
<span><p>One-shot Neural Architecture Search (NAS) has been widely used to discover
architectures due to its efficiency. However, previous studies reveal that
one-shot performance estimations of architectures might not be well correlated
with their performances in stand-alone training because of the excessive
sharing of operation parameters (i.e., large sharing extent) between
architectures. Thus, recent methods construct even more over-parameterized
supernets to reduce the sharing extent. But these improved methods introduce a
large number of extra parameters and thus cause an undesirable trade-off
between the training costs and the ranking quality. To alleviate the above
issues, we propose to apply Curriculum Learning On Sharing Extent (CLOSE) to
train the supernet both efficiently and effectively. Specifically, we train the
supernet with a large sharing extent (an easier curriculum) at the beginning
and gradually decrease the sharing extent of the supernet (a harder
curriculum). To support this training strategy, we design a novel supernet
(CLOSENet) that decouples the parameters from operations to realize a flexible
sharing scheme and adjustable sharing extent. Extensive experiments demonstrate
that CLOSE can obtain a better ranking quality across different computational
budget constraints than other one-shot supernets, and is able to discover
superior architectures when combined with various search strategies. Code is
available at https://github.com/walkerning/aw_nas.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CA-SpaceNet: Counterfactual Analysis for 6D Pose Estimation in Space. (arXiv:2207.07869v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07869">
<div class="article-summary-box-inner">
<span><p>Reliable and stable 6D pose estimation of uncooperative space objects plays
an essential role in on-orbit servicing and debris removal missions.
Considering that the pose estimator is sensitive to background interference,
this paper proposes a counterfactual analysis framework named CASpaceNet to
complete robust 6D pose estimation of the spaceborne targets under complicated
background. Specifically, conventional methods are adopted to extract the
features of the whole image in the factual case. In the counterfactual case, a
non-existent image without the target but only the background is imagined. Side
effect caused by background interference is reduced by counterfactual analysis,
which leads to unbiased prediction in final results. In addition, we also carry
out lowbit-width quantization for CA-SpaceNet and deploy part of the framework
to a Processing-In-Memory (PIM) accelerator on FPGA. Qualitative and
quantitative results demonstrate the effectiveness and efficiency of our
proposed method. To our best knowledge, this paper applies causal inference and
network quantization to the 6D pose estimation of space-borne targets for the
first time. The code is available at
https://github.com/Shunli-Wang/CA-SpaceNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeFSAC: Neurally Filtered Minimal Samples. (arXiv:2207.07872v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07872">
<div class="article-summary-box-inner">
<span><p>Since RANSAC, a great deal of research has been devoted to improving both its
accuracy and run-time. Still, only a few methods aim at recognizing invalid
minimal samples early, before the often expensive model estimation and quality
calculation are done. To this end, we propose NeFSAC, an efficient algorithm
for neural filtering of motion-inconsistent and poorly-conditioned minimal
samples. We train NeFSAC to predict the probability of a minimal sample leading
to an accurate relative pose, only based on the pixel coordinates of the image
correspondences. Our neural filtering model learns typical motion patterns of
samples which lead to unstable poses, and regularities in the possible motions
to favour well-conditioned and likely-correct samples. The novel lightweight
architecture implements the main invariants of minimal samples for pose
estimation, and a novel training scheme addresses the problem of extreme class
imbalance. NeFSAC can be plugged into any existing RANSAC-based pipeline. We
integrate it into USAC and show that it consistently provides strong speed-ups
even under extreme train-test domain gaps - for example, the model trained for
the autonomous driving scenario works on PhotoTourism too. We tested NeFSAC on
more than 100k image pairs from three publicly available real-world datasets
and found that it leads to one order of magnitude speed-up, while often finding
more accurate results than USAC alone. The source code is available at
https://github.com/cavalli1234/NeFSAC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Importance of Hyperparameters and Data Augmentation for Self-Supervised Learning. (arXiv:2207.07875v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07875">
<div class="article-summary-box-inner">
<span><p>Self-Supervised Learning (SSL) has become a very active area of Deep Learning
research where it is heavily used as a pre-training method for classification
and other tasks. However, the rapid pace of advancements in this area comes at
a price: training pipelines vary significantly across papers, which presents a
potentially crucial confounding factor. Here, we show that, indeed, the choice
of hyperparameters and data augmentation strategies can have a dramatic impact
on performance. To shed light on these neglected factors and help maximize the
power of SSL, we hyperparameterize these components and optimize them with
Bayesian optimization, showing improvements across multiple datasets for the
SimSiam SSL approach. Realizing the importance of data augmentations for SSL,
we also introduce a new automated data augmentation algorithm, GroupAugment,
which considers groups of augmentations and optimizes the sampling across
groups. In contrast to algorithms designed for supervised learning,
GroupAugment achieved consistently high linear evaluation accuracy across all
datasets we considered. Overall, our results indicate the importance and likely
underestimated role of data augmentation for SSL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Clover: Towards A Unified Video-Language Alignment and Fusion Model. (arXiv:2207.07885v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07885">
<div class="article-summary-box-inner">
<span><p>Building a universal video-language model for solving various video
understanding tasks (e.g., text-video retrieval, video question answering) is
an open challenge to the machine learning field. Towards this goal, most recent
attempts train the models, usually consisting of uni-modal and cross-modal
feature encoders, with supervised or pair-wise contrastive pre-text tasks.
Though offering attractive generality, the resulted models have to compromise
between efficiency and performance. We argue the flaws are caused by their
pre-training strategies\textemdash they cannot well align and fuse features
from different modalities simultaneously. We then introduce Clover -- a
Correlated Video-Language pre-training method -- towards a universal
video-language model for solving multiple video understanding tasks with
neither performance nor efficiency compromise. It improves cross-modal feature
alignment and fusion via a novel tri-modal alignment pre-training task.
Additionally, we propose to enhance the tri-modal alignment via incorporating
learning from masked samples and a novel pair-wise ranking loss. Clover
demonstrates outstanding generality. It establishes new state-of-the-arts on
multiple downstream tasks, including three retrieval tasks for both zero-shot
and fine-tuning settings, and eight video question answering tasks. Codes and
pre-trained models will be released at https://github.com/LeeYN-43/Clover.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">You Should Look at All Objects. (arXiv:2207.07889v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07889">
<div class="article-summary-box-inner">
<span><p>Feature pyramid network (FPN) is one of the key components for object
detectors. However, there is a long-standing puzzle for researchers that the
detection performance of large-scale objects are usually suppressed after
introducing FPN. To this end, this paper first revisits FPN in the detection
framework and reveals the nature of the success of FPN from the perspective of
optimization. Then, we point out that the degraded performance of large-scale
objects is due to the arising of improper back-propagation paths after
integrating FPN. It makes each level of the backbone network only has the
ability to look at the objects within a certain scale range. Based on these
analysis, two feasible strategies are proposed to enable each level of the
backbone to look at all objects in the FPN-based detection frameworks.
Specifically, one is to introduce auxiliary objective functions to make each
backbone level directly receive the back-propagation signals of various-scale
objects during training. The other is to construct the feature pyramid in a
more reasonable way to avoid the irrational back-propagation paths. Extensive
experiments on the COCO benchmark validate the soundness of our analysis and
the effectiveness of our methods. Without bells and whistles, we demonstrate
that our method achieves solid improvements (more than 2%) on various detection
frameworks: one-stage, two-stage, anchor-based, anchor-free and
transformer-based detectors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Modal Unsupervised Pre-Training for Surgical Operating Room Workflow Analysis. (arXiv:2207.07894v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07894">
<div class="article-summary-box-inner">
<span><p>Data-driven approaches to assist operating room (OR) workflow analysis depend
on large curated datasets that are time consuming and expensive to collect. On
the other hand, we see a recent paradigm shift from supervised learning to
self-supervised and/or unsupervised learning approaches that can learn
representations from unlabeled datasets. In this paper, we leverage the
unlabeled data captured in robotic surgery ORs and propose a novel way to fuse
the multi-modal data for a single video frame or image. Instead of producing
different augmentations (or 'views') of the same image or video frame which is
a common practice in self-supervised learning, we treat the multi-modal data as
different views to train the model in an unsupervised manner via clustering. We
compared our method with other state of the art methods and results show the
superior performance of our approach on surgical video activity recognition and
semantic segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">JPerceiver: Joint Perception Network for Depth, Pose and Layout Estimation in Driving Scenes. (arXiv:2207.07895v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07895">
<div class="article-summary-box-inner">
<span><p>Depth estimation, visual odometry (VO), and bird's-eye-view (BEV) scene
layout estimation present three critical tasks for driving scene perception,
which is fundamental for motion planning and navigation in autonomous driving.
Though they are complementary to each other, prior works usually focus on each
individual task and rarely deal with all three tasks together. A naive way is
to accomplish them independently in a sequential or parallel manner, but there
are many drawbacks, i.e., 1) the depth and VO results suffer from the inherent
scale ambiguity issue; 2) the BEV layout is directly predicted from the
front-view image without using any depth-related information, although the
depth map contains useful geometry clues for inferring scene layouts. In this
paper, we address these issues by proposing a novel joint perception framework
named JPerceiver, which can simultaneously estimate scale-aware depth and VO as
well as BEV layout from a monocular video sequence. It exploits the cross-view
geometric transformation (CGT) to propagate the absolute scale from the road
layout to depth and VO based on a carefully-designed scale loss. Meanwhile, a
cross-view and cross-modal transfer (CCT) module is devised to leverage the
depth clues for reasoning road and vehicle layout through an attention
mechanism. JPerceiver can be trained in an end-to-end multi-task learning way,
where the CGT scale loss and CCT module promote inter-task knowledge transfer
to benefit feature learning of each task. Experiments on Argoverse, Nuscenes
and KITTI show the superiority of JPerceiver over existing methods on all the
above three tasks in terms of accuracy, model size, and inference speed. The
code and models are available
at~\href{https://github.com/sunnyHelen/JPerceiver}{https://github.com/sunnyHelen/JPerceiver}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross Vision-RF Gait Re-identification with Low-cost RGB-D Cameras and mmWave Radars. (arXiv:2207.07896v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07896">
<div class="article-summary-box-inner">
<span><p>Human identification is a key requirement for many applications in everyday
life, such as personalized services, automatic surveillance, continuous
authentication, and contact tracing during pandemics, etc. This work studies
the problem of cross-modal human re-identification (ReID), in response to the
regular human movements across camera-allowed regions (e.g., streets) and
camera-restricted regions (e.g., offices) deployed with heterogeneous sensors.
By leveraging the emerging low-cost RGB-D cameras and mmWave radars, we propose
the first-of-its-kind vision-RF system for cross-modal multi-person ReID at the
same time. Firstly, to address the fundamental inter-modality discrepancy, we
propose a novel signature synthesis algorithm based on the observed specular
reflection model of a human body. Secondly, an effective cross-modal deep
metric learning model is introduced to deal with interference caused by
unsynchronized data across radars and cameras. Through extensive experiments in
both indoor and outdoor environments, we demonstrate that our proposed system
is able to achieve ~92.5% top-1 accuracy and ~97.5% top-5 accuracy out of 56
volunteers. We also show that our proposed system is able to robustly
reidentify subjects even when multiple subjects are present in the sensors'
field of view.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPSN: Superpixel Prototype Sampling Network for RGB-D Salient Object Detection. (arXiv:2207.07898v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07898">
<div class="article-summary-box-inner">
<span><p>RGB-D salient object detection (SOD) has been in the spotlight recently
because it is an important preprocessing operation for various vision tasks.
However, despite advances in deep learning-based methods, RGB-D SOD is still
challenging due to the large domain gap between an RGB image and the depth map
and low-quality depth maps. To solve this problem, we propose a novel
superpixel prototype sampling network (SPSN) architecture. The proposed model
splits the input RGB image and depth map into component superpixels to generate
component prototypes. We design a prototype sampling network so that the
network only samples prototypes corresponding to salient objects. In addition,
we propose a reliance selection module to recognize the quality of each RGB and
depth feature map and adaptively weight them in proportion to their
reliability. The proposed method makes the model robust to inconsistencies
between RGB images and depth maps and eliminates the influence of non-salient
objects. Our method is evaluated on five popular datasets, achieving
state-of-the-art performance. We prove the effectiveness of the proposed method
through comparative experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mutual Adaptive Reasoning for Monocular 3D Multi-Person Pose Estimation. (arXiv:2207.07900v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07900">
<div class="article-summary-box-inner">
<span><p>Inter-person occlusion and depth ambiguity make estimating the 3D poses of
monocular multiple persons as camera-centric coordinates a challenging problem.
Typical top-down frameworks suffer from high computational redundancy with an
additional detection stage. By contrast, the bottom-up methods enjoy low
computational costs as they are less affected by the number of humans. However,
most existing bottom-up methods treat camera-centric 3D human pose estimation
as two unrelated subtasks: 2.5D pose estimation and camera-centric depth
estimation. In this paper, we propose a unified model that leverages the mutual
benefits of both these subtasks. Within the framework, a robust structured 2.5D
pose estimation is designed to recognize inter-person occlusion based on depth
relationships. Additionally, we develop an end-to-end geometry-aware depth
reasoning method that exploits the mutual benefits of both 2.5D pose and
camera-centric root depths. This method first uses 2.5D pose and geometry
information to infer camera-centric root depths in a forward pass, and then
exploits the root depths to further improve representation learning of 2.5D
pose estimation in a backward pass. Further, we designed an adaptive fusion
scheme that leverages both visual perception and body geometry to alleviate
inherent depth ambiguity issues. Extensive experiments demonstrate the
superiority of our proposed model over a wide range of bottom-up methods. Our
accuracy is even competitive with top-down counterparts. Notably, our model
runs much faster than existing bottom-up and top-down methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual-branch Hybrid Learning Network for Unbiased Scene Graph Generation. (arXiv:2207.07913v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07913">
<div class="article-summary-box-inner">
<span><p>The current studies of Scene Graph Generation (SGG) focus on solving the
long-tailed problem for generating unbiased scene graphs. However, most
de-biasing methods overemphasize the tail predicates and underestimate head
ones throughout training, thereby wrecking the representation ability of head
predicate features. Furthermore, these impaired features from head predicates
harm the learning of tail predicates. In fact, the inference of tail predicates
heavily depends on the general patterns learned from head ones, e.g., "standing
on" depends on "on". Thus, these de-biasing SGG methods can neither achieve
excellent performance on tail predicates nor satisfying behaviors on head ones.
To address this issue, we propose a Dual-branch Hybrid Learning network (DHL)
to take care of both head predicates and tail ones for SGG, including a
Coarse-grained Learning Branch (CLB) and a Fine-grained Learning Branch (FLB).
Specifically, the CLB is responsible for learning expertise and robust features
of head predicates, while the FLB is expected to predict informative tail
predicates. Furthermore, DHL is equipped with a Branch Curriculum Schedule
(BCS) to make the two branches work well together. Experiments show that our
approach achieves a new state-of-the-art performance on VG and GQA datasets and
makes a trade-off between the performance of tail predicates and head ones.
Moreover, extensive experiments on two downstream tasks (i.e., Image Captioning
and Sentence-to-Graph Retrieval) further verify the generalization and
practicability of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discriminative Kernel Convolution Network for Multi-Label Ophthalmic Disease Detection on Imbalanced Fundus Image Dataset. (arXiv:2207.07918v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07918">
<div class="article-summary-box-inner">
<span><p>It is feasible to recognize the presence and seriousness of eye disease by
investigating the progressions in retinal biological structure. Fundus
examination is a diagnostic procedure to examine the biological structure and
anomaly of the eye. Ophthalmic diseases like glaucoma, diabetic retinopathy,
and cataract are the main reason for visual impairment around the world. Ocular
Disease Intelligent Recognition (ODIR-5K) is a benchmark structured fundus
image dataset utilized by researchers for multi-label multi-disease
classification of fundus images. This work presents a discriminative kernel
convolution network (DKCNet), which explores discriminative region-wise
features without adding extra computational cost. DKCNet is composed of an
attention block followed by a squeeze and excitation (SE) block. The attention
block takes features from the backbone network and generates discriminative
feature attention maps. The SE block takes the discriminative feature maps and
improves channel interdependencies. Better performance of DKCNet is observed
with InceptionResnet backbone network for multi-label classification of ODIR-5K
fundus images with 96.08 AUC, 94.28 F1-score and 0.81 kappa score. The proposed
method splits the common target label for an eye pair based on the diagnostic
keyword. Based on these labels oversampling and undersampling is done to
resolve class imbalance. To check the biasness of proposed model towards
training data, the model trained on ODIR dataset is tested on three publicly
available benchmark datasets. It is found to give good performance on
completely unseen fundus images also.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explainable vision transformer enabled convolutional neural network for plant disease identification: PlantXViT. (arXiv:2207.07919v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07919">
<div class="article-summary-box-inner">
<span><p>Plant diseases are the primary cause of crop losses globally, with an impact
on the world economy. To deal with these issues, smart agriculture solutions
are evolving that combine the Internet of Things and machine learning for early
disease detection and control. Many such systems use vision-based machine
learning methods for real-time disease detection and diagnosis. With the
advancement in deep learning techniques, new methods have emerged that employ
convolutional neural networks for plant disease detection and identification.
Another trend in vision-based deep learning is the use of vision transformers,
which have proved to be powerful models for classification and other problems.
However, vision transformers have rarely been investigated for plant pathology
applications. In this study, a Vision Transformer enabled Convolutional Neural
Network model called "PlantXViT" is proposed for plant disease identification.
The proposed model combines the capabilities of traditional convolutional
neural networks with the Vision Transformers to efficiently identify a large
number of plant diseases for several crops. The proposed model has a
lightweight structure with only 0.8 million trainable parameters, which makes
it suitable for IoT-based smart agriculture services. The performance of
PlantXViT is evaluated on five publicly available datasets. The proposed
PlantXViT network performs better than five state-of-the-art methods on all
five datasets. The average accuracy for recognising plant diseases is shown to
exceed 93.55%, 92.59%, and 98.33% on Apple, Maize, and Rice datasets,
respectively, even under challenging background conditions. The efficiency in
terms of explainability of the proposed model is evaluated using
gradient-weighted class activation maps and Local Interpretable Model Agnostic
Explanation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CNN-based Euler's Elastica Inpainting with Deep Energy and Deep Image Prior. (arXiv:2207.07921v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07921">
<div class="article-summary-box-inner">
<span><p>Euler's elastica constitute an appealing variational image inpainting model.
It minimises an energy that involves the total variation as well as the level
line curvature. These components are transparent and make it attractive for
shape completion tasks. However, its gradient flow is a singular, anisotropic,
and nonlinear PDE of fourth order, which is numerically challenging: It is
difficult to find efficient algorithms that offer sharp edges and good rotation
invariance. As a remedy, we design the first neural algorithm that simulates
inpainting with Euler's Elastica. We use the deep energy concept which employs
the variational energy as neural network loss. Furthermore, we pair it with a
deep image prior where the network architecture itself acts as a prior. This
yields better inpaintings by steering the optimisation trajectory closer to the
desired solution. Our results are qualitatively on par with state-of-the-art
algorithms on elastica-based shape completion. They combine good rotation
invariance with sharp edges. Moreover, we benefit from the high efficiency and
effortless parallelisation within a neural framework. Our neural elastica
approach only requires 3x3 central difference stencils. It is thus much simpler
than other well-performing algorithms for elastica inpainting. Last but not
least, it is unsupervised as it requires no ground truth training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Quality-aware Dynamic Memory for Video Object Segmentation. (arXiv:2207.07922v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07922">
<div class="article-summary-box-inner">
<span><p>Recently, several spatial-temporal memory-based methods have verified that
storing intermediate frames and their masks as memory are helpful to segment
target objects in videos. However, they mainly focus on better matching between
the current frame and the memory frames without explicitly paying attention to
the quality of the memory. Therefore, frames with poor segmentation masks are
prone to be memorized, which leads to a segmentation mask error accumulation
problem and further affect the segmentation performance. In addition, the
linear increase of memory frames with the growth of frame number also limits
the ability of the models to handle long videos. To this end, we propose a
Quality-aware Dynamic Memory Network (QDMN) to evaluate the segmentation
quality of each frame, allowing the memory bank to selectively store accurately
segmented frames to prevent the error accumulation problem. Then, we combine
the segmentation quality with temporal consistency to dynamically update the
memory bank to improve the practicability of the models. Without any bells and
whistles, our QDMN achieves new state-of-the-art performance on both DAVIS and
YouTube-VOS benchmarks. Moreover, extensive experiments demonstrate that the
proposed Quality Assessment Module (QAM) can be applied to memory-based methods
as generic plugins and significantly improves performance. Our source code is
available at https://github.com/workforai/QDMN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Lightweight Super-Resolution with Dual Regression Learning. (arXiv:2207.07929v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07929">
<div class="article-summary-box-inner">
<span><p>Deep neural networks have exhibited remarkable performance in image
super-resolution (SR) tasks by learning a mapping from low-resolution (LR)
images to high-resolution (HR) images. However, the SR problem is typically an
ill-posed problem and existing methods would come with several limitations.
First, the possible mapping space of SR can be extremely large since there may
exist many different HR images that can be downsampled to the same LR image. As
a result, it is hard to directly learn a promising SR mapping from such a large
space. Second, it is often inevitable to develop very large models with
extremely high computational cost to yield promising SR performance. In
practice, one can use model compression techniques to obtain compact models by
reducing model redundancy. Nevertheless, it is hard for existing model
compression methods to accurately identify the redundant components due to the
extremely large SR mapping space. To alleviate the first challenge, we propose
a dual regression learning scheme to reduce the space of possible SR mappings.
Specifically, in addition to the mapping from LR to HR images, we learn an
additional dual regression mapping to estimate the downsampling kernel and
reconstruct LR images. In this way, the dual mapping acts as a constraint to
reduce the space of possible mappings. To address the second challenge, we
propose a lightweight dual regression compression method to reduce model
redundancy in both layer-level and channel-level based on channel pruning.
Specifically, we first develop a channel number search method that minimizes
the dual regression loss to determine the redundancy of each layer. Given the
searched channel numbers, we further exploit the dual regression manner to
evaluate the importance of channels and prune the redundant ones. Extensive
experiments show the effectiveness of our method in obtaining accurate and
efficient SR models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Supervised Keypoint Detector and Descriptor for Retinal Image Matching. (arXiv:2207.07932v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07932">
<div class="article-summary-box-inner">
<span><p>For retinal image matching (RIM), we propose SuperRetina, the first
end-to-end method with jointly trainable keypoint detector and descriptor.
SuperRetina is trained in a novel semi-supervised manner. A small set of
(nearly 100) images are incompletely labeled and used to supervise the network
to detect keypoints on the vascular tree. To attack the incompleteness of
manual labeling, we propose Progressive Keypoint Expansion to enrich the
keypoint labels at each training epoch. By utilizing a keypoint-based improved
triplet loss as its description loss, SuperRetina produces highly
discriminative descriptors at full input image size. Extensive experiments on
multiple real-world datasets justify the viability of SuperRetina. Even with
manual labeling replaced by auto labeling and thus making the training process
fully manual-annotation free, SuperRetina compares favorably against a number
of strong baselines for two RIM tasks, i.e. image registration and identity
verification. SuperRetina will be open source.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Consistency of Implicit and Explicit Features Matters for Monocular 3D Object Detection. (arXiv:2207.07933v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07933">
<div class="article-summary-box-inner">
<span><p>Monocular 3D object detection is a common solution for low-cost autonomous
agents to perceive their surrounding environment. Monocular detection has
progressed into two categories: (1)Direct methods that infer 3D bounding boxes
directly from a frontal-view image; (2)3D intermedia representation methods
that map image features to 3D space for subsequent 3D detection. The second
category is standing out not only because 3D detection forges ahead at the
mercy of more meaningful and representative features, but because of emerging
SOTA end-to-end prediction and planning paradigms that require a
bird's-eye-view feature map from a perception pipeline. However, in
transforming to 3D representation, these methods do not guarantee that objects'
implicit orientations and locations in latent space are consistent with those
explicitly observed in Euclidean space, which will hurt model performance.
Hence, we argue that the consistency of implicit and explicit features matters
and present a novel monocular detection method, named CIEF, with the first
orientation-aware image backbone to eliminate the disparity of implicit and
explicit features in subsequent 3D representation. As a second contribution, we
introduce a ray attention mechanism. In contrast to previous methods that
repeat features along the projection ray or rely on another intermedia frustum
point cloud, we directly transform image features to voxel representations with
well-localized features. We also propose a handcrafted gaussian positional
encoding function that outperforms the sinusoidal encoding function but
maintains the benefit of being continuous. CIEF ranked 1st among all reported
methods on both 3D and BEV detection benchmark of KITTI at submission time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stochastic Attribute Modeling for Face Super-Resolution. (arXiv:2207.07945v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07945">
<div class="article-summary-box-inner">
<span><p>When a high-resolution (HR) image is degraded into a low-resolution (LR)
image, the image loses some of the existing information. Consequently, multiple
HR images can correspond to the LR image. Most of the existing methods do not
consider the uncertainty caused by the stochastic attribute, which can only be
probabilistically inferred. Therefore, the predicted HR images are often blurry
because the network tries to reflect all possibilities in a single output
image. To overcome this limitation, this paper proposes a novel face
super-resolution (SR) scheme to take into the uncertainty by stochastic
modeling. Specifically, the information in LR images is separately encoded into
deterministic and stochastic attributes. Furthermore, an Input Conditional
Attribute Predictor is proposed and separately trained to predict the partially
alive stochastic attributes from only the LR images. Extensive evaluation shows
that the proposed method successfully reduces the uncertainty in the learning
process and outperforms the existing state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Level Set-Based Camera Pose Estimation From Multiple 2D/3D Ellipse-Ellipsoid Correspondences. (arXiv:2207.07953v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07953">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose an object-based camera pose estimation from a
single RGB image and a pre-built map of objects, represented with ellipsoidal
models. We show that contrary to point correspondences, the definition of a
cost function characterizing the projection of a 3D object onto a 2D object
detection is not straightforward. We develop an ellipse-ellipse cost based on
level sets sampling, demonstrate its nice properties for handling partially
visible objects and compare its performance with other common metrics. Finally,
we show that the use of a predictive uncertainty on the detected ellipses
allows a fair weighting of the contribution of the correspondences which
improves the computed pose. The code is released at
https://gitlab.inria.fr/tangram/level-set-based-camera-pose-estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learn-to-Decompose: Cascaded Decomposition Network for Cross-Domain Few-Shot Facial Expression Recognition. (arXiv:2207.07973v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07973">
<div class="article-summary-box-inner">
<span><p>Most existing compound facial expression recognition (FER) methods rely on
large-scale labeled compound expression data for training. However, collecting
such data is labor-intensive and time-consuming. In this paper, we address the
compound FER task in the cross-domain few-shot learning (FSL) setting, which
requires only a few samples of compound expressions in the target domain.
Specifically, we propose a novel cascaded decomposition network (CDNet), which
cascades several learn-to-decompose modules with shared parameters based on a
sequential decomposition mechanism, to obtain a transferable feature space. To
alleviate the overfitting problem caused by limited base classes in our task, a
partial regularization strategy is designed to effectively exploit the best of
both episodic training and batch training. By training across similar tasks on
multiple basic expression datasets, CDNet learns the ability of
learn-to-decompose that can be easily adapted to identify unseen compound
expressions. Extensive experiments on both in-the-lab and in-the-wild compound
expression datasets demonstrate the superiority of our proposed CDNet against
several state-of-the-art FSL methods. Code is available at:
https://github.com/zouxinyi0625/CDNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Guided Bidirectional Attention Network for Human-Object Interaction Detection. (arXiv:2207.07979v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07979">
<div class="article-summary-box-inner">
<span><p>Human Object Interaction (HOI) detection is a challenging task that requires
to distinguish the interaction between a human-object pair. Attention based
relation parsing is a popular and effective strategy utilized in HOI. However,
current methods execute relation parsing in a "bottom-up" manner. We argue that
the independent use of the bottom-up parsing strategy in HOI is
counter-intuitive and could lead to the diffusion of attention. Therefore, we
introduce a novel knowledge-guided top-down attention into HOI, and propose to
model the relation parsing as a "look and search" process: execute
scene-context modeling (i.e. look), and then, given the knowledge of the target
pair, search visual clues for the discrimination of the interaction between the
pair. We implement the process via unifying the bottom-up and top-down
attention in a single encoder-decoder based model. The experimental results
show that our model achieves competitive performance on the V-COCO and HICO-DET
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiffuStereo: High Quality Human Reconstruction via Diffusion-based Stereo Using Sparse Cameras. (arXiv:2207.08000v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08000">
<div class="article-summary-box-inner">
<span><p>We propose DiffuStereo, a novel system using only sparse cameras (8 in this
work) for high-quality 3D human reconstruction. At its core is a novel
diffusion-based stereo module, which introduces diffusion models, a type of
powerful generative models, into the iterative stereo matching network. To this
end, we design a new diffusion kernel and additional stereo constraints to
facilitate stereo matching and depth estimation in the network. We further
present a multi-level stereo network architecture to handle high-resolution (up
to 4k) inputs without requiring unaffordable memory footprint. Given a set of
sparse-view color images of a human, the proposed multi-level diffusion-based
stereo network can produce highly accurate depth maps, which are then converted
into a high-quality 3D human model through an efficient multi-view fusion
strategy. Overall, our method enables automatic reconstruction of human models
with quality on par to high-end dense-view camera rigs, and this is achieved
using a much more light-weight hardware setup. Experiments show that our method
outperforms state-of-the-art methods by a large margin both qualitatively and
quantitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SVGraph: Learning Semantic Graphs from Instructional Videos. (arXiv:2207.08001v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08001">
<div class="article-summary-box-inner">
<span><p>In this work, we focus on generating graphical representations of noisy,
instructional videos for video understanding. We propose a self-supervised,
interpretable approach that does not require any annotations for graphical
representations, which would be expensive and time consuming to collect. We
attempt to overcome "black box" learning limitations by presenting Semantic
Video Graph or SVGraph, a multi-modal approach that utilizes narrations for
semantic interpretability of the learned graphs. SVGraph 1) relies on the
agreement between multiple modalities to learn a unified graphical structure
with the help of cross-modal attention and 2) assigns semantic interpretation
with the help of Semantic-Assignment, which captures the semantics from video
narration. We perform experiments on multiple datasets and demonstrate the
interpretability of SVGraph in semantic graph learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SSMTL++: Revisiting Self-Supervised Multi-Task Learning for Video Anomaly Detection. (arXiv:2207.08003v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08003">
<div class="article-summary-box-inner">
<span><p>A self-supervised multi-task learning (SSMTL) framework for video anomaly
detection was recently introduced in literature. Due to its highly accurate
results, the method attracted the attention of many researchers. In this work,
we revisit the self-supervised multi-task learning framework, proposing several
updates to the original method. First, we study various detection methods, e.g.
based on detecting high-motion regions using optical flow or background
subtraction, since we believe the currently used pre-trained YOLOv3 is
suboptimal, e.g. objects in motion or objects from unknown classes are never
detected. Second, we modernize the 3D convolutional backbone by introducing
multi-head self-attention modules, inspired by the recent success of vision
transformers. As such, we alternatively introduce both 2D and 3D convolutional
vision transformer (CvT) blocks. Third, in our attempt to further improve the
model, we study additional self-supervised learning tasks, such as predicting
segmentation maps through knowledge distillation, solving jigsaw puzzles,
estimating body pose through knowledge distillation, predicting masked regions
(inpainting), and adversarial learning with pseudo-anomalies. We conduct
experiments to assess the performance impact of the introduced changes. Upon
finding more promising configurations of the framework, dubbed SSMTL++v1 and
SSMTL++v2, we extend our preliminary experiments to more data sets,
demonstrating that our performance gains are consistent across all data sets.
In most cases, our results on Avenue, ShanghaiTech and UBnormal raise the
state-of-the-art performance to a new level.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Monitoring Vegetation From Space at Extremely Fine Resolutions via Coarsely-Supervised Smooth U-Net. (arXiv:2207.08022v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08022">
<div class="article-summary-box-inner">
<span><p>Monitoring vegetation productivity at extremely fine resolutions is valuable
for real-world agricultural applications, such as detecting crop stress and
providing early warning of food insecurity. Solar-Induced Chlorophyll
Fluorescence (SIF) provides a promising way to directly measure plant
productivity from space. However, satellite SIF observations are only available
at a coarse spatial resolution, making it impossible to monitor how individual
crop types or farms are doing. This poses a challenging coarsely-supervised
regression (or downscaling) task; at training time, we only have SIF labels at
a coarse resolution (3km), but we want to predict SIF at much finer spatial
resolutions (e.g. 30m, a 100x increase). We also have additional
fine-resolution input features, but the relationship between these features and
SIF is unknown. To address this, we propose Coarsely-Supervised Smooth U-Net
(CS-SUNet), a novel method for this coarse supervision setting. CS-SUNet
combines the expressive power of deep convolutional networks with novel
regularization methods based on prior knowledge (such as a smoothness loss)
that are crucial for preventing overfitting. Experiments show that CS-SUNet
resolves fine-grained variations in SIF more accurately than existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LAVA: Language Audio Vision Alignment for Contrastive Video Pre-Training. (arXiv:2207.08024v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08024">
<div class="article-summary-box-inner">
<span><p>Generating representations of video data is of key importance in advancing
the field of machine perception. Most current techniques rely on hand-annotated
data, which can be difficult to work with, expensive to generate, and hard to
scale. In this work, we propose a novel learning approach based on contrastive
learning, LAVA, which is capable of learning joint language, audio, and video
representations in a self-supervised manner. We pre-train LAVA on the Kinetics
700 dataset using transformer encoders to learn representations for each
modality. We then demonstrate that LAVA performs competitively with the current
state-of-the-art self-supervised and weakly-supervised pretraining techniques
on UCF-101 and HMDB-51 video action recognition while using a fraction of the
unlabeled data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analysis of liver cancer detection based on image processing. (arXiv:2207.08032v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08032">
<div class="article-summary-box-inner">
<span><p>Medical imaging is the most important tool for detecting complications in the
inner body of medicine. Nowadays, with the development of image processing
technology as well as changing the size of photos to higher resolution images
in the field of digital medical imaging, there is an efficient and accurate
system for segmenting this. Real-world images that for a variety of reasons
have poor heterogeneity, noise and contrast are essential. Digital image
segmentation in medicine is used for diagnostic and therapeutic analysis, which
is very helpful for physicians. In this study, we aim at liver cancer
photographs, which aim to more accurately detect the lesion or tumor of the
liver because accurate and timely detection of the tumor is very important in
the survival and life of the patient.The aim of this paper is to simplify the
obnoxious study problems related to the study of MR images. The liver is the
second organ most generic involved by metastatic disease being liver cancer one
of the prominent causes of death worldwide. Without healthy liver a person
cannot survive. It is life threatening disease which is very challenging
perceptible for both medical and engineering technologists. Medical image
processing is used as a non-invasive method to detect tumours. The chances of
survival having liver Tumor highly depends on early detection of Tumor and then
classification as cancerous and noncancerous tumours. Image processing
techniques for automatic detection of brain are includes pre-processing and
enhancement, image segmentation, classification and volume calculation, Poly
techniques have been developed for the detection of liver Tumor and different
liver toM oR detection algorithms and methodologies utilized for Tumor
diagnosis. Novel methodology for the detection and diagnosis of liver Tumor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Progress and limitations of deep networks to recognize objects in unusual poses. (arXiv:2207.08034v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08034">
<div class="article-summary-box-inner">
<span><p>Deep networks should be robust to rare events if they are to be successfully
deployed in high-stakes real-world applications (e.g., self-driving cars). Here
we study the capability of deep networks to recognize objects in unusual poses.
We create a synthetic dataset of images of objects in unusual orientations, and
evaluate the robustness of a collection of 38 recent and competitive deep
networks for image classification. We show that classifying these images is
still a challenge for all networks tested, with an average accuracy drop of
29.5% compared to when the objects are presented upright. This brittleness is
largely unaffected by various network design choices, such as training losses
(e.g., supervised vs. self-supervised), architectures (e.g., convolutional
networks vs. transformers), dataset modalities (e.g., images vs. image-text
pairs), and data-augmentation schemes. However, networks trained on very large
datasets substantially outperform others, with the best network
tested$\unicode{x2014}$Noisy Student EfficentNet-L2 trained on
JFT-300M$\unicode{x2014}$showing a relatively small accuracy drop of only 14.5%
on unusual poses. Nevertheless, a visual inspection of the failures of Noisy
Student reveals a remaining gap in robustness with the human visual system.
Furthermore, combining multiple object
transformations$\unicode{x2014}$3D-rotations and
scaling$\unicode{x2014}$further degrades the performance of all networks.
Altogether, our results provide another measurement of the robustness of deep
networks that is important to consider when using them in the real world. Code
and datasets are available at https://github.com/amro-kamal/ObjectPose.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Single MR Image Super-Resolution using Generative Adversarial Network. (arXiv:2207.08036v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08036">
<div class="article-summary-box-inner">
<span><p>Spatial resolution of medical images can be improved using super-resolution
methods. Real Enhanced Super Resolution Generative Adversarial Network
(Real-ESRGAN) is one of the recent effective approaches utilized to produce
higher resolution images, given input images of lower resolution. In this
paper, we apply this method to enhance the spatial resolution of 2D MR images.
In our proposed approach, we slightly modify the structure of the Real-ESRGAN
to train 2D Magnetic Resonance images (MRI) taken from the Brain Tumor
Segmentation Challenge (BraTS) 2018 dataset. The obtained results are validated
qualitatively and quantitatively by computing SSIM (Structural Similarity Index
Measure), NRMSE (Normalized Root Mean Square Error), MAE (Mean Absolute Error),
and VIF (Visual Information Fidelity) values.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DIMBA: Discretely Masked Black-Box Attack in Single Object Tracking. (arXiv:2207.08044v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08044">
<div class="article-summary-box-inner">
<span><p>The adversarial attack can force a CNN-based model to produce an incorrect
output by craftily manipulating human-imperceptible input. Exploring such
perturbations can help us gain a deeper understanding of the vulnerability of
neural networks, and provide robustness to deep learning against miscellaneous
adversaries. Despite extensive studies focusing on the robustness of image,
audio, and NLP, works on adversarial examples of visual object tracking --
especially in a black-box manner -- are quite lacking. In this paper, we
propose a novel adversarial attack method to generate noises for single object
tracking under black-box settings, where perturbations are merely added on
initial frames of tracking sequences, which is difficult to be noticed from the
perspective of a whole video clip. Specifically, we divide our algorithm into
three components and exploit reinforcement learning for localizing important
frame patches precisely while reducing unnecessary computational queries
overhead. Compared to existing techniques, our method requires fewer queries on
initialized frames of a video to manipulate competitive or even better attack
performance. We test our algorithm in both long-term and short-term datasets,
including OTB100, VOT2018, UAV123, and LaSOT. Extensive experiments demonstrate
the effectiveness of our method on three mainstream types of trackers:
discrimination, Siamese-based, and reinforcement learning-based trackers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MDM:Visual Explanations for Neural Networks via Multiple Dynamic Mask. (arXiv:2207.08046v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08046">
<div class="article-summary-box-inner">
<span><p>The active region lookup of a neural network tells us which regions the
neural network focuses on when making a decision, which gives us a basis for
interpretability when the neural network makes a classification decision. We
propose an algorithm Multiple Dynamic Mask(MDM), which is a general saliency
graph query method with interpretability of the inference process. Its proposal
is based on an assumption: when a picture is input to a neural network that has
been trained, the activation features related to classification will affect the
classification results of the neural network, and the features unrelated to
classification will hardly affect the classification results of the network.
MDM: A learning-based end-to-end algorithm for finding regions of interest for
neural network classification. It has the following advantages: 1. It has the
interpretability of the reasoning process. 2. It is universal, it can be used
for any neural network and does not depend on the internal structure of the
neural network. 3. The search performance is better. Because the algorithm is
based on learning to generate masks and has the ability to adapt to different
data and networks, the performance is better than the method proposed in the
previous paper. For the MDM saliency map search algorithm, we experimentally
compared the performance indicators of various saliency map search methods and
the MDM with ResNet and DenseNet as the trained neural networks. The search
effect performance of the MDM reached the state of the art. We applied the MDM
to the interpretable neural network ProtoPNet and XProtoNet, which improved the
interpretability of the model and the prototype search performance. We
visualize the performance of convolutional neural architecture and Transformer
architecture on saliency map search.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SatMAE: Pre-training Transformers for Temporal and Multi-Spectral Satellite Imagery. (arXiv:2207.08051v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08051">
<div class="article-summary-box-inner">
<span><p>Unsupervised pre-training methods for large vision models have shown to
enhance performance on downstream supervised tasks. Developing similar
techniques for satellite imagery presents significant opportunities as
unlabelled data is plentiful and the inherent temporal and multi-spectral
structure provides avenues to further improve existing pre-training strategies.
In this paper, we present SatMAE, a pre-training framework for temporal or
multi-spectral satellite imagery based on Masked Autoencoder (MAE). To leverage
temporal information, we include a temporal embedding along with independently
masking image patches across time. In addition, we demonstrate that encoding
multi-spectral data as groups of bands with distinct spectral positional
encodings is beneficial. Our approach yields strong improvements over previous
state-of-the-art techniques, both in terms of supervised learning performance
on benchmark datasets (up to $\uparrow$ 7\%), and transfer learning performance
on downstream remote sensing tasks, including land cover classification (up to
$\uparrow$ 14\%) and semantic segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Humans in RGB-D Data with CNNs. (arXiv:2207.08064v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08064">
<div class="article-summary-box-inner">
<span><p>We address the problem of people detection in RGB-D data where we leverage
depth information to develop a region-of-interest (ROI) selection method that
provides proposals to two color and depth CNNs. To combine the detections
produced by the two CNNs, we propose a novel fusion approach based on the
characteristics of depth images. We also present a new depth-encoding scheme,
which not only encodes depth images into three channels but also enhances the
information for classification. We conduct experiments on a publicly available
RGB-D people dataset and show that our approach outperforms the baseline models
that only use RGB data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effect of Instance Normalization on Fine-Grained Control for Sketch-Based Face Image Generation. (arXiv:2207.08072v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08072">
<div class="article-summary-box-inner">
<span><p>Sketching is an intuitive and effective way for content creation. While
significant progress has been made for photorealistic image generation by using
generative adversarial networks, it remains challenging to take a fine-grained
control on synthetic content. The instance normalization layer, which is widely
adopted in existing image translation networks, washes away details in the
input sketch and leads to loss of precise control on the desired shape of the
generated face images. In this paper, we comprehensively investigate the effect
of instance normalization on generating photorealistic face images from
hand-drawn sketches. We first introduce a visualization approach to analyze the
feature embedding for sketches with a group of specific changes. Based on the
visual analysis, we modify the instance normalization layers in the baseline
image translation model. We elaborate a new set of hand-drawn sketches with 11
categories of specially designed changes and conduct extensive experimental
analysis. The results and user studies demonstrate that our method markedly
improve the quality of synthesized images and the conformance with user
intention.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Performance degradation of ImageNet trained models by simple image transformations. (arXiv:2207.08079v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08079">
<div class="article-summary-box-inner">
<span><p>ImageNet trained PyTorch models are generally preferred as the off-the-shelf
models for direct use or for initialisation in most computer vision tasks. In
this paper, we simply test a representative set of these convolution and
transformer based models under many simple image transformations like
horizontal shifting, vertical shifting, scaling, rotation, presence of Gaussian
noise, cutout, horizontal flip and vertical flip and report the performance
drop caused by such transformations. We find that even simple transformations
like rotating the image by 10{\deg} or zooming in by 20% can reduce the top-1
accuracy of models like ResNet152 by 1%+. The code is available at
https://github.com/harshm121/imagenet-transformation-degradation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Color Operators for Sequential Image Retouching. (arXiv:2207.08080v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08080">
<div class="article-summary-box-inner">
<span><p>We propose a novel image retouching method by modeling the retouching process
as performing a sequence of newly introduced trainable neural color operators.
The neural color operator mimics the behavior of traditional color operators
and learns pixelwise color transformation while its strength is controlled by a
scalar. To reflect the homomorphism property of color operators, we employ
equivariant mapping and adopt an encoder-decoder structure which maps the
non-linear color transformation to a much simpler transformation (i.e.,
translation) in a high dimensional space. The scalar strength of each neural
color operator is predicted using CNN based strength predictors by analyzing
global image statistics. Overall, our method is rather lightweight and offers
flexible controls. Experiments and user studies on public datasets show that
our method consistently achieves the best results compared with SOTA methods in
both quantitative measures and visual qualities. The code and data will be made
publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CATRE: Iterative Point Clouds Alignment for Category-level Object Pose Refinement. (arXiv:2207.08082v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08082">
<div class="article-summary-box-inner">
<span><p>While category-level 9DoF object pose estimation has emerged recently,
previous correspondence-based or direct regression methods are both limited in
accuracy due to the huge intra-category variances in object shape and color,
etc. Orthogonal to them, this work presents a category-level object pose and
size refiner CATRE, which is able to iteratively enhance pose estimate from
point clouds to produce accurate results. Given an initial pose estimate, CATRE
predicts a relative transformation between the initial pose and ground truth by
means of aligning the partially observed point cloud and an abstract shape
prior. In specific, we propose a novel disentangled architecture being aware of
the inherent distinctions between rotation and translation/size estimation.
Extensive experiments show that our approach remarkably outperforms
state-of-the-art methods on REAL275, CAMERA25, and LM benchmarks up to a speed
of ~85.32Hz, and achieves competitive results on category-level tracking. We
further demonstrate that CATRE can perform pose refinement on unseen category.
Code and trained models are available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Threat Model-Agnostic Adversarial Defense using Diffusion Models. (arXiv:2207.08089v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08089">
<div class="article-summary-box-inner">
<span><p>Deep Neural Networks (DNNs) are highly sensitive to imperceptible malicious
perturbations, known as adversarial attacks. Following the discovery of this
vulnerability in real-world imaging and vision applications, the associated
safety concerns have attracted vast research attention, and many defense
techniques have been developed. Most of these defense methods rely on
adversarial training (AT) -- training the classification network on images
perturbed according to a specific threat model, which defines the magnitude of
the allowed modification. Although AT leads to promising results, training on a
specific threat model fails to generalize to other types of perturbations. A
different approach utilizes a preprocessing step to remove the adversarial
perturbation from the attacked image. In this work, we follow the latter path
and aim to develop a technique that leads to robust classifiers across various
realizations of threat models. To this end, we harness the recent advances in
stochastic generative modeling, and means to leverage these for sampling from
conditional distributions. Our defense relies on an addition of Gaussian i.i.d
noise to the attacked image, followed by a pretrained diffusion process -- an
architecture that performs a stochastic iterative process over a denoising
network, yielding a high perceptual quality denoised outcome. The obtained
robustness with this stochastic preprocessing step is validated through
extensive experiments on the CIFAR-10 dataset, showing that our method
outperforms the leading defense methods under various threat models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning from Temporal Spatial Cubism for Cross-Dataset Skeleton-based Action Recognition. (arXiv:2207.08095v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08095">
<div class="article-summary-box-inner">
<span><p>Rapid progress and superior performance have been achieved for skeleton-based
action recognition recently. In this article, we investigate this problem under
a cross-dataset setting, which is a new, pragmatic, and challenging task in
real-world scenarios. Following the unsupervised domain adaptation (UDA)
paradigm, the action labels are only available on a source dataset, but
unavailable on a target dataset in the training stage. Different from the
conventional adversarial learning-based approaches for UDA, we utilize a
self-supervision scheme to reduce the domain shift between two skeleton-based
action datasets. Our inspiration is drawn from Cubism, an art genre from the
early 20th century, which breaks and reassembles the objects to convey a
greater context. By segmenting and permuting temporal segments or human body
parts, we design two self-supervised learning classification tasks to explore
the temporal and spatial dependency of a skeleton-based action and improve the
generalization ability of the model. We conduct experiments on six datasets for
skeleton-based action recognition, including three large-scale datasets (NTU
RGB+D, PKU-MMD, and Kinetics) where new cross-dataset settings and benchmarks
are established. Extensive results demonstrate that our method outperforms
state-of-the-art approaches. The source codes of our model and all the compared
methods are available at https://github.com/shanice-l/st-cubism.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BCS-Net: Boundary, Context and Semantic for Automatic COVID-19 Lung Infection Segmentation from CT Images. (arXiv:2207.08114v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08114">
<div class="article-summary-box-inner">
<span><p>The spread of COVID-19 has brought a huge disaster to the world, and the
automatic segmentation of infection regions can help doctors to make diagnosis
quickly and reduce workload. However, there are several challenges for the
accurate and complete segmentation, such as the scattered infection area
distribution, complex background noises, and blurred segmentation boundaries.
To this end, in this paper, we propose a novel network for automatic COVID-19
lung infection segmentation from CT images, named BCS-Net, which considers the
boundary, context, and semantic attributes. The BCS-Net follows an
encoder-decoder architecture, and more designs focus on the decoder stage that
includes three progressively Boundary-Context-Semantic Reconstruction (BCSR)
blocks. In each BCSR block, the attention-guided global context (AGGC) module
is designed to learn the most valuable encoder features for decoder by
highlighting the important spatial and boundary locations and modeling the
global context dependence. Besides, a semantic guidance (SG) unit generates the
semantic guidance map to refine the decoder features by aggregating multi-scale
high-level features at the intermediate resolution. Extensive experiments
demonstrate that our proposed framework outperforms the existing competitors
both qualitatively and quantitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FloLPIPS: A Bespoke Video Quality Metric for Frame Interpoation. (arXiv:2207.08119v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08119">
<div class="article-summary-box-inner">
<span><p>Video frame interpolation (VFI) serves as a useful tool for many video
processing applications. Recently, it has also been applied in the video
compression domain for enhancing both conventional video codecs and
learning-based compression architectures. While there has been an increased
focus on the development of enhanced frame interpolation algorithms in recent
years, the perceptual quality assessment of interpolated content remains an
open field of research. In this paper, we present a bespoke full reference
video quality metric for VFI, FloLPIPS, that builds on the popular perceptual
image quality metric, LPIPS, which captures the perceptual degradation in
extracted image feature space. In order to enhance the performance of LPIPS for
evaluating interpolated content, we re-designed its spatial feature aggregation
step by using the temporal distortion (through comparing optical flows) to
weight the feature difference maps. Evaluated on the BVI-VFI database, which
contains 180 test sequences with various frame interpolation artefacts,
FloLPIPS shows superior correlation performance (with statistical significance)
with subjective ground truth over 12 popular quality assessors. To facilitate
further research in VFI quality assessment, our code is publicly available at
https://danielism97.github.io/FloLPIPS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Source-free Unsupervised Domain Adaptation for Blind Image Quality Assessment. (arXiv:2207.08124v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08124">
<div class="article-summary-box-inner">
<span><p>Existing learning-based methods for blind image quality assessment (BIQA) are
heavily dependent on large amounts of annotated training data, and usually
suffer from a severe performance degradation when encountering the
domain/distribution shift problem. Thanks to the development of unsupervised
domain adaptation (UDA), some works attempt to transfer the knowledge from a
label-sufficient source domain to a label-free target domain under domain shift
with UDA. However, it requires the coexistence of source and target data, which
might be impractical for source data due to the privacy or storage issues. In
this paper, we take the first step towards the source-free unsupervised domain
adaptation (SFUDA) in a simple yet efficient manner for BIQA to tackle the
domain shift without access to the source data. Specifically, we cast the
quality assessment task as a rating distribution prediction problem. Based on
the intrinsic properties of BIQA, we present a group of well-designed
self-supervised objectives to guide the adaptation of the BN affine parameters
towards the target domain. Among them, minimizing the prediction entropy and
maximizing the batch prediction diversity aim to encourage more confident
results while avoiding the trivial solution. Besides, based on the observation
that the IQA rating distribution of single image follows the Gaussian
distribution, we apply Gaussian regularization to the predicted rating
distribution to make it more consistent with the nature of human scoring.
Extensive experimental results under cross-domain scenarios demonstrated the
effectiveness of our proposed method to mitigate the domain shift.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">E-NeRV: Expedite Neural Video Representation with Disentangled Spatial-Temporal Context. (arXiv:2207.08132v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08132">
<div class="article-summary-box-inner">
<span><p>Recently, the image-wise implicit neural representation of videos, NeRV, has
gained popularity for its promising results and swift speed compared to regular
pixel-wise implicit representations. However, the redundant parameters within
the network structure can cause a large model size when scaling up for
desirable performance. The key reason of this phenomenon is the coupled
formulation of NeRV, which outputs the spatial and temporal information of
video frames directly from the frame index input. In this paper, we propose
E-NeRV, which dramatically expedites NeRV by decomposing the image-wise
implicit neural representation into separate spatial and temporal context.
Under the guidance of this new formulation, our model greatly reduces the
redundant model parameters, while retaining the representation ability. We
experimentally find that our method can improve the performance to a large
extent with fewer parameters, resulting in a more than $8\times$ faster speed
on convergence. Code is available at https://github.com/kyleleey/E-NeRV.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Editing Out-of-domain GAN Inversion via Differential Activations. (arXiv:2207.08134v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.08134">
<div class="article-summary-box-inner">
<span><p>Despite the demonstrated editing capacity in the latent space of a pretrained
GAN model, inverting real-world images is stuck in a dilemma that the
reconstruction cannot be faithful to the original input. The main reason for
this is that the distributions between training and real-world data are
misaligned, and because of that, it is unstable of GAN inversion for real image
editing. In this paper, we propose a novel GAN prior based editing framework to
tackle the out-of-domain inversion problem with a composition-decomposition
paradigm. In particular, during the phase of composition, we introduce a
differential activation module for detecting semantic changes from a global
perspective, \ie, the relative gap between the features of edited and unedited
images. With the aid of the generated Diff-CAM mask, a coarse reconstruction
can intuitively be composited by the paired original and edited images. In this
way, the attribute-irrelevant regions can be survived in almost whole, while
the quality of such an intermediate result is still limited by an unavoidable
ghosting effect. Consequently, in the decomposition phase, we further present a
GAN prior based deghosting network for separating the final fine edited image
from the coarse reconstruction. Extensive experiments exhibit superiorities
over the state-of-the-art methods, in terms of qualitative and quantitative
evaluations. The robustness and flexibility of our method is also validated on
both scenarios of single attribute and multi-attribute manipulations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Weakly-Supervised Learning Methods for Classification and Localization in Histology Images: A Survey. (arXiv:1909.03354v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1909.03354">
<div class="article-summary-box-inner">
<span><p>Using deep learning models to diagnose cancer from histology data presents
several challenges. Cancer grading and localization of regions of interest
(ROIs) in these images normally relies on both image- and pixel-level labels,
the latter requiring a costly annotation process. Deep weakly-supervised object
localization (WSOL) methods provide different strategies for low-cost training
of deep learning models. Using only image-class annotations, these methods can
be trained to classify an image, and yield class activation maps (CAMs) for ROI
localization. This paper provides a review of state-of-art DL methods for WSOL.
We propose a taxonomy where these methods are divided into bottom-up and
top-down methods according to the information flow in models. Although the
latter have seen limited progress, recent bottom-up methods are currently
driving much progress with deep WSOL methods. Early works focused on designing
different spatial pooling functions. However, these methods reached limited
localization accuracy, and unveiled a major limitation -- the under-activation
of CAMs which leads to high false negative localization. Subsequent works aimed
to alleviate this issue and recover complete object. Representative methods
from our taxonomy are evaluated and compared in terms of classification and
localization accuracy on two challenging histology datasets. Overall, the
results indicate poor localization performance, particularly for generic
methods that were initially designed to process natural images. Methods
designed to address the challenges of histology data yielded good results.
However, all methods suffer from high false positive/negative localization.
Four key challenges are identified for the application of deep WSOL methods in
histology -- under/over activation of CAMs, sensitivity to thresholding, and
model selection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dodging DeepFake Detection via Implicit Spatial-Domain Notch Filtering. (arXiv:2009.09213v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.09213">
<div class="article-summary-box-inner">
<span><p>The current high-fidelity generation and high-precision detection of DeepFake
images are at an arms race. We believe that producing DeepFakes that are highly
realistic and ``detection evasive'' can serve the ultimate goal of improving
future generation DeepFake detection capabilities. In this paper, we propose a
simple yet powerful pipeline to reduce the artifact patterns of fake images
without hurting image quality by performing implicit spatial-domain notch
filtering. We first demonstrate that frequency-domain notch filtering, although
famously shown to be effective in removing periodic noise in the spatial
domain, is infeasible for our task at hand due to manual designs required for
the notch filters. We, therefore, resort to a learning-based approach to
reproduce the notch filtering effects, but solely in the spatial domain. We
adopt a combination of adding overwhelming spatial noise for breaking the
periodic noise pattern and deep image filtering to reconstruct the noise-free
fake images, and we name our method DeepNotch. Deep image filtering provides a
specialized filter for each pixel in the noisy image, producing filtered images
with high fidelity compared to their DeepFake counterparts. Moreover, we also
use the semantic information of the image to generate an adversarial guidance
map to add noise intelligently. Our large-scale evaluation on 3 representative
state-of-the-art DeepFake detection methods (tested on 16 types of DeepFakes)
has demonstrated that our technique significantly reduces the accuracy of these
3 fake image detection methods, 36.79% on average and up to 97.02% in the best
case.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task-Adaptive Negative Envision for Few-Shot Open-Set Recognition. (arXiv:2012.13073v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.13073">
<div class="article-summary-box-inner">
<span><p>We study the problem of few-shot open-set recognition (FSOR), which learns a
recognition system capable of both fast adaptation to new classes with limited
labeled examples and rejection of unknown negative samples. Traditional
large-scale open-set methods have been shown ineffective for FSOR problem due
to data limitation. Current FSOR methods typically calibrate few-shot
closed-set classifiers to be sensitive to negative samples so that they can be
rejected via thresholding. However, threshold tuning is a challenging process
as different FSOR tasks may require different rejection powers. In this paper,
we instead propose task-adaptive negative class envision for FSOR to integrate
threshold tuning into the learning process. Specifically, we augment the
few-shot closed-set classifier with additional negative prototypes generated
from few-shot examples. By incorporating few-shot class correlations in the
negative generation process, we are able to learn dynamic rejection boundaries
for FSOR tasks. Besides, we extend our method to generalized few-shot open-set
recognition (GFSOR), which requires classification on both many-shot and
few-shot classes as well as rejection of negative samples. Extensive
experiments on public benchmarks validate our methods on both problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Focal and Efficient IOU Loss for Accurate Bounding Box Regression. (arXiv:2101.08158v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.08158">
<div class="article-summary-box-inner">
<span><p>In object detection, bounding box regression (BBR) is a crucial step that
determines the object localization performance. However, we find that most
previous loss functions for BBR have two main drawbacks: (i) Both $\ell_n$-norm
and IOU-based loss functions are inefficient to depict the objective of BBR,
which leads to slow convergence and inaccurate regression results. (ii) Most of
the loss functions ignore the imbalance problem in BBR that the large number of
anchor boxes which have small overlaps with the target boxes contribute most to
the optimization of BBR. To mitigate the adverse effects caused thereby, we
perform thorough studies to exploit the potential of BBR losses in this paper.
Firstly, an Efficient Intersection over Union (EIOU) loss is proposed, which
explicitly measures the discrepancies of three geometric factors in BBR, i.e.,
the overlap area, the central point and the side length. After that, we state
the Effective Example Mining (EEM) problem and propose a regression version of
focal loss to make the regression process focus on high-quality anchor boxes.
Finally, the above two parts are combined to obtain a new loss function, namely
Focal-EIOU loss. Extensive experiments on both synthetic and real datasets are
performed. Notable superiorities on both the convergence speed and the
localization accuracy can be achieved over other BBR losses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synthesizing MR Image Contrast Enhancement Using 3D High-resolution ConvNets. (arXiv:2104.01592v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01592">
<div class="article-summary-box-inner">
<span><p>\textit{Objective:} Gadolinium-based contrast agents (GBCAs) have been widely
used to better visualize disease in brain magnetic resonance imaging (MRI).
However, gadolinium deposition within the brain and body has raised safety
concerns about the use of GBCAs. Therefore, the development of novel approaches
that can decrease or even eliminate GBCA exposure while providing similar
contrast information would be of significant use clinically. \textit{Methods:}
In this work, we present a deep learning based approach for contrast-enhanced
T1 synthesis on brain tumor patients. A 3D high-resolution fully convolutional
network (FCN), which maintains high resolution information through processing
and aggregates multi-scale information in parallel, is designed to map
pre-contrast MRI sequences to contrast-enhanced MRI sequences. Specifically,
three pre-contrast MRI sequences, T1, T2 and apparent diffusion coefficient map
(ADC), are utilized as inputs and the post-contrast T1 sequences are utilized
as target output. To alleviate the data imbalance problem between normal
tissues and the tumor regions, we introduce a local loss to improve the
contribution of the tumor regions, which leads to better enhancement results on
tumors. \textit{Results:} Extensive quantitative and visual assessments are
performed, with our proposed model achieving a PSNR of 28.24dB in the brain and
21.2dB in tumor regions. \textit{Conclusion and Significance:} Our results
suggest the potential of substituting GBCAs with synthetic contrast images
generated via deep learning. Code is available at
\url{https://github.com/chenchao666/Contrast-enhanced-MRI-Synthesis
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CXR Segmentation by AdaIN-based Domain Adaptation and Knowledge Distillation. (arXiv:2104.05892v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05892">
<div class="article-summary-box-inner">
<span><p>As segmentation labels are scarce, extensive researches have been conducted
to train segmentation networks with domain adaptation, semi-supervised or
self-supervised learning techniques to utilize abundant unlabeled dataset.
However, these approaches appear different from each other, so it is not clear
how these approaches can be combined for better performance. Inspired by recent
multi-domain image translation approaches, here we propose a novel segmentation
framework using adaptive instance normalization (AdaIN), so that a single
generator is trained to perform both domain adaptation and semi-supervised
segmentation tasks via knowledge distillation by simply changing task-specific
AdaIN codes. Specifically, our framework is designed to deal with difficult
situations in chest X-ray radiograph (CXR) segmentation, where labels are only
available for normal data, but trained model should be applied to both normal
and abnormal data. The proposed network demonstrates great generalizability
under domain shift and achieves the state-of-the-art performance for abnormal
CXR segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Sequential Sampling and Reconstruction for MRI. (arXiv:2105.06460v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.06460">
<div class="article-summary-box-inner">
<span><p>Accelerated MRI shortens acquisition time by subsampling in the measurement
$\kappa$-space. Recovering a high-fidelity anatomical image from subsampled
measurements requires close cooperation between two components: (1) a sampler
that chooses the subsampling pattern and (2) a reconstructor that recovers
images from incomplete measurements. In this paper, we leverage the sequential
nature of MRI measurements, and propose a fully differentiable framework that
jointly learns a sequential sampling policy simultaneously with a
reconstruction strategy. This co-designed framework is able to adapt during
acquisition in order to capture the most informative measurements for a
particular target. Experimental results on the fastMRI knee dataset demonstrate
that the proposed approach successfully utilizes intermediate information
during the sampling process to boost reconstruction performance. In particular,
our proposed method can outperform the current state-of-the-art learned
$\kappa$-space sampling baseline on over 96% of test samples. We also
investigate the individual and collective benefits of the sequential sampling
and co-design strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emotion Recognition in Horses with Convolutional Neural Networks. (arXiv:2105.11953v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.11953">
<div class="article-summary-box-inner">
<span><p>Creating intelligent systems capable of recognizing emotions is a difficult
task, especially when looking at emotions in animals. This paper describes the
process of designing a "proof of concept" system to recognize emotions in
horses. This system is formed by two elements, a detector and a model. The
detector is a fast region-based convolutional neural network that detects
horses in an image. The model is a convolutional neural network that predicts
the emotions of those horses. These two elements were trained with multiple
images of horses until they achieved high accuracy in their tasks. In total,
400 images of horses were collected and labeled to train both the detector and
the model while 40 were used to test the system. Once the two components were
validated, they were combined into a testable system that would detect equine
emotions based on established behavioral ethograms indicating emotional affect
through head, neck, ear, muzzle and eye position. The system showed an accuracy
of 80% on the validation set and 65% on the test set, demonstrating that it is
possible to predict emotions in animals using autonomous intelligent systems.
Such a system has multiple applications including further studies in the
growing field of animal emotions as well as in the veterinary field to
determine the physical welfare of horses or other livestock.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language-Driven Image Style Transfer. (arXiv:2106.00178v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.00178">
<div class="article-summary-box-inner">
<span><p>Despite having promising results, style transfer, which requires preparing
style images in advance, may result in lack of creativity and accessibility.
Following human instruction, on the other hand, is the most natural way to
perform artistic style transfer that can significantly improve controllability
for visual effect applications. We introduce a new task, language-driven
artistic style transfer (LDAST), to manipulate the style of a content image,
guided by a text. We propose contrastive language visual artist (CLVA) that
learns to extract visual semantics from style instructions and accomplish LDAST
by the patch-wise style discriminator. The discriminator considers the
correlation between language and patches of style images or transferred results
to jointly embed style instructions. CLVA further compares contrastive pairs of
content images and style instructions to improve the mutual relativeness. The
results from the same content image can preserve consistent content structures.
Besides, they should present analogous style patterns from style instructions
that contain similar visual semantics. The experiments show that our CLVA is
effective and achieves superb transferred results on LDAST.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatially Invariant Unsupervised 3D Object-Centric Learning and Scene Decomposition. (arXiv:2106.05607v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05607">
<div class="article-summary-box-inner">
<span><p>We tackle the problem of object-centric learning on point clouds, which is
crucial for high-level relational reasoning and scalable machine intelligence.
In particular, we introduce a framework, SPAIR3D, to factorize a 3D point cloud
into a spatial mixture model where each component corresponds to one object. To
model the spatial mixture model on point clouds, we derive the Chamfer Mixture
Loss, which fits naturally into our variational training pipeline. Moreover, we
adopt an object-specification scheme that describes each object's location
relative to its local voxel grid cell. Such a scheme allows SPAIR3D to model
scenes with an arbitrary number of objects. We evaluate our method on the task
of unsupervised scene decomposition. Experimental results demonstrate that
SPAIR3D has strong scalability and is capable of detecting and segmenting an
unknown number of objects from a point cloud in an unsupervised manner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Personalized Trajectory Prediction via Distribution Discrimination. (arXiv:2107.14204v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.14204">
<div class="article-summary-box-inner">
<span><p>Trajectory prediction is confronted with the dilemma to capture the
multi-modal nature of future dynamics with both diversity and accuracy. In this
paper, we present a distribution discrimination (DisDis) method to predict
personalized motion patterns by distinguishing the potential distributions.
Motivated by that the motion pattern of each person is personalized due to
his/her habit, our DisDis learns the latent distribution to represent different
motion patterns and optimize it by the contrastive discrimination. This
distribution discrimination encourages latent distributions to be more
discriminative. Our method can be integrated with existing multi-modal
stochastic predictive models as a plug-and-play module to learn the more
discriminative latent distribution. To evaluate the latent distribution, we
further propose a new metric, probability cumulative minimum distance (PCMD)
curve, which cumulatively calculates the minimum distance on the sorted
probabilities. Experimental results on the ETH and UCY datasets show the
effectiveness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoVideo: An Automated Video Action Recognition System. (arXiv:2108.04212v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04212">
<div class="article-summary-box-inner">
<span><p>Action recognition is an important task for video understanding with broad
applications. However, developing an effective action recognition solution
often requires extensive engineering efforts in building and testing different
combinations of the modules and their hyperparameters. In this demo, we present
AutoVideo, a Python system for automated video action recognition. AutoVideo is
featured for 1) highly modular and extendable infrastructure following the
standard pipeline language, 2) an exhaustive list of primitives for pipeline
construction, 3) data-driven tuners to save the efforts of pipeline tuning, and
4) easy-to-use Graphical User Interface (GUI). AutoVideo is released under MIT
license at https://github.com/datamllab/autovideo
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Blind Image Decomposition. (arXiv:2108.11364v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11364">
<div class="article-summary-box-inner">
<span><p>We propose and study a novel task named Blind Image Decomposition (BID),
which requires separating a superimposed image into constituent underlying
images in a blind setting, that is, both the source components involved in
mixing as well as the mixing mechanism are unknown. For example, rain may
consist of multiple components, such as rain streaks, raindrops, snow, and
haze. Rainy images can be treated as an arbitrary combination of these
components, some of them or all of them. How to decompose superimposed images,
like rainy images, into distinct source components is a crucial step toward
real-world vision systems. To facilitate research on this new task, we
construct multiple benchmark datasets, including mixed image decomposition
across multiple domains, real-scenario deraining, and joint
shadow/reflection/watermark removal. Moreover, we propose a simple yet general
Blind Image Decomposition Network (BIDeN) to serve as a strong baseline for
future work. Experimental results demonstrate the tenability of our benchmarks
and the effectiveness of BIDeN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Natural Synthetic Anomalies for Self-Supervised Anomaly Detection and Localization. (arXiv:2109.15222v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15222">
<div class="article-summary-box-inner">
<span><p>We introduce a simple and intuitive self-supervision task, Natural Synthetic
Anomalies (NSA), for training an end-to-end model for anomaly detection and
localization using only normal training data. NSA integrates Poisson image
editing to seamlessly blend scaled patches of various sizes from separate
images. This creates a wide range of synthetic anomalies which are more similar
to natural sub-image irregularities than previous data-augmentation strategies
for self-supervised anomaly detection. We evaluate the proposed method using
natural and medical images. Our experiments with the MVTec AD dataset show that
a model trained to localize NSA anomalies generalizes well to detecting
real-world a priori unknown types of manufacturing defects. Our method achieves
an overall detection AUROC of 97.2 outperforming all previous methods that
learn without the use of additional datasets. Code available at
https://github.com/hmsch/natural-synthetic-anomalies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lightweight Transformer in Federated Setting for Human Activity Recognition. (arXiv:2110.00244v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00244">
<div class="article-summary-box-inner">
<span><p>Human activity recognition (HAR) is a machine learning task with applications
in many domains including health care, but it has proven a challenging research
problem. In health care, it is used mainly as an assistive technology for elder
care, often used together with other related technologies such as the Internet
of Things (IoT) because HAR can be achieved with the help of IoT devices such
as smartphones, wearables, environmental and on-body sensors. Deep neural
network techniques like convolutional neural networks (CNNs) and recurrent
neural networks (RNNs) have been used for HAR, both in centralized and
federated settings. However, these techniques have certain limitations: RNNs
cannot be easily parallelized, CNNs have the limitation of sequence length, and
both are computationally expensive. Moreover, the centralized approach has
privacy concerns when facing sensitive applications such as healthcare. In this
paper, to address some of the existing challenges facing HAR, we present a
novel one-patch transformer based on inertial sensors that can combine the
advantages of RNNs and CNNs without their major limitations. We designed a
testbed to collect real-time human activity data and used the data to train and
test the proposed transformer-based HAR classifier. We also propose TransFed: a
federated learning-based HAR classifier using the proposed transformer to
address privacy concerns. The experimental results showed that the proposed
solution outperformed the state-of-the-art HAR classifiers based on CNNs and
RNNs, in both federated and centralized settings. Moreover, the proposed HAR
classifier is computationally inexpensive as it uses much fewer parameters than
existing CNN/RNN-based classifiers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Score-based diffusion models for accelerated MRI. (arXiv:2110.05243v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05243">
<div class="article-summary-box-inner">
<span><p>Score-based diffusion models provide a powerful way to model images using the
gradient of the data distribution. Leveraging the learned score function as a
prior, here we introduce a way to sample data from a conditional distribution
given the measurements, such that the model can be readily used for solving
inverse problems in imaging, especially for accelerated MRI. In short, we train
a continuous time-dependent score function with denoising score matching. Then,
at the inference stage, we iterate between numerical SDE solver and data
consistency projection step to achieve reconstruction. Our model requires
magnitude images only for training, and yet is able to reconstruct
complex-valued data, and even extends to parallel imaging. The proposed method
is agnostic to sub-sampling patterns, and can be used with any sampling
schemes. Also, due to its generative nature, our approach can quantify
uncertainty, which is not possible with standard regression settings. On top of
all the advantages, our method also has very strong performance, even beating
the models trained with full supervision. With extensive experiments, we verify
the superiority of our method in terms of quality and practicality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-View Stereo Network with attention thin volume. (arXiv:2110.08556v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08556">
<div class="article-summary-box-inner">
<span><p>We propose an efficient multi-view stereo (MVS) network for infering depth
value from multiple RGB images. Recent studies have shown that mapping the
geometric relationship in real space to neural network is an essential topic of
the MVS problem. Specifically, these methods focus on how to express the
correspondence between different views by constructing a nice cost volume. In
this paper, we propose a more complete cost volume construction approach based
on absorbing previous experience. First of all, we introduce the self-attention
mechanism to fully aggregate the dominant information from input images and
accurately model the long-range dependency, so as to selectively aggregate
reference features. Secondly, we introduce the group-wise correlation to
feature aggregation, which greatly reduces the memory and calculation burden.
Meanwhile, this method enhances the information interaction between different
feature channels. With this approach, a more lightweight and efficient cost
volume is constructed. Finally we follow the coarse to fine strategy and refine
the depth sampling range scale by scale with the help of uncertainty
estimation. We further combine the previous steps to get the attention thin
volume. Quantitative and qualitative experiments are presented to demonstrate
the performance of our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reachability Embeddings: Scalable Self-Supervised Representation Learning from Mobility Trajectories for Multimodal Geospatial Computer Vision. (arXiv:2110.12521v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12521">
<div class="article-summary-box-inner">
<span><p>Self-supervised representation learning techniques utilize large datasets
without semantic annotations to learn meaningful, universal features that can
be conveniently transferred to solve a wide variety of downstream supervised
tasks. In this paper, we propose a self-supervised method for learning
representations of geographic locations from unlabeled GPS trajectories to
solve downstream geospatial computer vision tasks. Tiles resulting from a
raster representation of the earth's surface are modeled as nodes on a graph or
pixels of an image. GPS trajectories are modeled as allowed Markovian paths on
these nodes. A scalable and distributed algorithm is presented to compute
image-like tensors, called reachability summaries, of the spatial connectivity
patterns between tiles and their neighbors implied by the observed Markovian
paths. A convolutional, contractive autoencoder is trained to learn compressed
representations, called reachability embeddings, of reachability summaries for
every tile. Reachability embeddings serve as task-agnostic, feature
representations of geographic locations. Using reachability embeddings as pixel
representations for five different downstream geospatial tasks, cast as
supervised semantic segmentation problems, we quantitatively demonstrate that
reachability embeddings are semantically meaningful representations and result
in 4-23% gain in performance, while using upto 67% less trajectory data, as
measured using area under the precision-recall curve (AUPRC) metric, when
compared to baseline models that use pixel representations that do not account
for the spatial connectivity between tiles. Reachability embeddings transform
sequential, spatiotemporal mobility data into semantically meaningful
image-like tensor representations that can be combined with other sources of
imagery and are designed to facilitate multimodal learning in geospatial
computer vision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Authentication Attacks on Projection-based Cancelable Biometric Schemes. (arXiv:2110.15163v6 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15163">
<div class="article-summary-box-inner">
<span><p>Cancelable biometric schemes aim at generating secure biometric templates by
combining user specific tokens, such as password, stored secret or salt, along
with biometric data. This type of transformation is constructed as a
composition of a biometric transformation with a feature extraction algorithm.
The security requirements of cancelable biometric schemes concern the
irreversibility, unlinkability and revocability of templates, without losing in
accuracy of comparison. While several schemes were recently attacked regarding
these requirements, full reversibility of such a composition in order to
produce colliding biometric characteristics, and specifically presentation
attacks, were never demonstrated to the best of our knowledge. In this paper,
we formalize these attacks for a traditional cancelable scheme with the help of
integer linear programming (ILP) and quadratically constrained quadratic
programming (QCQP). Solving these optimization problems allows an adversary to
slightly alter its fingerprint image in order to impersonate any individual.
Moreover, in an even more severe scenario, it is possible to simultaneously
impersonate several individuals.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Transformer with Variable-length Memory for Vision-and-Language Navigation. (arXiv:2111.05759v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.05759">
<div class="article-summary-box-inner">
<span><p>Vision-and-Language Navigation (VLN) is a task that an agent is required to
follow a language instruction to navigate to the goal position, which relies on
the ongoing interactions with the environment during moving. Recent
Transformer-based VLN methods have made great progress benefiting from the
direct connections between visual observations and the language instruction via
the multimodal cross-attention mechanism. However, these methods usually
represent temporal context as a fixed-length vector by using an LSTM decoder or
using manually designed hidden states to build a recurrent Transformer.
Considering a single fixed-length vector is often insufficient to capture
long-term temporal context, in this paper, we introduce Multimodal Transformer
with Variable-length Memory (MTVM) for visually-grounded natural language
navigation by modelling the temporal context explicitly. Specifically, MTVM
enables the agent to keep track of the navigation trajectory by directly
storing previous activations in a memory bank. To further boost the
performance, we propose a memory-aware consistency loss to help learn a better
joint representation of temporal context with random masked instructions. We
evaluate MTVM on popular R2R and CVDN datasets, and our model improves Success
Rate on R2R unseen validation and test set by 2% each, and reduce Goal Process
by 1.6m on CVDN test set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EMScore: Evaluating Video Captioning via Coarse-Grained and Fine-Grained Embedding Matching. (arXiv:2111.08919v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.08919">
<div class="article-summary-box-inner">
<span><p>Current metrics for video captioning are mostly based on the text-level
comparison between reference and candidate captions. However, they have some
insuperable drawbacks, e.g., they cannot handle videos without references, and
they may result in biased evaluation due to the one-to-many nature of
video-to-text and the neglect of visual relevance. From the human evaluator's
viewpoint, a high-quality caption should be consistent with the provided video,
but not necessarily be similar to the reference in literal or semantics.
Inspired by human evaluation, we propose EMScore (Embedding Matching-based
score), a novel reference-free metric for video captioning, which directly
measures similarity between video and candidate captions. Benefit from the
recent development of large-scale pre-training models, we exploit a well
pre-trained vision-language model to extract visual and linguistic embeddings
for computing EMScore. Specifically, EMScore combines matching scores of both
coarse-grained (video and caption) and fine-grained (frames and words) levels,
which takes the overall understanding and detailed characteristics of the video
into account. Furthermore, considering the potential information gain, EMScore
can be flexibly extended to the conditions where human-labeled references are
available. Last but not least, we collect VATEX-EVAL and ActivityNet-FOIl
datasets to systematically evaluate the existing metrics. VATEX-EVAL
experiments demonstrate that EMScore has higher human correlation and lower
reference dependency. ActivityNet-FOIL experiment verifies that EMScore can
effectively identify "hallucinating" captions. The datasets will be released to
facilitate the development of video captioning metrics. The code is available
at: https://github.com/ShiYaya/emscore.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta Clustering Learning for Large-scale Unsupervised Person Re-identification. (arXiv:2111.10032v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10032">
<div class="article-summary-box-inner">
<span><p>Unsupervised Person Re-identification (U-ReID) with pseudo labeling recently
reaches a competitive performance compared to fully-supervised ReID methods
based on modern clustering algorithms. However, such clustering-based scheme
becomes computationally prohibitive for large-scale datasets. How to
efficiently leverage endless unlabeled data with limited computing resources
for better U-ReID is under-explored. In this paper, we make the first attempt
to the large-scale U-ReID and propose a "small data for big task" paradigm
dubbed Meta Clustering Learning (MCL). MCL only pseudo-labels a subset of the
entire unlabeled data via clustering to save computing for the first-phase
training. After that, the learned cluster centroids, termed as meta-prototypes
in our MCL, are regarded as a proxy annotator to softly annotate the rest
unlabeled data for further polishing the model. To alleviate the potential
noisy labeling issue in the polishment phase, we enforce two well-designed loss
constraints to promise intra-identity consistency and inter-identity strong
correlation. For multiple widely-used U-ReID benchmarks, our method
significantly saves computational cost while achieving a comparable or even
better performance compared to prior works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are Vision Transformers Robust to Patch Perturbations?. (arXiv:2111.10659v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10659">
<div class="article-summary-box-inner">
<span><p>Recent advances in Vision Transformer (ViT) have demonstrated its impressive
performance in image classification, which makes it a promising alternative to
Convolutional Neural Network (CNN). Unlike CNNs, ViT represents an input image
as a sequence of image patches. The patch-based input image representation
makes the following question interesting: How does ViT perform when individual
input image patches are perturbed with natural corruptions or adversarial
perturbations, compared to CNNs? In this work, we study the robustness of ViT
to patch-wise perturbations. Surprisingly, we find that ViTs are more robust to
naturally corrupted patches than CNNs, whereas they are more vulnerable to
adversarial patches. Furthermore, we discover that the attention mechanism
greatly affects the robustness of vision transformers. Specifically, the
attention module can help improve the robustness of ViT by effectively ignoring
natural corrupted patches. However, when ViTs are attacked by an adversary, the
attention mechanism can be easily fooled to focus more on the adversarially
perturbed patches and cause a mistake. Based on our analysis, we propose a
simple temperature-scaling based method to improve the robustness of ViT
against adversarial patches. Extensive qualitative and quantitative experiments
are performed to support our findings, understanding, and improvement of ViT
robustness to patch-wise perturbations across a set of transformer-based
architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Supervised Vision Transformers. (arXiv:2111.11067v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11067">
<div class="article-summary-box-inner">
<span><p>We study the training of Vision Transformers for semi-supervised image
classification. Transformers have recently demonstrated impressive performance
on a multitude of supervised learning tasks. Surprisingly, we show Vision
Transformers perform significantly worse than Convolutional Neural Networks
when only a small set of labeled data is available. Inspired by this
observation, we introduce a joint semi-supervised learning framework,
Semiformer, which contains a transformer stream, a convolutional stream and a
carefully designed fusion module for knowledge sharing between these streams.
The convolutional stream is trained on limited labeled data and further used to
generate pseudo labels to supervise the training of the transformer stream on
unlabeled data. Extensive experiments on ImageNet demonstrate that Semiformer
achieves 75.5% top-1 accuracy, outperforming the state-of-the-art by a clear
margin. In addition, we show, among other things, Semiformer is a general
framework that is compatible with most modern transformer and convolutional
neural architectures. Code is available at
https://github.com/wengzejia1/Semiformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Class-agnostic Object Detection with Multi-modal Transformer. (arXiv:2111.11430v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11430">
<div class="article-summary-box-inner">
<span><p>What constitutes an object? This has been a long-standing question in
computer vision. Towards this goal, numerous learning-free and learning-based
approaches have been developed to score objectness. However, they generally do
not scale well across new domains and novel objects. In this paper, we advocate
that existing methods lack a top-down supervision signal governed by
human-understandable semantics. For the first time in literature, we
demonstrate that Multi-modal Vision Transformers (MViT) trained with aligned
image-text pairs can effectively bridge this gap. Our extensive experiments
across various domains and novel objects show the state-of-the-art performance
of MViTs to localize generic objects in images. Based on the observation that
existing MViTs do not include multi-scale feature processing and usually
require longer training schedules, we develop an efficient MViT architecture
using multi-scale deformable attention and late vision-language fusion. We show
the significance of MViT proposals in a diverse range of applications including
open-world object detection, salient and camouflage object detection,
supervised and self-supervised detection tasks. Further, MViTs can adaptively
generate proposals given a specific language query and thus offer enhanced
interactability. Code: \url{https://git.io/J1HPY}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Video Transformers with Spatial-Temporal Token Selection. (arXiv:2111.11591v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11591">
<div class="article-summary-box-inner">
<span><p>Video transformers have achieved impressive results on major video
recognition benchmarks, which however suffer from high computational cost. In
this paper, we present STTS, a token selection framework that dynamically
selects a few informative tokens in both temporal and spatial dimensions
conditioned on input video samples. Specifically, we formulate token selection
as a ranking problem, which estimates the importance of each token through a
lightweight scorer network and only those with top scores will be used for
downstream evaluation. In the temporal dimension, we keep the frames that are
most relevant to the action categories, while in the spatial dimension, we
identify the most discriminative region in feature maps without affecting the
spatial context used in a hierarchical way in most video transformers. Since
the decision of token selection is non-differentiable, we employ a
perturbed-maximum based differentiable Top-K operator for end-to-end training.
We mainly conduct extensive experiments on Kinetics-400 with a recently
introduced video transformer backbone, MViT. Our framework achieves similar
results while requiring 20% less computation. We also demonstrate our approach
is generic for different transformer architectures and video datasets. Code is
available at https://github.com/wangjk666/STTS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KTNet: Knowledge Transfer for Unpaired 3D Shape Completion. (arXiv:2111.11976v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11976">
<div class="article-summary-box-inner">
<span><p>Unpaired 3D object completion aims to predict a complete 3D shape from an
incomplete input without knowing the correspondence between the complete and
incomplete shapes. In this paper, we propose the novel KTNet to solve this task
from the new perspective of knowledge transfer. KTNet elaborates a
teacher-assistant-student network to establish multiple knowledge transfer
processes. Specifically, the teacher network takes complete shape as input and
learns the knowledge of complete shape. The student network takes the
incomplete one as input and restores the corresponding complete shape. And the
assistant modules not only help to transfer the knowledge of complete shape
from the teacher to the student, but also judge the learning effect of the
student network. As a result, KTNet makes use of a more comprehensive
understanding to establish the geometric correspondence between complete and
incomplete shapes in a perspective of knowledge transfer, which enables more
detailed geometric inference for generating high-quality complete shapes. We
conduct comprehensive experiments on several datasets, and the results show
that our method outperforms previous methods of unpaired point cloud completion
by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Lightweight Graph Transformer Network for Human Mesh Reconstruction from 2D Human Pose. (arXiv:2111.12696v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12696">
<div class="article-summary-box-inner">
<span><p>Existing deep learning-based human mesh reconstruction approaches have a
tendency to build larger networks in order to achieve higher accuracy.
Computational complexity and model size are often neglected, despite being key
characteristics for practical use of human mesh reconstruction models (e.g.
virtual try-on systems). In this paper, we present GTRS, a lightweight
pose-based method that can reconstruct human mesh from 2D human pose. We
propose a pose analysis module that uses graph transformers to exploit
structured and implicit joint correlations, and a mesh regression module that
combines the extracted pose feature with the mesh template to reconstruct the
final human mesh. We demonstrate the efficiency and generalization of GTRS by
extensive evaluations on the Human3.6M and 3DPW datasets. In particular, GTRS
achieves better accuracy than the SOTA pose-based method Pose2Mesh while only
using 10.2% of the parameters (Params) and 2.5% of the FLOPs on the challenging
in-the-wild 3DPW dataset. Code will be publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving the Perceptual Quality of 2D Animation Interpolation. (arXiv:2111.12792v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12792">
<div class="article-summary-box-inner">
<span><p>Traditional 2D animation is labor-intensive, often requiring animators to
manually draw twelve illustrations per second of movement. While automatic
frame interpolation may ease this burden, 2D animation poses additional
difficulties compared to photorealistic video. In this work, we address
challenges unexplored in previous animation interpolation systems, with a focus
on improving perceptual quality. Firstly, we propose SoftsplatLite (SSL), a
forward-warping interpolation architecture with fewer trainable parameters and
better perceptual performance. Secondly, we design a Distance Transform Module
(DTM) that leverages line proximity cues to correct aberrations in difficult
solid-color regions. Thirdly, we define a Restricted Relative Linear
Discrepancy metric (RRLD) to automate the previously manual training data
collection process. Lastly, we explore evaluation of 2D animation generation
through a user study, and establish that the LPIPS perceptual metric and
chamfer line distance (CD) are more appropriate measures of quality than PSNR
and SSIM used in prior art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint stereo 3D object detection and implicit surface reconstruction. (arXiv:2111.12924v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12924">
<div class="article-summary-box-inner">
<span><p>We present the first learning-based framework for category-level 3D object
detection and implicit shape estimation based on a pair of stereo RGB images in
the wild. Previous stereo 3D object detection approaches cannot describe the
complete shape details of the detected objects and often fails for the small
objects. In contrast, we propose a new progressive approach that can (1)
perform precise localization as well as provide a complete and
resolution-agnostic shape description for the detected objects and (2) produce
significantly more accurate orientation predictions for the tiny instances.
This approach features a new instance-level network that explicitly models the
unseen surface hallucination problem using point-based representations and uses
a new geometric representation for orientation refinement. Extensive
experiments show that our approach achieves state-of-the-art performance using
various metrics on the KITTI benchmark. Code and pre-trained models will be
available at this https URL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Vicinal Space for Unsupervised Domain Adaptation. (arXiv:2111.13353v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13353">
<div class="article-summary-box-inner">
<span><p>Recent unsupervised domain adaptation methods have utilized vicinal space
between the source and target domains. However, the equilibrium collapse of
labels, a problem where the source labels are dominant over the target labels
in the predictions of vicinal instances, has never been addressed. In this
paper, we propose an instance-wise minimax strategy that minimizes the entropy
of high uncertainty instances in the vicinal space to tackle the stated
problem. We divide the vicinal space into two subspaces through the solution of
the minimax problem: contrastive space and consensus space. In the contrastive
space, inter-domain discrepancy is mitigated by constraining instances to have
contrastive views and labels, and the consensus space reduces the confusion
between intra-domain categories. The effectiveness of our method is
demonstrated on public benchmarks, including Office-31, Office-Home, and
VisDA-C, achieving state-of-the-art performances. We further show that our
method outperforms the current state-of-the-art methods on PACS, which
indicates that our instance-wise approach works well for multi-source domain
adaptation as well. Code is available at https://github.com/NaJaeMin92/CoVi.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GMFlow: Learning Optical Flow via Global Matching. (arXiv:2111.13680v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13680">
<div class="article-summary-box-inner">
<span><p>Learning-based optical flow estimation has been dominated with the pipeline
of cost volume with convolutions for flow regression, which is inherently
limited to local correlations and thus is hard to address the long-standing
challenge of large displacements. To alleviate this, the state-of-the-art
framework RAFT gradually improves its prediction quality by using a large
number of iterative refinements, achieving remarkable performance but
introducing linearly increasing inference time. To enable both high accuracy
and efficiency, we completely revamp the dominant flow regression pipeline by
reformulating optical flow as a global matching problem, which identifies the
correspondences by directly comparing feature similarities. Specifically, we
propose a GMFlow framework, which consists of three main components: a
customized Transformer for feature enhancement, a correlation and softmax layer
for global feature matching, and a self-attention layer for flow propagation.
We further introduce a refinement step that reuses GMFlow at higher feature
resolution for residual flow prediction. Our new framework outperforms
31-refinements RAFT on the challenging Sintel benchmark, while using only one
refinement and running faster, suggesting a new paradigm for accurate and
efficient optical flow estimation. Code is available at
https://github.com/haofeixu/gmflow.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AVA-AVD: Audio-Visual Speaker Diarization in the Wild. (arXiv:2111.14448v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14448">
<div class="article-summary-box-inner">
<span><p>Audio-visual speaker diarization aims at detecting "who spoke when" using
both auditory and visual signals. Existing audio-visual diarization datasets
are mainly focused on indoor environments like meeting rooms or news studios,
which are quite different from in-the-wild videos in many scenarios such as
movies, documentaries, and audience sitcoms. To develop diarization methods for
these challenging videos, we create the AVA Audio-Visual Diarization (AVA-AVD)
dataset. Our experiments demonstrate that adding AVA-AVD into training set can
produce significantly better diarization models for in-the-wild videos despite
that the data is relatively small. Moreover, this benchmark is challenging due
to the diverse scenes, complicated acoustic conditions, and completely
off-screen speakers. As a first step towards addressing the challenges, we
design the Audio-Visual Relation Network (AVR-Net) which introduces a simple
yet effective modality mask to capture discriminative information based on face
visibility. Experiments show that our method not only can outperform
state-of-the-art methods but is more robust as varying the ratio of off-screen
speakers. Our data and code has been made publicly available at
https://github.com/showlab/AVA-AVD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic-Sparse Colorization Network for Deep Exemplar-based Colorization. (arXiv:2112.01335v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01335">
<div class="article-summary-box-inner">
<span><p>Exemplar-based colorization approaches rely on reference image to provide
plausible colors for target gray-scale image. The key and difficulty of
exemplar-based colorization is to establish an accurate correspondence between
these two images. Previous approaches have attempted to construct such a
correspondence but are faced with two obstacles. First, using luminance
channels for the calculation of correspondence is inaccurate. Second, the dense
correspondence they built introduces wrong matching results and increases the
computation burden. To address these two problems, we propose Semantic-Sparse
Colorization Network (SSCN) to transfer both the global image style and
detailed semantic-related colors to the gray-scale image in a coarse-to-fine
manner. Our network can perfectly balance the global and local colors while
alleviating the ambiguous matching problem. Experiments show that our method
outperforms existing methods in both quantitative and qualitative evaluation
and achieves state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D-Aware Semantic-Guided Generative Model for Human Synthesis. (arXiv:2112.01422v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01422">
<div class="article-summary-box-inner">
<span><p>Generative Neural Radiance Field (GNeRF) models, which extract implicit 3D
representations from 2D images, have recently been shown to produce realistic
images representing rigid/semi-rigid objects, such as human faces or cars.
However, they usually struggle to generate high-quality images representing
non-rigid objects, such as the human body, which is of a great interest for
many computer graphics applications. This paper proposes a 3D-aware
Semantic-Guided Generative Model (3D-SGAN) for human image synthesis, which
combines a GNeRF with a texture generator. The former learns an implicit 3D
representation of the human body and outputs a set of 2D semantic segmentation
masks. The latter transforms these semantic masks into a real image, adding a
realistic texture to the human appearance. Without requiring additional 3D
information, our model can learn 3D human representations with a
photo-realistic, controllable generation. Our experiments on the DeepFashion
dataset show that 3D-SGAN significantly outperforms the most recent baselines.
The code is available at https://github.com/zhangqianhui/3DSGAN
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Channel Encoding Transformer for Point Cloud Analysis. (arXiv:2112.02507v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02507">
<div class="article-summary-box-inner">
<span><p>Transformer plays an increasingly important role in various computer vision
areas and remarkable achievements have also been made in point cloud analysis.
Since they mainly focus on point-wise transformer, an adaptive channel encoding
transformer is proposed in this paper. Specifically, a channel convolution
called Transformer-Conv is designed to encode the channel. It can encode
feature channels by capturing the potential relationship between coordinates
and features. Compared with simply assigning attention weight to each channel,
our method aims to encode the channel adaptively. In addition, our network
adopts the neighborhood search method of low-level and high-level dual semantic
receptive fields to improve the performance. Extensive experiments show that
our method is superior to state-of-the-art point cloud classification and
segmentation methods on three benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PolyphonicFormer: Unified Query Learning for Depth-aware Video Panoptic Segmentation. (arXiv:2112.02582v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02582">
<div class="article-summary-box-inner">
<span><p>The Depth-aware Video Panoptic Segmentation (DVPS) is a new challenging
vision problem that aims to predict panoptic segmentation and depth in a video
simultaneously. The previous work solves this task by extending the existing
panoptic segmentation method with an extra dense depth prediction and instance
tracking head. However, the relationship between the depth and panoptic
segmentation is not well explored -- simply combining existing methods leads to
competition and needs carefully weight balancing. In this paper, we present
PolyphonicFormer, a vision transformer to unify these sub-tasks under the DVPS
task and lead to more robust results. Our principal insight is that the depth
can be harmonized with the panoptic segmentation with our proposed new paradigm
of predicting instance level depth maps with object queries. Then the
relationship between the two tasks via query-based learning is explored. From
the experiments, we demonstrate the benefits of our design from both depth
estimation and panoptic segmentation aspects. Since each thing query also
encodes the instance-wise information, it is natural to perform tracking
directly with appearance learning. Our method achieves state-of-the-art results
on two DVPS datasets (Semantic KITTI, Cityscapes), and ranks 1st on the
ICCV-2021 BMTT Challenge video + depth track. Code is available at
https://github.com/HarborYuan/PolyphonicFormer .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GreenPCO: An Unsupervised Lightweight Point Cloud Odometry Method. (arXiv:2112.04054v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04054">
<div class="article-summary-box-inner">
<span><p>Visual odometry aims to track the incremental motion of an object using the
information captured by visual sensors. In this work, we study the point cloud
odometry problem, where only the point cloud scans obtained by the LiDAR (Light
Detection And Ranging) are used to estimate object's motion trajectory. A
lightweight point cloud odometry solution is proposed and named the green point
cloud odometry (GreenPCO) method. GreenPCO is an unsupervised learning method
that predicts object motion by matching features of consecutive point cloud
scans. It consists of three steps. First, a geometry-aware point sampling
scheme is used to select discriminant points from the large point cloud.
Second, the view is partitioned into four regions surrounding the object, and
the PointHop++ method is used to extract point features. Third, point
correspondences are established to estimate object motion between two
consecutive scans. Experiments on the KITTI dataset are conducted to
demonstrate the effectiveness of the GreenPCO method. It is observed that
GreenPCO outperforms benchmarking deep learning methods in accuracy while it
has a significantly smaller model size and less training time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformaly -- Two (Feature Spaces) Are Better Than One. (arXiv:2112.04185v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04185">
<div class="article-summary-box-inner">
<span><p>Anomaly detection is a well-established research area that seeks to identify
samples outside of a predetermined distribution. An anomaly detection pipeline
is comprised of two main stages: (1) feature extraction and (2) normality score
assignment. Recent papers used pre-trained networks for feature extraction
achieving state-of-the-art results. However, the use of pre-trained networks
does not fully-utilize the normal samples that are available at train time.
This paper suggests taking advantage of this information by using
teacher-student training. In our setting, a pretrained teacher network is used
to train a student network on the normal training samples. Since the student
network is trained only on normal samples, it is expected to deviate from the
teacher network in abnormal cases. This difference can serve as a complementary
representation to the pre-trained feature vector. Our method -- Transformaly --
exploits a pre-trained Vision Transformer (ViT) to extract both feature
vectors: the pre-trained (agnostic) features and the teacher-student
(fine-tuned) features. We report state-of-the-art AUROC results in both the
common unimodal setting, where one class is considered normal and the rest are
considered abnormal, and the multimodal setting, where all classes but one are
considered normal, and just one class is considered abnormal. The code is
available at https://github.com/MatanCohen1/Transformaly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Marine Bubble Flow Quantification Using Wide-Baseline Stereo Photogrammetry. (arXiv:2112.07414v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07414">
<div class="article-summary-box-inner">
<span><p>Reliable quantification of natural and anthropogenic gas release (e.g.\
CO$_2$, methane) from the seafloor into the water column, and potentially to
the atmosphere, is a challenging task. While ship-based echo sounders such as
single beam and multibeam systems allow detection of free gas, bubbles, in the
water even from a great distance, exact quantification utilizing the
hydroacoustic data requires additional parameters such as rise speed and bubble
size distribution. Optical methods are complementary in the sense that they can
provide high temporal and spatial resolution of single bubbles or bubble
streams from close distance. In this contribution we introduce a complete
instrument and evaluation method for optical bubble stream characterization
targeted at flows of up to 100ml/min and bubbles with a few millimeters radius.
The dedicated instrument employs a high-speed deep sea capable stereo camera
system that can record terabytes of bubble imagery when deployed at a seep site
for later automated analysis. Bubble characteristics can be obtained for short
sequences, then relocating the instrument to other locations, or in autonomous
mode of definable intervals up to several days, in order to capture bubble flow
variations due to e.g. tide dependent pressure changes or reservoir depletion.
Beside reporting the steps to make bubble characterization robust and
autonomous, we carefully evaluate the reachable accuracy to be in the range of
1-2\% of the bubble radius and propose a novel auto-calibration procedure that,
due to the lack of point correspondences, uses only the silhouettes of bubbles.
The system has been operated successfully in 1000m water depth at the Cascadia
margin offshore Oregon to assess methane fluxes from various seep locations.
Besides sample results we also report failure cases and lessons learnt during
deployment and method development.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AFDetV2: Rethinking the Necessity of the Second Stage for Object Detection from Point Clouds. (arXiv:2112.09205v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09205">
<div class="article-summary-box-inner">
<span><p>There have been two streams in the 3D detection from point clouds:
single-stage methods and two-stage methods. While the former is more
computationally efficient, the latter usually provides better detection
accuracy. By carefully examining the two-stage approaches, we have found that
if appropriately designed, the first stage can produce accurate box regression.
In this scenario, the second stage mainly rescores the boxes such that the
boxes with better localization get selected. From this observation, we have
devised a single-stage anchor-free network that can fulfill these requirements.
This network, named AFDetV2, extends the previous work by incorporating a
self-calibrated convolution block in the backbone, a keypoint auxiliary
supervision, and an IoU prediction branch in the multi-task head. As a result,
the detection accuracy is drastically boosted in the single-stage. To evaluate
our approach, we have conducted extensive experiments on the Waymo Open Dataset
and the nuScenes Dataset. We have observed that our AFDetV2 achieves the
state-of-the-art results on these two datasets, superior to all the prior arts,
including both the single-stage and the two-stage 3D detectors. AFDetV2 won the
1st place in the Real-Time 3D Detection of the Waymo Open Dataset Challenge
2021. In addition, a variant of our model AFDetV2-Base was entitled the "Most
Efficient Model" by the Challenge Sponsor, showing a superior computational
efficiency. To demonstrate the generality of this single-stage method, we have
also applied it to the first stage of the two-stage networks. Without
exception, the results show that with the strengthened backbone and the
rescoring approach, the second stage refinement is no longer needed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Vision-Language Pre-training with Limited Resources. (arXiv:2112.09331v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09331">
<div class="article-summary-box-inner">
<span><p>Pioneering dual-encoder pre-training works (e.g., CLIP and ALIGN) have
revealed the potential of aligning multi-modal representations with contrastive
learning. However, these works require a tremendous amount of data and
computational resources (e.g., billion-level web data and hundreds of GPUs),
which prevent researchers with limited resources from reproduction and further
exploration. To this end, we propose a stack of novel methods, which
significantly cut down the heavy resource dependency and allow us to conduct
dual-encoder multi-modal representation alignment with limited resources.
Besides, we provide a reproducible baseline of competitive results, namely
ZeroVL, with only 14M publicly accessible academic datasets and 8 V100 GPUs.
Additionally, we collect 100M web data for pre-training, and achieve comparable
or superior results than state-of-the-art methods, further proving the
effectiveness of our methods on large-scale data. We hope that this work will
provide useful data points and experience for future research in contrastive
vision-language pre-training. Code is available at
https://github.com/zerovl/ZeroVL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Visual Tracking with Exemplar Transformers. (arXiv:2112.09686v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09686">
<div class="article-summary-box-inner">
<span><p>The design of more complex and powerful neural network models has
significantly advanced the state-of-the-art in visual object tracking. These
advances can be attributed to deeper networks, or the introduction of new
building blocks, such as transformers. However, in the pursuit of increased
tracking performance, runtime is often hindered. Furthermore, efficient
tracking architectures have received surprisingly little attention. In this
paper, we introduce the Exemplar Transformer, a transformer module utilizing a
single instance level attention layer for realtime visual object tracking.
E.T.Track, our visual tracker that incorporates Exemplar Transformer modules,
runs at 47 FPS on a CPU. This is up to 8x faster than other transformer-based
models. When compared to lightweight trackers that can operate in realtime on
standard CPUs, E.T.Track consistently outperforms all other methods on the
LaSOT, OTB-100, NFS, TrackingNet, and VOT-ST2020 datasets. The code will be
made publicly available upon publication.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">iSegFormer: Interactive Segmentation via Transformers with Application to 3D Knee MR Images. (arXiv:2112.11325v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11325">
<div class="article-summary-box-inner">
<span><p>We propose iSegFormer, a memory-efficient transformer that combines a Swin
transformer with a lightweight multilayer perceptron (MLP) decoder. With the
efficient Swin transformer blocks for hierarchical self-attention and the
simple MLP decoder for aggregating both local and global attention, iSegFormer
learns powerful representations while achieving high computational
efficiencies. Specifically, we apply iSegFormer to interactive 3D medical image
segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages. (arXiv:2201.11732v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11732">
<div class="article-summary-box-inner">
<span><p>Reliable evaluation benchmarks designed for replicability and
comprehensiveness have driven progress in machine learning. Due to the lack of
a multilingual benchmark, however, vision-and-language research has mostly
focused on English language tasks. To fill this gap, we introduce the
Image-Grounded Language Understanding Evaluation benchmark. IGLUE brings
together - by both aggregating pre-existing datasets and creating new ones -
visual question answering, cross-modal retrieval, grounded reasoning, and
grounded entailment tasks across 20 diverse languages. Our benchmark enables
the evaluation of multilingual multimodal models for transfer learning, not
only in a zero-shot setting, but also in newly defined few-shot learning
setups. Based on the evaluation of the available state-of-the-art models, we
find that translate-test transfer is superior to zero-shot transfer and that
few-shot learning is hard to harness for many tasks. Moreover, downstream
performance is partially explained by the amount of available unlabelled
textual data for pretraining, and only weakly by the typological distance of
target-source languages. We hope to encourage future research efforts in this
area by releasing the benchmark to the community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FedMed-ATL: Misaligned Unpaired Brain Image Synthesis via Affine Transform Loss. (arXiv:2201.12589v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12589">
<div class="article-summary-box-inner">
<span><p>The existence of completely aligned and paired multi-modal neuroimaging data
has proved its effectiveness in the diagnosis of brain diseases. However,
collecting the full set of well-aligned and paired data is impractical, since
the practical difficulties may include high cost, long time acquisition, image
corruption, and privacy issues. Previously, the misaligned unpaired
neuroimaging data (termed as MUD) are generally treated as noisy label.
However, such a noisy label-based method fail to accomplish well when
misaligned data occurs distortions severely. For example, the angle of rotation
is different. In this paper, we propose a novel federated self-supervised
learning (FedMed) for brain image synthesis. An affine transform loss (ATL) was
formulated to make use of severely distorted images without violating privacy
legislation for the hospital. We then introduce a new data augmentation
procedure for self-supervised training and fed it into three auxiliary heads,
namely auxiliary rotation, auxiliary translation and auxiliary scaling heads.
The proposed method demonstrates the advanced performance in both the quality
of our synthesized results under a severely misaligned and unpaired data
setting, and better stability than other GAN-based algorithms. The proposed
method also reduces the demand for deformable registration while encouraging to
leverage the misaligned and unpaired data. Experimental results verify the
outstanding performance of our learning paradigm compared to other
state-of-the-art approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MINER: Multiscale Implicit Neural Representations. (arXiv:2202.03532v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03532">
<div class="article-summary-box-inner">
<span><p>We introduce a new neural signal model designed for efficient high-resolution
representation of large-scale signals. The key innovation in our multiscale
implicit neural representation (MINER) is an internal representation via a
Laplacian pyramid, which provides a sparse multiscale decomposition of the
signal that captures orthogonal parts of the signal across scales. We leverage
the advantages of the Laplacian pyramid by representing small disjoint patches
of the pyramid at each scale with a small MLP. This enables the capacity of the
network to adaptively increase from coarse to fine scales, and only represent
parts of the signal with strong signal energy. The parameters of each MLP are
optimized from coarse-to-fine scale which results in faster approximations at
coarser scales, thereby ultimately an extremely fast training process. We apply
MINER to a range of large-scale signal representation tasks, including
gigapixel images and very large point clouds, and demonstrate that it requires
fewer than 25% of the parameters, 33% of the memory footprint, and 10% of the
computation time of competing techniques such as ACORN to reach the same
representation accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NIMBLE: A Non-rigid Hand Model with Bones and Muscles. (arXiv:2202.04533v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04533">
<div class="article-summary-box-inner">
<span><p>Emerging Metaverse applications demand reliable, accurate, and photorealistic
reproductions of human hands to perform sophisticated operations as if in the
physical world. While real human hand represents one of the most intricate
coordination between bones, muscle, tendon, and skin, state-of-the-art
techniques unanimously focus on modeling only the skeleton of the hand. In this
paper, we present NIMBLE, a novel parametric hand model that includes the
missing key components, bringing 3D hand model to a new level of realism. We
first annotate muscles, bones and skins on the recent Magnetic Resonance
Imaging hand (MRI-Hand) dataset and then register a volumetric template hand
onto individual poses and subjects within the dataset. NIMBLE consists of 20
bones as triangular meshes, 7 muscle groups as tetrahedral meshes, and a skin
mesh. Via iterative shape registration and parameter learning, it further
produces shape blend shapes, pose blend shapes, and a joint regressor. We
demonstrate applying NIMBLE to modeling, rendering, and visual inference tasks.
By enforcing the inner bones and muscles to match anatomic and kinematic rules,
NIMBLE can animate 3D hands to new poses at unprecedented realism. To model the
appearance of skin, we further construct a photometric HandStage to acquire
high-quality textures and normal maps to model wrinkles and palm print.
Finally, NIMBLE also benefits learning-based hand pose and shape estimation by
either synthesizing rich data or acting directly as a differentiable layer in
the inference network.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph Neural Network for Cell Tracking in Microscopy Videos. (arXiv:2202.04731v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04731">
<div class="article-summary-box-inner">
<span><p>We present a novel graph neural network (GNN) approach for cell tracking in
high-throughput microscopy videos. By modeling the entire time-lapse sequence
as a direct graph where cell instances are represented by its nodes and their
associations by its edges, we extract the entire set of cell trajectories by
looking for the maximal paths in the graph. This is accomplished by several key
contributions incorporated into an end-to-end deep learning framework. We
exploit a deep metric learning algorithm to extract cell feature vectors that
distinguish between instances of different biological cells and assemble same
cell instances. We introduce a new GNN block type which enables a mutual update
of node and edge feature vectors, thus facilitating the underlying message
passing process. The message passing concept, whose extent is determined by the
number of GNN blocks, is of fundamental importance as it enables the `flow' of
information between nodes and edges much behind their neighbors in consecutive
frames. Finally, we solve an edge classification problem and use the identified
active edges to construct the cells' tracks and lineage trees. We demonstrate
the strengths of the proposed cell tracking approach by applying it to 2D and
3D datasets of different cell types, imaging setups, and experimental
conditions. We show that our framework outperforms current state-of-the-art
methods on most of the evaluated datasets. The code is available at our
repository: https://github.com/talbenha/cell-tracker-gnn.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FILM: Frame Interpolation for Large Motion. (arXiv:2202.04901v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04901">
<div class="article-summary-box-inner">
<span><p>We present a frame interpolation algorithm that synthesizes multiple
intermediate frames from two input images with large in-between motion. Recent
methods use multiple networks to estimate optical flow or depth and a separate
network dedicated to frame synthesis. This is often complex and requires scarce
optical flow or depth ground-truth. In this work, we present a single unified
network, distinguished by a multi-scale feature extractor that shares weights
at all scales, and is trainable from frames alone. To synthesize crisp and
pleasing frames, we propose to optimize our network with the Gram matrix loss
that measures the correlation difference between feature maps. Our approach
outperforms state-of-the-art methods on the Xiph large motion benchmark. We
also achieve higher scores on Vimeo-90K, Middlebury and UCF101, when comparing
to methods that use perceptual losses. We study the effect of weight sharing
and of training with datasets of increasing motion range. Finally, we
demonstrate our model's effectiveness in synthesizing high quality and
temporally coherent videos on a challenging near-duplicate photos dataset.
Codes and pre-trained models are available at https://film-net.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">D2ADA: Dynamic Density-aware Active Domain Adaptation for Semantic Segmentation. (arXiv:2202.06484v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06484">
<div class="article-summary-box-inner">
<span><p>In the field of domain adaptation, a trade-off exists between the model
performance and the number of target domain annotations. Active learning,
maximizing model performance with few informative labeled data, comes in handy
for such a scenario. In this work, we present D2ADA, a general active domain
adaptation framework for semantic segmentation. To adapt the model to the
target domain with minimum queried labels, we propose acquiring labels of the
samples with high probability density in the target domain yet with low
probability density in the source domain, complementary to the existing source
domain labeled data. To further facilitate labeling efficiency, we design a
dynamic scheduling policy to adjust the labeling budgets between domain
exploration and model uncertainty over time. Extensive experiments show that
our method outperforms existing active learning and domain adaptation baselines
on two benchmarks, GTA5 -&gt; Cityscapes and SYNTHIA -&gt; Cityscapes. With less than
5% target domain annotations, our method reaches comparable results with that
of full supervision. Our code is publicly available at
https://github.com/tsunghan-wu/D2ADA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Universal Adversarial Examples in Remote Sensing: Methodology and Benchmark. (arXiv:2202.07054v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07054">
<div class="article-summary-box-inner">
<span><p>Deep neural networks have achieved great success in many important remote
sensing tasks. Nevertheless, their vulnerability to adversarial examples should
not be neglected. In this study, we systematically analyze the universal
adversarial examples in remote sensing data for the first time, without any
knowledge from the victim model. Specifically, we propose a novel black-box
adversarial attack method, namely Mixup-Attack, and its simple variant
Mixcut-Attack, for remote sensing data. The key idea of the proposed methods is
to find common vulnerabilities among different networks by attacking the
features in the shallow layer of a given surrogate model. Despite their
simplicity, the proposed methods can generate transferable adversarial examples
that deceive most of the state-of-the-art deep neural networks in both scene
classification and semantic segmentation tasks with high success rates. We
further provide the generated universal adversarial examples in the dataset
named UAE-RS, which is the first dataset that provides black-box adversarial
samples in the remote sensing field. We hope UAE-RS may serve as a benchmark
that helps researchers to design deep neural networks with strong resistance
toward adversarial attacks in the remote sensing field. Codes and the UAE-RS
dataset are available online (https://github.com/YonghaoXu/UAE-RS).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ScoreNet: Learning Non-Uniform Attention and Augmentation for Transformer-Based Histopathological Image Classification. (arXiv:2202.07570v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07570">
<div class="article-summary-box-inner">
<span><p>Progress in digital pathology is hindered by high-resolution images and the
prohibitive cost of exhaustive localized annotations. The commonly used
paradigm to categorize pathology images is patch-based processing, which often
incorporates multiple instance learning (MIL) to aggregate local patch-level
representations yielding image-level prediction. Nonetheless, diagnostically
relevant regions may only take a small fraction of the whole tissue, and
current MIL-based approaches often process images uniformly, discarding the
inter-patches interactions. To alleviate these issues, we propose ScoreNet, a
new efficient transformer that exploits a differentiable recommendation stage
to extract discriminative image regions and dedicate computational resources
accordingly. The proposed transformer leverages the local and global attention
of a few dynamically recommended high-resolution regions at an efficient
computational cost. We further introduce a novel mixing data-augmentation,
namely ScoreMix, by leveraging the image's semantic distribution to guide the
data mixing and produce coherent sample-label pairs. ScoreMix is embarrassingly
simple and mitigates the pitfalls of previous augmentations, which assume a
uniform semantic distribution and risk mislabeling the samples. Thorough
experiments and ablation studies on three breast cancer histology datasets of
Haematoxylin &amp; Eosin (H&amp;E) have validated the superiority of our approach over
prior arts, including transformer-based models on tumour regions-of-interest
(TRoIs) classification. ScoreNet equipped with proposed ScoreMix augmentation
demonstrates better generalization capabilities and achieves new
state-of-the-art (SOTA) results with only 50% of the data compared to other
mixing augmentation variants. Finally, ScoreNet yields high efficacy and
outperforms SOTA efficient transformers, namely TransPath and SwinTransformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">V2X-Sim: Multi-Agent Collaborative Perception Dataset and Benchmark for Autonomous Driving. (arXiv:2202.08449v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08449">
<div class="article-summary-box-inner">
<span><p>Vehicle-to-everything (V2X) communication techniques enable the collaboration
between vehicles and many other entities in the neighboring environment, which
could fundamentally improve the perception system for autonomous driving.
However, the lack of a public dataset significantly restricts the research
progress of collaborative perception. To fill this gap, we present V2X-Sim, a
comprehensive simulated multi-agent perception dataset for V2X-aided autonomous
driving. V2X-Sim provides: (1) \hl{multi-agent} sensor recordings from the
road-side unit (RSU) and multiple vehicles that enable collaborative
perception, (2) multi-modality sensor streams that facilitate multi-modality
perception, and (3) diverse ground truths that support various perception
tasks. Meanwhile, we build an open-source testbed and provide a benchmark for
the state-of-the-art collaborative perception algorithms on three tasks,
including detection, tracking and segmentation. V2X-Sim seeks to stimulate
collaborative perception research for autonomous driving before realistic
datasets become widely available. Our dataset and code are available at
\url{https://ai4ce.github.io/V2X-Sim/}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Vision-Language Pre-Trained Models. (arXiv:2202.10936v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.10936">
<div class="article-summary-box-inner">
<span><p>As transformer evolves, pre-trained models have advanced at a breakneck pace
in recent years. They have dominated the mainstream techniques in natural
language processing (NLP) and computer vision (CV). How to adapt pre-training
to the field of Vision-and-Language (V-L) learning and improve downstream task
performance becomes a focus of multimodal learning. In this paper, we review
the recent progress in Vision-Language Pre-Trained Models (VL-PTMs). As the
core content, we first briefly introduce several ways to encode raw images and
texts to single-modal embeddings before pre-training. Then, we dive into the
mainstream architectures of VL-PTMs in modeling the interaction between text
and image representations. We further present widely-used pre-training tasks,
and then we introduce some common downstream tasks. We finally conclude this
paper and present some promising research directions. Our survey aims to
provide researchers with synthesis and pointer to related research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GroupViT: Semantic Segmentation Emerges from Text Supervision. (arXiv:2202.11094v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.11094">
<div class="article-summary-box-inner">
<span><p>Grouping and recognition are important components of visual scene
understanding, e.g., for object detection and semantic segmentation. With
end-to-end deep learning systems, grouping of image regions usually happens
implicitly via top-down supervision from pixel-level recognition labels.
Instead, in this paper, we propose to bring back the grouping mechanism into
deep networks, which allows semantic segments to emerge automatically with only
text supervision. We propose a hierarchical Grouping Vision Transformer
(GroupViT), which goes beyond the regular grid structure representation and
learns to group image regions into progressively larger arbitrary-shaped
segments. We train GroupViT jointly with a text encoder on a large-scale
image-text dataset via contrastive losses. With only text supervision and
without any pixel-level annotations, GroupViT learns to group together semantic
regions and successfully transfers to the task of semantic segmentation in a
zero-shot manner, i.e., without any further fine-tuning. It achieves a
zero-shot accuracy of 52.3% mIoU on the PASCAL VOC 2012 and 22.4% mIoU on
PASCAL Context datasets, and performs competitively to state-of-the-art
transfer-learning methods requiring greater levels of supervision. We
open-source our code at https://github.com/NVlabs/GroupViT .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LF-VIO: A Visual-Inertial-Odometry Framework for Large Field-of-View Cameras with Negative Plane. (arXiv:2202.12613v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12613">
<div class="article-summary-box-inner">
<span><p>Visual-inertial-odometry has attracted extensive attention in the field of
autonomous driving and robotics. The size of Field of View (FoV) plays an
important role in Visual-Odometry (VO) and Visual-Inertial-Odometry (VIO), as a
large FoV enables to perceive a wide range of surrounding scene elements and
features. However, when the field of the camera reaches the negative half
plane, one cannot simply use [u,v,1]^T to represent the image feature points
anymore. To tackle this issue, we propose LF-VIO, a real-time VIO framework for
cameras with extremely large FoV. We leverage a three-dimensional vector with
unit length to represent feature points, and design a series of algorithms to
overcome this challenge. To address the scarcity of panoramic visual odometry
datasets with ground-truth location and pose, we present the PALVIO dataset,
collected with a Panoramic Annular Lens (PAL) system with an entire FoV of
360{\deg}x(40{\deg}-120{\deg}) and an IMU sensor. With a comprehensive variety
of experiments, the proposed LF-VIO is verified on both the established PALVIO
benchmark and a public fisheye camera dataset with a FoV of
360{\deg}x(0{\deg}-93.5{\deg}). LF-VIO outperforms state-of-the-art
visual-inertial-odometry methods. Our dataset and code are made publicly
available at https://github.com/flysoaryun/LF-VIO
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discriminability-Transferability Trade-Off: An Information-Theoretic Perspective. (arXiv:2203.03871v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03871">
<div class="article-summary-box-inner">
<span><p>This work simultaneously considers the discriminability and transferability
properties of deep representations in the typical supervised learning task,
i.e., image classification. By a comprehensive temporal analysis, we observe a
trade-off between these two properties. The discriminability keeps increasing
with the training progressing while the transferability intensely diminishes in
the later training period.
</p>
<p>From the perspective of information-bottleneck theory, we reveal that the
incompatibility between discriminability and transferability is attributed to
the over-compression of input information. More importantly, we investigate why
and how the InfoNCE loss can alleviate the over-compression, and further
present a learning framework, named contrastive temporal coding~(CTC), to
counteract the over-compression and alleviate the incompatibility. Extensive
experiments validate that CTC successfully mitigates the incompatibility,
yielding discriminative and transferable representations. Noticeable
improvements are achieved on the image classification task and challenging
transfer learning tasks. We hope that this work will raise the significance of
the transferability property in the conventional supervised learning setting.
Code is available at https://github.com/DTennant/dt-tradeoff.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GaitEdge: Beyond Plain End-to-end Gait Recognition for Better Practicality. (arXiv:2203.03972v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03972">
<div class="article-summary-box-inner">
<span><p>Gait is one of the most promising biometrics to identify individuals at a
long distance. Although most previous methods have focused on recognizing the
silhouettes, several end-to-end methods that extract gait features directly
from RGB images perform better. However, we demonstrate that these end-to-end
methods may inevitably suffer from the gait-irrelevant noises, i.e., low-level
texture and colorful information. Experimentally, we design the cross-domain
evaluation to support this view. In this work, we propose a novel end-to-end
framework named GaitEdge which can effectively block gait-irrelevant
information and release end-to-end training potential. Specifically, GaitEdge
synthesizes the output of the pedestrian segmentation network and then feeds it
to the subsequent recognition network, where the synthetic silhouettes consist
of trainable edges of bodies and fixed interiors to limit the information that
the recognition network receives. Besides, GaitAlign for aligning silhouettes
is embedded into the GaitEdge without losing differentiability. Experimental
results on CASIA-B and our newly built TTG-200 indicate that GaitEdge
significantly outperforms the previous methods and provides a more practical
end-to-end paradigm. All the source code are available at
https://github.com/ShiqiYu/OpenGait.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Backbone is All Your Need: A Simplified Architecture for Visual Object Tracking. (arXiv:2203.05328v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05328">
<div class="article-summary-box-inner">
<span><p>Exploiting a general-purpose neural architecture to replace hand-wired
designs or inductive biases has recently drawn extensive interest. However,
existing tracking approaches rely on customized sub-modules and need prior
knowledge for architecture selection, hindering the tracking development in a
more general system. This paper presents a Simplified Tracking architecture
(SimTrack) by leveraging a transformer backbone for joint feature extraction
and interaction. Unlike existing Siamese trackers, we serialize the input
images and concatenate them directly before the one-branch backbone. Feature
interaction in the backbone helps to remove well-designed interaction modules
and produce a more efficient and effective framework. To reduce the information
loss from down-sampling in vision transformers, we further propose a foveal
window strategy, providing more diverse input patches with acceptable
computational costs. Our SimTrack improves the baseline with 2.5%/2.6% AUC
gains on LaSOT/TNL2K and gets results competitive with other specialized
tracking algorithms without bells and whistles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prediction-Guided Distillation for Dense Object Detection. (arXiv:2203.05469v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05469">
<div class="article-summary-box-inner">
<span><p>Real-world object detection models should be cheap and accurate. Knowledge
distillation (KD) can boost the accuracy of a small, cheap detection model by
leveraging useful information from a larger teacher model. However, a key
challenge is identifying the most informative features produced by the teacher
for distillation. In this work, we show that only a very small fraction of
features within a ground-truth bounding box are responsible for a teacher's
high detection performance. Based on this, we propose Prediction-Guided
Distillation (PGD), which focuses distillation on these key predictive regions
of the teacher and yields considerable gains in performance over many existing
KD baselines. In addition, we propose an adaptive weighting scheme over the key
regions to smooth out their influence and achieve even better performance. Our
proposed approach outperforms current state-of-the-art KD baselines on a
variety of advanced one-stage detection architectures. Specifically, on the
COCO dataset, our method achieves between +3.1% and +4.6% AP improvement using
ResNet-101 and ResNet-50 as the teacher and student backbones, respectively. On
the CrowdHuman dataset, we achieve +3.2% and +2.0% improvements in MR and AP,
also using these backbones. Our code is available at
https://github.com/ChenhongyiYang/PGD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VPFusion: Joint 3D Volume and Pixel-Aligned Feature Fusion for Single and Multi-view 3D Reconstruction. (arXiv:2203.07553v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07553">
<div class="article-summary-box-inner">
<span><p>We introduce a unified single and multi-view neural implicit 3D
reconstruction framework VPFusion. VPFusion attains high-quality reconstruction
using both - 3D feature volume to capture 3D-structure-aware context, and
pixel-aligned image features to capture fine local detail. Existing approaches
use RNN, feature pooling, or attention computed independently in each view for
multi-view fusion. RNNs suffer from long-term memory loss and permutation
variance, while feature pooling or independently computed attention leads to
representation in each view being unaware of other views before the final
pooling step. In contrast, we show improved multi-view feature fusion by
establishing transformer-based pairwise view association. In particular, we
propose a novel interleaved 3D reasoning and pairwise view association
architecture for feature volume fusion across different views. Using this
structure-aware and multi-view-aware feature volume, we show improved 3D
reconstruction performance compared to existing methods. VPFusion improves the
reconstruction quality further by also incorporating pixel-aligned local image
features to capture fine detail. We verify the effectiveness of VPFusion on the
ShapeNet and ModelNet datasets, where we outperform or perform on-par the
state-of-the-art single and multi-view 3D shape reconstruction methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inverted Pyramid Multi-task Transformer for Dense Scene Understanding. (arXiv:2203.07997v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07997">
<div class="article-summary-box-inner">
<span><p>Multi-task dense scene understanding is a thriving research domain that
requires simultaneous perception and reasoning on a series of correlated tasks
with pixel-wise prediction. Most existing works encounter a severe limitation
of modeling in the locality due to heavy utilization of convolution operations,
while learning interactions and inference in a global spatial-position and
multi-task context is critical for this problem. In this paper, we propose a
novel end-to-end Inverted Pyramid multi-task Transformer (InvPT) to perform
simultaneous modeling of spatial positions and multiple tasks in a unified
framework. To the best of our knowledge, this is the first work that explores
designing a transformer structure for multi-task dense prediction for scene
understanding. Besides, it is widely demonstrated that a higher spatial
resolution is remarkably beneficial for dense predictions, while it is very
challenging for existing transformers to go deeper with higher resolutions due
to huge complexity to large spatial size. InvPT presents an efficient
UP-Transformer block to learn multi-task feature interaction at gradually
increased resolutions, which also incorporates effective self-attention message
passing and multi-scale feature aggregation to produce task-specific prediction
at a high resolution. Our method achieves superior multi-task performance on
NYUD-v2 and PASCAL-Context datasets respectively, and significantly outperforms
previous state-of-the-arts. The code is available at
https://github.com/prismformore/InvPT
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perspective Flow Aggregation for Data-Limited 6D Object Pose Estimation. (arXiv:2203.09836v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09836">
<div class="article-summary-box-inner">
<span><p>Most recent 6D object pose estimation methods, including unsupervised ones,
require many real training images. Unfortunately, for some applications, such
as those in space or deep under water, acquiring real images, even unannotated,
is virtually impossible. In this paper, we propose a method that can be trained
solely on synthetic images, or optionally using a few additional real ones.
Given a rough pose estimate obtained from a first network, it uses a second
network to predict a dense 2D correspondence field between the image rendered
using the rough pose and the real image and infers the required pose
correction. This approach is much less sensitive to the domain shift between
synthetic and real images than state-of-the-art methods. It performs on par
with methods that require annotated real images for training when not using
any, and outperforms them considerably when using as few as twenty real images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FAR: Fourier Aerial Video Recognition. (arXiv:2203.10694v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10694">
<div class="article-summary-box-inner">
<span><p>We present an algorithm, Fourier Activity Recognition (FAR), for UAV video
activity recognition. Our formulation uses a novel Fourier object
disentanglement method to innately separate out the human agent (which is
typically small) from the background. Our disentanglement technique operates in
the frequency domain to characterize the extent of temporal change of spatial
pixels, and exploits convolution-multiplication properties of Fourier transform
to map this representation to the corresponding object-background entangled
features obtained from the network. To encapsulate contextual information and
long-range space-time dependencies, we present a novel Fourier Attention
algorithm, which emulates the benefits of self-attention by modeling the
weighted outer product in the frequency domain. Our Fourier attention
formulation uses much fewer computations than self-attention. We have evaluated
our approach on multiple UAV datasets including UAV Human RGB, UAV Human Night,
Drone Action, and NEC Drone. We demonstrate a relative improvement of 8.02% -
38.69% in top-1 accuracy and up to 3 times faster over prior works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compression of Generative Pre-trained Language Models via Quantization. (arXiv:2203.10705v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10705">
<div class="article-summary-box-inner">
<span><p>The increasing size of generative Pre-trained Language Models (PLMs) has
greatly increased the demand for model compression. Despite various methods to
compress BERT or its variants, there are few attempts to compress generative
PLMs, and the underlying difficulty remains unclear. In this paper, we compress
generative PLMs by quantization. We find that previous quantization methods
fail on generative tasks due to the \textit{homogeneous word embeddings} caused
by reduced capacity, and \textit{varied distribution of weights}.
Correspondingly, we propose a token-level contrastive distillation to learn
distinguishable word embeddings, and a module-wise dynamic scaling to make
quantizers adaptive to different modules. Empirical results on various tasks
show that our proposed method outperforms the state-of-the-art compression
methods on generative PLMs by a clear margin. With comparable performance with
the full-precision models, we achieve 14.4x and 13.4x compression rates on
GPT-2 and BART, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ScalableViT: Rethinking the Context-oriented Generalization of Vision Transformer. (arXiv:2203.10790v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10790">
<div class="article-summary-box-inner">
<span><p>The vanilla self-attention mechanism inherently relies on pre-defined and
steadfast computational dimensions. Such inflexibility restricts it from
possessing context-oriented generalization that can bring more contextual cues
and global representations. To mitigate this issue, we propose a Scalable
Self-Attention (SSA) mechanism that leverages two scaling factors to release
dimensions of query, key, and value matrices while unbinding them with the
input. This scalability fetches context-oriented generalization and enhances
object sensitivity, which pushes the whole network into a more effective
trade-off state between accuracy and cost. Furthermore, we propose an
Interactive Window-based Self-Attention (IWSA), which establishes interaction
between non-overlapping regions by re-merging independent value tokens and
aggregating spatial information from adjacent windows. By stacking the SSA and
IWSA alternately, the Scalable Vision Transformer (ScalableViT) achieves
state-of-the-art performance in general-purpose vision tasks. For example,
ScalableViT-S outperforms Twins-SVT-S by 1.4% and Swin-T by 1.8% on ImageNet-1K
classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Making Heads or Tails: Towards Semantically Consistent Visual Counterfactuals. (arXiv:2203.12892v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12892">
<div class="article-summary-box-inner">
<span><p>A visual counterfactual explanation replaces image regions in a query image
with regions from a distractor image such that the system's decision on the
transformed image changes to the distractor class. In this work, we present a
novel framework for computing visual counterfactual explanations based on two
key ideas. First, we enforce that the replaced and replacer regions contain the
same semantic part, resulting in more semantically consistent explanations.
Second, we use multiple distractor images in a computationally efficient way
and obtain more discriminative explanations with fewer region replacements. Our
approach is 27 % more semantically consistent and an order of magnitude faster
than a competing method on three fine-grained image recognition datasets. We
highlight the utility of our counterfactuals over existing works through
machine teaching experiments where we teach humans to classify different bird
species. We also complement our explanations with the vocabulary of parts and
attributes that contributed the most to the system's decision. In this task as
well, we obtain state-of-the-art results when using our counterfactual
explanations relative to existing works, reinforcing the importance of
semantically consistent explanations. Source code is available at
https://github.com/facebookresearch/visual-counterfactuals.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Perturbation-Constrained Adversarial Attack for Evaluating the Robustness of Optical Flow. (arXiv:2203.13214v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.13214">
<div class="article-summary-box-inner">
<span><p>Recent optical flow methods are almost exclusively judged in terms of
accuracy, while their robustness is often neglected. Although adversarial
attacks offer a useful tool to perform such an analysis, current attacks on
optical flow methods focus on real-world attacking scenarios rather than a
worst case robustness assessment. Hence, in this work, we propose a novel
adversarial attack - the Perturbation-Constrained Flow Attack (PCFA) - that
emphasizes destructivity over applicability as a real-world attack. PCFA is a
global attack that optimizes adversarial perturbations to shift the predicted
flow towards a specified target flow, while keeping the L2 norm of the
perturbation below a chosen bound. Our experiments demonstrate PCFA's
applicability in white- and black-box settings, and show it finds stronger
adversarial samples than previous attacks. Based on these strong samples, we
provide the first joint ranking of optical flow methods considering both
prediction quality and adversarial robustness, which reveals state-of-the-art
methods to be particularly vulnerable. Code is available at
https://github.com/cv-stuttgart/PCFA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PromptDet: Towards Open-vocabulary Detection using Uncurated Images. (arXiv:2203.16513v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16513">
<div class="article-summary-box-inner">
<span><p>The goal of this work is to establish a scalable pipeline for expanding an
object detector towards novel/unseen categories, using zero manual annotations.
To achieve that, we make the following four contributions: (i) in pursuit of
generalisation, we propose a two-stage open-vocabulary object detector, where
the class-agnostic object proposals are classified with a text encoder from
pre-trained visual-language model; (ii) To pair the visual latent space (of RPN
box proposals) with that of the pre-trained text encoder, we propose the idea
of regional prompt learning to align the textual embedding space with regional
visual object features; (iii) To scale up the learning procedure towards
detecting a wider spectrum of objects, we exploit the available online resource
via a novel self-training framework, which allows to train the proposed
detector on a large corpus of noisy uncurated web images. Lastly, (iv) to
evaluate our proposed detector, termed as PromptDet, we conduct extensive
experiments on the challenging LVIS and MS-COCO dataset. PromptDet shows
superior performance over existing approaches with fewer additional training
images and zero manual annotations whatsoever. Project page with code:
https://fcjian.github.io/promptdet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Point Scene Understanding via Disentangled Instance Mesh Reconstruction. (arXiv:2203.16832v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.16832">
<div class="article-summary-box-inner">
<span><p>Semantic scene reconstruction from point cloud is an essential and
challenging task for 3D scene understanding. This task requires not only to
recognize each instance in the scene, but also to recover their geometries
based on the partial observed point cloud. Existing methods usually attempt to
directly predict occupancy values of the complete object based on incomplete
point cloud proposals from a detection-based backbone. However, this framework
always fails to reconstruct high fidelity mesh due to the obstruction of
various detected false positive object proposals and the ambiguity of
incomplete point observations for learning occupancy values of complete
objects. To circumvent the hurdle, we propose a Disentangled Instance Mesh
Reconstruction (DIMR) framework for effective point scene understanding. A
segmentation-based backbone is applied to reduce false positive object
proposals, which further benefits our exploration on the relationship between
recognition and reconstruction. Based on the accurate proposals, we leverage a
mesh-aware latent code space to disentangle the processes of shape completion
and mesh generation, relieving the ambiguity caused by the incomplete point
observations. Furthermore, with access to the CAD model pool at test time, our
model can also be used to improve the reconstruction quality by performing mesh
retrieval without extra training. We thoroughly evaluate the reconstructed mesh
quality with multiple metrics, and demonstrate the superiority of our method on
the challenging ScanNet dataset. Code is available at
\url{https://github.com/ashawkey/dimr}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Drive by Watching YouTube Videos: Action-Conditioned Contrastive Policy Pretraining. (arXiv:2204.02393v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02393">
<div class="article-summary-box-inner">
<span><p>Deep visuomotor policy learning, which aims to map raw visual observation to
action, achieves promising results in control tasks such as robotic
manipulation and autonomous driving. However, it requires a huge number of
online interactions with the training environment, which limits its real-world
application. Compared to the popular unsupervised feature learning for visual
recognition, feature pretraining for visuomotor control tasks is much less
explored. In this work, we aim to pretrain policy representations for driving
tasks by watching hours-long uncurated YouTube videos. Specifically, we train
an inverse dynamic model with a small amount of labeled data and use it to
predict action labels for all the YouTube video frames. A new contrastive
policy pretraining method is then developed to learn action-conditioned
features from the video frames with pseudo action labels. Experiments show that
the resulting action-conditioned features obtain substantial improvements for
the downstream reinforcement learning and imitation learning tasks,
outperforming the weights pretrained from previous unsupervised learning
methods and ImageNet pretrained weight. Code, model weights, and data are
available at: https://metadriverse.github.io/ACO.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BMD: A General Class-balanced Multicentric Dynamic Prototype Strategy for Source-free Domain Adaptation. (arXiv:2204.02811v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02811">
<div class="article-summary-box-inner">
<span><p>Source-free Domain Adaptation (SFDA) aims to adapt a pre-trained source model
to the unlabeled target domain without accessing the well-labeled source data,
which is a much more practical setting due to the data privacy, security, and
transmission issues. To make up for the absence of source data, most existing
methods introduced feature prototype based pseudo-labeling strategies to
realize self-training model adaptation. However, feature prototypes are
obtained by instance-level predictions based feature clustering, which is
category-biased and tends to result in noisy labels since the visual domain
gaps between source and target are usually different between categories. In
addition, we found that a monocentric feature prototype may be ineffective to
represent each category and introduce negative transfer, especially for those
hard-transfer data. To address these issues, we propose a general
class-Balanced Multicentric Dynamic prototype (BMD) strategy for the SFDA task.
Specifically, for each target category, we first introduce a global inter-class
balanced sampling strategy to aggregate potential representative target
samples. Then, we design an intra-class multicentric clustering strategy to
achieve more robust and representative prototypes generation. In contrast to
existing strategies that update the pseudo label at a fixed training period, we
further introduce a dynamic pseudo labeling strategy to incorporate network
update information during model adaptation. Extensive experiments show that the
proposed model-agnostic BMD strategy significantly improves representative SFDA
methods to yield new state-of-the-art results. The code is available at
https://github.com/ispc-lab/BMD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChildCI Framework: Analysis of Motor and Cognitive Development in Children-Computer Interaction for Age Detection. (arXiv:2204.04236v2 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04236">
<div class="article-summary-box-inner">
<span><p>This article presents a comprehensive analysis of the different tests
proposed in the recent ChildCI framework, proving its potential for generating
a better understanding of children's neuromotor and cognitive development along
time, as well as their possible application in other research areas such as
e-Health and e-Learning. In particular, we propose a set of over 100 global
features related to motor and cognitive aspects of the children interaction
with mobile devices, some of them collected and adapted from the literature.
Furthermore, we analyse the robustness and discriminative power of the proposed
feature set including experimental results for the task of children age group
detection based on their motor and cognitive behaviors. Two different scenarios
are considered in this study: i) single-test scenario, and ii) multiple-test
scenario. Results over 93% accuracy are achieved using the publicly available
ChildCIdb_v1 database (over 400 children from 18 months to 8 years old),
proving the high correlation of children's age with the way they interact with
mobile devices.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Convolutional Neural Networks in the Frequency Domain. (arXiv:2204.06718v9 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.06718">
<div class="article-summary-box-inner">
<span><p>Convolutional neural network (CNN) has achieved impressive success in
computer vision during the past few decades. The image convolution operation
helps CNNs to get good performance on image-related tasks. However, the image
convolution has high computation complexity and hard to be implemented. This
paper proposes the CEMNet, which can be trained in the frequency domain. The
most important motivation of this research is that we can use the
straightforward element-wise multiplication operation to replace the image
convolution in the frequency domain based on the Cross-Correlation Theorem,
which obviously reduces the computation complexity. We further introduce a
Weight Fixation mechanism to alleviate the problem of over-fitting, and analyze
the working behavior of Batch Normalization, Leaky ReLU, and Dropout in the
frequency domain to design their counterparts for CEMNet. Also, to deal with
complex inputs brought by Discrete Fourier Transform, we design a two-branches
network structure for CEMNet. Experimental results imply that CEMNet achieves
good performance on MNIST and CIFAR-10 databases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Two Decades of Colorization and Decolorization for Images and Videos. (arXiv:2204.13322v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13322">
<div class="article-summary-box-inner">
<span><p>Colorization is a computer-aided process, which aims to give color to a gray
image or video. It can be used to enhance black-and-white images, including
black-and-white photos, old-fashioned films, and scientific imaging results. On
the contrary, decolorization is to convert a color image or video into a
grayscale one. A grayscale image or video refers to an image or video with only
brightness information without color information. It is the basis of some
downstream image processing applications such as pattern recognition, image
segmentation, and image enhancement. Different from image decolorization, video
decolorization should not only consider the image contrast preservation in each
video frame, but also respect the temporal and spatial consistency between
video frames. Researchers were devoted to develop decolorization methods by
balancing spatial-temporal consistency and algorithm efficiency. With the
prevalance of the digital cameras and mobile phones, image and video
colorization and decolorization have been paid more and more attention by
researchers. This paper gives an overview of the progress of image and video
colorization and decolorization methods in the last two decades.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Overview of Color Transfer and Style Transfer for Images and Videos. (arXiv:2204.13339v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.13339">
<div class="article-summary-box-inner">
<span><p>Image or video appearance features (e.g., color, texture, tone, illumination,
and so on) reflect one's visual perception and direct impression of an image or
video. Given a source image (video) and a target image (video), the image
(video) color transfer technique aims to process the color of the source image
or video (note that the source image or video is also referred to the reference
image or video in some literature) to make it look like that of the target
image or video, i.e., transferring the appearance of the target image or video
to that of the source image or video, which can thereby change one's perception
of the source image or video. As an extension of color transfer, style transfer
refers to rendering the content of a target image or video in the style of an
artist with either a style sample or a set of images through a style transfer
model. As an emerging field, the study of style transfer has attracted the
attention of a large number of researchers. After decades of development, it
has become a highly interdisciplinary research with a variety of artistic
expression styles can be achieved. This paper provides an overview of color
transfer and style transfer methods over the past years.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Retrieve Videos by Asking Questions. (arXiv:2205.05739v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.05739">
<div class="article-summary-box-inner">
<span><p>The majority of traditional text-to-video retrieval systems operate in static
environments, i.e., there is no interaction between the user and the agent
beyond the initial textual query provided by the user. This can be sub-optimal
if the initial query has ambiguities, which would lead to many falsely
retrieved videos. To overcome this limitation, we propose a novel framework for
Video Retrieval using Dialog (ViReD), which enables the user to interact with
an AI agent via multiple rounds of dialog, where the user refines retrieved
results by answering questions generated by an AI agent. Our novel multimodal
question generator learns to ask questions that maximize the subsequent video
retrieval performance using (i) the video candidates retrieved during the last
round of interaction with the user and (ii) the text-based dialog history
documenting all previous interactions, to generate questions that incorporate
both visual and linguistic cues relevant to video retrieval. Furthermore, to
generate maximally informative questions, we propose an Information-Guided
Supervision (IGS), which guides the question generator to ask questions that
would boost subsequent video retrieval accuracy. We validate the effectiveness
of our interactive ViReD framework on the AVSD dataset, showing that our
interactive method performs significantly better than traditional
non-interactive video retrieval systems. We also demonstrate that our proposed
approach generalizes to the real-world settings that involve interactions with
real humans, thus, demonstrating the robustness and generality of our framework
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-mentoring: a new deep learning pipeline to train a self-supervised U-net for few-shot learning of bio-artificial capsule segmentation. (arXiv:2205.10840v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.10840">
<div class="article-summary-box-inner">
<span><p>Background: Accurate segmentation of microscopic structures such as
bio-artificial capsules in microscopy imaging is a prerequisite to the
computer-aided understanding of important biomechanical phenomenons.
State-of-the-art segmentation performances are achieved by deep neural networks
and related data-driven approaches. Training these networks from only a few
annotated examples is challenging while producing manually annotated images
that provide supervision is tedious.
</p>
<p>Method: Recently, self-supervision, i.e. designing a neural pipeline
providing synthetic or indirect supervision, has proved to significantly
increase generalization performances of models trained on few shots. The
objective of this paper is to introduce one such neural pipeline in the context
of micro-capsule image segmentation. Our method leverages the rather simple
content of these images so that a trainee network can be mentored by a referee
network which has been previously trained on synthetically generated pairs of
corrupted/correct region masks.
</p>
<p>Results: Challenging experimental setups are investigated. They involve from
only 3 to 10 annotated images along with moderately large amounts of
unannotated images. In a bio-artificial capsule dataset, our approach
consistently and drastically improves accuracy. We also show that the learnt
referee network is transferable to another Glioblastoma cell dataset and that
it can be efficiently coupled with data augmentation strategies.
</p>
<p>Conclusions: Experimental results show that very significant accuracy
increments are obtained by the proposed pipeline, leading to the conclusion
that the self-supervision mechanism introduced in this paper has the potential
to replace human annotations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SelfReformer: Self-Refined Network with Transformer for Salient Object Detection. (arXiv:2205.11283v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.11283">
<div class="article-summary-box-inner">
<span><p>The global and local contexts significantly contribute to the integrity of
predictions in Salient Object Detection (SOD). Unfortunately, existing methods
still struggle to generate complete predictions with fine details. There are
two major problems in conventional approaches: first, for global context,
high-level CNN-based encoder features cannot effectively catch long-range
dependencies, resulting in incomplete predictions. Second, downsampling the
ground truth to fit the size of predictions will introduce inaccuracy as the
ground truth details are lost during interpolation or pooling. Thus, in this
work, we developed a Transformer-based network and framed a supervised task for
a branch to learn the global context information explicitly. Besides, we adopt
Pixel Shuffle from Super-Resolution (SR) to reshape the predictions back to the
size of ground truth instead of the reverse. Thus details in the ground truth
are untouched. In addition, we developed a two-stage Context Refinement Module
(CRM) to fuse global context and automatically locate and refine the local
details in the predictions. The proposed network can guide and correct itself
based on the global and local context generated, thus is named, Self-Refined
Transformer (SelfReformer). Extensive experiments and evaluation results on
five benchmark datasets demonstrate the outstanding performance of the network,
and we achieved the state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Task Learning with Multi-Query Transformer for Dense Prediction. (arXiv:2205.14354v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.14354">
<div class="article-summary-box-inner">
<span><p>Previous multi-task dense prediction studies developed complex pipelines such
as multi-modal distillations in multiple stages or searching for task
relational contexts for each task. The core insight beyond these methods is to
maximize the mutual effects between each task. Inspired by the recent
query-based Transformers, we propose a simpler pipeline named Multi-Query
Transformer (MQTransformer) that is equipped with multiple queries from
different tasks to facilitate the reasoning among multiple tasks and simplify
the cross task pipeline. Instead of modeling the dense per-pixel context among
different tasks, we seek a task-specific proxy to perform cross-task reasoning
via multiple queries where each query encodes the task-related context. The
MQTransformer is composed of three key components: shared encoder, cross task
attention and shared decoder. We first model each task with a task-relevant and
scale-aware query, and then both the image feature output by the feature
extractor and the task-relevant query feature are fed into the shared encoder,
thus encoding the query feature from the image feature. Secondly, we design a
cross task attention module to reason the dependencies among multiple tasks and
feature scales from two perspectives including different tasks of the same
scale and different scales of the same task. Then we use a shared decoder to
gradually refine the image features with the reasoned query features from
different tasks. Extensive experiment results on two dense prediction datasets
(NYUD-v2 and PASCAL-Context) show that the proposed method is an effective
approach and achieves the state-of-the-art result.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Co-Training for Unsupervised Domain Adaptation of Semantic Segmentation Models. (arXiv:2205.15781v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2205.15781">
<div class="article-summary-box-inner">
<span><p>Semantic image segmentation is addressed by training deep models. Since
supervised training draws to a curse of human-based image labeling, using
synthetic images with automatically generated ground truth together with
unlabeled real-world images is a promising alternative. This implies to address
an unsupervised domain adaptation (UDA) problem. In this paper, we proposed a
new co-training process for synth-to-real UDA of semantic segmentation models.
First, we design a self-training procedure which provides two initial models.
Then, we keep training these models in a collaborative manner for obtaining the
final model. The overall process treats the deep models as black boxes and
drives their collaboration at the level of pseudo-labeled target images, i.e.,
neither modifying loss functions is required, nor explicit feature alignment.
We test our proposal on standard synthetic and real-world datasets. Our
co-training shows improvements of 15-20 percentage points of mIoU over
baselines, so establishing new state-of-the-art results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Invariant Visual Representations for Compositional Zero-Shot Learning. (arXiv:2206.00415v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.00415">
<div class="article-summary-box-inner">
<span><p>Compositional Zero-Shot Learning (CZSL) aims to recognize novel compositions
using knowledge learned from seen attribute-object compositions in the training
set. Previous works mainly project an image and a composition into a common
embedding space to measure their compatibility score. However, both attributes
and objects share the visual representations learned above, leading the model
to exploit spurious correlations and bias towards seen pairs. Instead, we
reconsider CZSL as an out-of-distribution generalization problem. If an object
is treated as a domain, we can learn object-invariant features to recognize the
attributes attached to any object reliably. Similarly, attribute-invariant
features can also be learned when recognizing the objects with attributes as
domains. Specifically, we propose an invariant feature learning framework to
align different domains at the representation and gradient levels to capture
the intrinsic characteristics associated with the tasks. Experiments on two
CZSL benchmarks demonstrate that the proposed method significantly outperforms
the previous state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Turning a Curse Into a Blessing: Enabling Clean-Data-Free Defenses by Model Inversion. (arXiv:2206.07018v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.07018">
<div class="article-summary-box-inner">
<span><p>It is becoming increasingly common to utilize pre-trained models provided by
third parties due to their convenience. At the same time, however, these models
may be vulnerable to both poisoning and evasion attacks. We introduce an
algorithmic framework that can mitigate potential security vulnerabilities in a
pre-trained model when clean data from its training distribution is unavailable
to the defender. The framework reverse-engineers samples from a given
pre-trained model. The resulting synthetic samples can then be used as a
substitute for clean data to perform various defenses. We consider two
important attack scenarios -- backdoor attacks and evasion attacks -- to
showcase the utility of synthesized samples. For both attacks, we show that
when supplied with our synthetic data, the state-of-the-art defenses perform
comparably or sometimes even better than the case when it's supplied with the
same amount of clean data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GAN2X: Non-Lambertian Inverse Rendering of Image GANs. (arXiv:2206.09244v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.09244">
<div class="article-summary-box-inner">
<span><p>2D images are observations of the 3D physical world depicted with the
geometry, material, and illumination components. Recovering these underlying
intrinsic components from 2D images, also known as inverse rendering, usually
requires a supervised setting with paired images collected from multiple
viewpoints and lighting conditions, which is resource-demanding. In this work,
we present GAN2X, a new method for unsupervised inverse rendering that only
uses unpaired images for training. Unlike previous Shape-from-GAN approaches
that mainly focus on 3D shapes, we take the first attempt to also recover
non-Lambertian material properties by exploiting the pseudo paired data
generated by a GAN. To achieve precise inverse rendering, we devise a
specularity-aware neural surface representation that continuously models the
geometry and material properties. A shading-based refinement technique is
adopted to further distill information in the target image and recover more
fine details. Experiments demonstrate that GAN2X can accurately decompose 2D
images to 3D shape, albedo, and specular properties for different object
categories, and achieves the state-of-the-art performance for unsupervised
single-view 3D face reconstruction. We also show its applications in downstream
tasks including real image editing and lifting 2D GANs to decomposed 3D GANs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GNN-PMB: A Simple but Effective Online 3D Multi-Object Tracker without Bells and Whistles. (arXiv:2206.10255v3 [eess.SY] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.10255">
<div class="article-summary-box-inner">
<span><p>Multi-object tracking (MOT) is among crucial applications in modern advanced
driver assistance systems (ADAS) and autonomous driving (AD) systems. Global
nearest neighbor (GNN) filter, as the earliest random vector Bayesian tracking
framework, has been adopted in most of state-of-the-arts trackers and widely
accepted in the automotive industry. With the development of random finite set
(RFS) theory, the RFS Bayesian filters have been applied in MOT tasks recently.
However, their usefulness in the real traffic for ADAS and AD application is
still open to doubt. In this paper, we firstly demonstrate the latest RFS
Bayesian tracking framework could be superior to typical random vector Bayesian
tracking framework like GNN, via a systematic comparative study of both
traditional random vector Bayesian filters with rule-based heuristic track
maintenance and RFS Bayesian filters on nuScenes validation dataset. Then, we
propose a RFS-based tracker, namely Poisson multi-Bernoulli filter using the
global nearest neighbor (GNN-PMB), for LiDAR-based MOT tasks. This GNN-PMB
tracker is simple to use but can achieve competitive results on nuScenes
dataset. Specifically, the proposed GNN-PMB tracker outperforms most of the
state-of-the-art LiDAR-only trackers and LiDAR and camera fusion-based
trackers, ranks the 3rd among all LiDAR-only trackers on nuScenes tracking task
leader board1 at the time of submission. Our code is available at
https://github.com/chisyliu/gnn pmb tracker.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SearchMorph:Multi-scale Correlation Iterative Network for Deformable Registration. (arXiv:2206.13076v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2206.13076">
<div class="article-summary-box-inner">
<span><p>Deformable image registration can obtain dynamic information about images,
which is of great significance in medical image analysis. The unsupervised deep
learning registration method can quickly achieve high registration accuracy
without labels. However, these methods generally suffer from uncorrelated
features, poor ability to register large deformations and details, and
unnatural deformation fields. To address the issues above, we propose an
unsupervised multi-scale correlation iterative registration network
(SearchMorph). In the proposed network, we introduce a correlation layer to
strengthen the relevance between features and construct a correlation pyramid
to provide multi-scale relevance information for the network. We also design a
deformation field iterator, which improves the ability of the model to register
details and large deformations through the search module and GRU while ensuring
that the deformation field is realistic. We use single-temporal brain MR images
and multi-temporal echocardiographic sequences to evaluate the model's ability
to register large deformations and details. The experimental results
demonstrate that the method in this paper achieves the highest registration
accuracy and the lowest folding point ratio using a short elapsed time to
state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CAM/CAD Point Cloud Part Segmentation via Few-Shot Learning. (arXiv:2207.01218v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01218">
<div class="article-summary-box-inner">
<span><p>3D part segmentation is an essential step in advanced CAM/CAD workflow.
Precise 3D segmentation contributes to lower defective rate of work-pieces
produced by the manufacturing equipment (such as computer controlled CNCs),
thereby improving work efficiency and attaining the attendant economic
benefits. A large class of existing works on 3D model segmentation are mostly
based on fully-supervised learning, which trains the AI models with large,
annotated datasets. However, the disadvantage is that the resulting models from
the fully-supervised learning methodology are highly reliant on the
completeness of the available dataset, and its generalization ability is
relatively poor to new unknown segmentation types (i.e. further additional
novel classes). In this work, we propose and develop a noteworthy few-shot
learning-based approach for effective part segmentation in CAM/CAD; and this is
designed to significantly enhance its generalization ability and flexibly adapt
to new segmentation tasks by using only relatively rather few samples. As a
result, it not only reduces the requirements for the usually unattainable and
exhaustive completeness of supervision datasets, but also improves the
flexibility for real-world applications. As further improvement and innovation,
we additionally adopt the transform net and the center loss block in the
network. These characteristics serve to improve the comprehension for 3D
features of the various possible instances of the whole work-piece and ensure
the close distribution of the same class in feature space.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Embedding contrastive unsupervised features to cluster in- and out-of-distribution noise in corrupted image datasets. (arXiv:2207.01573v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.01573">
<div class="article-summary-box-inner">
<span><p>Using search engines for web image retrieval is a tempting alternative to
manual curation when creating an image dataset, but their main drawback remains
the proportion of incorrect (noisy) samples retrieved. These noisy samples have
been evidenced by previous works to be a mixture of in-distribution (ID)
samples, assigned to the incorrect category but presenting similar visual
semantics to other classes in the dataset, and out-of-distribution (OOD)
images, which share no semantic correlation with any category from the dataset.
The latter are, in practice, the dominant type of noisy images retrieved. To
tackle this noise duality, we propose a two stage algorithm starting with a
detection step where we use unsupervised contrastive feature learning to
represent images in a feature space. We find that the alignment and uniformity
principles of contrastive learning allow OOD samples to be linearly separated
from ID samples on the unit hypersphere. We then spectrally embed the
unsupervised representations using a fixed neighborhood size and apply an
outlier sensitive clustering at the class level to detect the clean and OOD
clusters as well as ID noisy outliers. We finally train a noise robust neural
network that corrects ID noise to the correct category and utilizes OOD samples
in a guided contrastive objective, clustering them to improve low-level
features. Our algorithm improves the state-of-the-art results on synthetic
noise image datasets as well as real-world web-crawled data. Our work is fully
reproducible github.com/PaulAlbert31/SNCF.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Network Binarization via Contrastive Learning. (arXiv:2207.02970v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.02970">
<div class="article-summary-box-inner">
<span><p>Neural network binarization accelerates deep models by quantizing their
weights and activations into 1-bit. However, there is still a huge performance
gap between Binary Neural Networks (BNNs) and their full-precision (FP)
counterparts. As the quantization error caused by weights binarization has been
reduced in earlier works, the activations binarization becomes the major
obstacle for further improvement of the accuracy. BNN characterises a unique
and interesting structure, where the binary and latent FP activations exist in
the same forward pass (i.e., $\text{Binarize}(\mathbf{a}_F) = \mathbf{a}_B$).
To mitigate the information degradation caused by the binarization operation
from FP to binary activations, we establish a novel contrastive learning
framework while training BNNs through the lens of Mutual Information (MI)
maximization. MI is introduced as the metric to measure the information shared
between binary and FP activations, which assists binarization with contrastive
learning. Specifically, the representation ability of the BNNs is greatly
strengthened via pulling the positive pairs with binary and FP activations from
the same input samples, as well as pushing negative pairs from different
samples (the number of negative pairs can be exponentially large). This
benefits the downstream tasks, not only classification but also segmentation
and depth estimation, etc. The experimental results show that our method can be
implemented as a pile-up module on existing state-of-the-art binarization
methods and can remarkably improve the performance over them on CIFAR-10/100
and ImageNet, in addition to the great generalization ability on NYUD-v2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Power of Transfer Learning in Agricultural Applications: AgriNet. (arXiv:2207.03881v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.03881">
<div class="article-summary-box-inner">
<span><p>Advances in deep learning and transfer learning have paved the way for
various automation classification tasks in agriculture, including plant
diseases, pests, weeds, and plant species detection. However, agriculture
automation still faces various challenges, such as the limited size of datasets
and the absence of plant-domain-specific pretrained models. Domain specific
pretrained models have shown state of art performance in various computer
vision tasks including face recognition and medical imaging diagnosis. In this
paper, we propose AgriNet dataset, a collection of 160k agricultural images
from more than 19 geographical locations, several images captioning devices,
and more than 423 classes of plant species and diseases. We also introduce
AgriNet models, a set of pretrained models on five ImageNet architectures:
VGG16, VGG19, Inception-v3, InceptionResNet-v2, and Xception. AgriNet-VGG19
achieved the highest classification accuracy of 94 % and the highest F1-score
of 92%. Additionally, all proposed models were found to accurately classify the
423 classes of plant species, diseases, pests, and weeds with a minimum
accuracy of 87% for the Inception-v3 model.Finally, experiments to evaluate of
superiority of AgriNet models compared to ImageNet models were conducted on two
external datasets: pest and plant diseases dataset from Bangladesh and a plant
diseases dataset from Kashmir.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoMER: Modeling Coverage for Transformer-based Handwritten Mathematical Expression Recognition. (arXiv:2207.04410v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04410">
<div class="article-summary-box-inner">
<span><p>The Transformer-based encoder-decoder architecture has recently made
significant advances in recognizing handwritten mathematical expressions.
However, the transformer model still suffers from the lack of coverage problem,
making its expression recognition rate (ExpRate) inferior to its RNN
counterpart. Coverage information, which records the alignment information of
the past steps, has proven effective in the RNN models. In this paper, we
propose CoMER, a model that adopts the coverage information in the transformer
decoder. Specifically, we propose a novel Attention Refinement Module (ARM) to
refine the attention weights with past alignment information without hurting
its parallelism. Furthermore, we take coverage information to the extreme by
proposing self-coverage and cross-coverage, which utilize the past alignment
information from the current and previous layers. Experiments show that CoMER
improves the ExpRate by 0.61%/2.09%/1.59% compared to the current
state-of-the-art model, and reaches 59.33%/59.81%/62.97% on the CROHME
2014/2016/2019 test sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Principles of Parsimony and Self-Consistency for the Emergence of Intelligence. (arXiv:2207.04630v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04630">
<div class="article-summary-box-inner">
<span><p>Ten years into the revival of deep networks and artificial intelligence, we
propose a theoretical framework that sheds light on understanding deep networks
within a bigger picture of Intelligence in general. We introduce two
fundamental principles, Parsimony and Self-consistency, that address two
fundamental questions regarding Intelligence: what to learn and how to learn,
respectively. We believe the two principles are the cornerstones for the
emergence of Intelligence, artificial or natural. While these two principles
have rich classical roots, we argue that they can be stated anew in entirely
measurable and computable ways. More specifically, the two principles lead to
an effective and efficient computational framework, compressive closed-loop
transcription, that unifies and explains the evolution of modern deep networks
and many artificial intelligence practices. While we mainly use modeling of
visual data as an example, we believe the two principles will unify
understanding of broad families of autonomous intelligent systems and provide a
framework for understanding the brain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Contextual Relationships for Cervical Abnormal Cell Detection. (arXiv:2207.04693v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04693">
<div class="article-summary-box-inner">
<span><p>Cervical abnormal cell detection is a challenging task as the morphological
discrepancies between abnormal and normal cells are usually subtle. To
determine whether a cervical cell is normal or abnormal, cytopathologists
always take surrounding cells as references to identify its abnormality. To
mimic these behaviors, we propose to explore contextual relationships to boost
the performance of cervical abnormal cell detection. Specifically, both
contextual relationships between cells and cell-to-global images are exploited
to enhance features of each region of interest (RoI) proposals. Accordingly,
two modules, dubbed as RoI-relationship attention module (RRAM) and global RoI
attention module (GRAM), are developed and their combination strategies are
also investigated. We establish a strong baseline by using Double-Head Faster
R-CNN with feature pyramid network (FPN) and integrate our RRAM and GRAM into
it to validate the effectiveness of the proposed modules. Experiments conducted
on a large cervical cell detection dataset reveal that the introduction of RRAM
and GRAM both achieves better average precision (AP) than the baseline methods.
Moreover, when cascading RRAM and GRAM, our method outperforms the
state-of-the-art (SOTA) methods. Furthermore, we also show the proposed feature
enhancing scheme can facilitate both image-level and smear-level
classification. The code and trained models are publicly available at
https://github.com/CVIU-CSU/CR4CACD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CCPL: Contrastive Coherence Preserving Loss for Versatile Style Transfer. (arXiv:2207.04808v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.04808">
<div class="article-summary-box-inner">
<span><p>In this paper, we aim to devise a universally versatile style transfer method
capable of performing artistic, photo-realistic, and video style transfer
jointly, without seeing videos during training. Previous single-frame methods
assume a strong constraint on the whole image to maintain temporal consistency,
which could be violated in many cases. Instead, we make a mild and reasonable
assumption that global inconsistency is dominated by local inconsistencies and
devise a generic Contrastive Coherence Preserving Loss (CCPL) applied to local
patches. CCPL can preserve the coherence of the content source during style
transfer without degrading stylization. Moreover, it owns a neighbor-regulating
mechanism, resulting in a vast reduction of local distortions and considerable
visual quality improvement. Aside from its superior performance on versatile
style transfer, it can be easily extended to other tasks, such as
image-to-image translation. Besides, to better fuse content and style features,
we propose Simple Covariance Transformation (SCT) to effectively align
second-order statistics of the content feature with the style feature.
Experiments demonstrate the effectiveness of the resulting model for versatile
style transfer, when armed with CCPL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CANF-VC: Conditional Augmented Normalizing Flows for Video Compression. (arXiv:2207.05315v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05315">
<div class="article-summary-box-inner">
<span><p>This paper presents an end-to-end learning-based video compression system,
termed CANF-VC, based on conditional augmented normalizing flows (CANF). Most
learned video compression systems adopt the same hybrid-based coding
architecture as the traditional codecs. Recent research on conditional coding
has shown the sub-optimality of the hybrid-based coding and opens up
opportunities for deep generative models to take a key role in creating new
coding frameworks. CANF-VC represents a new attempt that leverages the
conditional ANF to learn a video generative model for conditional inter-frame
coding. We choose ANF because it is a special type of generative model, which
includes variational autoencoder as a special case and is able to achieve
better expressiveness. CANF-VC also extends the idea of conditional coding to
motion coding, forming a purely conditional coding framework. Extensive
experimental results on commonly used datasets confirm the superiority of
CANF-VC to the state-of-the-art methods. The source code of CANF-VC is
available at https://github.com/NYCU-MAPL/CANF-VC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Graph Transformer for Video Question Answering. (arXiv:2207.05342v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05342">
<div class="article-summary-box-inner">
<span><p>This paper proposes a Video Graph Transformer (VGT) model for Video Quetion
Answering (VideoQA). VGT's uniqueness are two-fold: 1) it designs a dynamic
graph transformer module which encodes video by explicitly capturing the visual
objects, their relations, and dynamics for complex spatio-temporal reasoning;
and 2) it exploits disentangled video and text Transformers for relevance
comparison between the video and text to perform QA, instead of entangled
cross-modal Transformer for answer classification. Vision-text communication is
done by additional cross-modal interaction modules. With more reasonable video
encoding and QA solution, we show that VGT can achieve much better performances
on VideoQA tasks that challenge dynamic relation reasoning than prior arts in
the pretraining-free scenario. Its performances even surpass those models that
are pretrained with millions of external data. We further show that VGT can
also benefit a lot from self-supervised cross-modal pretraining, yet with
orders of magnitude smaller data. These results clearly demonstrate the
effectiveness and superiority of VGT, and reveal its potential for more
data-efficient pretraining. With comprehensive analyses and some heuristic
observations, we hope that VGT can promote VQA research beyond coarse
recognition/description towards fine-grained relation reasoning in realistic
videos. Our code is available at https://github.com/sail-sg/VGT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compound Prototype Matching for Few-shot Action Recognition. (arXiv:2207.05515v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05515">
<div class="article-summary-box-inner">
<span><p>Few-shot action recognition aims to recognize novel action classes using only
a small number of labeled training samples. In this work, we propose a novel
approach that first summarizes each video into compound prototypes consisting
of a group of global prototypes and a group of focused prototypes, and then
compares video similarity based on the prototypes. Each global prototype is
encouraged to summarize a specific aspect from the entire video, for example,
the start/evolution of the action. Since no clear annotation is provided for
the global prototypes, we use a group of focused prototypes to focus on certain
timestamps in the video. We compare video similarity by matching the compound
prototypes between the support and query videos. The global prototypes are
directly matched to compare videos from the same perspective, for example, to
compare whether two actions start similarly. For the focused prototypes, since
actions have various temporal variations in the videos, we apply bipartite
matching to allow the comparison of actions with different temporal positions
and shifts. Experiments demonstrate that our proposed method achieves
state-of-the-art results on multiple benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MSP-Former: Multi-Scale Projection Transformer for Single Image Desnowing. (arXiv:2207.05621v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05621">
<div class="article-summary-box-inner">
<span><p>Image restoration of snow scenes in severe weather is a difficult task. Snow
images have complex degradations and are cluttered over clean images, changing
the distribution of clean images. The previous methods based on CNNs are
challenging to remove perfectly in restoring snow scenes due to their local
inductive biases' lack of a specific global modeling ability. In this paper, we
apply the vision transformer to the task of snow removal from a single image.
Specifically, we propose a parallel network architecture split along the
channel, performing local feature refinement and global information modeling
separately. We utilize a channel shuffle operation to combine their respective
strengths to enhance network performance. Second, we propose the MSP module,
which utilizes multi-scale avgpool to aggregate information of different sizes
and simultaneously performs multi-scale projection self-attention on multi-head
self-attention to improve the representation ability of the model under
different scale degradations. Finally, we design a lightweight and simple local
capture module, which can refine the local capture capability of the model.
</p>
<p>In the experimental part, we conduct extensive experiments to demonstrate the
superiority of our method. We compared the previous snow removal methods on
three snow scene datasets. The experimental results show that our method
surpasses the state-of-the-art methods with fewer parameters and computation.
We achieve substantial growth by 1.99dB and SSIM 0.03 on the CSD test dataset.
On the SRRS and Snow100K datasets, we also increased PSNR by 2.47dB and 1.62dB
compared with the Transweather approach and improved by 0.03 in SSIM. In the
visual comparison section, our MSP-Former also achieves better visual effects
than existing methods, proving the usability of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A new database of Houma Alliance Book ancient handwritten characters and its baseline algorithm. (arXiv:2207.05993v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.05993">
<div class="article-summary-box-inner">
<span><p>The Houma Alliance Book is one of the national treasures of the Museum in
Shanxi Museum Town in China. It has great historical significance in
researching ancient history. To date, the research on the Houma Alliance Book
has been staying in the identification of paper documents, which is inefficient
to identify and difficult to display, study and publicize. Therefore, the
digitization of the recognized ancient characters of Houma League can
effectively improve the efficiency of recognizing ancient characters and
provide more reliable technical support and text data. This paper proposes a
new database of Houma Alliance Book ancient handwritten characters and a
multi-modal fusion method to recognize ancient handwritten characters. In the
database, 297 classes and 3,547 samples of Houma Alliance ancient handwritten
characters are collected from the original book collection and by human
imitative writing. Furthermore, the decision-level classifier fusion strategy
is applied to fuse three well-known deep neural network architectures for
ancient handwritten character recognition. Experiments are performed on our new
database. The experimental results first provide the baseline result of the new
database to the research community and then demonstrate the efficiency of our
proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Eliminating Gradient Conflict in Reference-based Line-Art Colorization. (arXiv:2207.06095v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06095">
<div class="article-summary-box-inner">
<span><p>Reference-based line-art colorization is a challenging task in computer
vision. The color, texture, and shading are rendered based on an abstract
sketch, which heavily relies on the precise long-range dependency modeling
between the sketch and reference. Popular techniques to bridge the cross-modal
information and model the long-range dependency employ the attention mechanism.
However, in the context of reference-based line-art colorization, several
techniques would intensify the existing training difficulty of attention, for
instance, self-supervised training protocol and GAN-based losses. To understand
the instability in training, we detect the gradient flow of attention and
observe gradient conflict among attention branches. This phenomenon motivates
us to alleviate the gradient issue by preserving the dominant gradient branch
while removing the conflict ones. We propose a novel attention mechanism using
this training strategy, Stop-Gradient Attention (SGA), outperforming the
attention baseline by a large margin with better training stability. Compared
with state-of-the-art modules in line-art colorization, our approach
demonstrates significant improvements in Fr\'echet Inception Distance (FID, up
to 27.21%) and structural similarity index measure (SSIM, up to 25.67%) on
several benchmarks. The code of SGA is available at
https://github.com/kunkun0w0/SGA .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DynaST: Dynamic Sparse Transformer for Exemplar-Guided Image Generation. (arXiv:2207.06124v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06124">
<div class="article-summary-box-inner">
<span><p>One key challenge of exemplar-guided image generation lies in establishing
fine-grained correspondences between input and guided images. Prior approaches,
despite the promising results, have relied on either estimating dense attention
to compute per-point matching, which is limited to only coarse scales due to
the quadratic memory cost, or fixing the number of correspondences to achieve
linear complexity, which lacks flexibility. In this paper, we propose a dynamic
sparse attention based Transformer model, termed Dynamic Sparse Transformer
(DynaST), to achieve fine-level matching with favorable efficiency. The heart
of our approach is a novel dynamic-attention unit, dedicated to covering the
variation on the optimal number of tokens one position should focus on.
Specifically, DynaST leverages the multi-layer nature of Transformer structure,
and performs the dynamic attention scheme in a cascaded manner to refine
matching results and synthesize visually-pleasing outputs. In addition, we
introduce a unified training objective for DynaST, making it a versatile
reference-based image translation framework for both supervised and
unsupervised scenarios. Extensive experiments on three applications,
pose-guided person image generation, edge-based face synthesis, and undistorted
image style transfer, demonstrate that DynaST achieves superior performance in
local details, outperforming the state of the art while reducing the
computational cost significantly. Our code is available at
https://github.com/Huage001/DynaST
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Organic Priors in Non-Rigid Structure from Motion. (arXiv:2207.06262v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06262">
<div class="article-summary-box-inner">
<span><p>This paper advocates the use of organic priors in classical non-rigid
structure from motion (NRSfM). By organic priors, we mean invaluable
intermediate prior information intrinsic to the NRSfM matrix factorization
theory. It is shown that such priors reside in the factorized matrices, and
quite surprisingly, existing methods generally disregard them. The paper's main
contribution is to put forward a simple, methodical, and practical method that
can effectively exploit such organic priors to solve NRSfM. The proposed method
does not make assumptions other than the popular one on the low-rank shape and
offers a reliable solution to NRSfM under orthographic projection. Our work
reveals that the accessibility of organic priors is independent of the camera
motion and shape deformation type. Besides that, the paper provides insights
into the NRSfM factorization -- both in terms of shape and motion -- and is the
first approach to show the benefit of single rotation averaging for NRSfM.
Furthermore, we outline how to effectively recover motion and non-rigid 3D
shape using the proposed organic prior based approach and demonstrate results
that outperform prior-free NRSfM performance by a significant margin. Finally,
we present the benefits of our method via extensive experiments and evaluations
on several benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PyMAF-X: Towards Well-aligned Full-body Model Regression from Monocular Images. (arXiv:2207.06400v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06400">
<div class="article-summary-box-inner">
<span><p>We present PyMAF-X, a regression-based approach to recovering a full-body
parametric model from a single image. This task is very challenging since minor
parametric deviation may lead to noticeable misalignment between the estimated
mesh and the input image. Moreover, when integrating part-specific estimations
to the full-body model, existing solutions tend to either degrade the alignment
or produce unnatural wrist poses. To address these issues, we propose a
Pyramidal Mesh Alignment Feedback (PyMAF) loop in our regression network for
well-aligned human mesh recovery and extend it as PyMAF-X for the recovery of
expressive full-body models. The core idea of PyMAF is to leverage a feature
pyramid and rectify the predicted parameters explicitly based on the mesh-image
alignment status. Specifically, given the currently predicted parameters,
mesh-aligned evidence will be extracted from finer-resolution features
accordingly and fed back for parameter rectification. To enhance the alignment
perception, an auxiliary dense supervision is employed to provide mesh-image
correspondence guidance while spatial alignment attention is introduced to
enable the awareness of the global contexts for our network. When extending
PyMAF for full-body mesh recovery, an adaptive integration strategy is proposed
in PyMAF-X to produce natural wrist poses while maintaining the well-aligned
performance of the part-specific estimations. The efficacy of our approach is
validated on several benchmark datasets for body-only and full-body mesh
recovery, where PyMAF and PyMAF-X effectively improve the mesh-image alignment
and achieve new state-of-the-art results. The project page with code and video
results can be found at https://www.liuyebin.com/pymaf-x.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Data-Efficient Deep Learning Framework for Segmentation and Classification of Histopathology Images. (arXiv:2207.06489v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06489">
<div class="article-summary-box-inner">
<span><p>The current study of cell architecture of inflammation in histopathology
images commonly performed for diagnosis and research purposes excludes a lot of
information available on the biopsy slide. In autoimmune diseases, major
outstanding research questions remain regarding which cell types participate in
inflammation at the tissue level,and how they interact with each other. While
these questions can be partially answered using traditional methods, artificial
intelligence approaches for segmentation and classification provide a much more
efficient method to understand the architecture of inflammation in autoimmune
disease, holding a great promise for novel insights. In this paper, we
empirically develop deep learning approaches that uses dermatomyositis biopsies
of human tissue to detect and identify inflammatory cells. Our approach
improves classification performance by 26% and segmentation performance by 5%.
We also propose a novel post-processing autoencoder architecture that improves
segmentation performance by an additional 3%. We have open-sourced our approach
and architecture at https://github.com/pranavsinghps1/DEDL
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lipschitz Continuity Retained Binary Neural Network. (arXiv:2207.06540v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06540">
<div class="article-summary-box-inner">
<span><p>Relying on the premise that the performance of a binary neural network can be
largely restored with eliminated quantization error between full-precision
weight vectors and their corresponding binary vectors, existing works of
network binarization frequently adopt the idea of model robustness to reach the
aforementioned objective. However, robustness remains to be an ill-defined
concept without solid theoretical support. In this work, we introduce the
Lipschitz continuity, a well-defined functional property, as the rigorous
criteria to define the model robustness for BNN. We then propose to retain the
Lipschitz continuity as a regularization term to improve the model robustness.
Particularly, while the popular Lipschitz-involved regularization methods often
collapse in BNN due to its extreme sparsity, we design the Retention Matrices
to approximate spectral norms of the targeted weight matrices, which can be
deployed as the approximation for the Lipschitz constant of BNNs without the
exact Lipschitz constant computation (NP-hard). Our experiments prove that our
BNN-specific regularization method can effectively strengthen the robustness of
BNN (testified on ImageNet-C), achieving state-of-the-art performance on CIFAR
and ImageNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DEXTER: An end-to-end system to extract table contents from electronic medical health documents. (arXiv:2207.06823v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.06823">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose DEXTER, an end to end system to extract information
from tables present in medical health documents, such as electronic health
records (EHR) and explanation of benefits (EOB). DEXTER consists of four
sub-system stages: i) table detection ii) table type classification iii) cell
detection; and iv) cell content extraction. We propose a two-stage transfer
learning-based approach using CDeC-Net architecture along with Non-Maximal
suppression for table detection. We design a conventional computer vision-based
approach for table type classification and cell detection using parameterized
kernels based on image size for detecting rows and columns. Finally, we extract
the text from the detected cells using pre-existing OCR engine Tessaract. To
evaluate our system, we manually annotated a sample of the real-world medical
dataset (referred to as Meddata) consisting of wide variations of documents (in
terms of appearance) covering different table structures, such as bordered,
partially bordered, borderless, or coloured tables. We experimentally show that
DEXTER outperforms the commercially available Amazon Textract and Microsoft
Azure Form Recognizer systems on the annotated real-world medical dataset
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Convolutional Bypasses Are Better Vision Transformer Adapters. (arXiv:2207.07039v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07039">
<div class="article-summary-box-inner">
<span><p>The pretrain-then-finetune paradigm has been widely adopted in computer
vision. But as the size of Vision Transformer (ViT) grows exponentially, the
full finetuning becomes prohibitive in view of the heavier storage overhead.
Motivated by parameter-efficient transfer learning (PETL) on language
transformers, recent studies attempt to insert lightweight adaptation modules
(e.g., adapter layers or prompt tokens) to pretrained ViT and only finetune
these modules while the pretrained weights are frozen. However, these modules
were originally proposed to finetune language models. Although ported well to
ViT, their design lacks prior knowledge for visual tasks. In this paper, we
propose to construct Convolutional Bypasses (Convpass) in ViT as adaptation
modules, introducing only a small amount (less than 0.5% of model parameters)
of trainable parameters to adapt the large ViT. Different from other PETL
methods, Convpass benefits from the hard-coded inductive bias of convolutional
layers and thus is more suitable for visual tasks, especially in the low-data
regime. Experimental results on VTAB-1k benchmark and few-shot learning
datasets demonstrate that Convpass outperforms current language-oriented
adaptation modules, demonstrating the necessity to tailor vision-oriented
adaptation modules for vision models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Grand Unification of Object Tracking. (arXiv:2207.07078v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07078">
<div class="article-summary-box-inner">
<span><p>We present a unified method, termed Unicorn, that can simultaneously solve
four tracking problems (SOT, MOT, VOS, MOTS) with a single network using the
same model parameters. Due to the fragmented definitions of the object tracking
problem itself, most existing trackers are developed to address a single or
part of tasks and overspecialize on the characteristics of specific tasks. By
contrast, Unicorn provides a unified solution, adopting the same input,
backbone, embedding, and head across all tracking tasks. For the first time, we
accomplish the great unification of the tracking network architecture and
learning paradigm. Unicorn performs on-par or better than its task-specific
counterparts in 8 tracking datasets, including LaSOT, TrackingNet, MOT17,
BDD100K, DAVIS16-17, MOTS20, and BDD100K MOTS. We believe that Unicorn will
serve as a solid step towards the general vision model. Code is available at
https://github.com/MasterBin-IIAU/Unicorn.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">XMem: Long-Term Video Object Segmentation with an Atkinson-Shiffrin Memory Model. (arXiv:2207.07115v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07115">
<div class="article-summary-box-inner">
<span><p>We present XMem, a video object segmentation architecture for long videos
with unified feature memory stores inspired by the Atkinson-Shiffrin memory
model. Prior work on video object segmentation typically only uses one type of
feature memory. For videos longer than a minute, a single feature memory model
tightly links memory consumption and accuracy. In contrast, following the
Atkinson-Shiffrin model, we develop an architecture that incorporates multiple
independent yet deeply-connected feature memory stores: a rapidly updated
sensory memory, a high-resolution working memory, and a compact thus sustained
long-term memory. Crucially, we develop a memory potentiation algorithm that
routinely consolidates actively used working memory elements into the long-term
memory, which avoids memory explosion and minimizes performance decay for
long-term prediction. Combined with a new memory reading mechanism, XMem
greatly exceeds state-of-the-art performance on long-video datasets while being
on par with state-of-the-art methods (that do not work on long videos) on
short-video datasets. Code is available at https://hkchengrex.github.io/XMem
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decoupling Recognition from Detection: Single Shot Self-Reliant Scene Text Spotter. (arXiv:2207.07253v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07253">
<div class="article-summary-box-inner">
<span><p>Typical text spotters follow the two-stage spotting strategy: detect the
precise boundary for a text instance first and then perform text recognition
within the located text region. While such strategy has achieved substantial
progress, there are two underlying limitations. 1) The performance of text
recognition depends heavily on the precision of text detection, resulting in
the potential error propagation from detection to recognition. 2) The RoI
cropping which bridges the detection and recognition brings noise from
background and leads to information loss when pooling or interpolating from
feature maps. In this work we propose the single shot Self-Reliant Scene Text
Spotter (SRSTS), which circumvents these limitations by decoupling recognition
from detection. Specifically, we conduct text detection and recognition in
parallel and bridge them by the shared positive anchor point. Consequently, our
method is able to recognize the text instances correctly even though the
precise text boundaries are challenging to detect. Additionally, our method
reduces the annotation cost for text detection substantially. Extensive
experiments on regular-shaped benchmark and arbitrary-shaped benchmark
demonstrate that our SRSTS compares favorably to previous state-of-the-art
spotters in terms of both accuracy and efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Privacy-Preserving Face Recognition with Learnable Privacy Budgets in Frequency Domain. (arXiv:2207.07316v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07316">
<div class="article-summary-box-inner">
<span><p>Face recognition technology has been used in many fields due to its high
recognition accuracy, including the face unlocking of mobile devices, community
access control systems, and city surveillance. As the current high accuracy is
guaranteed by very deep network structures, facial images often need to be
transmitted to third-party servers with high computational power for inference.
However, facial images visually reveal the user's identity information. In this
process, both untrusted service providers and malicious users can significantly
increase the risk of a personal privacy breach. Current privacy-preserving
approaches to face recognition are often accompanied by many side effects, such
as a significant increase in inference time or a noticeable decrease in
recognition accuracy. This paper proposes a privacy-preserving face recognition
method using differential privacy in the frequency domain. Due to the
utilization of differential privacy, it offers a guarantee of privacy in
theory. Meanwhile, the loss of accuracy is very slight. This method first
converts the original image to the frequency domain and removes the direct
component termed DC. Then a privacy budget allocation method can be learned
based on the loss of the back-end face recognition network within the
differential privacy framework. Finally, it adds the corresponding noise to the
frequency domain features. Our method performs very well with several classical
face recognition test sets according to the extensive experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Instances as 1D Kernels. (arXiv:2207.07372v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07372">
<div class="article-summary-box-inner">
<span><p>We introduce a 3D instance representation, termed instance kernels, where
instances are represented by one-dimensional vectors that encode the semantic,
positional, and shape information of 3D instances. We show that instance
kernels enable easy mask inference by simply scanning kernels over the entire
scenes, avoiding the heavy reliance on proposals or heuristic clustering
algorithms in standard 3D instance segmentation pipelines. The idea of instance
kernel is inspired by recent success of dynamic convolutions in 2D/3D instance
segmentation. However, we find it non-trivial to represent 3D instances due to
the disordered and unstructured nature of point cloud data, e.g., poor instance
localization can significantly degrade instance representation. To remedy this,
we construct a novel 3D instance encoding paradigm. First, potential instance
centroids are localized as candidates. Then, a candidate merging scheme is
devised to simultaneously aggregate duplicated candidates and collect context
around the merged centroids to form the instance kernels. Once instance kernels
are available, instance masks can be reconstructed via dynamic convolutions
whose weights are conditioned on instance kernels. The whole pipeline is
instantiated with a dynamic kernel network (DKNet). Results show that DKNet
outperforms the state of the arts on both ScanNetV2 and S3DIS datasets with
better instance localization. Code is available:
https://github.com/W1zheng/DKNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ST-P3: End-to-end Vision-based Autonomous Driving via Spatial-Temporal Feature Learning. (arXiv:2207.07601v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2207.07601">
<div class="article-summary-box-inner">
<span><p>Many existing autonomous driving paradigms involve a multi-stage discrete
pipeline of tasks. To better predict the control signals and enhance user
safety, an end-to-end approach that benefits from joint spatial-temporal
feature learning is desirable. While there are some pioneering works on
LiDAR-based input or implicit design, in this paper we formulate the problem in
an interpretable vision-based setting. In particular, we propose a
spatial-temporal feature learning scheme towards a set of more representative
features for perception, prediction and planning tasks simultaneously, which is
called ST-P3. Specifically, an egocentric-aligned accumulation technique is
proposed to preserve geometry information in 3D space before the bird's eye
view transformation for perception; a dual pathway modeling is devised to take
past motion variations into account for future prediction; a temporal-based
refinement unit is introduced to compensate for recognizing vision-based
elements for planning. To the best of our knowledge, we are the first to
systematically investigate each part of an interpretable end-to-end
vision-based autonomous driving system. We benchmark our approach against
previous state-of-the-arts on both open-loop nuScenes dataset as well as
closed-loop CARLA simulation. The results show the effectiveness of our method.
Source code, model and protocol details are made publicly available at
https://github.com/OpenPerceptionX/ST-P3.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-07-19 23:08:37.563521611 UTC">2022-07-19 23:08:37 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>