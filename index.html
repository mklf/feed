<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-12-21T01:30:00Z">12-21</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving scripts with a memory of natural feedback. (arXiv:2112.09737v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09737">
<div class="article-summary-box-inner">
<span><p>How can an end-user provide feedback if a deployed structured prediction
model generates incorrect output? Our goal is to allow users to correct errors
directly through interaction, without retraining, by giving feedback on the
model's output. We create a dynamic memory architecture with a growing memory
of feedbacks about errors in the output. Given a new, unseen input, our model
can use feedback from a similar, past erroneous state. On a script generation
task, we show empirically that the model learns to apply feedback effectively
(up to 30 points improvement), while avoiding similar past mistakes after
deployment (up to 10 points improvement on an unseen set). This is a first step
towards strengthening deployed models, potentially broadening their utility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Ethical Outcomes with Machine-in-the-Loop: Broadening Human Understanding of Data Annotations. (arXiv:2112.09738v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09738">
<div class="article-summary-box-inner">
<span><p>We introduce a machine-in-the-loop pipeline that aims to address root causes
of unwanted bias in natural language based supervised machine learning tasks in
the education domain. Learning from the experiences of students is foundational
for education researchers, and academic administrators. 21st-century skills
learned from experience are becoming a core part of college and career
readiness as well as the hiring process in the new knowledge economy.
Minoritized students demonstrate these skills in their daily lives, but
documenting, assessing, and validating these skills is a huge problem for
educational institutions. As an equity focused online platform, LivedX
translates minoritized students' lived experiences into the 21st century
skills, issues micro-credentials, and creates personal 21st century skills
portfolio. To automate the micro credential mining from the natural language
texts received from the students' submitted essays, we employed a bag-of-word
model to construct a multi-output classifier. Despite our goal, our model
initially exacerbated disparate impact on minoritized students. We used a
machine-in-the-loop model development pipeline to address the problem and
refine the aforementioned model to ensure fairness in its prediction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can we Fix the Scope for Coreference? Problems and Solutions for Benchmarks beyond OntoNotes. (arXiv:2112.09742v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09742">
<div class="article-summary-box-inner">
<span><p>Current work on automatic coreference resolution has focused on the OntoNotes
benchmark dataset, due to both its size and consistency. However many aspects
of the OntoNotes annotation scheme are not well understood by NLP
practitioners, including the treatment of generic NPs, noun modifiers,
indefinite anaphora, predication and more. These often lead to counterintuitive
claims, results and system behaviors. This opinion piece aims to highlight some
of the problems with the OntoNotes rendition of coreference, and to propose a
way forward relying on three principles: 1. a focus on semantics, not
morphosyntax; 2. cross-linguistic generalizability; and 3. a separation of
identity and scope, which can resolve old problems involving temporal and modal
domain consistency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assessing Post-editing Effort in the English-Hindi Direction. (arXiv:2112.09841v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09841">
<div class="article-summary-box-inner">
<span><p>We present findings from a first in-depth post-editing effort estimation
study in the English-Hindi direction along multiple effort indicators. We
conduct a controlled experiment involving professional translators, who
complete assigned tasks alternately, in a translation from scratch and a
post-edit condition. We find that post-editing reduces translation time (by
63%), utilizes fewer keystrokes (by 59%), and decreases the number of pauses
(by 63%) when compared to translating from scratch. We further verify the
quality of translations thus produced via a human evaluation task in which we
do not detect any discernible quality differences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Morpheme Boundary Detection & Grammatical Feature Prediction for Gujarati : Dataset & Model. (arXiv:2112.09860v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09860">
<div class="article-summary-box-inner">
<span><p>Developing Natural Language Processing resources for a low resource language
is a challenging but essential task. In this paper, we present a Morphological
Analyzer for Gujarati. We have used a Bi-Directional LSTM based approach to
perform morpheme boundary detection and grammatical feature tagging. We have
created a data set of Gujarati words with lemma and grammatical features. The
Bi-LSTM based model of Morph Analyzer discussed in the paper handles the
language morphology effectively without the knowledge of any hand-crafted
suffix rules. To the best of our knowledge, this is the first dataset and morph
analyzer model for the Gujarati language which performs both grammatical
feature tagging and morpheme boundary detection tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cascading Adaptors to Leverage English Data to Improve Performance of Question Answering for Low-Resource Languages. (arXiv:2112.09866v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09866">
<div class="article-summary-box-inner">
<span><p>Transformer based architectures have shown notable results on many down
streaming tasks including question answering. The availability of data, on the
other hand, impedes obtaining legitimate performance for low-resource
languages. In this paper, we investigate the applicability of pre-trained
multilingual models to improve the performance of question answering in
low-resource languages. We tested four combinations of language and task
adapters using multilingual transformer architectures on seven languages
similar to MLQA dataset. Additionally, we have also proposed zero-shot transfer
learning of low-resource question answering using language and task adapters.
We observed that stacking the language and the task adapters improves the
multilingual transformer models' performance significantly for low-resource
languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Web Is Your Oyster -- Knowledge-Intensive NLP against a Very Large Web Corpus. (arXiv:2112.09924v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09924">
<div class="article-summary-box-inner">
<span><p>In order to address the increasing demands of real-world applications, the
research for knowledge-intensive NLP (KI-NLP) should advance by capturing the
challenges of a truly open-domain environment: web scale knowledge, lack of
structure, inconsistent quality, and noise. To this end, we propose a new setup
for evaluating existing KI-NLP tasks in which we generalize the background
corpus to a universal web snapshot. We repurpose KILT, a standard KI-NLP
benchmark initially developed for Wikipedia, and ask systems to use a subset of
CCNet - the Sphere corpus - as a knowledge source. In contrast to Wikipedia,
Sphere is orders of magnitude larger and better reflects the full diversity of
knowledge on the Internet. We find that despite potential gaps of coverage,
challenges of scale, lack of structure and lower quality, retrieval from Sphere
enables a state-of-the-art retrieve-and-read system to match and even
outperform Wikipedia-based models on several KILT tasks - even if we
aggressively filter content that looks like Wikipedia. We also observe that
while a single dense passage index over Wikipedia can outperform a sparse BM25
version, on Sphere this is not yet possible. To facilitate further research
into this area, and minimise the community's reliance on proprietary black box
search engines, we will share our indices, evaluation metrics and
infrastructure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Word Graph Guided Summarization for Radiology Findings. (arXiv:2112.09925v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09925">
<div class="article-summary-box-inner">
<span><p>Radiology reports play a critical role in communicating medical findings to
physicians. In each report, the impression section summarizes essential
radiology findings. In clinical practice, writing impression is highly demanded
yet time-consuming and prone to errors for radiologists. Therefore, automatic
impression generation has emerged as an attractive research direction to
facilitate such clinical practice. Existing studies mainly focused on
introducing salient word information to the general text summarization
framework to guide the selection of the key content in radiology findings.
However, for this task, a model needs not only capture the important words in
findings but also accurately describe their relations so as to generate
high-quality impressions. In this paper, we propose a novel method for
automatic impression generation, where a word graph is constructed from the
findings to record the critical words and their relations, then a Word Graph
guided Summarization model (WGSum) is designed to generate impressions with the
help of the word graph. Experimental results on two datasets, OpenI and
MIMIC-CXR, confirm the validity and effectiveness of our proposed approach,
where the state-of-the-art results are achieved on both datasets. Further
experiments are also conducted to analyze the impact of different graph designs
to the performance of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Syntactic-GCN Bert based Chinese Event Extraction. (arXiv:2112.09939v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09939">
<div class="article-summary-box-inner">
<span><p>With the rapid development of information technology, online platforms (e.g.,
news portals and social media) generate enormous web information every moment.
Therefore, it is crucial to extract structured representations of events from
social streams. Generally, existing event extraction research utilizes pattern
matching, machine learning, or deep learning methods to perform event
extraction tasks. However, the performance of Chinese event extraction is not
as good as English due to the unique characteristics of the Chinese language.
In this paper, we propose an integrated framework to perform Chinese event
extraction. The proposed approach is a multiple channel input neural framework
that integrates semantic features and syntactic features. The semantic features
are captured by BERT architecture. The Part of Speech (POS) features and
Dependency Parsing (DP) features are captured by profiling embeddings and Graph
Convolutional Network (GCN), respectively. We also evaluate our model on a
real-world dataset. Experimental results show that the proposed method
outperforms the benchmark approaches significantly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Transformers for Hate Speech Detection in Conversational Code-Mixed Tweets. (arXiv:2112.09986v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09986">
<div class="article-summary-box-inner">
<span><p>In the current era of the internet, where social media platforms are easily
accessible for everyone, people often have to deal with threats, identity
attacks, hate, and bullying due to their association with a cast, creed,
gender, religion, or even acceptance or rejection of a notion. Existing works
in hate speech detection primarily focus on individual comment classification
as a sequence labeling task and often fail to consider the context of the
conversation. The context of a conversation often plays a substantial role when
determining the author's intent and sentiment behind the tweet. This paper
describes the system proposed by team MIDAS-IIITD for HASOC 2021 subtask 2, one
of the first shared tasks focusing on detecting hate speech from Hindi-English
code-mixed conversations on Twitter. We approach this problem using neural
networks, leveraging the transformer's cross-lingual embeddings and further
finetuning them for low-resource hate-speech classification in transliterated
Hindi text. Our best performing system, a hard voting ensemble of Indic-BERT,
XLM-RoBERTa, and Multilingual BERT, achieved a macro F1 score of 0.7253,
placing us first on the overall leaderboard standings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual Learning with Knowledge Transfer for Sentiment Classification. (arXiv:2112.10021v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10021">
<div class="article-summary-box-inner">
<span><p>This paper studies continual learning (CL) for sentiment classification (SC).
In this setting, the CL system learns a sequence of SC tasks incrementally in a
neural network, where each task builds a classifier to classify the sentiment
of reviews of a particular product category or domain. Two natural questions
are: Can the system transfer the knowledge learned in the past from the
previous tasks to the new task to help it learn a better model for the new
task? And, can old models for previous tasks be improved in the process as
well? This paper proposes a novel technique called KAN to achieve these
objectives. KAN can markedly improve the SC accuracy of both the new task and
the old tasks via forward and backward knowledge transfer. The effectiveness of
KAN is demonstrated through extensive experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Augmentation for Mental Health Classification on Social Media. (arXiv:2112.10064v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10064">
<div class="article-summary-box-inner">
<span><p>The mental disorder of online users is determined using social media posts.
The major challenge in this domain is to avail the ethical clearance for using
the user generated text on social media platforms. Academic re searchers
identified the problem of insufficient and unlabeled data for mental health
classification. To handle this issue, we have studied the effect of data
augmentation techniques on domain specific user generated text for mental
health classification. Among the existing well established data augmentation
techniques, we have identified Easy Data Augmentation (EDA), conditional BERT,
and Back Translation (BT) as the potential techniques for generating additional
text to improve the performance of classifiers. Further, three different
classifiers Random Forest (RF), Support Vector Machine (SVM) and Logistic
Regression (LR) are employed for analyzing the impact of data augmentation on
two publicly available social media datasets. The experiments mental results
show significant improvements in classifiers performance when trained on the
augmented data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified Named Entity Recognition as Word-Word Relation Classification. (arXiv:2112.10070v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10070">
<div class="article-summary-box-inner">
<span><p>So far, named entity recognition (NER) has been involved with three major
types, including flat, overlapped (aka. nested), and discontinuous NER, which
have mostly been studied individually. Recently, a growing interest has been
built for unified NER, tackling the above three jobs concurrently with one
single model. Current best-performing methods mainly include span-based and
sequence-to-sequence models, where unfortunately the former merely focus on
boundary identification and the latter may suffer from exposure bias. In this
work, we present a novel alternative by modeling the unified NER as word-word
relation classification, namely W^2NER. The architecture resolves the kernel
bottleneck of unified NER by effectively modeling the neighboring relations
between entity words with Next-Neighboring-Word (NNW) and Tail-Head-Word-*
(THW-*) relations. Based on the W^2NER scheme we develop a neural framework, in
which the unified NER is modeled as a 2D grid of word pairs. We then propose
multi-granularity 2D convolutions for better refining the grid representations.
Finally, a co-predictor is used to sufficiently reason the word-word relations.
We perform extensive experiments on 14 widely-used benchmark datasets for flat,
overlapped, and discontinuous NER (8 English and 6 Chinese datasets), where our
model beats all the current top-performing baselines, pushing the
state-of-the-art performances of unified NER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigation of Densely Connected Convolutional Networks with Domain Adversarial Learning for Noise Robust Speech Recognition. (arXiv:2112.10108v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10108">
<div class="article-summary-box-inner">
<span><p>We investigate densely connected convolutional networks (DenseNets) and their
extension with domain adversarial training for noise robust speech recognition.
DenseNets are very deep, compact convolutional neural networks which have
demonstrated incredible improvements over the state-of-the-art results in
computer vision. Our experimental results reveal that DenseNets are more robust
against noise than other neural network based models such as deep feed forward
neural networks and convolutional neural networks. Moreover, domain adversarial
learning can further improve the robustness of DenseNets against both, known
and unknown noise conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Early Detection of Security-Relevant Bug Reports using Machine Learning: How Far Are We?. (arXiv:2112.10123v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10123">
<div class="article-summary-box-inner">
<span><p>Bug reports are common artefacts in software development. They serve as the
main channel for users to communicate to developers information about the
issues that they encounter when using released versions of software programs.
In the descriptions of issues, however, a user may, intentionally or not,
expose a vulnerability. In a typical maintenance scenario, such
security-relevant bug reports are prioritised by the development team when
preparing corrective patches. Nevertheless, when security relevance is not
immediately expressed (e.g., via a tag) or rapidly identified by triaging
teams, the open security-relevant bug report can become a critical leak of
sensitive information that attackers can leverage to perform zero-day attacks.
To support practitioners in triaging bug reports, the research community has
proposed a number of approaches for the detection of security-relevant bug
reports. In recent years, approaches in this respect based on machine learning
have been reported with promising performance. Our work focuses on such
approaches, and revisits their building blocks to provide a comprehensive view
on the current achievements. To that end, we built a large experimental dataset
and performed extensive experiments with variations in feature sets and
learning algorithms. Eventually, our study highlights different approach
configurations that yield best performing classifiers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LUC at ComMA-2021 Shared Task: Multilingual Gender Biased and Communal Language Identification without using linguistic features. (arXiv:2112.10189v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10189">
<div class="article-summary-box-inner">
<span><p>This work aims to evaluate the ability that both probabilistic and
state-of-the-art vector space modeling (VSM) methods provide to well known
machine learning algorithms to identify social network documents to be
classified as aggressive, gender biased or communally charged. To this end, an
exploratory stage was performed first in order to find relevant settings to
test, i.e. by using training and development samples, we trained multiple
algorithms using multiple vector space modeling and probabilistic methods and
discarded the less informative configurations. These systems were submitted to
the competition of the ComMA@ICON'21 Workshop on Multilingual Gender Biased and
Communal Language Identification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-turn RNN-T for streaming recognition of multi-party speech. (arXiv:2112.10200v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10200">
<div class="article-summary-box-inner">
<span><p>Automatic speech recognition (ASR) of single channel far-field recordings
with an unknown number of speakers is traditionally tackled by cascaded
modules. Recent research shows that end-to-end (E2E) multi-speaker ASR models
can achieve superior recognition accuracy compared to modular systems. However,
these models do not ensure real-time applicability due to their dependency on
full audio context. This work takes real-time applicability as the first
priority in model design and addresses a few challenges in previous work on
multi-speaker recurrent neural network transducer (MS-RNN-T). First, we
introduce on-the-fly overlapping speech simulation during training, yielding
14% relative word error rate (WER) improvement on LibriSpeechMix test set.
Second, we propose a novel multi-turn RNN-T (MT-RNN-T) model with an
overlap-based target arrangement strategy that generalizes to an arbitrary
number of speakers without changes in the model architecture. We investigate
the impact of the maximum number of speakers seen during training on MT-RNN-T
performance on LibriCSS test set, and report 28% relative WER improvement over
the two-speaker MS-RNN-T. Third, we experiment with a rich transcription
strategy for joint recognition and segmentation of multi-party speech. Through
an in-depth analysis, we discuss potential pitfalls of the proposed system as
well as promising future research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Integrating Knowledge in End-to-End Automatic Speech Recognition for Mandarin-English Code-Switching. (arXiv:2112.10202v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10202">
<div class="article-summary-box-inner">
<span><p>Code-Switching (CS) is a common linguistic phenomenon in multilingual
communities that consists of switching between languages while speaking. This
paper presents our investigations on end-to-end speech recognition for
Mandarin-English CS speech. We analyse different CS specific issues such as the
properties mismatches between languages in a CS language pair, the
unpredictable nature of switching points, and the data scarcity problem. We
exploit and improve the state-of-the-art end-to-end system by merging
nonlinguistic symbols, by integrating language identification using
hierarchical softmax, by modeling sub-word units, by artificially lowering the
speaking rate, and by augmenting data using speed perturbed technique and
several monolingual datasets to improve the final performance not only on CS
speech but also on monolingual benchmarks in order to make the system more
applicable on real life settings. Finally, we explore the effect of different
language model integration methods on the performance of the proposed model.
Our experimental results reveal that all the proposed techniques improve the
recognition performance. The best combined system improves the baseline system
by up to 35% relatively in terms of mixed error rate and delivers acceptable
performance on monolingual benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">English-to-Chinese Transliteration with Phonetic Back-transliteration. (arXiv:2112.10321v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10321">
<div class="article-summary-box-inner">
<span><p>Transliteration is a task of translating named entities from a language to
another, based on phonetic similarity. The task has embraced deep learning
approaches in recent years, yet, most ignore the phonetic features of the
involved languages. In this work, we incorporate phonetic information into
neural networks in two ways: we synthesize extra data using forward and
back-translation but in a phonetic manner; and we pre-train models on a
phonetic task before learning transliteration. Our experiments include three
language pairs and six directions, namely English to and from Chinese, Hebrew
and Thai. Results indicate that our proposed approach brings benefits to the
model and achieves better or similar performance when compared to state of the
art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Article Reranking by Memory-Enhanced Key Sentence Matching for Detecting Previously Fact-Checked Claims. (arXiv:2112.10322v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10322">
<div class="article-summary-box-inner">
<span><p>False claims that have been previously fact-checked can still spread on
social media. To mitigate their continual spread, detecting previously
fact-checked claims is indispensable. Given a claim, existing works focus on
providing evidence for detection by reranking candidate fact-checking articles
(FC-articles) retrieved by BM25. However, these performances may be limited
because they ignore the following characteristics of FC-articles: (1) claims
are often quoted to describe the checked events, providing lexical information
besides semantics; (2) sentence templates to introduce or debunk claims are
common across articles, providing pattern information. Models that ignore the
two aspects only leverage semantic relevance and may be misled by sentences
that describe similar but irrelevant events. In this paper, we propose a novel
reranker, MTM (Memory-enhanced Transformers for Matching) to rank FC-articles
using key sentences selected with event (lexical and semantic) and pattern
information. For event information, we propose a ROUGE-guided Transformer which
is finetuned with regression of ROUGE. For pattern information, we generate
pattern vectors for matching with sentences. By fusing event and pattern
information, we select key sentences to represent an article and then predict
if the article fact-checks the given claim using the claim, key sentences, and
patterns. Experiments on two real-world datasets show that MTM outperforms
existing methods. Human evaluation proves that MTM can capture key sentences
for explanations. The code and the dataset are at
https://github.com/ICTMCG/MTM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">May the Force Be with Your Copy Mechanism: Enhanced Supervised-Copy Method for Natural Language Generation. (arXiv:2112.10360v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10360">
<div class="article-summary-box-inner">
<span><p>Recent neural sequence-to-sequence models with a copy mechanism have achieved
remarkable progress in various text generation tasks. These models addressed
out-of-vocabulary problems and facilitated the generation of rare words.
However, the identification of the word which needs to be copied is difficult,
as observed by prior copy models, which suffer from incorrect generation and
lacking abstractness. In this paper, we propose a novel supervised approach of
a copy network that helps the model decide which words need to be copied and
which need to be generated. Specifically, we re-define the objective function,
which leverages source sequences and target vocabularies as guidance for
copying. The experimental results on data-to-text generation and abstractive
summarization tasks verify that our approach enhances the copying quality and
improves the degree of abstractness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unifying Model Explainability and Robustness for Joint Text Classification and Rationale Extraction. (arXiv:2112.10424v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10424">
<div class="article-summary-box-inner">
<span><p>Recent works have shown explainability and robustness are two crucial
ingredients of trustworthy and reliable text classification. However, previous
works usually address one of two aspects: i) how to extract accurate rationales
for explainability while being beneficial to prediction; ii) how to make the
predictive model robust to different types of adversarial attacks. Intuitively,
a model that produces helpful explanations should be more robust against
adversarial attacks, because we cannot trust the model that outputs
explanations but changes its prediction under small perturbations. To this end,
we propose a joint classification and rationale extraction model named AT-BMC.
It includes two key mechanisms: mixed Adversarial Training (AT) is designed to
use various perturbations in discrete and embedding space to improve the
model's robustness, and Boundary Match Constraint (BMC) helps to locate
rationales more precisely with the guidance of boundary information.
Performances on benchmark datasets demonstrate that the proposed AT-BMC
outperforms baselines on both classification and rationale extraction by a
large margin. Robustness analysis shows that the proposed AT-BMC decreases the
attack success rate effectively by up to 69%. The empirical results indicate
that there are connections between robust models and better explanations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP. (arXiv:2112.10508v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10508">
<div class="article-summary-box-inner">
<span><p>What are the units of text that we want to model? From bytes to multi-word
expressions, text can be analyzed and generated at many granularities. Until
recently, most natural language processing (NLP) models operated over words,
treating those as discrete and atomic tokens, but starting with byte-pair
encoding (BPE), subword-based approaches have become dominant in many areas,
enabling small vocabularies while still allowing for fast inference. Is the end
of the road character-level model or byte-level processing? In this survey, we
connect several lines of work from the pre-neural and neural era, by showing
how hybrid approaches of words and characters as well as subword-based
approaches based on learned segmentation have been proposed and evaluated. We
conclude that there is and likely will never be a silver bullet singular
solution for all applications and that thinking seriously about tokenization
remains important for many applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spiral Language Modeling. (arXiv:2112.10543v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10543">
<div class="article-summary-box-inner">
<span><p>In almost all text generation applications, word sequences are constructed in
a left-to-right (L2R) or right-to-left (R2L) manner, as natural language
sentences are written either L2R or R2L. However, we find that the natural
language written order is not essential for text generation. In this paper, we
propose Spiral Language Modeling (SLM), a general approach that enables one to
construct natural language sentences beyond the L2R and R2L order. SLM allows
one to form natural language text by starting from an arbitrary token inside
the result text and expanding the rest tokens around the selected ones. It
makes the decoding order a new optimization objective besides the language
model perplexity, which further improves the diversity and quality of the
generated text. Furthermore, SLM makes it possible to manipulate the text
construction process by selecting a proper starting token. SLM also introduces
generation orderings as additional regularization to improve model robustness
in low-resource scenarios. Experiments on 8 widely studied Neural Machine
Translation (NMT) tasks show that SLM is constantly effective with up to 4.7
BLEU increase comparing to the conventional L2R decoding approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training dataset and dictionary sizes matter in BERT models: the case of Baltic languages. (arXiv:2112.10553v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10553">
<div class="article-summary-box-inner">
<span><p>Large pretrained masked language models have become state-of-the-art
solutions for many NLP problems. While studies have shown that monolingual
models produce better results than multilingual models, the training datasets
must be sufficiently large. We trained a trilingual LitLat BERT-like model for
Lithuanian, Latvian, and English, and a monolingual Est-RoBERTa model for
Estonian. We evaluate their performance on four downstream tasks: named entity
recognition, dependency parsing, part-of-speech tagging, and word analogy. To
analyze the importance of focusing on a single language and the importance of a
large training set, we compare created models with existing monolingual and
multilingual BERT models for Estonian, Latvian, and Lithuanian. The results
show that the newly created LitLat BERT and Est-RoBERTa models improve the
results of existing models on all tested tasks in most situations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An ensemble deep learning technique for detecting suicidal ideation from posts in social media platforms. (arXiv:2112.10609v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10609">
<div class="article-summary-box-inner">
<span><p>Suicidal ideation detection from social media is an evolving research with
great challenges. Many of the people who have the tendency to suicide share
their thoughts and opinions through social media platforms. As part of many
researches it is observed that the publicly available posts from social media
contain valuable criteria to effectively detect individuals with suicidal
thoughts. The most difficult part to prevent suicide is to detect and
understand the complex risk factors and warning signs that may lead to suicide.
This can be achieved by identifying the sudden changes in a user behavior
automatically. Natural language processing techniques can be used to collect
behavioral and textual features from social media interactions and these
features can be passed to a specially designed framework to detect anomalies in
human interactions that are indicators of suicidal intentions. We can achieve
fast detection of suicidal ideation using deep learning and/or machine learning
based classification approaches. For such a purpose, we can employ the
combination of LSTM and CNN models to detect such emotions from posts of the
users. In order to improve the accuracy, some approaches like using more data
for training, using attention model to improve the efficiency of existing
models etc. could be done. This paper proposes a LSTM-Attention-CNN combined
model to analyze social media submissions to detect any underlying suicidal
intentions. During evaluations, the proposed model demonstrated an accuracy of
90.3 percent and an F1-score of 92.6 percent, which is greater than the
baseline models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Intelligent Online Selling Point Extraction for E-Commerce Recommendation. (arXiv:2112.10613v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10613">
<div class="article-summary-box-inner">
<span><p>In the past decade, automatic product description generation for e-commerce
have witnessed significant advancement. As the services provided by e-commerce
platforms become diverse, it is necessary to dynamically adapt the patterns of
descriptions generated. The selling point of products is an important type of
product description for which the length should be as short as possible while
still conveying key information. In addition, this kind of product description
should be eye-catching to the readers. Currently, product selling points are
normally written by human experts. Thus, the creation and maintenance of these
contents incur high costs. These costs can be significantly reduced if product
selling points can be automatically generated by machines. In this paper, we
report our experience developing and deploying the Intelligent Online Selling
Point Extraction (IOSPE) system to serve the recommendation system in the
JD.com e-commerce platform. Since July 2020, IOSPE has become a core service
for 62 key categories of products (covering more than 4 million products). So
far, it has generated more than 0.1 billion selling points, thereby
significantly scaling up the selling point creation operation and saving human
labour. These IOSPE generated selling points have increased the click-through
rate (CTR) by 1.89\% and the average duration the customers spent on the
products by more than 2.03\% compared to the previous practice, which are
significant improvements for such a large-scale e-commerce platform.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantum Language Model with Entanglement Embedding for Question Answering. (arXiv:2008.09943v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.09943">
<div class="article-summary-box-inner">
<span><p>Quantum Language Models (QLMs) in which words are modelled as quantum
superposition of sememes have demonstrated a high level of model transparency
and good post-hoc interpretability. Nevertheless, in the current literature
word sequences are basically modelled as a classical mixture of word states,
which cannot fully exploit the potential of a quantum probabilistic
description. A full quantum model is yet to be developed to explicitly capture
the non-classical correlations within the word sequences. We propose a neural
network model with a novel Entanglement Embedding (EE) module, whose function
is to transform the word sequences into entangled pure states of many-body
quantum systems. Strong quantum entanglement, which is the central concept of
quantum information and an indication of parallelized correlations among the
words, is observed within the word sequences. Numerical experiments show that
the proposed QLM with EE (QLM-EE) achieves superior performance compared with
the classical deep neural network models and other QLMs on Question Answering
(QA) datasets. In addition, the post-hoc interpretability of the model can be
improved by quantizing the degree of entanglement among the words.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A learning perspective on the emergence of abstractions: the curious case of phonemes. (arXiv:2012.07499v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.07499">
<div class="article-summary-box-inner">
<span><p>In the present paper we use a range of modeling techniques to investigate
whether an abstract phone could emerge from exposure to speech sounds. In
effect, the study represents an attempt for operationalize a theoretical device
of Usage-based Linguistics of emergence of an abstraction from language use.
Our quest focuses on the simplest of such hypothesized abstractions. We test
two opposing principles regarding the development of language knowledge in
linguistically untrained language users: Memory-Based Learning (MBL) and
Error-Correction Learning (ECL). A process of generalization underlies the
abstractions linguists operate with, and we probed whether MBL and ECL could
give rise to a type of language knowledge that resembles linguistic
abstractions. Each model was presented with a significant amount of
pre-processed speech produced by one speaker. We assessed the consistency or
stability of what these simple models have learned and their ability to give
rise to abstract categories. Both types of models fare differently with regard
to these tests. We show that ECL models can learn abstractions and that at
least part of the phone inventory and grouping into traditional types can be
reliably identified from the input.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Adapters for Cross-lingual Low-resource Speech Recognition. (arXiv:2105.11905v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.11905">
<div class="article-summary-box-inner">
<span><p>Cross-lingual speech adaptation aims to solve the problem of leveraging
multiple rich-resource languages to build models for a low-resource target
language. Since the low-resource language has limited training data, speech
recognition models can easily overfit. In this paper, we propose to use
adapters to investigate the performance of multiple adapters for
parameter-efficient cross-lingual speech adaptation. Based on our previous
MetaAdapter that implicitly leverages adapters, we propose a novel algorithms
called SimAdapter for explicitly learning knowledge from adapters. Our
algorithm leverages adapters which can be easily integrated into the
Transformer structure.MetaAdapter leverages meta-learning to transfer the
general knowledge from training data to the test language. SimAdapter aims to
learn the similarities between the source and target languages during
fine-tuning using the adapters. We conduct extensive experiments on
five-low-resource languages in Common Voice dataset. Results demonstrate that
our MetaAdapter and SimAdapter methods can reduce WER by 2.98% and 2.55% with
only 2.5% and 15.5% of trainable parameters compared to the strong full-model
fine-tuning baseline. Moreover, we also show that these two novel algorithms
can be integrated for better performance with up to 3.55% relative WER
reduction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SAS: Self-Augmentation Strategy for Language Model Pre-training. (arXiv:2106.07176v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07176">
<div class="article-summary-box-inner">
<span><p>The core of self-supervised learning for pre-training language models
includes pre-training task design as well as appropriate data augmentation.
Most data augmentations in language model pre-training are context-independent.
A seminal contextualized augmentation was recently proposed in ELECTRA and
achieved state-of-the-art performance by introducing an auxiliary generation
network (generator) to produce contextualized data augmentation for the
training of a main discrimination network (discriminator). This design,
however, introduces extra computation cost of the generator and a need to
adjust the relative capability between the generator and the discriminator. In
this paper, we propose a self-augmentation strategy (SAS) where a single
network is utilized for both regular pre-training and contextualized data
augmentation for the training in later epochs. Essentially, this strategy
eliminates a separate generator and uses the single network to jointly conduct
two pre-training tasks with MLM (Masked Language Modeling) and RTD (Replaced
Token Detection) heads. It avoids the challenge to search for an appropriate
size of the generator, which is critical to the performance as evidenced in
ELECTRA and its subsequent variant models. In addition, SAS is a general
strategy that can be seamlessly combined with many new techniques emerging
recently or in the future, such as the disentangled attention mechanism from
DeBERTa. Our experiments show that SAS is able to outperform ELECTRA and other
state-of-the-art models in the GLUE tasks with similar or less computation
cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Universality of Deep Contextual Language Models. (arXiv:2109.07140v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07140">
<div class="article-summary-box-inner">
<span><p>Deep Contextual Language Models (LMs) like ELMO, BERT, and their successors
dominate the landscape of Natural Language Processing due to their ability to
scale across multiple tasks rapidly by pre-training a single model, followed by
task-specific fine-tuning. Furthermore, multilingual versions of such models
like XLM-R and mBERT have given promising results in zero-shot cross-lingual
transfer, potentially enabling NLP applications in many under-served and
under-resourced languages. Due to this initial success, pre-trained models are
being used as `Universal Language Models' as the starting point across diverse
tasks, domains, and languages. This work explores the notion of `Universality'
by identifying seven dimensions across which a universal model should be able
to scale, that is, perform equally well or reasonably well, to be useful across
diverse settings. We outline the current theoretical and empirical results that
support model performance across these dimensions, along with extensions that
may help address some of their current limitations. Through this survey, we lay
the foundation for understanding the capabilities and limitations of massive
contextual language models and help discern research gaps and directions for
future work to make these LMs inclusive and fair to diverse applications,
users, and linguistic phenomena.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VoxCeleb Enrichment for Age and Gender Recognition. (arXiv:2109.13510v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13510">
<div class="article-summary-box-inner">
<span><p>VoxCeleb datasets are widely used in speaker recognition studies. Our work
serves two purposes. First, we provide speaker age labels and (an alternative)
annotation of speaker gender. Second, we demonstrate the use of this metadata
by constructing age and gender recognition models with different features and
classifiers. We query different celebrity databases and apply consensus rules
to derive age and gender labels. We also compare the original VoxCeleb gender
labels with our labels to identify records that might be mislabeled in the
original VoxCeleb data. On modeling side, we design a comprehensive study of
multiple features and models for recognizing gender and age. Our best system,
using i-vector features, achieved an F1-score of 0.9829 for gender recognition
task using logistic regression, and the lowest mean absolute error (MAE) in age
regression, 9.443 years, is obtained with ridge regression. This indicates
challenge in age estimation from in-the-wild style speech data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Knowledge Assimilation for Expert-Layman Text Style Transfer. (arXiv:2110.02950v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02950">
<div class="article-summary-box-inner">
<span><p>Expert-layman text style transfer technologies have the potential to improve
communication between members of scientific communities and the general public.
High-quality information produced by experts is often filled with difficult
jargon laypeople struggle to understand. This is a particularly notable issue
in the medical domain, where layman are often confused by medical text online.
At present, two bottlenecks interfere with the goal of building high-quality
medical expert-layman style transfer systems: a dearth of pretrained
medical-domain language models spanning both expert and layman terminologies
and a lack of parallel corpora for training the transfer task itself. To
mitigate the first issue, we propose a novel language model (LM) pretraining
task, Knowledge Base Assimilation, to synthesize pretraining data from the
edges of a graph of expert- and layman-style medical terminology terms into an
LM during self-supervised learning. To mitigate the second issue, we build a
large-scale parallel corpus in the medical expert-layman domain using a
margin-based criterion. Our experiments show that transformer-based models
pretrained on knowledge base assimilation and other well-established
pretraining tasks fine-tuning on our new parallel corpus leads to considerable
improvement against expert-layman transfer benchmarks, gaining an average
relative improvement of our human evaluation, the Overall Success Rate (OSR),
by 106%. We release our code and parallel corpus for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Explanation of In-context Learning as Implicit Bayesian Inference. (arXiv:2111.02080v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.02080">
<div class="article-summary-box-inner">
<span><p>Large pretrained language models such as GPT-3 have the surprising ability to
do in-context learning, where the model learns to do a downstream task simply
by conditioning on a prompt consisting of input-output examples. Without being
explicitly pretrained to do so, the language model learns from these examples
during its forward pass without parameter updates on "out-of-distribution"
prompts. Thus, it is unclear what mechanism enables in-context learning. In
this paper, we study the role of the pretraining distribution on the emergence
of in-context learning under a mathematical setting where the pretraining texts
have long-range coherence. Here, language model pretraining requires inferring
a latent document-level concept from the conditioning text to generate coherent
next tokens. At test time, this mechanism enables in-context learning by
inferring the shared latent concept between prompt examples and applying it to
make a prediction on the test example. Concretely, we prove that in-context
learning occurs implicitly via Bayesian inference of the latent concept when
the pretraining distribution is a mixture of HMMs. This can occur despite the
distribution mismatch between prompts and pretraining data. In contrast to
messy large-scale pretraining datasets for in-context learning in natural
language, we generate a family of small-scale synthetic datasets (GINC) where
Transformer and LSTM language models both exhibit in-context learning. Beyond
the theory which focuses on the effect of the pretraining distribution, we
empirically find that scaling model size improves in-context accuracy even when
the pretraining loss is the same.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">STEM: Unsupervised STructural EMbedding for Stance Detection. (arXiv:2112.00712v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00712">
<div class="article-summary-box-inner">
<span><p>Stance detection is an important task, supporting many downstream tasks such
as discourse parsing and modeling the propagation of fake news, rumors, and
science denial. In this paper, we propose a novel framework for stance
detection. Our framework is unsupervised and domain-independent. Given a claim
and a multi-participant discussion - we construct the interaction network from
which we derive topological embedding for each speaker. These speaker embedding
enjoy the following property: speakers with the same stance tend to be
represented by similar vectors, while antipodal vectors represent speakers with
opposing stances. These embedding are then used to divide the speakers into
stance-partitions. We evaluate our method on three different datasets from
different platforms. Our method outperforms or is comparable with supervised
models while providing confidence levels for its output. Furthermore, we
demonstrate how the structural embedding relate to the valence expressed by the
speakers. Finally, we discuss some limitations inherent to the framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Interactions Using Pretrained Unimodal Models for SIMMC 2.0. (arXiv:2112.05328v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05328">
<div class="article-summary-box-inner">
<span><p>This paper presents our work on the Situated Interactive MultiModal
Conversations 2.0 challenge held at Dialog State Tracking Challenge 10. SIMMC
2.0 includes 4 subtasks, and we introduce our multimodal approaches for the
subtask \#1, \#2 and the generation of subtask \#4. SIMMC 2.0 dataset is a
multimodal dataset containing image and text information, which is more
challenging than the problem of only text-based conversations because it must
be solved by understanding the relationship between image and text. Therefore,
since there is a limit to solving only text models such as BERT or GPT2, we
propose a multimodal model combining image and text. We first pretrain the
multimodal model to understand the relationship between image and text, then
finetune our model for each task. We achieve the 3rd best performance in
subtask \#1, \#2 and a runner-up in the generation of subtask \#4. The source
code is available at https://github.com/rungjoo/simmc2.0.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Looking Outside the Box to Ground Language in 3D Scenes. (arXiv:2112.08879v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08879">
<div class="article-summary-box-inner">
<span><p>Existing language grounding models often use object proposal bottlenecks: a
pre-trained detector proposes objects in the scene and the model learns to
select the answer from these box proposals, without attending to the original
image or 3D point cloud. Object detectors are typically trained on a fixed
vocabulary of objects and attributes that is often too restrictive for
open-domain language grounding, where an utterance may refer to visual entities
at various levels of abstraction, such as a chair, the leg of a chair, or the
tip of the front leg of a chair. We propose a model for grounding language in
3D scenes that bypasses box proposal bottlenecks with three main innovations:
i) Iterative attention across the language stream, the point cloud feature
stream and 3D box proposals. ii) Transformer decoders with non-parametric
entity queries that decode 3D boxes for object and part referentials. iii)
Joint supervision from 3D object annotations and language grounding
annotations, by treating object detection as grounding of referential
utterances comprised of a list of candidate category labels. These innovations
result in significant quantitative gains (up to +9% absolute improvement on the
SR3D benchmark) over previous approaches on popular 3D language grounding
benchmarks. We ablate each of our innovations to show its contribution to the
performance of the model. When applied on language grounding on 2D images with
minor changes, it performs on par with the state-of-the-art while converges in
half of the GPU time. The code and checkpoints will be made available at
https://github.com/nickgkan/beauty_detr
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Achieving Forgetting Prevention and Knowledge Transfer in Continual Learning. (arXiv:2112.02706v1 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02706">
<div class="article-summary-box-inner">
<span><p>Continual learning (CL) learns a sequence of tasks incrementally with the
goal of achieving two main objectives: overcoming catastrophic forgetting (CF)
and encouraging knowledge transfer (KT) across tasks. However, most existing
techniques focus only on overcoming CF and have no mechanism to encourage KT,
and thus do not do well in KT. Although several papers have tried to deal with
both CF and KT, our experiments show that they suffer from serious CF when the
tasks do not have much shared knowledge. Another observation is that most
current CL methods do not use pre-trained models, but it has been shown that
such models can significantly improve the end task performance. For example, in
natural language processing, fine-tuning a BERT-like pre-trained language model
is one of the most effective approaches. However, for CL, this approach suffers
from serious CF. An interesting question is how to make the best use of
pre-trained models for CL. This paper proposes a novel model called CTR to
solve these problems. Our experimental results demonstrate the effectiveness of
CTR
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLASSIC: Continual and Contrastive Learning of Aspect Sentiment Classification Tasks. (arXiv:2112.02714v1 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02714">
<div class="article-summary-box-inner">
<span><p>This paper studies continual learning (CL) of a sequence of aspect sentiment
classification(ASC) tasks in a particular CL setting called domain incremental
learning (DIL). Each task is from a different domain or product. The DIL
setting is particularly suited to ASC because in testing the system needs not
know the task/domain to which the test data belongs. To our knowledge, this
setting has not been studied before for ASC. This paper proposes a novel model
called CLASSIC. The key novelty is a contrastive continual learning method that
enables both knowledge transfer across tasks and knowledge distillation from
old tasks to the new task, which eliminates the need for task ids in testing.
Experimental results show the high effectiveness of CLASSIC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adapting BERT for Continual Learning of a Sequence of Aspect Sentiment Classification Tasks. (arXiv:2112.03271v1 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03271">
<div class="article-summary-box-inner">
<span><p>This paper studies continual learning (CL) of a sequence of aspect sentiment
classification (ASC) tasks. Although some CL techniques have been proposed for
document sentiment classification, we are not aware of any CL work on ASC. A CL
system that incrementally learns a sequence of ASC tasks should address the
following two issues: (1) transfer knowledge learned from previous tasks to the
new task to help it learn a better model, and (2) maintain the performance of
the models for previous tasks so that they are not forgotten. This paper
proposes a novel capsule network based model called B-CL to address these
issues. B-CL markedly improves the ASC performance on both the new task and the
old tasks via forward and backward knowledge transfer. The effectiveness of
B-CL is demonstrated through extensive experiments.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Can uncertainty boost the reliability of AI-based diagnostic methods in digital pathology?. (arXiv:2112.09693v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09693">
<div class="article-summary-box-inner">
<span><p>Deep learning (DL) has shown great potential in digital pathology
applications. The robustness of a diagnostic DL-based solution is essential for
safe clinical deployment. In this work we evaluate if adding uncertainty
estimates for DL predictions in digital pathology could result in increased
value for the clinical applications, by boosting the general predictive
performance or by detecting mispredictions. We compare the effectiveness of
model-integrated methods (MC dropout and Deep ensembles) with a model-agnostic
approach (Test time augmentation, TTA). Moreover, four uncertainty metrics are
compared. Our experiments focus on two domain shift scenarios: a shift to a
different medical center and to an underrepresented subtype of cancer. Our
results show that uncertainty estimates can add some reliability and reduce
sensitivity to classification threshold selection. While advanced metrics and
deep ensembles perform best in our comparison, the added value over simpler
metrics and TTA is small. Importantly, the benefit of all evaluated uncertainty
estimation methods is diminished by domain shift.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpretable and Interactive Deep Multiple Instance Learning for Dental Caries Classification in Bitewing X-rays. (arXiv:2112.09694v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09694">
<div class="article-summary-box-inner">
<span><p>We propose a simple and efficient image classification architecture based on
deep multiple instance learning, and apply it to the challenging task of caries
detection in dental radiographs. Technically, our approach contributes in two
ways: First, it outputs a heatmap of local patch classification probabilities
despite being trained with weak image-level labels. Second, it is amenable to
learning from segmentation labels to guide training. In contrast to existing
methods, the human user can faithfully interpret predictions and interact with
the model to decide which regions to attend to. Experiments are conducted on a
large clinical dataset of $\sim$38k bitewings ($\sim$316k teeth), where we
achieve competitive performance compared to various baselines. When guided by
an external caries segmentation model, a significant improvement in
classification and localization performance is observed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Soundify: Matching Sound Effects to Video. (arXiv:2112.09726v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09726">
<div class="article-summary-box-inner">
<span><p>In the art of video editing, sound is really half the story. A skilled video
editor overlays sounds, such as effects and ambients, over footage to add
character to an object or immerse the viewer within a space. However, through
formative interviews with professional video editors, we found that this
process can be extremely tedious and time-consuming. We introduce Soundify, a
system that matches sound effects to video. By leveraging labeled,
studio-quality sound effects libraries and extending CLIP, a neural network
with impressive zero-shot image classification capabilities, into a "zero-shot
detector", we are able to produce high-quality results without
resource-intensive correspondence learning or audio generation. We encourage
you to have a look at, or better yet, have a listen to the results at
https://chuanenlin.com/soundify.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neurashed: A Phenomenological Model for Imitating Deep Learning Training. (arXiv:2112.09741v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09741">
<div class="article-summary-box-inner">
<span><p>To advance deep learning methodologies in the next decade, a theoretical
framework for reasoning about modern neural networks is needed. While efforts
are increasing toward demystifying why deep learning is so effective, a
comprehensive picture remains lacking, suggesting that a better theory is
possible. We argue that a future deep learning theory should inherit three
characteristics: a \textit{hierarchically} structured network architecture,
parameters \textit{iteratively} optimized using stochastic gradient-based
methods, and information from the data that evolves \textit{compressively}. As
an instantiation, we integrate these characteristics into a graphical model
called \textit{neurashed}. This model effectively explains some common
empirical patterns in deep learning. In particular, neurashed enables insights
into implicit regularization, information bottleneck, and local elasticity.
Finally, we discuss how neurashed can guide the development of deep learning
theories.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simple Single-Scale Vision Transformer for Object Localization and Instance Segmentation. (arXiv:2112.09747v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09747">
<div class="article-summary-box-inner">
<span><p>This work presents a simple vision transformer design as a strong baseline
for object localization and instance segmentation tasks. Transformers recently
demonstrate competitive performance in image classification tasks. To adopt ViT
to object detection and dense prediction tasks, many works inherit the
multistage design from convolutional networks and highly customized ViT
architectures. Behind this design, the goal is to pursue a better trade-off
between computational cost and effective aggregation of multiscale global
contexts. However, existing works adopt the multistage architectural design as
a black-box solution without a clear understanding of its true benefits. In
this paper, we comprehensively study three architecture design choices on ViT
-- spatial reduction, doubled channels, and multiscale features -- and
demonstrate that a vanilla ViT architecture can fulfill this goal without
handcrafting multiscale features, maintaining the original ViT design
philosophy. We further complete a scaling rule to optimize our model's
trade-off on accuracy and computation cost / model size. By leveraging a
constant feature resolution and hidden size throughout the encoder blocks, we
propose a simple and compact ViT architecture called Universal Vision
Transformer (UViT) that achieves strong performance on COCO object detection
and instance segmentation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learned Half-Quadratic Splitting Network for Magnetic Resonance Image Reconstruction. (arXiv:2112.09760v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09760">
<div class="article-summary-box-inner">
<span><p>Magnetic Resonance (MR) image reconstruction from highly undersampled
$k$-space data is critical in accelerated MR imaging (MRI) techniques. In
recent years, deep learning-based methods have shown great potential in this
task. This paper proposes a learned half-quadratic splitting algorithm for MR
image reconstruction and implements the algorithm in an unrolled deep learning
network architecture. We compare the performance of our proposed method on a
public cardiac MR dataset against DC-CNN and LPDNet, and our method outperforms
other methods in both quantitative results and qualitative results with fewer
model parameters and faster reconstruction speed. Finally, we enlarge our model
to achieve superior reconstruction quality, and the improvement is $1.76$ dB
and $2.74$ dB over LPDNet in peak signal-to-noise ratio on $5\times$ and
$10\times$ acceleration, respectively. Code for our method is publicly
available at https://github.com/hellopipu/HQS-Net.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Subsampling for ROI-based Visual Tracking: Algorithms and FPGA Implementation. (arXiv:2112.09775v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09775">
<div class="article-summary-box-inner">
<span><p>There is tremendous scope for improving the energy efficiency of embedded
vision systems by incorporating programmable region-of-interest (ROI) readout
in the image sensor design. In this work, we study how ROI programmability can
be leveraged for tracking applications by anticipating where the ROI will be
located in future frames and switching pixels off outside of this region. We
refer to this process of ROI prediction and corresponding sensor configuration
as adaptive subsampling. Our adaptive subsampling algorithms comprise an object
detector and an ROI predictor (Kalman filter) which operate in conjunction to
optimize the energy efficiency of the vision pipeline with the end task being
object tracking. To further facilitate the implementation of our adaptive
algorithms in real life, we select a candidate algorithm and map it onto an
FPGA. Leveraging Xilinx Vitis AI tools, we designed and accelerated a YOLO
object detector-based adaptive subsampling algorithm. In order to further
improve the algorithm post-deployment, we evaluated several competing baselines
on the OTB100 and LaSOT datasets. We found that coupling the ECO tracker with
the Kalman filter has a competitive AUC score of 0.4568 and 0.3471 on the
OTB100 and LaSOT datasets respectively. Further, the power efficiency of this
algorithm is on par with, and in a couple of instances superior to, the other
baselines. The ECO-based algorithm incurs a power consumption of approximately
4 W averaged across both datasets while the YOLO-based approach requires power
consumption of approximately 6 W (as per our power consumption model). In terms
of accuracy-latency tradeoff, the ECO-based algorithm provides near-real-time
performance (19.23 FPS) while managing to attain competitive tracking
precision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distill and De-bias: Mitigating Bias in Face Recognition using Knowledge Distillation. (arXiv:2112.09786v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09786">
<div class="article-summary-box-inner">
<span><p>Face recognition networks generally demonstrate bias with respect to
sensitive attributes like gender, skintone etc. For gender and skintone, we
observe that the regions of the face that a network attends to vary by the
category of an attribute. This might contribute to bias. Building on this
intuition, we propose a novel distillation-based approach called Distill and
De-bias (D&amp;D) to enforce a network to attend to similar face regions,
irrespective of the attribute category. In D&amp;D, we train a teacher network on
images from one category of an attribute; e.g. light skintone. Then distilling
information from the teacher, we train a student network on images of the
remaining category; e.g., dark skintone. A feature-level distillation loss
constrains the student network to generate teacher-like representations. This
allows the student network to attend to similar face regions for all attribute
categories and enables it to reduce bias. We also propose a second distillation
step on top of D&amp;D, called D&amp;D++. For the D&amp;D++ network, we distill the
`un-biasedness' of the D&amp;D network into a new student network, the D&amp;D++
network. We train the new network on all attribute categories; e.g., both light
and dark skintones. This helps us train a network that is less biased for an
attribute, while obtaining higher face verification performance than D&amp;D. We
show that D&amp;D++ outperforms existing baselines in reducing gender and skintone
bias on the IJB-C dataset, while obtaining higher face verification performance
than existing adversarial de-biasing methods. We evaluate the effectiveness of
our proposed methods on two state-of-the-art face recognition networks:
Crystalface and ArcFace.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Query Adaptive Few-Shot Object Detection with Heterogeneous Graph Convolutional Networks. (arXiv:2112.09791v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09791">
<div class="article-summary-box-inner">
<span><p>Few-shot object detection (FSOD) aims to detect never-seen objects using few
examples. This field sees recent improvement owing to the meta-learning
techniques by learning how to match between the query image and few-shot class
examples, such that the learned model can generalize to few-shot novel classes.
However, currently, most of the meta-learning-based methods perform pairwise
matching between query image regions (usually proposals) and novel classes
separately, therefore failing to take into account multiple relationships among
them. In this paper, we propose a novel FSOD model using heterogeneous graph
convolutional networks. Through efficient message passing among all the
proposal and class nodes with three different types of edges, we could obtain
context-aware proposal features and query-adaptive, multiclass-enhanced
prototype representations for each class, which could help promote the pairwise
matching and improve final FSOD accuracy. Extensive experimental results show
that our proposed model, denoted as QA-FewDet, outperforms the current
state-of-the-art approaches on the PASCAL VOC and MSCOCO FSOD benchmarks under
different shots and evaluation metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Multi-Domain Generalization through Domain Re-labeling. (arXiv:2112.09802v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09802">
<div class="article-summary-box-inner">
<span><p>Domain generalization (DG) methods aim to develop models that generalize to
settings where the test distribution is different from the training data. In
this paper, we focus on the challenging problem of multi-source zero-shot DG,
where labeled training data from multiple source domains is available but with
no access to data from the target domain. Though this problem has become an
important topic of research, surprisingly, the simple solution of pooling all
source data together and training a single classifier is highly competitive on
standard benchmarks. More importantly, even sophisticated approaches that
explicitly optimize for invariance across different domains do not necessarily
provide non-trivial gains over ERM. In this paper, for the first time, we study
the important link between pre-specified domain labels and the generalization
performance. Using a motivating case-study and a new variant of a
distributional robust optimization algorithm, GroupDRO++, we first demonstrate
how inferring custom domain groups can lead to consistent improvements over the
original domain labels that come with the dataset. Subsequently, we introduce a
general approach for multi-domain generalization, MulDEns, that uses an
ERM-based deep ensembling backbone and performs implicit domain re-labeling
through a meta-optimization algorithm. Using empirical studies on multiple
standard benchmarks, we show that MulDEns does not require tailoring the
augmentation strategy or the training process specific to a dataset,
consistently outperforms ERM by significant margins, and produces
state-of-the-art generalization performance, even when compared to existing
methods that exploit the domain labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Direct simple computation of middle surface between 3D point clouds and/or discrete surfaces by tracking sources in distance function calculation algorithms. (arXiv:2112.09808v1 [math.NA])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09808">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce novel methods for computing middle surfaces
between various 3D data sets such as point clouds and/or discrete surfaces.
Traditionally the middle surface is obtained by detecting singularities in
computed distance function such as ridges, triple junctions, etc. It requires
to compute second order differential characteristics and also some kinds of
heuristics must be applied. Opposite to that, we determine the middle surface
just from computing the distance function itself which is a fast and simple
approach. We present and compare the results of the fast sweeping method, the
vector distance transform algorithm, the fast marching method, and the
Dijkstra-Pythagoras method in finding the middle surface between 3D data sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Streaming Volumetric Image Generation Framework for Development and Evaluation of Out-of-Core Methods. (arXiv:2112.09809v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09809">
<div class="article-summary-box-inner">
<span><p>Advances in 3D imaging technology in recent years have allowed for
increasingly high resolution volumetric images of large specimen. The resulting
datasets of hundreds of Gigabytes in size call for new scalable and memory
efficient approaches in the field of image processing, where some progress has
been made already. At the same time, quantitative evaluation of these new
methods is difficult both in terms of the availability of specific data sizes
and in the generation of associated ground truth data. In this paper we present
an algorithmic framework that can be used to efficiently generate test (and
ground truth) volume data, optionally even in a streaming fashion. As the
proposed nested sweeps algorithm is fast, it can be used to generate test data
on demand. We analyze the asymptotic run time of the presented algorithm and
compare it experimentally to alternative approaches as well as a hypothetical
best-case baseline method. In a case study, the framework is applied to the
popular VascuSynth software for vascular image generation, making it capable of
efficiently producing larger-than-main memory volumes which is demonstrated by
generating a trillion voxel (1TB) image. Implementations of the presented
framework are available online in the form of the modified version of
Vascusynth and the code used for the experimental evaluation. In addition, the
test data generation procedure has been integrated into the popular volume
rendering and processing framework Voreen.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Long-Term Dependencies for Generating Dynamic Scene Graphs. (arXiv:2112.09828v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09828">
<div class="article-summary-box-inner">
<span><p>Structured video representation in the form of dynamic scene graphs is an
effective tool for several video understanding tasks. Compared to the task of
scene graph generation from images, dynamic scene graph generation is more
challenging due to the temporal dynamics of the scene and the inherent temporal
fluctuations of predictions. We show that capturing long-term dependencies is
the key to effective generation of dynamic scene graphs. We present the
detect-track-recognize paradigm by constructing consistent long-term object
tracklets from a video, followed by transformers to capture the dynamics of
objects and visual relations. Experimental results demonstrate that our Dynamic
Scene Graph Detection Transformer (DSG-DETR) outperforms state-of-the-art
methods by a significant margin on the benchmark dataset Action Genome. We also
perform ablation studies and validate the effectiveness of each component of
the proposed approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Face Deblurring Based on Separable Normalization and Adaptive Denormalization. (arXiv:2112.09833v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09833">
<div class="article-summary-box-inner">
<span><p>Face deblurring aims to restore a clear face image from a blurred input image
with more explicit structure and facial details. However, most conventional
image and face deblurring methods focus on the whole generated image resolution
without consideration of special face part texture and generally produce
unsufficient details. Considering that faces and backgrounds have different
distribution information, in this study, we designed an effective face
deblurring network based on separable normalization and adaptive
denormalization (SNADNet). First, We fine-tuned the face parsing network to
obtain an accurate face structure. Then, we divided the face parsing feature
into face foreground and background. Moreover, we constructed a new feature
adaptive denormalization to regularize fafcial structures as a condition of the
auxiliary to generate more harmonious and undistorted face structure. In
addition, we proposed a texture extractor and multi-patch discriminator to
enhance the generated facial texture information. Experimental results on both
CelebA and CelebA-HQ datasets demonstrate that the proposed face deblurring
network restores face structure with more facial details and performs favorably
against state-of-the-art methods in terms of structured similarity indexing
method (SSIM), peak signal-to-noise ratio (PSNR), Frechet inception distance
(FID) and L1, and qualitative comparisons.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Calorie Aware Automatic Meal Kit Generation from an Image. (arXiv:2112.09839v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09839">
<div class="article-summary-box-inner">
<span><p>Calorie and nutrition research has attained increased interest in recent
years. But, due to the complexity of the problem, literature in this area
focuses on a limited subset of ingredients or dish types and simple
convolutional neural networks or traditional machine learning. Simultaneously,
estimation of ingredient portions can help improve calorie estimation and meal
re-production from a given image. In this paper, given a single cooking image,
a pipeline for calorie estimation and meal re-production for different servings
of the meal is proposed. The pipeline contains two stages. In the first stage,
a set of ingredients associated with the meal in the given image are predicted.
In the second stage, given image features and ingredients, portions of the
ingredients and finally the total meal calorie are simultaneously estimated
using a deep transformer-based model. Portion estimation introduced in the
model helps improve calorie estimation and is also beneficial for meal
re-production in different serving sizes. To demonstrate the benefits of the
pipeline, the model can be used for meal kits generation. To evaluate the
pipeline, the large scale dataset Recipe1M is used. Prior to experiments, the
Recipe1M dataset is parsed and explicitly annotated with portions of
ingredients. Experiments show that using ingredients and their portions
significantly improves calorie estimation. Also, a visual interface is created
in which a user can interact with the pipeline to reach accurate calorie
estimations and generate a meal kit for cooking purposes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhanced Object Detection in Floor-plan through Super Resolution. (arXiv:2112.09844v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09844">
<div class="article-summary-box-inner">
<span><p>Building Information Modelling (BIM) software use scalable vector formats to
enable flexible designing of floor plans in the industry. Floor plans in the
architectural domain can come from many sources that may or may not be in
scalable vector format. The conversion of floor plan images to fully annotated
vector images is a process that can now be realized by computer vision. Novel
datasets in this field have been used to train Convolutional Neural Network
(CNN) architectures for object detection. Image enhancement through
Super-Resolution (SR) is also an established CNN based network in computer
vision that is used for converting low resolution images to high resolution
ones. This work focuses on creating a multi-component module that stacks a SR
model on a floor plan object detection model. The proposed stacked model shows
greater performance than the corresponding vanilla object detection model. For
the best case, the the inclusion of SR showed an improvement of 39.47% in
object detection over the vanilla network. Data and code are made publicly
available at https://github.com/rbg-research/Floor-Plan-Detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LegoDNN: Block-grained Scaling of Deep Neural Networks for Mobile Vision. (arXiv:2112.09852v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09852">
<div class="article-summary-box-inner">
<span><p>Deep neural networks (DNNs) have become ubiquitous techniques in mobile and
embedded systems for applications such as image/object recognition and
classification. The trend of executing multiple DNNs simultaneously exacerbate
the existing limitations of meeting stringent latency/accuracy requirements on
resource constrained mobile devices. The prior art sheds light on exploring the
accuracy-resource tradeoff by scaling the model sizes in accordance to resource
dynamics. However, such model scaling approaches face to imminent challenges:
(i) large space exploration of model sizes, and (ii) prohibitively long
training time for different model combinations. In this paper, we present
LegoDNN, a lightweight, block-grained scaling solution for running multi-DNN
workloads in mobile vision systems. LegoDNN guarantees short model training
times by only extracting and training a small number of common blocks (e.g. 5
in VGG and 8 in ResNet) in a DNN. At run-time, LegoDNN optimally combines the
descendant models of these blocks to maximize accuracy under specific resources
and latency constraints, while reducing switching overhead via smart
block-level scaling of the DNN. We implement LegoDNN in TensorFlow Lite and
extensively evaluate it against state-of-the-art techniques (FLOP scaling,
knowledge distillation and model compression) using a set of 12 popular DNN
models. Evaluation results show that LegoDNN provides 1,296x to 279,936x more
options in model sizes without increasing training time, thus achieving as much
as 31.74% improvement in inference accuracy and 71.07% reduction in scaling
energy consumptions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Space Non-cooperative Object Active Tracking with Deep Reinforcement Learning. (arXiv:2112.09854v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09854">
<div class="article-summary-box-inner">
<span><p>Active visual tracking of space non-cooperative object is significant for
future intelligent spacecraft to realise space debris removal, asteroid
exploration, autonomous rendezvous and docking. However, existing works often
consider this task into different subproblems (e.g. image preprocessing,
feature extraction and matching, position and pose estimation, control law
design) and optimize each module alone, which are trivial and sub-optimal. To
this end, we propose an end-to-end active visual tracking method based on DQN
algorithm, named as DRLAVT. It can guide the chasing spacecraft approach to
arbitrary space non-cooperative target merely relied on color or RGBD images,
which significantly outperforms position-based visual servoing baseline
algorithm that adopts state-of-the-art 2D monocular tracker, SiamRPN. Extensive
experiments implemented with diverse network architectures, different
perturbations and multiple targets demonstrate the advancement and robustness
of DRLAVT. In addition, We further prove our method indeed learnt the motion
patterns of target with deep reinforcement learning through hundreds of
trial-and-errors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An effective coaxiality error measurement for twist drill based on line structured light sensor. (arXiv:2112.09873v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09873">
<div class="article-summary-box-inner">
<span><p>Since the structure of twist drill is complex, it is hard and challenging for
its coaxiality error measurement. In this paper, a novel mechanism, framework
and method of coaxiality error measurement for twist drill is proposed. The
mechanism includes encoder, PLC controller, line structured sensor and high
precision turntable. First, profile point cloud data of the twist drill is
collected through the line structured light sensor when the drill turns around
in the controlling of PLC. Second, a GMM-based point cloud segmentation
algorithm based on local depth features is investigated to extract blade back
data. To improve the measurement accuracy, a statistical filter is designed to
remove outliers during the target region extraction. Then, according to two
characteristics of coaxiality error, an axis reconstruction method based on
orthogonal synthesis of axisymmetric contour differences is presented, which is
facilitated to pre-position the maximum deviation cross sections of the drill
axis. Finally, the coaxiality error is measured through fitting the benchmark
axis and the axis at the pre-positioned maximum deviation position. At the end,
a large number of experiments are carried out, and it shows that our method is
accuracy and robust.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Memory Networks for Action Prediction. (arXiv:2112.09875v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09875">
<div class="article-summary-box-inner">
<span><p>Action prediction aims to infer the forthcoming human action with
partially-observed videos, which is a challenging task due to the limited
information underlying early observations. Existing methods mainly adopt a
reconstruction strategy to handle this task, expecting to learn a single
mapping function from partial observations to full videos to facilitate the
prediction process. In this study, we propose adversarial memory networks
(AMemNet) to generate the "full video" feature conditioning on a partial video
query from two new aspects. Firstly, a key-value structured memory generator is
designed to memorize different partial videos as key memories and dynamically
write full videos in value memories with gating mechanism and querying
attention. Secondly, we develop a class-aware discriminator to guide the memory
generator to deliver not only realistic but also discriminative full video
features upon adversarial training. The final prediction result of AMemNet is
given by late fusion over RGB and optical flow streams. Extensive experimental
results on two benchmark video datasets, UCF-101 and HMDB51, are provided to
demonstrate the effectiveness of the proposed AMemNet model over
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does Explainable Machine Learning Uncover the Black Box in Vision Applications?. (arXiv:2112.09898v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09898">
<div class="article-summary-box-inner">
<span><p>Machine learning (ML) in general and deep learning (DL) in particular has
become an extremely popular tool in several vision applications (like object
detection, super resolution, segmentation, object tracking etc.). Almost in
parallel, the issue of explainability in ML (i.e. the ability to
explain/elaborate the way a trained ML model arrived at its decision) in vision
has also received fairly significant attention from various quarters. However,
we argue that the current philosophy behind explainable ML suffers from certain
limitations, and the resulting explanations may not meaningfully uncover black
box ML models. To elaborate our assertion, we first raise a few fundamental
questions which have not been adequately discussed in the corresponding
literature. We also provide perspectives on how explainablity in ML can benefit
by relying on more rigorous principles in the related areas.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Instance Segmentation of MVS Buildings. (arXiv:2112.09902v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09902">
<div class="article-summary-box-inner">
<span><p>We present a novel framework for instance segmentation of 3D buildings from
Multi-view Stereo (MVS) urban scenes. Unlike existing works focusing on
semantic segmentation of an urban scene, the emphasis of this work lies in
detecting and segmenting 3D building instances even if they are attached and
embedded in a large and imprecise 3D surface model. Multi-view RGB images are
first enhanced to RGBH images by adding a heightmap and are segmented to obtain
all roof instances using a fine-tuned 2D instance segmentation neural network.
Roof instance masks from different multi-view images are then clustered into
global masks. Our mask clustering accounts for spatial occlusion and
overlapping, which can eliminate segmentation ambiguities among multi-view
images. Based on these global masks, 3D roof instances are segmented out by
mask back-projections and extended to the entire building instances through a
Markov random field (MRF) optimization. Quantitative evaluations and ablation
studies have shown the effectiveness of all major steps of the method. A
dataset for the evaluation of instance segmentation of 3D building models is
provided as well. To the best of our knowledge, it is the first dataset for 3D
urban buildings on the instance segmentation level.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anomaly Discovery in Semantic Segmentation via Distillation Comparison Networks. (arXiv:2112.09908v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09908">
<div class="article-summary-box-inner">
<span><p>This paper aims to address the problem of anomaly discovery in semantic
segmentation. Our key observation is that semantic classification plays a
critical role in existing approaches, while the incorrectly classified pixels
are easily regarded as anomalies. Such a phenomenon frequently appears and is
rarely discussed, which significantly reduces the performance of anomaly
discovery. To this end, we propose a novel Distillation Comparison Network
(DiCNet). It comprises of a teacher branch which is a semantic segmentation
network that removed the semantic classification head, and a student branch
that is distilled from the teacher branch through a distribution distillation.
We show that the distillation guarantees the semantic features of the two
branches hold consistency in the known classes, while reflect inconsistency in
the unknown class. Therefore, we leverage the semantic feature discrepancy
between the two branches to discover the anomalies. DiCNet abandons the
semantic classification head in the inference process, and hence significantly
alleviates the issue caused by incorrect semantic classification. Extensive
experimental results on StreetHazards dataset and BDD-Anomaly dataset are
conducted to verify the superior performance of DiCNet. In particular, DiCNet
obtains a 6.3% improvement in AUPR and a 5.2% improvement in FPR95 on
StreetHazards dataset, achieves a 4.2% improvement in AUPR and a 6.8%
improvement in FPR95 on BDD-Anomaly dataset. Codes are available at
https://github.com/zhouhuan-hust/DiCNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast and Robust Registration of Partially Overlapping Point Clouds. (arXiv:2112.09922v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09922">
<div class="article-summary-box-inner">
<span><p>Real-time registration of partially overlapping point clouds has emerging
applications in cooperative perception for autonomous vehicles and multi-agent
SLAM. The relative translation between point clouds in these applications is
higher than in traditional SLAM and odometry applications, which challenges the
identification of correspondences and a successful registration. In this paper,
we propose a novel registration method for partially overlapping point clouds
where correspondences are learned using an efficient point-wise feature
encoder, and refined using a graph-based attention network. This attention
network exploits geometrical relationships between key points to improve the
matching in point clouds with low overlap. At inference time, the relative pose
transformation is obtained by robustly fitting the correspondences through
sample consensus. The evaluation is performed on the KITTI dataset and a novel
synthetic dataset including low-overlapping point clouds with displacements of
up to 30m. The proposed method achieves on-par performance with
state-of-the-art methods on the KITTI dataset, and outperforms existing methods
for low overlapping point clouds. Additionally, the proposed method achieves
significantly faster inference times, as low as 410ms, between 5 and 35 times
faster than competing methods. Our code and dataset are available at
https://github.com/eduardohenriquearnold/fastreg.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepUME: Learning the Universal Manifold Embedding for Robust Point Cloud Registration. (arXiv:2112.09938v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09938">
<div class="article-summary-box-inner">
<span><p>Registration of point clouds related by rigid transformations is one of the
fundamental problems in computer vision. However, a solution to the practical
scenario of aligning sparsely and differently sampled observations in the
presence of noise is still lacking. We approach registration in this scenario
with a fusion of the closed-form Universal Mani-fold Embedding (UME) method and
a deep neural network. The two are combined into a single unified framework,
named DeepUME, trained end-to-end and in an unsupervised manner. To
successfully provide a global solution in the presence of large
transformations, we employ an SO(3)-invariant coordinate system to learn both a
joint-resampling strategy of the point clouds and SO(3)-invariant features.
These features are then utilized by the geometric UME method for transformation
estimation. The parameters of DeepUME are optimized using a metric designed to
overcome an ambiguity problem emerging in the registration of symmetric shapes,
when noisy scenarios are considered. We show that our hybrid method outperforms
state-of-the-art registration methods in various scenarios, and generalizes
well to unseen data sets. Our code is publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rapid Face Mask Detection and Person Identification Model based on Deep Neural Networks. (arXiv:2112.09951v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09951">
<div class="article-summary-box-inner">
<span><p>As Covid-19 has been constantly getting mutated and in three or four months a
new variant gets introduced to us and it comes with more deadly problems. The
things that prevent us from getting Covid is getting vaccinated and wearing a
face mask. In this paper, we have implemented a new Face Mask Detection and
Person Recognition model named Insight face which is based on SoftMax loss
classification algorithm Arc Face loss and names it as RFMPI-DNN(Rapid Face
Detection and Peron Identification Model based on Deep Neural Networks) to
detect face mask and person identity rapidly as compared to other models
available. To compare our new model, we have used previous MobileNet_V2 model
and face recognition module for effective comparison on the basis of time. The
proposed model implemented in the system has outperformed the model compared in
this paper in every aspect
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pre-Training Transformers for Domain Adaptation. (arXiv:2112.09965v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09965">
<div class="article-summary-box-inner">
<span><p>The Visual Domain Adaptation Challenge 2021 called for unsupervised domain
adaptation methods that could improve the performance of models by transferring
the knowledge obtained from source datasets to out-of-distribution target
datasets. In this paper, we utilize BeiT [1] and demonstrate its capability of
capturing key attributes from source datasets and apply it to target datasets
in a semi-supervised manner. Our method was able to outperform current
state-of-the-art (SoTA) techniques and was able to achieve 1st place on the
ViSDA Domain Adaptation Challenge with ACC of 56.29% and AUROC of 69.79%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Structural Analysis of the Optic Nerve Head to Robustly Discriminate Between Papilledema and Optic Disc Drusen. (arXiv:2112.09970v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09970">
<div class="article-summary-box-inner">
<span><p>Purpose: (1) To develop a deep learning algorithm to identify major tissue
structures of the optic nerve head (ONH) in 3D optical coherence tomography
(OCT) scans; (2) to exploit such information to robustly differentiate among
healthy, optic disc drusen (ODD), and papilledema ONHs.
</p>
<p>It was a cross-sectional comparative study with confirmed ODD (105 eyes),
papilledema due to high intracranial pressure (51 eyes), and healthy controls
(100 eyes). 3D scans of the ONHs were acquired using OCT, then processed to
improve deep-tissue visibility. At first, a deep learning algorithm was
developed using 984 B-scans (from 130 eyes) in order to identify: major
neural/connective tissues, and ODD regions. The performance of our algorithm
was assessed using the Dice coefficient (DC). In a 2nd step, a classification
algorithm (random forest) was designed using 150 OCT volumes to perform 3-class
classifications (1: ODD, 2: papilledema, 3: healthy) strictly from their drusen
and prelamina swelling scores (derived from the segmentations). To assess
performance, we reported the area under the receiver operating characteristic
curves (AUCs) for each class.
</p>
<p>Our segmentation algorithm was able to isolate neural and connective tissues,
and ODD regions whenever present. This was confirmed by an average DC of
0.93$\pm$0.03 on the test set, corresponding to good performance.
Classification was achieved with high AUCs, i.e. 0.99$\pm$0.01 for the
detection of ODD, 0.99 $\pm$ 0.01 for the detection of papilledema, and
0.98$\pm$0.02 for the detection of healthy ONHs.
</p>
<p>Our AI approach accurately discriminated ODD from papilledema, using a single
OCT scan. Our classification performance was excellent, with the caveat that
validation in a much larger population is warranted. Our approach may have the
potential to establish OCT as the mainstay of diagnostic imaging in
neuro-ophthalmology.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tell me what you see: A zero-shot action recognition method based on natural language descriptions. (arXiv:2112.09976v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09976">
<div class="article-summary-box-inner">
<span><p>Recently, several approaches have explored the detection and classification
of objects in videos to perform Zero-Shot Action Recognition with remarkable
results. In these methods, class-object relationships are used to associate
visual patterns with the semantic side information because these relationships
also tend to appear in texts. Therefore, word vector methods would reflect them
in their latent representations. Inspired by these methods and by video
captioning's ability to describe events not only with a set of objects but with
contextual information, we propose a method in which video captioning models,
called observers, provide different and complementary descriptive sentences. We
demonstrate that representing videos with descriptive sentences instead of deep
features, in ZSAR, is viable and naturally alleviates the domain adaptation
problem, as we reached state-of-the-art (SOTA) performance on the UCF101
dataset and competitive performance on HMDB51 without their training sets. We
also demonstrate that word vectors are unsuitable for building the semantic
embedding space of our descriptions. Thus, we propose to represent the classes
with sentences extracted from documents acquired with search engines on the
Internet, without any human evaluation on the quality of descriptions. Lastly,
we build a shared semantic space employing BERT-based embedders pre-trained in
the paraphrasing task on multiple text datasets. We show that this pre-training
is essential for bridging the semantic gap. The projection onto this space is
straightforward for both types of information, visual and semantic, because
they are sentences, enabling the classification with nearest neighbour rule in
this shared space. Our code is available at
https://github.com/valterlej/zsarcap.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Domain Federated Learning in Medical Imaging. (arXiv:2112.10001v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10001">
<div class="article-summary-box-inner">
<span><p>Federated learning is increasingly being explored in the field of medical
imaging to train deep learning models on large scale datasets distributed
across different data centers while preserving privacy by avoiding the need to
transfer sensitive patient information. In this manuscript, we explore
federated learning in a multi-domain, multi-task setting wherein different
participating nodes may contain datasets sourced from different domains and are
trained to solve different tasks. We evaluated cross-domain federated learning
for the tasks of object detection and segmentation across two different
experimental settings: multi-modal and multi-organ. The result from our
experiments on cross-domain federated learning framework were very encouraging
with an overlap similarity of 0.79 for organ localization and 0.65 for lesion
segmentation. Our results demonstrate the potential of federated learning in
developing multi-domain, multi-task deep learning models without sharing data
from different domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt-Based Multi-Modal Image Segmentation. (arXiv:2112.10003v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10003">
<div class="article-summary-box-inner">
<span><p>Image segmentation is usually addressed by training a model for a fixed set
of object classes. Incorporating additional classes or more complex queries
later is expensive as it requires re-training the model on a dataset that
encompasses these expressions. Here we propose a system that can generate image
segmentations based on arbitrary prompts at test time. A prompt can be either a
text or an image. This approach enables us to create a unified model (trained
once) for three common segmentation tasks, which come with distinct challenges:
referring expression segmentation, zero-shot segmentation and one-shot
segmentation. We build upon the CLIP model as a backbone which we extend with a
transformer-based decoder that enables dense prediction. After training on an
extended version of the PhraseCut dataset, our system generates a binary
segmentation map for an image based on a free-text prompt or on an additional
image expressing the query. Different variants of the latter image-based
prompts are analyzed in detail. This novel hybrid input allows for dynamic
adaptation not only to the three segmentation tasks mentioned above, but to any
binary segmentation task where a text or image query can be formulated.
Finally, we find our system to adapt well to generalized queries involving
affordances or properties. Source code: https://eckerlab.org/code/clipseg
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual Learning of a Mixed Sequence of Similar and Dissimilar Tasks. (arXiv:2112.10017v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10017">
<div class="article-summary-box-inner">
<span><p>Existing research on continual learning of a sequence of tasks focused on
dealing with catastrophic forgetting, where the tasks are assumed to be
dissimilar and have little shared knowledge. Some work has also been done to
transfer previously learned knowledge to the new task when the tasks are
similar and have shared knowledge. To the best of our knowledge, no technique
has been proposed to learn a sequence of mixed similar and dissimilar tasks
that can deal with forgetting and also transfer knowledge forward and backward.
This paper proposes such a technique to learn both types of tasks in the same
network. For dissimilar tasks, the algorithm focuses on dealing with
forgetting, and for similar tasks, the algorithm focuses on selectively
transferring the knowledge learned from some similar previous tasks to improve
the new task learning. Additionally, the algorithm automatically detects
whether a new task is similar to any previous tasks. Empirical evaluation using
sequences of mixed tasks demonstrates the effectiveness of the proposed model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Supervised laser-speckle image sampling of skin tissue to detect very early stage of diabetes by its effects on skin subcellular properties. (arXiv:2112.10024v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10024">
<div class="article-summary-box-inner">
<span><p>This paper investigates the effectiveness of an expert system based on
K-nearest neighbors algorithm for laser speckle image sampling applied to the
early detection of diabetes. With the latest developments in artificial
intelligent guided laser speckle imaging technologies, it may be possible to
optimise laser parameters, such as wavelength, energy level and image texture
measures in association with a suitable AI technique to interact effectively
with the subcellular properties of a skin tissue to detect early signs of
diabetes. The new approach is potentially more effective than the classical
skin glucose level observation because of its optimised combination of laser
physics and AI techniques, and additionally, it allows non-expert individuals
to perform more frequent skin tissue tests for an early detection of diabetes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A-ESRGAN: Training Real-World Blind Super-Resolution with Attention U-Net Discriminators. (arXiv:2112.10046v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10046">
<div class="article-summary-box-inner">
<span><p>Blind image super-resolution(SR) is a long-standing task in CV that aims to
restore low-resolution images suffering from unknown and complex distortions.
Recent work has largely focused on adopting more complicated degradation models
to emulate real-world degradations. The resulting models have made
breakthroughs in perceptual loss and yield perceptually convincing results.
However, the limitation brought by current generative adversarial network
structures is still significant: treating pixels equally leads to the ignorance
of the image's structural features, and results in performance drawbacks such
as twisted lines and background over-sharpening or blurring. In this paper, we
present A-ESRGAN, a GAN model for blind SR tasks featuring an attention U-Net
based, multi-scale discriminator that can be seamlessly integrated with other
generators. To our knowledge, this is the first work to introduce attention
U-Net structure as the discriminator of GAN to solve blind SR problems. And the
paper also gives an interpretation for the mechanism behind multi-scale
attention U-Net that brings performance breakthrough to the model. Through
comparison experiments with prior works, our model presents state-of-the-art
level performance on the non-reference natural image quality evaluator metric.
And our ablation studies have shown that with our discriminator, the RRDB based
generator can leverage the structural features of an image in multiple scales,
and consequently yields more perceptually realistic high-resolution images
compared to prior works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controlling the Quality of Distillation in Response-Based Network Compression. (arXiv:2112.10047v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10047">
<div class="article-summary-box-inner">
<span><p>The performance of a distillation-based compressed network is governed by the
quality of distillation. The reason for the suboptimal distillation of a large
network (teacher) to a smaller network (student) is largely attributed to the
gap in the learning capacities of given teacher-student pair. While it is hard
to distill all the knowledge of a teacher, the quality of distillation can be
controlled to a large extent to achieve better performance. Our experiments
show that the quality of distillation is largely governed by the quality of
teacher's response, which in turn is heavily affected by the presence of
similarity information in its response. A well-trained large capacity teacher
loses similarity information between classes in the process of learning
fine-grained discriminative properties for classification. The absence of
similarity information causes the distillation process to be reduced from one
example-many class learning to one example-one class learning, thereby
throttling the flow of diverse knowledge from the teacher. With the implicit
assumption that only the instilled knowledge can be distilled, instead of
focusing only on the knowledge distilling process, we scrutinize the knowledge
inculcation process. We argue that for a given teacher-student pair, the
quality of distillation can be improved by finding the sweet spot between batch
size and number of epochs while training the teacher. We discuss the steps to
find this sweet spot for better distillation. We also propose the distillation
hypothesis to differentiate the behavior of the distillation process between
knowledge distillation and regularization effect. We conduct all our
experiments on three different datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Precondition and Effect Reasoning for Action Recognition. (arXiv:2112.10057v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10057">
<div class="article-summary-box-inner">
<span><p>Human action recognition has drawn a lot of attention in the recent years due
to the research and application significance. Most existing works on action
recognition focus on learning effective spatial-temporal features from videos,
but neglect the strong causal relationship among the precondition, action and
effect. Such relationships are also crucial to the accuracy of action
recognition. In this paper, we propose to model the causal relationships based
on the precondition and effect to improve the performance of action
recognition. Specifically, a Cycle-Reasoning model is proposed to capture the
causal relationships for action recognition. To this end, we annotate
precondition and effect for a large-scale action dataset. Experimental results
show that the proposed Cycle-Reasoning model can effectively reason about the
precondition and effect and can enhance action recognition performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Graph-level Anomaly Detection by Glocal Knowledge Distillation. (arXiv:2112.10063v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10063">
<div class="article-summary-box-inner">
<span><p>Graph-level anomaly detection (GAD) describes the problem of detecting graphs
that are abnormal in their structure and/or the features of their nodes, as
compared to other graphs. One of the challenges in GAD is to devise graph
representations that enable the detection of both locally- and
globally-anomalous graphs, i.e., graphs that are abnormal in their fine-grained
(node-level) or holistic (graph-level) properties, respectively. To tackle this
challenge we introduce a novel deep anomaly detection approach for GAD that
learns rich global and local normal pattern information by joint random
distillation of graph and node representations. The random distillation is
achieved by training one GNN to predict another GNN with randomly initialized
network weights. Extensive experiments on 16 real-world graph datasets from
diverse domains show that our model significantly outperforms seven
state-of-the-art models. Code and datasets are available at
https://git.io/GLocalKD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Strong Scaling Through Burst Parallel Training. (arXiv:2112.10065v1 [cs.DC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10065">
<div class="article-summary-box-inner">
<span><p>As emerging deep neural network (DNN) models continue to grow in size, using
large GPU clusters to train DNNs is becoming an essential requirement to
achieving acceptable training times. In this paper, we consider the case where
future increases in cluster size will cause the global batch size that can be
used to train models to reach a fundamental limit: beyond a certain point,
larger global batch sizes cause sample efficiency to degrade, increasing
overall time to accuracy. As a result, to achieve further improvements in
training performance, we must instead consider "strong scaling" strategies that
hold the global batch size constant and allocate smaller batches to each GPU.
Unfortunately, this makes it significantly more difficult to use cluster
resources efficiently. We present DeepPool, a system that addresses this
efficiency challenge through two key ideas. First, burst parallelism allocates
large numbers of GPUs to foreground jobs in bursts to exploit the unevenness in
parallelism across layers. Second, GPU multiplexing prioritizes throughput for
foreground training jobs, while packing in background training jobs to reclaim
underutilized GPU resources, thereby improving cluster-wide utilization.
Together, these two ideas enable DeepPool to deliver a 2.2 - 2.4x improvement
in total cluster throughput over standard data parallelism with a single task
when the cluster scale is large.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LocFormer: Enabling Transformers to Perform Temporal Moment Localization on Long Untrimmed Videos With a Feature Sampling Approach. (arXiv:2112.10066v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10066">
<div class="article-summary-box-inner">
<span><p>We propose LocFormer, a Transformer-based model for video grounding which
operates at a constant memory footprint regardless of the video length, i.e.
number of frames. LocFormer is designed for tasks where it is necessary to
process the entire long video and at its core lie two main contributions.
First, our model incorporates a new sampling technique that splits the input
feature sequence into a fixed number of sections and selects a single feature
per section using a stochastic approach, which allows us to obtain a feature
sample set that is representative of the video content for the task at hand
while keeping the memory footprint constant. Second, we propose a modular
design that separates functionality, enabling us to learn an inductive bias via
supervising the self-attention heads, while also effectively leveraging
pre-trained text and video encoders. We test our proposals on relevant
benchmark datasets for video grounding, showing that not only LocFormer can
achieve excellent results including state-of-the-art performance on YouCookII,
but also that our sampling technique is more effective than competing
counterparts and that it consistently improves the performance of prior work,
by up to 3.13\% in the mean temporal IoU, ultimately leading to a new
state-of-the-art performance on Charades-STA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A New Image Codec Paradigm for Human and Machine Uses. (arXiv:2112.10071v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10071">
<div class="article-summary-box-inner">
<span><p>With the AI of Things (AIoT) development, a huge amount of visual data, e.g.,
images and videos, are produced in our daily work and life. These visual data
are not only used for human viewing or understanding but also for machine
analysis or decision-making, e.g., intelligent surveillance, automated
vehicles, and many other smart city applications. To this end, a new image
codec paradigm for both human and machine uses is proposed in this work.
Firstly, the high-level instance segmentation map and the low-level signal
features are extracted with neural networks. Then, the instance segmentation
map is further represented as a profile with the proposed 16-bit gray-scale
representation. After that, both 16-bit gray-scale profile and signal features
are encoded with a lossless codec. Meanwhile, an image predictor is designed
and trained to achieve the general-quality image reconstruction with the 16-bit
gray-scale profile and signal features. Finally, the residual map between the
original image and the predicted one is compressed with a lossy codec, used for
high-quality image reconstruction. With such designs, on the one hand, we can
achieve scalable image compression to meet the requirements of different human
consumption; on the other hand, we can directly achieve several machine vision
tasks at the decoder side with the decoded 16-bit gray-scale profile, e.g.,
object classification, detection, and segmentation. Experimental results show
that the proposed codec achieves comparable results as most learning-based
codecs and outperforms the traditional codecs (e.g., BPG and JPEG2000) in terms
of PSNR and MS-SSIM for image reconstruction. At the same time, it outperforms
the existing codecs in terms of the mAP for object detection and segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QU-BraTS: MICCAI BraTS 2020 Challenge on Quantifying Uncertainty in Brain Tumor Segmentation -- Analysis of Ranking Metrics and Benchmarking Results. (arXiv:2112.10074v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10074">
<div class="article-summary-box-inner">
<span><p>Deep learning (DL) models have provided the state-of-the-art performance in a
wide variety of medical imaging benchmarking challenges, including the Brain
Tumor Segmentation (BraTS) challenges. However, the task of focal pathology
multi-compartment segmentation (e.g., tumor and lesion sub-regions) is
particularly challenging, and potential errors hinder the translation of DL
models into clinical workflows. Quantifying the reliability of DL model
predictions in the form of uncertainties, could enable clinical review of the
most uncertain regions, thereby building trust and paving the way towards
clinical translation. Recently, a number of uncertainty estimation methods have
been introduced for DL medical image segmentation tasks. Developing metrics to
evaluate and compare the performance of uncertainty measures will assist the
end-user in making more informed decisions. In this study, we explore and
evaluate a metric developed during the BraTS 2019-2020 task on uncertainty
quantification (QU-BraTS), and designed to assess and rank uncertainty
estimates for brain tumor multi-compartment segmentation. This metric (1)
rewards uncertainty estimates that produce high confidence in correct
assertions, and those that assign low confidence levels at incorrect
assertions, and (2) penalizes uncertainty measures that lead to a higher
percentages of under-confident correct assertions. We further benchmark the
segmentation uncertainties generated by 14 independent participating teams of
QU-BraTS 2020, all of which also participated in the main BraTS segmentation
task. Overall, our findings confirm the importance and complementary value that
uncertainty estimates provide to segmentation algorithms, and hence highlight
the need for uncertainty quantification in medical image analyses. Our
evaluation code is made publicly available at
https://github.com/RagMeh11/QU-BraTS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MoCaNet: Motion Retargeting in-the-wild via Canonicalization Networks. (arXiv:2112.10082v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10082">
<div class="article-summary-box-inner">
<span><p>We present a novel framework that brings the 3D motion retargeting task from
controlled environments to in-the-wild scenarios. In particular, our method is
capable of retargeting body motion from a character in a 2D monocular video to
a 3D character without using any motion capture system or 3D reconstruction
procedure. It is designed to leverage massive online videos for unsupervised
training, needless of 3D annotations or motion-body pairing information. The
proposed method is built upon two novel canonicalization operations, structure
canonicalization and view canonicalization. Trained with the canonicalization
operations and the derived regularizations, our method learns to factorize a
skeleton sequence into three independent semantic subspaces, i.e., motion,
structure, and view angle. The disentangled representation enables motion
retargeting from 2D to 3D with high precision. Our method achieves superior
performance on motion transfer benchmarks with large body variations and
challenging actions. Notably, the canonicalized skeleton sequence could serve
as a disentangled and interpretable representation of human motion that
benefits action analysis and motion retrieval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reasoning Structural Relation for Occlusion-Robust Facial Landmark Localization. (arXiv:2112.10087v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10087">
<div class="article-summary-box-inner">
<span><p>In facial landmark localization tasks, various occlusions heavily degrade the
localization accuracy due to the partial observability of facial features. This
paper proposes a structural relation network (SRN) for occlusion-robust
landmark localization. Unlike most existing methods that simply exploit the
shape constraint, the proposed SRN aims to capture the structural relations
among different facial components. These relations can be considered a more
powerful shape constraint against occlusion. To achieve this, a hierarchical
structural relation module (HSRM) is designed to hierarchically reason the
structural relations that represent both long- and short-distance spatial
dependencies. Compared with existing network architectures, HSRM can
efficiently model the spatial relations by leveraging its geometry-aware
network architecture, which reduces the semantic ambiguity caused by occlusion.
Moreover, the SRN augments the training data by synthesizing occluded faces. To
further extend our SRN for occluded video data, we formulate the occluded face
synthesis as a Markov decision process (MDP). Specifically, it plans the
movement of the dynamic occlusion based on an accumulated reward associated
with the performance degradation of the pre-trained SRN. This procedure
augments hard samples for robust facial landmark tracking. Extensive
experimental results indicate that the proposed method achieves outstanding
performance on occluded and masked faces. Code is available at
https://github.com/zhuccly/SRN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Camera-aware Style Separation and Contrastive Learning for Unsupervised Person Re-identification. (arXiv:2112.10089v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10089">
<div class="article-summary-box-inner">
<span><p>Unsupervised person re-identification (ReID) is a challenging task without
data annotation to guide discriminative learning. Existing methods attempt to
solve this problem by clustering extracted embeddings to generate pseudo
labels. However, most methods ignore the intra-class gap caused by camera style
variance, and some methods are relatively complex and indirect although they
try to solve the negative impact of the camera style on feature distribution.
To solve this problem, we propose a camera-aware style separation and
contrastive learning method (CA-UReID), which directly separates camera styles
in the feature space with the designed camera-aware attention module. It can
explicitly divide the learnable feature into camera-specific and
camera-agnostic parts, reducing the influence of different cameras. Moreover,
to further narrow the gap across cameras, we design a camera-aware contrastive
center loss to learn more discriminative embedding for each identity. Extensive
experiments demonstrate the superiority of our method over the state-of-the-art
methods on the unsupervised person ReID task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Initiative Defense against Facial Manipulation. (arXiv:2112.10098v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10098">
<div class="article-summary-box-inner">
<span><p>Benefiting from the development of generative adversarial networks (GAN),
facial manipulation has achieved significant progress in both academia and
industry recently. It inspires an increasing number of entertainment
applications but also incurs severe threats to individual privacy and even
political security meanwhile. To mitigate such risks, many countermeasures have
been proposed. However, the great majority methods are designed in a passive
manner, which is to detect whether the facial images or videos are tampered
after their wide propagation. These detection-based methods have a fatal
limitation, that is, they only work for ex-post forensics but can not prevent
the engendering of malicious behavior. To address the limitation, in this
paper, we propose a novel framework of initiative defense to degrade the
performance of facial manipulation models controlled by malicious users. The
basic idea is to actively inject imperceptible venom into target facial data
before manipulation. To this end, we first imitate the target manipulation
model with a surrogate model, and then devise a poison perturbation generator
to obtain the desired venom. An alternating training strategy are further
leveraged to train both the surrogate model and the perturbation generator. Two
typical facial manipulation tasks: face attribute editing and face reenactment,
are considered in our initiative defense framework. Extensive experiments
demonstrate the effectiveness and robustness of our framework in different
settings. Finally, we hope this work can shed some light on initiative
countermeasures against more adversarial scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ArcFace Knows the Gender, Too!. (arXiv:2112.10101v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10101">
<div class="article-summary-box-inner">
<span><p>The main idea of this paper is that if a model can recognize a person, of
course, it must be able to know the gender of that person, too. Therefore,
instead of defining a new model for gender classification, this paper uses
ArcFace features to determine gender, based on the facial features. A face
image is given to ArcFace and 512 features are obtained for the face. Then,
with the help of traditional machine learning models, gender is determined.
Discriminative methods such as Support Vector Machine (SVM), Linear
Discriminant, and Logistic Regression well demonstrate that the features
extracted from the ArcFace create a remarkable distinction between the gender
classes. Experiments on the Gender Classification Dataset show that SVM with
Gaussian kernel is able to classify gender with an accuracy of 96.4% using
ArcFace features.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SAGA: Stochastic Whole-Body Grasping with Contact. (arXiv:2112.10103v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10103">
<div class="article-summary-box-inner">
<span><p>Human grasping synthesis has numerous applications including AR/VR, video
games, and robotics. While some methods have been proposed to generate
realistic hand-object interaction for object grasping and manipulation, they
typically only consider the hand interacting with objects. In this work, our
goal is to synthesize whole-body grasping motion. Given a 3D object, we aim to
generate diverse and natural whole-body human motions that approach and grasp
the object. This task is challenging as it requires modeling both whole-body
dynamics and dexterous finger movements. To this end, we propose SAGA
(StochAstic whole-body Grasping with contAct) which consists of two key
components: (a) Static whole-body grasping pose generation. Specifically, we
propose a multi-task generative model, to jointly learn static whole-body
grasping poses and human-object contacts. (b) Grasping motion infilling. Given
an initial pose and the generated whole-body grasping pose as the starting and
ending poses of the motion respectively, we design a novel contact-aware
generative motion infilling module to generate a diverse set of grasp-oriented
motions. We demonstrate the effectiveness of our method being the first
generative framework to synthesize realistic and expressive whole-body motions
that approach and grasp randomly placed unseen objects. The code and videos are
available at: https://jiahaoplus.github.io/SAGA/saga.html.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anisotropic mesh adaptation for region-based segmentation accounting for image spatial information. (arXiv:2112.10138v1 [math.NA])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10138">
<div class="article-summary-box-inner">
<span><p>A finite element-based image segmentation strategy enhanced by an anisotropic
mesh adaptation procedure is presented. The methodology relies on a split
Bregman algorithm for the minimisation of a region-based energy functional and
on an anisotropic recovery-based error estimate to drive mesh adaptation. More
precisely, a Bayesian energy functional is considered to account for image
spatial information, ensuring that the methodology is able to identify
inhomogeneous spatial patterns in complex images. In addition, the anisotropic
mesh adaptation guarantees a sharp detection of the interface between
background and foreground of the image, with a reduced number of degrees of
freedom. The resulting split-adapt Bregman algorithm is tested on a set of real
images showing the accuracy and robustness of the method, even in the presence
of Gaussian, salt and pepper and speckle noise.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Denoised Labels for Financial Time-Series Data via Self-Supervised Learning. (arXiv:2112.10139v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10139">
<div class="article-summary-box-inner">
<span><p>The introduction of electronic trading platforms effectively changed the
organisation of traditional systemic trading from quote-driven markets into
order-driven markets. Its convenience led to an exponentially increasing amount
of financial data, which is however hard to use for the prediction of future
prices, due to the low signal-to-noise ratio and the non-stationarity of
financial time series. Simpler classification tasks -- where the goal is to
predict the directions of future price movement -- via supervised learning
algorithms, need sufficiently reliable labels to generalise well. Labelling
financial data is however less well defined than other domains: did the price
go up because of noise or because of signal? The existing labelling methods
have limited countermeasures against noise and limited effects in improving
learning algorithms. This work takes inspiration from image classification in
trading and success in self-supervised learning. We investigate the idea of
applying computer vision techniques to financial time-series to reduce the
noise exposure and hence generate correct labels. We look at the label
generation as the pretext task of a self-supervised learning approach and
compare the naive (and noisy) labels, commonly used in the literature, with the
labels generated by a denoising autoencoder for the same downstream
classification task. Our results show that our denoised labels improve the
performances of the downstream learning algorithm, for both small and large
datasets. We further show that the signals we obtain can be used to effectively
trade with binary strategies. We suggest that with proposed techniques,
self-supervised learning constitutes a powerful framework for generating
"better" financial labels that are useful for studying the underlying patterns
of the market.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RoboAssembly: Learning Generalizable Furniture Assembly Policy in a Novel Multi-robot Contact-rich Simulation Environment. (arXiv:2112.10143v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10143">
<div class="article-summary-box-inner">
<span><p>Part assembly is a typical but challenging task in robotics, where robots
assemble a set of individual parts into a complete shape. In this paper, we
develop a robotic assembly simulation environment for furniture assembly. We
formulate the part assembly task as a concrete reinforcement learning problem
and propose a pipeline for robots to learn to assemble a diverse set of chairs.
Experiments show that when testing with unseen chairs, our approach achieves a
success rate of 74.5% under the object-centric setting and 50.0% under the full
setting. We adopt an RRT-Connect algorithm as the baseline, which only achieves
a success rate of 18.8% after a significantly longer computation time.
Supplemental materials and videos are available on our project webpage.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Elastic-Link for Binarized Neural Network. (arXiv:2112.10149v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10149">
<div class="article-summary-box-inner">
<span><p>Recent work has shown that Binarized Neural Networks (BNNs) are able to
greatly reduce computational costs and memory footprints, facilitating model
deployment on resource-constrained devices. However, in comparison to their
full-precision counterparts, BNNs suffer from severe accuracy degradation.
Research aiming to reduce this accuracy gap has thus far largely focused on
specific network architectures with few or no 1x1 convolutional layers, for
which standard binarization methods do not work well. Because 1x1 convolutions
are common in the design of modern architectures (e.g. GoogleNet, ResNet,
DenseNet), it is crucial to develop a method to binarize them effectively for
BNNs to be more widely adopted. In this work, we propose an "Elastic-Link" (EL)
module to enrich information flow within a BNN by adaptively adding real-valued
input features to the subsequent convolutional output features. The proposed EL
module is easily implemented and can be used in conjunction with other methods
for BNNs. We demonstrate that adding EL to BNNs produces a significant
improvement on the challenging large-scale ImageNet dataset. For example, we
raise the top-1 accuracy of binarized ResNet26 from 57.9% to 64.0%. EL also
aids convergence in the training of binarized MobileNet, for which a top-1
accuracy of 56.4% is achieved. Finally, with the integration of ReActNet, it
yields a new state-of-the-art result of 71.9% top-1 accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topology Preserving Local Road Network Estimation from Single Onboard Camera Image. (arXiv:2112.10155v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10155">
<div class="article-summary-box-inner">
<span><p>Knowledge of the road network topology is crucial for autonomous planning and
navigation. Yet, recovering such topology from a single image has only been
explored in part. Furthermore, it needs to refer to the ground plane, where
also the driving actions are taken. This paper aims at extracting the local
road network topology, directly in the bird's-eye-view (BEV), all in a complex
urban setting. The only input consists of a single onboard, forward looking
camera image. We represent the road topology using a set of directed lane
curves and their interactions, which are captured using their intersection
points. To better capture topology, we introduce the concept of \emph{minimal
cycles} and their covers. A minimal cycle is the smallest cycle formed by the
directed curve segments (between two intersections). The cover is a set of
curves whose segments are involved in forming a minimal cycle. We first show
that the covers suffice to uniquely represent the road topology. The covers are
then used to supervise deep neural networks, along with the lane curve
supervision. These learn to predict the road topology from a single input
image. The results on the NuScenes and Argoverse benchmarks are significantly
better than those obtained with baselines. Our source code will be made
publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Face-Based Age Estimation with Attention-Based Dynamic Patch Fusion. (arXiv:2112.10167v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10167">
<div class="article-summary-box-inner">
<span><p>With the increasing popularity of convolutional neural networks (CNNs),
recent works on face-based age estimation employ these networks as the
backbone. However, state-of-the-art CNN-based methods treat each facial region
equally, thus entirely ignoring the importance of some facial patches that may
contain rich age-specific information. In this paper, we propose a face-based
age estimation framework, called Attention-based Dynamic Patch Fusion (ADPF).
In ADPF, two separate CNNs are implemented, namely the AttentionNet and the
FusionNet. The AttentionNet dynamically locates and ranks age-specific patches
by employing a novel Ranking-guided Multi-Head Hybrid Attention (RMHHA)
mechanism. The FusionNet uses the discovered patches along with the facial
image to predict the age of the subject. Since the proposed RMHHA mechanism
ranks the discovered patches based on their importance, the length of the
learning path of each patch in the FusionNet is proportional to the amount of
information it carries (the longer, the more important). ADPF also introduces a
novel diversity loss to guide the training of the AttentionNet and reduce the
overlap among patches so that the diverse and important patches are discovered.
Through extensive experiments, we show that our proposed framework outperforms
state-of-the-art methods on several age estimation benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Efficient Transformer and Image Pre-training for Low-level Vision. (arXiv:2112.10175v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10175">
<div class="article-summary-box-inner">
<span><p>Pre-training has marked numerous state of the arts in high-level computer
vision, but few attempts have ever been made to investigate how pre-training
acts in image processing systems. In this paper, we present an in-depth study
of image pre-training. To conduct this study on solid ground with practical
value in mind, we first propose a generic, cost-effective Transformer-based
framework for image processing. It yields highly competitive performance across
a range of low-level tasks, though under constrained parameters and
computational complexity. Then, based on this framework, we design a whole set
of principled evaluation tools to seriously and comprehensively diagnose image
pre-training in different tasks, and uncover its effects on internal network
representations. We find pre-training plays strikingly different roles in
low-level tasks. For example, pre-training introduces more local information to
higher layers in super-resolution (SR), yielding significant performance gains,
while pre-training hardly affects internal feature representations in
denoising, resulting in a little gain. Further, we explore different methods of
pre-training, revealing that multi-task pre-training is more effective and
data-efficient. All codes and models will be released at
https://github.com/fenglinglwb/EDT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Deep Learning Based Workflow for Detection of Lung Nodules With Chest Radiograph. (arXiv:2112.10184v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10184">
<div class="article-summary-box-inner">
<span><p>PURPOSE: This study aimed to develop a deep learning-based tool to detect and
localize lung nodules with chest radiographs(CXRs). We expected it to enhance
the efficiency of interpreting CXRs and reduce the possibilities of delayed
diagnosis of lung cancer.
</p>
<p>MATERIALS AND METHODS: We collected CXRs from NCKUH database and VBD, an
open-source medical image dataset, as our training and validation data. A
number of CXRs from the Ministry of Health and Welfare(MOHW) database served as
our test data. We built a segmentation model to identify lung areas from CXRs,
and sliced them into 16 patches. Physicians labeled the CXRs by clicking the
patches. These labeled patches were then used to train and fine-tune a deep
neural network(DNN) model, classifying the patches as positive or negative.
Finally, we test the DNN model with the lung patches of CXRs from MOHW.
</p>
<p>RESULTS: Our segmentation model identified the lung regions well from the
whole CXR. The Intersection over Union(IoU) between the ground truth and the
segmentation result was 0.9228. In addition, our DNN model achieved a
sensitivity of 0.81, specificity of 0.82, and AUROC of 0.869 in 98 of 125
cases. For the other 27 difficult cases, the sensitivity was 0.54, specificity
0.494, and AUROC 0.682. Overall, we obtained a sensitivity of 0.78, specificity
of 0.79, and AUROC 0.837.
</p>
<p>CONCLUSIONS: Our two-step workflow is comparable to state-of-the-art
algorithms in the sensitivity and specificity of localizing lung nodules from
CXRs. Notably, our workflow provides an efficient way for specialists to label
the data, which is valuable for relevant researches because of the relative
rarity of labeled medical image data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UnweaveNet: Unweaving Activity Stories. (arXiv:2112.10194v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10194">
<div class="article-summary-box-inner">
<span><p>Our lives can be seen as a complex weaving of activities; we switch from one
activity to another, to maximise our achievements or in reaction to demands
placed upon us. Observing a video of unscripted daily activities, we parse the
video into its constituent activity threads through a process we call
unweaving. To accomplish this, we introduce a video representation explicitly
capturing activity threads called a thread bank, along with a neural controller
capable of detecting goal changes and resuming of past activities, together
forming UnweaveNet. We train and evaluate UnweaveNet on sequences from the
unscripted egocentric dataset EPIC-KITCHENS. We propose and showcase the
efficacy of pretraining UnweaveNet in a self-supervised manner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Learning of Multi-category 3D Pose and Shape Estimation. (arXiv:2112.10196v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10196">
<div class="article-summary-box-inner">
<span><p>In this paper, we study the representation of the shape and pose of objects
using their keypoints. Therefore, we propose an end-to-end method that
simultaneously detects 2D keypoints from an image and lifts them to 3D. The
proposed method learns both 2D detection and 3D lifting only from 2D keypoints
annotations. In this regard, a novel method that explicitly disentangles the
pose and 3D shape by means of augmentation-based cyclic self-supervision is
proposed, for the first time. In addition of being end-to-end in image to 3D
learning, our method also handles objects from multiple categories using a
single neural network. We use a Transformer-based architecture to detect the
keypoints, as well as to summarize the visual context of the image. This visual
context information is then used while lifting the keypoints to 3D, so as to
allow the context-based reasoning for better performance. While lifting, our
method learns a small set of basis shapes and their sparse non-negative
coefficients to represent the 3D shape in canonical frame. Our method can
handle occlusions as well as wide variety of object classes. Our experiments on
three benchmarks demonstrate that our method performs better than the
state-of-the-art. Our source code will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HVTR: Hybrid Volumetric-Textural Rendering for Human Avatars. (arXiv:2112.10203v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10203">
<div class="article-summary-box-inner">
<span><p>We propose a novel neural rendering pipeline, Hybrid Volumetric-Textural
Rendering (HVTR), which synthesizes virtual human avatars from arbitrary poses
efficiently and at high quality. First, we learn to encode articulated human
motions on a dense UV manifold of the human body surface. To handle complicated
motions (e.g., self-occlusions), we then leverage the encoded information on
the UV manifold to construct a 3D volumetric representation based on a dynamic
pose-conditioned neural radiance field. While this allows us to represent 3D
geometry with changing topology, volumetric rendering is computationally heavy.
Hence we employ only a rough volumetric representation using a pose-conditioned
downsampled neural radiance field (PD-NeRF), which we can render efficiently at
low resolutions. In addition, we learn 2D textural features that are fused with
rendered volumetric features in image space. The key advantage of our approach
is that we can then convert the fused features into a high resolution,
high-quality avatar by a fast GAN-based textural renderer. We demonstrate that
hybrid rendering enables HVTR to handle complicated motions, render
high-quality avatars under user-controlled poses/shapes and even loose
clothing, and most importantly, be fast at inference time. Our experimental
results also demonstrate state-of-the-art quantitative results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GPU optimization of the 3D Scale-invariant Feature Transform Algorithm and a Novel BRIEF-inspired 3D Fast Descriptor. (arXiv:2112.10258v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10258">
<div class="article-summary-box-inner">
<span><p>This work details a highly efficient implementation of the 3D scale-invariant
feature transform (SIFT) algorithm, for the purpose of machine learning from
large sets of volumetric medical image data. The primary operations of the 3D
SIFT code are implemented on a graphics processing unit (GPU), including
convolution, sub-sampling, and 4D peak detection from scale-space pyramids. The
performance improvements are quantified in keypoint detection and
image-to-image matching experiments, using 3D MRI human brain volumes of
different people. Computationally efficient 3D keypoint descriptors are
proposed based on the Binary Robust Independent Elementary Feature (BRIEF)
code, including a novel descriptor we call Ranked Robust Independent Elementary
Features (RRIEF), and compared to the original 3D SIFT-Rank
method\citep{toews2013efficient}. The GPU implementation affords a speedup of
approximately 7X beyond an optimised CPU implementation, where computation time
is reduced from 1.4 seconds to 0.2 seconds for 3D volumes of size (145, 174,
145) voxels with approximately 3000 keypoints. Notable speedups include the
convolution operation (20X), 4D peak detection (3X), sub-sampling (3X), and
difference-of-Gaussian pyramid construction (2X). Efficient descriptors offer a
speedup of 2X and a memory savings of 6X compared to standard SIFT-Rank
descriptors, at a cost of reduced numbers of keypoint correspondences,
revealing a trade-off between computational efficiency and algorithmic
performance. The speedups gained by our implementation will allow for a more
efficient analysis on larger data sets. Our optimized GPU implementation of the
3D SIFT-Rank extractor is available at
https://github.com/CarluerJB/3D_SIFT_CUDA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wiener Guided DIP for Unsupervised Blind Image Deconvolution. (arXiv:2112.10271v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10271">
<div class="article-summary-box-inner">
<span><p>Blind deconvolution is an ill-posed problem arising in various fields ranging
from microscopy to astronomy. The ill-posed nature of the problem requires
adequate priors to arrive to a desirable solution. Recently, it has been shown
that deep learning architectures can serve as an image generation prior during
unsupervised blind deconvolution optimization, however often exhibiting a
performance fluctuation even on a single image. We propose to use
Wiener-deconvolution to guide the image generator during optimization by
providing it a sharpened version of the blurry image using an auxiliary kernel
estimate starting from a Gaussian. We observe that the high-frequency artifacts
of deconvolution are reproduced with a delay compared to low-frequency
features. In addition, the image generator reproduces low-frequency features of
the deconvolved image faster than that of a blurry image. We embed the
computational process in a constrained optimization framework and show that the
proposed method yields higher stability and performance across multiple
datasets. In addition, we provide the code.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parallel Multi-Scale Networks with Deep Supervision for Hand Keypoint Detection. (arXiv:2112.10275v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10275">
<div class="article-summary-box-inner">
<span><p>Keypoint detection plays an important role in a wide range of applications.
However, predicting keypoints of small objects such as human hands is a
challenging problem. Recent works fuse feature maps of deep Convolutional
Neural Networks (CNNs), either via multi-level feature integration or
multi-resolution aggregation. Despite achieving some success, the feature
fusion approaches increase the complexity and the opacity of CNNs. To address
this issue, we propose a novel CNN model named Multi-Scale Deep Supervision
Network (P-MSDSNet) that learns feature maps at different scales with deep
supervisions to produce attention maps for adaptive feature propagation from
layers to layers. P-MSDSNet has a multi-stage architecture which makes it
scalable while its deep supervision with spatial attention improves
transparency to the feature learning at each stage. We show that P-MSDSNet
outperforms the state-of-the-art approaches on benchmark datasets while
requiring fewer number of parameters. We also show the application of P-MSDSNet
to quantify finger tapping hand movements in a neuroscience study.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Driver Drowsiness Detection Using Ensemble Convolutional Neural Networks on YawDD. (arXiv:2112.10298v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10298">
<div class="article-summary-box-inner">
<span><p>Driver drowsiness detection using videos/images is one of the most essential
areas in today's time for driver safety. The development of deep learning
techniques, notably Convolutional Neural Networks (CNN), applied in computer
vision applications such as drowsiness detection, has shown promising results
due to the tremendous increase in technology in the recent few decades. Eyes
that are closed or blinking excessively, yawning, nodding, and occlusion are
all key aspects of drowsiness. In this work, we have applied four different
Convolutional Neural Network (CNN) techniques on the YawDD dataset to detect
and examine the extent of drowsiness depending on the yawning frequency with
specific pose and occlusion variation. Preliminary computational results show
that our proposed Ensemble Convolutional Neural Network (ECNN) outperformed the
traditional CNN-based approach by achieving an F1 score of 0.935, whereas the
other three CNN, such as CNN1, CNN2, and CNN3 approaches gained 0.92, 0.90, and
0.912 F1 scores, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Model-based gait recognition using graph network on very large population database. (arXiv:2112.10305v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10305">
<div class="article-summary-box-inner">
<span><p>At present, the existing gait recognition systems are focusing on developing
methods to extract robust gait feature from silhouette images and they indeed
achieved great success. However, gait can be sensitive to appearance features
such as clothing and carried items. Compared with appearance-based method,
model-based gait recognition is promising due to the robustness against these
variations. In recent years, with the development of human pose estimation, the
difficulty of model-based gait recognition methods has been mitigated. In this
paper, to resist the increase of subjects and views variation, local features
are built and a siamese network is proposed to maximize the distance of samples
from the same subject. We leverage recent advances in action recognition to
embed human pose sequence to a vector and introduce Spatial-Temporal Graph
Convolution Blocks (STGCB) which has been commonly used in action recognition
for gait recognition. Experiments on the very large population dataset named
OUMVLP-Pose and the popular dataset, CASIA-B, show that our method archives
some state-of-the-art (SOTA) performances in model-based gait recognition. The
code and models of our method are available at
https://github.com/timelessnaive/Gait-for-Large-Dataset after being accepted.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Skin lesion segmentation and classification using deep learning and handcrafted features. (arXiv:2112.10307v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10307">
<div class="article-summary-box-inner">
<span><p>Accurate diagnostics of a skin lesion is a critical task in classification
dermoscopic images. In this research, we form a new type of image features,
called hybrid features, which has stronger discrimination ability than single
method features. This study involves a new technique where we inject the
handcrafted features or feature transfer into the fully connected layer of
Convolutional Neural Network (CNN) model during the training process. Based on
our literature review until now, no study has examined or investigated the
impact on classification performance by injecting the handcrafted features into
the CNN model during the training process. In addition, we also investigated
the impact of segmentation mask and its effect on the overall classification
performance. Our model achieves an 92.3% balanced multiclass accuracy, which is
6.8% better than the typical single method classifier architecture for deep
learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Attention Network with Dense Field Estimation for Face Completion. (arXiv:2112.10310v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10310">
<div class="article-summary-box-inner">
<span><p>Most modern face completion approaches adopt an autoencoder or its variants
to restore missing regions in face images. Encoders are often utilized to learn
powerful representations that play an important role in meeting the challenges
of sophisticated learning tasks. Specifically, various kinds of masks are often
presented in face images in the wild, forming complex patterns, especially in
this hard period of COVID-19. It's difficult for encoders to capture such
powerful representations under this complex situation. To address this
challenge, we propose a self-supervised Siamese inference network to improve
the generalization and robustness of encoders. It can encode contextual
semantics from full-resolution images and obtain more discriminative
representations. To deal with geometric variations of face images, a dense
correspondence field is integrated into the network. We further propose a
multi-scale decoder with a novel dual attention fusion module (DAF), which can
combine the restored and known regions in an adaptive manner. This multi-scale
architecture is beneficial for the decoder to utilize discriminative
representations learned from encoders into images. Extensive experiments
clearly demonstrate that the proposed approach not only achieves more appealing
results compared with state-of-the-art methods but also improves the
performance of masked face recognition dramatically.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Product Re-identification System in Fully Automated Defect Detection. (arXiv:2112.10324v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10324">
<div class="article-summary-box-inner">
<span><p>In this work, we introduce a method and present an improved neural work to
perform product re-identification, which is an essential core function of a
fully automated product defect detection system. Our method is based on feature
distance. It is the combination of feature extraction neural networks, such as
VGG16, AlexNet, with an image search engine - Vearch. The dataset that we used
to develop product re-identification systems is a water-bottle dataset that
consists of 400 images of 18 types of water bottles. This is a small dataset,
which was the biggest challenge of our work. However, the combination of neural
networks with Vearch shows potential to tackle the product re-identification
problems. Especially, our new neural network - AlphaAlexNet that a neural
network was improved based on AlexNet could improve the production
identification accuracy by four percent. This indicates that an ideal
production identification accuracy could be achieved when efficient feature
extraction methods could be introduced and redesigned for image feature
extractions of nearly identical products. In order to solve the biggest
challenges caused by the small size of the dataset and the difficult nature of
identifying productions that have little differences from each other. In our
future work, we propose a new roadmap to tackle nearly-identical production
identifications: to introduce or develop new algorithms that need very few
images to train themselves.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incremental Cross-view Mutual Distillation for Self-supervised Medical CT Synthesis. (arXiv:2112.10325v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10325">
<div class="article-summary-box-inner">
<span><p>Due to the constraints of the imaging device and high cost in operation time,
computer tomography (CT) scans are usually acquired with low intra-slice
resolution. Improving the intra-slice resolution is beneficial to the disease
diagnosis for both human experts and computer-aided systems. To this end, this
paper builds a novel medical slice synthesis to increase the between-slice
resolution. Considering that the ground-truth intermediate medical slices are
always absent in clinical practice, we introduce the incremental cross-view
mutual distillation strategy to accomplish this task in the self-supervised
learning manner. Specifically, we model this problem from three different
views: slice-wise interpolation from axial view and pixel-wise interpolation
from coronal and sagittal views. Under this circumstance, the models learned
from different views can distill valuable knowledge to guide the learning
processes of each other. We can repeat this process to make the models
synthesize intermediate slice data with increasing inter-slice resolution. To
demonstrate the effectiveness of the proposed approach, we conduct
comprehensive experiments on a large-scale CT dataset. Quantitative and
qualitative comparison results show that our method outperforms
state-of-the-art algorithms by clear margins.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DMS-GCN: Dynamic Mutiscale Spatiotemporal Graph Convolutional Networks for Human Motion Prediction. (arXiv:2112.10365v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10365">
<div class="article-summary-box-inner">
<span><p>Human motion prediction is an important and challenging task in many computer
vision application domains. Recent work concentrates on utilizing the timing
processing ability of recurrent neural networks (RNNs) to achieve smooth and
reliable results in short-term prediction. However, as evidenced by previous
work, RNNs suffer from errors accumulation, leading to unreliable results. In
this paper, we propose a simple feed-forward deep neural network for motion
prediction, which takes into account temporal smoothness and spatial
dependencies between human body joints. We design a Multi-scale Spatio-temporal
graph convolutional networks (GCNs) to implicitly establish the Spatio-temporal
dependence in the process of human movement, where different scales fused
dynamically during training. The entire model is suitable for all actions and
follows a framework of encoder-decoder. The encoder consists of temporal GCNs
to capture motion features between frames and semi-autonomous learned spatial
GCNs to extract spatial structure among joint trajectories. The decoder uses
temporal convolution networks (TCNs) to maintain its extensive ability.
Extensive experiments show that our approach outperforms SOTA methods on the
datasets of Human3.6M and CMU Mocap while only requiring much lesser
parameters. Code will be available at https://github.com/yzg9353/DMSGCN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Co-supervision and Attention Fusion Strategy for Automatic COVID-19 Lung Infection Segmentation on CT Images. (arXiv:2112.10368v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10368">
<div class="article-summary-box-inner">
<span><p>Due to the irregular shapes,various sizes and indistinguishable boundaries
between the normal and infected tissues, it is still a challenging task to
accurately segment the infected lesions of COVID-19 on CT images. In this
paper, a novel segmentation scheme is proposed for the infections of COVID-19
by enhancing supervised information and fusing multi-scale feature maps of
different levels based on the encoder-decoder architecture. To this end, a deep
collaborative supervision (Co-supervision) scheme is proposed to guide the
network learning the features of edges and semantics. More specifically, an
Edge Supervised Module (ESM) is firstly designed to highlight low-level
boundary features by incorporating the edge supervised information into the
initial stage of down-sampling. Meanwhile, an Auxiliary Semantic Supervised
Module (ASSM) is proposed to strengthen high-level semantic information by
integrating mask supervised information into the later stage. Then an Attention
Fusion Module (AFM) is developed to fuse multiple scale feature maps of
different levels by using an attention mechanism to reduce the semantic gaps
between high-level and low-level feature maps. Finally, the effectiveness of
the proposed scheme is demonstrated on four various COVID-19 CT datasets. The
results show that the proposed three modules are all promising. Based on the
baseline (ResUnet), using ESM, ASSM, or AFM alone can respectively increase
Dice metric by 1.12\%, 1.95\%,1.63\% in our dataset, while the integration by
incorporating three models together can rise 3.97\%. Compared with the existing
approaches in various datasets, the proposed method can obtain better
segmentation performance in some main metrics, and can achieve the best
generalization and comprehensive performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Adversarially Learned Inference with Factorized Discriminators. (arXiv:2112.10384v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10384">
<div class="article-summary-box-inner">
<span><p>Learning from multimodal data is an important research topic in machine
learning, which has the potential to obtain better representations. In this
work, we propose a novel approach to generative modeling of multimodal data
based on generative adversarial networks. To learn a coherent multimodal
generative model, we show that it is necessary to align different encoder
distributions with the joint decoder distribution simultaneously. To this end,
we construct a specific form of the discriminator to enable our model to
utilize data efficiently, which can be trained constrastively. By taking
advantage of contrastive learning through factorizing the discriminator, we
train our model on unimodal data. We have conducted experiments on the
benchmark datasets, whose promising results show that our proposed approach
outperforms the-state-of-the-art methods on a variety of metrics. The source
code will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluation and Comparison of Deep Learning Methods for Pavement Crack Identification with Visual Images. (arXiv:2112.10390v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10390">
<div class="article-summary-box-inner">
<span><p>Compared with contact detection techniques, pavement crack identification
with visual images via deep learning algorithms has the advantages of not being
limited by the material of object to be detected, fast speed and low cost. The
fundamental frameworks and typical model architectures of transfer learning
(TL), encoder-decoder (ED), generative adversarial networks (GAN), and their
common modules were first reviewed, and then the evolution of convolutional
neural network (CNN) backbone models and GAN models were summarized. The crack
classification, segmentation performance, and effect were tested on the
SDNET2018 and CFD public data sets. In the aspect of patch sample
classification, the fine-tuned TL models can be equivalent to or even slightly
better than the ED models in accuracy, and the predicting time is faster; In
the aspect of accurate crack location, both ED and GAN algorithms can achieve
pixel-level segmentation and is expected to be detected in real time on low
computing power platform. Furthermore, a weakly supervised learning framework
of combined TL-SSGAN and its performance enhancement measures are proposed,
which can maintain comparable crack identification performance with that of the
supervised learning, while greatly reducing the number of labeled samples
required.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UFPMP-Det: Toward Accurate and Efficient Object Detection on Drone Imagery. (arXiv:2112.10415v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10415">
<div class="article-summary-box-inner">
<span><p>This paper proposes a novel approach to object detection on drone imagery,
namely Multi-Proxy Detection Network with Unified Foreground Packing
(UFPMP-Det). To deal with the numerous instances of very small scales,
different from the common solution that divides the high-resolution input image
into quite a number of chips with low foreground ratios to perform detection on
them each, the Unified Foreground Packing (UFP) module is designed, where the
sub-regions given by a coarse detector are initially merged through clustering
to suppress background and the resulting ones are subsequently packed into a
mosaic for a single inference, thus significantly reducing overall time cost.
Furthermore, to address the more serious confusion between inter-class
similarities and intra-class variations of instances, which deteriorates
detection performance but is rarely discussed, the Multi-Proxy Detection
Network (MP-Det) is presented to model object distributions in a fine-grained
manner by employing multiple proxy learning, and the proxies are enforced to be
diverse by minimizing a Bag-of-Instance-Words (BoIW) guided optimal transport
loss. By such means, UFPMP-Det largely promotes both the detection accuracy and
efficiency. Extensive experiments are carried out on the widely used VisDrone
and UAVDT datasets, and UFPMP-Det reports new state-of-the-art scores at a much
higher speed, highlighting its advantages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning with Label Noise for Image Retrieval by Selecting Interactions. (arXiv:2112.10453v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10453">
<div class="article-summary-box-inner">
<span><p>Learning with noisy labels is an active research area for image
classification. However, the effect of noisy labels on image retrieval has been
less studied. In this work, we propose a noise-resistant method for image
retrieval named Teacher-based Selection of Interactions, T-SINT, which
identifies noisy interactions, ie. elements in the distance matrix, and selects
correct positive and negative interactions to be considered in the retrieval
loss by using a teacher-based training setup which contributes to the
stability. As a result, it consistently outperforms state-of-the-art methods on
high noise rates across benchmark datasets with synthetic noise and more
realistic noise.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Animation with Keypoint Mask. (arXiv:2112.10457v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10457">
<div class="article-summary-box-inner">
<span><p>Motion transfer is the task of synthesizing future video frames of a single
source image according to the motion from a given driving video. This task is
challenging due to the complexity of motion representation and the unknown
relations between the driving video and the source image. Despite this
difficulty, this problem attracted great interests from researches at the
recent years, with gradual improvements. The problem can be thought as
decoupling of motion and appearance, which is often solved by extracting the
motion from keypoint movement. We chose to tackle the generic, unsupervised
setting, where we need to apply animation to any arbitrary object, without any
domain specific model for the structure of the input. In this work, we extract
the structure from a keypoint heatmap, without an explicit motion
representation. Then, the structures from the image and the video are extracted
to warp the image according to the video, by a deep generator.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reciprocal Normalization for Domain Adaptation. (arXiv:2112.10474v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10474">
<div class="article-summary-box-inner">
<span><p>Batch normalization (BN) is widely used in modern deep neural networks, which
has been shown to represent the domain-related knowledge, and thus is
ineffective for cross-domain tasks like unsupervised domain adaptation (UDA).
Existing BN variant methods aggregate source and target domain knowledge in the
same channel in normalization module. However, the misalignment between the
features of corresponding channels across domains often leads to a sub-optimal
transferability. In this paper, we exploit the cross-domain relation and
propose a novel normalization method, Reciprocal Normalization (RN).
Specifically, RN first presents a Reciprocal Compensation (RC) module to
acquire the compensatory for each channel in both domains based on the
cross-domain channel-wise correlation. Then RN develops a Reciprocal
Aggregation (RA) module to adaptively aggregate the feature with its
cross-domain compensatory components. As an alternative to BN, RN is more
suitable for UDA problems and can be easily integrated into popular domain
adaptation methods. Experiments show that the proposed RN outperforms existing
normalization counterparts by a large margin and helps state-of-the-art
adaptation approaches achieve better results. The source code is available on
https://github.com/Openning07/reciprocal-normalization-for-DA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">a novel attention-based network for fast salient object detection. (arXiv:2112.10481v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10481">
<div class="article-summary-box-inner">
<span><p>In the current salient object detection network, the most popular method is
using U-shape structure. However, the massive number of parameters leads to
more consumption of computing and storage resources which are not feasible to
deploy on the limited memory device. Some others shallow layer network will not
maintain the same accuracy compared with U-shape structure and the deep network
structure with more parameters will not converge to a global minimum loss with
great speed. To overcome all of these disadvantages, we proposed a new deep
convolution network architecture with three contributions: (1) using smaller
convolution neural networks (CNNs) to compress the model in our improved
salient object features compression and reinforcement extraction module
(ISFCREM) to reduce parameters of the model. (2) introducing channel attention
mechanism in ISFCREM to weigh different channels for improving the ability of
feature representation. (3) applying a new optimizer to accumulate the
long-term gradient information during training to adaptively tune the learning
rate. The results demonstrate that the proposed method can compress the model
to 1/3 of the original size nearly without losing the accuracy and converging
faster and more smoothly on six widely used datasets of salient object
detection compared with the others models. Our code is published in
https://gitee.com/binzhangbinzhangbin/code-a-novel-attention-based-network-for-fast-salient-object-detection.git
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ScanQA: 3D Question Answering for Spatial Scene Understanding. (arXiv:2112.10482v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10482">
<div class="article-summary-box-inner">
<span><p>We propose a new 3D spatial understanding task of 3D Question Answering
(3D-QA). In the 3D-QA task, models receive visual information from the entire
3D scene of the rich RGB-D indoor scan and answer the given textual questions
about the 3D scene. Unlike the 2D-question answering of VQA, the conventional
2D-QA models suffer from problems with spatial understanding of object
alignment and directions and fail the object localization from the textual
questions in 3D-QA. We propose a baseline model for 3D-QA, named ScanQA model,
where the model learns a fused descriptor from 3D object proposals and encoded
sentence embeddings. This learned descriptor correlates the language
expressions with the underlying geometric features of the 3D scan and
facilitates the regression of 3D bounding boxes to determine described objects
in textual questions. We collected human-edited question-answer pairs with
free-form answers that are grounded to 3D objects in each 3D scene. Our new
ScanQA dataset contains over 41K question-answer pairs from the 800 indoor
scenes drawn from the ScanNet dataset. To the best of our knowledge, ScanQA is
the first large-scale effort to perform object-grounded question-answering in
3D environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fusion and Orthogonal Projection for Improved Face-Voice Association. (arXiv:2112.10483v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10483">
<div class="article-summary-box-inner">
<span><p>We study the problem of learning association between face and voice, which is
gaining interest in the computer vision community lately. Prior works adopt
pairwise or triplet loss formulations to learn an embedding space amenable for
associated matching and verification tasks. Albeit showing some progress, such
loss formulations are, however, restrictive due to dependency on
distance-dependent margin parameter, poor run-time training complexity, and
reliance on carefully crafted negative mining procedures. In this work, we
hypothesize that enriched feature representation coupled with an effective yet
efficient supervision is necessary in realizing a discriminative joint
embedding space for improved face-voice association. To this end, we propose a
light-weight, plug-and-play mechanism that exploits the complementary cues in
both modalities to form enriched fused embeddings and clusters them based on
their identity labels via orthogonality constraints. We coin our proposed
mechanism as fusion and orthogonal projection (FOP) and instantiate in a
two-stream pipeline. The overall resulting framework is evaluated on a
large-scale VoxCeleb dataset with a multitude of tasks, including cross-modal
verification and matching. Results show that our method performs favourably
against the current state-of-the-art methods and our proposed supervision
formulation is more effective and efficient than the ones employed by the
contemporary methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scale-Net: Learning to Reduce Scale Differences for Large-Scale Invariant Image Matching. (arXiv:2112.10485v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10485">
<div class="article-summary-box-inner">
<span><p>Most image matching methods perform poorly when encountering large scale
changes in images. To solve this problem, firstly, we propose a
scale-difference-aware image matching method (SDAIM) that reduces image scale
differences before local feature extraction, via resizing both images of an
image pair according to an estimated scale ratio. Secondly, in order to
accurately estimate the scale ratio, we propose a
covisibility-attention-reinforced matching module (CVARM) and then design a
novel neural network, termed as Scale-Net, based on CVARM. The proposed CVARM
can lay more stress on covisible areas within the image pair and suppress the
distraction from those areas visible in only one image. Quantitative and
qualitative experiments confirm that the proposed Scale-Net has higher scale
ratio estimation accuracy and much better generalization ability compared with
all the existing scale ratio estimation methods. Further experiments on image
matching and relative pose estimation tasks demonstrate that our SDAIM and
Scale-Net are able to greatly boost the performance of representative local
features and state-of-the-art local feature matching methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Object Recognition as Classification of Visual Properties. (arXiv:2112.10531v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10531">
<div class="article-summary-box-inner">
<span><p>We base our work on the teleosemantic modelling of concepts as abilities
implementing the distinct functions of recognition and classification.
Accordingly, we model two types of concepts - substance concepts suited for
object recognition exploiting visual properties, and classification concepts
suited for classification of substance concepts exploiting linguistically
grounded properties. The goal in this paper is to demonstrate that object
recognition can be construed as classification of visual properties, as
distinct from work in mainstream computer vision. Towards that, we present an
object recognition process based on Ranganathan's four-phased faceted knowledge
organization process, grounded in the teleosemantic distinctions of substance
concept and classification concept. We also briefly introduce the ongoing
project MultiMedia UKC, whose aim is to build an object recognition resource
following our proposed process
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Implicit Neural Representation Learning for Hyperspectral Image Super-Resolution. (arXiv:2112.10541v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10541">
<div class="article-summary-box-inner">
<span><p>Hyperspectral image (HSI) super-resolution without additional auxiliary image
remains a constant challenge due to its high-dimensional spectral patterns,
where learning an effective spatial and spectral representation is a
fundamental issue. Recently, Implicit Neural Representations (INRs) are making
strides as a novel and effective representation, especially in the
reconstruction task. Therefore, in this work, we propose a novel HSI
reconstruction model based on INR which represents HSI by a continuous function
mapping a spatial coordinate to its corresponding spectral radiance values. In
particular, as a specific implementation of INR, the parameters of parametric
model are predicted by a hypernetwork that operates on feature extraction using
convolution network. It makes the continuous functions map the spatial
coordinates to pixel values in a content-aware manner. Moreover, periodic
spatial encoding are deeply integrated with the reconstruction procedure, which
makes our model capable of recovering more high frequency details. To verify
the efficacy of our model, we conduct experiments on three HSI datasets (CAVE,
NUS, and NTIRE2018). Experimental results show that the proposed model can
achieve competitive reconstruction performance in comparison with the
state-of-the-art methods. In addition, we provide an ablation study on the
effect of individual components of our model. We hope this paper could server
as a potent reference for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Hypergraph Convolutional Networks for Skeleton-Based Action Recognition. (arXiv:2112.10570v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10570">
<div class="article-summary-box-inner">
<span><p>Graph convolutional networks (GCNs) based methods have achieved advanced
performance on skeleton-based action recognition task. However, the skeleton
graph cannot fully represent the motion information contained in skeleton data.
In addition, the topology of the skeleton graph in the GCN-based methods is
manually set according to natural connections, and it is fixed for all samples,
which cannot well adapt to different situations. In this work, we propose a
novel dynamic hypergraph convolutional networks (DHGCN) for skeleton-based
action recognition. DHGCN uses hypergraph to represent the skeleton structure
to effectively exploit the motion information contained in human joints. Each
joint in the skeleton hypergraph is dynamically assigned the corresponding
weight according to its moving, and the hypergraph topology in our model can be
dynamically adjusted to different samples according to the relationship between
the joints. Experimental results demonstrate that the performance of our model
achieves competitive performance on three datasets: Kinetics-Skeleton 400, NTU
RGB+D 60, and NTU RGB+D 120.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">General Greedy De-bias Learning. (arXiv:2112.10572v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10572">
<div class="article-summary-box-inner">
<span><p>Neural networks often make predictions relying on the spurious correlations
from the datasets rather than the intrinsic properties of the task of interest,
facing sharp degradation on out-of-distribution (OOD) test data. Existing
de-bias learning frameworks try to capture specific dataset bias by bias
annotations, they fail to handle complicated OOD scenarios. Others implicitly
identify the dataset bias by the special design on the low capability biased
model or the loss, but they degrade when the training and testing data are from
the same distribution. In this paper, we propose a General Greedy De-bias
learning framework (GGD), which greedily trains the biased models and the base
model like gradient descent in functional space. It encourages the base model
to focus on examples that are hard to solve with biased models, thus remaining
robust against spurious correlations in the test stage. GGD largely improves
models' OOD generalization ability on various tasks, but sometimes
over-estimates the bias level and degrades on the in-distribution test. We
further re-analyze the ensemble process of GGD and introduce the Curriculum
Regularization into GGD inspired by curriculum learning, which achieves a good
trade-off between in-distribution and out-of-distribution performance.
Extensive experiments on image classification, adversarial question answering,
and visual question answering demonstrate the effectiveness of our method. GGD
can learn a more robust base model under the settings of both task-specific
biased models with prior knowledge and self-ensemble biased model without prior
knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image-free multi-character recognition. (arXiv:2112.10587v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10587">
<div class="article-summary-box-inner">
<span><p>The recently developed image-free sensing technique maintains the advantages
of both the light hardware and software, which has been applied in simple
target classification and motion tracking. In practical applications, however,
there usually exist multiple targets in the field of view, where existing
trials fail to produce multi-semantic information. In this letter, we report a
novel image-free sensing technique to tackle the multi-target recognition
challenge for the first time. Different from the convolutional layer stack of
image-free single-pixel networks, the reported CRNN network utilities the
bidirectional LSTM architecture to predict the distribution of multiple
characters simultaneously. The framework enables to capture the long-range
dependencies, providing a high recognition accuracy of multiple characters. We
demonstrated the technique's effectiveness in license plate detection, which
achieved 87.60% recognition accuracy at a 5% sampling rate with a higher than
100 FPS refresh rate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-Time Optical Flow for Vehicular Perception with Low- and High-Resolution Event Cameras. (arXiv:2112.10591v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10591">
<div class="article-summary-box-inner">
<span><p>Event cameras capture changes of illumination in the observed scene rather
than accumulating light to create images. Thus, they allow for applications
under high-speed motion and complex lighting conditions, where traditional
framebased sensors show their limits with blur and over- or underexposed
pixels. Thanks to these unique properties, they represent nowadays an highly
attractive sensor for ITS-related applications. Event-based optical flow (EBOF)
has been studied following the rise in popularity of these neuromorphic
cameras. The recent arrival of high-definition neuromorphic sensors, however,
challenges the existing approaches, because of the increased resolution of the
events pixel array and a much higher throughput. As an answer to these points,
we propose an optimized framework for computing optical flow in real-time with
both low- and high-resolution event cameras. We formulate a novel dense
representation for the sparse events flow, in the form of the "inverse
exponential distance surface". It serves as an interim frame, designed for the
use of proven, state-of-the-art frame-based optical flow computation methods.
We evaluate our approach on both low- and high-resolution driving sequences,
and show that it often achieves better results than the current state of the
art, while also reaching higher frame rates, 250Hz at 346 x 260 pixels and 77Hz
at 1280 x 720 pixels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeePaste -- Inpainting for Pasting. (arXiv:2112.10600v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10600">
<div class="article-summary-box-inner">
<span><p>One of the challenges of supervised learning training is the need to procure
an substantial amount of tagged data. A well-known method of solving this
problem is to use synthetic data in a copy-paste fashion, so that we cut
objects and paste them onto relevant backgrounds. Pasting the objects naively
results in artifacts that cause models to give poor results on real data. We
present a new method for cleanly pasting objects on different backgrounds so
that the dataset created gives competitive performance on real data. The main
emphasis is on the treatment of the border of the pasted object using
inpainting. We show state-of-the-art results both on instance detection and
foreground segmentation
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multi-user Oriented Live Free-viewpoint Video Streaming System Based On View Interpolation. (arXiv:2112.10603v1 [cs.MM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10603">
<div class="article-summary-box-inner">
<span><p>As an important application form of immersive multimedia services,
free-viewpoint video(FVV) enables users with great immersive experience by
strong interaction. However, the computational complexity of virtual view
synthesis algorithms poses a significant challenge to the real-time performance
of an FVV system. Furthermore, the individuality of user interaction makes it
difficult to serve multiple users simultaneously for a system with conventional
architecture. In this paper, we novelly introduce a CNN-based view
interpolation algorithm to synthesis dense virtual views in real time. Based on
this, we also build an end-to-end live free-viewpoint system with a multi-user
oriented streaming strategy. Our system can utilize a single edge server to
serve multiple users at the same time without having to bring a large view
synthesis load on the client side. We analysis the whole system and show that
our approaches give the user a pleasant immersive experience, in terms of both
visual quality and latency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-Time Rotation-Invariant Face Detection with Progressive Calibration Networks. (arXiv:1804.06039v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1804.06039">
<div class="article-summary-box-inner">
<span><p>Rotation-invariant face detection, i.e. detecting faces with arbitrary
rotation-in-plane (RIP) angles, is widely required in unconstrained
applications but still remains as a challenging task, due to the large
variations of face appearances. Most existing methods compromise with speed or
accuracy to handle the large RIP variations. To address this problem more
efficiently, we propose Progressive Calibration Networks (PCN) to perform
rotation-invariant face detection in a coarse-to-fine manner. PCN consists of
three stages, each of which not only distinguishes the faces from non-faces,
but also calibrates the RIP orientation of each face candidate to upright
progressively. By dividing the calibration process into several progressive
steps and only predicting coarse orientations in early stages, PCN can achieve
precise and fast calibration. By performing binary classification of face vs.
non-face with gradually decreasing RIP ranges, PCN can accurately detect faces
with full $360^{\circ}$ RIP angles. Such designs lead to a real-time
rotation-invariant face detector. The experiments on multi-oriented FDDB and a
challenging subset of WIDER FACE containing rotated faces in the wild show that
our PCN achieves quite promising performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MW-GAN: Multi-Warping GAN for Caricature Generation with Multi-Style Geometric Exaggeration. (arXiv:2001.01870v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2001.01870">
<div class="article-summary-box-inner">
<span><p>Given an input face photo, the goal of caricature generation is to produce
stylized, exaggerated caricatures that share the same identity as the photo. It
requires simultaneous style transfer and shape exaggeration with rich
diversity, and meanwhile preserving the identity of the input. To address this
challenging problem, we propose a novel framework called Multi-Warping GAN
(MW-GAN), including a style network and a geometric network that are designed
to conduct style transfer and geometric exaggeration respectively. We bridge
the gap between the style and landmarks of an image with corresponding latent
code spaces by a dual way design, so as to generate caricatures with arbitrary
styles and geometric exaggeration, which can be specified either through random
sampling of latent code or from a given caricature sample. Besides, we apply
identity preserving loss to both image space and landmark space, leading to a
great improvement in quality of generated caricatures. Experiments show that
caricatures generated by MW-GAN have better quality than existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Domain Adaptation with Prototype-Based Normalized Output Conditioner. (arXiv:2003.13274v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.13274">
<div class="article-summary-box-inner">
<span><p>In this work, we attempt to address unsupervised domain adaptation by
devising simple and compact conditional domain adversarial training methods. We
first revisit the simple concatenation conditioning strategy where features are
concatenated with output predictions as the input of the discriminator. We find
the concatenation strategy suffers from the weak conditioning strength. We
further demonstrate that enlarging the norm of concatenated predictions can
effectively energize the conditional domain alignment. Thus we improve
concatenation conditioning by normalizing the output predictions to have the
same norm of features, and term the derived method as Normalized OutpUt
coNditioner~(NOUN). However, conditioning on raw output predictions for domain
alignment, NOUN suffers from inaccurate predictions of the target domain. To
this end, we propose to condition the cross-domain feature alignment in the
prototype space rather than in the output space. Combining the novel
prototype-based conditioning with NOUN, we term the enhanced method as
PROtotype-based Normalized OutpUt coNditioner~(PRONOUN). Experiments on both
object recognition and semantic segmentation show that NOUN can effectively
align the multi-modal structures across domains and even outperform
state-of-the-art domain adversarial training methods. Together with
prototype-based conditioning, PRONOUN further improves the adaptation
performance over NOUN on multiple object recognition benchmarks for UDA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Slimming Neural Networks using Adaptive Connectivity Scores. (arXiv:2006.12463v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.12463">
<div class="article-summary-box-inner">
<span><p>In general, deep neural network (DNN) pruning methods fall into two
categories: 1) Weight-based deterministic constraints, and 2) Probabilistic
frameworks. While each approach has its merits and limitations there are a set
of common practical issues such as, trial-and-error to analyze sensitivity and
hyper-parameters to prune DNNs, which plague them both. In this work, we
propose a new single-shot, fully automated pruning algorithm called Slimming
Neural networks using Adaptive Connectivity Scores (SNACS). Our proposed
approach combines a probabilistic pruning framework with constraints on the
underlying weight matrices, via a novel connectivity measure, at multiple
levels to capitalize on the strengths of both approaches while solving their
deficiencies. In \alg{}, we propose a fast hash-based estimator of Adaptive
Conditional Mutual Information (ACMI), that uses a weight-based scaling
criterion, to evaluate the connectivity between filters and prune unimportant
ones. To automatically determine the limit up to which a layer can be pruned,
we propose a set of operating constraints that jointly define the upper pruning
percentage limits across all the layers in a deep network. Finally, we define a
novel sensitivity criterion for filters that measures the strength of their
contributions to the succeeding layer and highlights critical filters that need
to be completely protected from pruning. Through our experimental validation we
show that SNACS is faster by over 17x the nearest comparable method and is the
state of the art single-shot pruning method across three standard Dataset-DNN
pruning benchmarks: CIFAR10-VGG16, CIFAR10-ResNet56 and ILSVRC2012-ResNet50.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A deep primal-dual proximal network for image restoration. (arXiv:2007.00959v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.00959">
<div class="article-summary-box-inner">
<span><p>Image restoration remains a challenging task in image processing. Numerous
methods tackle this problem, often solved by minimizing a non-smooth penalized
co-log-likelihood function. Although the solution is easily interpretable with
theoretic guarantees, its estimation relies on an optimization process that can
take time. Considering the research effort in deep learning for image
classification and segmentation, this class of methods offers a serious
alternative to perform image restoration but stays challenging to solve inverse
problems. In this work, we design a deep network, named DeepPDNet, built from
primal-dual proximal iterations associated with the minimization of a standard
penalized likelihood with an analysis prior, allowing us to take advantage of
both worlds.
</p>
<p>We reformulate a specific instance of the Condat-Vu primal-dual hybrid
gradient (PDHG) algorithm as a deep network with fixed layers. The learned
parameters are both the PDHG algorithm step-sizes and the analysis linear
operator involved in the penalization (including the regularization parameter).
These parameters are allowed to vary from a layer to another one. Two different
learning strategies: "Full learning" and "Partial learning" are proposed, the
first one is the most efficient numerically while the second one relies on
standard constraints ensuring convergence in the standard PDHG iterations.
Moreover, global and local sparse analysis prior are studied to seek a better
feature representation. We apply the proposed methods to image restoration on
the MNIST and BSD68 datasets and to single image super-resolution on the BSD100
and SET14 datasets. Extensive results show that the proposed DeepPDNet
demonstrates excellent performance on the MNIST and the more complex BSD68,
BSD100, and SET14 datasets for image restoration and single image
super-resolution task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decoder-side Cross Resolution Synthesis for Video Compression Enhancement. (arXiv:2012.00650v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.00650">
<div class="article-summary-box-inner">
<span><p>This paper proposes a decoder-side Cross Resolution Synthesis (CRS) module to
pursue better compression efficiency beyond the latest Versatile Video Coding
(VVC), where we encode intra frames at original high resolution (HR), compress
inter frames at a lower resolution (LR), and then super-resolve decoded LR
inter frames with the help from preceding HR intra and neighboring LR inter
frames.
</p>
<p>For a LR inter frame, a motion alignment and aggregation network (MAN) is
devised to produce temporally aggregated motion representation to best
guarantee the temporal smoothness; Another texture compensation network (TCN)
is utilized to generate texture representation from decoded HR intra frame for
better augmenting spatial details; Finally, a similarity-driven fusion engine
synthesizes motion and texture representations to upscale LR inter frames for
the removal of compression and resolution re-sampling noises.
</p>
<p>We enhance the VVC using proposed CRS, showing averaged 8.76% and 11.93%
Bj{\o}ntegaard Delta Rate (BD-Rate) gains against the latest VVC anchor in
Random Access (RA) and Low-delay P (LDP) settings respectively. In addition,
experimental comparisons to the state-of-the-art super-resolution (SR) based
VVC enhancement methods, and ablation studies are conducted to further report
superior efficiency and generalization of the proposed algorithm. All materials
will be made to public at https://njuvision.github.io/CRS for reproducible
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accurate Object Association and Pose Updating for Semantic SLAM. (arXiv:2012.11368v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.11368">
<div class="article-summary-box-inner">
<span><p>Current pandemic has caused the medical system to operate under high load. To
relieve it, robots with high autonomy can be used to effectively execute
contactless operations in hospitals and reduce cross-infection between medical
staff and patients. Although semantic Simultaneous Localization and Mapping
(SLAM) technology can improve the autonomy of robots, semantic object
association is still a problem that is worthy of being studied. The key to
solving this problem is to correctly associate multiple object measurements of
one object landmark by using semantic information, and to refine the pose of
object landmark in real time. To this end, we propose a hierarchical object
association strategy and a pose-refinement approach. The former one consists of
two levels, i.e., a short-term object association and a global one. In the
first level, we employ the multiple-object-tracking for short-term object
association, through which the incorrect association among objects whose
locations are close and appearances are similar can be avoided. Moreover, the
short-term object association can provide more abundant object appearance and
more robust estimation of object pose for the global object association in the
second level. To refine the object pose in the map, we develop an approach to
choose the optimal object pose from all object measurements associated with an
object landmark. The proposed method is comprehensively evaluated on seven
simulated hospital sequences1, a real hospital environment and the KITTI
dataset. Experimental results show that our method has an obviously improvement
in terms of robustness and accuracy for the object association and the
trajectory estimation in the semantic SLAM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Shape Learning for Building Extraction in VHR Remote Sensing Images. (arXiv:2102.11262v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.11262">
<div class="article-summary-box-inner">
<span><p>Building extraction in VHR RSIs remains a challenging task due to occlusion
and boundary ambiguity problems. Although conventional convolutional neural
networks (CNNs) based methods are capable of exploiting local texture and
context information, they fail to capture the shape patterns of buildings,
which is a necessary constraint in the human recognition. To address this
issue, we propose an adversarial shape learning network (ASLNet) to model the
building shape patterns that improve the accuracy of building segmentation. In
the proposed ASLNet, we introduce the adversarial learning strategy to
explicitly model the shape constraints, as well as a CNN shape regularizer to
strengthen the embedding of shape features. To assess the geometric accuracy of
building segmentation results, we introduced several object-based quality
assessment metrics. Experiments on two open benchmark datasets show that the
proposed ASLNet improves both the pixel-based accuracy and the object-based
quality measurements by a large margin. The code is available at:
https://github.com/ggsDing/ASLNet
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Landmarks Augmentation with Manifold-Barycentric Oversampling. (arXiv:2104.00925v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.00925">
<div class="article-summary-box-inner">
<span><p>The training of Generative Adversarial Networks (GANs) requires a large
amount of data, stimulating the development of new augmentation methods to
alleviate the challenge. Oftentimes, these methods either fail to produce
enough new data or expand the dataset beyond the original manifold. In this
paper, we propose a new augmentation method that guarantees to keep the new
data within the original data manifold thanks to the optimal transport theory.
The proposed algorithm finds cliques in the nearest-neighbors graph and, at
each sampling iteration, randomly draws one clique to compute the Wasserstein
barycenter with random uniform weights. These barycenters then become the new
natural-looking elements that one could add to the dataset. We apply this
approach to the problem of landmarks detection and augment the available
annotation in both unpaired and in semi-supervised scenarios. Additionally, the
idea is validated on cardiac data for the task of medical segmentation. Our
approach reduces the overfitting and improves the quality metrics beyond the
original data outcome and beyond the result obtained with popular modern
augmentation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context-self contrastive pretraining for crop type semantic segmentation. (arXiv:2104.04310v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.04310">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a fully supervised pre-training scheme based on
contrastive learning particularly tailored to dense classification tasks. The
proposed Context-Self Contrastive Loss (CSCL) learns an embedding space that
makes semantic boundaries pop-up by use of a similarity metric between every
location in a training sample and its local context. For crop type semantic
segmentation from Satellite Image Time Series (SITS) we find performance at
parcel boundaries to be a critical bottleneck and explain how CSCL tackles the
underlying cause of that problem, improving the state-of-the-art performance in
this task. Additionally, using images from the Sentinel-2 (S2) satellite
missions we compile the largest, to our knowledge, SITS dataset densely
annotated by crop type and parcel identities, which we make publicly available
together with the data generation pipeline. Using that data we find CSCL, even
with minimal pre-training, to improve all respective baselines and present a
process for semantic segmentation at super-resolution for obtaining crop
classes at a more granular level. The code and instructions to download the
data can be found in https://github.com/michaeltrs/DeepSatModels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SoT: Delving Deeper into Classification Head for Transformer. (arXiv:2104.10935v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.10935">
<div class="article-summary-box-inner">
<span><p>Transformer models are not only successful in natural language processing
(NLP) but also demonstrate high potential in computer vision (CV). Despite
great advance, most of works only focus on improvement of architectures but pay
little attention to the classification head. For years transformer models base
exclusively on classification token to construct the final classifier, without
explicitly harnessing high-level word tokens. In this paper, we propose a novel
transformer model called second-order transformer (SoT), exploiting
simultaneously the classification token and word tokens for the classifier.
Specifically, we empirically disclose that high-level word tokens contain rich
information, which per se are very competent with the classifier and moreover,
are complementary to the classification token. To effectively harness such rich
information, we propose multi-headed global cross-covariance pooling with
singular value power normalization, which shares similar philosophy and thus is
compatible with the transformer block, better than commonly used pooling
methods. Then, we study comprehensively how to explicitly combine word tokens
with classification token for building the final classification head. For CV
tasks, our SoT significantly improves state-of-the-art vision transformers on
challenging benchmarks including ImageNet and ImageNet-A. For NLP tasks,
through fine-tuning based on pretrained language transformers including GPT and
BERT, our SoT greatly boosts the performance on widely used tasks such as CoLA
and RTE. Code will be available at https://peihuali.org/SoT
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pri3D: Can 3D Priors Help 2D Representation Learning?. (arXiv:2104.11225v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.11225">
<div class="article-summary-box-inner">
<span><p>Recent advances in 3D perception have shown impressive progress in
understanding geometric structures of 3Dshapes and even scenes. Inspired by
these advances in geometric understanding, we aim to imbue image-based
perception with representations learned under geometric constraints. We
introduce an approach to learn view-invariant,geometry-aware representations
for network pre-training, based on multi-view RGB-D data, that can then be
effectively transferred to downstream 2D tasks. We propose to employ
contrastive learning under both multi-view im-age constraints and
image-geometry constraints to encode3D priors into learned 2D representations.
This results not only in improvement over 2D-only representation learning on
the image-based tasks of semantic segmentation, instance segmentation, and
object detection on real-world in-door datasets, but moreover, provides
significant improvement in the low data regime. We show a significant
improvement of 6.0% on semantic segmentation on full data as well as 11.9% on
20% data against baselines on ScanNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visformer: The Vision-friendly Transformer. (arXiv:2104.12533v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12533">
<div class="article-summary-box-inner">
<span><p>The past year has witnessed the rapid development of applying the Transformer
module to vision problems. While some researchers have demonstrated that
Transformer-based models enjoy a favorable ability of fitting data, there are
still growing number of evidences showing that these models suffer over-fitting
especially when the training data is limited. This paper offers an empirical
study by performing step-by-step operations to gradually transit a
Transformer-based model to a convolution-based model. The results we obtain
during the transition process deliver useful messages for improving visual
recognition. Based on these observations, we propose a new architecture named
Visformer, which is abbreviated from the `Vision-friendly Transformer'. With
the same computational complexity, Visformer outperforms both the
Transformer-based and convolution-based models in terms of ImageNet
classification accuracy, and the advantage becomes more significant when the
model complexity is lower or the training set is smaller. The code is available
at https://github.com/danczs/Visformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Consistent Multiple Graph Embedding for Multi-View Clustering. (arXiv:2105.04880v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.04880">
<div class="article-summary-box-inner">
<span><p>Graph-based multi-view clustering aiming to obtain a partition of data across
multiple views, has received considerable attention in recent years. Although
great efforts have been made for graph-based multi-view clustering, it remains
a challenge to fuse characteristics from various views to learn a common
representation for clustering. In this paper, we propose a novel Consistent
Multiple Graph Embedding Clustering framework(CMGEC). Specifically, a multiple
graph auto-encoder(M-GAE) is designed to flexibly encode the complementary
information of multi-view data using a multi-graph attention fusion encoder. To
guide the learned common representation maintaining the similarity of the
neighboring characteristics in each view, a Multi-view Mutual Information
Maximization module(MMIM) is introduced. Furthermore, a graph fusion
network(GFN) is devised to explore the relationship among graphs from different
views and provide a common consensus graph needed in M-GAE. By jointly training
these models, the common latent representation can be obtained which encodes
more complementary information from multiple views and depicts data more
comprehensively. Experiments on three types of multi-view datasets demonstrate
CMGEC outperforms the state-of-the-art clustering methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Optical physics inspired CNN approach for intrinsic image decomposition. (arXiv:2105.10076v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.10076">
<div class="article-summary-box-inner">
<span><p>Intrinsic Image Decomposition is an open problem of generating the
constituents of an image. Generating reflectance and shading from a single
image is a challenging task specifically when there is no ground truth. There
is a lack of unsupervised learning approaches for decomposing an image into
reflectance and shading using a single image. We propose a neural network
architecture capable of this decomposition using physics-based parameters
derived from the image. Through experimental results, we show that (a) the
proposed methodology outperforms the existing deep learning-based IID
techniques and (b) the derived parameters improve the efficacy significantly.
We conclude with a closer analysis of the results (numerical and example
images) showing several avenues for improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Unfolding of Iteratively Reweighted ADMM for Wireless RF Sensing. (arXiv:2106.03686v3 [eess.SP] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03686">
<div class="article-summary-box-inner">
<span><p>We address the detection of material defects, which are inside a layered
material structure using compressive sensing based multiple-input and
multiple-output (MIMO) wireless radar. Here, the strong clutter due to the
reflection of the layered structure's surface often makes the detection of the
defects challenging. Thus, sophisticated signal separation methods are required
for improved defect detection. In many scenarios, the number of defects that we
are interested in is limited and the signaling response of the layered
structure can be modeled as a low-rank structure. Therefore, we propose joint
rank and sparsity minimization for defect detection. In particular, we propose
a non-convex approach based on the iteratively reweighted nuclear and
$\ell_1-$norm (a double-reweighted approach) to obtain a higher accuracy
compared to the conventional nuclear norm and $\ell_1-$norm minimization. To
this end, an iterative algorithm is designed to estimate the low-rank and
sparse contributions. Further, we propose deep learning to learn the parameters
of the algorithm (i.e., algorithm unfolding) to improve the accuracy and the
speed of convergence of the algorithm. Our numerical results show that the
proposed approach outperforms the conventional approaches in terms of mean
square errors of the recovered low-rank and sparse components and the speed of
convergence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pedestrian Attribute Recognition in Video Surveillance Scenarios Based on View-attribute Attention Localization. (arXiv:2106.06485v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06485">
<div class="article-summary-box-inner">
<span><p>Pedestrian attribute recognition in surveillance scenarios is still a
challenging task due to the inaccurate localization of specific attributes. In
this paper, we propose a novel view-attribute localization method based on
attention (VALA), which utilizes view information to guide the recognition
process to focus on specific attributes and attention mechanism to localize
specific attribute-corresponding areas. Concretely, view information is
leveraged by the view prediction branch to generate four view weights that
represent the confidences for attributes from different views. View weights are
then delivered back to compose specific view-attributes, which will participate
and supervise deep feature extraction. In order to explore the spatial location
of a view-attribute, regional attention is introduced to aggregate spatial
information and encode inter-channel dependencies of the view feature.
Subsequently, a fine attentive attribute-specific region is localized, and
regional weights for the view-attribute from different spatial locations are
gained by the regional attention. The final view-attribute recognition outcome
is obtained by combining the view weights with the regional weights.
Experiments on three wide datasets (RAP, RAPv2, and PA-100K) demonstrate the
effectiveness of our approach compared with state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">nuPlan: A closed-loop ML-based planning benchmark for autonomous vehicles. (arXiv:2106.11810v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.11810">
<div class="article-summary-box-inner">
<span><p>In this work, we propose the world's first closed-loop ML-based planning
benchmark for autonomous driving. While there is a growing body of ML-based
motion planners, the lack of established datasets and metrics has limited the
progress in this area. Existing benchmarks for autonomous vehicle motion
prediction have focused on short-term motion forecasting, rather than long-term
planning. This has led previous works to use open-loop evaluation with L2-based
metrics, which are not suitable for fairly evaluating long-term planning. Our
benchmark overcomes these limitations by introducing a large-scale driving
dataset, lightweight closed-loop simulator, and motion-planning-specific
metrics. We provide a high-quality dataset with 1500h of human driving data
from 4 cities across the US and Asia with widely varying traffic patterns
(Boston, Pittsburgh, Las Vegas and Singapore). We will provide a closed-loop
simulation framework with reactive agents and provide a large set of both
general and scenario-specific planning metrics. We plan to release the dataset
at NeurIPS 2021 and organize benchmark challenges starting in early 2022.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Kimera-Multi: Robust, Distributed, Dense Metric-Semantic SLAM for Multi-Robot Systems. (arXiv:2106.14386v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.14386">
<div class="article-summary-box-inner">
<span><p>This paper presents Kimera-Multi, the first multi-robot system that (i) is
robust and capable of identifying and rejecting incorrect inter and intra-robot
loop closures resulting from perceptual aliasing, (ii) is fully distributed and
only relies on local (peer-to-peer) communication to achieve distributed
localization and mapping, and (iii) builds a globally consistent
metric-semantic 3D mesh model of the environment in real-time, where faces of
the mesh are annotated with semantic labels. Kimera-Multi is implemented by a
team of robots equipped with visual-inertial sensors. Each robot builds a local
trajectory estimate and a local mesh using Kimera. When communication is
available, robots initiate a distributed place recognition and robust pose
graph optimization protocol based on a novel distributed graduated
non-convexity algorithm. The proposed protocol allows the robots to improve
their local trajectory estimates by leveraging inter-robot loop closures while
being robust to outliers. Finally, each robot uses its improved trajectory
estimate to correct the local mesh using mesh deformation techniques.
</p>
<p>We demonstrate Kimera-Multi in photo-realistic simulations, SLAM benchmarking
datasets, and challenging outdoor datasets collected using ground robots. Both
real and simulated experiments involve long trajectories (e.g., up to 800
meters per robot). The experiments show that Kimera-Multi (i) outperforms the
state of the art in terms of robustness and accuracy, (ii) achieves estimation
errors comparable to a centralized SLAM system while being fully distributed,
(iii) is parsimonious in terms of communication bandwidth, (iv) produces
accurate metric-semantic 3D meshes, and (v) is modular and can be also used for
standard 3D reconstruction (i.e., without semantic labels) or for trajectory
estimation (i.e., without reconstructing a 3D mesh).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deformation-Compensated Learning for Image Reconstruction without Ground Truth. (arXiv:2107.05533v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.05533">
<div class="article-summary-box-inner">
<span><p>Deep neural networks for medical image reconstruction are traditionally
trained using high-quality ground-truth images as training targets. Recent work
on Noise2Noise (N2N) has shown the potential of using multiple noisy
measurements of the same object as an alternative to having a ground-truth.
However, existing N2N-based methods are not suitable for learning from the
measurements of an object undergoing nonrigid deformation. This paper addresses
this issue by proposing the deformation-compensated learning (DeCoLearn) method
for training deep reconstruction networks by compensating for object
deformations. A key component of DeCoLearn is a deep registration module, which
is jointly trained with the deep reconstruction network without any
ground-truth supervision. We validate DeCoLearn on both simulated and
experimentally collected magnetic resonance imaging (MRI) data and show that it
significantly improves imaging quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parallel and High-Fidelity Text-to-Lip Generation. (arXiv:2107.06831v2 [cs.MM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.06831">
<div class="article-summary-box-inner">
<span><p>As a key component of talking face generation, lip movements generation
determines the naturalness and coherence of the generated talking face video.
Prior literature mainly focuses on speech-to-lip generation while there is a
paucity in text-to-lip (T2L) generation. T2L is a challenging task and existing
end-to-end works depend on the attention mechanism and autoregressive (AR)
decoding manner. However, the AR decoding manner generates current lip frame
conditioned on frames generated previously, which inherently hinders the
inference speed, and also has a detrimental effect on the quality of generated
lip frames due to error propagation. This encourages the research of parallel
T2L generation. In this work, we propose a parallel decoding model for fast and
high-fidelity text-to-lip generation (ParaLip). Specifically, we predict the
duration of the encoded linguistic features and model the target lip frames
conditioned on the encoded linguistic features with their duration in a
non-autoregressive manner. Furthermore, we incorporate the structural
similarity index loss and adversarial learning to improve perceptual quality of
generated lip frames and alleviate the blurry prediction problem. Extensive
experiments conducted on GRID and TCD-TIMIT datasets demonstrate the
superiority of proposed methods. Video samples are available via
\url{https://paralip.github.io/}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards robust vision by multi-task learning on monkey visual cortex. (arXiv:2107.14344v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.14344">
<div class="article-summary-box-inner">
<span><p>Deep neural networks set the state-of-the-art across many tasks in computer
vision, but their generalization ability to image distortions is surprisingly
fragile. In contrast, the mammalian visual system is robust to a wide range of
perturbations. Recent work suggests that this generalization ability can be
explained by useful inductive biases encoded in the representations of visual
stimuli throughout the visual cortex. Here, we successfully leveraged these
inductive biases with a multi-task learning approach: we jointly trained a deep
network to perform image classification and to predict neural activity in
macaque primary visual cortex (V1). We measured the out-of-distribution
generalization abilities of our network by testing its robustness to image
distortions. We found that co-training on monkey V1 data leads to increased
robustness despite the absence of those distortions during training.
Additionally, we showed that our network's robustness is very close to that of
an Oracle network where parts of the architecture are directly trained on noisy
images. Our results also demonstrated that the network's representations become
more brain-like as their robustness improves. Using a novel constrained
reconstruction analysis, we investigated what makes our brain-regularized
network more robust. We found that our co-trained network is more sensitive to
content than noise when compared to a Baseline network that we trained for
image classification alone. Using DeepGaze-predicted saliency maps for ImageNet
images, we found that our monkey co-trained network tends to be more sensitive
to salient regions in a scene, reminiscent of existing theories on the role of
V1 in the detection of object borders and bottom-up saliency. Overall, our work
expands the promising research avenue of transferring inductive biases from the
brain, and provides a novel analysis of the effects of our transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Polyp-PVT: Polyp Segmentation with Pyramid Vision Transformers. (arXiv:2108.06932v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06932">
<div class="article-summary-box-inner">
<span><p>Most polyp segmentation methods use CNNs as their backbone, leading to two
key issues when exchanging information between the encoder and decoder: 1)
taking into account the differences in contribution between different-level
features; and 2) designing an effective mechanism for fusing these features.
Different from existing CNN-based methods, we adopt a transformer encoder,
which learns more powerful and robust representations. In addition, considering
the image acquisition influence and elusive properties of polyps, we introduce
three novel modules, including a cascaded fusion module (CFM), a camouflage
identification module (CIM), a and similarity aggregation module (SAM). Among
these, the CFM is used to collect the semantic and location information of
polyps from high-level features, while the CIM is applied to capture polyp
information disguised in low-level features. With the help of the SAM, we
extend the pixel features of the polyp area with high-level semantic position
information to the entire polyp area, thereby effectively fusing cross-level
features. The proposed model, named Polyp-PVT, effectively suppresses noises in
the features and significantly improves their expressive capabilities.
Extensive experiments on five widely adopted datasets show that the proposed
model is more robust to various challenging situations (e.g., appearance
changes, small objects) than existing methods, and achieves the new
state-of-the-art performance. The proposed model is available at
https://github.com/DengPingFan/Polyp-PVT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MISSFormer: An Effective Medical Image Segmentation Transformer. (arXiv:2109.07162v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07162">
<div class="article-summary-box-inner">
<span><p>The CNN-based methods have achieved impressive results in medical image
segmentation, but they failed to capture the long-range dependencies due to the
inherent locality of the convolution operation. Transformer-based methods are
recently popular in vision tasks because of their capacity for long-range
dependencies and promising performance. However, it lacks in modeling local
context. In this paper, taking medical image segmentation as an example, we
present MISSFormer, an effective and powerful Medical Image Segmentation
tranSFormer. MISSFormer is a hierarchical encoder-decoder network with two
appealing designs: 1) A feed-forward network is redesigned with the proposed
Enhanced Transformer Block, which enhances the long-range dependencies and
supplements the local context, making the feature more discriminative. 2) We
proposed Enhanced Transformer Context Bridge, different from previous methods
of modeling only global information, the proposed context bridge with the
enhanced transformer block extracts the long-range dependencies and local
context of multi-scale features generated by our hierarchical transformer
encoder. Driven by these two designs, the MISSFormer shows a solid capacity to
capture more discriminative dependencies and context in medical image
segmentation. The experiments on multi-organ and cardiac segmentation tasks
demonstrate the superiority, effectiveness and robustness of our MISSFormer,
the experimental results of MISSFormer trained from scratch even outperform
state-of-the-art methods pre-trained on ImageNet. The core designs can be
generalized to other visual segmentation tasks. The code has been released on
Github: https://github.com/ZhifangDeng/MISSFormer
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting the Timing of Camera Movements From the Kinematics of Instruments in Robotic-Assisted Surgery Using Artificial Neural Networks. (arXiv:2109.11192v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11192">
<div class="article-summary-box-inner">
<span><p>Robotic-assisted surgeries benefit both surgeons and patients, however,
surgeons frequently need to adjust the endoscopic camera to achieve good
viewpoints. Simultaneously controlling the camera and the surgical instruments
is impossible, and consequentially, these camera adjustments repeatedly
interrupt the surgery. Autonomous camera control could help overcome this
challenge, but most existing systems are reactive, e.g., by having the camera
follow the surgical instruments. We propose a predictive approach for
anticipating when camera movements will occur using artificial neural networks.
We used the kinematic data of the surgical instruments, which were recorded
during robotic-assisted surgical training on porcine models. We split the data
into segments, and labeled each either as a segment that immediately precedes a
camera movement, or one that does not. Due to the large class imbalance, we
trained an ensemble of networks, each on a balanced sub-set of the training
data. We found that the instruments' kinematic data can be used to predict when
camera movements will occur, and evaluated the performance on different segment
durations and ensemble sizes. We also studied how much in advance an upcoming
camera movement can be predicted, and found that predicting a camera movement
0.25, 0.5, and 1 second before they occurred achieved 98%, 94%, and 84%
accuracy relative to the prediction of an imminent camera movement. This
indicates that camera movement events can be predicted early enough to leave
time for computing and executing an autonomous camera movement and suggests
that an autonomous camera controller for RAMIS may one day be feasible.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Pose Transfer with Correspondence Learning and Mesh Refinement. (arXiv:2109.15025v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15025">
<div class="article-summary-box-inner">
<span><p>3D pose transfer is one of the most challenging 3D generation tasks. It aims
to transfer the pose of a source mesh to a target mesh and keep the identity
(e.g., body shape) of the target mesh. Some previous works require key point
annotations to build reliable correspondence between the source and target
meshes, while other methods do not consider any shape correspondence between
sources and targets, which leads to limited generation quality. In this work,
we propose a correspondence-refinement network to achieve the 3D pose transfer
for both human and animal meshes. The correspondence between source and target
meshes is first established by solving an optimal transport problem. Then, we
warp the source mesh according to the dense correspondence and obtain a coarse
warped mesh. The warped mesh will be better refined with our proposed Elastic
Instance Normalization, which is a conditional normalization layer and can help
to generate high-quality meshes. Extensive experimental results show that the
proposed architecture can effectively transfer the poses from source to target
meshes and produce better results with satisfied visual performance than
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Kernel Representation for Image Reconstruction in PET. (arXiv:2110.01174v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01174">
<div class="article-summary-box-inner">
<span><p>Image reconstruction for positron emission tomography (PET) is challenging
because of the ill-conditioned tomographic problem and low counting statistics.
Kernel methods address this challenge by using kernel representation to
incorporate image prior information in the forward model of iterative PET image
reconstruction. Existing kernel methods construct the kernels commonly using an
empirical process, which may lead to suboptimal performance. In this paper, we
describe the equivalence between the kernel representation and a trainable
neural network model. A deep kernel method is proposed by exploiting deep
neural networks to enable an automated learning of an optimized kernel model.
The proposed method is directly applicable to single subjects. The training
process utilizes available image prior data to seek the best way to form a set
of robust kernels optimally rather than empirically. The results from computer
simulations and a real patient dataset demonstrate that the proposed deep
kernel method can outperform existing kernel method and neural network method
for dynamic PET image reconstruction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Attacks on Spiking Convolutional Networks for Event-based Vision. (arXiv:2110.02929v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02929">
<div class="article-summary-box-inner">
<span><p>Event-based sensing using dynamic vision sensors is gaining traction in
low-power vision applications. Spiking neural networks work well with the
sparse nature of event-based data and suit deployment on low-power neuromorphic
hardware. Being a nascent field, the sensitivity of spiking neural networks to
potentially malicious adversarial attacks has received very little attention so
far. In this work, we show how white-box adversarial attack algorithms can be
adapted to the discrete and sparse nature of event-based visual data, and to
the continuous-time setting of spiking neural networks. We test our methods on
the N-MNIST and IBM Gestures neuromorphic vision datasets and show adversarial
perturbations achieve a high success rate, by injecting a relatively small
number of appropriately placed events. We also verify, for the first time, the
effectiveness of these perturbations directly on neuromorphic hardware.
Finally, we discuss the properties of the resulting perturbations and possible
future directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Model Adaptation: Historical Contrastive Learning for Unsupervised Domain Adaptation without Source Data. (arXiv:2110.03374v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03374">
<div class="article-summary-box-inner">
<span><p>Unsupervised domain adaptation aims to align a labeled source domain and an
unlabeled target domain, but it requires to access the source data which often
raises concerns in data privacy, data portability and data transmission
efficiency. We study unsupervised model adaptation (UMA), or called
Unsupervised Domain Adaptation without Source Data, an alternative setting that
aims to adapt source-trained models towards target distributions without
accessing source data. To this end, we design an innovative historical
contrastive learning (HCL) technique that exploits historical source hypothesis
to make up for the absence of source data in UMA. HCL addresses the UMA
challenge from two perspectives. First, it introduces historical contrastive
instance discrimination (HCID) that learns from target samples by contrasting
their embeddings which are generated by the currently adapted model and the
historical models. With the historical models, HCID encourages UMA to learn
instance-discriminative target representations while preserving the source
hypothesis. Second, it introduces historical contrastive category
discrimination (HCCD) that pseudo-labels target samples to learn
category-discriminative target representations. Specifically, HCCD re-weights
pseudo labels according to their prediction consistency across the current and
historical models. Extensive experiments show that HCL outperforms and
state-of-the-art methods consistently across a variety of visual tasks and
setups.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi Proxy Anchor Loss and Effectiveness of Deep Metric Learning Performance Metrics. (arXiv:2110.03997v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03997">
<div class="article-summary-box-inner">
<span><p>Deep metric learning (DML) learns the mapping, which maps into embedding
space in which similar data is near and dissimilar data is far. However,
conventional proxy-based losses for DML have two problems: gradient problems
and applying the real-world dataset with multiple local centers. Besides, DML
performance metrics also have some issues have stability and flexibility. This
paper proposes multi-proxies anchor (MPA) loss and normalized discounted
cumulative gain (nDCG@k) metric. This study contributes three following: (1)
MPA loss is able to learn the real-world dataset with multi proxies. (2) MPA
loss improves the training capacity of a neural network, which solves the
gradient issues. (3) nDCG@k metric encourages full evaluation for various
datasets. Finally, we demonstrate MPA loss's effectiveness, and MPA loss
achieves the highest accuracy on two datasets for fine-grained images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep learning-based person re-identification methods: A survey and outlook of recent works. (arXiv:2110.04764v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04764">
<div class="article-summary-box-inner">
<span><p>In recent years, with the increasing demand for public safety and the rapid
development of intelligent surveillance networks, person re-identification
(Re-ID) has become one of the hot research topics in the computer vision field.
The main research goal of person Re-ID is to retrieve persons with the same
identity from different cameras. However, traditional person Re-ID methods
require manual marking of person targets, which consumes a lot of labor cost.
With the widespread application of deep neural networks, many deep
learning-based person Re-ID methods have emerged. Therefore, this paper is to
facilitate researchers to understand the latest research results and the future
trends in the field. Firstly, we summarize the studies of several recently
published person Re-ID surveys and complement the latest research methods to
systematically classify deep learning-based person Re-ID methods. Secondly, we
propose a multi-dimensional taxonomy that classifies current deep
learning-based person Re-ID methods into four categories according to metric
and representation learning, including methods for deep metric learning, local
feature learning, generative adversarial learning and sequence feature
learning. Furthermore, we subdivide the above four categories according to
their methodologies and motivations, discussing the advantages and limitations
of part subcategories. Finally, we discuss some challenges and possible
research directions for person Re-ID.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decomposing Convolutional Neural Networks into Reusable and Replaceable Modules. (arXiv:2110.07720v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07720">
<div class="article-summary-box-inner">
<span><p>Training from scratch is the most common way to build a Convolutional Neural
Network (CNN) based model. What if we can build new CNN models by reusing parts
from previously build CNN models? What if we can improve a CNN model by
replacing (possibly faulty) parts with other parts? In both cases, instead of
training, can we identify the part responsible for each output class (module)
in the model(s) and reuse or replace only the desired output classes to build a
model? Prior work has proposed decomposing dense-based networks into modules
(one for each output class) to enable reusability and replaceability in various
scenarios. However, this work is limited to the dense layers and based on the
one-to-one relationship between the nodes in consecutive layers. Due to the
shared architecture in the CNN model, prior work cannot be adapted directly. In
this paper, we propose to decompose a CNN model used for image classification
problems into modules for each output class. These modules can further be
reused or replaced to build a new model. We have evaluated our approach with
CIFAR-10, CIFAR-100, and ImageNet tiny datasets with three variations of ResNet
models and found that enabling decomposition comes with a small cost (1.77% and
0.85% for top-1 and top-5 accuracy, respectively). Also, building a model by
reusing or replacing modules can be done with a 2.3% and 0.5% average loss of
accuracy. Furthermore, reusing and replacing these modules reduces CO2e
emission by ~37 times compared to training the model from scratch.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Event Guided Depth Sensing. (arXiv:2110.10505v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10505">
<div class="article-summary-box-inner">
<span><p>Active depth sensors like structured light, lidar, and time-of-flight systems
sample the depth of the entire scene uniformly at a fixed scan rate. This leads
to limited spatio-temporal resolution where redundant static information is
over-sampled and precious motion information might be under-sampled. In this
paper, we present an efficient bio-inspired event-camera-driven depth
estimation algorithm. In our approach, we dynamically illuminate areas of
interest densely, depending on the scene activity detected by the event camera,
and sparsely illuminate areas in the field of view with no motion. The depth
estimation is achieved by an event-based structured light system consisting of
a laser point projector coupled with a second event-based sensor tuned to
detect the reflection of the laser from the scene. We show the feasibility of
our approach in a simulated autonomous driving scenario and real indoor
sequences using our prototype. We show that, in natural scenes like autonomous
driving and indoor environments, moving edges correspond to less than 10% of
the scene on average. Thus our setup requires the sensor to scan only 10% of
the scene, which could lead to almost 90% less power consumption by the
illumination source. While we present the evaluation and proof-of-concept for
an event-based structured-light system, the ideas presented here are applicable
for a wide range of depth-sensing modalities like LIDAR, time-of-flight, and
standard stereo. Video is available at \url{https://youtu.be/Rvv9IQLYjCQ}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Influential Prototypical Networks for Few Shot Learning: A Dermatological Case Study. (arXiv:2111.00698v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.00698">
<div class="article-summary-box-inner">
<span><p>Prototypical network (PN) is a simple yet effective few shot learning
strategy. It is a metric-based meta-learning technique where classification is
performed by computing Euclidean distances to prototypical representations of
each class. Conventional PN attributes equal importance to all samples and
generates prototypes by simply averaging the support sample embeddings
belonging to each class. In this work, we propose a novel version of PN that
attributes weights to support samples corresponding to their influence on the
support sample distribution. Influence weights of samples are calculated based
on maximum mean discrepancy (MMD) between the mean embeddings of sample
distributions including and excluding the sample. Comprehensive evaluation of
our proposed influential PN (IPNet) is performed by comparing its performance
with other baseline PNs on three different benchmark dermatological datasets.
IPNet outperforms all baseline models with compelling results across all three
datasets and various N-way, K-shot classification tasks. Findings from
cross-domain adaptation experiments further establish the robustness and
generalizability of IPNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Visual Quality of Image Synthesis by A Token-based Generator with Transformers. (arXiv:2111.03481v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.03481">
<div class="article-summary-box-inner">
<span><p>We present a new perspective of achieving image synthesis by viewing this
task as a visual token generation problem. Different from existing paradigms
that directly synthesize a full image from a single input (e.g., a latent
code), the new formulation enables a flexible local manipulation for different
image regions, which makes it possible to learn content-aware and fine-grained
style control for image synthesis. Specifically, it takes as input a sequence
of latent tokens to predict the visual tokens for synthesizing an image. Under
this perspective, we propose a token-based generator (i.e.,TokenGAN).
Particularly, the TokenGAN inputs two semantically different visual tokens,
i.e., the learned constant content tokens and the style tokens from the latent
space. Given a sequence of style tokens, the TokenGAN is able to control the
image synthesis by assigning the styles to the content tokens by attention
mechanism with a Transformer. We conduct extensive experiments and show that
the proposed TokenGAN has achieved state-of-the-art results on several
widely-used image synthesis benchmarks, including FFHQ and LSUN CHURCH with
different resolutions. In particular, the generator is able to synthesize
high-fidelity images with 1024x1024 size, dispensing with convolutions
entirely.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Masked Autoencoders Are Scalable Vision Learners. (arXiv:2111.06377v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.06377">
<div class="article-summary-box-inner">
<span><p>This paper shows that masked autoencoders (MAE) are scalable self-supervised
learners for computer vision. Our MAE approach is simple: we mask random
patches of the input image and reconstruct the missing pixels. It is based on
two core designs. First, we develop an asymmetric encoder-decoder architecture,
with an encoder that operates only on the visible subset of patches (without
mask tokens), along with a lightweight decoder that reconstructs the original
image from the latent representation and mask tokens. Second, we find that
masking a high proportion of the input image, e.g., 75%, yields a nontrivial
and meaningful self-supervisory task. Coupling these two designs enables us to
train large models efficiently and effectively: we accelerate training (by 3x
or more) and improve accuracy. Our scalable approach allows for learning
high-capacity models that generalize well: e.g., a vanilla ViT-Huge model
achieves the best accuracy (87.8%) among methods that use only ImageNet-1K
data. Transfer performance in downstream tasks outperforms supervised
pre-training and shows promising scaling behavior.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Action2video: Generating Videos of Human 3D Actions. (arXiv:2111.06925v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.06925">
<div class="article-summary-box-inner">
<span><p>We aim to tackle the interesting yet challenging problem of generating videos
of diverse and natural human motions from prescribed action categories. The key
issue lies in the ability to synthesize multiple distinct motion sequences that
are realistic in their visual appearances. It is achieved in this paper by a
two-step process that maintains internal 3D pose and shape representations,
action2motion and motion2video. Action2motion stochastically generates
plausible 3D pose sequences of a prescribed action category, which are
processed and rendered by motion2video to form 2D videos. Specifically, the Lie
algebraic theory is engaged in representing natural human motions following the
physical law of human kinematics; a temporal variational auto-encoder (VAE) is
developed that encourages diversity of output motions. Moreover, given an
additional input image of a clothed human character, an entire pipeline is
proposed to extract his/her 3D detailed shape, and to render in videos the
plausible motions from different views. This is realized by improving existing
methods to extract 3D human shapes and textures from single 2D images, rigging,
animating, and rendering to form 2D videos of human motions. It also
necessitates the curation and reannotation of 3D human motion datasets for
training purpose. Thorough empirical experiments including ablation study,
qualitative and quantitative evaluations manifest the applicability of our
approach, and demonstrate its competitiveness in addressing related tasks,
where components of our approach are compared favorably to the
state-of-the-arts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Model-Based Single Image Deep Dehazing. (arXiv:2111.10943v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10943">
<div class="article-summary-box-inner">
<span><p>Model-based single image dehazing algorithms restore images with sharp edges
and rich details at the expense of low PSNR values. Data-driven ones restore
images with high PSNR values but with low contrast, and even some remaining
haze. In this paper, a novel single image dehazing algorithm is introduced by
fusing model-based and data-driven approaches. Both transmission map and
atmospheric light are initialized by the model-based methods, and refined by
deep learning approaches which form a neural augmentation. Haze-free images are
restored by using the transmission map and atmospheric light. Experimental
results indicate that the proposed algorithm can remove haze well from
real-world and synthetic hazy images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain-Agnostic Clustering with Self-Distillation. (arXiv:2111.12170v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12170">
<div class="article-summary-box-inner">
<span><p>Recent advancements in self-supervised learning have reduced the gap between
supervised and unsupervised representation learning. However, most
self-supervised and deep clustering techniques rely heavily on data
augmentation, rendering them ineffective for many learning tasks where
insufficient domain knowledge exists for performing augmentation. We propose a
new self-distillation based algorithm for domain-agnostic clustering. Our
method builds upon the existing deep clustering frameworks and requires no
separate student model. The proposed method outperforms existing domain
agnostic (augmentation-free) algorithms on CIFAR-10. We empirically demonstrate
that knowledge distillation can improve unsupervised representation learning by
extracting richer `dark knowledge' from the model than using predicted labels
alone. Preliminary experiments also suggest that self-distillation improves the
convergence of DeepCluster-v2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ACPL: Anti-curriculum Pseudo-labelling for Semi-supervised Medical Image Classification. (arXiv:2111.12918v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12918">
<div class="article-summary-box-inner">
<span><p>Effective semi-supervised learning (SSL) in medical im-age analysis (MIA)
must address two challenges: 1) workeffectively on both multi-class (e.g.,
lesion classification)and multi-label (e.g., multiple-disease diagnosis)
problems,and 2) handle imbalanced learning (because of the highvariance in
disease prevalence). One strategy to explorein SSL MIA is based on the pseudo
labelling strategy, butit has a few shortcomings. Pseudo-labelling has in
generallower accuracy than consistency learning, it is not specifi-cally design
for both multi-class and multi-label problems,and it can be challenged by
imbalanced learning. In this paper, unlike traditional methods that select
confident pseudo label by threshold, we propose a new SSL algorithm, called
anti-curriculum pseudo-labelling (ACPL), which introduces novel techniques to
select informative unlabelled samples, improving training balance and allowing
the model to work for both multi-label and multi-class problems, and to
estimate pseudo labels by an accurate ensemble of classifiers(improving pseudo
label accuracy). We run extensive experiments to evaluate ACPL on two public
medical image classification benchmarks: Chest X-Ray14 for thorax disease
multi-label classification and ISIC2018 for skin lesion multi-class
classification. Our method outperforms previous SOTA SSL methods on both
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vector Quantized Diffusion Model for Text-to-Image Synthesis. (arXiv:2111.14822v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14822">
<div class="article-summary-box-inner">
<span><p>We present the vector quantized diffusion (VQ-Diffusion) model for
text-to-image generation. This method is based on a vector quantized
variational autoencoder (VQ-VAE) whose latent space is modeled by a conditional
variant of the recently developed Denoising Diffusion Probabilistic Model
(DDPM). We find that this latent-space method is well-suited for text-to-image
generation tasks because it not only eliminates the unidirectional bias with
existing methods but also allows us to incorporate a mask-and-replace diffusion
strategy to avoid the accumulation of errors, which is a serious problem with
existing methods. Our experiments show that the VQ-Diffusion produces
significantly better text-to-image generation results when compared with
conventional autoregressive (AR) models with similar numbers of parameters.
Compared with previous GAN-based text-to-image methods, our VQ-Diffusion can
handle more complex scenes and improve the synthesized image quality by a large
margin. Finally, we show that the image generation computation in our method
can be made highly efficient by reparameterization. With traditional AR
methods, the text-to-image generation time increases linearly with the output
image resolution and hence is quite time consuming even for normal size images.
The VQ-Diffusion allows us to achieve a better trade-off between quality and
speed. Our experiments indicate that the VQ-Diffusion model with the
reparameterization is fifteen times faster than traditional AR methods while
achieving a better image quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Semantic Segmentation via Spatial and Multi-Scale Aware Visual Class Embedding. (arXiv:2111.15181v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.15181">
<div class="article-summary-box-inner">
<span><p>Fully supervised semantic segmentation technologies bring a paradigm shift in
scene understanding. However, the burden of expensive labeling cost remains as
a challenge. To solve the cost problem, recent studies proposed language model
based zero-shot semantic segmentation (L-ZSSS) approaches. In this paper, we
address L-ZSSS has a limitation in generalization which is a virtue of
zero-shot learning. Tackling the limitation, we propose a language-model-free
zero-shot semantic segmentation framework, Spatial and Multi-scale aware Visual
Class Embedding Network (SM-VCENet). Furthermore, leveraging vision-oriented
class embedding SM-VCENet enriches visual information of the class embedding by
multi-scale attention and spatial attention. We also propose a novel benchmark
(PASCAL2COCO) for zero-shot semantic segmentation, which provides
generalization evaluation by domain adaptation and contains visually
challenging samples. In experiments, our SM-VCENet outperforms zero-shot
semantic segmentation state-of-the-art by a relative margin in PASCAL-5i
benchmark and shows generalization-robustness in PASCAL2COCO benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpretable Deep Learning-Based Forensic Iris Segmentation and Recognition. (arXiv:2112.00849v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00849">
<div class="article-summary-box-inner">
<span><p>Iris recognition of living individuals is a mature biometric modality that
has been adopted globally from governmental ID programs, border crossing, voter
registration and de-duplication, to unlocking mobile phones. On the other hand,
the possibility of recognizing deceased subjects with their iris patterns has
emerged recently. In this paper, we present an end-to-end deep learning-based
method for postmortem iris segmentation and recognition with a special
visualization technique intended to support forensic human examiners in their
efforts. The proposed postmortem iris segmentation approach outperforms the
state of the art and in addition to iris annulus, as in case of classical iris
segmentation methods - detects abnormal regions caused by eye decomposition
processes, such as furrows or irregular specular highlights present on the
drying and wrinkling cornea. The method was trained and validated with data
acquired from 171 cadavers, kept in mortuary conditions, and tested on
subject-disjoint data acquired from 259 deceased subjects. To our knowledge,
this is the largest corpus of data used in postmortem iris recognition research
to date. The source code of the proposed method are offered with the paper. The
test data will be available through the National Archive of Criminal Justice
Data (NACJD) archives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Global Statistics Aggregation for Improving Image Restoration. (arXiv:2112.04491v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04491">
<div class="article-summary-box-inner">
<span><p>Global spatial statistics, which are aggregated along entire spatial
dimensions, are widely used in top-performance image restorers. For example,
mean, variance in Instance Normalization (IN) which is adopted by HINet, and
global average pooling (i.e. mean) in Squeeze and Excitation (SE) which is
applied to MPRNet. This paper first shows that statistics aggregated on the
patches-based/entire-image-based feature in the training/testing phase
respectively may distribute very differently and lead to performance
degradation in image restorers. It has been widely overlooked by previous
works. To solve this issue, we propose a simple approach, Test-time Local
Statistics Converter (TLSC), that replaces the region of statistics aggregation
operation from global to local, only in the test time. Without retraining or
finetuning, our approach significantly improves the image restorer's
performance. In particular, by extending SE with TLSC to the state-of-the-art
models, MPRNet boost by 0.65 dB in PSNR on GoPro dataset, achieves 33.31 dB,
exceeds the previous best result 0.6 dB. In addition, we simply apply TLSC to
the high-level vision task, i.e. semantic segmentation, and achieves
competitive results. Extensive quantity and quality experiments are conducted
to demonstrate TLSC solves the issue with marginal costs while significant
gain. The code is available at https://github.com/megvii-research/tlsc.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual Cluster Contrastive learning for Person Re-Identification. (arXiv:2112.04662v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04662">
<div class="article-summary-box-inner">
<span><p>Recently, cluster contrastive learning has been proven effective for person
ReID by computing the contrastive loss between the individual feature and the
cluster memory. However, existing methods that use the individual feature to
momentum update the cluster memory are not robust to the noisy samples, such as
the samples with wrong annotated labels or the pseudo-labels. Unlike the
individual-based updating mechanism, the centroid-based updating mechanism that
applies the mean feature of each cluster to update the cluster memory is robust
against minority noisy samples. Therefore, we formulate the individual-based
updating and centroid-based updating mechanisms in a unified cluster
contrastive framework, named Dual Cluster Contrastive learning (DCC), which
maintains two types of memory banks: individual and centroid cluster memory
banks. Significantly, the individual cluster memory is momentum updated based
on the individual feature.The centroid cluster memory applies the mean feature
of each cluter to update the corresponding cluster memory. Besides the vallina
contrastive loss for each memory, a consistency constraint is applied to
guarantee the consistency of the output of two memories. Note that DCC can be
easily applied for unsupervised or supervised person ReID by using ground-truth
labels or pseudo-labels generated with clustering method, respectively.
Extensive experiments on two benchmarks under supervised person ReID and
unsupervised person ReID demonstrate the superior of the proposed DCC. Code is
available at: https://github.com/htyao89/Dual-Cluster-Contrastive/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IFR-Explore: Learning Inter-object Functional Relationships in 3D Indoor Scenes. (arXiv:2112.05298v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05298">
<div class="article-summary-box-inner">
<span><p>Building embodied intelligent agents that can interact with 3D indoor
environments has received increasing research attention in recent years. While
most works focus on single-object or agent-object visual functionality and
affordances, our work proposes to study a new kind of visual relationship that
is also important to perceive and model -- inter-object functional
relationships (e.g., a switch on the wall turns on or off the light, a remote
control operates the TV). Humans often spend little or no effort to infer these
relationships, even when entering a new room, by using our strong prior
knowledge (e.g., we know that buttons control electrical devices) or using only
a few exploratory interactions in cases of uncertainty (e.g., multiple switches
and lights in the same room). In this paper, we take the first step in building
AI system learning inter-object functional relationships in 3D indoor
environments with key technical contributions of modeling prior knowledge by
training over large-scale scenes and designing interactive policies for
effectively exploring the training scenes and quickly adapting to novel test
scenes. We create a new benchmark based on the AI2Thor and PartNet datasets and
perform extensive experiments that prove the effectiveness of our proposed
method. Results show that our model successfully learns priors and
fast-interactive-adaptation strategies for exploring inter-object functional
relationships in complex 3D scenes. Several ablation studies further validate
the usefulness of each proposed module.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attacking Point Cloud Segmentation with Color-only Perturbation. (arXiv:2112.05871v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05871">
<div class="article-summary-box-inner">
<span><p>Recent research efforts on 3D point-cloud semantic segmentation have achieved
outstanding performance by adopting deep CNN (convolutional neural networks)
and GCN (graph convolutional networks). However, the robustness of these
complex models has not been systematically analyzed. Given that semantic
segmentation has been applied in many safety-critical applications (e.g.,
autonomous driving, geological sensing), it is important to fill this knowledge
gap, in particular, how these models are affected under adversarial samples.
While adversarial attacks against point cloud have been studied, we found all
of them were targeting single-object recognition, and the perturbation is done
on the point coordinates. We argue that the coordinate-based perturbation is
unlikely to realize under the physical-world constraints. Hence, we propose a
new color-only perturbation method named COLPER, and tailor it to semantic
segmentation. By evaluating COLPER on an indoor dataset (S3DIS) and an outdoor
dataset (Semantic3D) against three point cloud segmentation models (PointNet++,
DeepGCNs, and RandLA-Net), we found color-only perturbation is sufficient to
significantly drop the segmentation accuracy and aIoU, under both targeted and
non-targeted attack settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Automatic Data Augmentation for 3D Point Cloud Classification. (arXiv:2112.06029v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06029">
<div class="article-summary-box-inner">
<span><p>Data augmentation is an important technique to reduce overfitting and improve
learning performance, but existing works on data augmentation for 3D point
cloud data are based on heuristics. In this work, we instead propose to
automatically learn a data augmentation strategy using bilevel optimization. An
augmentor is designed in a similar fashion to a conditional generator and is
optimized by minimizing a base model's loss on a validation set when the
augmented input is used for training the model. This formulation provides a
more principled way to learn data augmentation on 3D point clouds. We evaluate
our approach on standard point cloud classification tasks and a more
challenging setting with pose misalignment between training and validation/test
sets. The proposed strategy achieves competitive performance on both tasks and
we provide further insight into the augmentor's ability to learn the validation
set distribution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tracking and Long-Term Identification Using Non-Visual Markers. (arXiv:2112.06809v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06809">
<div class="article-summary-box-inner">
<span><p>Our objective is to track and identify mice in a cluttered home-cage
environment, as a precursor to automated behaviour recognition for biological
research. This is a very challenging problem due to (i) the lack of
distinguishing visual features for each mouse, and (ii) the close confines of
the scene with constant occlusion, making standard visual tracking approaches
unusable. However, a coarse estimate of each mouse's location is available from
a unique RFID implant, so there is the potential to optimally combine
information from (weak) tracking with coarse information on identity. To
achieve our objective, we make the following key contributions: (a) the
formulation of the identification problem as an assignment problem (solved
using Integer Linear Programming), and (b) a novel probabilistic model of the
affinity between tracklets and RFID data. The latter is a crucial part of the
model, as it provides a principled probabilistic treatment of object detections
given coarse localisation. Our approach achieves 77% accuracy on this
identification problem, and is able to reject spurious detections when the
animals are hidden.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HVH: Learning a Hybrid Neural Volumetric Representation for Dynamic Hair Performance Capture. (arXiv:2112.06904v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06904">
<div class="article-summary-box-inner">
<span><p>Capturing and rendering life-like hair is particularly challenging due to its
fine geometric structure, the complex physical interaction and its non-trivial
visual appearance.Yet, hair is a critical component for believable avatars. In
this paper, we address the aforementioned problems: 1) we use a novel,
volumetric hair representation that is com-posed of thousands of primitives.
Each primitive can be rendered efficiently, yet realistically, by building on
the latest advances in neural rendering. 2) To have a reliable control signal,
we present a novel way of tracking hair on the strand level. To keep the
computational effort manageable, we use guide hairs and classic techniques to
expand those into a dense hood of hair. 3) To better enforce temporal
consistency and generalization ability of our model, we further optimize the 3D
scene flow of our representation with multi-view optical flow, using volumetric
ray marching. Our method can not only create realistic renders of recorded
multi-view sequences, but also create renderings for new hair configurations by
providing new control signals. We compare our method with existing work on
viewpoint synthesis and drivable animation and achieve state-of-the-art
results. Please check out our project website at
https://ziyanw1.github.io/hvh/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Object Pursuit: Building a Space of Objects via Discriminative Weight Generation. (arXiv:2112.07954v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07954">
<div class="article-summary-box-inner">
<span><p>We propose a framework to continuously learn object-centric representations
for visual learning and understanding. Existing object-centric representations
either rely on supervisions that individualize objects in the scene, or perform
unsupervised disentanglement that can hardly deal with complex scenes in the
real world. To mitigate the annotation burden and relax the constraints on the
statistical complexity of the data, our method leverages interactions to
effectively sample diverse variations of an object and the corresponding
training signals while learning the object-centric representations. Throughout
learning, objects are streamed one by one in random order with unknown
identities, and are associated with latent codes that can synthesize
discriminative weights for each object through a convolutional hypernetwork.
Moreover, re-identification of learned objects and forgetting prevention are
employed to make the learning process efficient and robust. We perform an
extensive study of the key features of the proposed framework and analyze the
characteristics of the learned representations. Furthermore, we demonstrate the
capability of the proposed framework in learning representations that can
improve label efficiency in downstream tasks. Our code and trained models will
be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Nearest Neighbors for Visual Classification. (arXiv:2112.08459v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08459">
<div class="article-summary-box-inner">
<span><p>Neural network classifiers have become the de-facto choice for current
"pre-train then fine-tune" paradigms of visual classification. In this paper,
we investigate k-Nearest-Neighbor (k-NN) classifiers, a classical model-free
learning method from the pre-deep learning era, as an augmentation to modern
neural network based approaches. As a lazy learning method, k-NN simply
aggregates the distance between the test image and top-k neighbors in a
training set. We adopt k-NN with pre-trained visual representations produced by
either supervised or self-supervised methods in two steps: (1) Leverage k-NN
predicted probabilities as indications for easy vs. hard examples during
training. (2) Linearly interpolate the k-NN predicted distribution with that of
the augmented classifier. Via extensive experiments on a wide range of
classification tasks, our study reveals the generality and flexibility of k-NN
integration with additional insights: (1) k-NN achieves competitive results,
sometimes even outperforming a standard linear classifier. (2) Incorporating
k-NN is especially beneficial for tasks where parametric classifiers perform
poorly and / or in low-data regimes. We hope these discoveries will encourage
people to rethink the role of pre-deep learning, classical methods in computer
vision. Our code is available at: https://github.com/KMnP/nn-revisit.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Looking Outside the Box to Ground Language in 3D Scenes. (arXiv:2112.08879v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08879">
<div class="article-summary-box-inner">
<span><p>Existing language grounding models often use object proposal bottlenecks: a
pre-trained detector proposes objects in the scene and the model learns to
select the answer from these box proposals, without attending to the original
image or 3D point cloud. Object detectors are typically trained on a fixed
vocabulary of objects and attributes that is often too restrictive for
open-domain language grounding, where an utterance may refer to visual entities
at various levels of abstraction, such as a chair, the leg of a chair, or the
tip of the front leg of a chair. We propose a model for grounding language in
3D scenes that bypasses box proposal bottlenecks with three main innovations:
i) Iterative attention across the language stream, the point cloud feature
stream and 3D box proposals. ii) Transformer decoders with non-parametric
entity queries that decode 3D boxes for object and part referentials. iii)
Joint supervision from 3D object annotations and language grounding
annotations, by treating object detection as grounding of referential
utterances comprised of a list of candidate category labels. These innovations
result in significant quantitative gains (up to +9% absolute improvement on the
SR3D benchmark) over previous approaches on popular 3D language grounding
benchmarks. We ablate each of our innovations to show its contribution to the
performance of the model. When applied on language grounding on 2D images with
minor changes, it performs on par with the state-of-the-art while converges in
half of the GPU time. The code and checkpoints will be made available at
https://github.com/nickgkan/beauty_detr
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Spatio-Temporal Pretext Learning for Self-supervised Video Representation. (arXiv:2112.08913v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08913">
<div class="article-summary-box-inner">
<span><p>Spatio-temporal representation learning is critical for video self-supervised
representation. Recent approaches mainly use contrastive learning and pretext
tasks. However, these approaches learn representation by discriminating sampled
instances via feature similarity in the latent space while ignoring the
intermediate state of the learned representations, which limits the overall
performance. In this work, taking into account the degree of similarity of
sampled instances as the intermediate state, we propose a novel pretext task -
spatio-temporal overlap rate (STOR) prediction. It stems from the observation
that humans are capable of discriminating the overlap rates of videos in space
and time. This task encourages the model to discriminate the STOR of two
generated samples to learn the representations. Moreover, we employ a joint
optimization combining pretext tasks with contrastive learning to further
enhance the spatio-temporal representation learning. We also study the mutual
influence of each component in the proposed scheme. Extensive experiments
demonstrate that our proposed STOR task can favor both contrastive learning and
pretext tasks. The joint optimization scheme can significantly improve the
spatio-temporal representation in video understanding. The code is available at
https://github.com/Katou2/CSTP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Efficient Language-supervised Zero-shot Recognition with Optimal Transport Distillation. (arXiv:2112.09445v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09445">
<div class="article-summary-box-inner">
<span><p>Traditional computer vision models are trained to predict a fixed set of
predefined categories. Recently, natural language has been shown to be a
broader and richer source of supervision that provides finer descriptions to
visual concepts than supervised "gold" labels. Previous works, such as CLIP,
use InfoNCE loss to train a model to predict the pairing between images and
text captions. CLIP, however, is data hungry and requires more than 400M
image-text pairs for training. The inefficiency can be partially attributed to
the fact that the image-text pairs are noisy. To address this, we propose OTTER
(Optimal TransporT distillation for Efficient zero-shot Recognition), which
uses online entropic optimal transport to find a soft image-text match as
labels for contrastive learning. Based on pretrained image and text encoders,
models trained with OTTER achieve strong performance with only 3M image text
pairs. Compared with InfoNCE loss, label smoothing, and knowledge distillation,
OTTER consistently outperforms these baselines in zero shot evaluation on
Google Open Images (19,958 classes) and multi-labeled ImageNet 10K (10032
classes) from Tencent ML-Images. Over 42 evaluations on 7 different
dataset/architecture settings x 6 metrics, OTTER outperforms (32) or ties (2)
all baselines in 34 of them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deformable Registration of Brain MR Images via a Hybrid Loss. (arXiv:2110.15027v2 [eess.IV] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.15027">
<div class="article-summary-box-inner">
<span><p>Unsupervised learning strategy is widely adopted by the deformable
registration models due to the lack of ground truth of deformation fields.
These models typically depend on the intensity-based similarity loss to obtain
the learning convergence. Despite the success, such dependence is insufficient.
For the deformable registration of mono-modality image, well-aligned two images
not only have indistinguishable intensity differences, but also are close in
the statistical distribution and the boundary areas. Considering that
well-designed loss functions can facilitate a learning model into a desirable
convergence, we learn a deformable registration model for T1-weighted MR images
by integrating multiple image characteristics via a hybrid loss. Our method
registers the OASIS dataset with high accuracy while preserving deformation
smoothness.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-12-21 23:07:16.568398302 UTC">2021-12-21 23:07:16 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>