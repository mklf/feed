<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-04-11T01:30:00Z">04-11</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Read, Revise, Repeat: A System Demonstration for Human-in-the-loop Iterative Text Revision. (arXiv:2204.03685v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03685">
<div class="article-summary-box-inner">
<span><p>Revision is an essential part of the human writing process. It tends to be
strategic, adaptive, and, more importantly, iterative in nature. Despite the
success of large language models on text revision tasks, they are limited to
non-iterative, one-shot revisions. Examining and evaluating the capability of
large language models for making continuous revisions and collaborating with
human writers is a critical step towards building effective writing assistants.
In this work, we present a human-in-the-loop iterative text revision system,
Read, Revise, Repeat (R3), which aims at achieving high quality text revisions
with minimal human efforts by reading model-generated revisions and user
feedbacks, revising documents, and repeating human-machine interactions. In R3,
a text revision model provides text editing suggestions for human writers, who
can accept or reject the suggested edits. The accepted edits are then
incorporated into the model for the next iteration of document revision.
Writers can therefore revise documents iteratively by interacting with the
system and simply accepting/rejecting its suggested edits until the text
revision model stops making further revisions or reaches a predefined maximum
number of revisions. Empirical experiments show that R3 can generate revisions
with comparable acceptance rate to human writers at early revision depths, and
the human-machine interaction can get higher quality revisions with fewer
iterations and edits. The collected human-model interaction dataset and system
code are available at \url{https://github.com/vipulraheja/IteraTeR}. Our system
demonstration is available at \url{https://youtu.be/lK08tIpEoaE}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MHMS: Multimodal Hierarchical Multimedia Summarization. (arXiv:2204.03734v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03734">
<div class="article-summary-box-inner">
<span><p>Multimedia summarization with multimodal output can play an essential role in
real-world applications, i.e., automatically generating cover images and titles
for news articles or providing introductions to online videos. In this work, we
propose a multimodal hierarchical multimedia summarization (MHMS) framework by
interacting visual and language domains to generate both video and textual
summaries. Our MHMS method contains video and textual segmentation and
summarization module, respectively. It formulates a cross-domain alignment
objective with optimal transport distance which leverages cross-domain
interaction to generate the representative keyframe and textual summary. We
evaluated MHMS on three recent multimodal datasets and demonstrated the
effectiveness of our method in producing high-quality multimodal summaries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does Simultaneous Speech Translation need Simultaneous Models?. (arXiv:2204.03783v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03783">
<div class="article-summary-box-inner">
<span><p>In simultaneous speech translation (SimulST), finding the best trade-off
between high translation quality and low latency is a challenging task. To meet
the latency constraints posed by different application scenarios, multiple
dedicated SimulST models are usually trained and maintained, causing high
computational costs and increased environmental impact. In this paper, we show
that a single model trained offline can effectively serve not only offline but
also simultaneous tasks at different latency regimes, bypassing any
training/adaptation procedures. This single-model solution does not only
facilitate the adoption of well-established offline techniques and
architectures without affecting latency but also yields similar or even better
translation quality compared to the same model trained in the simultaneous
setting. Experiments on En$\rightarrow$\{De, Es\} indicate the effectiveness of
our approach, showing competitive results with the SimulST state of the art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PharmMT: A Neural Machine Translation Approach to Simplify Prescription Directions. (arXiv:2204.03830v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03830">
<div class="article-summary-box-inner">
<span><p>The language used by physicians and health professionals in prescription
directions includes medical jargon and implicit directives and causes much
confusion among patients. Human intervention to simplify the language at the
pharmacies may introduce additional errors that can lead to potentially severe
health outcomes. We propose a novel machine translation-based approach,
PharmMT, to automatically and reliably simplify prescription directions into
patient-friendly language, thereby significantly reducing pharmacist workload.
We evaluate the proposed approach over a dataset consisting of over 530K
prescriptions obtained from a large mail-order pharmacy. The end-to-end system
achieves a BLEU score of 60.27 against the reference directions generated by
pharmacists, a 39.6% relative improvement over the rule-based normalization.
Pharmacists judged 94.3% of the simplified directions as usable as-is or with
minimal changes. This work demonstrates the feasibility of a machine
translation-based tool for simplifying prescription directions in real-life.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Marvelous Agglutinative Language Effect on Cross Lingual Transfer Learning. (arXiv:2204.03831v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03831">
<div class="article-summary-box-inner">
<span><p>As for multilingual language models, it is important to select languages for
training because of the curse of multilinguality. (Conneau et al., 2020). It is
known that using languages with similar language structures is effective for
cross lingual transfer learning (Pires et al., 2019). However, we demonstrate
that using agglutinative languages such as Korean is more effective in cross
lingual transfer learning. This is a great discovery that will change the
training strategy of cross lingual transfer learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Infusing Knowledge from Wikipedia to Enhance Stance Detection. (arXiv:2204.03839v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03839">
<div class="article-summary-box-inner">
<span><p>Stance detection infers a text author's attitude towards a target. This is
challenging when the model lacks background knowledge about the target. Here,
we show how background knowledge from Wikipedia can help enhance the
performance on stance detection. We introduce Wikipedia Stance Detection BERT
(WS-BERT) that infuses the knowledge into stance encoding. Extensive results on
three benchmark datasets covering social media discussions and online debates
indicate that our model significantly outperforms the state-of-the-art methods
on target-specific stance detection, cross-target stance detection, and
zero/few-shot stance detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Softmax for End-to-End Low-resource Multilingual Speech Recognition. (arXiv:2204.03855v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03855">
<div class="article-summary-box-inner">
<span><p>Low resource speech recognition has been long-suffering from insufficient
training data. While neighbour languages are often used as assistant training
data, it would be difficult for the model to induct similar units (character,
subword, etc.) across the languages. In this paper, we assume similar units in
neighbour language share similar term frequency and form a Huffman tree to
perform multi-lingual hierarchical Softmax decoding. During decoding, the
hierarchical structure can benefit the training of low-resource languages.
Experimental results show the effectiveness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Pronunciation Assessment using Self-Supervised Speech Representation Learning. (arXiv:2204.03863v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03863">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning (SSL) approaches such as wav2vec 2.0 and HuBERT
models have shown promising results in various downstream tasks in the speech
community. In particular, speech representations learned by SSL models have
been shown to be effective for encoding various speech-related characteristics.
In this context, we propose a novel automatic pronunciation assessment method
based on SSL models. First, the proposed method fine-tunes the pre-trained SSL
models with connectionist temporal classification to adapt the English
pronunciation of English-as-a-second-language (ESL) learners in a data
environment. Then, the layer-wise contextual representations are extracted from
all across the transformer layers of the SSL models. Finally, the automatic
pronunciation score is estimated using bidirectional long short-term memory
with the layer-wise contextual representations and the corresponding text. We
show that the proposed SSL model-based methods outperform the baselines, in
terms of the Pearson correlation coefficient, on datasets of Korean ESL learner
children and Speechocean762. Furthermore, we analyze how different
representations of transformer layers in the SSL model affect the performance
of the pronunciation assessment task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CrudeOilNews: An Annotated Crude Oil News Corpus for Event Extraction. (arXiv:2204.03871v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03871">
<div class="article-summary-box-inner">
<span><p>In this paper, we present CrudeOilNews, a corpus of English Crude Oil news
for event extraction. It is the first of its kind for Commodity News and serve
to contribute towards resource building for economic and financial text mining.
This paper describes the data collection process, the annotation methodology
and the event typology used in producing the corpus. Firstly, a seed set of 175
news articles were manually annotated, of which a subset of 25 news were used
as the adjudicated reference test set for inter-annotator and system
evaluation. Agreement was generally substantial and annotator performance was
adequate, indicating that the annotation scheme produces consistent event
annotations of high quality. Subsequently the dataset is expanded through (1)
data augmentation and (2) Human-in-the-loop active learning. The resulting
corpus has 425 news articles with approximately 11k events annotated. As part
of active learning process, the corpus was used to train basic event extraction
models for machine labeling, the resulting models also serve as a validation or
as a pilot study demonstrating the use of the corpus in machine learning
purposes. The annotated corpus is made available for academic research purpose
at https://github.com/meisin/CrudeOilNews-Corpus.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Study of Different Ways to Use The Conformer Model For Spoken Language Understanding. (arXiv:2204.03879v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03879">
<div class="article-summary-box-inner">
<span><p>SLU combines ASR and NLU capabilities to accomplish speech-to-intent
understanding. In this paper, we compare different ways to combine ASR and NLU,
in particular using a single Conformer model with different ways to use its
components, to better understand the strengths and weaknesses of each approach.
We find that it is not necessarily a choice between two-stage decoding and
end-to-end systems which determines the best system for research or
application. System optimization still entails carefully improving the
performance of each component. It is difficult to prove that one direction is
conclusively better than the other. In this paper, we also propose a novel
connectionist temporal summarization (CTS) method to reduce the length of
acoustic encoding sequences while improving the accuracy and processing speed
of end-to-end models. This method achieves the same intent accuracy as the best
two-stage SLU recognition with complicated and time-consuming decoding but does
so at lower computational cost. This stacked end-to-end SLU system yields an
intent accuracy of 93.97% for the SmartLights far-field set, 95.18% for the
close-field set, and 99.71% for FluentSpeech.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transducer-based language embedding for spoken language identification. (arXiv:2204.03888v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03888">
<div class="article-summary-box-inner">
<span><p>The acoustic and linguistic features are important cues for the spoken
language identification (LID) task. Recent advanced LID systems mainly use
acoustic features that lack the usage of explicit linguistic feature encoding.
In this paper, we propose a novel transducer-based language embedding approach
for LID tasks by integrating an RNN transducer model into a language embedding
framework. Benefiting from the advantages of the RNN transducer's linguistic
representation capability, the proposed method can exploit both
phonetically-aware acoustic features and explicit linguistic features for LID
tasks. Experiments were carried out on the large-scale multilingual LibriSpeech
and VoxLingua107 datasets. Experimental results showed the proposed method
significantly improves the performance on LID tasks with 12% to 59% and 16% to
24% relative improvement on in-domain and cross-domain datasets, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adding Connectionist Temporal Summarization into Conformer to Improve Its Decoder Efficiency For Speech Recognition. (arXiv:2204.03889v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03889">
<div class="article-summary-box-inner">
<span><p>The Conformer model is an excellent architecture for speech recognition
modeling that effectively utilizes the hybrid losses of connectionist temporal
classification (CTC) and attention to train model parameters. To improve the
decoding efficiency of Conformer, we propose a novel connectionist temporal
summarization (CTS) method that reduces the number of frames required for the
attention decoder fed from the acoustic sequences generated by the encoder,
thus reducing operations. However, to achieve such decoding improvements, we
must fine-tune model parameters, as cross-attention observations are changed
and thus require corresponding refinements. Our final experiments show that,
with a beamwidth of 4, the LibriSpeech's decoding budget can be reduced by up
to 20% and for FluentSpeech data it can be reduced by 11%, without losing ASR
accuracy. An improvement in accuracy is even found for the LibriSpeech
"test-other" set. The word error rate (WER) is reduced by 6\% relative at the
beam width of 1 and by 3% relative at the beam width of 4.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Semi-Supervised Learning of Automatic Post-Editing: Data-Synthesis by Infilling Mask with Erroneous Tokens. (arXiv:2204.03896v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03896">
<div class="article-summary-box-inner">
<span><p>Semi-supervised learning that leverages synthetic training data has been
widely adopted in the field of Automatic post-editing (APE) to overcome the
lack of human-annotated training data. In that context, data-synthesis methods
to create high-quality synthetic data have also received much attention.
Considering that APE takes machine-translation outputs containing translation
errors as input, we propose a noising-based data-synthesis method that uses a
mask language model to create noisy texts through substituting masked tokens
with erroneous tokens, yet following the error-quantity statistics appearing in
genuine APE data. In addition, we propose corpus interleaving, which is to
combine two separate synthetic data by taking only advantageous samples, to
further enhance the quality of the synthetic data created with our noising
method. Experimental results reveal that using the synthetic data created with
our approach results in significant improvements in APE performance upon using
other synthetic data created with different existing data-synthesis methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BioBART: Pretraining and Evaluation of A Biomedical Generative Language Model. (arXiv:2204.03905v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03905">
<div class="article-summary-box-inner">
<span><p>Pretrained language models have served as important backbones for natural
language processing. Recently, in-domain pretraining has been shown to benefit
various domain-specific downstream tasks. In the biomedical domain, natural
language generation (NLG) tasks are of critical importance, while understudied.
Approaching natural language understanding (NLU) tasks as NLG achieves
satisfying performance in the general domain through constrained language
generation or language prompting. We emphasize the lack of in-domain generative
language models and the unsystematic generative downstream benchmarks in the
biomedical domain, hindering the development of the research community. In this
work, we introduce the generative language model BioBART that adapts BART to
the biomedical domain. We collate various biomedical language generation tasks
including dialogue, summarization, entity linking, and named entity
recognition. BioBART pretrained on PubMed abstracts has enhanced performance
compared to BART and set strong baselines on several tasks. Furthermore, we
conduct ablation studies on the pretraining tasks for BioBART and find that
sentence permutation has negative effects on downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Rewriting to Remembering: Common Ground for Conversational QA Models. (arXiv:2204.03930v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03930">
<div class="article-summary-box-inner">
<span><p>In conversational QA, models have to leverage information in previous turns
to answer upcoming questions. Current approaches, such as Question Rewriting,
struggle to extract relevant information as the conversation unwinds. We
introduce the Common Ground (CG), an approach to accumulate conversational
information as it emerges and select the relevant information at every turn. We
show that CG offers a more efficient and human-like way to exploit
conversational information compared to existing approaches, leading to
improvements on Open Domain Conversational QA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GigaST: A 10,000-hour Pseudo Speech Translation Corpus. (arXiv:2204.03939v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03939">
<div class="article-summary-box-inner">
<span><p>This paper introduces GigaST, a large-scale pseudo speech translation (ST)
corpus. We create the corpus by translating the text in GigaSpeech, an English
ASR corpus, into German and Chinese. The training set is translated by a strong
machine translation system and the test set is translated by human. ST models
trained with an addition of our corpus obtain new state-of-the-art results on
the MuST-C English-German benchmark test set. We provide a detailed description
of the translation process and verify its quality. We make the translated text
data public and hope to facilitate research in speech translation.
Additionally, we also release the training scripts on NeurST to make it easy to
replicate our systems. GigaST dataset is available at
https://st-benchmark.github.io/resources/GigaST.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RuBioRoBERTa: a pre-trained biomedical language model for Russian language biomedical text mining. (arXiv:2204.03951v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03951">
<div class="article-summary-box-inner">
<span><p>This paper presents several BERT-based models for Russian language biomedical
text mining (RuBioBERT, RuBioRoBERTa). The models are pre-trained on a corpus
of freely available texts in the Russian biomedical domain. With this
pre-training, our models demonstrate state-of-the-art results on RuMedBench -
Russian medical language understanding benchmark that covers a diverse set of
tasks, including text classification, question answering, natural language
inference, and named entity recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RubCSG at SemEval-2022 Task 5: Ensemble learning for identifying misogynous MEMEs. (arXiv:2204.03953v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03953">
<div class="article-summary-box-inner">
<span><p>This work presents an ensemble system based on various uni-modal and bi-modal
model architectures developed for the SemEval 2022 Task 5: MAMI-Multimedia
Automatic Misogyny Identification. The challenge organizers provide an English
meme dataset to develop and train systems for identifying and classifying
misogynous memes. More precisely, the competition is separated into two
sub-tasks: sub-task A asks for a binary decision as to whether a meme expresses
misogyny, while sub-task B is to classify misogynous memes into the potentially
overlapping sub-categories of stereotype, shaming, objectification, and
violence. For our submission, we implement a new model fusion network and
employ an ensemble learning approach for better performance. With this
structure, we achieve a 0.755 macroaverage F1-score (11th) in sub-task A and a
0.709 weighted-average F1-score (10th) in sub-task B.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bag-of-Words vs. Sequence vs. Graph vs. Hierarchy for Single- and Multi-Label Text Classification. (arXiv:2204.03954v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03954">
<div class="article-summary-box-inner">
<span><p>Graph neural networks have triggered a resurgence of graph-based text
classification methods, defining today's state of the art. We show that a
simple multi-layer perceptron (MLP) using a Bag of Words (BoW) outperforms the
recent graph-based models TextGCN and HeteGCN in an inductive text
classification setting and is comparable with HyperGAT in single-label
classification. We also run our own experiments on multi-label classification,
where the simple MLP outperforms the recent sequential-based gMLP and aMLP
models. Moreover, we fine-tune a sequence-based BERT and a lightweight
DistilBERT model, which both outperform all models on both single-label and
multi-label settings in most datasets. These results question the importance of
synthetic graphs used in modern text classifiers. In terms of parameters,
DistilBERT is still twice as large as our BoW-based wide MLP, while graph-based
models like TextGCN require setting up an $\mathcal{O}(N^2)$ graph, where $N$
is the vocabulary plus corpus size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhance Incomplete Utterance Restoration by Joint Learning Token Extraction and Text Generation. (arXiv:2204.03958v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03958">
<div class="article-summary-box-inner">
<span><p>This paper introduces a model for incomplete utterance restoration (IUR).
Different from prior studies that only work on extraction or abstraction
datasets, we design a simple but effective model, working for both scenarios of
IUR. Our design simulates the nature of IUR, where omitted tokens from the
context contribute to restoration. From this, we construct a Picker that
identifies the omitted tokens. To support the picker, we design two label
creation methods (soft and hard labels), which can work in cases of no
annotation of the omitted tokens. The restoration is done by using a Generator
with the help of the Picker on joint learning. Promising results on four
benchmark datasets in extraction and abstraction scenarios show that our model
is better than the pretrained T5 and non-generative language model methods in
both rich and limited training data settings. The code will be also available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FashionCLIP: Connecting Language and Images for Product Representations. (arXiv:2204.03972v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03972">
<div class="article-summary-box-inner">
<span><p>The steady rise of online shopping goes hand in hand with the development of
increasingly complex ML and NLP models. While most use cases are cast as
specialized supervised learning problems, we argue that practitioners would
greatly benefit from more transferable representations of products. In this
work, we build on recent developments in contrastive learning to train
FashionCLIP, a CLIP-like model for the fashion industry. We showcase its
capabilities for retrieval, classification and grounding, and release our model
and code to the community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KGI: An Integrated Framework for Knowledge Intensive Language Tasks. (arXiv:2204.03985v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03985">
<div class="article-summary-box-inner">
<span><p>In a recent work, we presented a novel state-of-the-art approach to zero-shot
slot filling that extends dense passage retrieval with hard negatives and
robust training procedures for retrieval augmented generation models. In this
paper, we propose a system based on an enhanced version of this approach where
we train task specific models for other knowledge intensive language tasks,
such as open domain question answering (QA), dialogue and fact checking. Our
system achieves results comparable to the best models in the KILT leaderboards.
Moreover, given a user query, we show how the output from these different
models can be combined to cross-examine each other. Particularly, we show how
accuracy in dialogue can be improved using the QA model. A short video
demonstrating the system is available here -
\url{https://ibm.box.com/v/kgi-interactive-demo} .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Disentangled Latent Speech Representation for Automatic Pathological Intelligibility Assessment. (arXiv:2204.04016v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04016">
<div class="article-summary-box-inner">
<span><p>Speech intelligibility assessment plays an important role in the therapy of
patients suffering from pathological speech disorders. Automatic and objective
measures are desirable to assist therapists in their traditionally subjective
and labor-intensive assessments. In this work, we investigate a novel approach
for obtaining such a measure using the divergence in disentangled latent speech
representations of a parallel utterance pair, obtained from a healthy reference
and a pathological speaker. Experiments on an English database of Cerebral
Palsy patients, using all available utterances per speaker, show high and
significant correlation values (R = -0.9) with subjective intelligibility
measures, while having only minimal deviation (+-0.01) across four different
reference speaker pairs. We also demonstrate the robustness of the proposed
method (R = -0.89 deviating +-0.02 over 1000 iterations) by considering a
significantly smaller amount of utterances per speaker. Our results are among
the first to show that disentangled speech representations can be used for
automatic pathological speech intelligibility assessment, resulting in a
reference speaker pair invariant method, applicable in scenarios with only few
utterances available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fair and Argumentative Language Modeling for Computational Argumentation. (arXiv:2204.04026v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04026">
<div class="article-summary-box-inner">
<span><p>Although much work in NLP has focused on measuring and mitigating
stereotypical bias in semantic spaces, research addressing bias in
computational argumentation is still in its infancy. In this paper, we address
this research gap and conduct a thorough investigation of bias in argumentative
language models. To this end, we introduce ABBA, a novel resource for bias
measurement specifically tailored to argumentation. We employ our resource to
assess the effect of argumentative fine-tuning and debiasing on the intrinsic
bias found in transformer-based language models using a lightweight
adapter-based approach that is more sustainable and parameter-efficient than
full fine-tuning. Finally, we analyze the potential impact of language model
debiasing on the performance in argument quality prediction, a downstream task
of computational argumentation. Our results show that we are able to
successfully and sustainably remove bias in general and argumentative language
models while preserving (and sometimes improving) model performance in
downstream tasks. We make all experimental code and data available at
https://github.com/umanlp/FairArgumentativeLM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Checking HateCheck: a cross-functional analysis of behaviour-aware learning for hate speech detection. (arXiv:2204.04042v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04042">
<div class="article-summary-box-inner">
<span><p>Behavioural testing -- verifying system capabilities by validating
human-designed input-output pairs -- is an alternative evaluation method of
natural language processing systems proposed to address the shortcomings of the
standard approach: computing metrics on held-out data. While behavioural tests
capture human prior knowledge and insights, there has been little exploration
on how to leverage them for model training and development. With this in mind,
we explore behaviour-aware learning by examining several fine-tuning schemes
using HateCheck, a suite of functional tests for hate speech detection systems.
To address potential pitfalls of training on data originally intended for
evaluation, we train and evaluate models on different configurations of
HateCheck by holding out categories of test cases, which enables us to estimate
performance on potentially overlooked system properties. The fine-tuning
procedure led to improvements in the classification accuracy of held-out
functionalities and identity groups, suggesting that models can potentially
generalise to overlooked functionalities. However, performance on held-out
functionality classes and i.i.d. hate speech detection data decreased, which
indicates that generalisation occurs mostly across functionalities from the
same class and that the procedure led to overfitting to the HateCheck data
distribution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">C-NMT: A Collaborative Inference Framework for Neural Machine Translation. (arXiv:2204.04043v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04043">
<div class="article-summary-box-inner">
<span><p>Collaborative Inference (CI) optimizes the latency and energy consumption of
deep learning inference through the inter-operation of edge and cloud devices.
Albeit beneficial for other tasks, CI has never been applied to the sequence-
to-sequence mapping problem at the heart of Neural Machine Translation (NMT).
In this work, we address the specific issues of collaborative NMT, such as
estimating the latency required to generate the (unknown) output sequence, and
show how existing CI methods can be adapted to these applications. Our
experiments show that CI can reduce the latency of NMT by up to 44% compared to
a non-collaborative approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Tokenisation by Alternative Treatment of Spaces. (arXiv:2204.04058v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04058">
<div class="article-summary-box-inner">
<span><p>Tokenisation is the first step in almost all NLP tasks, and state-of-the-art
transformer-based language models all use subword tokenisation algorithms to
process input text. Existing algorithms have problems, often producing
tokenisations of limited linguistic validity, and representing equivalent
strings differently depending on their position within a word. We hypothesise
that these problems hinder the ability of transformer-based models to handle
complex words, and suggest that these problems are a result of allowing tokens
to include spaces. We thus experiment with an alternative tokenisation approach
where spaces are always treated as individual tokens. Specifically, we apply
this modification to the BPE and Unigram algorithms. We find that our modified
algorithms lead to improved performance on downstream NLP tasks that involve
handling complex words, whilst having no detrimental effect on performance in
general natural language understanding tasks. Intrinsically, we find our
modified algorithms give more morphologically correct tokenisations, in
particular when handling prefixes. Given the results of our experiments, we
advocate for always treating spaces as individual tokens as an improved
tokenisation method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning the Ordering of Coordinate Compounds and Elaborate Expressions in Hmong, Lahu, and Chinese. (arXiv:2204.04080v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04080">
<div class="article-summary-box-inner">
<span><p>Coordinate compounds (CCs) and elaborate expressions (EEs) are coordinate
constructions common in languages of East and Southeast Asia. Mortensen (2006)
claims that (1) the linear ordering of EEs and CCs in Hmong, Lahu, and Chinese
can be predicted via phonological hierarchies and (2) these phonological
hierarchies lack a clear phonetic rationale. These claims are significant
because morphosyntax has often been seen as in a feed-forward relationship with
phonology, and phonological generalizations have often been assumed to be
phonetically "natural". We investigate whether the ordering of CCs and EEs can
be learned empirically and whether computational models (classifiers and
sequence labeling models) learn unnatural hierarchies similar to those posited
by Mortensen (2006). We find that decision trees and SVMs learn to predict the
order of CCs/EEs on the basis of phonology, with DTs learning hierarchies
strikingly similar to those proposed by Mortensen. However, we also find that a
neural sequence labeling model is able to learn the ordering of elaborate
expressions in Hmong very effectively without using any phonological
information. We argue that EE ordering can be learned through two independent
routes: phonology and lexical distribution, presenting a more nuanced picture
than previous work. [ISO 639-3:hmn, lhu, cmn]
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextual Representation Learning beyond Masked Language Modeling. (arXiv:2204.04163v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04163">
<div class="article-summary-box-inner">
<span><p>How do masked language models (MLMs) such as BERT learn contextual
representations? In this work, we analyze the learning dynamics of MLMs. We
find that MLMs adopt sampled embeddings as anchors to estimate and inject
contextual semantics to representations, which limits the efficiency and
effectiveness of MLMs. To address these issues, we propose TACO, a simple yet
effective representation learning approach to directly model global semantics.
TACO extracts and aligns contextual semantics hidden in contextualized
representations to encourage models to attend global semantics when generating
contextualized representations. Experiments on the GLUE benchmark show that
TACO achieves up to 5x speedup and up to 1.2 points average improvement over
existing MLMs. The code is available at https://github.com/FUZHIYI/TACO.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GRAM: Fast Fine-tuning of Pre-trained Language Models for Content-based Collaborative Filtering. (arXiv:2204.04179v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04179">
<div class="article-summary-box-inner">
<span><p>Content-based collaborative filtering (CCF) provides personalized item
recommendations based on both users' interaction history and items' content
information. Recently, pre-trained language models (PLM) have been used to
extract high-quality item encodings for CCF. However, it is resource-intensive
to finetune PLM in an end-to-end (E2E) manner in CCF due to its multi-modal
nature: optimization involves redundant content encoding for interactions from
users. For this, we propose GRAM (GRadient Accumulation for Multi-modality):
(1) Single-step GRAM which aggregates gradients for each item while maintaining
theoretical equivalence with E2E, and (2) Multi-step GRAM which further
accumulates gradients across multiple training steps, with less than 40\% GPU
memory footprint of E2E. We empirically confirm that GRAM achieves a remarkable
boost in training efficiency based on five datasets from two task domains of
Knowledge Tracing and News Recommendation, where single-step and multi-step
GRAM achieve 4x and 45x training speedup on average, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Survey on Publicly Available Sinhala Natural Language Processing Tools and Research. (arXiv:1906.02358v12 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1906.02358">
<div class="article-summary-box-inner">
<span><p>Sinhala is the native language of the Sinhalese people who make up the
largest ethnic group of Sri Lanka. The language belongs to the globe-spanning
language tree, Indo-European. However, due to poverty in both linguistic and
economic capital, Sinhala, in the perspective of Natural Language Processing
tools and research, remains a resource-poor language which has neither the
economic drive its cousin English has nor the sheer push of the law of numbers
a language such as Chinese has. A number of research groups from Sri Lanka have
noticed this dearth and the resultant dire need for proper tools and research
for Sinhala natural language processing. However, due to various reasons, these
attempts seem to lack coordination and awareness of each other. The objective
of this paper is to fill that gap of a comprehensive literature survey of the
publicly available Sinhala natural language tools and research so that the
researchers working in this field can better utilize contributions of their
peers. As such, we shall be uploading this paper to arXiv and perpetually
update it periodically to reflect the advances made in the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Few-Shot Semantic Parser for Wizard-of-Oz Dialogues with the Precise ThingTalk Representation. (arXiv:2009.07968v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.07968">
<div class="article-summary-box-inner">
<span><p>Previous attempts to build effective semantic parsers for Wizard-of-Oz (WOZ)
conversations suffer from the difficulty in acquiring a high-quality, manually
annotated training set. Approaches based only on dialogue synthesis are
insufficient, as dialogues generated from state-machine based models are poor
approximations of real-life conversations. Furthermore, previously proposed
dialogue state representations are ambiguous and lack the precision necessary
for building an effective agent. This paper proposes a new dialogue
representation and a sample-efficient methodology that can predict precise
dialogue states in WOZ conversations. We extended the ThingTalk representation
to capture all information an agent needs to respond properly. Our training
strategy is sample-efficient: we combine (1) fewshot data sparsely sampling the
full dialogue space and (2) synthesized data covering a subset space of
dialogues generated by a succinct state-based dialogue model. The completeness
of the extended ThingTalk language is demonstrated with a fully operational
agent, which is also used in training data synthesis. We demonstrate the
effectiveness of our methodology on MultiWOZ 3.0, a reannotation of the
MultiWOZ 2.1 dataset in ThingTalk. ThingTalk can represent 98% of the test
turns, while the simulator can emulate 85% of the validation set. We train a
contextual semantic parser using our strategy, and obtain 79% turn-by-turn
exact match accuracy on the reannotated test set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Math Word Problems using Pretrained Multilingual Language Models. (arXiv:2105.08928v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.08928">
<div class="article-summary-box-inner">
<span><p>In this paper, we revisit math word problems~(MWPs) from the cross-lingual
and multilingual perspective. We construct our MWP solvers over pretrained
multilingual language models using sequence-to-sequence model with copy
mechanism. We compare how the MWP solvers perform in cross-lingual and
multilingual scenarios. To facilitate the comparison of cross-lingual
performance, we first adapt the large-scale English dataset MathQA as a
counterpart of the Chinese dataset Math23K. Then we extend several English
datasets to bilingual datasets through machine translation plus human
annotation. Our experiments show that the MWP solvers may not be transferred to
a different language even if the target expressions have the same operator set
and constants. But for both cross-lingual and multilingual cases, it can be
better generalized if problem types exist on both source language and target
language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lightweight Cross-Lingual Sentence Representation Learning. (arXiv:2105.13856v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.13856">
<div class="article-summary-box-inner">
<span><p>Large-scale models for learning fixed-dimensional cross-lingual sentence
representations like LASER (Artetxe and Schwenk, 2019b) lead to significant
improvement in performance on downstream tasks. However, further increases and
modifications based on such large-scale models are usually impractical due to
memory limitations. In this work, we introduce a lightweight dual-transformer
architecture with just 2 layers for generating memory-efficient cross-lingual
sentence representations. We explore different training tasks and observe that
current cross-lingual training tasks leave a lot to be desired for this shallow
architecture. To ameliorate this, we propose a novel cross-lingual language
model, which combines the existing single-word masked language model with the
newly proposed cross-lingual token-level reconstruction task. We further
augment the training task by the introduction of two computationally-lite
sentence-level contrastive learning tasks to enhance the alignment of
cross-lingual sentence representation space, which compensates for the learning
bottleneck of the lightweight transformer for generative tasks. Our comparisons
with competing models on cross-lingual sentence retrieval and multilingual
document classification confirm the effectiveness of the newly proposed
training tasks for a shallow model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Don't Take It Literally: An Edit-Invariant Sequence Loss for Text Generation. (arXiv:2106.15078v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.15078">
<div class="article-summary-box-inner">
<span><p>Neural text generation models are typically trained by maximizing
log-likelihood with the sequence cross entropy (CE) loss, which encourages an
\emph{exact} token-by-token match between a target sequence with a generated
sequence. Such training objective is sub-optimal when the target sequence is
not perfect, e.g., when the target sequence is corrupted with noises, or when
only weak sequence supervision is available. To address the challenge, we
propose a novel Edit-Invariant Sequence Loss (EISL), which computes the
matching loss of a target n-gram with all n-grams in the generated sequence.
EISL is designed to be robust to various noises and edits in the target
sequences. Moreover, the EISL computation is essentially an approximate
convolution operation with target n-grams as kernels, which is easy to
implement and efficient to compute with existing libraries. To demonstrate the
effectiveness of EISL, we conduct experiments on a wide range of tasks,
including machine translation with noisy target sequences, unsupervised text
style transfer with only weak training signals, and non-autoregressive
generation with non-predefined generation order. Experimental results show our
method significantly outperforms the common CE loss and other strong baselines
on all the tasks. EISL has a simple API which can be used as a drop-in
replacement of the CE loss: https://anonymous.4open.science/r/EISLLoss.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Low-Resource Adaptation of Open-Domain Generative Chatbots. (arXiv:2108.06329v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06329">
<div class="article-summary-box-inner">
<span><p>Recent work building open-domain chatbots has demonstrated that increasing
model size improves performance. On the other hand, latency and connectivity
considerations dictate the move of digital assistants on the device. Giving a
digital assistant like Siri, Alexa, or Google Assistant the ability to discuss
just about anything leads to the need for reducing the chatbot model size such
that it fits on the user's device. We demonstrate that low parameter models can
simultaneously retain their general knowledge conversational abilities while
improving in a specific domain. Additionally, we propose a generic framework
that accounts for variety in question types, tracks reference throughout
multi-turn conversations, and removes inconsistent and potentially toxic
responses. Our framework seamlessly transitions between chatting and performing
transactional tasks, which will ultimately make interactions with digital
assistants more human-like. We evaluate our framework on 1 internal and 4
public benchmark datasets using both automatic (Perplexity) and human (SSA -
Sensibleness and Specificity Average) evaluation metrics and establish
comparable performance while reducing model parameters by 90%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Group-based Distinctive Image Captioning with Memory Attention. (arXiv:2108.09151v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09151">
<div class="article-summary-box-inner">
<span><p>Describing images using natural language is widely known as image captioning,
which has made consistent progress due to the development of computer vision
and natural language generation techniques. Though conventional captioning
models achieve high accuracy based on popular metrics, i.e., BLEU, CIDEr, and
SPICE, the ability of captions to distinguish the target image from other
similar images is under-explored. To generate distinctive captions, a few
pioneers employ contrastive learning or re-weighted the ground-truth captions,
which focuses on one single input image. However, the relationships between
objects in a similar image group (e.g., items or properties within the same
album or fine-grained events) are neglected. In this paper, we improve the
distinctiveness of image captions using a Group-based Distinctive Captioning
Model (GdisCap), which compares each image with other images in one similar
group and highlights the uniqueness of each image. In particular, we propose a
group-based memory attention (GMA) module, which stores object features that
are unique among the image group (i.e., with low similarity to objects in other
images). These unique object features are highlighted when generating captions,
resulting in more distinctive captions. Furthermore, the distinctive words in
the ground-truth captions are selected to supervise the language decoder and
GMA. Finally, we propose a new evaluation metric, distinctive word rate
(DisWordRate) to measure the distinctiveness of captions. Quantitative results
indicate that the proposed method significantly improves the distinctiveness of
several baseline models, and achieves the state-of-the-art performance on both
accuracy and distinctiveness. Results of a user study agree with the
quantitative evaluation and demonstrate the rationality of the new metric
DisWordRate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AxoNN: An asynchronous, message-driven parallel framework for extreme-scale deep learning. (arXiv:2110.13005v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.13005">
<div class="article-summary-box-inner">
<span><p>In the last few years, the memory requirements to train state-of-the-art
neural networks have far exceeded the DRAM capacities of modern hardware
accelerators. This has necessitated the development of efficient algorithms to
train these neural networks in parallel on large-scale GPU-based clusters.
Since computation is relatively inexpensive on modern GPUs, designing and
implementing extremely efficient communication in these parallel training
algorithms is critical for extracting the maximum performance. This paper
presents AxoNN, a parallel deep learning framework that exploits asynchrony and
message-driven execution to schedule neural network operations on each GPU,
thereby reducing GPU idle time and maximizing hardware efficiency. By using the
CPU memory as a scratch space for offloading data periodically during training,
AxoNN is able to reduce GPU memory consumption by four times. This allows us to
increase the number of parameters per GPU by four times, thus reducing the
amount of communication and increasing performance by over 13%. When tested
against large transformer models with 12-100 billion parameters on 48-384
NVIDIA Tesla V100 GPUs, AxoNN achieves a per-GPU throughput of 49.4-54.78% of
theoretical peak and reduces the training time by 22-37 days (15-25% speedup)
as compared to the state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spinning Language Models: Risks of Propaganda-As-A-Service and Countermeasures. (arXiv:2112.05224v2 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05224">
<div class="article-summary-box-inner">
<span><p>We investigate a new threat to neural sequence-to-sequence (seq2seq) models:
training-time attacks that cause models to "spin" their outputs so as to
support an adversary-chosen sentiment or point of view -- but only when the
input contains adversary-chosen trigger words. For example, a spinned
summarization model outputs positive summaries of any text that mentions the
name of some individual or organization.
</p>
<p>Model spinning introduces a "meta-backdoor" into a model. Whereas
conventional backdoors cause models to produce incorrect outputs on inputs with
the trigger, outputs of spinned models preserve context and maintain standard
accuracy metrics, yet also satisfy a meta-task chosen by the adversary.
</p>
<p>Model spinning enables propaganda-as-a-service, where propaganda is defined
as biased speech. An adversary can create customized language models that
produce desired spins for chosen triggers, then deploy these models to generate
disinformation (a platform attack), or else inject them into ML training
pipelines (a supply-chain attack), transferring malicious functionality to
downstream models trained by victims.
</p>
<p>To demonstrate the feasibility of model spinning, we develop a new
backdooring technique. It stacks an adversarial meta-task onto a seq2seq model,
backpropagates the desired meta-task output to points in the word-embedding
space we call "pseudo-words," and uses pseudo-words to shift the entire output
distribution of the seq2seq model. We evaluate this attack on language
generation, summarization, and translation models with different triggers and
meta-tasks such as sentiment, toxicity, and entailment. Spinned models largely
maintain their accuracy metrics (ROUGE and BLEU) while shifting their outputs
to satisfy the adversary's meta-task. We also show that, in the case of a
supply-chain attack, the spin functionality transfers to downstream models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Homepage2Vec: Language-Agnostic Website Embedding and Classification. (arXiv:2201.03677v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.03677">
<div class="article-summary-box-inner">
<span><p>Currently, publicly available models for website classification do not offer
an embedding method and have limited support for languages beyond English. We
release a dataset of more than two million category-labeled websites in 92
languages collected from Curlie, the largest multilingual human-edited Web
directory. The dataset contains 14 website categories aligned across languages.
Alongside it, we introduce Homepage2Vec, a machine-learned pre-trained model
for classifying and embedding websites based on their homepage in a
language-agnostic way. Homepage2Vec, thanks to its feature set (textual
content, metadata tags, and visual attributes) and recent progress in natural
language representation, is language-independent by design and generates
embedding-based representations. We show that Homepage2Vec correctly classifies
websites with a macro-averaged F1-score of 0.90, with stable performance across
low- as well as high-resource languages. Feature analysis shows that a small
subset of efficiently computable features suffices to achieve high performance
even with limited computational resources. We make publicly available the
curated Curlie dataset aligned across languages, the pre-trained Homepage2Vec
model, and libraries
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Many Ways to be Lonely: Fine-grained Characterization of Loneliness and its Potential Changes in COVID-19. (arXiv:2201.07423v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07423">
<div class="article-summary-box-inner">
<span><p>Loneliness has been associated with negative outcomes for physical and mental
health. Understanding how people express and cope with various forms of
loneliness is critical for early screening and targeted interventions to reduce
loneliness, particularly among vulnerable groups such as young adults. To
examine how different forms of loneliness and coping strategies manifest in
loneliness self-disclosure, we built a dataset, FIG-Loneliness (FIne-Grained
Loneliness) by using Reddit posts in two young adult-focused forums and two
loneliness related forums consisting of a diverse age group. We provided
annotations by trained human annotators for binary and fine-grained loneliness
classifications of the posts. Trained on FIG-Loneliness, two BERT-based models
were used to understand loneliness forms and authors' coping strategies in
these forums. Our binary loneliness classification achieved an accuracy above
97%, and fine-grained loneliness category classification reached an average
accuracy of 77% across all labeled categories. With FIG-Loneliness and model
predictions, we found that loneliness expressions in the young adults related
forums were distinct from other forums. Those in young adult-focused forums
were more likely to express concerns pertaining to peer relationship, and were
potentially more sensitive to geographical isolation impacted by the COVID-19
pandemic lockdown. Also, we showed that different forms of loneliness have
differential use in coping strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Embarrassingly Simple Performance Prediction for Abductive Natural Language Inference. (arXiv:2202.10408v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.10408">
<div class="article-summary-box-inner">
<span><p>The task of abductive natural language inference (\alpha{}nli), to decide
which hypothesis is the more likely explanation for a set of observations, is a
particularly difficult type of NLI. Instead of just determining a causal
relationship, it requires common sense to also evaluate how reasonable an
explanation is. All recent competitive systems build on top of contextualized
representations and make use of transformer architectures for learning an NLI
model. When somebody is faced with a particular NLI task, they need to select
the best model that is available. This is a time-consuming and resource-intense
endeavour. To solve this practical problem, we propose a simple method for
predicting the performance without actually fine-tuning the model. We do this
by testing how well the pre-trained models perform on the \alpha{}nli task when
just comparing sentence embeddings with cosine similarity to what the
performance that is achieved when training a classifier on top of these
embeddings. We show that the accuracy of the cosine similarity approach
correlates strongly with the accuracy of the classification approach with a
Pearson correlation coefficient of 0.65. Since the similarity computation is
orders of magnitude faster to compute on a given dataset (less than a minute
vs. hours), our method can lead to significant time savings in the process of
model selection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Items from Psychometric Tests as Training Data for Personality Profiling Models of Twitter Users. (arXiv:2202.10415v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.10415">
<div class="article-summary-box-inner">
<span><p>Machine-learned models for author profiling in social media often rely on
data acquired via self-reporting-based psychometric tests (questionnaires)
filled out by social media users. This is an expensive but accurate data
collection strategy. Another, less costly alternative, which leads to
potentially more noisy and biased data, is to rely on labels inferred from
publicly available information in the profiles of the users, for instance
self-reported diagnoses or test results. In this paper, we explore a third
strategy, namely to directly use a corpus of items from validated psychometric
tests as training data. Items from psychometric tests often consist of
sentences from an I-perspective (e.g., "I make friends easily."). Such corpora
of test items constitute 'small data', but their availability for many concepts
is a rich resource. We investigate this approach for personality profiling, and
evaluate BERT classifiers fine-tuned on such psychometric test items for the
big five personality traits (openness, conscientiousness, extraversion,
agreeableness, neuroticism) and analyze various augmentation strategies
regarding their potential to address the challenges coming with such a small
corpus. Our evaluation on a publicly available Twitter corpus shows a
comparable performance to in-domain training for 4/5 personality traits with
T5-based data augmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accelerating Attention through Gradient-Based Learned Runtime Pruning. (arXiv:2204.03227v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03227">
<div class="article-summary-box-inner">
<span><p>Self-attention is a key enabler of state-of-art accuracy for various
transformer-based Natural Language Processing models. This attention mechanism
calculates a correlation score for each word with respect to the other words in
a sentence. Commonly, only a small subset of words highly correlates with the
word under attention, which is only determined at runtime. As such, a
significant amount of computation is inconsequential due to low attention
scores and can potentially be pruned. The main challenge is finding the
threshold for the scores below which subsequent computation will be
inconsequential. Although such a threshold is discrete, this paper formulates
its search through a soft differentiable regularizer integrated into the loss
function of the training. This formulation piggy backs on the back-propagation
training to analytically co-optimize the threshold and the weights
simultaneously, striking a formally optimal balance between accuracy and
computation pruning. To best utilize this mathematical innovation, we devise a
bit-serial architecture, dubbed LeOPArd, for transformer language models with
bit-level early termination microarchitectural mechanism. We evaluate our
design across 43 back-end tasks for MemN2N, BERT, ALBERT, GPT-2, and Vision
transformer models. Post-layout results show that, on average, LeOPArd yields
1.9x and 3.9x speedup and energy reduction, respectively, while keeping the
average accuracy virtually intact (&lt;0.2% degradation)
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Korean Online Hate Speech Dataset for Multilabel Classification: How Can Social Science Improve Dataset on Hate Speech?. (arXiv:2204.03262v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03262">
<div class="article-summary-box-inner">
<span><p>We suggest a multilabel Korean online hate speech dataset that covers seven
categories of hate speech: (1) Race and Nationality, (2) Religion, (3)
Regionalism, (4) Ageism, (5) Misogyny, (6) Sexual Minorities, and (7) Male. Our
35K dataset consists of 24K online comments with Krippendorff's Alpha label
accordance of .713, 2.2K neutral sentences from Wikipedia, 1.7K additionally
labeled sentences generated by the Human-in-the-Loop procedure and
rule-generated 7.1K neutral sentences. The base model with 24K initial dataset
achieved the accuracy of LRAP .892, but improved to .919 after being combined
with 11K additional data. Unlike the conventional binary hate and non-hate
dichotomy approach, we designed a dataset considering both the cultural and
linguistic context to overcome the limitations of western culture-based English
texts. Thus, this paper is not only limited to presenting a local hate speech
dataset but extends as a manual for building a more generalized hate speech
dataset with diverse cultural backgrounds based on social science perspectives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comprehensive Review of Sign Language Recognition: Different Types, Modalities, and Datasets. (arXiv:2204.03328v1 [cs.CV] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03328">
<div class="article-summary-box-inner">
<span><p>A machine can understand human activities, and the meaning of signs can help
overcome the communication barriers between the inaudible and ordinary people.
Sign Language Recognition (SLR) is a fascinating research area and a crucial
task concerning computer vision and pattern recognition. Recently, SLR usage
has increased in many applications, but the environment, background image
resolution, modalities, and datasets affect the performance a lot. Many
researchers have been striving to carry out generic real-time SLR models. This
review paper facilitates a comprehensive overview of SLR and discusses the
needs, challenges, and problems associated with SLR. We study related works
about manual and non-manual, various modalities, and datasets. Research
progress and existing state-of-the-art SLR models over the past decade have
been reviewed. Finally, we find the research gap and limitations in this domain
and suggest future directions. This review paper will be helpful for readers
and researchers to get complete guidance about SLR and the progressive design
of the state-of-the-art SLR model
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">PlutoNet: An Efficient Polyp Segmentation Network. (arXiv:2204.03652v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03652">
<div class="article-summary-box-inner">
<span><p>Polyps in the colon can turn into cancerous cells if not removed with early
intervention. Deep learning models are used to minimize the number of polyps
that goes unnoticed by the experts, and to accurately segment the detected
polyps during these interventions. Although these models perform well on these
tasks, they require too many parameters, which can pose a problem with
real-time applications. To address this problem, we propose a novel
segmentation model called PlutoNet which requires only 2,626,337 parameters
while outperforming state-of-the-art models on multiple medical image
segmentation tasks. We use EfficientNetB0 architecture as a backbone and
propose the novel modified partial decoder, which is a combination of partial
decoder and full scale connections, which further reduces the number of
parameters required, as well as captures semantic details. We use asymmetric
convolutions to handle varying polyp sizes. Finally, we weight each feature map
to improve segmentation by using a squeeze and excitation block. In addition to
polyp segmentation in colonoscopy, we tested our model on segmentation of
nuclei and surgical instruments to demonstrate its generalizability to
different medical image segmentation tasks. Our model outperformed the
state-of-the-art models with a Dice score of %92.3 in CVC-ClinicDB dataset and
%89.3 in EndoScene dataset, a Dice score of %91.93 on the 2018 Data Science
Bowl Challenge dataset, and a Dice score of %94.8 on Kvasir-Instrument dataset.
Our experiments and ablation studies show that our model is superior in terms
of accuracy, and it is able generalize well to multiple medical segmentation
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identification of Autism spectrum disorder based on a novel feature selection method and Variational Autoencoder. (arXiv:2204.03654v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03654">
<div class="article-summary-box-inner">
<span><p>The development of noninvasive brain imaging such as resting-state functional
magnetic resonance imaging (rs-fMRI) and its combination with AI algorithm
provides a promising solution for the early diagnosis of Autism spectrum
disorder (ASD). However, the performance of the current ASD classification
based on rs-fMRI still needs to be improved. This paper introduces a
classification framework to aid ASD diagnosis based on rs-fMRI. In the
framework, we proposed a novel filter feature selection method based on the
difference between step distribution curves (DSDC) to select remarkable
functional connectivities (FCs) and utilized a multilayer perceptron (MLP)
which was pretrained by a simplified Variational Autoencoder (VAE) for
classification. We also designed a pipeline consisting of a normalization
procedure and a modified hyperbolic tangent (tanh) activation function to
replace the original tanh function, further improving the model accuracy. Our
model was evaluated by 10 times 10-fold cross-validation and achieved an
average accuracy of 78.12%, outperforming the state-of-the-art methods reported
on the same dataset. Given the importance of sensitivity and specificity in
disease diagnosis, two constraints were designed in our model which can improve
the model's sensitivity and specificity by up to 9.32% and 10.21%,
respectively. The added constraints allow our model to handle different
application scenarios and can be used broadly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TemporalUV: Capturing Loose Clothing with Temporally Coherent UV Coordinates. (arXiv:2204.03671v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03671">
<div class="article-summary-box-inner">
<span><p>We propose a novel approach to generate temporally coherent UV coordinates
for loose clothing. Our method is not constrained by human body outlines and
can capture loose garments and hair. We implemented a differentiable pipeline
to learn UV mapping between a sequence of RGB inputs and textures via UV
coordinates. Instead of treating the UV coordinates of each frame separately,
our data generation approach connects all UV coordinates via feature matching
for temporal stability. Subsequently, a generative model is trained to balance
the spatial quality and temporal stability. It is driven by supervised and
unsupervised losses in both UV and image spaces. Our experiments show that the
trained models output high-quality UV coordinates and generalize to new poses.
Once a sequence of UV coordinates has been inferred by our model, it can be
used to flexibly synthesize new looks and modified visual styles. Compared to
existing methods, our approach reduces the computational workload to animate
new outfits by several orders of magnitude.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DAD-3DHeads: A Large-scale Dense, Accurate and Diverse Dataset for 3D Head Alignment from a Single Image. (arXiv:2204.03688v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03688">
<div class="article-summary-box-inner">
<span><p>We present DAD-3DHeads, a dense and diverse large-scale dataset, and a robust
model for 3D Dense Head Alignment in the wild. It contains annotations of over
3.5K landmarks that accurately represent 3D head shape compared to the
ground-truth scans. The data-driven model, DAD-3DNet, trained on our dataset,
learns shape, expression, and pose parameters, and performs 3D reconstruction
of a FLAME mesh. The model also incorporates a landmark prediction branch to
take advantage of rich supervision and co-training of multiple related tasks.
Experimentally, DAD-3DNet outperforms or is comparable to the state-of-the-art
models in (i) 3D Head Pose Estimation on AFLW2000-3D and BIWI, (ii) 3D Face
Shape Reconstruction on NoW and Feng, and (iii) 3D Dense Head Alignment and 3D
Landmarks Estimation on DAD-3DHeads dataset. Finally, the diversity of
DAD-3DHeads in camera angles, facial expressions, and occlusions enables a
benchmark to study in-the-wild generalization and robustness to distribution
shifts. The dataset webpage is https://p.farm/research/dad-3dheads.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive-Gravity: A Defense Against Adversarial Samples. (arXiv:2204.03694v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03694">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel model training solution, denoted as
Adaptive-Gravity, for enhancing the robustness of deep neural network
classifiers against adversarial examples. We conceptualize the model
parameters/features associated with each class as a mass characterized by its
centroid location and the spread (standard deviation of the distance) of
features around the centroid. We use the centroid associated with each cluster
to derive an anti-gravity force that pushes the centroids of different classes
away from one another during network training. Then we customized an objective
function that aims to concentrate each class's features toward their
corresponding new centroid, which has been obtained by anti-gravity force. This
methodology results in a larger separation between different masses and reduces
the spread of features around each centroid. As a result, the samples are
pushed away from the space that adversarial examples could be mapped to,
effectively increasing the degree of perturbation needed for making an
adversarial example. We have implemented this training solution as an iterative
method consisting of four steps at each iteration: 1) centroid extraction, 2)
anti-gravity force calculation, 3) centroid relocation, and 4) gravity
training. Gravity's efficiency is evaluated by measuring the corresponding
fooling rates against various attack models, including FGSM, MIM, BIM, and PGD
using LeNet and ResNet110 networks, benchmarked against MNIST and CIFAR10
classification problems. Test results show that Gravity not only functions as a
powerful instrument to robustify a model against state-of-the-art adversarial
attacks but also effectively improves the model training accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting Solar Flares Using CNN and LSTM on Two Solar Cycles of Active Region Data. (arXiv:2204.03710v1 [astro-ph.SR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03710">
<div class="article-summary-box-inner">
<span><p>We consider the flare prediction problem that distinguishes flare-imminent
active regions that produce an M- or X-class flare in the future 24 hours, from
quiet active regions that do not produce any flare within $\pm 24$ hours. Using
line-of-sight magnetograms and parameters of active regions in two data
products covering Solar Cycle 23 and 24, we train and evaluate two deep
learning algorithms -- CNN and LSTM -- and their stacking ensembles. The
decisions of CNN are explained using visual attribution methods. We have the
following three main findings. (1) LSTM trained on data from two solar cycles
achieves significantly higher True Skill Scores (TSS) than that trained on data
from a single solar cycle with a confidence level of at least 0.95. (2) On data
from Solar Cycle 23, a stacking ensemble that combines predictions from LSTM
and CNN using the TSS criterion achieves significantly higher TSS than the
"select-best" strategy with a confidence level of at least 0.95. (3) A visual
attribution method called Integrated Gradients is able to attribute the CNN's
predictions of flares to the emerging magnetic flux in the active region. It
also reveals a limitation of CNN as a flare prediction method using
line-of-sight magnetograms: it treats the polarity artifact of line-of-sight
magnetograms as positive evidence of flares.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Multiple Self-Supervised Tasks Improves Model Robustness. (arXiv:2204.03714v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03714">
<div class="article-summary-box-inner">
<span><p>Deep networks achieve state-of-the-art performance on computer vision tasks,
yet they fail under adversarial attacks that are imperceptible to humans. In
this paper, we propose a novel defense that can dynamically adapt the input
using the intrinsic structure from multiple self-supervised tasks. By
simultaneously using many self-supervised tasks, our defense avoids
over-fitting the adapted image to one specific self-supervised task and
restores more intrinsic structure in the image compared to a single
self-supervised task approach. Our approach further improves robustness and
clean accuracy significantly compared to the state-of-the-art single task
self-supervised defense. Our work is the first to connect multiple
self-supervised tasks to robustness, and suggests that we can achieve better
robustness with more intrinsic signal from visual data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gravitationally Lensed Black Hole Emission Tomography. (arXiv:2204.03715v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03715">
<div class="article-summary-box-inner">
<span><p>Measurements from the Event Horizon Telescope enabled the visualization of
light emission around a black hole for the first time. So far, these
measurements have been used to recover a 2D image under the assumption that the
emission field is static over the period of acquisition. In this work, we
propose BH-NeRF, a novel tomography approach that leverages gravitational
lensing to recover the continuous 3D emission field near a black hole. Compared
to other 3D reconstruction or tomography settings, this task poses two
significant challenges: first, rays near black holes follow curved paths
dictated by general relativity, and second, we only observe measurements from a
single viewpoint. Our method captures the unknown emission field using a
continuous volumetric function parameterized by a coordinate-based neural
network, and uses knowledge of Keplerian orbital dynamics to establish
correspondence between 3D points over time. Together, these enable BH-NeRF to
recover accurate 3D emission fields, even in challenging situations with sparse
measurements and uncertain orbital dynamics. This work takes the first steps in
showing how future measurements from the Event Horizon Telescope could be used
to recover evolving 3D emission around the supermassive black hole in our
Galactic center.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Design of Salient Object Detection Algorithms with Brain Programming. (arXiv:2204.03722v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03722">
<div class="article-summary-box-inner">
<span><p>Despite recent improvements in computer vision, artificial visual systems'
design is still daunting since an explanation of visual computing algorithms
remains elusive. Salient object detection is one problem that is still open due
to the difficulty of understanding the brain's inner workings. Progress on this
research area follows the traditional path of hand-made designs using
neuroscience knowledge. In recent years two different approaches based on
genetic programming appear to enhance their technique. One follows the idea of
combining previous hand-made methods through genetic programming and fuzzy
logic. The other approach consists of improving the inner computational
structures of basic hand-made models through artificial evolution. This
research work proposes expanding the artificial dorsal stream using a recent
proposal to solve salient object detection problems. This approach uses the
benefits of the two main aspects of this research area: fixation prediction and
detection of salient objects. We decided to apply the fusion of visual saliency
and image segmentation algorithms as a template. The proposed methodology
discovers several critical structures in the template through artificial
evolution. We present results on a benchmark designed by experts with
outstanding results in comparison with the state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MHMS: Multimodal Hierarchical Multimedia Summarization. (arXiv:2204.03734v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03734">
<div class="article-summary-box-inner">
<span><p>Multimedia summarization with multimodal output can play an essential role in
real-world applications, i.e., automatically generating cover images and titles
for news articles or providing introductions to online videos. In this work, we
propose a multimodal hierarchical multimedia summarization (MHMS) framework by
interacting visual and language domains to generate both video and textual
summaries. Our MHMS method contains video and textual segmentation and
summarization module, respectively. It formulates a cross-domain alignment
objective with optimal transport distance which leverages cross-domain
interaction to generate the representative keyframe and textual summary. We
evaluated MHMS on three recent multimodal datasets and demonstrated the
effectiveness of our method in producing high-quality multimodal summaries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BankNote-Net: Open dataset for assistive universal currency recognition. (arXiv:2204.03738v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03738">
<div class="article-summary-box-inner">
<span><p>Millions of people around the world have low or no vision. Assistive software
applications have been developed for a variety of day-to-day tasks, including
optical character recognition, scene identification, person recognition, and
currency recognition. This last task, the recognition of banknotes from
different denominations, has been addressed by the use of computer vision
models for image recognition. However, the datasets and models available for
this task are limited, both in terms of dataset size and in variety of
currencies covered. In this work, we collect a total of 24,826 images of
banknotes in variety of assistive settings, spanning 17 currencies and 112
denominations. Using supervised contrastive learning, we develop a machine
learning model for universal currency recognition. This model learns compliant
embeddings of banknote images in a variety of contexts, which can be shared
publicly (as a compressed vector representation), and can be used to train and
test specialized downstream models for any currency, including those not
covered by our dataset or for which only a few real images per denomination are
available (few-shot learning). We deploy a variation of this model for public
use in the last version of the Seeing AI app developed by Microsoft. We share
our encoder model and the embeddings as an open dataset in our BankNote-Net
repository.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Drivers' attention detection: a systematic literature review. (arXiv:2204.03741v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03741">
<div class="article-summary-box-inner">
<span><p>Countless traffic accidents often occur because of the inattention of the
drivers. Many factors can contribute to distractions while driving, since
objects or events to physiological conditions, as drowsiness and fatigue, do
not allow the driver to stay attentive. The technological progress allowed the
development and application of many solutions to detect the attention in real
situations, promoting the interest of the scientific community in these last
years. Commonly, these solutions identify the lack of attention and alert the
driver, in order to help her/him to recover the attention, avoiding serious
accidents and preserving lives. Our work presents a Systematic Literature
Review (SLR) of the methods and criteria used to detect attention of drivers at
the wheel, focusing on those methods based on images. As results, 50 studies
were selected from the literature on drivers' attention detection, in which 22
contain solutions in the desired context. The results of SLR can be used as a
resource in the preparation of new research projects in drivers' attention
detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mitosis domain generalization in histopathology images -- The MIDOG challenge. (arXiv:2204.03742v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03742">
<div class="article-summary-box-inner">
<span><p>The density of mitotic figures within tumor tissue is known to be highly
correlated with tumor proliferation and thus is an important marker in tumor
grading. Recognition of mitotic figures by pathologists is known to be subject
to a strong inter-rater bias, which limits the prognostic value.
State-of-the-art deep learning methods can support the expert in this
assessment but are known to strongly deteriorate when applied in a different
clinical environment than was used for training. One decisive component in the
underlying domain shift has been identified as the variability caused by using
different whole slide scanners. The goal of the MICCAI MIDOG 2021 challenge has
been to propose and evaluate methods that counter this domain shift and derive
scanner-agnostic mitosis detection algorithms. The challenge used a training
set of 200 cases, split across four scanning systems. As a test set, an
additional 100 cases split across four scanning systems, including two
previously unseen scanners, were given. The best approaches performed on an
expert level, with the winning algorithm yielding an F_1 score of 0.748 (CI95:
0.704-0.781). In this paper, we evaluate and compare the approaches that were
submitted to the challenge and identify methodological factors contributing to
better performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Powering Finetuning in Few-shot Learning: Domain-Agnostic Feature Adaptation with Rectified Class Prototypes. (arXiv:2204.03749v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03749">
<div class="article-summary-box-inner">
<span><p>In recent works, utilizing a deep network trained on meta-training set serves
as a strong baseline in few-shot learning. In this paper, we move forward to
refine novel-class features by finetuning a trained deep network. Finetuning is
designed to focus on reducing biases in novel-class feature distributions,
which we define as two aspects: class-agnostic and class-specific biases.
Class-agnostic bias is defined as the distribution shifting introduced by
domain difference, which we propose Distribution Calibration Module(DCM) to
reduce. DCM owes good property of eliminating domain difference and fast
feature adaptation during optimization. Class-specific bias is defined as the
biased estimation using a few samples in novel classes, which we propose
Selected Sampling(SS) to reduce. Without inferring the actual class
distribution, SS is designed by running sampling using proposal distributions
around support-set samples. By powering finetuning with DCM and SS, we achieve
state-of-the-art results on Meta-Dataset with consistent performance boosts
over ten datasets from different domains. We believe our simple yet effective
method demonstrates its possibility to be applied on practical few-shot
applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-objective optimization determines when, which and how to fuse deep networks: an application to predict COVID-19 outcomes. (arXiv:2204.03772v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03772">
<div class="article-summary-box-inner">
<span><p>The COVID-19 pandemic has caused millions of cases and deaths and the
AI-related scientific community, after being involved with detecting COVID-19
signs in medical images, has been now directing the efforts towards the
development of methods that can predict the progression of the disease. This
task is multimodal by its very nature and, recently, baseline results achieved
on the publicly available AIforCOVID dataset have shown that chest X-ray scans
and clinical information are useful to identify patients at risk of severe
outcomes. While deep learning has shown superior performance in several medical
fields, in most of the cases it considers unimodal data only. In this respect,
when, which and how to fuse the different modalities is an open challenge in
multimodal deep learning. To cope with these three questions here we present a
novel approach optimizing the setup of a multimodal end-to-end model. It
exploits Pareto multi-objective optimization working with a performance metric
and the diversity score of multiple candidate unimodal neural networks to be
fused. We test our method on the AIforCOVID dataset, attaining state-of-the-art
results, not only outperforming the baseline performance but also being robust
to external validation. Moreover, exploiting XAI algorithms we figure out a
hierarchy among the modalities and we extract the features' intra-modality
importance, enriching the trust on the predictions made by the model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TorMentor: Deterministic dynamic-path, data augmentations with fractals. (arXiv:2204.03776v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03776">
<div class="article-summary-box-inner">
<span><p>We propose the use of fractals as a means of efficient data augmentation.
Specifically, we employ plasma fractals for adapting global image augmentation
transformations into continuous local transforms. We formulate the diamond
square algorithm as a cascade of simple convolution operations allowing
efficient computation of plasma fractals on the GPU. We present the TorMentor
image augmentation framework that is totally modular and deterministic across
images and point-clouds. All image augmentation operations can be combined
through pipelining and random branching to form flow networks of arbitrary
width and depth. We demonstrate the efficiency of the proposed approach with
experiments on document image segmentation (binarization) with the DIBCO
datasets. The proposed approach demonstrates superior performance to
traditional image augmentation techniques. Finally, we use extended synthetic
binary text images in a self-supervision regiment and outperform the same model
when trained with limited data and simple extensions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Representation and Dependency Learning for Multi-Label Image Recognition. (arXiv:2204.03795v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03795">
<div class="article-summary-box-inner">
<span><p>Recently many multi-label image recognition (MLR) works have made significant
progress by introducing pre-trained object detection models to generate lots of
proposals or utilizing statistical label co-occurrence enhance the correlation
among different categories. However, these works have some limitations: (1) the
effectiveness of the network significantly depends on pre-trained object
detection models that bring expensive and unaffordable computation; (2) the
network performance degrades when there exist occasional co-occurrence objects
in images, especially for the rare categories. To address these problems, we
propose a novel and effective semantic representation and dependency learning
(SRDL) framework to learn category-specific semantic representation for each
category and capture semantic dependency among all categories. Specifically, we
design a category-specific attentional regions (CAR) module to generate
channel/spatial-wise attention matrices to guide model to focus on
semantic-aware regions. We also design an object erasing (OE) module to
implicitly learn semantic dependency among categories by erasing semantic-aware
regions to regularize the network training. Extensive experiments and
comparisons on two popular MLR benchmark datasets (i.e., MS-COCO and Pascal VOC
2007) demonstrate the effectiveness of the proposed framework over current
state-of-the-art algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Learnable Variational Model for Joint Multimodal MRI Reconstruction and Synthesis. (arXiv:2204.03804v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03804">
<div class="article-summary-box-inner">
<span><p>Generating multi-contrasts/modal MRI of the same anatomy enriches diagnostic
information but is limited in practice due to excessive data acquisition time.
In this paper, we propose a novel deep-learning model for joint reconstruction
and synthesis of multi-modal MRI using incomplete k-space data of several
source modalities as inputs. The output of our model includes reconstructed
images of the source modalities and high-quality image synthesized in the
target modality. Our proposed model is formulated as a variational problem that
leverages several learnable modality-specific feature extractors and a
multimodal synthesis module. We propose a learnable optimization algorithm to
solve this model, which induces a multi-phase network whose parameters can be
trained using multi-modal MRI data. Moreover, a bilevel-optimization framework
is employed for robust parameter training. We demonstrate the effectiveness of
our approach using extensive numerical experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Canonical Mean Filter for Almost Zero-Shot Multi-Task classification. (arXiv:2204.03815v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03815">
<div class="article-summary-box-inner">
<span><p>The support set is a key to providing conditional prior for fast adaption of
the model in few-shot tasks. But the strict form of support set makes its
construction actually difficult in practical application. Motivated by ANIL, we
rethink the role of adaption in the feature extractor of CNAPs, which is a
state-of-the-art representative few-shot method. To investigate the role,
Almost Zero-Shot (AZS) task is designed by fixing the support set to replace
the common scheme, which provides corresponding support sets for the different
conditional prior of different tasks. The AZS experiment results infer that the
adaptation works little in the feature extractor. However, CNAPs cannot be
robust to randomly selected support sets and perform poorly on some datasets of
Meta-Dataset because of its scattered mean embeddings responded by the simple
mean operator. To enhance the robustness of CNAPs, Canonical Mean Filter (CMF)
module is proposed to make the mean embeddings intensive and stable in feature
space by mapping the support sets into a canonical form. CMFs make CNAPs robust
to any fixed support sets even if they are random matrices. This attribution
makes CNAPs be able to remove the mean encoder and the parameter adaptation
network at the test stage, while CNAP-CMF on AZS tasks keeps the performance
with one-shot tasks. It leads to a big parameter reduction. Precisely, 40.48\%
parameters are dropped at the test stage. Also, CNAP-CMF outperforms CNAPs in
one-shot tasks because it addresses inner-task unstable performance problems.
Classification performance, visualized and clustering results verify that CMFs
make CNAPs better and simpler.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reusing the Task-specific Classifier as a Discriminator: Discriminator-free Adversarial Domain Adaptation. (arXiv:2204.03838v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03838">
<div class="article-summary-box-inner">
<span><p>Adversarial learning has achieved remarkable performances for unsupervised
domain adaptation (UDA). Existing adversarial UDA methods typically adopt an
additional discriminator to play the min-max game with a feature extractor.
However, most of these methods failed to effectively leverage the predicted
discriminative information, and thus cause mode collapse for generator. In this
work, we address this problem from a different perspective and design a simple
yet effective adversarial paradigm in the form of a discriminator-free
adversarial learning network (DALN), wherein the category classifier is reused
as a discriminator, which achieves explicit domain alignment and category
distinguishment through a unified objective, enabling the DALN to leverage the
predicted discriminative information for sufficient feature alignment.
Basically, we introduce a Nuclear-norm Wasserstein discrepancy (NWD) that has
definite guidance meaning for performing discrimination. Such NWD can be
coupled with the classifier to serve as a discriminator satisfying the
K-Lipschitz constraint without the requirements of additional weight clipping
or gradient penalty strategy. Without bells and whistles, DALN compares
favorably against the existing state-of-the-art (SOTA) methods on a variety of
public datasets. Moreover, as a plug-and-play technique, NWD can be directly
used as a generic regularizer to benefit existing UDA algorithms. Code is
available at https://github.com/xiaoachen98/DALN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From 2D Images to 3D Model:Weakly Supervised Multi-View Face Reconstruction with Deep Fusion. (arXiv:2204.03842v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03842">
<div class="article-summary-box-inner">
<span><p>We consider the problem of Multi-view 3D Face Reconstruction (MVR) with
weakly supervised learning that leverages a limited number of 2D face images
(e.g. 3) to generate a high-quality 3D face model with very light annotation.
Despite their encouraging performance, present MVR methods simply concatenate
multi-view image features and pay less attention to critical areas (e.g. eye,
brow, nose and mouth). To this end, we propose a novel model called Deep Fusion
MVR (DF-MVR) and design a multi-view encoding to a single decoding framework
with skip connections, able to extract, integrate, and compensate deep features
with attention from multi-view images. In addition, we develop a multi-view
face parse network to learn, identify, and emphasize the critical common face
area. Finally, though our model is trained with a few 2D images, it can
reconstruct an accurate 3D model even if one single 2D image is input. We
conduct extensive experiments to evaluate various multi-view 3D face
reconstruction methods. Our proposed model attains superior performance,
leading to 11.4% RMSE improvement over the existing best weakly supervised
MVRs. Source codes are available in the supplementary materials.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prediction of COVID-19 using chest X-ray images. (arXiv:2204.03849v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03849">
<div class="article-summary-box-inner">
<span><p>COVID-19, also known as Novel Coronavirus Disease, is a highly contagious
disease that first surfaced in China in late 2019. SARS-CoV-2 is a coronavirus
that belongs to the vast family of coronaviruses that causes this disease. The
sickness originally appeared in Wuhan, China in December 2019 and quickly
spread to over 213 nations, becoming a global pandemic. Fever, dry cough, and
tiredness are the most typical COVID-19 symptoms. Aches, pains, and difficulty
breathing are some of the other symptoms that patients may face. The majority
of these symptoms are indicators of respiratory infections and lung
abnormalities, which radiologists can identify. Chest x-rays of COVID-19
patients seem similar, with patchy and hazy lungs rather than clear and healthy
lungs. On x-rays, however, pneumonia and other chronic lung disorders can
resemble COVID-19. Trained radiologists must be able to distinguish between
COVID-19 and an illness that is less contagious. Our AI algorithm seeks to give
doctors a quantitative estimate of the risk of deterioration. So that patients
at high risk of deterioration can be triaged and treated efficiently. The
method could be particularly useful in pandemic hotspots when screening upon
admission is important for allocating limited resources like hospital beds.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-scale temporal network for continuous sign language recognition. (arXiv:2204.03864v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03864">
<div class="article-summary-box-inner">
<span><p>Continuous Sign Language Recognition (CSLR) is a challenging research task
due to the lack of accurate annotation on the temporal sequence of sign
language data. The recent popular usage is a hybrid model based on "CNN + RNN"
for CSLR. However, when extracting temporal features in these works, most of
the methods using a fixed temporal receptive field and cannot extract the
temporal features well for each sign language word. In order to obtain more
accurate temporal features, this paper proposes a multi-scale temporal network
(MSTNet). The network mainly consists of three parts. The Resnet and two fully
connected (FC) layers constitute the frame-wise feature extraction part. The
time-wise feature extraction part performs temporal feature learning by first
extracting temporal receptive field features of different scales using the
proposed multi-scale temporal block (MST-block) to improve the temporal
modeling capability, and then further encoding the temporal features of
different scales by the transformers module to obtain more accurate temporal
features. Finally, the proposed multi-level Connectionist Temporal
Classification (CTC) loss part is used for training to obtain recognition
results. The multi-level CTC loss enables better learning and updating of the
shallow network parameters in CNN, and the method has no parameter increase and
can be flexibly embedded in other models. Experimental results on two publicly
available datasets demonstrate that our method can effectively extract sign
language features in an end-to-end manner without any prior knowledge,
improving the accuracy of CSLR and reaching the state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatiotemporal Augmentation on Selective Frequencies for Video Representation Learning. (arXiv:2204.03865v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03865">
<div class="article-summary-box-inner">
<span><p>Recent self-supervised video representation learning methods focus on
maximizing the similarity between multiple augmented views from the same video
and largely rely on the quality of generated views. In this paper, we propose
frequency augmentation (FreqAug), a spatio-temporal data augmentation method in
the frequency domain for video representation learning. FreqAug stochastically
removes undesirable information from the video by filtering out specific
frequency components so that learned representation captures essential features
of the video for various downstream tasks. Specifically, FreqAug pushes the
model to focus more on dynamic features rather than static features in the
video via dropping spatial or temporal low-frequency components. In other
words, learning invariance between remaining frequency components results in
high-frequency enhanced representation with less static bias. To verify the
generality of the proposed method, we experiment with FreqAug on multiple
self-supervised learning frameworks along with standard augmentations.
Transferring the improved representation to five video action recognition and
two temporal action localization downstream tasks shows consistent improvements
over baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controllable Missingness from Uncontrollable Missingness: Joint Learning Measurement Policy and Imputation. (arXiv:2204.03872v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03872">
<div class="article-summary-box-inner">
<span><p>Due to the cost or interference of measurement, we need to control
measurement system. Assuming that each variable can be measured sequentially,
there exists optimal policy choosing next measurement for the former
observations. Though optimal measurement policy is actually dependent on the
goal of measurement, we mainly focus on retrieving complete data, so called as
imputation. Also, we adapt the imputation method to missingness varying with
measurement policy. However, learning measurement policy and imputation
requires complete data which is impossible to be observed, unfortunately. To
tackle this problem, we propose a data generation method and joint learning
algorithm. The main idea is that 1) the data generation method is inherited by
imputation method, and 2) the adaptation of imputation encourages measurement
policy to learn more than individual learning. We implemented some variations
of proposed algorithm for two different datasets and various missing rates.
From the experimental results, we demonstrate that our algorithm is generally
applicable and outperforms baseline methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatial Transformer Network on Skeleton-based Gait Recognition. (arXiv:2204.03873v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03873">
<div class="article-summary-box-inner">
<span><p>Skeleton-based gait recognition models usually suffer from the robustness
problem, as the Rank-1 accuracy varies from 90\% in normal walking cases to
70\% in walking with coats cases. In this work, we propose a state-of-the-art
robust skeleton-based gait recognition model called Gait-TR, which is based on
the combination of spatial transformer frameworks and temporal convolutional
networks. Gait-TR achieves substantial improvements over other skeleton-based
gait models with higher accuracy and better robustness on the well-known gait
dataset CASIA-B. Particularly in walking with coats cases, Gait-TR get a 90\%
Rank-1 gait recognition accuracy rate, which is higher than the best result of
silhouette-based models, which usually have higher accuracy than the
silhouette-based gait recognition models. Moreover, our experiment on CASIA-B
shows that the spatial transformer can extract gait features from the human
skeleton better than the widely used graph convolutional network.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CD$^2$-pFed: Cyclic Distillation-guided Channel Decoupling for Model Personalization in Federated Learning. (arXiv:2204.03880v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03880">
<div class="article-summary-box-inner">
<span><p>Federated learning (FL) is a distributed learning paradigm that enables
multiple clients to collaboratively learn a shared global model. Despite the
recent progress, it remains challenging to deal with heterogeneous data
clients, as the discrepant data distributions usually prevent the global model
from delivering good generalization ability on each participating client. In
this paper, we propose CD^2-pFed, a novel Cyclic Distillation-guided Channel
Decoupling framework, to personalize the global model in FL, under various
settings of data heterogeneity. Different from previous works which establish
layer-wise personalization to overcome the non-IID data across different
clients, we make the first attempt at channel-wise assignment for model
personalization, referred to as channel decoupling. To further facilitate the
collaboration between private and shared weights, we propose a novel cyclic
distillation scheme to impose a consistent regularization between the local and
global model representations during the federation. Guided by the cyclical
distillation, our channel decoupling framework can deliver more accurate and
generalized results for different kinds of heterogeneity, such as feature skew,
label distribution skew, and concept shift. Comprehensive experiments on four
benchmarks, including natural image and medical image analysis tasks,
demonstrate the consistent effectiveness of our method on both local and
external validations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision Transformers for Single Image Dehazing. (arXiv:2204.03883v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03883">
<div class="article-summary-box-inner">
<span><p>Image dehazing is a representative low-level vision task that estimates
latent haze-free images from hazy images. In recent years, convolutional neural
network-based methods have dominated image dehazing. However, vision
Transformers, which has recently made a breakthrough in high-level vision
tasks, has not brought new dimensions to image dehazing. We start with the
popular Swin Transformer and find that several of its key designs are
unsuitable for image dehazing. To this end, we propose DehazeFormer, which
consists of various improvements, such as the modified normalization layer,
activation function, and spatial information aggregation scheme. We train
multiple variants of DehazeFormer on various datasets to demonstrate its
effectiveness. Specifically, on the most frequently used SOTS indoor set, our
small model outperforms FFA-Net with only 25% #Param and 5% computational cost.
To the best of our knowledge, our large model is the first method with the PSNR
over 40 dB on the SOTS indoor set, dramatically outperforming the previous
state-of-the-art methods. We also collect a large-scale realistic remote
sensing dehazing dataset for evaluating the method's capability to remove
highly non-homogeneous haze.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SuperNet in Neural Architecture Search: A Taxonomic Survey. (arXiv:2204.03916v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03916">
<div class="article-summary-box-inner">
<span><p>Deep Neural Networks (DNN) have made significant progress in a wide range of
visual recognition tasks such as image classification, object detection, and
semantic segmentation. The evolution of convolutional architectures has led to
better performance by incurring expensive computational costs. In addition,
network design has become a difficult task, which is labor-intensive and
requires a high level of domain knowledge. To mitigate such issues, there have
been studies for a variety of neural architecture search methods that
automatically search for optimal architectures, achieving models with
impressive performance that outperform human-designed counterparts. This survey
aims to provide an overview of existing works in this field of research and
specifically focus on the supernet optimization that builds a neural network
that assembles all the architectures as its sub models by using weight sharing.
We aim to accomplish that by categorizing supernet optimization by proposing
them as solutions to the common challenges found in the literature: data-side
optimization, poor rank correlation alleviation, and transferable NAS for a
number of deployment scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Biometric identification by means of hand geometry and a neural net classifier. (arXiv:2204.03925v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03925">
<div class="article-summary-box-inner">
<span><p>This Paper describes a hand geometry biometric identification system. We have
acquired a database of 22 people using a conventional document scanner. The
experimental section consists of a study about the discrimination capability of
different extracted features, and the identification rate using different
classifiers based on neural networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Hyperspectral-Depth Reconstruction Using Single Color-Dot Projection. (arXiv:2204.03929v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03929">
<div class="article-summary-box-inner">
<span><p>Depth reconstruction and hyperspectral reflectance reconstruction are two
active research topics in computer vision and image processing. Conventionally,
these two topics have been studied separately using independent imaging setups
and there is no existing method which can acquire depth and spectral
reflectance simultaneously in one shot without using special hardware. In this
paper, we propose a novel single-shot hyperspectral-depth reconstruction method
using an off-the-shelf RGB camera and projector. Our method is based on a
single color-dot projection, which simultaneously acts as structured light for
depth reconstruction and spatially-varying color illuminations for
hyperspectral reflectance reconstruction. To jointly reconstruct the depth and
the hyperspectral reflectance from a single color-dot image, we propose a novel
end-to-end network architecture that effectively incorporates a geometric
color-dot pattern loss and a photometric hyperspectral reflectance loss.
Through the experiments, we demonstrate that our hyperspectral-depth
reconstruction method outperforms the combination of an existing
state-of-the-art single-shot hyperspectral reflectance reconstruction method
and depth reconstruction method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does Robustness on ImageNet Transfer to Downstream Tasks?. (arXiv:2204.03934v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03934">
<div class="article-summary-box-inner">
<span><p>As clean ImageNet accuracy nears its ceiling, the research community is
increasingly more concerned about robust accuracy under distributional shifts.
While a variety of methods have been proposed to robustify neural networks,
these techniques often target models trained on ImageNet classification. At the
same time, it is a common practice to use ImageNet pretrained backbones for
downstream tasks such as object detection, semantic segmentation, and image
classification from different domains. This raises a question: Can these robust
image classifiers transfer robustness to downstream tasks? For object detection
and semantic segmentation, we find that a vanilla Swin Transformer, a variant
of Vision Transformer tailored for dense prediction tasks, transfers robustness
better than Convolutional Neural Networks that are trained to be robust to the
corrupted version of ImageNet. For CIFAR10 classification, we find that models
that are robustified for ImageNet do not retain robustness when fully
fine-tuned. These findings suggest that current robustification techniques tend
to emphasize ImageNet evaluations. Moreover, network architecture is a strong
source of robustness when we consider transfer learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Study of a committee of neural networks for biometric hand-geometry recognition. (arXiv:2204.03935v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03935">
<div class="article-summary-box-inner">
<span><p>This Paper studies different committees of neural networks for biometric
pattern recognition. We use the neural nets as classifiers for identification
and verification purposes. We show that a committee of nets can improve the
recognition rates when compared with a multi-start initialization algo-rithm
that just picks up the neural net which offers the best performance. On the
other hand, we found that there is no strong correlation between
identifi-cation and verification applications using the same classifier.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Distinctive Image Captioning via Comparing and Reweighting. (arXiv:2204.03938v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03938">
<div class="article-summary-box-inner">
<span><p>Recent image captioning models are achieving impressive results based on
popular metrics, i.e., BLEU, CIDEr, and SPICE. However, focusing on the most
popular metrics that only consider the overlap between the generated captions
and human annotation could result in using common words and phrases, which
lacks distinctiveness, i.e., many similar images have the same caption. In this
paper, we aim to improve the distinctiveness of image captions via comparing
and reweighting with a set of similar images. First, we propose a
distinctiveness metric -- between-set CIDEr (CIDErBtw) to evaluate the
distinctiveness of a caption with respect to those of similar images. Our
metric reveals that the human annotations of each image in the MSCOCO dataset
are not equivalent based on distinctiveness; however, previous works normally
treat the human annotations equally during training, which could be a reason
for generating less distinctive captions. In contrast, we reweight each
ground-truth caption according to its distinctiveness during training. We
further integrate a long-tailed weight strategy to highlight the rare words
that contain more information, and captions from the similar image set are
sampled as negative examples to encourage the generated sentence to be unique.
Finally, extensive experiments are conducted, showing that our proposed
approach significantly improves both distinctiveness (as measured by CIDErBtw
and retrieval metrics) and accuracy (e.g., as measured by CIDEr) for a wide
variety of image captioning baselines. These results are further confirmed
through a user study.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probabilistic Representations for Video Contrastive Learning. (arXiv:2204.03946v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03946">
<div class="article-summary-box-inner">
<span><p>This paper presents Probabilistic Video Contrastive Learning, a
self-supervised representation learning method that bridges contrastive
learning with probabilistic representation. We hypothesize that the clips
composing the video have different distributions in short-term duration, but
can represent the complicated and sophisticated video distribution through
combination in a common embedding space. Thus, the proposed method represents
video clips as normal distributions and combines them into a Mixture of
Gaussians to model the whole video distribution. By sampling embeddings from
the whole video distribution, we can circumvent the careful sampling strategy
or transformations to generate augmented views of the clips, unlike previous
deterministic methods that have mainly focused on such sample generation
strategies for contrastive learning. We further propose a stochastic
contrastive loss to learn proper video distributions and handle the inherent
uncertainty from the nature of the raw video. Experimental results verify that
our probabilistic embedding stands as a state-of-the-art video representation
learning for action recognition and video retrieval on the most popular
benchmarks, including UCF101 and HMDB51.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Points to Patches: Enabling the Use of Self-Attention for 3D Shape Recognition. (arXiv:2204.03957v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03957">
<div class="article-summary-box-inner">
<span><p>While the Transformer architecture has become ubiquitous in the machine
learning field, its adaptation to 3D shape recognition is non-trivial. Due to
its quadratic computational complexity, the self-attention operator quickly
becomes inefficient as the set of input points grows larger. Furthermore, we
find that the attention mechanism struggles to find useful connections between
individual points on a global scale. In order to alleviate these problems, we
propose a two-stage Point Transformer-in-Transformer (Point-TnT) approach which
combines local and global attention mechanisms, enabling both individual points
and patches of points to attend to each other effectively. Experiments on shape
classification show that such an approach provides more useful features for
downstream tasks than the baseline Transformer, while also being more
computationally efficient. In addition, we also extend our method to feature
matching for scene reconstruction, showing that it can be used in conjunction
with existing scene reconstruction pipelines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SnapMode: An Intelligent and Distributed Large-Scale Fashion Image Retrieval Platform Based On Big Data and Deep Generative Adversarial Network Technologies. (arXiv:2204.03998v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03998">
<div class="article-summary-box-inner">
<span><p>Fashion is now among the largest industries worldwide, for it represents
human history and helps tell the worlds story. As a result of the Fourth
Industrial Revolution, the Internet has become an increasingly important source
of fashion information. However, with a growing number of web pages and social
data, it is nearly impossible for humans to manually catch up with the ongoing
evolution and the continuously variable content in this domain. The proper
management and exploitation of big data can pave the way for the substantial
growth of the global economy as well as citizen satisfaction. Therefore,
computer scientists have found it challenging to handle e-commerce fashion
websites by using big data and machine learning technologies. This paper first
proposes a scalable focused Web Crawler engine based on the distributed
computing platforms to extract and process fashion data on e-commerce websites.
The role of the proposed platform is then described in developing a
disentangled feature extraction method by employing deep convolutional
generative adversarial networks (DCGANs) for content-based image indexing and
retrieval. Finally, the state-of-the-art solutions are compared, and the
results of the proposed approach are analyzed on a standard dataset. For the
real-life implementation of the proposed solution, a Web-based application is
developed on Apache Storm, Kafka, Solr, and Milvus platforms to create a
fashion search engine called SnapMode.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Quasi-AutoRegression: Forecasting the visual popularity of new fashion products. (arXiv:2204.04014v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04014">
<div class="article-summary-box-inner">
<span><p>Estimating the preferences of consumers is of utmost importance for the
fashion industry as appropriately leveraging this information can be beneficial
in terms of profit. Trend detection in fashion is a challenging task due to the
fast pace of change in the fashion industry. Moreover, forecasting the visual
popularity of new garment designs is even more demanding due to lack of
historical data. To this end, we propose MuQAR, a Multimodal
Quasi-AutoRegressive deep learning architecture that combines two modules: (1)
a multi-modal multi-layer perceptron processing categorical and visual features
extracted by computer vision networks and (2) a quasi-autoregressive neural
network modelling the time series of the product's attributes, which are used
as a proxy of temporal popularity patterns mitigating the lack of historical
data. We perform an extensive ablation analysis on two large scale image
fashion datasets, Mallzee-popularity and SHIFT15m to assess the adequacy of
MuQAR and also use the Amazon Reviews: Home and Kitchen dataset to assess
generalisability to other domains. A comparative study on the VISUELLE dataset,
shows that MuQAR is capable of competing and surpassing the domain's current
state of the art by 2.88% in terms of WAPE and 3.04% in terms of MAE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Engagement Detection with Multi-Task Training in E-Learning Environments. (arXiv:2204.04020v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04020">
<div class="article-summary-box-inner">
<span><p>Recognition of user interaction, in particular engagement detection, became
highly crucial for online working and learning environments, especially during
the COVID-19 outbreak. Such recognition and detection systems significantly
improve the user experience and efficiency by providing valuable feedback. In
this paper, we propose a novel Engagement Detection with Multi-Task Training
(ED-MTT) system which minimizes mean squared error and triplet loss together to
determine the engagement level of students in an e-learning environment. The
performance of this system is evaluated and compared against the
state-of-the-art on a publicly available dataset as well as videos collected
from real-life scenarios. The results show that ED-MTT achieves 6% lower MSE
than the best state-of-the-art performance with highly acceptable training time
and lightweight feature extraction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Generic Image Retrieval Method for Date Estimation of Historical Document Collections. (arXiv:2204.04028v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04028">
<div class="article-summary-box-inner">
<span><p>Date estimation of historical document images is a challenging problem, with
several contributions in the literature that lack of the ability to generalize
from one dataset to others. This paper presents a robust date estimation system
based in a retrieval approach that generalizes well in front of heterogeneous
collections. we use a ranking loss function named smooth-nDCG to train a
Convolutional Neural Network that learns an ordination of documents for each
problem. One of the main usages of the presented approach is as a tool for
historical contextual retrieval. It means that scholars could perform
comparative analysis of historical images from big datasets in terms of the
period where they were produced. We provide experimental evaluation on
different types of documents from real datasets of manuscript and newspaper
images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Confidence Score for Unsupervised Foreground Background Separation of Document Images. (arXiv:2204.04044v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04044">
<div class="article-summary-box-inner">
<span><p>Foreground-background separation is an important problem in document image
analysis. Popular unsupervised binarization methods (such as the Sauvola's
algorithm) employ adaptive thresholding to classify pixels as foreground or
background. In this work, we propose a novel approach for computing confidence
scores of the classification in such algorithms. This score provides an insight
of the confidence level of the prediction. The computational complexity of the
proposed approach is the same as the underlying binarization algorithm. Our
experiments illustrate the utility of the proposed scores in various
applications like document binarization, document image cleanup, and texture
addition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient tracking of team sport players with few game-specific annotations. (arXiv:2204.04049v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04049">
<div class="article-summary-box-inner">
<span><p>One of the requirements for team sports analysis is to track and recognize
players. Many tracking and reidentification methods have been proposed in the
context of video surveillance. They show very convincing results when tested on
public datasets such as the MOT challenge. However, the performance of these
methods are not as satisfactory when applied to player tracking. Indeed, in
addition to moving very quickly and often being occluded, the players wear the
same jersey, which makes the task of reidentification very complex. Some recent
tracking methods have been developed more specifically for the team sport
context. Due to the lack of public data, these methods use private datasets
that make impossible a comparison with them. In this paper, we propose a new
generic method to track team sport players during a full game thanks to few
human annotations collected via a semi-interactive system. Non-ambiguous
tracklets and their appearance features are automatically generated with a
detection and a reidentification network both pre-trained on public datasets.
Then an incremental learning mechanism trains a Transformer to classify
identities using few game-specific human annotations. Finally, tracklets are
linked by an association algorithm. We demonstrate the efficiency of our
approach on a challenging rugby sevens dataset. To overcome the lack of public
sports tracking dataset, we publicly release this dataset at
https://kalisteo.cea.fr/index.php/free-resources/. We also show that our method
is able to track rugby sevens players during a full match, if they are
observable at a minimal resolution, with the annotation of only 6 few seconds
length tracklets per player.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying Ambiguous Similarity Conditions via Semantic Matching. (arXiv:2204.04053v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04053">
<div class="article-summary-box-inner">
<span><p>Rich semantics inside an image result in its ambiguous relationship with
others, i.e., two images could be similar in one condition but dissimilar in
another. Given triplets like "aircraft" is similar to "bird" than "train",
Weakly Supervised Conditional Similarity Learning (WS-CSL) learns multiple
embeddings to match semantic conditions without explicit condition labels such
as "can fly". However, similarity relationships in a triplet are uncertain
except providing a condition. For example, the previous comparison becomes
invalid once the conditional label changes to "is vehicle". To this end, we
introduce a novel evaluation criterion by predicting the comparison's
correctness after assigning the learned embeddings to their optimal conditions,
which measures how much WS-CSL could cover latent semantics as the supervised
model. Furthermore, we propose the Distance Induced Semantic COndition
VERification Network (DiscoverNet), which characterizes the instance-instance
and triplets-condition relations in a "decompose-and-fuse" manner. To make the
learned embeddings cover all semantics, DiscoverNet utilizes a set module or an
additional regularizer over the correspondence between a triplet and a
condition. DiscoverNet achieves state-of-the-art performance on benchmarks like
UT-Zappos-50k and Celeb-A w.r.t. different criteria.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning-Based Intra Mode Derivation for Versatile Video Coding. (arXiv:2204.04059v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04059">
<div class="article-summary-box-inner">
<span><p>In intra coding, Rate Distortion Optimization (RDO) is performed to achieve
the optimal intra mode from a pre-defined candidate list. The optimal intra
mode is also required to be encoded and transmitted to the decoder side besides
the residual signal, where lots of coding bits are consumed. To further improve
the performance of intra coding in Versatile Video Coding (VVC), an intelligent
intra mode derivation method is proposed in this paper, termed as Deep Learning
based Intra Mode Derivation (DLIMD). In specific, the process of intra mode
derivation is formulated as a multi-class classification task, which aims to
skip the module of intra mode signaling for coding bits reduction. The
architecture of DLIMD is developed to adapt to different quantization parameter
settings and variable coding blocks including non-square ones, which are
handled by one single trained model. Different from the existing deep learning
based classification problems, the hand-crafted features are also fed into the
intra mode derivation network besides the learned features from feature
learning network. To compete with traditional method, one additional binary
flag is utilized in the video codec to indicate the selected scheme with RDO.
Extensive experimental results reveal that the proposed method can achieve
2.28%, 1.74%, and 2.18% bit rate reduction on average for Y, U, and V
components on the platform of VVC test model, which outperforms the
state-of-the-art works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transfer Attacks Revisited: A Large-Scale Empirical Study in Real Computer Vision Settings. (arXiv:2204.04063v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04063">
<div class="article-summary-box-inner">
<span><p>One intriguing property of adversarial attacks is their "transferability" --
an adversarial example crafted with respect to one deep neural network (DNN)
model is often found effective against other DNNs as well. Intensive research
has been conducted on this phenomenon under simplistic controlled conditions.
Yet, thus far, there is still a lack of comprehensive understanding about
transferability-based attacks ("transfer attacks") in real-world environments.
</p>
<p>To bridge this critical gap, we conduct the first large-scale systematic
empirical study of transfer attacks against major cloud-based MLaaS platforms,
taking the components of a real transfer attack into account. The study leads
to a number of interesting findings which are inconsistent to the existing
ones, including: (1) Simple surrogates do not necessarily improve real transfer
attacks. (2) No dominant surrogate architecture is found in real transfer
attacks. (3) It is the gap between posterior (output of the softmax layer)
rather than the gap between logit (so-called $\kappa$ value) that increases
transferability. Moreover, by comparing with prior works, we demonstrate that
transfer attacks possess many previously unknown properties in real-world
environments, such as (1) Model similarity is not a well-defined concept. (2)
$L_2$ norm of perturbation can generate high transferability without usage of
gradient and is a more powerful source than $L_\infty$ norm. We believe this
work sheds light on the vulnerabilities of popular MLaaS platforms and points
to a few promising research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Invariant Descriptors for Intrinsic Reflectance Optimization. (arXiv:2204.04076v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04076">
<div class="article-summary-box-inner">
<span><p>Intrinsic image decomposition aims to factorize an image into albedo
(reflectance) and shading (illumination) sub-components. Being ill-posed and
under-constrained, it is a very challenging computer vision problem. There are
infinite pairs of reflectance and shading images that can reconstruct the same
input. To address the problem, Intrinsic Images in the Wild provides an
optimization framework based on a dense conditional random field (CRF)
formulation that considers long-range material relations. We improve upon their
model by introducing illumination invariant image descriptors: color ratios.
The color ratios and the reflectance intrinsic are both invariant to
illumination and thus are highly correlated. Through detailed experiments, we
provide ways to inject the color ratios into the dense CRF optimization. Our
approach is physics-based, learning-free and leads to more accurate and robust
reflectance decompositions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">General Incremental Learning with Domain-aware Categorical Representations. (arXiv:2204.04078v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04078">
<div class="article-summary-box-inner">
<span><p>Continual learning is an important problem for achieving human-level
intelligence in real-world applications as an agent must continuously
accumulate knowledge in response to streaming data/tasks. In this work, we
consider a general and yet under-explored incremental learning problem in which
both the class distribution and class-specific domain distribution change over
time. In addition to the typical challenges in class incremental learning, this
setting also faces the intra-class stability-plasticity dilemma and intra-class
domain imbalance problems. To address above issues, we develop a novel
domain-aware continual learning method based on the EM framework. Specifically,
we introduce a flexible class representation based on the von Mises-Fisher
mixture model to capture the intra-class structure, using an
expansion-and-reduction strategy to dynamically increase the number of
components according to the class complexity. Moreover, we design a bi-level
balanced memory to cope with data imbalances within and across classes, which
combines with a distillation loss to achieve better inter- and intra-class
stability-plasticity trade-off. We conduct exhaustive experiments on three
benchmarks: iDigits, iDomainNet and iCIFAR-20. The results show that our
approach consistently outperforms previous methods by a significant margin,
demonstrating its superiority.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">POSTER: A Pyramid Cross-Fusion Transformer Network for Facial Expression Recognition. (arXiv:2204.04083v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04083">
<div class="article-summary-box-inner">
<span><p>Facial Expression Recognition (FER) has received increasing interest in the
computer vision community. As a challenging task, there are three key issues
especially prevalent in FER: inter-class similarity, intra-class discrepancy,
and scale sensitivity. Existing methods typically address some of these issues,
but do not tackle them all in a unified framework. Therefore, in this paper, we
propose a two-stream Pyramid crOss-fuSion TransformER network (POSTER) that
aims to holistically solve these issues. Specifically, we design a
transformer-based cross-fusion paradigm that enables effective collaboration of
facial landmark and direct image features to maximize proper attention to
salient facial regions. Furthermore, POSTER employs a pyramid structure to
promote scale invariance. Extensive experimental results demonstrate that our
POSTER outperforms SOTA methods on RAF-DB with 92.05%, FERPlus with 91.62%,
AffectNet (7 cls) with 67.31%, and AffectNet (8 cls) with 63.34%, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic super-resolution in particle tracking problems. (arXiv:2204.04092v1 [eess.SP])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04092">
<div class="article-summary-box-inner">
<span><p>Particle tracking in biological imaging is concerned with reconstructing the
trajectories, locations, or velocities of the targeting particles. The standard
approach of particle tracking consists of two steps: first reconstructing
statically the source locations in each time step, and second applying tracking
techniques to obtain the trajectories and velocities. In contrast, the dynamic
reconstruction seeks to simultaneously recover the source locations and
velocities from all frames, which enjoys certain advantages. In this paper, we
provide a rigorous mathematical analysis for the resolution limit of
reconstructing source number, locations, and velocities by general dynamical
reconstruction in particle tracking problems, by which we demonstrate the
possibility of achieving super-resolution for the dynamic reconstruction. We
show that when the location-velocity pairs of the particles are separated
beyond certain distances (the resolution limits), the number of particles and
the location-velocity pair can be stably recovered. The resolution limits are
related to the cut-off frequency of the imaging system, signal-to-noise ratio,
and the sparsity of the source. By these estimates, we also derive a stability
result for a sparsity-promoting dynamic reconstruction. In addition, we further
show that the reconstruction of velocities has a better resolution limit which
improves constantly as the particles moving. This result is derived by an
observation that the inherent cut-off frequency for the velocity recovery can
be viewed as the total observation time multiplies the cut-off frequency of the
imaging system, which may lead to a better resolution limit as compared to the
one for each diffraction-limited frame. It is anticipated that this observation
can inspire new reconstruction algorithms that improve the resolution of
particle tracking in practice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visible-Thermal UAV Tracking: A Large-Scale Benchmark and New Baseline. (arXiv:2204.04120v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04120">
<div class="article-summary-box-inner">
<span><p>With the popularity of multi-modal sensors, visible-thermal (RGB-T) object
tracking is to achieve robust performance and wider application scenarios with
the guidance of objects' temperature information. However, the lack of paired
training samples is the main bottleneck for unlocking the power of RGB-T
tracking. Since it is laborious to collect high-quality RGB-T sequences, recent
benchmarks only provide test sequences. In this paper, we construct a
large-scale benchmark with high diversity for visible-thermal UAV tracking
(VTUAV), including 500 sequences with 1.7 million high-resolution (1920
$\times$ 1080 pixels) frame pairs. In addition, comprehensive applications
(short-term tracking, long-term tracking and segmentation mask prediction) with
diverse categories and scenes are considered for exhaustive evaluation.
Moreover, we provide a coarse-to-fine attribute annotation, where frame-level
attributes are provided to exploit the potential of challenge-specific
trackers. In addition, we design a new RGB-T baseline, named Hierarchical
Multi-modal Fusion Tracker (HMFT), which fuses RGB-T data in various levels.
Numerous experiments on several datasets are conducted to reveal the
effectiveness of HMFT and the complement of different fusion types. The project
is available at here.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sat2lod2: A Software For Automated Lod-2 Modeling From Satellite-Derived Orthophoto And Digital Surface Model. (arXiv:2204.04139v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04139">
<div class="article-summary-box-inner">
<span><p>Deriving LoD2 models from orthophoto and digital surface models (DSM)
reconstructed from satellite images is a challenging task. Existing solutions
are mostly system approaches that require complicated step-wise processes,
including not only heuristic geometric operations, but also high-level steps
such as machine learning-based semantic segmentation and building detection.
Here in this paper, we describe an open-source tool, called SAT2LOD2, built
based on a minorly modified version of our recently published work. SAT2LoD2 is
a fully open-source and GUI (Graphics User Interface) based software, coded in
Python, which takes an orthophoto and DSM as inputs, and outputs individual
building models, and it can additionally take road network shapefiles, and
customized classification maps to further improve the reconstruction results.
We further improve the robustness of the method by 1) intergrading building
segmentation based on HRNetV2 into our software; and 2) having implemented a
decision strategy to identify complex buildings and directly generate mesh to
avoid erroneous LoD2 reconstruction from a system point of view. The software
can process a moderate level of data (around 5000*5000 size of orthophoto and
DSM) using a PC with a graphics card supporting CUDA. Furthermore, the GUI is
self-contained and stores the intermediate processing results facilitating
researchers to learn the process easily and reuse intermediate files as needed.
The updated codes and software are available under this GitHub page:
https://github.com/GDAOSU/LOD2BuildingModel.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Spherical Epipolar Rectification for Multi-View Stereo 3D Reconstruction. (arXiv:2204.04141v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04141">
<div class="article-summary-box-inner">
<span><p>Multi-view stereo (MVS) reconstruction is essential for creating 3D models.
The approach involves applying epipolar rectification followed by dense
matching for disparity estimation. However, existing approaches face challenges
in applying dense matching for images with different viewpoints primarily due
to large differences in object scale. In this paper, we propose a spherical
model for epipolar rectification to minimize distortions caused by differences
in principal rays. We evaluate the proposed approach using two aerial-based
datasets consisting of multi-camera head systems. We show through qualitative
and quantitative evaluation that the proposed approach performs better than
frame-based epipolar correction by enhancing the completeness of point clouds
by up to 4.05% while improving the accuracy by up to 10.23% using LiDAR data as
ground truth.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Intrinsic Image Decomposition Method to Recover Albedo for Aerial Images in Photogrammetry Processing. (arXiv:2204.04142v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04142">
<div class="article-summary-box-inner">
<span><p>Recovering surface albedos from photogrammetric images for realistic
rendering and synthetic environments can greatly facilitate its downstream
applications in VR/AR/MR and digital twins. The textured 3D models from
standard photogrammetric pipelines are suboptimal to these applications because
these textures are directly derived from images, which intrinsically embedded
the spatially and temporally variant environmental lighting information, such
as the sun illumination, direction, causing different looks of the surface,
making such models less realistic when used in 3D rendering under synthetic
lightings. On the other hand, since albedo images are less variable by
environmental lighting, it can, in turn, benefit basic photogrammetric
processing. In this paper, we attack the problem of albedo recovery for aerial
images for the photogrammetric process and demonstrate the benefit of albedo
recovery for photogrammetry data processing through enhanced feature matching
and dense matching. To this end, we proposed an image formation model with
respect to outdoor aerial imagery under natural illumination conditions; we
then, derived the inverse model to estimate the albedo by utilizing the typical
photogrammetric products as an initial approximation of the geometry. The
estimated albedo images are tested in intrinsic image decomposition,
relighting, feature matching, and dense matching/point cloud generation
results. Both synthetic and real-world experiments have demonstrated that our
method outperforms existing methods and can enhance photogrammetric processing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optical tracking in team sports. (arXiv:2204.04143v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04143">
<div class="article-summary-box-inner">
<span><p>Sports analysis has gained paramount importance for coaches, scouts, and
fans. Recently, computer vision researchers have taken on the challenge of
collecting the necessary data by proposing several methods of automatic player
and ball tracking. Building on the gathered tracking data, data miners are able
to perform quantitative analysis on the performance of players and teams. With
this survey, our goal is to provide a basic understanding for quantitative data
analysts about the process of creating the input data and the characteristics
thereof. Thus, we summarize the recent methods of optical tracking by providing
a comprehensive taxonomy of conventional and deep learning methods, separately.
Moreover, we discuss the preprocessing steps of tracking, the most common
challenges in this domain, and the application of tracking data to sports
teams. Finally, we compare the methods by their cost and limitations, and
conclude the work by highlighting potential future research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Constrained Bundle Adjustment for Structure From Motion Using Uncalibrated Multi-Camera Systems. (arXiv:2204.04145v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04145">
<div class="article-summary-box-inner">
<span><p>Structure from motion using uncalibrated multi-camera systems is a
challenging task. This paper proposes a bundle adjustment solution that
implements a baseline constraint respecting that these cameras are static to
each other. We assume these cameras are mounted on a mobile platform,
uncalibrated, and coarsely synchronized. To this end, we propose the baseline
constraint that is formulated for the scenario in which the cameras have
overlapping views. The constraint is incorporated in the bundle adjustment
solution to keep the relative motion of different cameras static. Experiments
were conducted using video frames of two collocated GoPro cameras mounted on a
vehicle with no system calibration. These two cameras were placed capturing
overlapping contents. We performed our bundle adjustment using the proposed
constraint and then produced 3D dense point clouds. Evaluations were performed
by comparing these dense point clouds against LiDAR reference data. We showed
that, as compared to traditional bundle adjustment, our proposed method
achieved an improvement of 29.38%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Video Anomaly Detection Framework based on Appearance-Motion Semantics Representation Consistency. (arXiv:2204.04151v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04151">
<div class="article-summary-box-inner">
<span><p>Video anomaly detection refers to the identification of events that deviate
from the expected behavior. Due to the lack of anomalous samples in training,
video anomaly detection becomes a very challenging task. Existing methods
almost follow a reconstruction or future frame prediction mode. However, these
methods ignore the consistency between appearance and motion information of
samples, which limits their anomaly detection performance. Anomalies only occur
in the moving foreground of surveillance videos, so the semantics expressed by
video frame sequences and optical flow without background information in
anomaly detection should be highly consistent and significant for anomaly
detection. Based on this idea, we propose Appearance-Motion Semantics
Representation Consistency (AMSRC), a framework that uses normal data's
appearance and motion semantic representation consistency to handle anomaly
detection. Firstly, we design a two-stream encoder to encode the appearance and
motion information representations of normal samples and introduce constraints
to further enhance the consistency of the feature semantics between appearance
and motion information of normal samples so that abnormal samples with low
consistency appearance and motion feature representation can be identified.
Moreover, the lower consistency of appearance and motion features of anomalous
samples can be used to generate predicted frames with larger reconstruction
error, which makes anomalies easier to spot. Experimental results demonstrate
the effectiveness of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Particle Videos Revisited: Tracking Through Occlusions Using Point Trajectories. (arXiv:2204.04153v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04153">
<div class="article-summary-box-inner">
<span><p>Tracking pixels in videos is typically studied as an optical flow estimation
problem, where every pixel is described with a displacement vector that locates
it in the next frame. Even though wider temporal context is freely available,
prior efforts to take this into account have yielded only small gains over
2-frame methods. In this paper, we revisit Sand and Teller's "particle video"
approach, and study pixel tracking as a long-range motion estimation problem,
where every pixel is described with a trajectory that locates it in multiple
future frames. We re-build this classic approach using components that drive
the current state-of-the-art in flow and object tracking, such as dense cost
maps, iterative optimization, and learned appearance updates. We train our
models using long-range amodal point trajectories mined from existing optical
flow datasets that we synthetically augment with occlusions. We test our
approach in trajectory estimation benchmarks and in keypoint label propagation
tasks, and compare favorably against state-of-the-art optical flow and feature
tracking methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Underwater Image Enhancement Using Pre-trained Transformer. (arXiv:2204.04199v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04199">
<div class="article-summary-box-inner">
<span><p>The goal of this work is to apply a denoising image transformer to remove the
distortion from underwater images and compare it with other similar approaches.
Automatic restoration of underwater images plays an important role since it
allows to increase the quality of the images, without the need for more
expensive equipment. This is a critical example of the important role of the
machine learning algorithms to support marine exploration and monitoring,
reducing the need for human intervention like the manual processing of the
images, thus saving time, effort, and cost. This paper is the first application
of the image transformer-based approach called "Pre-Trained Image Processing
Transformer" to underwater images. This approach is tested on the UFO-120
dataset, containing 1500 images with the corresponding clean images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dancing under the stars: video denoising in starlight. (arXiv:2204.04210v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.04210">
<div class="article-summary-box-inner">
<span><p>Imaging in low light is extremely challenging due to low photon counts. Using
sensitive CMOS cameras, it is currently possible to take videos at night under
moonlight (0.05-0.3 lux illumination). In this paper, we demonstrate
photorealistic video under starlight (no moon present, $&lt;$0.001 lux) for the
first time. To enable this, we develop a GAN-tuned physics-based noise model to
more accurately represent camera noise at the lowest light levels. Using this
noise model, we train a video denoiser using a combination of simulated noisy
video clips and real noisy still images. We capture a 5-10 fps video dataset
with significant motion at approximately 0.6-0.7 millilux with no active
illumination. Comparing against alternative methods, we achieve improved video
quality at the lowest light levels, demonstrating photorealistic video
denoising in starlight for the first time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Searching for Apparel Products from Images in the Wild. (arXiv:1907.02244v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1907.02244">
<div class="article-summary-box-inner">
<span><p>In this age of social media, people often look at what others are wearing. In
particular, Instagram and Twitter influencers often provide images of
themselves wearing different outfits and their followers are often inspired to
buy similar clothes.We propose a system to automatically find the closest
visually similar clothes in the online Catalog (street-to-shop searching). The
problem is challenging since the original images are taken under different pose
and lighting conditions. The system initially localizes high-level descriptive
regions (top, bottom, wristwear. . . ) using multiple CNN detectors such as
YOLO and SSD that are trained specifically for apparel domain. It then
classifies these regions into more specific regions such as t-shirts, tunic or
dresses. Finally, a feature embedding learned using a multi-task function is
recovered for every item and then compared with corresponding items in the
online Catalog database and ranked according to distance. We validate our
approach component-wise using benchmark datasets and end-to-end using human
evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gaining Scale Invariance in UAV Bird's Eye View Object Detection by Adaptive Resizing. (arXiv:2101.12694v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.12694">
<div class="article-summary-box-inner">
<span><p>This work introduces a new preprocessing step for object detection applicable
to UAV bird's eye view imagery, which we call Adaptive Resizing. By design, it
helps alleviate the challenges coming with the vast variances in objects'
scales, naturally inherent to UAV data sets. Furthermore, it improves inference
speed by two to three times on average. We test this extensively on UAVDT,
VisDrone, and on a new data set we captured ourselves and achieve consistent
improvements while being considerably faster. Moreover, we show how to apply
this method to generic UAV object detection tasks. Additionally, we
successfully test our approach on a height transfer task where we train on some
interval of altitudes and test on a different one. Furthermore, we introduce a
small, fast detector meant for deployment to an embedded GPU. Code will be made
publicly available on our website.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalizing to Unseen Domains: A Survey on Domain Generalization. (arXiv:2103.03097v6 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.03097">
<div class="article-summary-box-inner">
<span><p>Machine learning systems generally assume that the training and testing
distributions are the same. To this end, a key requirement is to develop models
that can generalize to unseen distributions. Domain generalization (DG), i.e.,
out-of-distribution generalization, has attracted increasing interests in
recent years. Domain generalization deals with a challenging setting where one
or several different but related domain(s) are given, and the goal is to learn
a model that can generalize to an unseen test domain. Great progress has been
made in the area of domain generalization for years. This paper presents the
first review of recent advances in this area. First, we provide a formal
definition of domain generalization and discuss several related fields. We then
thoroughly review the theories related to domain generalization and carefully
analyze the theory behind generalization. We categorize recent algorithms into
three classes: data manipulation, representation learning, and learning
strategy, and present several popular algorithms in detail for each category.
Third, we introduce the commonly used datasets, applications, and our
open-sourced codebase for fair evaluation. Finally, we summarize existing
literature and present some potential research topics for the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Graph Embeddings for Open World Compositional Zero-Shot Learning. (arXiv:2105.01017v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.01017">
<div class="article-summary-box-inner">
<span><p>Compositional Zero-Shot learning (CZSL) aims to recognize unseen compositions
of state and object visual primitives seen during training. A problem with
standard CZSL is the assumption of knowing which unseen compositions will be
available at test time. In this work, we overcome this assumption operating on
the open world setting, where no limit is imposed on the compositional space at
test time, and the search space contains a large number of unseen compositions.
To address this problem, we propose a new approach, Compositional Cosine Graph
Embeddings (Co-CGE), based on two principles. First, Co-CGE models the
dependency between states, objects and their compositions through a graph
convolutional neural network. The graph propagates information from seen to
unseen concepts, improving their representations. Second, since not all unseen
compositions are equally feasible, and less feasible ones may damage the
learned representations, Co-CGE estimates a feasibility score for each unseen
composition, using the scores as margins in a cosine similarity-based loss and
as weights in the adjacency matrix of the graphs. Experiments show that our
approach achieves state-of-the-art performances in standard CZSL while
outperforming previous methods in the open world scenario.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Voxel Graph CNN for Object Classification with Event Cameras. (arXiv:2106.00216v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.00216">
<div class="article-summary-box-inner">
<span><p>Event cameras attract researchers' attention due to their low power
consumption, high dynamic range, and extremely high temporal resolution.
Learning models on event-based object classification have recently achieved
massive success by accumulating sparse events into dense frames to apply
traditional 2D learning methods. Yet, these approaches necessitate heavy-weight
models and are with high computational complexity due to the redundant
information introduced by the sparse-to-dense conversion, limiting the
potential of event cameras on real-life applications. This study aims to
address the core problem of balancing accuracy and model complexity for
event-based classification models. To this end, we introduce a novel graph
representation for event data to exploit their sparsity better and customize a
lightweight voxel graph convolutional neural network (\textit{EV-VGCNN}) for
event-based classification. Specifically, (1) using voxel-wise vertices rather
than previous point-wise inputs to explicitly exploit regional 2D semantics of
event streams while keeping the sparsity;(2) proposing a multi-scale feature
relational layer (\textit{MFRL}) to extract spatial and motion cues from each
vertex discriminatively concerning its distances to neighbors. Comprehensive
experiments show that our model can advance state-of-the-art classification
accuracy with extremely low model complexity (merely 0.84M parameters).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Provident Vehicle Detection at Night for Advanced Driver Assistance Systems. (arXiv:2107.11302v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.11302">
<div class="article-summary-box-inner">
<span><p>In recent years, computer vision algorithms have become more powerful.
However, current algorithms mainly share one limitation: They rely on directly
visible objects. This is a significant drawback compared to human behavior,
where visual cues caused by objects (e.g., shadows) are already used
intuitively to retrieve information or anticipate occurring objects. While
driving at night, this performance deficit becomes even more obvious: Humans
already process the light artifacts caused by the headlamps of oncoming
vehicles to estimate where they appear, whereas current object detection
systems require that the oncoming vehicle is directly visible before it can be
detected. Based on previous work on this subject, in this paper, we present a
complete system that can detect light artifacts caused by the headlights of
oncoming vehicles so that it detects that a vehicle is approaching providently.
For that, an entire algorithm architecture is investigated, including the
detection in the image space, the three-dimensional localization, and the
tracking of light artifacts. To demonstrate the usefulness of such an
algorithm, the proposed algorithm is deployed in a test vehicle to use the
detected light artifacts to control the glare-free high beam system
proactively. Using this experimental setting, the provident vehicle detection
system's time benefit compared to an in-production computer vision system is
quantified. Additionally, the glare-free high beam use case provides a
real-time and real-world visualization interface of the detection results by
considering the adaptive headlamps as projectors. With this investigation of
provident vehicle detection, we want to put awareness on the unconventional
sensing task of detecting objects providently and further close the performance
gap between human behavior and computer vision algorithms to bring autonomous
and automated driving a step forward.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CONet: Channel Optimization for Convolutional Neural Networks. (arXiv:2108.06822v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06822">
<div class="article-summary-box-inner">
<span><p>Neural Architecture Search (NAS) has shifted network design from using human
intuition to leveraging search algorithms guided by evaluation metrics. We
study channel size optimization in convolutional neural networks (CNN) and
identify the role it plays in model accuracy and complexity. Current channel
size selection methods are generally limited by discrete sample spaces while
suffering from manual iteration and simple heuristics. To solve this, we
introduce an efficient dynamic scaling algorithm -- CONet -- that automatically
optimizes channel sizes across network layers for a given CNN. Two metrics --
"\textit{Rank}" and "\textit{Rank Average Slope}" -- are introduced to identify
the information accumulated in training. The algorithm dynamically scales
channel sizes up or down over a fixed searching phase. We conduct experiments
on CIFAR10/100 and ImageNet datasets and show that CONet can find efficient and
accurate architectures searched in ResNet, DARTS, and DARTS+ spaces that
outperform their baseline models.
</p>
<p>This document supersedes previously published paper in ICCV2021-NeurArch
workshop. An additional section is included on manual scaling of channel size
in CNNs to numerically validate of the metrics used in searching optimum
channel configurations in CNNs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Group-based Distinctive Image Captioning with Memory Attention. (arXiv:2108.09151v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09151">
<div class="article-summary-box-inner">
<span><p>Describing images using natural language is widely known as image captioning,
which has made consistent progress due to the development of computer vision
and natural language generation techniques. Though conventional captioning
models achieve high accuracy based on popular metrics, i.e., BLEU, CIDEr, and
SPICE, the ability of captions to distinguish the target image from other
similar images is under-explored. To generate distinctive captions, a few
pioneers employ contrastive learning or re-weighted the ground-truth captions,
which focuses on one single input image. However, the relationships between
objects in a similar image group (e.g., items or properties within the same
album or fine-grained events) are neglected. In this paper, we improve the
distinctiveness of image captions using a Group-based Distinctive Captioning
Model (GdisCap), which compares each image with other images in one similar
group and highlights the uniqueness of each image. In particular, we propose a
group-based memory attention (GMA) module, which stores object features that
are unique among the image group (i.e., with low similarity to objects in other
images). These unique object features are highlighted when generating captions,
resulting in more distinctive captions. Furthermore, the distinctive words in
the ground-truth captions are selected to supervise the language decoder and
GMA. Finally, we propose a new evaluation metric, distinctive word rate
(DisWordRate) to measure the distinctiveness of captions. Quantitative results
indicate that the proposed method significantly improves the distinctiveness of
several baseline models, and achieves the state-of-the-art performance on both
accuracy and distinctiveness. Results of a user study agree with the
quantitative evaluation and demonstrate the rationality of the new metric
DisWordRate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking the Misalignment Problem in Dense Object Detection. (arXiv:2108.12176v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12176">
<div class="article-summary-box-inner">
<span><p>Object detection aims to localize and classify the objects in a given image,
and these two tasks are sensitive to different object regions. Therefore, some
locations predict high-quality bounding boxes but low classification scores,
and some locations are quite the opposite. A misalignment exists between the
two tasks, and their features are spatially entangled. In order to solve the
misalignment problem, we propose a plug-in Spatial-disentangled and
Task-aligned operator (SALT). By predicting two task-aware point sets that are
located in each task's sensitive regions, SALT can reassign features from those
regions and align them to the corresponding anchor point. Therefore, features
for the two tasks are spatially aligned and disentangled. To minimize the
difference between the two regression stages, we propose a Self-distillation
regression (SDR) loss that can transfer knowledge from the refined regression
results to the coarse regression results. On the basis of SALT and SDR loss, we
propose SALT-Net, which explicitly exploits task-aligned point-set features for
accurate detection results. Extensive experiments on the MS-COCO dataset show
that our proposed methods can consistently boost different state-of-the-art
dense detectors by $\sim$2 AP. Notably, SALT-Net with Res2Net-101-DCN backbone
achieves 53.8 AP on the MS-COCO test-dev.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CardiSort: a convolutional neural network for cross vendor automated sorting of cardiac MR images. (arXiv:2109.08479v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08479">
<div class="article-summary-box-inner">
<span><p>Objectives: To develop an image-based automatic deep learning method to
classify cardiac MR images by sequence type and imaging plane for improved
clinical post-processing efficiency. Methods: Multi-vendor cardiac MRI studies
were retrospectively collected from 4 centres and 3 vendors. A two-head
convolutional neural network ('CardiSort') was trained to classify 35 sequences
by imaging sequence (n=17) and plane (n=10). Single vendor training (SVT) on
single centre images (n=234 patients) and multi-vendor training (MVT) with
multicentre images (n = 479 patients, 3 centres) was performed. Model accuracy
was compared to manual ground truth labels by an expert radiologist on a
hold-out test set for both SVT and MVT. External validation of MVT
(MVTexternal) was performed on data from 3 previously unseen magnet systems
from 2 vendors (n=80 patients). Results: High sequence and plane accuracies
were observed for SVT (85.2% and 93.2% respectively), and MVT (96.5% and 98.1%
respectively) on the hold-out test set. MVTexternal yielded sequence accuracy
of 92.7% and plane accuracy of 93.0%. There was high accuracy for common
sequences and conventional cardiac planes. Poor accuracy was observed for
underrepresented classes and sequences where there was greater variability in
acquisition parameters across centres, such as perfusion imaging. Conclusions:
A deep learning network was developed on multivendor data to classify MRI
studies into component sequences and planes, with external validation. With
refinement, it has potential to improve workflow by enabling automated sequence
selection, an important first step in completely automated post-processing
pipelines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Consistent Explanations by Contrastive Learning. (arXiv:2110.00527v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00527">
<div class="article-summary-box-inner">
<span><p>Post-hoc explanation methods, e.g., Grad-CAM, enable humans to inspect the
spatial regions responsible for a particular network decision. However, it is
shown that such explanations are not always consistent with human priors, such
as consistency across image transformations. Given an interpretation algorithm,
e.g., Grad-CAM, we introduce a novel training method to train the model to
produce more consistent explanations. Since obtaining the ground truth for a
desired model interpretation is not a well-defined task, we adopt ideas from
contrastive self-supervised learning, and apply them to the interpretations of
the model rather than its embeddings. We show that our method, Contrastive
Grad-CAM Consistency (CGC), results in Grad-CAM interpretation heatmaps that
are more consistent with human annotations while still achieving comparable
classification accuracy. Moreover, our method acts as a regularizer and
improves the accuracy on limited-data, fine-grained classification settings. In
addition, because our method does not rely on annotations, it allows for the
incorporation of unlabeled data into training, which enables better
generalization of the model. Our code is available here:
https://github.com/UCDvision/CGC
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image prediction of disease progression by style-based manifold extrapolation. (arXiv:2111.11439v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11439">
<div class="article-summary-box-inner">
<span><p>Disease-modifying management aims to prevent deterioration and progression of
the disease, not just relieve symptoms. Unfortunately, the development of
necessary therapies is often hampered by the failure to recognize the
presymptomatic disease and limited understanding of disease development. We
present a generic solution for this problem by a methodology that allows the
prediction of progression risk and morphology in individuals using a latent
extrapolation optimization approach. To this end, we combined a regularized
generative adversarial network (GAN) and a latent nearest neighbor algorithm
for joint optimization to generate plausible images of future time points. We
evaluated our method on osteoarthritis (OA) data from a multi-center
longitudinal study (the Osteoarthritis Initiative, OAI). With presymptomatic
baseline data, our model is generative and significantly outperforms the
end-to-end learning model in discriminating the progressive cohort. Two
experiments were performed with seven experienced radiologists. When no
synthetic follow-up radiographs were provided, our model performed better than
all seven radiologists. In cases where the synthetic follow-ups generated by
our model were available, the specificity and sensitivity of all readers in
discriminating progressors increased from $72.3\%$ to $88.6\%$ and from
$42.1\%$ to $51.6\%$, respectively. Our results open up a new possibility of
using model-based morphology and risk prediction to make predictions about
future disease occurrence, as demonstrated in the example of OA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A War Beyond Deepfake: Benchmarking Facial Counterfeits and Countermeasures. (arXiv:2111.12912v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12912">
<div class="article-summary-box-inner">
<span><p>In recent years, visual forgery has reached a level of sophistication that
humans cannot identify fraud, which poses a significant threat to information
security. A wide range of malicious applications have emerged, such as fake
news, defamation or blackmailing of celebrities, impersonation of politicians
in political warfare, and the spreading of rumours to attract views. As a
result, a rich body of visual forensic techniques has been proposed in an
attempt to stop this dangerous trend. In this paper, we present a benchmark
that provides in-depth insights into visual forgery and visual forensics, using
a comprehensive and empirical approach. More specifically, we develop an
independent framework that integrates state-of-the-arts counterfeit generators
and detectors, and measure the performance of these techniques using various
criteria. We also perform an exhaustive analysis of the benchmarking results,
to determine the characteristics of the methods that serve as a comparative
reference in this never-ending war between measures and countermeasures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPIN: Simplifying Polar Invariance for Neural networks Application to vision-based irradiance forecasting. (arXiv:2111.14507v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14507">
<div class="article-summary-box-inner">
<span><p>Translational invariance induced by pooling operations is an inherent
property of convolutional neural networks, which facilitates numerous computer
vision tasks such as classification. Yet to leverage rotational invariant
tasks, convolutional architectures require specific rotational invariant layers
or extensive data augmentation to learn from diverse rotated versions of a
given spatial configuration. Unwrapping the image into its polar coordinates
provides a more explicit representation to train a convolutional architecture
as the rotational invariance becomes translational, hence the visually distinct
but otherwise equivalent rotated versions of a given scene can be learnt from a
single image. We show with two common vision-based solar irradiance forecasting
challenges (i.e. using ground-taken sky images or satellite images), that this
preprocessing step significantly improves prediction results by standardising
the scene representation, while decreasing training time by a factor of 4
compared to augmenting data with rotations. In addition, this transformation
magnifies the area surrounding the centre of the rotation, leading to more
accurate short-term irradiance predictions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Partial-to-Partial Point Cloud Registration in a Full Range. (arXiv:2111.15606v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.15606">
<div class="article-summary-box-inner">
<span><p>Point cloud registration for 3D objects is a challenging task due to sparse
and noisy measurements, incomplete observations and large transformations. In
this work, we propose \textbf{G}raph \textbf{M}atching \textbf{C}onsensus
\textbf{Net}work (\textbf{GMCNet}), which estimates pose-invariant
correspondences for full-range Partial-to-Partial point cloud Registration
(PPR) in the object-level registration scenario. To encode robust point
descriptors, \textbf{1)} we first comprehensively investigate
transformation-robustness and noise-resilience of various geometric features.
\textbf{2)} Then, we employ a novel {T}ransformation-robust {P}oint
{T}ransformer (\textbf{TPT}) module to adaptively aggregate local features
regarding the structural relations, which takes advantage from both handcrafted
rotation-invariant ({\textit{RI}}) features and noise-resilient spatial
coordinates. \textbf{3)} Based on a synergy of hierarchical graph networks and
graphical modeling, we propose the {H}ierarchical {G}raphical {M}odeling
(\textbf{HGM}) architecture to encode robust descriptors consisting of i) a
unary term learned from {\textit{RI}} features; and ii) multiple smoothness
terms encoded from neighboring point relations at different scales through our
TPT modules. Moreover, we construct a challenging PPR dataset (\textbf{MVP-RG})
based on the recent MVP dataset that features high-quality scans. Extensive
experiments show that GMCNet outperforms previous state-of-the-art methods for
PPR. Notably, GMCNet encodes point descriptors for each point cloud
individually without using cross-contextual information, or ground truth
correspondences for training. Our code and datasets are available at:
https://github.com/paul007pl/GMCNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Event-Based Fusion for Motion Deblurring with Cross-modal Attention. (arXiv:2112.00167v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00167">
<div class="article-summary-box-inner">
<span><p>Traditional frame-based cameras inevitably suffer from motion blur due to
long exposure times. As a kind of bio-inspired camera, the event camera records
the intensity changes in an asynchronous way with high temporal resolution,
providing valid image degradation information within the exposure time. In this
paper, we rethink the eventbased image deblurring problem and unfold it into an
end-to-end two-stage image restoration network. To effectively fuse event and
image features, we design an event-image cross-modal attention module applied
at multiple levels of our network, which allows to focus on relevant features
from the event branch and filter out noise. We also introduce a novel symmetric
cumulative event representation specifically for image deblurring as well as an
event mask gated connection between the two stages of our network which helps
avoid information loss. At the dataset level, to foster event-based motion
deblurring and to facilitate evaluation on challenging real-world images, we
introduce the Real Event Blur (REBlur) dataset, captured with an event camera
in an illumination controlled optical laboratory. Our Event Fusion Network
(EFNet) sets the new state of the art in motion deblurring, surpassing both the
prior best-performing image-based method and all event-based methods with
public implementations on the GoPro dataset (by up to 2.47dB) and on our REBlur
dataset, even in extreme blurry conditions. The code and our REBlur dataset
will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Federated Learning with Adaptive Batchnorm for Personalized Healthcare. (arXiv:2112.00734v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00734">
<div class="article-summary-box-inner">
<span><p>There is a growing interest in applying machine learning techniques for
healthcare. Recently, federated machine learning (FL) is gaining popularity
since it allows researchers to train powerful models without compromising data
privacy and security. However, the performance of existing FL approaches often
deteriorates when encountering non-iid situations where there exist
distribution gaps among clients, and few previous efforts focus on
personalization in healthcare. In this article, we propose AdaFed to tackle
domain shifts and obtain personalized models for local clients. AdaFed learns
the similarity between clients via the statistics of the batch normalization
layers while preserving the specificity of each client with different local
batch normalization. Comprehensive experiments on five healthcare benchmarks
demonstrate that AdaFed achieves better accuracy compared to state-of-the-art
methods (e.g., \textbf{10}\%+ accuracy improvement for PAMAP2) with faster
convergence speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Putting 3D Spatially Sparse Networks on a Diet. (arXiv:2112.01316v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01316">
<div class="article-summary-box-inner">
<span><p>3D neural networks have become prevalent for many 3D vision tasks including
object detection, segmentation, registration, and various perception tasks for
3D inputs. However, due to the sparsity and irregularity of 3D data, custom 3D
operators or network designs have been the primary focus of research, while the
size of networks or efficacy of parameters has been overlooked. In this work,
we perform the first comprehensive study on the weight sparsity of spatially
sparse 3D convolutional networks and propose a compact weight-sparse and
spatially sparse 3D convnet (WS^3-Convnet) for semantic and instance
segmentation on the real-world indoor and outdoor datasets. We employ various
network pruning strategies to find compact networks and show our WS^3-Convnet
achieves minimal loss in performance (2.15\% drop) with orders-of-magnitude
smaller number of parameters (99\% compression rate) and computational cost
(95\% reduction). Finally, we systematically analyze the compression patterns
of WS^3-Convnet and show interesting emerging sparsity patterns common in our
compressed networks to further speed up inference (45\% faster).
\keywords{Efficient network architecture, Network pruning, 3D scene
segmentation, Spatially sparse convolution}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GCA-Net : Utilizing Gated Context Attention for Improving Image Forgery Localization and Detection. (arXiv:2112.04298v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.04298">
<div class="article-summary-box-inner">
<span><p>Forensic analysis of manipulated pixels requires the identification of
various hidden and subtle features from images. Conventional image recognition
models generally fail at this task because they are biased and more attentive
toward the dominant local and spatial features. In this paper, we propose a
novel Gated Context Attention Network (GCA-Net) that utilizes non-local
attention in conjunction with a gating mechanism in order to capture the finer
image discrepancies and better identify forged regions. The proposed framework
uses high dimensional embeddings to filter and aggregate the relevant context
from coarse feature maps at various stages of the decoding process. This
improves the network's understanding of global differences and reduces
false-positive localizations. Our evaluation on standard image forensic
benchmarks shows that GCA-Net can both compete against and improve over
state-of-the-art networks by an average of 4.7% AUC. Additional ablation
studies also demonstrate the method's robustness against attributions and
resilience to false-positive predictions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human Hands as Probes for Interactive Object Understanding. (arXiv:2112.09120v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09120">
<div class="article-summary-box-inner">
<span><p>Interactive object understanding, or what we can do to objects and how is a
long-standing goal of computer vision. In this paper, we tackle this problem
through observation of human hands in in-the-wild egocentric videos. We
demonstrate that observation of what human hands interact with and how can
provide both the relevant data and the necessary supervision. Attending to
hands, readily localizes and stabilizes active objects for learning and reveals
places where interactions with objects occur. Analyzing the hands shows what we
can do to objects and how. We apply these basic principles on the EPIC-KITCHENS
dataset, and successfully learn state-sensitive features, and object
affordances (regions of interaction and afforded grasps), purely by observing
hands in egocentric videos.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An effective coaxiality measurement for twist drill based on line structured light sensor. (arXiv:2112.09873v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09873">
<div class="article-summary-box-inner">
<span><p>Aiming at the accurate and effective coaxiality measurement for twist drill
with irregular surface, an optical measurement mechanism is proposed in this
paper. First, A high-precision rotation instrument based on four core units is
designed, which can obtain the 3-D point cloud data of full angle for the twist
drill. Second, in the data processing stage, an improved robust Gaussian
mixture model is established for accurate and rapid blade back segmentation. To
improve measurement efficiency, a rapid reconstruction method of the twist
drill axis based on orthogonal synthesis is provided to locate the axial
position of the maximum deviation from the benchmark by utilizing the extracted
blade back data. Finally, by calculating the maximum radial Euclidean distance
from the benchmark, the coaxiality error of the twist drill is obtained.
Comparing with other measurement methods, experimental results show that our
proposed method is effective with high precision of 3 um and high efficiency of
less than 3 s/pc. The result demonstrate that the proposed method is effective,
robust and automatic, it can be applied in many actual industrial scene.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An analysis of over-sampling labeled data in semi-supervised learning with FixMatch. (arXiv:2201.00604v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00604">
<div class="article-summary-box-inner">
<span><p>Most semi-supervised learning methods over-sample labeled data when
constructing training mini-batches. This paper studies whether this common
practice improves learning and how. We compare it to an alternative setting
where each mini-batch is uniformly sampled from all the training data, labeled
or not, which greatly reduces direct supervision from true labels in typical
low-label regimes. However, this simpler setting can also be seen as more
general and even necessary in multi-task problems where over-sampling labeled
data would become intractable. Our experiments on semi-supervised CIFAR-10
image classification using FixMatch show a performance drop when using the
uniform sampling approach which diminishes when the amount of labeled data or
the training time increases. Further, we analyse the training dynamics to
understand how over-sampling of labeled data compares to uniform sampling. Our
main finding is that over-sampling is especially beneficial early in training
but gets less important in the later stages when more pseudo-labels become
correct. Nevertheless, we also find that keeping some true labels remains
important to avoid the accumulation of confirmation errors from incorrect
pseudo-labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalised Image Outpainting with U-Transformer. (arXiv:2201.11403v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11403">
<div class="article-summary-box-inner">
<span><p>While most present image outpainting conducts horizontal extrapolation, we
study the generalised image outpainting problem that extrapolates visual
context all-side around a given image. To this end, we develop a novel
transformer-based generative adversarial network called U-Transformer able to
extend image borders with plausible structure and details even for complicated
scenery images. Specifically, we design a generator as an encoder-to-decoder
structure embedded with the popular Swin Transformer blocks. As such, our novel
framework can better cope with image long-range dependencies which are
crucially important for generalised image outpainting. We propose additionally
a U-shaped structure and multi-view Temporal Spatial Predictor network to
reinforce image self-reconstruction as well as unknown-part prediction smoothly
and realistically. We experimentally demonstrate that our proposed method could
produce visually appealing results for generalized image outpainting against
the state-of-the-art image outpainting approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Should I take a walk? Estimating Energy Expenditure from Video Data. (arXiv:2202.00712v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.00712">
<div class="article-summary-box-inner">
<span><p>We explore the problem of automatically inferring the amount of kilocalories
used by human during physical activity from his/her video observation. To study
this underresearched task, we introduce Vid2Burn -- an omni-source benchmark
for estimating caloric expenditure from video data featuring both, high- and
low-intensity activities for which we derive energy expenditure annotations
based on models established in medical literature. In practice, a training set
would only cover a certain amount of activity types, and it is important to
validate, if the model indeed captures the essence of energy expenditure,
(e.g., how many and which muscles are involved and how intense they work)
instead of memorizing fixed values of specific activity categories seen during
training. Ideally, the models should look beyond such category-specific biases
and regress the caloric cost in videos depicting activity categories not
explicitly present during training. With this property in mind, Vid2Burn is
accompanied with a cross-category benchmark, where the task is to regress
caloric expenditure for types of physical activities not present during
training. An extensive evaluation of state-of-the-art approaches for video
recognition modified for the energy expenditure estimation task demonstrates
the difficulty of this problem, especially for new activity types at test-time,
marking a new research direction. Dataset and code are available at
https://github.com/KPeng9510/Vid2Burn.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MedNeRF: Medical Neural Radiance Fields for Reconstructing 3D-aware CT-Projections from a Single X-ray. (arXiv:2202.01020v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01020">
<div class="article-summary-box-inner">
<span><p>Computed tomography (CT) is an effective medical imaging modality, widely
used in the field of clinical medicine for the diagnosis of various
pathologies. Advances in Multidetector CT imaging technology have enabled
additional functionalities, including generation of thin slice multiplanar
cross-sectional body imaging and 3D reconstructions. However, this involves
patients being exposed to a considerable dose of ionising radiation. Excessive
ionising radiation can lead to deterministic and harmful effects on the body.
This paper proposes a Deep Learning model that learns to reconstruct CT
projections from a few or even a single-view X-ray. This is based on a novel
architecture that builds from neural radiance fields, which learns a continuous
representation of CT scans by disentangling the shape and volumetric depth of
surface and internal anatomical structures from 2D images. Our model is trained
on chest and knee datasets, and we demonstrate qualitative and quantitative
high-fidelity renderings and compare our approach to other recent radiance
field-based methods. Our code and link to our datasets are available at
https://github.com/abrilcf/mednerf
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cyber Mobility Mirror: A Deep Learning-based Real-World Object Perception Platform Using Roadside LiDAR. (arXiv:2202.13505v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13505">
<div class="article-summary-box-inner">
<span><p>Object perception plays a fundamental role in Cooperative Driving Automation
(CDA) which is regarded as a revolutionary promoter for the next-generation
transportation systems. However, the vehicle-based perception may suffer from
the limited sensing range and occlusion as well as low penetration rates in
connectivity. In this paper, we propose Cyber Mobility Mirror (CMM), a
next-generation real-time traffic surveillance system for 3D object perception
and reconstruction, to explore the potential of roadside sensors for enabling
CDA in the real world. The CMM system consists of six main components: 1) the
data pre-processor to retrieve and preprocess the raw data; 2) the roadside 3D
object detector to generate 3D detection results; 3) the multi-object tracker
to identify detected objects; 4) the global locator to map positioning
information from the LiDAR coordinate to geographic coordinate using coordinate
transformation; 5) the cloud-based communicator to transmit perception
information from roadside sensors to equipped vehicles, and 6) the onboard
advisor to reconstruct and display the real-time traffic conditions via
Graphical User Interface (GUI). In this study, a field-operational system is
deployed at a real-world intersection, University Avenue and Iowa Avenue in
Riverside, California to assess the feasibility and performance of our CMM
system. Results from field tests demonstrate that our CMM prototype system can
provide satisfactory perception performance with 96.99% precision and 83.62%
recall. High-fidelity real-time traffic conditions (at the object level) can be
geo-localized with an average error of 0.14m and displayed on the GUI of the
equipped vehicle with a frequency of 3-4 Hz.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Point Cloud Based Place Recognition with Ranking-based Loss and Large Batch Training. (arXiv:2203.00972v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00972">
<div class="article-summary-box-inner">
<span><p>The paper presents a simple and effective learning-based method for computing
a discriminative 3D point cloud descriptor for place recognition purposes.
Recent state-of-the-art methods have relatively complex architectures such as
multi-scale oyramid of point Transformers combined with a pyramid of feature
aggregation modules. Our method uses a simple and efficient 3D convolutional
feature extraction, based on a sparse voxelized representation, enhanced with
channel attention blocks. We employ recent advances in image retrieval and
propose a modified version of a loss function based on a differentiable average
precision approximation. Such loss function requires training with very large
batches for the best results. This is enabled by using multistaged
backpropagation. Experimental evaluation on the popular benchmarks proves the
effectiveness of our approach, with a consistent improvement over the state of
the art
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OVE6D: Object Viewpoint Encoding for Depth-based 6D Object Pose Estimation. (arXiv:2203.01072v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01072">
<div class="article-summary-box-inner">
<span><p>This paper proposes a universal framework, called OVE6D, for model-based 6D
object pose estimation from a single depth image and a target object mask. Our
model is trained using purely synthetic data rendered from ShapeNet, and,
unlike most of the existing methods, it generalizes well on new real-world
objects without any fine-tuning. We achieve this by decomposing the 6D pose
into viewpoint, in-plane rotation around the camera optical axis and
translation, and introducing novel lightweight modules for estimating each
component in a cascaded manner. The resulting network contains less than 4M
parameters while demonstrating excellent performance on the challenging T-LESS
and Occluded LINEMOD datasets without any dataset-specific training. We show
that OVE6D outperforms some contemporary deep learning-based pose estimation
methods specifically trained for individual objects or datasets with real-world
training data.
</p>
<p>The implementation and the pre-trained model will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HOI4D: A 4D Egocentric Dataset for Category-Level Human-Object Interaction. (arXiv:2203.01577v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01577">
<div class="article-summary-box-inner">
<span><p>We present HOI4D, a large-scale 4D egocentric dataset with rich annotations,
to catalyze the research of category-level human-object interaction. HOI4D
consists of 2.4M RGB-D egocentric video frames over 4000 sequences collected by
4 participants interacting with 800 different object instances from 16
categories over 610 different indoor rooms. Frame-wise annotations for panoptic
segmentation, motion segmentation, 3D hand pose, category-level object pose and
hand action have also been provided, together with reconstructed object meshes
and scene point clouds. With HOI4D, we establish three benchmarking tasks to
promote category-level HOI from 4D visual signals including semantic
segmentation of 4D dynamic point cloud sequences, category-level object pose
tracking, and egocentric action segmentation with diverse interaction targets.
In-depth analysis shows HOI4D poses great challenges to existing methods and
produces great research opportunities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PINA: Learning a Personalized Implicit Neural Avatar from a Single RGB-D Video Sequence. (arXiv:2203.01754v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01754">
<div class="article-summary-box-inner">
<span><p>We present a novel method to learn Personalized Implicit Neural Avatars
(PINA) from a short RGB-D sequence. This allows non-expert users to create a
detailed and personalized virtual copy of themselves, which can be animated
with realistic clothing deformations. PINA does not require complete scans, nor
does it require a prior learned from large datasets of clothed humans. Learning
a complete avatar in this setting is challenging, since only few depth
observations are available, which are noisy and incomplete (i.e. only partial
visibility of the body per frame). We propose a method to learn the shape and
non-rigid deformations via a pose-conditioned implicit surface and a
deformation field, defined in canonical space. This allows us to fuse all
partial observations into a single consistent canonical representation. Fusion
is formulated as a global optimization problem over the pose, shape and
skinning parameters. The method can learn neural avatars from real noisy RGB-D
sequences for a diverse set of people and clothing styles and these avatars can
be animated given unseen motion sequences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Voice-Face Homogeneity Tells Deepfake. (arXiv:2203.02195v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02195">
<div class="article-summary-box-inner">
<span><p>Detecting forgery videos is highly desired due to the abuse of deepfake.
Existing detection approaches contribute to exploring the specific artifacts in
deepfake videos and fit well on certain data. However, the growing technique on
these artifacts keeps challenging the robustness of traditional deepfake
detectors. As a result, the development of generalizability of these approaches
has reached a blockage. To address this issue, given the empirical results that
the identities behind voices and faces are often mismatched in deepfake videos,
and the voices and faces have homogeneity to some extent, in this paper, we
propose to perform the deepfake detection from an unexplored voice-face
matching view. To this end, a voice-face matching detection model is devised to
measure the matching degree of these two on a generic audio-visual dataset.
Thereafter, this model can be smoothly transferred to deepfake datasets without
any fine-tuning, and the generalization across datasets is accordingly
enhanced. We conduct extensive experiments over two widely exploited datasets -
DFDC and FakeAVCeleb. Our model obtains significantly improved performance as
compared to other state-of-the-art competitors and maintains favorable
generalizability. The code has been released at
https://github.com/xaCheng1996/VFD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HSC4D: Human-centered 4D Scene Capture in Large-scale Indoor-outdoor Space Using Wearable IMUs and LiDAR. (arXiv:2203.09215v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09215">
<div class="article-summary-box-inner">
<span><p>We propose Human-centered 4D Scene Capture (HSC4D) to accurately and
efficiently create a dynamic digital world, containing large-scale
indoor-outdoor scenes, diverse human motions, and rich interactions between
humans and environments. Using only body-mounted IMUs and LiDAR, HSC4D is
space-free without any external devices' constraints and map-free without
pre-built maps. Considering that IMUs can capture human poses but always drift
for long-period use, while LiDAR is stable for global localization but rough
for local positions and orientations, HSC4D makes both sensors complement each
other by a joint optimization and achieves promising results for long-term
capture. Relationships between humans and environments are also explored to
make their interaction more realistic. To facilitate many down-stream tasks,
like AR, VR, robots, autonomous driving, etc., we propose a dataset containing
three large scenes (1k-5k $m^2$) with accurate dynamic human motions and
locations. Diverse scenarios (climbing gym, multi-story building, slope, etc.)
and challenging human activities (exercising, walking up/down stairs, climbing,
etc.) demonstrate the effectiveness and the generalization ability of HSC4D.
The dataset and code are available at <a href="http://www.lidarhumanmotion.net/hsc4d/.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Fixation: Dynamic Window Visual Transformer. (arXiv:2203.12856v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12856">
<div class="article-summary-box-inner">
<span><p>Recently, a surge of interest in visual transformers is to reduce the
computational cost by limiting the calculation of self-attention to a local
window. Most current work uses a fixed single-scale window for modeling by
default, ignoring the impact of window size on model performance. However, this
may limit the modeling potential of these window-based models for multi-scale
information. In this paper, we propose a novel method, named Dynamic Window
Vision Transformer (DW-ViT). The dynamic window strategy proposed by DW-ViT
goes beyond the model that employs a fixed single window setting. To the best
of our knowledge, we are the first to use dynamic multi-scale windows to
explore the upper limit of the effect of window settings on model performance.
In DW-ViT, multi-scale information is obtained by assigning windows of
different sizes to different head groups of window multi-head self-attention.
Then, the information is dynamically fused by assigning different weights to
the multi-scale window branches. We conducted a detailed performance evaluation
on three datasets, ImageNet-1K, ADE20K, and COCO. Compared with related
state-of-the-art (SoTA) methods, DW-ViT obtains the best performance.
Specifically, compared with the current SoTA Swin Transformers
\cite{liu2021swin}, DW-ViT has achieved consistent and substantial improvements
on all three datasets with similar parameters and computational costs. In
addition, DW-ViT exhibits good scalability and can be easily inserted into any
window-based visual transformers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Facial Expression Classification using Fusion of Deep Neural Network in Video for the 3rd ABAW3 Competition. (arXiv:2203.12899v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12899">
<div class="article-summary-box-inner">
<span><p>For computers to recognize human emotions, expression classification is an
equally important problem in the human-computer interaction area. In the 3rd
Affective Behavior Analysis In-The-Wild competition, the task of expression
classification includes eight classes with six basic expressions of human faces
from videos. In this paper, we employ a transformer mechanism to encode the
robust representation from the backbone. Fusion of the robust representations
plays an important role in the expression classification task. Our approach
achieves 30.35\% and 28.60\% for the $F_1$ score on the validation set and the
test set, respectively. This result shows the effectiveness of the proposed
architecture based on the Aff-Wild2 dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MGRR-Net: Multi-level Graph Relational Reasoning Network for Facial Action Units Detection. (arXiv:2204.01349v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01349">
<div class="article-summary-box-inner">
<span><p>The Facial Action Coding System (FACS) encodes the action units (AUs) in
facial images, which has attracted extensive research attention due to its wide
use in facial expression analysis. Many methods that perform well on automatic
facial action unit (AU) detection primarily focus on modeling various types of
AU relations between corresponding local muscle areas, or simply mining global
attention-aware facial features, however, neglect the dynamic interactions
among local-global features. We argue that encoding AU features just from one
perspective may not capture the rich contextual information between regional
and global face features, as well as the detailed variability across AUs,
because of the diversity in expression and individual characteristics. In this
paper, we propose a novel Multi-level Graph Relational Reasoning Network
(termed MGRR-Net) for facial AU detection. Each layer of MGRR-Net performs a
multi-level (i.e., region-level, pixel-wise and channel-wise level) feature
learning. While the region-level feature learning from local face patches
features via graph neural network can encode the correlation across different
AUs, the pixel-wise and channel-wise feature learning via graph attention
network can enhance the discrimination ability of AU features from global face
features. The fused features from the three levels lead to improved AU
discriminative ability. Extensive experiments on DISFA and BP4D AU datasets
show that the proposed approach achieves superior performance than the
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DAD: Data-free Adversarial Defense at Test Time. (arXiv:2204.01568v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01568">
<div class="article-summary-box-inner">
<span><p>Deep models are highly susceptible to adversarial attacks. Such attacks are
carefully crafted imperceptible noises that can fool the network and can cause
severe consequences when deployed. To encounter them, the model requires
training data for adversarial training or explicit regularization-based
techniques. However, privacy has become an important concern, restricting
access to only trained models but not the training data (e.g. biometric data).
Also, data curation is expensive and companies may have proprietary rights over
it. To handle such situations, we propose a completely novel problem of
'test-time adversarial defense in absence of training data and even their
statistics'. We solve it in two stages: a) detection and b) correction of
adversarial samples. Our adversarial sample detection framework is initially
trained on arbitrary data and is subsequently adapted to the unlabelled test
data through unsupervised domain adaptation. We further correct the predictions
on detected adversarial samples by transforming them in Fourier domain and
obtaining their low frequency component at our proposed suitable radius for
model prediction. We demonstrate the efficacy of our proposed technique via
extensive experiments against several adversarial attacks and for different
model architectures and datasets. For a non-robust Resnet-18 model pre-trained
on CIFAR-10, our detection method correctly identifies 91.42% adversaries.
Also, we significantly improve the adversarial accuracy from 0% to 37.37% with
a minimal drop of 0.02% in clean accuracy on state-of-the-art 'Auto Attack'
without having to retrain the model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Audio-visual multi-channel speech separation, dereverberation and recognition. (arXiv:2204.01977v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.01977">
<div class="article-summary-box-inner">
<span><p>Despite the rapid advance of automatic speech recognition (ASR) technologies,
accurate recognition of cocktail party speech characterised by the interference
from overlapping speakers, background noise and room reverberation remains a
highly challenging task to date. Motivated by the invariance of visual modality
to acoustic signal corruption, audio-visual speech enhancement techniques have
been developed, although predominantly targeting overlapping speech separation
and recognition tasks. In this paper, an audio-visual multi-channel speech
separation, dereverberation and recognition approach featuring a full
incorporation of visual information into all three stages of the system is
proposed. The advantage of the additional visual modality over using audio only
is demonstrated on two neural dereverberation approaches based on DNN-WPE and
spectral mapping respectively. The learning cost function mismatch between the
separation and dereverberation models and their integration with the back-end
recognition system is minimised using fine-tuning on the MSE and LF-MMI
criteria. Experiments conducted on the LRS2 dataset suggest that the proposed
audio-visual multi-channel speech separation, dereverberation and recognition
system outperforms the baseline audio-visual multi-channel speech separation
and recognition system containing no dereverberation module by a statistically
significant word error rate (WER) reduction of 2.06% absolute (8.77% relative).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-Grained Predicates Learning for Scene Graph Generation. (arXiv:2204.02597v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.02597">
<div class="article-summary-box-inner">
<span><p>The performance of current Scene Graph Generation models is severely hampered
by some hard-to-distinguish predicates, e.g., "woman-on/standing on/walking
on-beach" or "woman-near/looking at/in front of-child". While general SGG
models are prone to predict head predicates and existing re-balancing
strategies prefer tail categories, none of them can appropriately handle these
hard-to-distinguish predicates. To tackle this issue, inspired by fine-grained
image classification, which focuses on differentiating among
hard-to-distinguish object classes, we propose a method named Fine-Grained
Predicates Learning (FGPL) which aims at differentiating among
hard-to-distinguish predicates for Scene Graph Generation task. Specifically,
we first introduce a Predicate Lattice that helps SGG models to figure out
fine-grained predicate pairs. Then, utilizing the Predicate Lattice, we propose
a Category Discriminating Loss and an Entity Discriminating Loss, which both
contribute to distinguishing fine-grained predicates while maintaining learned
discriminatory power over recognizable ones. The proposed model-agnostic
strategy significantly boosts the performances of three benchmark models
(Transformer, VCTree, and Motif) by 22.8\%, 24.1\% and 21.7\% of Mean Recall
(mR@100) on the Predicate Classification sub-task, respectively. Our model also
outperforms state-of-the-art methods by a large margin (i.e., 6.1\%, 4.6\%, and
3.2\% of Mean Recall (mR@100)) on the Visual Genome dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MDA GAN: Adversarial-Learning-based 3-D Seismic Data Interpolation and Reconstruction for Complex Missing. (arXiv:2204.03197v2 [physics.geo-ph] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03197">
<div class="article-summary-box-inner">
<span><p>The interpolation and reconstruction of missing traces is a crucial step in
seismic data processing, moreover it is also a highly ill-posed problem,
especially for complex cases such as high-ratio random discrete missing,
continuous missing and missing in fault-rich or salt body surveys. These
complex cases are rarely mentioned in current sparse or low-rank priorbased and
deep learning-based approaches. To cope with complex missing cases, we propose
Multi-Dimensional Adversarial GAN (MDA GAN), a novel 3-D GAN framework. It
employs three discriminators to ensure the consistency of the reconstructed
data with the original data distribution in each dimension. The feature
splicing module (FSM) is designed and embedded into the generator of this
framework, which automatically splices the features of the unmissing part with
those of the reconstructed part (missing part), thus fully preserving the
information of the unmissing part. To prevent pixel distortion in the seismic
data caused by the adversarial learning process, we propose a new
reconstruction loss Tanh Cross Entropy (TCE) loss to provide smoother
gradients. We experimentally verified the effectiveness of the individual
components of the study and then tested the method on multiple publicly
available data. The method achieves reasonable reconstructions for up to 95% of
random discrete missing, 100 traces of continuous missing and more complex
hybrid missing. In surveys of fault-rich and salt bodies, the method can
achieve promising reconstructions with up to 75% missing in each of the three
directions (98.2% in total).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context-Sensitive Temporal Feature Learning for Gait Recognition. (arXiv:2204.03270v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03270">
<div class="article-summary-box-inner">
<span><p>Although gait recognition has drawn increasing research attention recently,
it remains challenging to learn discriminative temporal representation, since
the silhouette differences are quite subtle in spatial domain. Inspired by the
observation that human can distinguish gaits of different subjects by
adaptively focusing on temporal clips with different time scales, we propose a
context-sensitive temporal feature learning (CSTL) network for gait
recognition. CSTL produces temporal features in three scales, and adaptively
aggregates them according to the contextual information from local and global
perspectives. Specifically, CSTL contains an adaptive temporal aggregation
module that subsequently performs local relation modeling and global relation
modeling to fuse the multi-scale features. Besides, in order to remedy the
spatial feature corruption caused by temporal operations, CSTL incorporates a
salient spatial feature learning (SSFL) module to select groups of
discriminative spatial features. Particularly, we utilize transformers to
implement the global relation modeling and the SSFL module. To the best of our
knowledge, this is the first work that adopts transformer in gait recognition.
Extensive experiments conducted on three datasets demonstrate the
state-of-the-art performance. Concretely, we achieve rank-1 accuracies of
98.7%, 96.2% and 88.7% under normal-walking, bag-carrying and coat-wearing
conditions on CASIA-B, 97.5% on OU-MVLP and 50.6% on GREW.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Habitat-Web: Learning Embodied Object-Search Strategies from Human Demonstrations at Scale. (arXiv:2204.03514v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03514">
<div class="article-summary-box-inner">
<span><p>We present a large-scale study of imitating human demonstrations on tasks
that require a virtual robot to search for objects in new environments -- (1)
ObjectGoal Navigation (e.g. 'find &amp; go to a chair') and (2) Pick&amp;Place (e.g.
'find mug, pick mug, find counter, place mug on counter'). First, we develop a
virtual teleoperation data-collection infrastructure -- connecting Habitat
simulator running in a web browser to Amazon Mechanical Turk, allowing remote
users to teleoperate virtual robots, safely and at scale. We collect 80k
demonstrations for ObjectNav and 12k demonstrations for Pick&amp;Place, which is an
order of magnitude larger than existing human demonstration datasets in
simulation or on real robots.
</p>
<p>Second, we attempt to answer the question -- how does large-scale imitation
learning (IL) (which hasn't been hitherto possible) compare to reinforcement
learning (RL) (which is the status quo)? On ObjectNav, we find that IL (with no
bells or whistles) using 70k human demonstrations outperforms RL using 240k
agent-gathered trajectories. The IL-trained agent demonstrates efficient
object-search behavior -- it peeks into rooms, checks corners for small
objects, turns in place to get a panoramic view -- none of these are exhibited
as prominently by the RL agent, and to induce these behaviors via RL would
require tedious reward engineering. Finally, accuracy vs. training data size
plots show promising scaling behavior, suggesting that simply collecting more
demonstrations is likely to advance the state of art further. On Pick&amp;Place,
the comparison is starker -- IL agents achieve ${\sim}$18% success on episodes
with new object-receptacle locations when trained with 9.5k human
demonstrations, while RL agents fail to get beyond 0%. Overall, our work
provides compelling evidence for investing in large-scale imitation learning.
</p>
<p>Project page: https://ram81.github.io/projects/habitat-web.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comprehensive Review of Sign Language Recognition: Different Types, Modalities, and Datasets. (arXiv:2204.03328v1 [cs.CV] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2204.03328">
<div class="article-summary-box-inner">
<span><p>A machine can understand human activities, and the meaning of signs can help
overcome the communication barriers between the inaudible and ordinary people.
Sign Language Recognition (SLR) is a fascinating research area and a crucial
task concerning computer vision and pattern recognition. Recently, SLR usage
has increased in many applications, but the environment, background image
resolution, modalities, and datasets affect the performance a lot. Many
researchers have been striving to carry out generic real-time SLR models. This
review paper facilitates a comprehensive overview of SLR and discusses the
needs, challenges, and problems associated with SLR. We study related works
about manual and non-manual, various modalities, and datasets. Research
progress and existing state-of-the-art SLR models over the past decade have
been reviewed. Finally, we find the research gap and limitations in this domain
and suggest future directions. This review paper will be helpful for readers
and researchers to get complete guidance about SLR and the progressive design
of the state-of-the-art SLR model
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-04-11 23:09:13.266150708 UTC">2022-04-11 23:09:13 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>