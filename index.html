<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-03-22T01:30:00Z">03-22</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Event Coreference Resolution for Contentious Politics Events. (arXiv:2203.10123v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10123">
<div class="article-summary-box-inner">
<span><p>We propose a dataset for event coreference resolution, which is based on
random samples drawn from multiple sources, languages, and countries. Early
scholarship on event information collection has not quantified the contribution
of event coreference resolution. We prepared and analyzed a representative
multilingual corpus and measured the performance and contribution of the
state-of-the-art event coreference resolution approaches. We found that almost
half of the event mentions in documents co-occur with other event mentions and
this makes it inevitable to obtain erroneous or partial event information. We
showed that event coreference resolution could help improving this situation.
Our contribution sheds light on a challenge that has been overlooked or hard to
study to date. Future event information collection studies can be designed
based on the results we present in this report. The repository for this study
is on https://github.com/emerging-welfare/ECR4-Contentious-Politics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing Factually Grounded Content Transfer with Factual Ablation. (arXiv:2203.10133v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10133">
<div class="article-summary-box-inner">
<span><p>Despite recent success, large neural models often generate factually
incorrect text. Compounding this is the lack of a standard automatic evaluation
for factuality--it cannot be meaningfully improved if it cannot be measured.
Grounded generation promises a path to solving both of these problems: models
draw on a reliable external document (grounding) for factual information,
simplifying the challenge of factuality. Measuring factuality is also
simplified--to factual consistency, testing whether the generation agrees with
the grounding, rather than all facts. Yet, without a standard automatic metric
for factual consistency, factually grounded generation remains an open problem.
</p>
<p>We study this problem for content transfer, in which generations extend a
prompt, using information from factual grounding. Particularly, this domain
allows us to introduce the notion of factual ablation for automatically
measuring factual consistency: this captures the intuition that the model
should be less likely to produce an output given a less relevant grounding
document. In practice, we measure this by presenting a model with two grounding
documents, and the model should prefer to use the more factually relevant one.
We contribute two evaluation sets to measure this. Applying our new evaluation,
we propose multiple novel methods improving over strong baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DuReader_retrieval: A Large-scale Chinese Benchmark for Passage Retrieval from Web Search Engine. (arXiv:2203.10232v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10232">
<div class="article-summary-box-inner">
<span><p>In this paper, we present DuReader_retrieval, a large-scale Chinese dataset
for passage retrieval. DuReader_retrieval contains more than 90K queries and
over 8M unique passages from Baidu search. To ensure the quality of our
benchmark and address the shortcomings in other existing datasets, we (1)
reduce the false negatives in development and testing sets by pooling the
results from multiple retrievers with human annotations, (2) and remove the
semantically similar questions between training with development and testing
sets. We further introduce two extra out-of-domain testing sets for
benchmarking the domain generalization capability. Our experiment results
demonstrate that DuReader_retrieval is challenging and there is still plenty of
room for the community to improve, e.g. the generalization across domains,
salient phrase and syntax mismatch between query and paragraph and robustness.
DuReader_retrieval will be publicly available at
https://github.com/baidu/DuReader/tree/master/DuReader-Retrieval
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning. (arXiv:2203.10244v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10244">
<div class="article-summary-box-inner">
<span><p>Charts are very popular for analyzing data. When exploring charts, people
often ask a variety of complex reasoning questions that involve several logical
and arithmetic operations. They also commonly refer to visual features of a
chart in their questions. However, most existing datasets do not focus on such
complex reasoning questions as their questions are template-based and answers
come from a fixed-vocabulary. In this work, we present a large-scale benchmark
covering 9.6K human-written questions as well as 23.1K questions generated from
human-written chart summaries. To address the unique challenges in our
benchmark involving visual and logical reasoning over charts, we present two
transformer-based models that combine visual features and the data table of the
chart in a unified way to answer questions. While our models achieve the
state-of-the-art results on the previous datasets as well as on our benchmark,
the evaluation also reveals several challenges in answering complex reasoning
questions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning-by-Narrating: Narrative Pre-Training for Zero-Shot Dialogue Comprehension. (arXiv:2203.10249v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10249">
<div class="article-summary-box-inner">
<span><p>Comprehending a dialogue requires a model to capture diverse kinds of key
information in the utterances, which are either scattered around or implicitly
implied in different turns of conversations. Therefore, dialogue comprehension
requires diverse capabilities such as paraphrasing, summarizing, and
commonsense reasoning. Towards the objective of pre-training a zero-shot
dialogue comprehension model, we develop a novel narrative-guided pre-training
strategy that learns by narrating the key information from a dialogue input.
However, the dialogue-narrative parallel corpus for such a pre-training
strategy is currently unavailable. For this reason, we first construct a
dialogue-narrative parallel corpus by automatically aligning movie subtitles
and their synopses. We then pre-train a BART model on the data and evaluate its
performance on four dialogue-based tasks that require comprehension.
Experimental results show that our model not only achieves superior zero-shot
performance but also exhibits stronger fine-grained dialogue comprehension
capabilities. The data and code are available at
https://github.com/zhaochaocs/Diana
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta-X$_{NLG}$: A Meta-Learning Approach Based on Language Clustering for Zero-Shot Cross-Lingual Transfer and Generation. (arXiv:2203.10250v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10250">
<div class="article-summary-box-inner">
<span><p>Recently, the NLP community has witnessed a rapid advancement in multilingual
and cross-lingual transfer research where the supervision is transferred from
high-resource languages (HRLs) to low-resource languages (LRLs). However, the
cross-lingual transfer is not uniform across languages, particularly in the
zero-shot setting. Towards this goal, one promising research direction is to
learn shareable structures across multiple tasks with limited annotated data.
The downstream multilingual applications may benefit from such a learning setup
as most of the languages across the globe are low-resource and share some
structures with other languages. In this paper, we propose a novel
meta-learning framework (called Meta-X$_{NLG}$) to learn shareable structures
from typologically diverse languages based on meta-learning and language
clustering. This is a step towards uniform cross-lingual transfer for unseen
languages. We first cluster the languages based on language representations and
identify the centroid language of each cluster. Then, a meta-learning algorithm
is trained with all centroid languages and evaluated on the other languages in
the zero-shot setting. We demonstrate the effectiveness of this modeling on two
NLG tasks (Abstractive Text Summarization and Question Generation), 5 popular
datasets and 30 typologically diverse languages. Consistent improvements over
strong baselines demonstrate the efficacy of the proposed framework. The
careful design of the model makes this end-to-end NLG setup less vulnerable to
the accidental translation problem, which is a prominent concern in zero-shot
cross-lingual NLG tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Similarity and Content-based Phonetic Self Attention for Speech Recognition. (arXiv:2203.10252v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10252">
<div class="article-summary-box-inner">
<span><p>Transformer-based speech recognition models have achieved great success due
to the self-attention (SA) mechanism that utilizes every frame in the feature
extraction process. Especially, SA heads in lower layers capture various
phonetic characteristics by the query-key dot product, which is designed to
compute the pairwise relationship between frames. In this paper, we propose a
variant of SA to extract more representative phonetic features. The proposed
phonetic self-attention (phSA) is composed of two different types of phonetic
attention; one is similarity-based and the other is content-based. In short,
similarity-based attention utilizes the correlation between frames while
content-based attention only considers each frame without being affected by
others. We identify which parts of the original dot product are related to two
different attention patterns and improve each part by simple modifications. Our
experiments on phoneme classification and speech recognition show that
replacing SA with phSA for lower layers improves the recognition performance
without increasing the latency and the parameter size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Read Top News First: A Document Reordering Approach for Multi-Document News Summarization. (arXiv:2203.10254v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10254">
<div class="article-summary-box-inner">
<span><p>A common method for extractive multi-document news summarization is to
re-formulate it as a single-document summarization problem by concatenating all
documents as a single meta-document. However, this method neglects the relative
importance of documents. We propose a simple approach to reorder the documents
according to their relative importance before concatenating and summarizing
them. The reordering makes the salient content easier to learn by the
summarization model. Experiments show that our approach outperforms previous
state-of-the-art methods with more complex architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dependency-based Mixture Language Models. (arXiv:2203.10256v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10256">
<div class="article-summary-box-inner">
<span><p>Various models have been proposed to incorporate knowledge of syntactic
structures into neural language models. However, previous works have relied
heavily on elaborate components for a specific language model, usually
recurrent neural network (RNN), which makes themselves unwieldy in practice to
fit into other neural language models, such as Transformer and GPT-2. In this
paper, we introduce the Dependency-based Mixture Language Models. In detail, we
first train neural language models with a novel dependency modeling objective
to learn the probability distribution of future dependent tokens given context.
We then formulate the next-token probability by mixing the previous dependency
modeling probability distributions with self-attention. Extensive experiments
and human evaluations show that our method can be easily and effectively
applied to different neural language models while improving neural text
generation on various tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FaiRR: Faithful and Robust Deductive Reasoning over Natural Language. (arXiv:2203.10261v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10261">
<div class="article-summary-box-inner">
<span><p>Transformers have been shown to be able to perform deductive reasoning on a
logical rulebase containing rules and statements written in natural language.
Recent works show that such models can also produce the reasoning steps (i.e.,
the proof graph) that emulate the model's logical reasoning process. Currently,
these black-box models generate both the proof graph and intermediate
inferences within the same model and thus may be unfaithful. In this work, we
frame the deductive logical reasoning task by defining three modular
components: rule selection, fact selection, and knowledge composition. The rule
and fact selection steps select the candidate rule and facts to be used and
then the knowledge composition combines them to generate new inferences. This
ensures model faithfulness by assured causal relation from the proof step to
the inference reasoning. To test our framework, we propose FaiRR (Faithful and
Robust Reasoner) where the above three components are independently modeled by
transformers. We observe that FaiRR is robust to novel language perturbations,
and is faster at inference than previous works on existing reasoning datasets.
Additionally, in contrast to black-box generative models, the errors made by
FaiRR are more interpretable due to the modular approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Clickbait Spoiling via Question Answering and Passage Retrieval. (arXiv:2203.10282v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10282">
<div class="article-summary-box-inner">
<span><p>We introduce and study the task of clickbait spoiling: generating a short
text that satisfies the curiosity induced by a clickbait post. Clickbait links
to a web page and advertises its contents by arousing curiosity instead of
providing an informative summary. Our contributions are approaches to classify
the type of spoiler needed (i.e., a phrase or a passage), and to generate
appropriate spoilers. A large-scale evaluation and error analysis on a new
corpus of 5,000 manually spoiled clickbait posts -- the Webis Clickbait
Spoiling Corpus 2022 -- shows that our spoiler type classifier achieves an
accuracy of 80%, while the question answering model DeBERTa-large outperforms
all others in generating spoilers for both types.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-channel CNN to classify nepali covid-19 related tweets using hybrid features. (arXiv:2203.10286v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10286">
<div class="article-summary-box-inner">
<span><p>Because of the current COVID-19 pandemic with its increasing fears among
people, it has triggered several health complications such as depression and
anxiety. Such complications have not only affected the developed countries but
also developing countries such as Nepal. These complications can be understood
from peoples' tweets/comments posted online after their proper analysis and
sentiment classification. Nevertheless, owing to the limited number of
tokens/words in each tweet, it is always crucial to capture multiple
information associated with them for their better understanding. In this study,
we, first, represent each tweet by combining both syntactic and semantic
information, called hybrid features. The syntactic information is generated
from the bag of words method, whereas the semantic information is generated
from the combination of the fastText-based (ft) and domain-specific (ds)
methods. Second, we design a novel multi-channel convolutional neural network
(MCNN), which ensembles the multiple CNNs, to capture multi-scale information
for better classification. Last, we evaluate the efficacy of both the proposed
feature extraction method and the MCNN model classifying tweets into three
sentiment classes (positive, neutral and negative) on NepCOV19Tweets dataset,
which is the only public COVID-19 tweets dataset in Nepali language. The
evaluation results show that the proposed hybrid features outperform individual
feature extraction methods with the highest classification accuracy of 69.7%
and the MCNN model outperforms the existing methods with the highest
classification accuracy of 71.3% during classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From meaning to perception -- exploring the space between word and odor perception embeddings. (arXiv:2203.10294v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10294">
<div class="article-summary-box-inner">
<span><p>In this paper we propose the use of the Word2vec algorithm in order to obtain
odor perception embeddings (or smell embeddings), only using publicly available
perfume descriptions. Besides showing meaningful similarity relationships among
each other, these embeddings also demonstrate to possess some shared
information with their respective word embeddings. The meaningfulness of these
embeddings suggests that aesthetics might provide enough constraints for using
algorithms motivated by distributional semantics on non-randomly combined data.
Furthermore, they provide possibilities for new ways of classifying odors and
analyzing perfumes. We have also employed the embeddings in an attempt to
understand the aesthetic nature of perfumes, based on the difference between
real and randomly generated perfumes. In an additional tentative experiment we
explore the possibility of a mapping between the word embedding space and the
odor perception embedding space by fitting a regressor on the shared vocabulary
and then predict the odor perception embeddings of words without an a priori
associated smell, such as night or sky.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Machine Translation with Phrase-Level Universal Visual Representations. (arXiv:2203.10299v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10299">
<div class="article-summary-box-inner">
<span><p>Multimodal machine translation (MMT) aims to improve neural machine
translation (NMT) with additional visual information, but most existing MMT
methods require paired input of source sentence and image, which makes them
suffer from shortage of sentence-image pairs. In this paper, we propose a
phrase-level retrieval-based method for MMT to get visual information for the
source input from existing sentence-image data sets so that MMT can break the
limitation of paired sentence-image input. Our method performs retrieval at the
phrase level and hence learns visual information from pairs of source phrase
and grounded region, which can mitigate data sparsity. Furthermore, our method
employs the conditional variational auto-encoder to learn visual
representations which can filter redundant visual information and only retain
visual information related to the phrase. Experiments show that the proposed
method significantly outperforms strong baselines on multiple MMT datasets,
especially when the textual context is limited.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridging Pre-trained Language Models and Hand-crafted Features for Unsupervised POS Tagging. (arXiv:2203.10315v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10315">
<div class="article-summary-box-inner">
<span><p>In recent years, large-scale pre-trained language models (PLMs) have made
extraordinary progress in most NLP tasks. But, in the unsupervised POS tagging
task, works utilizing PLMs are few and fail to achieve state-of-the-art (SOTA)
performance. The recent SOTA performance is yielded by a Guassian HMM variant
proposed by He et al. (2018). However, as a generative model, HMM makes very
strong independence assumptions, making it very challenging to incorporate
contexualized word representations from PLMs. In this work, we for the first
time propose a neural conditional random field autoencoder (CRF-AE) model for
unsupervised POS tagging. The discriminative encoder of CRF-AE can
straightforwardly incorporate ELMo word representations. Moreover, inspired by
feature-rich HMM, we reintroduce hand-crafted features into the decoder of
CRF-AE. Finally, experiments clearly show that our model outperforms previous
state-of-the-art models by a large margin on Penn Treebank and multilingual
Universal Dependencies treebank v2.0.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction. (arXiv:2203.10316v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10316">
<div class="article-summary-box-inner">
<span><p>Solving math word problems requires deductive reasoning over the quantities
in the text. Various recent research efforts mostly relied on
sequence-to-sequence or sequence-to-tree models to generate mathematical
expressions without explicitly performing relational reasoning between
quantities in the given context. While empirically effective, such approaches
typically do not provide explanations for the generated expressions. In this
work, we view the task as a complex relation extraction problem, proposing a
novel approach that presents explainable deductive reasoning steps to
iteratively construct target expressions, where each step involves a primitive
operation over two quantities defining their relation. Through extensive
experiments on four benchmark datasets, we show that the proposed model
significantly outperforms existing strong baselines. We further demonstrate
that the deductive procedure not only presents more explainable steps but also
enables us to make more accurate predictions on questions that require more
complex reasoning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequence-to-Sequence Knowledge Graph Completion and Question Answering. (arXiv:2203.10321v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10321">
<div class="article-summary-box-inner">
<span><p>Knowledge graph embedding (KGE) models represent each entity and relation of
a knowledge graph (KG) with low-dimensional embedding vectors. These methods
have recently been applied to KG link prediction and question answering over
incomplete KGs (KGQA). KGEs typically create an embedding for each entity in
the graph, which results in large model sizes on real-world graphs with
millions of entities. For downstream tasks these atomic entity representations
often need to be integrated into a multi stage pipeline, limiting their
utility. We show that an off-the-shelf encoder-decoder Transformer model can
serve as a scalable and versatile KGE model obtaining state-of-the-art results
for KG link prediction and incomplete KG question answering. We achieve this by
posing KG link prediction as a sequence-to-sequence task and exchange the
triple scoring approach taken by prior KGE methods with autoregressive
decoding. Such a simple but powerful method reduces the model size up to 98%
compared to conventional KGE models while keeping inference time tractable.
After finetuning this model on the task of KGQA over incomplete KGs, our
approach outperforms baselines on multiple large-scale datasets without
extensive hyperparameter tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pretraining with Synthetic Language: Studying Transferable Knowledge in Language Models. (arXiv:2203.10326v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10326">
<div class="article-summary-box-inner">
<span><p>We investigate what kind of structural knowledge learned in neural network
encoders is transferable to processing natural language. We design synthetic
languages with structural properties that mimic natural language, pretrain
encoders on the data, and see how much performance the encoder exhibits on
downstream tasks in natural language. Our experimental results show that
pretraining with a synthetic language with a nesting dependency structure
provides some knowledge transferable to natural language. A follow-up probing
analysis indicates that its success in the transfer is related to the amount of
encoded contextual information and what is transferred is the knowledge of
position-aware context dependence of language. Our results provide insights
into how neural network encoders process human languages and the source of
cross-lingual transferability of recent multilingual language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding COVID-19 News Coverage using Medical NLP. (arXiv:2203.10338v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10338">
<div class="article-summary-box-inner">
<span><p>Being a global pandemic, the COVID-19 outbreak received global media
attention. In this study, we analyze news publications from CNN and The
Guardian - two of the world's most influential media organizations. The dataset
includes more than 36,000 articles, analyzed using the clinical and biomedical
Natural Language Processing (NLP) models from the Spark NLP for Healthcare
library, which enables a deeper analysis of medical concepts than previously
achieved. The analysis covers key entities and phrases, observed biases, and
change over time in news coverage by correlating mined medical symptoms,
procedures, drugs, and guidance with commonly mentioned demographic and
occupational groups. Another analysis is of extracted Adverse Drug Events about
drug and vaccine manufacturers, which when reported by major news outlets has
an impact on vaccine hesitancy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Detection of Entity-Manipulated Text using Factual Knowledge. (arXiv:2203.10343v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10343">
<div class="article-summary-box-inner">
<span><p>In this work, we focus on the problem of distinguishing a human written news
article from a news article that is created by manipulating entities in a human
written news article (e.g., replacing entities with factually incorrect
entities). Such manipulated articles can mislead the reader by posing as a
human written news article. We propose a neural network based detector that
detects manipulated news articles by reasoning about the facts mentioned in the
article. Our proposed detector exploits factual knowledge via graph
convolutional neural network along with the textual information in the news
article. We also create challenging datasets for this task by considering
various strategies to generate the new replacement entity (e.g., entity
generation from GPT-2). In all the settings, our proposed model either matches
or outperforms the state-of-the-art detector in terms of accuracy. Our code and
data are available at https://github.com/UBC-NLP/manipulated_entity_detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perturbations in the Wild: Leveraging Human-Written Text Perturbations for Realistic Adversarial Attack and Defense. (arXiv:2203.10346v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10346">
<div class="article-summary-box-inner">
<span><p>We proposes a novel algorithm, ANTHRO, that inductively extracts over 600K
human-written text perturbations in the wild and leverages them for realistic
adversarial attack. Unlike existing character-based attacks which often
deductively hypothesize a set of manipulation strategies, our work is grounded
on actual observations from real-world texts. We find that adversarial texts
generated by ANTHRO achieve the best trade-off between (1) attack success rate,
(2) semantic preservation of the original text, and (3) stealthiness--i.e.
indistinguishable from human writings hence harder to be flagged as suspicious.
Specifically, our attacks accomplished around 83% and 91% attack success rates
on BERT and RoBERTa, respectively. Moreover, it outperformed the TextBugger
baseline with an increase of 50% and 40% in terms of semantic preservation and
stealthiness when evaluated by both layperson and professional human workers.
ANTHRO can further enhance a BERT classifier's performance in understanding
different variations of human-written toxic texts via adversarial training when
compared to the Perspective API.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Representative Keywords Selection: A Probabilistic Approach. (arXiv:2203.10365v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10365">
<div class="article-summary-box-inner">
<span><p>We propose a probabilistic approach to select a subset of a \textit{target
domain representative keywords} from a candidate set, contrasting with a
context domain. Such a task is crucial for many downstream tasks in natural
language processing. To contrast the target domain and the context domain, we
adapt the \textit{two-component mixture model} concept to generate a
distribution of candidate keywords. It provides more importance to the
\textit{distinctive} keywords of the target domain than common keywords
contrasting with the context domain. To support the \textit{representativeness}
of the selected keywords towards the target domain, we introduce an
\textit{optimization algorithm} for selecting the subset from the generated
candidate distribution. We have shown that the optimization algorithm can be
efficiently implemented with a near-optimal approximation guarantee. Finally,
extensive experiments on multiple domains demonstrate the superiority of our
approach over other baselines for the tasks of keyword summary generation and
trending keywords selection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Online Behaviour of the Algerian Abusers in Social Media Networks. (arXiv:2203.10369v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10369">
<div class="article-summary-box-inner">
<span><p>Connecting to social media networks becomes a daily task for the majority of
people around the world, and the amount of shared information is growing
exponentially. Thus, controlling the way in which people communicate is
necessary, in order to protect them from disorientation, conflicts,
aggressions, etc. In this paper, we conduct a statistical study on the
cyber-bullying and the abusive content in social media (i.e. Facebook), where
we try to spot the online behaviour of the abusers in the Algerian community.
More specifically, we have involved 200 Facebook users from different regions
among 600 to carry out this study. The aim of this investigation is to aid
automatic systems of abuse detection to take decision by incorporating the
online activity. Abuse detection systems require a large amount of data to
perform better on such kind of texts (i.e. unstructured and informal texts),
and this is due to the lack of standard orthography, where there are various
Algerian dialects and languages spoken.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Robust Prefix-Tuning for Text Classification. (arXiv:2203.10378v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10378">
<div class="article-summary-box-inner">
<span><p>Recently, prefix-tuning has gained increasing attention as a
parameter-efficient finetuning method for large-scale pretrained language
models. The method keeps the pretrained models fixed and only updates the
prefix token parameters for each downstream task. Despite being lightweight and
modular, prefix-tuning still lacks robustness to textual adversarial attacks.
However, most currently developed defense techniques necessitate auxiliary
model update and storage, which inevitably hamper the modularity and low
storage of prefix-tuning. In this work, we propose a robust prefix-tuning
framework that preserves the efficiency and modularity of prefix-tuning. The
core idea of our framework is leveraging the layerwise activations of the
language model by correctly-classified training data as the standard for
additional prefix finetuning. During the test phase, an extra batch-level
prefix is tuned for each batch and added to the original prefix for robustness
enhancement. Extensive experiments on three text classification benchmarks show
that our framework substantially improves robustness over several strong
baselines against five textual attacks of different types while maintaining
comparable accuracy on clean texts. We also interpret our robust prefix-tuning
framework from the optimal control perspective and pose several directions for
future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How does the pre-training objective affect what large language models learn about linguistic properties?. (arXiv:2203.10415v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10415">
<div class="article-summary-box-inner">
<span><p>Several pre-training objectives, such as masked language modeling (MLM), have
been proposed to pre-train language models (e.g. BERT) with the aim of learning
better language representations. However, to the best of our knowledge, no
previous work so far has investigated how different pre-training objectives
affect what BERT learns about linguistics properties. We hypothesize that
linguistically motivated objectives such as MLM should help BERT to acquire
better linguistic knowledge compared to other non-linguistically motivated
objectives that are not intuitive or hard for humans to guess the association
between the input and the label to be predicted. To this end, we pre-train BERT
with two linguistically motivated objectives and three non-linguistically
motivated ones. We then probe for linguistic characteristics encoded in the
representation of the resulting models. We find strong evidence that there are
only small differences in probing performance between the representations
learned by the two different types of objectives. These surprising results
question the dominant narrative of linguistically informed pre-training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MetaOnce: A Metaverse Framework Based on Multi-scene Relations and Entity-relation-event Game. (arXiv:2203.10424v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10424">
<div class="article-summary-box-inner">
<span><p>Existing metaverse systems lack rich relation types between entities and
events. The challenge is that there is no portable framework to introduce rich
concepts, relations, events into the metaverse. This paper introduces a new
metaverse framework, MetaOnce. This framework proposes to build multi-scene
graphs. This framework not only describes rich relations in a single scene but
also combines multiple scene graphs into a complete graph for more
comprehensive analysis and inference. Prior social network systems mainly
describe friend relations. They ignore the effect of entity-relation-event
games on the metaverse system and existing rule constraints. We propose a rule
controller and impose constraints on the relations that allow the framework to
behave in a compliant manner. We build a metaverse system to test the features
of the framework, and experimental results show that our framework can build a
multi-scene metaverse with memory and rule constraints.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">STEMM: Self-learning with Speech-text Manifold Mixup for Speech Translation. (arXiv:2203.10426v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10426">
<div class="article-summary-box-inner">
<span><p>How to learn a better speech representation for end-to-end speech-to-text
translation (ST) with limited labeled data? Existing techniques often attempt
to transfer powerful machine translation (MT) capabilities to ST, but neglect
the representation discrepancy across modalities. In this paper, we propose the
Speech-TExt Manifold Mixup (STEMM) method to calibrate such discrepancy.
Specifically, we mix up the representation sequences of different modalities,
and take both unimodal speech sequences and multimodal mixed sequences as input
to the translation model in parallel, and regularize their output predictions
with a self-learning framework. Experiments on MuST-C speech translation
benchmark and further analysis show that our method effectively alleviates the
cross-modal representation discrepancy, and achieves significant improvements
over a strong baseline on eight translation directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">g2pW: A Conditional Weighted Softmax BERT for Polyphone Disambiguation in Mandarin. (arXiv:2203.10430v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10430">
<div class="article-summary-box-inner">
<span><p>Polyphone disambiguation is the most crucial task in Mandarin
grapheme-to-phoneme (g2p) conversion. Previous studies have benefited from this
problem because of pre-trained language models, restricted output, and extra
information from Part-Of-Speech (POS) tagging. Inspired by the strategies, we
proposed a novel approach, called g2pW, which adapts learnable softmax-weights
to condition the outputs of BERT with the polyphonic character of interest and
its POS tagging. Rather than using the hard mask as in previous works, our
experiments showed that learning a soft-weighting function for the candidate
phonemes benefits performance. Besides, our g2pW does not require extra
pre-trained POS tagging models while using POS tags as auxiliary features since
we train the POS tagging model simultaneously with the unified encoder. The
experiments show that our g2pW outperforms existing methods on the public
dataset. All codes, model weights, and a user-friendly package are publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpretability of Fine-grained Classification of Sadness and Depression. (arXiv:2203.10432v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10432">
<div class="article-summary-box-inner">
<span><p>While sadness is a human emotion that people experience at certain times
throughout their lives, inflicting them with emotional disappointment and pain,
depression is a longer term mental illness which impairs social, occupational,
and other vital regions of functioning making it a much more serious issue and
needs to be catered to at the earliest. NLP techniques can be utilized for the
detection and subsequent diagnosis of these emotions. Most of the open sourced
data on the web deal with sadness as a part of depression, as an emotion even
though the difference in severity of both is huge. Thus, we create our own
novel dataset illustrating the difference between the two. In this paper, we
aim to highlight the difference between the two and highlight how interpretable
our models are to distinctly label sadness and depression. Due to the sensitive
nature of such information, privacy measures need to be taken for handling and
training of such data. Hence, we also explore the effect of Federated Learning
(FL) on contextualised language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Structuring Real-World Data at Scale: Deep Learning for Extracting Key Oncology Information from Clinical Text with Patient-Level Supervision. (arXiv:2203.10442v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10442">
<div class="article-summary-box-inner">
<span><p>Objective: The majority of detailed patient information in real-world data
(RWD) is only consistently available in free-text clinical documents. Manual
curation is expensive and time-consuming. Developing natural language
processing (NLP) methods for structuring RWD is thus essential for scaling
real-world evidence generation.
</p>
<p>Materials and Methods: Traditional rule-based systems are vulnerable to the
prevalent linguistic variations and ambiguities in clinical text, and prior
applications of machine-learning methods typically require sentence-level or
report-level labeled examples that are hard to produce at scale. We propose
leveraging patient-level supervision from medical registries, which are often
readily available and capture key patient information, for general RWD
applications. To combat the lack of sentence-level or report-level annotations,
we explore advanced deep-learning methods by combining domain-specific
pretraining, recurrent neural networks, and hierarchical attention.
</p>
<p>Results: We conduct an extensive study on 135,107 patients from the cancer
registry of a large integrated delivery network (IDN) comprising healthcare
systems in five western US states. Our deep learning methods attain test AUROC
of 94-99% for key tumor attributes and comparable performance on held-out data
from separate health systems and states.
</p>
<p>Discussion and Conclusion: Ablation results demonstrate clear superiority of
these advanced deep-learning methods over prior approaches. Error analysis
shows that our NLP system sometimes even corrects errors in registrar labels.
We also conduct a preliminary investigation in accelerating registry curation
and general RWD structuring via assisted curation for over 1.2 million cancer
patients in this healthcare network.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DEIM: An effective deep encoding and interaction model for sentence matching. (arXiv:2203.10482v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10482">
<div class="article-summary-box-inner">
<span><p>Natural language sentence matching is the task of comparing two sentences and
identifying the relationship between them.It has a wide range of applications
in natural language processing tasks such as reading comprehension, question
and answer systems. The main approach is to compute the interaction between
text representations and sentence pairs through an attention mechanism, which
can extract the semantic information between sentence pairs well. However,this
kind of method can not gain satisfactory results when dealing with complex
semantic features. To solve this problem, we propose a sentence matching method
based on deep encoding and interaction to extract deep semantic information. In
the encoder layer,we refer to the information of another sentence in the
process of encoding a single sentence, and later use a heuristic algorithm to
fuse the information. In the interaction layer, we use a bidirectional
attention mechanism and a self-attention mechanism to obtain deep semantic
information.Finally, we perform a pooling operation and input it to the MLP for
classification. we evaluate our model on three tasks: recognizing textual
entailment, paraphrase recognition, and answer selection. We conducted
experiments on the SNLI and SciTail datasets for the recognizing textual
entailment task, the Quora dataset for the paraphrase recognition task, and the
WikiQA dataset for the answer selection task. The experimental results show
that the proposed algorithm can effectively extract deep semantic features that
verify the effectiveness of the algorithm on sentence matching tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Entailment Relation Aware Paraphrase Generation. (arXiv:2203.10483v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10483">
<div class="article-summary-box-inner">
<span><p>We introduce a new task of entailment relation aware paraphrase generation
which aims at generating a paraphrase conforming to a given entailment relation
(e.g. equivalent, forward entailing, or reverse entailing) with respect to a
given input. We propose a reinforcement learning-based weakly-supervised
paraphrasing system, ERAP, that can be trained using existing paraphrase and
natural language inference (NLI) corpora without an explicit task-specific
corpus. A combination of automated and human evaluations show that ERAP
generates paraphrases conforming to the specified entailment relation and are
of good quality as compared to the baselines and uncontrolled paraphrasing
systems. Using ERAP for augmenting training data for downstream textual
entailment task improves performance over an uncontrolled paraphrasing system,
and introduces fewer training artifacts, indicating the benefit of explicit
control during paraphrasing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Inductive Transfer for Continual Dialogue Learning. (arXiv:2203.10484v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10484">
<div class="article-summary-box-inner">
<span><p>Pre-trained models have achieved excellent performance on the dialogue task.
However, for the continual increase of online chit-chat scenarios, directly
fine-tuning these models for each of the new tasks not only explodes the
capacity of the dialogue system on the embedded devices but also causes
knowledge forgetting on pre-trained models and knowledge interference among
diverse dialogue tasks. In this work, we propose a hierarchical inductive
transfer framework to learn and deploy the dialogue skills continually and
efficiently. First, we introduce the adapter module into pre-trained models for
learning new dialogue tasks. As the only trainable module, it is beneficial for
the dialogue system on the embedded devices to acquire new dialogue skills with
negligible additional parameters. Then, for alleviating knowledge interference
between tasks yet benefiting the regularization between them, we further design
hierarchical inductive transfer that enables new tasks to use general knowledge
in the base adapter without being misled by diverse knowledge in task-specific
adapters. Empirical evaluation and analysis indicate that our framework obtains
comparable performance under deployment-friendly model capacity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parallel Instance Query Network for Named Entity Recognition. (arXiv:2203.10545v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10545">
<div class="article-summary-box-inner">
<span><p>Named entity recognition (NER) is a fundamental task in natural language
processing. Recent works treat named entity recognition as a reading
comprehension task, constructing type-specific queries manually to extract
entities. This paradigm suffers from three issues. First, type-specific queries
can only extract one type of entities per inference, which is inefficient.
Second, the extraction for different types of entities is isolated, ignoring
the dependencies between them. Third, query construction relies on external
knowledge and is difficult to apply to realistic scenarios with hundreds of
entity types. To deal with them, we propose Parallel Instance Query Network
(PIQN), which sets up global and learnable instance queries to extract entities
from a sentence in a parallel manner. Each instance query predicts one entity,
and by feeding all instance queries simultaneously, we can query all entities
in parallel. Instead of being constructed from external knowledge, instance
queries can learn their different query semantics during training. For training
the model, we treat label assignment as a one-to-many Linear Assignment Problem
(LAP) and dynamically assign gold entities to instance queries with minimal
assignment cost. Experiments on both nested and flat NER datasets demonstrate
that our proposed method outperforms previous state-of-the-art models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Neural-Symbolic Approach to Natural Language Understanding. (arXiv:2203.10557v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10557">
<div class="article-summary-box-inner">
<span><p>Deep neural networks, empowered by pre-trained language models, have achieved
remarkable results in natural language understanding (NLU) tasks. However,
their performances can deteriorate drastically when logical reasoning is needed
in the process. This is because, ideally, NLU needs to depend on not only
analogical reasoning, which deep neural networks are good at, but also logical
reasoning. According to the dual-process theory, analogical reasoning and
logical reasoning are respectively carried out by System 1 and System 2 in the
human brain. Inspired by the theory, we present a novel framework for NLU
called Neural-Symbolic Processor (NSP), which performs analogical reasoning
based on neural processing and performs logical reasoning based on both neural
and symbolic processing. As a case study, we conduct experiments on two NLU
tasks, question answering (QA) and natural language inference (NLI), when
numerical reasoning (a type of logical reasoning) is necessary. The
experimental results show that our method significantly outperforms
state-of-the-art methods in both tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Who will share Fake-News on Twitter? Psycholinguistic cues in online post histories discriminate Between actors in the misinformation ecosystem. (arXiv:2203.10560v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10560">
<div class="article-summary-box-inner">
<span><p>The spread of misinformation or fake-news is a global concern that undermines
progress on issues such as protecting democracy and public health. Past
research aiming to combat its spread has largely focused on identifying its
semantic content and media outlets publishing such news. In contrast, we aim to
identify individuals who are more likely to share fake-news by studying the
language of actors in the fake-news ecosystem (such as fake-news sharers,
fact-check sharers and random twitter users), and creating a linguistic profile
of them. Fake-news sharers and fact-check sharers use significantly more
high-arousal negative emotions in their language, but fake-news sharers express
more existentially-based needs than other actors. Incorporating
psycholinguistic cues as inferred from their tweets into a model of
socio-demographic predictors considerably improves classification accuracy of
fake-news sharers. The finding that fake-news sharers differ in important ways
from other actors in the fake-news ecosystem (such as in their existential
needs), but are also similar to them in other ways (such as in their anger
levels), highlights the importance of studying the entire fake-news ecosystem
to increase accuracy in identification and prediction. Our approach can help
mitigate fake-news sharing by enabling platforms to pre-emptively screen
potential fake-news sharers' posts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Small Batch Sizes Improve Training of Low-Resource Neural MT. (arXiv:2203.10579v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10579">
<div class="article-summary-box-inner">
<span><p>We study the role of an essential hyper-parameter that governs the training
of Transformers for neural machine translation in a low-resource setting: the
batch size. Using theoretical insights and experimental evidence, we argue
against the widespread belief that batch size should be set as large as allowed
by the memory of the GPUs. We show that in a low-resource setting, a smaller
batch size leads to higher scores in a shorter training time, and argue that
this is due to better regularization of the gradients during training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cluster & Tune: Boost Cold Start Performance in Text Classification. (arXiv:2203.10581v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10581">
<div class="article-summary-box-inner">
<span><p>In real-world scenarios, a text classification task often begins with a cold
start, when labeled data is scarce. In such cases, the common practice of
fine-tuning pre-trained models, such as BERT, for a target classification task,
is prone to produce poor performance. We suggest a method to boost the
performance of such models by adding an intermediate unsupervised
classification task, between the pre-training and fine-tuning phases. As such
an intermediate task, we perform clustering and train the pre-trained model on
predicting the cluster labels. We test this hypothesis on various data sets,
and show that this additional classification phase can significantly improve
performance, mainly for topical classification tasks, when the number of
labeled instances available for fine-tuning is only a couple of dozen to a few
hundred.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Five Psycholinguistic Characteristics for Better Interaction with Users. (arXiv:2012.09692v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.09692">
<div class="article-summary-box-inner">
<span><p>When two people pay attention to each other and are interested in what the
other has to say or write, they almost instantly adapt their writing/speaking
style to match the other. For a successful interaction with a user, chatbots
and dialogue systems should be able to do the same. We propose a framework
consisting of five psycholinguistic textual characteristics for better
human-computer interaction. We describe the annotation processes used for
collecting the data, and benchmark five binary classification tasks,
experimenting with different training sizes and model architectures. The best
architectures noticeably outperform several baselines and achieve
macro-averaged F$_1$-scores between 72\% and 96\% depending on the language and
the task. The proposed framework proved to be fairly easy to model for various
languages even with small amount of manually annotated data if right
architectures are used.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Curriculum Learning: A Survey. (arXiv:2101.10382v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.10382">
<div class="article-summary-box-inner">
<span><p>Training machine learning models in a meaningful order, from the easy samples
to the hard ones, using curriculum learning can provide performance
improvements over the standard training approach based on random data
shuffling, without any additional computational costs. Curriculum learning
strategies have been successfully employed in all areas of machine learning, in
a wide range of tasks. However, the necessity of finding a way to rank the
samples from easy to hard, as well as the right pacing function for introducing
more difficult data can limit the usage of the curriculum approaches. In this
survey, we show how these limits have been tackled in the literature, and we
present different curriculum learning instantiations for various tasks in
machine learning. We construct a multi-perspective taxonomy of curriculum
learning approaches by hand, considering various classification criteria. We
further build a hierarchical tree of curriculum learning methods using an
agglomerative clustering algorithm, linking the discovered clusters with our
taxonomy. At the end, we provide some interesting directions for future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Domain Multi-Task Learning for Sequential Sentence Classification in Research Papers. (arXiv:2102.06008v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.06008">
<div class="article-summary-box-inner">
<span><p>Sequential sentence classification deals with the categorisation of sentences
based on their content and context. Applied to scientific texts, it enables the
automatic structuring of research papers and the improvement of academic search
engines. However, previous work has not investigated the potential of transfer
learning for sentence classification across different scientific domains and
the issue of different text structure of full papers and abstracts. In this
paper, we derive seven related research questions and present several
contributions to address them: First, we suggest a novel uniform deep learning
architecture and multi-task learning for cross-domain sequential sentence
classification in scientific texts. Second, we tailor two common transfer
learning methods, sequential transfer learning and multi-task learning, to deal
with the challenges of the given task. Semantic relatedness of tasks is a
prerequisite for successful transfer learning of neural models. Consequently,
our third contribution is an approach to semi-automatically identify
semantically related classes from different annotation schemes and we present
an analysis of four annotation schemes. Comprehensive experimental results
indicate that models, which are trained on datasets from different scientific
domains, benefit from one another when using the proposed multi-task learning
architecture. We also report comparisons with several state-of-the-art
approaches. Our approach outperforms the state of the art on full paper
datasets significantly while being on par for datasets consisting of abstracts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SalKG: Learning From Knowledge Graph Explanations for Commonsense Reasoning. (arXiv:2104.08793v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08793">
<div class="article-summary-box-inner">
<span><p>Augmenting pre-trained language models with knowledge graphs (KGs) has
achieved success on various commonsense reasoning tasks. However, for a given
task instance, the KG, or certain parts of the KG, may not be useful. Although
KG-augmented models often use attention to focus on specific KG components, the
KG is still always used, and the attention mechanism is never explicitly taught
which KG components should be used. Meanwhile, saliency methods can measure how
much a KG feature (e.g., graph, node, path) influences the model to make the
correct prediction, thus explaining which KG features are useful. This paper
explores how saliency explanations can be used to improve KG-augmented models'
performance. First, we propose to create coarse (Is the KG useful?) and fine
(Which nodes/paths in the KG are useful?) saliency explanations. Second, to
motivate saliency-based supervision, we analyze oracle KG-augmented models
which directly use saliency explanations as extra inputs for guiding their
attention. Third, we propose SalKG, a framework for KG-augmented models to
learn from coarse and/or fine saliency explanations. Given saliency
explanations created from a task's training set, SalKG jointly trains the model
to predict the explanations, then solve the task by attending to KG features
highlighted by the predicted explanations. On three commonsense QA benchmarks
(CSQA, OBQA, CODAH) and a range of KG-augmented models, we show that SalKG can
yield considerable performance gains -- up to 2.76% absolute improvement on
CSQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BertGCN: Transductive Text Classification by Combining GCN and BERT. (arXiv:2105.05727v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.05727">
<div class="article-summary-box-inner">
<span><p>In this work, we propose BertGCN, a model that combines large scale
pretraining and transductive learning for text classification. BertGCN
constructs a heterogeneous graph over the dataset and represents documents as
nodes using BERT representations. By jointly training the BERT and GCN modules
within BertGCN, the proposed model is able to leverage the advantages of both
worlds: large-scale pretraining which takes the advantage of the massive amount
of raw data and transductive learning which jointly learns representations for
both training data and unlabeled test data by propagating label influence
through graph convolution. Experiments show that BertGCN achieves SOTA
performances on a wide range of text classification datasets. Code is available
at https://github.com/ZeroRin/BertGCN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Automatic Speech Recognition: A Review. (arXiv:2106.04897v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04897">
<div class="article-summary-box-inner">
<span><p>Automatic Speech Recognition (ASR) systems can be trained to achieve
remarkable performance given large amounts of manually transcribed speech, but
large labeled data sets can be difficult or expensive to acquire for all
languages of interest. In this paper, we review the research literature to
identify models and ideas that could lead to fully unsupervised ASR, including
unsupervised segmentation of the speech signal, unsupervised mapping from
speech segments to text, and semi-supervised models with nominal amounts of
labeled examples. The objective of the study is to identify the limitations of
what can be learned from speech data alone and to understand the minimum
requirements for speech recognition. Identifying these limitations would help
optimize the resources and efforts in ASR development for low-resource
languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Automated Quality Evaluation Framework of Psychotherapy Conversations with Local Quality Estimates. (arXiv:2106.07922v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07922">
<div class="article-summary-box-inner">
<span><p>Text-based computational approaches for assessing the quality of
psychotherapy are being developed to support quality assurance and clinical
training. However, due to the long durations of typical conversation based
therapy sessions, and due to limited annotated modeling resources,
computational methods largely rely on frequency-based lexical features or
dialogue acts to assess the overall session level characteristics. In this
work, we propose a hierarchical framework to automatically evaluate the quality
of transcribed Cognitive Behavioral Therapy (CBT) interactions. Given the
richly dynamic nature of the spoken dialog within a talk therapy session, to
evaluate the overall session level quality, we propose to consider modeling it
as a function of local variations across the interaction. To implement that
empirically, we divide each psychotherapy session into conversation segments
and initialize the segment-level qualities with the session-level scores.
First, we produce segment embeddings by fine-tuning a BERT-based model, and
predict segment-level (local) quality scores. These embeddings are used as the
lower-level input to a Bidirectional LSTM-based neural network to predict the
session-level (global) quality estimates. In particular, we model the global
quality as a linear function of the local quality scores, which allows us to
update the segment-level quality estimates based on the session-level quality
prediction. These newly estimated segment-level scores benefit the BERT
fine-tuning process, which in turn results in better segment embeddings. We
evaluate the proposed framework on automatically derived transcriptions from
real-world CBT clinical recordings to predict session-level behavior codes. The
results indicate that our approach leads to improved evaluation accuracy for
most codes when used for both regression and classification tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models. (arXiv:2106.10199v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.10199">
<div class="article-summary-box-inner">
<span><p>We introduce BitFit, a sparse-finetuning method where only the bias-terms of
the model (or a subset of them) are being modified. We show that with
small-to-medium training data, applying BitFit on pre-trained BERT models is
competitive with (and sometimes better than) fine-tuning the entire model. For
larger data, the method is competitive with other sparse fine-tuning methods.
Besides their practical utility, these findings are relevant for the question
of understanding the commonly-used process of finetuning: they support the
hypothesis that finetuning is mainly about exposing knowledge induced by
language-modeling training, rather than learning new task-specific linguistic
knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ethics Sheets for AI Tasks. (arXiv:2107.01183v4 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.01183">
<div class="article-summary-box-inner">
<span><p>Several high-profile events, such as the mass testing of emotion recognition
systems on vulnerable sub-populations and using question answering systems to
make moral judgments, have highlighted how technology will often lead to more
adverse outcomes for those that are already marginalized. At issue here are not
just individual systems and datasets, but also the AI tasks themselves. In this
position paper, I make a case for thinking about ethical considerations not
just at the level of individual models and datasets, but also at the level of
AI tasks. I will present a new form of such an effort, Ethics Sheets for AI
Tasks, dedicated to fleshing out the assumptions and ethical considerations
hidden in how a task is commonly framed and in the choices we make regarding
the data, method, and evaluation. I will also present a template for ethics
sheets with 50 ethical considerations, using the task of emotion recognition as
a running example. Ethics sheets are a mechanism to engage with and document
ethical considerations before building datasets and systems. Similar to survey
articles, a small number of ethics sheets can serve numerous researchers and
developers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Uncertainty-based Query Strategies for Active Learning with Transformers. (arXiv:2107.05687v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.05687">
<div class="article-summary-box-inner">
<span><p>Active learning is the iterative construction of a classification model
through targeted labeling, enabling significant labeling cost savings. As most
research on active learning has been carried out before transformer-based
language models ("transformers") became popular, despite its practical
importance, comparably few papers have investigated how transformers can be
combined with active learning to date. This can be attributed to the fact that
using state-of-the-art query strategies for transformers induces a prohibitive
runtime overhead, which effectively nullifies, or even outweighs the desired
cost savings. For this reason, we revisit uncertainty-based query strategies,
which had been largely outperformed before, but are particularly suited in the
context of fine-tuning transformers. In an extensive evaluation, we connect
transformers to experiments from previous research, assessing their performance
on five widely used text classification benchmarks. For active learning with
transformers, several other uncertainty-based approaches outperform the
well-known prediction entropy query strategy, thereby challenging its status as
most popular uncertainty baseline in active learning for text classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">(Un)solving Morphological Inflection: Lemma Overlap Artificially Inflates Models' Performance. (arXiv:2108.05682v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.05682">
<div class="article-summary-box-inner">
<span><p>In the domain of Morphology, Inflection is a fundamental and important task
that gained a lot of traction in recent years, mostly via SIGMORPHON's
shared-tasks. With average accuracy above 0.9 over the scores of all languages,
the task is considered mostly solved using relatively generic neural seq2seq
models, even with little data provided. In this work, we propose to re-evaluate
morphological inflection models by employing harder train-test splits that will
challenge the generalization capacity of the models. In particular, as opposed
to the na{\"i}ve split-by-form, we propose a split-by-lemma method to challenge
the performance on existing benchmarks. Our experiments with the three
top-ranked systems on the SIGMORPHON's 2020 shared-task show that the
lemma-split presents an average drop of 30 percentage points in macro-average
for the 90 languages included. The effect is most significant for low-resourced
languages with a drop as high as 95 points, but even high-resourced languages
lose about 10 points on average. Our results clearly show that generalizing
inflection to unseen lemmas is far from being solved, presenting a simple yet
effective means to promote more sophisticated models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">It is AI's Turn to Ask Humans a Question: Question-Answer Pair Generation for Children's Story Books. (arXiv:2109.03423v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03423">
<div class="article-summary-box-inner">
<span><p>Existing question answering (QA) techniques are created mainly to answer
questions asked by humans. But in educational applications, teachers often need
to decide what questions they should ask, in order to help students to improve
their narrative understanding capabilities. We design an automated
question-answer generation (QAG) system for this education scenario: given a
story book at the kindergarten to eighth-grade level as input, our system can
automatically generate QA pairs that are capable of testing a variety of
dimensions of a student's comprehension skills. Our proposed QAG model
architecture is demonstrated using a new expert-annotated FairytaleQA dataset,
which has 278 child-friendly storybooks with 10,580 QA pairs. Automatic and
human evaluations show that our model outperforms state-of-the-art QAG baseline
systems. On top of our QAG system, we also start to build an interactive
story-telling application for the future real-world deployment in this
educational scenario.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Parsing in Task-Oriented Dialog with Recursive Insertion-based Encoder. (arXiv:2109.04500v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04500">
<div class="article-summary-box-inner">
<span><p>We introduce a Recursive INsertion-based Encoder (RINE), a novel approach for
semantic parsing in task-oriented dialog. Our model consists of an encoder
network that incrementally builds the semantic parse tree by predicting the
non-terminal label and its positions in the linearized tree. At the generation
time, the model constructs the semantic parse tree by recursively inserting the
predicted non-terminal labels at the predicted positions until termination.
RINE achieves state-of-the-art exact match accuracy on low- and high-resource
versions of the conversational semantic parsing benchmark TOP (Gupta et al.,
2018; Chen et al., 2020), outperforming strong sequence-to-sequence models and
transition-based parsers. We also show that our model design is applicable to
nested named entity recognition task, where it performs on par with
state-of-the-art approach designed for that task. Finally, we demonstrate that
our approach is 2-3.5 times faster than the sequence-to-sequence model at
inference time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CINS: Comprehensive Instruction for Few-shot Learning in Task-oriented Dialog Systems. (arXiv:2109.04645v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04645">
<div class="article-summary-box-inner">
<span><p>As labeling cost for different modules in task-oriented dialog (ToD) systems
is high, a major challenge in practice is to learn different tasks with the
least amount of labeled data. Recently, prompting methods over pre-trained
language models (PLMs) have shown promising results for few-shot learning in
ToD. To better utilize the power of PLMs, this paper proposes Comprehensive
Instruction (CINS) that exploits PLMs with extra task-specific instructions. We
design a schema (definition, constraint, prompt) of instructions and their
customized realizations for three important downstream tasks in ToD, i.e.
intent classification, dialog state tracking, and natural language generation.
A sequence-to-sequence model (T5) is adopted to solve these three tasks in a
unified framework. Extensive experiments are conducted on these ToD tasks in
realistic few-shot learning scenarios with small validation data. Empirical
results demonstrate that the proposed CINS approach consistently improves
techniques that finetune PLMs with raw input or short prompts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Universal Simultaneous Machine Translation with Mixture-of-Experts Wait-k Policy. (arXiv:2109.05238v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05238">
<div class="article-summary-box-inner">
<span><p>Simultaneous machine translation (SiMT) generates translation before reading
the entire source sentence and hence it has to trade off between translation
quality and latency. To fulfill the requirements of different translation
quality and latency in practical applications, the previous methods usually
need to train multiple SiMT models for different latency levels, resulting in
large computational costs. In this paper, we propose a universal SiMT model
with Mixture-of-Experts Wait-k Policy to achieve the best translation quality
under arbitrary latency with only one trained model. Specifically, our method
employs multi-head attention to accomplish the mixture of experts where each
head is treated as a wait-k expert with its own waiting words number, and given
a test latency and source inputs, the weights of the experts are accordingly
adjusted to produce the best translation. Experiments on three datasets show
that our method outperforms all the strong baselines under different latency,
including the state-of-the-art adaptive policy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning When to Translate for Streaming Speech. (arXiv:2109.07368v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07368">
<div class="article-summary-box-inner">
<span><p>How to find proper moments to generate partial sentence translation given a
streaming speech input? Existing approaches waiting-and-translating for a fixed
duration often break the acoustic units in speech, since the boundaries between
acoustic units in speech are not even. In this paper, we propose MoSST, a
simple yet effective method for translating streaming speech content. Given a
usually long speech sequence, we develop an efficient monotonic segmentation
module inside an encoder-decoder model to accumulate acoustic information
incrementally and detect proper speech unit boundaries for the input in speech
translation task. Experiments on multiple translation directions of the MuST-C
dataset show that MoSST outperforms existing methods and achieves the best
trade-off between translation quality (BLEU) and latency. Our code is available
at https://github.com/dqqcasia/mosst.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ethics Sheet for Automatic Emotion Recognition and Sentiment Analysis. (arXiv:2109.08256v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08256">
<div class="article-summary-box-inner">
<span><p>The importance and pervasiveness of emotions in our lives makes affective
computing a tremendously important and vibrant line of work. Systems for
automatic emotion recognition (AER) and sentiment analysis can be facilitators
of enormous progress (e.g., in improving public health and commerce) but also
enablers of great harm (e.g., for suppressing dissidents and manipulating
voters). Thus, it is imperative that the affective computing community actively
engage with the ethical ramifications of their creations. In this paper, I have
synthesized and organized information from AI Ethics and Emotion Recognition
literature to present fifty ethical considerations relevant to AER. Notably,
the sheet fleshes out assumptions hidden in how AER is commonly framed, and in
the choices often made regarding the data, method, and evaluation. Special
attention is paid to the implications of AER on privacy and social groups.
Along the way, key recommendations are made for responsible AER. The objective
of the sheet is to facilitate and encourage more thoughtfulness on why to
automate, how to automate, and how to judge success well before the building of
AER systems. Additionally, the sheet acts as a useful introductory document on
emotion recognition (complementing survey articles).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RnG-KBQA: Generation Augmented Iterative Ranking for Knowledge Base Question Answering. (arXiv:2109.08678v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08678">
<div class="article-summary-box-inner">
<span><p>Existing KBQA approaches, despite achieving strong performance on i.i.d. test
data, often struggle in generalizing to questions involving unseen KB schema
items. Prior ranking-based approaches have shown some success in
generalization, but suffer from the coverage issue. We present RnG-KBQA, a
Rank-and-Generate approach for KBQA, which remedies the coverage issue with a
generation model while preserving a strong generalization capability. Our
approach first uses a contrastive ranker to rank a set of candidate logical
forms obtained by searching over the knowledge graph. It then introduces a
tailored generation model conditioned on the question and the top-ranked
candidates to compose the final logical form. We achieve new state-of-the-art
results on GrailQA and WebQSP datasets. In particular, our method surpasses the
prior state-of-the-art by a large margin on the GrailQA leaderboard. In
addition, RnG-KBQA outperforms all prior approaches on the popular WebQSP
benchmark, even including the ones that use the oracle entity linking. The
experimental results demonstrate the effectiveness of the interplay between
ranking and generation, which leads to the superior performance of our proposed
approach across all settings with especially strong improvements in zero-shot
generalization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-task Voice Activated Framework using Self-supervised Learning. (arXiv:2110.01077v3 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01077">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning methods such as wav2vec 2.0 have shown promising
results in learning speech representations from unlabelled and untranscribed
speech data that are useful for speech recognition. Since these representations
are learned without any task-specific supervision, they can also be useful for
other voice-activated tasks like speaker verification, keyword spotting,
emotion classification etc. In our work, we propose a general purpose framework
for adapting a pre-trained wav2vec 2.0 model for different voice-activated
tasks. We develop downstream network architectures that operate on the
contextualized speech representations of wav2vec 2.0 to adapt the
representations for solving a given task. Finally, we extend our framework to
perform multi-task learning by jointly optimizing the network parameters on
multiple voice activated tasks using a shared transformer backbone. Both of our
single and multi-task frameworks achieve state-of-the-art results in speaker
verification and keyword spotting benchmarks. Our best performing models
achieve 1.98% and 3.15% EER on VoxCeleb1 test set when trained on VoxCeleb2 and
VoxCeleb1 respectively, and 98.23% accuracy on Google Speech Commands v1.0
keyword spotting dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Co-training an Unsupervised Constituency Parser with Weak Supervision. (arXiv:2110.02283v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02283">
<div class="article-summary-box-inner">
<span><p>We introduce a method for unsupervised parsing that relies on bootstrapping
classifiers to identify if a node dominates a specific span in a sentence.
There are two types of classifiers, an inside classifier that acts on a span,
and an outside classifier that acts on everything outside of a given span.
Through self-training and co-training with the two classifiers, we show that
the interplay between them helps improve the accuracy of both, and as a result,
effectively parse. A seed bootstrapping technique prepares the data to train
these classifiers. Our analyses further validate that such an approach in
conjunction with weak supervision using prior branching knowledge of a known
language (left/right-branching) and minimal heuristics injects strong inductive
bias into the parser, achieving 63.1 F$_1$ on the English (PTB) test set. In
addition, we show the effectiveness of our architecture by evaluating on
treebanks for Chinese (CTB) and Japanese (KTB) and achieve new state-of-the-art
results. Our code and pre-trained models are available at
https://github.com/Nickil21/weakly-supervised-parsing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Inductive Bias of In-Context Learning: Rethinking Pretraining Example Design. (arXiv:2110.04541v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04541">
<div class="article-summary-box-inner">
<span><p>Pretraining Neural Language Models (NLMs) over a large corpus involves
chunking the text into training examples, which are contiguous text segments of
sizes processable by the neural architecture. We highlight a bias introduced by
this common practice: we prove that the pretrained NLM can model much stronger
dependencies between text segments that appeared in the same training example,
than it can between text segments that appeared in different training examples.
This intuitive result has a twofold role. First, it formalizes the motivation
behind a broad line of recent successful NLM training heuristics, proposed for
the pretraining and fine-tuning stages, which do not necessarily appear related
at first glance. Second, our result clearly indicates further improvements to
be made in NLM pretraining for the benefit of Natural Language Understanding
tasks. As an example, we propose "kNN-Pretraining": we show that including
semantically related non-neighboring sentences in the same pretraining example
yields improved sentence representations and open domain question answering
abilities. This theoretically motivated degree of freedom for pretraining
example design indicates new training schemes for self-improving
representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Internal Language Model Adaptation with Text-Only Data for End-to-End Speech Recognition. (arXiv:2110.05354v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05354">
<div class="article-summary-box-inner">
<span><p>Text-only adaptation of an end-to-end (E2E) model remains a challenging task
for automatic speech recognition (ASR). Language model (LM) fusion-based
approaches require an additional external LM during inference, significantly
increasing the computation cost. To overcome this, we propose an internal LM
adaptation (ILMA) of the E2E model using text-only data. Trained with
audio-transcript pairs, an E2E model implicitly learns an internal LM that
characterizes the token sequence probability which is approximated by the E2E
model output after zeroing out the encoder contribution. During ILMA, we
fine-tune the internal LM, i.e., the E2E components excluding the encoder, to
minimize a cross-entropy loss. To make ILMA effective, it is essential to train
the E2E model with an internal LM loss besides the standard E2E loss.
Furthermore, we propose to regularize ILMA by minimizing the Kullback-Leibler
divergence between the output distributions of the adapted and unadapted
internal LMs. ILMA is the most effective when we update only the last linear
layer of the joint network. ILMA enables a fast text-only adaptation of the E2E
model without increasing the run-time computational cost. Experimented with
30K-hour trained transformer transducer models, ILMA achieves up to 34.9%
relative word error rate reduction from the unadapted baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dict-BERT: Enhancing Language Model Pre-training with Dictionary. (arXiv:2110.06490v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06490">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (PLMs) aim to learn universal language
representations by conducting self-supervised training tasks on large-scale
corpora. Since PLMs capture word semantics in different contexts, the quality
of word representations highly depends on word frequency, which usually follows
a heavy-tailed distributions in the pre-training corpus. Therefore, the
embeddings of rare words on the tail are usually poorly optimized. In this
work, we focus on enhancing language model pre-training by leveraging
definitions of the rare words in dictionaries (e.g., Wiktionary). To
incorporate a rare word definition as a part of input, we fetch its definition
from the dictionary and append it to the end of the input text sequence. In
addition to training with the masked language modeling objective, we propose
two novel self-supervised pre-training tasks on word and sentence-level
alignment between input text sequence and rare word definitions to enhance
language modeling representation with dictionary. We evaluate the proposed
Dict-BERT model on the language understanding benchmark GLUE and eight
specialized domain benchmark datasets. Extensive experiments demonstrate that
Dict-BERT can significantly improve the understanding of rare words and boost
model performance on various NLP downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Morphosyntactic Tagging with Pre-trained Language Models for Arabic and its Dialects. (arXiv:2110.06852v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06852">
<div class="article-summary-box-inner">
<span><p>We present state-of-the-art results on morphosyntactic tagging across
different varieties of Arabic using fine-tuned pre-trained transformer language
models. Our models consistently outperform existing systems in Modern Standard
Arabic and all the Arabic dialects we study, achieving 2.6% absolute
improvement over the previous state-of-the-art in Modern Standard Arabic, 2.8%
in Gulf, 1.6% in Egyptian, and 8.3% in Levantine. We explore different training
setups for fine-tuning pre-trained transformer language models, including
training data size, the use of external linguistic resources, and the use of
annotated data from other dialects in a low-resource scenario. Our results show
that strategic fine-tuning using datasets from other high-resource dialects is
beneficial for a low-resource dialect. Additionally, we show that high-quality
morphological analyzers as external linguistic resources are beneficial
especially in low-resource settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Self-Supervision Objectives for Generalizable Coherence Modeling. (arXiv:2110.07198v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07198">
<div class="article-summary-box-inner">
<span><p>Given the claims of improved text generation quality across various
pre-trained neural models, we consider the coherence evaluation of machine
generated text to be one of the principal applications of coherence models that
needs to be investigated. Prior work in neural coherence modeling has primarily
focused on devising new architectures for solving the permuted document task.
We instead use a basic model architecture and show significant improvements
over state of the art within the same training regime. We then design a harder
self-supervision objective by increasing the ratio of negative samples within a
contrastive learning setup, and enhance the model further through automatic
hard negative mining coupled with a large global negative queue encoded by a
momentum encoder. We show empirically that increasing the density of negative
samples improves the basic model, and using a global negative queue further
improves and stabilizes the model while training with hard negative samples. We
evaluate the coherence model on task-independent test sets that resemble
real-world applications and show significant improvements in coherence
evaluations of downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks. (arXiv:2110.07602v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07602">
<div class="article-summary-box-inner">
<span><p>Prompt tuning, which only tunes continuous prompts with a frozen language
model, substantially reduces per-task storage and memory usage at training.
However, in the context of NLU, prior work reveals that prompt tuning does not
perform well for normal-sized pretrained models. We also find that existing
methods of prompt tuning cannot handle hard sequence labeling tasks, indicating
a lack of universality. We present a novel empirical finding that properly
optimized prompt tuning can be universally effective across a wide range of
model scales and NLU tasks. It matches the performance of finetuning while
having only 0.1%-3% tuned parameters. Our method P-Tuning v2 is an
implementation of Deep Prompt Tuning \cite{li2021prefix,qin2021learning}
optimized and adapted for NLU. Given the universality and simplicity of
P-Tuning v2, we believe it can serve as an alternative to finetuning and a
strong baseline for future research.Our code and data are released at
https://github.com/THUDM/P-tuning-v2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Scale Substitution-based Word Sense Induction. (arXiv:2110.07681v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07681">
<div class="article-summary-box-inner">
<span><p>We present a word-sense induction method based on pre-trained masked language
models (MLMs), which can cheaply scale to large vocabularies and large corpora.
The result is a corpus which is sense-tagged according to a corpus-derived
sense inventory and where each sense is associated with indicative words.
Evaluation on English Wikipedia that was sense-tagged using our method shows
that both the induced senses, and the per-instance sense assignment, are of
high quality even compared to WSD methods, such as Babelfy. Furthermore, by
training a static word embeddings algorithm on the sense-tagged corpus, we
obtain high-quality static senseful embeddings. These outperform existing
senseful embeddings methods on the WiC dataset and on a new outlier detection
dataset we developed. The data driven nature of the algorithm allows to induce
corpora-specific senses, which may not appear in standard sense inventories, as
we demonstrate using a case study on the scientific domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models. (arXiv:2110.08151v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08151">
<div class="article-summary-box-inner">
<span><p>Recent studies have shown that multilingual pretrained language models can be
effectively improved with cross-lingual alignment information from Wikipedia
entities. However, existing methods only exploit entity information in
pretraining and do not explicitly use entities in downstream tasks. In this
study, we explore the effectiveness of leveraging entity representations for
downstream cross-lingual tasks. We train a multilingual language model with 24
languages with entity representations and show the model consistently
outperforms word-based pretrained models in various cross-lingual transfer
tasks. We also analyze the model and the key insight is that incorporating
entity representations into the input allows us to extract more
language-agnostic features. We also evaluate the model with a multilingual
cloze prompt task with the mLAMA dataset. We show that entity-based prompt
elicits correct factual knowledge more likely than using only word
representations. Our source code and pretrained models are available at
https://github.com/studio-ousia/luke.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open Domain Question Answering with A Unified Knowledge Interface. (arXiv:2110.08417v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08417">
<div class="article-summary-box-inner">
<span><p>The retriever-reader framework is popular for open-domain question answering
(ODQA) due to its ability to use explicit knowledge. Although prior work has
sought to increase the knowledge coverage by incorporating structured knowledge
beyond text, accessing heterogeneous knowledge sources through a unified
interface remains an open question. While data-to-text generation has the
potential to serve as a universal interface for data and text, its feasibility
for downstream tasks remains largely unknown. In this work, we bridge this gap
and use the data-to-text method as a means for encoding structured knowledge
for ODQA. Specifically, we propose a verbalizer-retriever-reader framework for
ODQA over data and text where verbalized tables from Wikipedia and graphs from
Wikidata are used as augmented knowledge sources. We show that our Unified Data
and Text QA, UDT-QA, can effectively benefit from the expanded knowledge index,
leading to large gains over text-only baselines. Notably, our approach sets the
single-model state-of-the-art on Natural Questions. Furthermore, our analyses
indicate that verbalized knowledge is preferred for answer reasoning for both
adapted and hot-swap settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Think Before You Speak: Explicitly Generating Implicit Commonsense Knowledge for Response Generation. (arXiv:2110.08501v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08501">
<div class="article-summary-box-inner">
<span><p>Implicit knowledge, such as common sense, is key to fluid human
conversations. Current neural response generation (RG) models are trained to
generate responses directly, omitting unstated implicit knowledge. In this
paper, we present Think-Before-Speaking (TBS), a generative approach to first
externalize implicit commonsense knowledge (think) and use this knowledge to
generate responses (speak). We expect that externalizing implicit knowledge
allows more efficient learning, produces more informative responses, and
enables more explainable models. We analyze different choices to collect
knowledge-aligned dialogues, represent implicit knowledge, and transition
between knowledge and dialogues. Empirical results show TBS models outperform
end-to-end and knowledge-augmented RG baselines on most automatic metrics and
generate more informative, specific, and commonsense-following responses, as
evaluated by human annotators. TBS also generates knowledge that makes sense
and is relevant to the dialogue around 85\% of the time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RDF-to-Text Generation with Reinforcement Learning Based Graph-augmented Structural Neural Encoders. (arXiv:2111.10545v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10545">
<div class="article-summary-box-inner">
<span><p>Considering a collection of RDF triples, the RDF-to-text generation task aims
to generate a text description. Most previous methods solve this task using a
sequence-to-sequence model or using a graph-based model to encode RDF triples
and to generate a text sequence. Nevertheless, these approaches fail to clearly
model the local and global structural information between and within RDF
triples. Moreover, the previous methods also face the non-negligible problem of
low faithfulness of the generated text, which seriously affects the overall
performance of these models. To solve these problems, we propose a model
combining two new graph-augmented structural neural encoders to jointly learn
both local and global structural information in the input RDF triples. To
further improve text faithfulness, we innovatively introduce a reinforcement
learning (RL) reward based on information extraction (IE). We first extract
triples from the generated text using a pretrained IE model and regard the
correct number of the extracted triples as the additional RL reward.
Experimental results on two benchmark datasets demonstrate that our proposed
model outperforms the state-of-the-art baselines, and the additional
reinforcement learning reward does help to improve the faithfulness of the
generated text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open-Domain, Content-based, Multi-modal Fact-checking of Out-of-Context Images via Online Resources. (arXiv:2112.00061v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00061">
<div class="article-summary-box-inner">
<span><p>Misinformation is now a major problem due to its potential high risks to our
core democratic and societal values and orders. Out-of-context misinformation
is one of the easiest and effective ways used by adversaries to spread viral
false stories. In this threat, a real image is re-purposed to support other
narratives by misrepresenting its context and/or elements. The internet is
being used as the go-to way to verify information using different sources and
modalities. Our goal is an inspectable method that automates this
time-consuming and reasoning-intensive process by fact-checking the
image-caption pairing using Web evidence. To integrate evidence and cues from
both modalities, we introduce the concept of 'multi-modal cycle-consistency
check'; starting from the image/caption, we gather textual/visual evidence,
which will be compared against the other paired caption/image, respectively.
Moreover, we propose a novel architecture, Consistency-Checking Network (CCN),
that mimics the layered human reasoning across the same and different
modalities: the caption vs. textual evidence, the image vs. visual evidence,
and the image vs. caption. Our work offers the first step and benchmark for
open-domain, content-based, multi-modal fact-checking, and significantly
outperforms previous baselines that did not leverage external evidence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLIPstyler: Image Style Transfer with a Single Text Condition. (arXiv:2112.00374v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00374">
<div class="article-summary-box-inner">
<span><p>Existing neural style transfer methods require reference style images to
transfer texture information of style images to content images. However, in
many practical situations, users may not have reference style images but still
be interested in transferring styles by just imagining them. In order to deal
with such applications, we propose a new framework that enables a style
transfer `without' a style image, but only with a text description of the
desired style. Using the pre-trained text-image embedding model of CLIP, we
demonstrate the modulation of the style of content images only with a single
text condition. Specifically, we propose a patch-wise text-image matching loss
with multiview augmentations for realistic texture transfer. Extensive
experimental results confirmed the successful image style transfer with
realistic textures that reflect semantic query texts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing Linguistic Information For Logical Inference In Pre-trained Language Models. (arXiv:2112.01753v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01753">
<div class="article-summary-box-inner">
<span><p>Progress in pre-trained language models has led to a surge of impressive
results on downstream tasks for natural language understanding. Recent work on
probing pre-trained language models uncovered a wide range of linguistic
properties encoded in their contextualized representations. However, it is
unclear whether they encode semantic knowledge that is crucial to symbolic
inference methods. We propose a methodology for probing linguistic information
for logical inference in pre-trained language model representations. Our
probing datasets cover a list of linguistic phenomena required by major
symbolic inference systems. We find that (i) pre-trained language models do
encode several types of linguistic information for inference, but there are
also some types of information that are weakly encoded, (ii) language models
can effectively learn missing linguistic information through fine-tuning.
Overall, our findings provide insights into which aspects of linguistic
information for logical inference do language models and their pre-training
procedures capture. Moreover, we have demonstrated language models' potential
as semantic and background knowledge bases for supporting symbolic inference
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse Structure Learning via Graph Neural Networks for Inductive Document Classification. (arXiv:2112.06386v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06386">
<div class="article-summary-box-inner">
<span><p>Recently, graph neural networks (GNNs) have been widely used for document
classification. However, most existing methods are based on static word
co-occurrence graphs without sentence-level information, which poses three
challenges:(1) word ambiguity, (2) word synonymity, and (3) dynamic contextual
dependency. To address these challenges, we propose a novel GNN-based sparse
structure learning model for inductive document classification. Specifically, a
document-level graph is initially generated by a disjoint union of
sentence-level word co-occurrence graphs. Our model collects a set of trainable
edges connecting disjoint words between sentences and employs structure
learning to sparsely select edges with dynamic contextual dependencies. Graphs
with sparse structures can jointly exploit local and global contextual
information in documents through GNNs. For inductive learning, the refined
document graph is further fed into a general readout function for graph-level
classification and optimization in an end-to-end manner. Extensive experiments
on several real-world datasets demonstrate that the proposed model outperforms
most state-of-the-art results, and reveal the necessity to learn sparse
structures for each document.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridging between Cognitive Processing Signals and Linguistic Features via a Unified Attentional Network. (arXiv:2112.08831v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08831">
<div class="article-summary-box-inner">
<span><p>Cognitive processing signals can be used to improve natural language
processing (NLP) tasks. However, it is not clear how these signals correlate
with linguistic information. Bridging between human language processing and
linguistic features has been widely studied in neurolinguistics, usually via
single-variable controlled experiments with highly-controlled stimuli. Such
methods not only compromises the authenticity of natural reading, but also are
time-consuming and expensive. In this paper, we propose a data-driven method to
investigate the relationship between cognitive processing signals and
linguistic features. Specifically, we present a unified attentional framework
that is composed of embedding, attention, encoding and predicting layers to
selectively map cognitive processing signals to linguistic features. We define
the mapping procedure as a bridging task and develop 12 bridging tasks for
lexical, syntactic and semantic features. The proposed framework only requires
cognitive processing signals recorded under natural reading as inputs, and can
be used to detect a wide range of linguistic features with a single cognitive
dataset. Observations from experiment results resonate with previous
neuroscience findings. In addition to this, our experiments also reveal a
number of interesting findings, such as the correlation between contextual
eye-tracking features and tense of sentence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiple-Source Domain Adaptation via Coordinated Domain Encoders and Paired Classifiers. (arXiv:2201.11870v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11870">
<div class="article-summary-box-inner">
<span><p>We present a novel multiple-source unsupervised model for text classification
under domain shift. Our model exploits the update rates in document
representations to dynamically integrate domain encoders. It also employs a
probabilistic heuristic to infer the error rate in the target domain in order
to pair source classifiers. Our heuristic exploits data transformation cost and
the classifier accuracy in the target feature space. We have used real world
scenarios of Domain Adaptation to evaluate the efficacy of our algorithm. We
also used pretrained multi-layer transformers as the document encoder in the
experiments to demonstrate whether the improvement achieved by domain
adaptation models can be delivered by out-of-the-box language model
pretraining. The experiments testify that our model is the top performing
approach in this setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Locating and Editing Factual Knowledge in GPT. (arXiv:2202.05262v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05262">
<div class="article-summary-box-inner">
<span><p>We investigate the mechanisms underlying factual knowledge recall in
autoregressive transformer language models. First, we develop a causal
intervention for identifying neuron activations capable of altering a model's
factual predictions. Within large GPT-style models, this reveals two distinct
sets of neurons that we hypothesize correspond to knowing an abstract fact and
saying a concrete word, respectively. This insight inspires the development of
ROME, a novel method for editing facts stored in model weights. For evaluation,
we assemble CounterFact, a dataset of over twenty thousand counterfactuals and
tools to facilitate sensitive measurements of knowledge editing. Using
CounterFact, we confirm the distinction between saying and knowing neurons, and
we find that ROME achieves state-of-the-art performance in knowledge editing
compared to other methods. An interactive demo notebook, full code
implementation, and the dataset are available at https://rome.baulab.info/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EdgeFormer: A Parameter-Efficient Transformer for On-Device Seq2seq Generation. (arXiv:2202.07959v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07959">
<div class="article-summary-box-inner">
<span><p>We propose EdgeFormer -- a parameter-efficient Transformer of the
encoder-decoder architecture for on-device seq2seq generation, which is
customized under strict computation and memory constraints. EdgeFormer proposes
two novel principles for cost-effective parameterization and further enhance
the model with efficient layer adaptation. We conduct extensive experiments on
two practical on-device seq2seq tasks: Machine Translation and Grammatical
Error Correction, and show that EdgeFormer can effectively outperform previous
parameter-efficient Transformer baselines and achieve very competitive results
with knowledge distillation under both the computation and memory constraints.
Moreover, we release the pretrained EdgeFormer -- the first publicly available
pretrained model that can be easily fine-tuned for English seq2seq tasks with
strong results, largely facilitating on-device seq2seq generation in practice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking and Refining the Distinct Metric. (arXiv:2202.13587v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13587">
<div class="article-summary-box-inner">
<span><p>Distinct-$n$ score\cite{Li2016} is a widely used automatic metric for
evaluating diversity in language generation tasks. However, we observed that
the original approach for calculating distinct scores has evident biases that
tend to assign higher penalties to longer sequences. We refine the calculation
of distinct scores by scaling the number of distinct tokens based on their
expectations. We provide both empirical and theoretical evidence to show that
our method effectively removes the biases existing in the original distinct
score. Our experiments show that our proposed metric,
\textit{Expectation-Adjusted Distinct (EAD)}, correlates better with human
judgment in evaluating response diversity. To foster future research, we
provide an example implementation at
\url{https://github.com/lsy641/Expectation-Adjusted-Distinct}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LCP-dropout: Compression-based Multiple Subword Segmentation for Neural Machine Translation. (arXiv:2202.13590v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13590">
<div class="article-summary-box-inner">
<span><p>In this study, we propose a simple and effective preprocessing method for
subword segmentation based on a data compression algorithm. Compression-based
subword segmentation has recently attracted significant attention as a
preprocessing method for training data in Neural Machine Translation. Among
them, BPE/BPE-dropout is one of the fastest and most effective method compared
to conventional approaches. However, compression-based approach has a drawback
in that generating multiple segmentations is difficult due to the determinism.
To overcome this difficulty, we focus on a probabilistic string algorithm,
called locally-consistent parsing (LCP), that has been applied to achieve
optimum compression. Employing the probabilistic mechanism of LCP, we propose
LCP-dropout for multiple subword segmentation that improves BPE/BPE-dropout,
and show that it outperforms various baselines in learning from especially
small training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TRILLsson: Distilled Universal Paralinguistic Speech Representations. (arXiv:2203.00236v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00236">
<div class="article-summary-box-inner">
<span><p>Recent advances in self-supervision have dramatically improved the quality of
speech representations. However, deployment of state-of-the-art embedding
models on devices has been restricted due to their limited public availability
and large resource footprint. Our work addresses these issues by publicly
releasing a collection of paralinguistic speech models that are small and near
state-of-the-art performance. Our approach is based on knowledge distillation,
and our models are distilled on public data only. We explore different
architectures and thoroughly evaluate our models on the Non-Semantic Speech
(NOSS) benchmark. Our largest distilled model is less than 15% the size of the
original model (314MB vs 2.2GB), achieves over 96% the accuracy on 6 of 7
tasks, and is trained on 6.5% the data. The smallest model is 1% in size (22MB)
and achieves over 90% the accuracy on 6 of 7 tasks. Our models outperform the
open source Wav2Vec 2.0 model on 6 of 7 tasks, and our smallest model
outperforms the open source Wav2Vec 2.0 on both emotion recognition tasks
despite being 7% the size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Conformer Based Acoustic Model for Robust Automatic Speech Recognition. (arXiv:2203.00725v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00725">
<div class="article-summary-box-inner">
<span><p>This study addresses robust automatic speech recognition (ASR) by introducing
a Conformer-based acoustic model. The proposed model builds on a
state-of-the-art recognition system using a bi-directional long short-term
memory (BLSTM) model with utterance-wise dropout and iterative speaker
adaptation, but employs a Conformer encoder instead of the BLSTM network. The
Conformer encoder uses a convolution-augmented attention mechanism for acoustic
modeling. The proposed system is evaluated on the monaural ASR task of the
CHiME-4 corpus. Coupled with utterance-wise normalization and speaker
adaptation, our model achieves $6.25\%$ word error rate, which outperforms the
previous best system by $8.4\%$ relatively. In addition, the proposed
Conformer-based model is $18.3\%$ smaller in model size and reduces total
training time by $79.6\%$.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Just Rank: Rethinking Evaluation with Word and Sentence Similarities. (arXiv:2203.02679v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02679">
<div class="article-summary-box-inner">
<span><p>Word and sentence embeddings are useful feature representations in natural
language processing. However, intrinsic evaluation for embeddings lags far
behind, and there has been no significant update since the past decade. Word
and sentence similarity tasks have become the de facto evaluation method. It
leads models to overfit to such evaluations, negatively impacting embedding
models' development. This paper first points out the problems using semantic
similarity as the gold standard for word and sentence embedding evaluations.
Further, we propose a new intrinsic evaluation method called EvalRank, which
shows a much stronger correlation with downstream tasks. Extensive experiments
are conducted based on 60+ models and popular datasets to certify our
judgments. Finally, the practical evaluation toolkit is released for future
benchmarking purposes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Sketch Induction for Paraphrase Generation. (arXiv:2203.03463v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03463">
<div class="article-summary-box-inner">
<span><p>We propose a generative model of paraphrase generation, that encourages
syntactic diversity by conditioning on an explicit syntactic sketch. We
introduce Hierarchical Refinement Quantized Variational Autoencoders (HRQ-VAE),
a method for learning decompositions of dense encodings as a sequence of
discrete latent variables that make iterative refinements of increasing
granularity. This hierarchy of codes is learned through end-to-end training,
and represents fine-to-coarse grained information about the input. We use
HRQ-VAE to encode the syntactic form of an input sentence as a path through the
hierarchy, allowing us to more easily predict syntactic sketches at test time.
Extensive experiments, including a human evaluation, confirm that HRQ-VAE
learns a hierarchical representation of the input space, and generates
paraphrases of higher quality than previous systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A new approach to calculating BERTScore for automatic assessment of translation quality. (arXiv:2203.05598v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05598">
<div class="article-summary-box-inner">
<span><p>The study of the applicability of the BERTScore metric was conducted to
translation quality assessment at the sentence level for English -&gt; Russian
direction. Experiments were performed with a pre-trained Multilingual BERT as
well as with a pair of Monolingual BERT models. To align monolingual
embeddings, an orthogonal transformation based on anchor tokens was used. It
was demonstrated that such transformation helps to prevent mismatching issue
and shown that this approach gives better results than using embeddings of the
Multilingual model. To improve the token matching process it is proposed to
combine all incomplete WorkPiece tokens into meaningful words and use simple
averaging of corresponding vectors and to calculate BERTScore based on anchor
tokens only. Such modifications allowed us to achieve a better correlation of
the model predictions with human judgments. In addition to evaluating machine
translation, several versions of human translation were evaluated as well, the
problems of this approach were listed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Low-Rank Softmax Can Have Unargmaxable Classes in Theory but Rarely in Practice. (arXiv:2203.06462v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06462">
<div class="article-summary-box-inner">
<span><p>Classifiers in natural language processing (NLP) often have a large number of
output classes. For example, neural language models (LMs) and machine
translation (MT) models both predict tokens from a vocabulary of thousands. The
Softmax output layer of these models typically receives as input a dense
feature representation, which has much lower dimensionality than the output. In
theory, the result is some words may be impossible to be predicted via argmax,
irrespective of input features, and empirically, there is evidence this happens
in small language models. In this paper we ask whether it can happen in
practical large language models and translation models. To do so, we develop
algorithms to detect such \emph{unargmaxable} tokens in public models. We find
that 13 out of 150 models do indeed have such tokens; however, they are very
infrequent and unlikely to impact model quality. We release our code so that
others can inspect their models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chart-to-Text: A Large-Scale Benchmark for Chart Summarization. (arXiv:2203.06486v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06486">
<div class="article-summary-box-inner">
<span><p>Charts are commonly used for exploring data and communicating insights.
Generating natural language summaries from charts can be very helpful for
people in inferring key insights that would otherwise require a lot of
cognitive and perceptual efforts. We present Chart-to-text, a large-scale
benchmark with two datasets and a total of 44,096 charts covering a wide range
of topics and chart types. We explain the dataset construction process and
analyze the datasets. We also introduce a number of state-of-the-art neural
models as baselines that utilize image captioning and data-to-text generation
techniques to tackle two problem variations: one assumes the underlying data
table of the chart is available while the other needs to extract data from
chart images. Our analysis with automatic and human evaluation shows that while
our best models usually generate fluent summaries and yield reasonable BLEU
scores, they also suffer from hallucinations and factual errors as well as
difficulties in correctly explaining complex patterns and trends in charts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Visual-Prompt Temporal Answering Grounding in Medical Instructional Video. (arXiv:2203.06667v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06667">
<div class="article-summary-box-inner">
<span><p>The temporal answering grounding in the video (TAGV) is a new task naturally
deriving from temporal sentence grounding in the video (TSGV). Given an
untrimmed video and a text question, this task aims at locating the matching
span from the video that can semantically answer the question. Existing methods
tend to formulate the TAGV task with a visual span-based question answering
(QA) approach by matching the visual frame span queried by the text question.
However, due to the weak correlations and huge gaps in semantics in features
between the textual question and visual answer, existing methods adopting
visual span predictor fail to perform well in the TAGV task. In this work, we
propose a visual-prompt text span localizing (VPTSL) method, which enhances the
text span localization in the pre-trained language model (PLM) with the visual
highlight features. Specifically, the context query attention is utilized to
perform cross-modal modeling between the textual and visual features. Then, the
highlight features are obtained through the highlight module with a linear
layer to provide the visual prompt. To alleviate the differences in semantics
and correlations between textual and visual features, we design the text span
predictor by encoding the question, the subtitles, and the visual prompt in the
PLM. As a result, the TAGV task is formulated to predict the span of subtitles
matching the answering frame timeline. Extensive experiments on the medical
instructional dataset, namely MedVidQA, show that the proposed VPTSL
outperforms other state-of-the-art (SOTA) methods by 28.36 in mIOU score with a
large margin, which demonstrates the effectiveness of visual prompt and the
text span predictor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Imputing Out-of-Vocabulary Embeddings with LOVE Makes Language Models Robust with Little Cost. (arXiv:2203.07860v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07860">
<div class="article-summary-box-inner">
<span><p>State-of-the-art NLP systems represent inputs with word embeddings, but these
are brittle when faced with Out-of-Vocabulary (OOV) words. To address this
issue, we follow the principle of mimick-like models to generate vectors for
unseen words, by learning the behavior of pre-trained embeddings using only the
surface form of words. We present a simple contrastive learning framework,
LOVE, which extends the word representation of an existing pre-trained language
model (such as BERT), and makes it robust to OOV with few additional
parameters. Extensive evaluations demonstrate that our lightweight model
achieves similar or even better performances than prior competitors, both on
original datasets and on corrupted variants. Moreover, it can be used in a
plug-and-play fashion with FastText and BERT, where it significantly improves
their robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridging the Data Gap between Training and Inference for Unsupervised Neural Machine Translation. (arXiv:2203.08394v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08394">
<div class="article-summary-box-inner">
<span><p>Back-translation is a critical component of Unsupervised Neural Machine
Translation (UNMT), which generates pseudo parallel data from target
monolingual data. A UNMT model is trained on the pseudo parallel data with
translated source, and translates natural source sentences in inference. The
source discrepancy between training and inference hinders the translation
performance of UNMT models. By carefully designing experiments, we identify two
representative characteristics of the data gap in source: (1) style gap (i.e.,
translated vs. natural text style) that leads to poor generalization
capability; (2) content gap that induces the model to produce hallucination
content biased towards the target language. To narrow the data gap, we propose
an online self-training approach, which simultaneously uses the pseudo parallel
data {natural source, translated target} to mimic the inference scenario.
Experimental results on several widely-used language pairs show that our
approach outperforms two strong baselines (XLM and MASS) by remedying the style
and content gaps.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ConTinTin: Continual Learning from Task Instructions. (arXiv:2203.08512v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08512">
<div class="article-summary-box-inner">
<span><p>The mainstream machine learning paradigms for NLP often work with two
underlying presumptions. First, the target task is predefined and static; a
system merely needs to learn to solve it exclusively. Second, the supervision
of a task mainly comes from a set of labeled examples. A question arises: how
to build a system that can keep learning new tasks from their instructions?
This work defines a new learning paradigm ConTinTin (Continual Learning from
Task Instructions), in which a system should learn a sequence of new tasks one
by one, each task is explained by a piece of textual instruction. The system is
required to (i) generate the expected outputs of a new task by learning from
its instruction, (ii) transfer the knowledge acquired from upstream tasks to
help solve downstream tasks (i.e., forward-transfer), and (iii) retain or even
improve the performance on earlier tasks after learning new tasks (i.e.,
backward-transfer). This new problem is studied on a stream of more than 60
tasks, each equipped with an instruction. Technically, our method
InstructionSpeak contains two strategies that make full use of task
instructions to improve forward-transfer and backward-transfer: one is to learn
from negative outputs, the other is to re-visit instructions of previous tasks.
To our knowledge, this is the first time to study ConTinTin in NLP. In addition
to the problem formulation and our promising approach, this work also
contributes to providing rich analyses for the community to better understand
this novel learning problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Morphological Reinflection with Multiple Arguments: An Extended Annotation schema and a Georgian Case Study. (arXiv:2203.08527v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08527">
<div class="article-summary-box-inner">
<span><p>In recent years, a flurry of morphological datasets had emerged, most notably
UniMorph, a multi-lingual repository of inflection tables. However, the flat
structure of the current morphological annotation schema makes the treatment of
some languages quirky, if not impossible, specifically in cases of polypersonal
agreement, where verbs agree with multiple arguments using true affixes. In
this paper, we propose to address this phenomenon by expanding the UniMorph
annotation schema to a hierarchical feature structure that naturally
accommodates complex argument marking. We apply this extended schema to one
such language, Georgian, and provide a human-verified, accurate and balanced
morphological dataset for Georgian verbs. The dataset has 4 times more tables
and 6 times more verb forms compared to the existing UniMorph dataset, covering
all possible variants of argument marking, demonstrating the adequacy of our
proposed scheme. Experiments with a standard reinflection model show that
generalization is easy when the data is split at the form level, but extremely
hard when splitting along lemma lines. Expanding the other languages in
UniMorph to this schema is expected to improve both the coverage, consistency
and interpretability of this benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AI Autonomy: Self-Initiation, Adaptation and Continual Learning. (arXiv:2203.08994v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08994">
<div class="article-summary-box-inner">
<span><p>As more and more AI agents are used in practice, it is time to think about
how to make these agents fully autonomous so that they can (1) learn by
themselves continually in a self-motivated and self-initiated manner rather
than being retrained offline periodically on the initiation of human engineers
and (2) accommodate or adapt to unexpected or novel circumstances. As the
real-world is an open environment that is full of unknowns or novelties,
detecting novelties, characterizing them, accommodating or adapting to them,
and gathering ground-truth training data and incrementally learning the
unknowns/novelties are critical to making the AI agent more and more
knowledgeable and powerful over time. The key challenge is how to automate the
process so that it is carried out continually on the agent's own initiative and
through its own interactions with humans, other agents and the environment just
like human on-the-job learning. This paper proposes a framework (called SOLA)
for this learning paradigm to promote the research of building autonomous and
continual learning enabled AI agents. To show feasibility, an implemented agent
is also described.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Universal Conditional Masked Language Pre-training for Neural Machine Translation. (arXiv:2203.09210v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09210">
<div class="article-summary-box-inner">
<span><p>Pre-trained sequence-to-sequence models have significantly improved Neural
Machine Translation (NMT). Different from prior works where pre-trained models
usually adopt an unidirectional decoder, this paper demonstrates that
pre-training a sequence-to-sequence model but with a bidirectional decoder can
produce notable performance gains for both Autoregressive and
Non-autoregressive NMT. Specifically, we propose CeMAT, a conditional masked
language model pre-trained on large-scale bilingual and monolingual corpora in
many languages. We also introduce two simple but effective methods to enhance
the CeMAT, aligned code-switching &amp; masking and dynamic dual-masking. We
conduct extensive experiments and show that our CeMAT can achieve significant
performance improvement for all scenarios from low- to extremely high-resource
languages, i.e., up to +14.4 BLEU on low resource and +7.9 BLEU improvements on
average for Autoregressive NMT. For Non-autoregressive NMT, we demonstrate it
can also produce consistent performance gains, i.e., up to +5.3 BLEU. To the
best of our knowledge, this is the first work to pre-train a unified model for
fine-tuning on both NMT tasks. Code, data, and pre-trained models are available
at https://github.com/huawei-noah/Pretrained-Language-Model/CeMAT
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhanced Temporal Knowledge Embeddings with Contextualized Language Representations. (arXiv:2203.09590v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09590">
<div class="article-summary-box-inner">
<span><p>With the emerging research effort to integrate structured and unstructured
knowledge, many approaches incorporate factual knowledge into pre-trained
language models (PLMs) and apply the knowledge-enhanced PLMs on downstream NLP
tasks. However, (1) they only consider static factual knowledge, but knowledge
graphs (KGs) also contain temporal facts or events indicating evolutionary
relationships among entities at different timestamps. (2) PLMs cannot be
directly applied to many KG tasks, such as temporal KG completion.
</p>
<p>In this paper, we focus on \textbf{e}nhancing temporal knowledge embeddings
with \textbf{co}ntextualized \textbf{la}nguage representations (ECOLA). We
align structured knowledge contained in temporal knowledge graphs with their
textual descriptions extracted from news articles and propose a novel
knowledge-text prediction task to inject the abundant information from
descriptions into temporal knowledge embeddings. ECOLA jointly optimizes the
knowledge-text prediction objective and the temporal knowledge embeddings,
which can simultaneously take full advantage of textual and knowledge
information. For training ECOLA, we introduce three temporal KG datasets with
aligned textual descriptions. Experimental results on the temporal knowledge
graph completion task show that ECOLA outperforms state-of-the-art temporal KG
models by a large margin. The proposed datasets can serve as new temporal KG
benchmarks and facilitate future research on structured and unstructured
knowledge integration.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">FaceMap: Towards Unsupervised Face Clustering via Map Equation. (arXiv:2203.10090v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10090">
<div class="article-summary-box-inner">
<span><p>Face clustering is an essential task in computer vision due to the explosion
of related applications such as augmented reality or photo album management.
The main challenge of this task lies in the imperfectness of similarities among
image feature representations. Given an existing feature extraction model, it
is still an unresolved problem that how can the inherent characteristics of
similarities of unlabelled images be leveraged to improve the clustering
performance. Motivated by answering the question, we develop an effective
unsupervised method, named as FaceMap, by formulating face clustering as a
process of non-overlapping community detection, and minimizing the entropy of
information flows on a network of images. The entropy is denoted by the map
equation and its minimum represents the least description of paths among images
in expectation. Inspired by observations on the ranked transition probabilities
in the affinity graph constructed from facial images, we develop an outlier
detection strategy to adaptively adjust transition probabilities among images.
Experiments with ablation studies demonstrate that FaceMap significantly
outperforms existing methods and achieves new state-of-the-arts on three
popular large-scale datasets for face clustering, e.g., an absolute improvement
of more than $10\%$ and $4\%$ comparing with prior unsupervised and supervised
methods respectively in terms of average of Pairwise F-score. Our code is
publicly available on github.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Label conditioned segmentation. (arXiv:2203.10091v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10091">
<div class="article-summary-box-inner">
<span><p>Semantic segmentation is an important task in computer vision that is often
tackled with convolutional neural networks (CNNs). A CNN learns to produce
pixel-level predictions through training on pairs of images and their
corresponding ground-truth segmentation labels. For segmentation tasks with
multiple classes, the standard approach is to use a network that computes a
multi-channel probabilistic segmentation map, with each channel representing
one class. In applications where the image grid size (e.g., when it is a 3D
volume) and/or the number of labels is relatively large, the standard
(baseline) approach can become prohibitively expensive for our computational
resources. In this paper, we propose a simple yet effective method to address
this challenge. In our approach, the segmentation network produces a
single-channel output, while being conditioned on a single class label, which
determines the output class of the network. Our method, called label
conditioned segmentation (LCS), can be used to segment images with a very large
number of classes, which might be infeasible for the baseline approach. We also
demonstrate in the experiments that label conditioning can improve the accuracy
of a given backbone architecture, likely, thanks to its parameter efficiency.
Finally, as we show in our results, an LCS model can produce previously unseen
fine-grained labels during inference time, when only coarse labels were
available during training. We provide all of our code here:
https://github.com/tym002/Label-conditioned-segmentation
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Selection of entropy based features for the analysis of the Archimedes' spiral applied to essential tremor. (arXiv:2203.10094v1 [q-bio.QM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10094">
<div class="article-summary-box-inner">
<span><p>Biomedical systems are regulated by interacting mechanisms that operate
across multiple spatial and temporal scales and produce biosignals with linear
and non-linear information inside. In this sense entropy could provide a useful
measure about disorder in the system, lack of information in time-series and/or
irregularity of the signals. Essential tremor (ET) is the most common movement
disorder, being 20 times more common than Parkinson's disease, and 50-70% of
this disease cases are estimated to be genetic in origin. Archimedes spiral
drawing is one of the most used standard tests for clinical diagnosis. This
work, on selection of nonlinear biomarkers from drawings and handwriting, is
part of a wide-ranging cross study for the diagnosis of essential tremor in
BioDonostia Health Institute. Several entropy algorithms are used to generate
nonlinear feayures. The automatic analysis system consists of several Machine
Learning paradigms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AlignTransformer: Hierarchical Alignment of Visual Regions and Disease Tags for Medical Report Generation. (arXiv:2203.10095v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10095">
<div class="article-summary-box-inner">
<span><p>Recently, medical report generation, which aims to automatically generate a
long and coherent descriptive paragraph of a given medical image, has received
growing research interests. Different from the general image captioning tasks,
medical report generation is more challenging for data-driven neural models.
This is mainly due to 1) the serious data bias: the normal visual regions
dominate the dataset over the abnormal visual regions, and 2) the very long
sequence. To alleviate above two problems, we propose an AlignTransformer
framework, which includes the Align Hierarchical Attention (AHA) and the
Multi-Grained Transformer (MGT) modules: 1) AHA module first predicts the
disease tags from the input image and then learns the multi-grained visual
features by hierarchically aligning the visual regions and disease tags. The
acquired disease-grounded visual features can better represent the abnormal
regions of the input image, which could alleviate data bias problem; 2) MGT
module effectively uses the multi-grained features and Transformer framework to
generate the long medical report. The experiments on the public IU-Xray and
MIMIC-CXR datasets show that the AlignTransformer can achieve results
competitive with state-of-the-art methods on the two datasets. Moreover, the
human evaluation conducted by professional radiologists further proves the
effectiveness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards a Perceptual Model for Estimating the Quality of Visual Speech. (arXiv:2203.10117v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10117">
<div class="article-summary-box-inner">
<span><p>Generating realistic lip motions to simulate speech production is key for
driving natural character animations from audio. Previous research has shown
that traditional metrics used to optimize and assess models for generating lip
motions from speech are not a good indicator of subjective opinion of animation
quality. Yet, running repetitive subjective studies for assessing the quality
of animations can be time-consuming and difficult to replicate. In this work,
we seek to understand the relationship between perturbed lip motion and
subjective opinion of lip motion quality. Specifically, we adjust the degree of
articulation for lip motion sequences and run a user-study to examine how this
adjustment impacts the perceived quality of lip motion. We then train a model
using the scores collected from our user-study to automatically predict the
subjective quality of an animated sequence. Our results show that (1) users
score lip motions with slight over-articulation the highest in terms of
perceptual quality; (2) under-articulation had a more detrimental effect on
perceived quality of lip motion compared to the effect of over-articulation;
and (3) we can automatically estimate the subjective perceptual score for a
given lip motion sequences with low error rates.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AI system for fetal ultrasound in low-resource settings. (arXiv:2203.10139v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10139">
<div class="article-summary-box-inner">
<span><p>Despite considerable progress in maternal healthcare, maternal and perinatal
deaths remain high in low-to-middle income countries. Fetal ultrasound is an
important component of antenatal care, but shortage of adequately trained
healthcare workers has limited its adoption. We developed and validated an
artificial intelligence (AI) system that uses novice-acquired "blind sweep"
ultrasound videos to estimate gestational age (GA) and fetal malpresentation.
We further addressed obstacles that may be encountered in low-resourced
settings. Using a simplified sweep protocol with real-time AI feedback on sweep
quality, we have demonstrated the generalization of model performance to
minimally trained novice ultrasound operators using low cost ultrasound devices
with on-device AI integration. The GA model was non-inferior to standard fetal
biometry estimates with as few as two sweeps, and the fetal malpresentation
model had high AUC-ROCs across operators and devices. Our AI models have the
potential to assist in upleveling the capabilities of lightly trained
ultrasound operators in low resource settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Closing the Generalization Gap of Cross-silo Federated Medical Image Segmentation. (arXiv:2203.10144v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10144">
<div class="article-summary-box-inner">
<span><p>Cross-silo federated learning (FL) has attracted much attention in medical
imaging analysis with deep learning in recent years as it can resolve the
critical issues of insufficient data, data privacy, and training efficiency.
However, there can be a generalization gap between the model trained from FL
and the one from centralized training. This important issue comes from the
non-iid data distribution of the local data in the participating clients and is
well-known as client drift. In this work, we propose a novel training framework
FedSM to avoid the client drift issue and successfully close the generalization
gap compared with the centralized training for medical image segmentation tasks
for the first time. We also propose a novel personalized FL objective
formulation and a new method SoftPull to solve it in our proposed framework
FedSM. We conduct rigorous theoretical analysis to guarantee its convergence
for optimizing the non-convex smooth objective function. Real-world medical
image segmentation experiments using deep FL validate the motivations and
effectiveness of our proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ViewFormer: NeRF-free Neural Rendering from Few Images Using Transformers. (arXiv:2203.10157v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10157">
<div class="article-summary-box-inner">
<span><p>Novel view synthesis is a long-standing problem. In this work, we consider a
variant of the problem where we are given only a few context views sparsely
covering a scene or an object. The goal is to predict novel viewpoints in the
scene, which requires learning priors. The current state of the art is based on
Neural Radiance Fields (NeRFs), and while achieving impressive results, the
methods suffer from long training times as they require evaluating thousands of
3D point samples via a deep neural network for each image. We propose a 2D-only
method that maps multiple context views and a query pose to a new image in a
single pass of a neural network. Our model uses a two-stage architecture
consisting of a codebook and a transformer model. The codebook is used to embed
individual images into a smaller latent space, and the transformer solves the
view synthesis task in this more compact space. To train our model efficiently,
we introduce a novel branching attention mechanism that allows us to use the
same model not only for neural rendering but also for camera pose estimation.
Experimental results on real-world scenes show that our approach is competitive
compared to NeRF-based methods while not reasoning in 3D, and it is faster to
train.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discovering Objects that Can Move. (arXiv:2203.10159v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10159">
<div class="article-summary-box-inner">
<span><p>This paper studies the problem of object discovery -- separating objects from
the background without manual labels. Existing approaches utilize appearance
cues, such as color, texture, and location, to group pixels into object-like
regions. However, by relying on appearance alone, these methods fail to
separate objects from the background in cluttered scenes. This is a fundamental
limitation since the definition of an object is inherently ambiguous and
context-dependent. To resolve this ambiguity, we choose to focus on dynamic
objects -- entities that can move independently in the world. We then scale the
recent auto-encoder based frameworks for unsupervised object discovery from toy
synthetic images to complex real-world scenes. To this end, we simplify their
architecture, and augment the resulting model with a weak learning signal from
general motion segmentation algorithms. Our experiments demonstrate that,
despite only capturing a small subset of the objects that move, this signal is
enough to generalize to segment both moving and static instances of dynamic
objects. We show that our model scales to a newly collected, photo-realistic
synthetic dataset with street driving scenarios. Additionally, we leverage
ground truth segmentation and flow annotations in this dataset for thorough
ablation and evaluation. Finally, our experiments on the real-world KITTI
benchmark demonstrate that the proposed approach outperforms both heuristic-
and learning-based methods by capitalizing on motion cues.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Closer Look at Knowledge Distillation with Features, Logits, and Gradients. (arXiv:2203.10163v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10163">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation (KD) is a substantial strategy for transferring
learned knowledge from one neural network model to another. A vast number of
methods have been developed for this strategy. While most method designs a more
efficient way to facilitate knowledge transfer, less attention has been put on
comparing the effect of knowledge sources such as features, logits, and
gradients. This work provides a new perspective to motivate a set of knowledge
distillation strategies by approximating the classical KL-divergence criteria
with different knowledge sources, making a systematic comparison possible in
model compression and incremental learning. Our analysis indicates that logits
are generally a more efficient knowledge source and suggests that having
sufficient feature dimensions is crucial for the model design, providing a
practical guideline for effective KD-based transfer learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Concept-based Adversarial Attacks: Tricking Humans and Classifiers Alike. (arXiv:2203.10166v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10166">
<div class="article-summary-box-inner">
<span><p>We propose to generate adversarial samples by modifying activations of upper
layers encoding semantically meaningful concepts. The original sample is
shifted towards a target sample, yielding an adversarial sample, by using the
modified activations to reconstruct the original sample. A human might (and
possibly should) notice differences between the original and the adversarial
sample. Depending on the attacker-provided constraints, an adversarial sample
can exhibit subtle differences or appear like a "forged" sample from another
class. Our approach and goal are in stark contrast to common attacks involving
perturbations of single pixels that are not recognizable by humans. Our
approach is relevant in, e.g., multi-stage processing of inputs, where both
humans and machines are involved in decision-making because invisible
perturbations will not fool a human. Our evaluation focuses on deep neural
networks. We also show the transferability of our adversarial examples among
networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluation of April Tag and WhyCode Fiducial Systems for Autonomous Precision Drone Landing with a Gimbal-Mounted Camera. (arXiv:2203.10180v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10180">
<div class="article-summary-box-inner">
<span><p>Fiducial markers provide a computationally cheap way for drones to determine
their location with respect to a landing pad and execute precision landings.
However, most existing work in this field uses a fixed, downward facing camera
that does not leverage the common gimbal-mounted camera setup found on many
drones. Such rigid systems cannot easily track detected markers, and may lose
sight of the markers in non-ideal conditions (e.g. wind gusts). This paper
evaluates April Tag and WhyCode fiducial systems for drone landing with a
gimbal-mounted, monocular camera, with the advantage that the drone system can
track the marker over time. However, since the orientation of the camera
changes, we must know the orientation of the marker, which is unreliable in
monocular fiducial systems. Additionally, the system must be fast. We propose 2
methods for mitigating the orientation ambiguity of WhyCode, and 1 method for
increasing the runtime detection rate of April Tag. We evaluate our 3 systems
against 2 default systems in terms of marker orientation ambiguity, and
detection rate. We test rates of marker detection in a ROS framework on a
Raspberry Pi 4, and we rank the systems in terms of their performance. Our
first WhyCode variant significantly reduces orientation ambiguity with an
insignificant reduction in detection rate. Our second WhyCode variant does not
show significantly different orientation ambiguity from the default WhyCode
system, but does provide additional functionality in terms of multi-marker
WhyCode bundle arrangements. Our April Tag variant does not show performance
improvements on a Raspberry Pi 4.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Attacks on Deep Learning-based Video Compression and Classification Systems. (arXiv:2203.10183v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10183">
<div class="article-summary-box-inner">
<span><p>Video compression plays a crucial role in enabling video streaming and
classification systems and maximizing the end-user quality of experience (QoE)
at a given bandwidth budget. In this paper, we conduct the first systematic
study for adversarial attacks on deep learning based video compression and
downstream classification systems. We propose an adaptive adversarial attack
that can manipulate the Rate-Distortion (R-D) relationship of a video
compression model to achieve two adversarial goals: (1) increasing the network
bandwidth or (2) degrading the video quality for end-users. We further devise
novel objectives for targeted and untargeted attacks to a downstream video
classification service. Finally, we design an input-invariant perturbation that
universally disrupts video compression and classification systems in real time.
Unlike previously proposed attacks on video classification, our adversarial
perturbations are the first to withstand compression. We empirically show the
resilience of our attacks against various defenses, i.e., adversarial training,
video denoising, and JPEG compression. Our extensive experimental results on
various video datasets demonstrate the effectiveness of our attacks. Our video
quality and bandwidth attacks deteriorate peak signal-to-noise ratio by up to
5.4dB and the bit-rate by up to 2.4 times on the standard video compression
datasets while achieving over 90% attack success rate on a downstream
classifier.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conditional-Flow NeRF: Accurate 3D Modelling with Reliable Uncertainty Quantification. (arXiv:2203.10192v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10192">
<div class="article-summary-box-inner">
<span><p>A critical limitation of current methods based on Neural Radiance Fields
(NeRF) is that they are unable to quantify the uncertainty associated with the
learned appearance and geometry of the scene. This information is paramount in
real applications such as medical diagnosis or autonomous driving where, to
reduce potentially catastrophic failures, the confidence on the model outputs
must be included into the decision-making process. In this context, we
introduce Conditional-Flow NeRF (CF-NeRF), a novel probabilistic framework to
incorporate uncertainty quantification into NeRF-based approaches. For this
purpose, our method learns a distribution over all possible radiance fields
modelling which is used to quantify the uncertainty associated with the
modelled scene. In contrast to previous approaches enforcing strong constraints
over the radiance field distribution, CF-NeRF learns it in a flexible and fully
data-driven manner by coupling Latent Variable Modelling and Conditional
Normalizing Flows. This strategy allows to obtain reliable uncertainty
estimation while preserving model expressivity. Compared to previous
state-of-the-art methods proposed for uncertainty quantification in NeRF, our
experiments show that the proposed method achieves significantly lower
prediction errors and more reliable uncertainty values for synthetic novel view
and depth-map estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analysis and Adaptation of YOLOv4 for Object Detection in Aerial Images. (arXiv:2203.10194v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10194">
<div class="article-summary-box-inner">
<span><p>The recent and rapid growth in Unmanned Aerial Vehicles (UAVs) deployment for
various computer vision tasks has paved the path for numerous opportunities to
make them more effective and valuable. Object detection in aerial images is
challenging due to variations in appearance, pose, and scale. Autonomous aerial
flight systems with their inherited limited memory and computational power
demand accurate and computationally efficient detection algorithms for
real-time applications. Our work shows the adaptation of the popular YOLOv4
framework for predicting the objects and their locations in aerial images with
high accuracy and inference speed. We utilized transfer learning for faster
convergence of the model on the VisDrone DET aerial object detection dataset.
The trained model resulted in a mean average precision (mAP) of 45.64% with an
inference speed reaching 8.7 FPS on the Tesla K80 GPU and was highly accurate
in detecting truncated and occluded objects. We experimentally evaluated the
impact of varying network resolution sizes and training epochs on the
performance. A comparative study with several contemporary aerial object
detectors proved that YOLOv4 performed better, implying a more suitable
detection algorithm to incorporate on aerial platforms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Morphological Feature Perturbations for Calibrated Semi-Supervised Segmentation. (arXiv:2203.10196v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10196">
<div class="article-summary-box-inner">
<span><p>We propose MisMatch, a novel consistency-driven semi-supervised segmentation
framework which produces predictions that are invariant to learnt feature
perturbations. MisMatch consists of an encoder and a two-head decoders. One
decoder learns positive attention to the foreground regions of interest (RoI)
on unlabelled images thereby generating dilated features. The other decoder
learns negative attention to the foreground on the same unlabelled images
thereby generating eroded features. We then apply a consistency regularisation
on the paired predictions. MisMatch outperforms state-of-the-art
semi-supervised methods on a CT-based pulmonary vessel segmentation task and a
MRI-based brain tumour segmentation task. In addition, we show that the
effectiveness of MisMatch comes from better model calibration than its
supervised learning counterpart.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relationformer: A Unified Framework for Image-to-Graph Generation. (arXiv:2203.10202v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10202">
<div class="article-summary-box-inner">
<span><p>A comprehensive representation of an image requires understanding objects and
their mutual relationship, especially in image-to-graph generation, e.g., road
network extraction, blood-vessel network extraction, or scene graph generation.
Traditionally, image-to-graph generation is addressed with a two-stage approach
consisting of object detection followed by a separate relation prediction,
which prevents simultaneous object-relation interaction. This work proposes a
unified one-stage transformer-based framework, namely Relationformer, that
jointly predicts objects and their relations. We leverage direct set-based
object prediction and incorporate the interaction among the objects to learn an
object-relation representation jointly. In addition to existing [obj]-tokens,
we propose a novel learnable token, namely [rln]-token. Together with
[obj]-tokens, [rln]-token exploits local and global semantic reasoning in an
image through a series of mutual associations. In combination with the
pair-wise [obj]-token, the [rln]-token contributes to a computationally
efficient relation prediction. We achieve state-of-the-art performance on
multiple, diverse and multi-domain datasets that demonstrate our approach's
effectiveness and generalizability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inferring topological transitions in pattern-forming processes with self-supervised learning. (arXiv:2203.10204v1 [cond-mat.mtrl-sci])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10204">
<div class="article-summary-box-inner">
<span><p>The identification and classification of transitions in topological and
microstructural regimes in pattern-forming processes is critical for
understanding and fabricating microstructurally precise novel materials in many
application domains. Unfortunately, relevant microstructure transitions may
depend on process parameters in subtle and complex ways that are not captured
by the classic theory of phase transition. While supervised machine learning
methods may be useful for identifying transition regimes, they need labels
which require prior knowledge of order parameters or relevant structures.
Motivated by the universality principle for dynamical systems, we instead use a
self-supervised approach to solve the inverse problem of predicting process
parameters from observed microstructures using neural networks. This approach
does not require labeled data about the target task of predicting
microstructure transitions. We show that the difficulty of performing this
prediction task is related to the goal of discovering microstructure regimes,
because qualitative changes in microstructural patterns correspond to changes
in uncertainty for our self-supervised prediction problem. We demonstrate the
value of our approach by automatically discovering transitions in
microstructural regimes in two distinct pattern-forming processes: the spinodal
decomposition of a two-phase mixture and the formation of concentration
modulations of binary alloys during physical vapor deposition of thin films.
This approach opens a promising path forward for discovering and understanding
unseen or hard-to-detect transition regimes, and ultimately for controlling
complex pattern-forming processes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SwinTextSpotter: Scene Text Spotting via Better Synergy between Text Detection and Text Recognition. (arXiv:2203.10209v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10209">
<div class="article-summary-box-inner">
<span><p>End-to-end scene text spotting has attracted great attention in recent years
due to the success of excavating the intrinsic synergy of the scene text
detection and recognition. However, recent state-of-the-art methods usually
incorporate detection and recognition simply by sharing the backbone, which
does not directly take advantage of the feature interaction between the two
tasks. In this paper, we propose a new end-to-end scene text spotting framework
termed SwinTextSpotter. Using a transformer encoder with dynamic head as the
detector, we unify the two tasks with a novel Recognition Conversion mechanism
to explicitly guide text localization through recognition loss. The
straightforward design results in a concise framework that requires neither
additional rectification module nor character-level annotation for the
arbitrarily-shaped text. Qualitative and quantitative experiments on
multi-oriented datasets RoIC13 and ICDAR 2015, arbitrarily-shaped datasets
Total-Text and CTW1500, and multi-lingual datasets ReCTS (Chinese) and VinText
(Vietnamese) demonstrate SwinTextSpotter significantly outperforms existing
methods. Code is available at https://github.com/mxin262/SwinTextSpotter.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Learning of 3D Semantic Keypoints with Mutual Reconstruction. (arXiv:2203.10212v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10212">
<div class="article-summary-box-inner">
<span><p>Semantic 3D keypoints are category-level semantic consistent points on 3D
objects. Detecting 3D semantic keypoints is a foundation for a number of 3D
vision tasks but remains challenging, due to the ambiguity of semantic
information, especially when the objects are represented by unordered 3D point
clouds. Existing unsupervised methods tend to generate category-level keypoints
in implicit manners, making it difficult to extract high-level information,
such as semantic labels and topology. From a novel mutual reconstruction
perspective, we present an unsupervised method to generate consistent semantic
keypoints from point clouds explicitly. To achieve this, the proposed model
predicts keypoints that not only reconstruct the object itself but also
reconstruct other instances in the same category. To the best of our knowledge,
the proposed method is the first to mine 3D semantic consistent keypoints from
a mutual reconstruction view. Experiments under various evaluation metrics as
well as comparisons with the state-of-the-arts demonstrate the efficacy of our
new solution to mining semantic consistent keypoints with mutual
reconstruction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Volkit: A Performance-Portable Computer Vision Library for 3D Volumetric Data. (arXiv:2203.10213v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10213">
<div class="article-summary-box-inner">
<span><p>We present volkit, an open source library with high performance
implementations of image manipulation and computer vision algorithms that focus
on 3D volumetric representations. Volkit implements a cross-platform,
performance-portable API targeting both CPUs and GPUs that defers data and
resource movement and hides them from the application developer using a managed
API. We use volkit to process medical and simulation data that is rendered in
VR and consequently integrated the library into the C++ virtual reality
software CalVR. The paper presents case studies and performance results and by
that demonstrates the library's effectiveness and the efficiency of this
approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DirecFormer: A Directed Attention in Transformer Approach to Robust Action Recognition. (arXiv:2203.10233v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10233">
<div class="article-summary-box-inner">
<span><p>Human action recognition has recently become one of the popular research
topics in the computer vision community. Various 3D-CNN based methods have been
presented to tackle both the spatial and temporal dimensions in the task of
video action recognition with competitive results. However, these methods have
suffered some fundamental limitations such as lack of robustness and
generalization, e.g., how does the temporal ordering of video frames affect the
recognition results? This work presents a novel end-to-end Transformer-based
Directed Attention (DirecFormer) framework for robust action recognition. The
method takes a simple but novel perspective of Transformer-based approach to
understand the right order of sequence actions. Therefore, the contributions of
this work are three-fold. Firstly, we introduce the problem of ordered temporal
learning issues to the action recognition problem. Secondly, a new Directed
Attention mechanism is introduced to understand and provide attentions to human
actions in the right order. Thirdly, we introduce the conditional dependency in
action sequence modeling that includes orders and classes. The proposed
approach consistently achieves the state-of-the-art (SOTA) results compared
with the recent action recognition methods, on three standard large-scale
benchmarks, i.e. Jester, Kinetics-400 and Something-Something-V2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HIPA: Hierarchical Patch Transformer for Single Image Super Resolution. (arXiv:2203.10247v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10247">
<div class="article-summary-box-inner">
<span><p>Transformer-based architectures start to emerge in single image super
resolution (SISR) and have achieved promising performance. Most existing Vision
Transformers divide images into the same number of patches with a fixed size,
which may not be optimal for restoring patches with different levels of texture
richness. This paper presents HIPA, a novel Transformer architecture that
progressively recovers the high resolution image using a hierarchical patch
partition. Specifically, we build a cascaded model that processes an input
image in multiple stages, where we start with tokens with small patch sizes and
gradually merge to the full resolution. Such a hierarchical patch mechanism not
only explicitly enables feature aggregation at multiple resolutions but also
adaptively learns patch-aware features for different image regions, e.g., using
a smaller patch for areas with fine details and a larger patch for textureless
regions. Meanwhile, a new attention-based position encoding scheme for
Transformer is proposed to let the network focus on which tokens should be paid
more attention by assigning different weights to different tokens, which is the
first time to our best knowledge. Furthermore, we also propose a new
multi-reception field attention module to enlarge the convolution reception
field from different branches. The experimental results on several public
datasets demonstrate the superior performance of the proposed HIPA over
previous methods quantitatively and qualitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Representation-Agnostic Shape Fields. (arXiv:2203.10259v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10259">
<div class="article-summary-box-inner">
<span><p>3D shape analysis has been widely explored in the era of deep learning.
Numerous models have been developed for various 3D data representation formats,
e.g., MeshCNN for meshes, PointNet for point clouds and VoxNet for voxels. In
this study, we present Representation-Agnostic Shape Fields (RASF), a
generalizable and computation-efficient shape embedding module for 3D deep
learning. RASF is implemented with a learnable 3D grid with multiple channels
to store local geometry. Based on RASF, shape embeddings for various 3D shape
representations (point clouds, meshes and voxels) are retrieved by coordinate
indexing. While there are multiple ways to optimize the learnable parameters of
RASF, we provide two effective schemes among all in this paper for RASF
pre-training: shape reconstruction and normal estimation. Once trained, RASF
becomes a plug-and-play performance booster with negligible cost. Extensive
experiments on diverse 3D representation formats, networks and applications,
validate the universal effectiveness of the proposed RASF. Code and pre-trained
models are publicly available https://github.com/seanywang0408/RASF
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Self-Supervised Low-Rank Network for Single-Stage Weakly and Semi-Supervised Semantic Segmentation. (arXiv:2203.10278v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10278">
<div class="article-summary-box-inner">
<span><p>Semantic segmentation with limited annotations, such as weakly supervised
semantic segmentation (WSSS) and semi-supervised semantic segmentation (SSSS),
is a challenging task that has attracted much attention recently. Most leading
WSSS methods employ a sophisticated multi-stage training strategy to estimate
pseudo-labels as precise as possible, but they suffer from high model
complexity. In contrast, there exists another research line that trains a
single network with image-level labels in one training cycle. However, such a
single-stage strategy often performs poorly because of the compounding effect
caused by inaccurate pseudo-label estimation. To address this issue, this paper
presents a Self-supervised Low-Rank Network (SLRNet) for single-stage WSSS and
SSSS. The SLRNet uses cross-view self-supervision, that is, it simultaneously
predicts several complementary attentive LR representations from different
views of an image to learn precise pseudo-labels. Specifically, we reformulate
the LR representation learning as a collective matrix factorization problem and
optimize it jointly with the network learning in an end-to-end manner. The
resulting LR representation deprecates noisy information while capturing stable
semantics across different views, making it robust to the input variations,
thereby reducing overfitting to self-supervision errors. The SLRNet can provide
a unified single-stage framework for various label-efficient semantic
segmentation settings: 1) WSSS with image-level labeled data, 2) SSSS with a
few pixel-level labeled data, and 3) SSSS with a few pixel-level labeled data
and many image-level labeled data. Extensive experiments on the Pascal VOC
2012, COCO, and L2ID datasets demonstrate that our SLRNet outperforms both
state-of-the-art WSSS and SSSS methods with a variety of different settings,
proving its good generalizability and efficacy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Motion Ambiguity and Alignment for High-Quality Video Frame Interpolation. (arXiv:2203.10291v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10291">
<div class="article-summary-box-inner">
<span><p>For video frame interpolation (VFI), existing deep-learning-based approaches
strongly rely on the ground-truth (GT) intermediate frames, which sometimes
ignore the non-unique nature of motion judging from the given adjacent frames.
As a result, these methods tend to produce averaged solutions that are not
clear enough. To alleviate this issue, we propose to relax the requirement of
reconstructing an intermediate frame as close to the GT as possible. Towards
this end, we develop a texture consistency loss (TCL) upon the assumption that
the interpolated content should maintain similar structures with their
counterparts in the given frames. Predictions satisfying this constraint are
encouraged, though they may differ from the pre-defined GT. Without the bells
and whistles, our plug-and-play TCL is capable of improving the performance of
existing VFI frameworks. On the other hand, previous methods usually adopt the
cost volume or correlation map to achieve more accurate image/feature warping.
However, the O(N^2) ({N refers to the pixel count}) computational complexity
makes it infeasible for high-resolution cases. In this work, we design a
simple, efficient (O(N)) yet powerful cross-scale pyramid alignment (CSPA)
module, where multi-scale information is highly exploited. Extensive
experiments justify the efficiency and effectiveness of the proposed strategy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incremental Few-Shot Learning via Implanting and Compressing. (arXiv:2203.10297v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10297">
<div class="article-summary-box-inner">
<span><p>This work focuses on tackling the challenging but realistic visual task of
Incremental Few-Shot Learning (IFSL), which requires a model to continually
learn novel classes from only a few examples while not forgetting the base
classes on which it was pre-trained. Our study reveals that the challenges of
IFSL lie in both inter-class separation and novel-class representation. Dur to
intra-class variation, a novel class may implicitly leverage the knowledge from
multiple base classes to construct its feature representation. Hence, simply
reusing the pre-trained embedding space could lead to a scattered feature
distribution and result in category confusion. To address such issues, we
propose a two-step learning strategy referred to as \textbf{Im}planting and
\textbf{Co}mpressing (\textbf{IMCO}), which optimizes both feature space
partition and novel class reconstruction in a systematic manner. Specifically,
in the \textbf{Implanting} step, we propose to mimic the data distribution of
novel classes with the assistance of data-abundant base set, so that a model
could learn semantically-rich features that are beneficial for discriminating
between the base and other unseen classes. In the \textbf{Compressing} step, we
adapt the feature extractor to precisely represent each novel class for
enhancing intra-class compactness, together with a regularized parameter
updating rule for preventing aggressive model updating. Finally, we demonstrate
that IMCO outperforms competing baselines with a significant margin, both in
image classification task and more challenging object detection task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modelling nonlinear dependencies in the latent space of inverse scattering. (arXiv:2203.10307v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10307">
<div class="article-summary-box-inner">
<span><p>The problem of inverse scattering proposed by Angles and Mallat in 2018,
concerns training a deep neural network to invert the scattering transform
applied to an image. After such a network is trained, it can be used as a
generative model given that we can sample from the distribution of principal
components of scattering coefficients. For this purpose, Angles and Mallat
simply use samples from independent Gaussians. However, as shown in this paper,
the distribution of interest can actually be very far from normal and
non-negligible dependencies might exist between different coefficients. This
motivates using models for this distribution that allow for non-linear
dependencies between variables. Within this paper, two such models are
explored, namely a Variational AutoEncoder and a Generative Adversarial
Network. We demonstrate the results obtained can be extremely realistic on some
datasets and look better than those produced by Angles and Mallat. The
conducted meta-analysis also shows a clear practical advantage of such
constructed generative models in terms of the efficiency of their training
process compared to existing generative models for images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Voxel Set Transformer: A Set-to-Set Approach to 3D Object Detection from Point Clouds. (arXiv:2203.10314v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10314">
<div class="article-summary-box-inner">
<span><p>Transformer has demonstrated promising performance in many 2D vision tasks.
However, it is cumbersome to compute the self-attention on large-scale point
cloud data because point cloud is a long sequence and unevenly distributed in
3D space. To solve this issue, existing methods usually compute self-attention
locally by grouping the points into clusters of the same size, or perform
convolutional self-attention on a discretized representation. However, the
former results in stochastic point dropout, while the latter typically has
narrow attention fields. In this paper, we propose a novel voxel-based
architecture, namely Voxel Set Transformer (VoxSeT), to detect 3D objects from
point clouds by means of set-to-set translation. VoxSeT is built upon a
voxel-based set attention (VSA) module, which reduces the self-attention in
each voxel by two cross-attentions and models features in a hidden space
induced by a group of latent codes. With the VSA module, VoxSeT can manage
voxelized point clusters with arbitrary size in a wide range, and process them
in parallel with linear complexity. The proposed VoxSeT integrates the high
performance of transformer with the efficiency of voxel-based model, which can
be used as a good alternative to the convolutional and point-based backbones.
VoxSeT reports competitive results on the KITTI and Waymo detection benchmarks.
The source codes can be found at \url{https://github.com/skyhehe123/VoxSeT}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Adaptation Meets Zero-Shot Learning: An Annotation-Efficient Approach to Multi-Modality Medical Image Segmentation. (arXiv:2203.10332v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10332">
<div class="article-summary-box-inner">
<span><p>Due to the lack of properly annotated medical data, exploring the
generalization capability of the deep model is becoming a public concern.
Zero-shot learning (ZSL) has emerged in recent years to equip the deep model
with the ability to recognize unseen classes. However, existing studies mainly
focus on natural images, which utilize linguistic models to extract auxiliary
information for ZSL. It is impractical to apply the natural image ZSL solutions
directly to medical images, since the medical terminology is very
domain-specific, and it is not easy to acquire linguistic models for the
medical terminology. In this work, we propose a new paradigm of ZSL
specifically for medical images utilizing cross-modality information. We make
three main contributions with the proposed paradigm. First, we extract the
prior knowledge about the segmentation targets, called relation prototypes,
from the prior model and then propose a cross-modality adaptation module to
inherit the prototypes to the zero-shot model. Second, we propose a relation
prototype awareness module to make the zero-shot model aware of information
contained in the prototypes. Last but not least, we develop an inheritance
attention module to recalibrate the relation prototypes to enhance the
inheritance process. The proposed framework is evaluated on two public
cross-modality datasets including a cardiac dataset and an abdominal dataset.
Extensive experiments show that the proposed framework significantly
outperforms the state of the arts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TO-FLOW: Efficient Continuous Normalizing Flows with Temporal Optimization adjoint with Moving Speed. (arXiv:2203.10335v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10335">
<div class="article-summary-box-inner">
<span><p>Continuous normalizing flows (CNFs) construct invertible mappings between an
arbitrary complex distribution and an isotropic Gaussian distribution using
Neural Ordinary Differential Equations (neural ODEs). It has not been tractable
on large datasets due to the incremental complexity of the neural ODE training.
Optimal Transport theory has been applied to regularize the dynamics of the ODE
to speed up training in recent works. In this paper, a temporal optimization is
proposed by optimizing the evolutionary time for forward propagation of the
neural ODE training. In this appoach, we optimize the network weights of the
CNF alternately with evolutionary time by coordinate descent. Further with
temporal regularization, stability of the evolution is ensured. This approach
can be used in conjunction with the original regularization approach. We have
experimentally demonstrated that the proposed approach can significantly
accelerate training without sacrifying performance over baseline models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Occlusion-Aware Self-Supervised Monocular 6D Object Pose Estimation. (arXiv:2203.10339v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10339">
<div class="article-summary-box-inner">
<span><p>6D object pose estimation is a fundamental yet challenging problem in
computer vision. Convolutional Neural Networks (CNNs) have recently proven to
be capable of predicting reliable 6D pose estimates even under monocular
settings. Nonetheless, CNNs are identified as being extremely data-driven, and
acquiring adequate annotations is oftentimes very time-consuming and labor
intensive. To overcome this limitation, we propose a novel monocular 6D pose
estimation approach by means of self-supervised learning, removing the need for
real annotations. After training our proposed network fully supervised with
synthetic RGB data, we leverage current trends in noisy student training and
differentiable rendering to further self-supervise the model on these
unsupervised real RGB(-D) samples, seeking for a visually and geometrically
optimal alignment. Moreover, employing both visible and amodal mask
information, our self-supervision becomes very robust towards challenging
scenarios such as occlusion. Extensive evaluations demonstrate that our
proposed self-supervision outperforms all other methods relying on synthetic
data or employing elaborate techniques from the domain adaptation realm.
Noteworthy, our self-supervised approach consistently improves over its
synthetically trained baseline and often almost closes the gap towards its
fully supervised counterpart. The code and models are publicly available at
https://github.com/THU-DA-6D-Pose-Group/self6dpp.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">No Shifted Augmentations (NSA): compact distributions for robust self-supervised Anomaly Detection. (arXiv:2203.10344v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10344">
<div class="article-summary-box-inner">
<span><p>Unsupervised Anomaly detection (AD) requires building a notion of normalcy,
distinguishing in-distribution (ID) and out-of-distribution (OOD) data, using
only available ID samples. Recently, large gains were made on this task for the
domain of natural images using self-supervised contrastive feature learning as
a first step followed by kNN or traditional one-class classifiers for feature
scoring. Learned representations that are non-uniformly distributed on the unit
hypersphere have been shown to be beneficial for this task. We go a step
further and investigate how the \emph {geometrical compactness} of the ID
feature distribution makes isolating and detecting outliers easier, especially
in the realistic situation when ID training data is polluted (i.e. ID data
contains some OOD data that is used for learning the feature extractor
parameters). We propose novel architectural modifications to the
self-supervised feature learning step, that enable such compact distributions
for ID data to be learned. We show that the proposed modifications can be
effectively applied to most existing self-supervised objectives, with large
gains in performance. Furthermore, this improved OOD performance is obtained
without resorting to tricks such as using strongly augmented ID images (e.g. by
90 degree rotations) as proxies for the unseen OOD data, as these impose overly
prescriptive assumptions about ID data and its invariances. We perform
extensive studies on benchmark datasets for one-class OOD detection and show
state-of-the-art performance in the presence of pollution in the ID data, and
comparable performance otherwise. We also propose and extensively evaluate a
novel feature scoring technique based on the angular Mahalanobis distance, and
propose a simple and novel technique for feature ensembling during evaluation
that enables a big boost in performance at nearly zero run-time cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Font Generation with Missing Impression Labels. (arXiv:2203.10348v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10348">
<div class="article-summary-box-inner">
<span><p>Our goal is to generate fonts with specific impressions, by training a
generative adversarial network with a font dataset with impression labels. The
main difficulty is that font impression is ambiguous and the absence of an
impression label does not always mean that the font does not have the
impression. This paper proposes a font generation model that is robust against
missing impression labels. The key ideas of the proposed method are (1)a
co-occurrence-based missing label estimator and (2)an impression label space
compressor. The first is to interpolate missing impression labels based on the
co-occurrence of labels in the dataset and use them for training the model as
completed label conditions. The second is an encoder-decoder module to compress
the high-dimensional impression space into low-dimensional. We proved that the
proposed model generates high-quality font images using multi-label data with
missing labels through qualitative and quantitative evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLRNet: Cross Layer Refinement Network for Lane Detection. (arXiv:2203.10350v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10350">
<div class="article-summary-box-inner">
<span><p>Lane is critical in the vision navigation system of the intelligent vehicle.
Naturally, lane is a traffic sign with high-level semantics, whereas it owns
the specific local pattern which needs detailed low-level features to localize
accurately. Using different feature levels is of great importance for accurate
lane detection, but it is still under-explored. In this work, we present Cross
Layer Refinement Network (CLRNet) aiming at fully utilizing both high-level and
low-level features in lane detection. In particular, it first detects lanes
with high-level semantic features then performs refinement based on low-level
features. In this way, we can exploit more contextual information to detect
lanes while leveraging local detailed lane features to improve localization
accuracy. We present ROIGather to gather global context, which further enhances
the feature representation of lanes. In addition to our novel network design,
we introduce Line IoU loss which regresses the lane line as a whole unit to
improve the localization accuracy. Experiments demonstrate that the proposed
method greatly outperforms the state-of-the-art lane detection approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Domain Multi-Definition Landmark Localization for Small Datasets. (arXiv:2203.10358v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10358">
<div class="article-summary-box-inner">
<span><p>We present a novel method for multi image domain and multi-landmark
definition learning for small dataset facial localization. Training a small
dataset alongside a large(r) dataset helps with robust learning for the former,
and provides a universal mechanism for facial landmark localization for new
and/or smaller standard datasets. To this end, we propose a Vision Transformer
encoder with a novel decoder with a definition agnostic shared landmark
semantic group structured prior, that is learnt, as we train on more than one
dataset concurrently. Due to our novel definition agnostic group prior the
datasets may vary in landmark definitions and domains. During the decoder stage
we use cross- and self-attention, whose output is later fed into
domain/definition specific heads that minimize a Laplacian-log-likelihood loss.
We achieve state-of-the-art performance on standard landmark localization
datasets such as COFW and WFLW, when trained with a bigger dataset. We also
show state-of-the-art performance on several varied image domain small datasets
for animals, caricatures, and facial portrait paintings. Further, we contribute
a small dataset (150 images) of pareidolias to show efficacy of our method.
Finally, we provide several analysis and ablation studies to justify our
claims.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ALAP-AE: As-Lite-as-Possible Auto-Encoder. (arXiv:2203.10363v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10363">
<div class="article-summary-box-inner">
<span><p>We present a novel algorithm to reduce tensor compute required by a
conditional image generation autoencoder and make it as-lite-as-possible,
without sacrificing quality of photo-realistic image generation. Our method is
device agnostic, and can optimize an autoencoder for a given CPU-only, GPU
compute device(s) in about normal time it takes to train an autoencoder on a
generic workstation. We achieve this via a two-stage novel strategy where,
first, we condense the channel weights, such that, as few as possible channels
are used. Then, we prune the nearly zeroed out weight activations, and
fine-tune this lite autoencoder. To maintain image quality, fine-tuning is done
via student-teacher training, where we reuse the condensed autoencoder as the
teacher. We show performance gains for various conditional image generation
tasks: segmentation mask to face images, face images to cartoonization, and
finally CycleGAN-based model on horse to zebra dataset over multiple compute
devices. We perform various ablation studies to justify the claims and design
choices, and achieve real-time versions of various autoencoders on CPU-only
devices while maintaining image quality, thus enabling at-scale deployment of
such autoencoders.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A naive method to discover directions in the StyleGAN2 latent space. (arXiv:2203.10373v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10373">
<div class="article-summary-box-inner">
<span><p>Several research groups have shown that Generative Adversarial Networks
(GANs) can generate photo-realistic images in recent years. Using the GANs, a
map is created between a latent code and a photo-realistic image. This process
can also be reversed: given a photo as input, it is possible to obtain the
corresponding latent code. In this paper, we will show how the inversion
process can be easily exploited to interpret the latent space and control the
output of StyleGAN2, a GAN architecture capable of generating photo-realistic
faces. From a biological perspective, facial features such as nose size depend
on important genetic factors, and we explore the latent spaces that correspond
to such biological features, including masculinity and eye colour. We show the
results obtained by applying the proposed method to a set of photos extracted
from the CelebA-HQ database. We quantify some of these measures by utilizing
two landmarking protocols, and evaluate their robustness through statistical
analysis. Finally we correlate these measures with the input parameters used to
perturb the latent spaces along those interpretable directions. Our results
contribute towards building the groundwork of using such GAN architecture in
forensics to generate photo-realistic faces that satisfy certain biological
attributes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PressureVision: Estimating Hand Pressure from a Single RGB Image. (arXiv:2203.10385v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10385">
<div class="article-summary-box-inner">
<span><p>People often interact with their surroundings by applying pressure with their
hands. Machine perception of hand pressure has been limited by the challenges
of placing sensors between the hand and the contact surface. We explore the
possibility of using a conventional RGB camera to infer hand pressure. The
central insight is that the application of pressure by a hand results in
informative appearance changes. Hands share biomechanical properties that
result in similar observable phenomena, such as soft-tissue deformation, blood
distribution, hand pose, and cast shadows. We collected videos of 36
participants with diverse skin tone applying pressure to an instrumented planar
surface. We then trained a deep model (PressureVisionNet) to infer a pressure
image from a single RGB image. Our model infers pressure for participants
outside of the training data and outperforms baselines. We also show that the
output of our model depends on the appearance of the hand and cast shadows near
contact regions. Overall, our results suggest the appearance of a previously
unobserved human hand can be used to accurately infer applied pressure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Robust Semantic Segmentation of Accident Scenes via Multi-Source Mixed Sampling and Meta-Learning. (arXiv:2203.10395v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10395">
<div class="article-summary-box-inner">
<span><p>Autonomous vehicles utilize urban scene segmentation to understand the real
world like a human and react accordingly. Semantic segmentation of normal
scenes has experienced a remarkable rise in accuracy on conventional
benchmarks. However, a significant portion of real-life accidents features
abnormal scenes, such as those with object deformations, overturns, and
unexpected traffic behaviors. Since even small mis-segmentation of driving
scenes can lead to serious threats to human lives, the robustness of such
models in accident scenarios is an extremely important factor in ensuring
safety of intelligent transportation systems.
</p>
<p>In this paper, we propose a Multi-source Meta-learning Unsupervised Domain
Adaptation (MMUDA) framework, to improve the generalization of segmentation
transformers to extreme accident scenes. In MMUDA, we make use of Multi-Domain
Mixed Sampling to augment the images of multiple-source domains (normal scenes)
with the target data appearances (abnormal scenes). To train our model, we
intertwine and study a meta-learning strategy in the multi-source setting for
robustifying the segmentation results. We further enhance the segmentation
backbone (SegFormer) with a HybridASPP decoder design, featuring large window
attention spatial pyramid pooling and strip pooling, to efficiently aggregate
long-range contextual dependencies. Our approach achieves a mIoU score of
46.97% on the DADA-seg benchmark, surpassing the previous state-of-the-art
model by more than 7.50%. Code will be made publicly available at
https://github.com/xinyu-laura/MMUDA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attri-VAE: attribute-based, disentangled and interpretable representations of medical images with variational autoencoders. (arXiv:2203.10417v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10417">
<div class="article-summary-box-inner">
<span><p>Deep learning (DL) methods where interpretability is intrinsically considered
as part of the model are required to better understand the relationship of
clinical and imaging-based attributes with DL outcomes, thus facilitating their
use in reasoning medical decisions. Latent space representations built with
variational autoencoders (VAE) do not ensure individual control of data
attributes. Attribute-based methods enforcing attribute disentanglement have
been proposed in the literature for classical computer vision tasks in
benchmark data. In this paper, we propose a VAE approach, the Attri-VAE, that
includes an attribute regularization term to associate clinical and medical
imaging attributes with different regularized dimensions in the generated
latent space, enabling a better disentangled interpretation of the attributes.
Furthermore, the generated attention maps explained the attribute encoding in
the regularized latent space dimensions. The Attri-VAE approach analyzed
healthy and myocardial infarction patients with clinical, cardiac morphology,
and radiomics attributes. The proposed model provided an excellent trade-off
between reconstruction fidelity, disentanglement, and interpretability,
outperforming state-of-the-art VAE approaches according to several quantitative
metrics. The resulting latent space allowed the generation of realistic
synthetic data in the trajectory between two distinct input samples or along a
specific attribute dimension to better interpret changes between different
cardiac conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLIP on Wheels: Zero-Shot Object Navigation as Object Localization and Exploration. (arXiv:2203.10421v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10421">
<div class="article-summary-box-inner">
<span><p>Households across the world contain arbitrary objects: from mate gourds and
coffee mugs to sitars and guitars. Considering this diversity, robot perception
must handle a large variety of semantic objects without additional fine-tuning
to be broadly applicable in homes. Recently, zero-shot models have demonstrated
impressive performance in image classification of arbitrary objects (i.e.,
classifying images at inference with categories not explicitly seen during
training). In this paper, we translate the success of zero-shot vision models
(e.g., CLIP) to the popular embodied AI task of object navigation. In our
setting, an agent must find an arbitrary goal object, specified via text, in
unseen environments coming from different datasets. Our key insight is to
modularize the task into zero-shot object localization and exploration.
Employing this philosophy, we design CLIP on Wheels (CoW) baselines for the
task and evaluate each zero-shot model in both Habitat and RoboTHOR simulators.
We find that a straightforward CoW, with CLIP-based object localization plus
classical exploration, and no additional training, often outperforms learnable
approaches in terms of success, efficiency, and robustness to dataset
distribution shift. This CoW achieves 6.3% SPL in Habitat and 10.0% SPL in
RoboTHOR, when tested zero-shot on all categories. On a subset of four RoboTHOR
categories considered in prior work, the same CoW shows a 16.1 percentage point
improvement in Success over the learnable state-of-the-art baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Human-Gaze-Target Detection with Transformers. (arXiv:2203.10433v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10433">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose an effective and efficient method for
Human-Gaze-Target (HGT) detection, i.e., gaze following. Current approaches
decouple the HGT detection task into separate branches of salient object
detection and human gaze prediction, employing a two-stage framework where
human head locations must first be detected and then be fed into the next gaze
target prediction sub-network. In contrast, we redefine the HGT detection task
as detecting human head locations and their gaze targets, simultaneously. By
this way, our method, named Human-Gaze-Target detection TRansformer or HGTTR,
streamlines the HGT detection pipeline by eliminating all other additional
components. HGTTR reasons about the relations of salient objects and human gaze
from the global image context. Moreover, unlike existing two-stage methods that
require human head locations as input and can predict only one human's gaze
target at a time, HGTTR can directly predict the locations of all people and
their gaze targets at one time in an end-to-end manner. The effectiveness and
robustness of our proposed method are verified with extensive experiments on
the two standard benchmark datasets, GazeFollowing and VideoAttentionTarget.
Without bells and whistles, HGTTR outperforms existing state-of-the-art methods
by large margins (6.4 mAP gain on GazeFollowing and 10.3 mAP gain on
VideoAttentionTarget) with a much simpler architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision Transformer with Convolutions Architecture Search. (arXiv:2203.10435v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10435">
<div class="article-summary-box-inner">
<span><p>Transformers exhibit great advantages in handling computer vision tasks. They
model image classification tasks by utilizing a multi-head attention mechanism
to process a series of patches consisting of split images. However, for complex
tasks, Transformer in computer vision not only requires inheriting a bit of
dynamic attention and global context, but also needs to introduce features
concerning noise reduction, shifting, and scaling invariance of objects.
Therefore, here we take a step forward to study the structural characteristics
of Transformer and convolution and propose an architecture search method-Vision
Transformer with Convolutions Architecture Search (VTCAS). The high-performance
backbone network searched by VTCAS introduces the desirable features of
convolutional neural networks into the Transformer architecture while
maintaining the benefits of the multi-head attention mechanism. The searched
block-based backbone network can extract feature maps at different scales.
These features are compatible with a wider range of visual tasks, such as image
classification (32 M parameters, 82.0% Top-1 accuracy on ImageNet-1K) and
object detection (50.4% mAP on COCO2017). The proposed topology based on the
multi-head attention mechanism and CNN adaptively associates relational
features of pixels with multi-scale features of objects. It enhances the
robustness of the neural network for object recognition, especially in the low
illumination indoor scene.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VGSE: Visually-Grounded Semantic Embeddings for Zero-Shot Learning. (arXiv:2203.10444v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10444">
<div class="article-summary-box-inner">
<span><p>Human-annotated attributes serve as powerful semantic embeddings in zero-shot
learning. However, their annotation process is labor-intensive and needs expert
supervision. Current unsupervised semantic embeddings, i.e., word embeddings,
enable knowledge transfer between classes. However, word embeddings do not
always reflect visual similarities and result in inferior zero-shot
performance. We propose to discover semantic embeddings containing
discriminative visual properties for zero-shot learning, without requiring any
human annotation. Our model visually divides a set of images from seen classes
into clusters of local image regions according to their visual similarity, and
further imposes their class discrimination and semantic relatedness. To
associate these clusters with previously unseen classes, we use external
knowledge, e.g., word embeddings and propose a novel class relation discovery
module. Through quantitative and qualitative evaluation, we demonstrate that
our model discovers semantic embeddings that model the visual properties of
both seen and unseen classes. Furthermore, we demonstrate on three benchmarks
that our visually-grounded semantic embeddings further improve performance over
word embeddings across various ZSL models by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Partitioning Image Representation in Contrastive Learning. (arXiv:2203.10454v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10454">
<div class="article-summary-box-inner">
<span><p>In contrastive learning in the image domain, the anchor and positive samples
are forced to have as close representations as possible. However, forcing the
two samples to have the same representation could be misleading because the
data augmentation techniques make the two samples different. In this paper, we
introduce a new representation, partitioned representation, which can learn
both common and unique features of the anchor and positive samples in
contrastive learning. The partitioned representation consists of two parts: the
content part and the style part. The content part represents common features of
the class, and the style part represents the own features of each sample, which
can lead to the representation of the data augmentation method. We can achieve
the partitioned representation simply by decomposing a loss function of
contrastive learning into two terms on the two separate representations,
respectively. To evaluate our representation with two parts, we take two
framework models: Variational AutoEncoder (VAE) and BootstrapYour Own
Latent(BYOL) to show the separability of content and style, and to confirm the
generalization ability in classification, respectively. Based on the
experiments, we show that our approach can separate two types of information in
the VAE framework and outperforms the conventional BYOL in linear separability
and a few-shot learning task as downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Mutual Leakage Network for Cell Image Segmentation. (arXiv:2203.10455v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10455">
<div class="article-summary-box-inner">
<span><p>We propose three segmentation methods using GAN and information leakage
between generator and discriminator. First, we propose an Adversarial Training
Attention Module (ATA-Module) that uses an attention mechanism from the
discriminator to the generator to enhance and leak important information in the
discriminator. ATA-Module transmits important information to the generator from
the discriminator. Second, we propose a Top-Down Pixel-wise Difficulty
Attention Module (Top-Down PDA-Module) that leaks an attention map based on
pixel-wise difficulty in the generator to the discriminator. The generator
trains to focus on pixel-wise difficulty, and the discriminator uses the
difficulty information leaked from the generator for classification. Finally,
we propose an Adversarial Mutual Leakage Network (AML-Net) that mutually leaks
the information each other between the generator and the discriminator. By
using the information of the other network, it is able to train more
efficiently than ordinary segmentation models. Three proposed methods have been
evaluated on two datasets for cell image segmentation. The experimental results
show that the segmentation accuracy of AML-Net was much improved in comparison
with conventional methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">simCrossTrans: A Simple Cross-Modality Transfer Learning for Object Detection with ConvNets or Vision Transformers. (arXiv:2203.10456v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10456">
<div class="article-summary-box-inner">
<span><p>Transfer learning is widely used in computer vision (CV), natural language
processing (NLP) and achieves great success. Most transfer learning systems are
based on the same modality (e.g. RGB image in CV and text in NLP). However, the
cross-modality transfer learning (CMTL) systems are scarce. In this work, we
study CMTL from 2D to 3D sensor to explore the upper bound performance of 3D
sensor only systems, which play critical roles in robotic navigation and
perform well in low light scenarios. While most CMTL pipelines from 2D to 3D
vision are complicated and based on Convolutional Neural Networks (ConvNets),
ours is easy to implement, expand and based on both ConvNets and Vision
transformers(ViTs): 1) By converting point clouds to pseudo-images, we can use
an almost identical network from pre-trained models based on 2D images. This
makes our system easy to implement and expand. 2) Recently ViTs have been
showing good performance and robustness to occlusions, one of the key reasons
for poor performance of 3D vision systems. We explored both ViT and ConvNet
with similar model sizes to investigate the performance difference. We name our
approach simCrossTrans: simple cross-modality transfer learning with ConvNets
or ViTs. Experiments on SUN RGB-D dataset show: with simCrossTrans we achieve
$13.2\%$ and $16.1\%$ absolute performance gain based on ConvNets and ViTs
separately. We also observed the ViTs based performs $9.7\%$ better than the
ConvNets one, showing the power of simCrossTrans with ViT. simCrossTrans with
ViTs surpasses the previous state-of-the-art (SOTA) by a large margin of
$+15.4\%$ mAP50. Compared with the previous 2D detection SOTA based RGB images,
our depth image only system only has a $1\%$ gap. The code, training/inference
logs and models are publicly available at
https://github.com/liketheflower/simCrossTrans
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optical Flow for Video Super-Resolution: A Survey. (arXiv:2203.10462v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10462">
<div class="article-summary-box-inner">
<span><p>Video super-resolution is currently one of the most active research topics in
computer vision as it plays an important role in many visual applications.
Generally, video super-resolution contains a significant component, i.e.,
motion compensation, which is used to estimate the displacement between
successive video frames for temporal alignment. Optical flow, which can supply
dense and sub-pixel motion between consecutive frames, is among the most common
ways for this task. To obtain a good understanding of the effect that optical
flow acts in video super-resolution, in this work, we conduct a comprehensive
review on this subject for the first time. This investigation covers the
following major topics: the function of super-resolution (i.e., why we require
super-resolution); the concept of video super-resolution (i.e., what is video
super-resolution); the description of evaluation metrics (i.e., how (video)
superresolution performs); the introduction of optical flow based video
super-resolution; the investigation of using optical flow to capture temporal
dependency for video super-resolution. Prominently, we give an in-depth study
of the deep learning based video super-resolution method, where some
representative algorithms are analyzed and compared. Additionally, we highlight
some promising research directions and open issues that should be further
addressed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">{Unidirectional Thin Adapter for Efficient Adaptation of Deep Neural Networks. (arXiv:2203.10463v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10463">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a new adapter network for adapting a pre-trained
deep neural network to a target domain with minimal computation. The proposed
model, unidirectional thin adapter (UDTA), helps the classifier adapt to new
data by providing auxiliary features that complement the backbone network. UDTA
takes outputs from multiple layers of the backbone as input features but does
not transmit any feature to the backbone. As a result, UDTA can learn without
computing the gradient of the backbone, which saves computation for training
significantly. In addition, since UDTA learns the target task without modifying
the backbone, a single backbone can adapt to multiple tasks by learning only
UDTAs separately. In experiments on five fine-grained classification datasets
consisting of a small number of samples, UDTA significantly reduced computation
and training time required for backpropagation while showing comparable or even
improved accuracy compared with conventional adapter models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Portrait Eyeglasses and Shadow Removal by Leveraging 3D Synthetic Data. (arXiv:2203.10474v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10474">
<div class="article-summary-box-inner">
<span><p>In portraits, eyeglasses may occlude facial regions and generate cast shadows
on faces, which degrades the performance of many techniques like face
verification and expression recognition. Portrait eyeglasses removal is
critical in handling these problems. However, completely removing the
eyeglasses is challenging because the lighting effects (e.g., cast shadows)
caused by them are often complex. In this paper, we propose a novel framework
to remove eyeglasses as well as their cast shadows from face images. The method
works in a detect-then-remove manner, in which eyeglasses and cast shadows are
both detected and then removed from images. Due to the lack of paired data for
supervised training, we present a new synthetic portrait dataset with both
intermediate and final supervisions for both the detection and removal tasks.
Furthermore, we apply a cross-domain technique to fill the gap between the
synthetic and real data. To the best of our knowledge, the proposed technique
is the first to remove eyeglasses and their cast shadows simultaneously. The
code and synthetic dataset are available at
https://github.com/StoryMY/take-off-eyeglasses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimizing Camera Placements for Overlapped Coverage with 3D Camera Projections. (arXiv:2203.10479v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10479">
<div class="article-summary-box-inner">
<span><p>This paper proposes a method to compute camera 6Dof poses to achieve a user
defined coverage. The camera placement problem is modeled as a combinatorial
optimization where given the maximum number of cameras, a camera set is
selected from a larger pool of possible camera poses. We propose to minimize
the squared error between the desired and the achieved coverage, and formulate
the non-linear cost function as a mixed integer linear programming problem. A
camera lens model is utilized to project the cameras view on a 3D voxel map to
compute a coverage score which makes the optimization problem in real
environments tractable. Experimental results in two real retail store
environments demonstrate the better performance of the proposed formulation in
terms of coverage and overlap for triangulation compared to existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inferring Articulated Rigid Body Dynamics from RGBD Video. (arXiv:2203.10488v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10488">
<div class="article-summary-box-inner">
<span><p>Being able to reproduce physical phenomena ranging from light interaction to
contact mechanics, simulators are becoming increasingly useful in more and more
application domains where real-world interaction or labeled data are difficult
to obtain. Despite recent progress, significant human effort is needed to
configure simulators to accurately reproduce real-world behavior. We introduce
a pipeline that combines inverse rendering with differentiable simulation to
create digital twins of real-world articulated mechanisms from depth or RGB
videos. Our approach automatically discovers joint types and estimates their
kinematic parameters, while the dynamic properties of the overall mechanism are
tuned to attain physically accurate simulations. Control policies optimized in
our derived simulation transfer successfully back to the original system, as we
demonstrate on a simulated system. Further, our approach accurately
reconstructs the kinematic tree of an articulated mechanism being manipulated
by a robot, and highly nonlinear dynamics of a real-world coupled pendulum
mechanism.
</p>
<p>Website: https://eric-heiden.github.io/video2sim
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TVConv: Efficient Translation Variant Convolution for Layout-aware Visual Processing. (arXiv:2203.10489v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10489">
<div class="article-summary-box-inner">
<span><p>As convolution has empowered many smart applications, dynamic convolution
further equips it with the ability to adapt to diverse inputs. However, the
static and dynamic convolutions are either layout-agnostic or
computation-heavy, making it inappropriate for layout-specific applications,
e.g., face recognition and medical image segmentation. We observe that these
applications naturally exhibit the characteristics of large intra-image
(spatial) variance and small cross-image variance. This observation motivates
our efficient translation variant convolution (TVConv) for layout-aware visual
processing. Technically, TVConv is composed of affinity maps and a
weight-generating block. While affinity maps depict pixel-paired relationships
gracefully, the weight-generating block can be explicitly overparameterized for
better training while maintaining efficient inference. Although conceptually
simple, TVConv significantly improves the efficiency of the convolution and can
be readily plugged into various network architectures. Extensive experiments on
face recognition show that TVConv reduces the computational cost by up to 3.1x
and improves the corresponding throughput by 2.3x while maintaining a high
accuracy compared to the depthwise convolution. Moreover, for the same
computation cost, we boost the mean accuracy by up to 4.21%. We also conduct
experiments on the optic disc/cup segmentation task and obtain better
generalization performance, which helps mitigate the critical data scarcity
issue. Code is available at https://github.com/JierunChen/TVConv.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SimAN: Exploring Self-Supervised Representation Learning of Scene Text via Similarity-Aware Normalization. (arXiv:2203.10492v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10492">
<div class="article-summary-box-inner">
<span><p>Recently self-supervised representation learning has drawn considerable
attention from the scene text recognition community. Different from previous
studies using contrastive learning, we tackle the issue from an alternative
perspective, i.e., by formulating the representation learning scheme in a
generative manner. Typically, the neighboring image patches among one text line
tend to have similar styles, including the strokes, textures, colors, etc.
Motivated by this common sense, we augment one image patch and use its
neighboring patch as guidance to recover itself. Specifically, we propose a
Similarity-Aware Normalization (SimAN) module to identify the different
patterns and align the corresponding styles from the guiding patch. In this
way, the network gains representation capability for distinguishing complex
patterns such as messy strokes and cluttered backgrounds. Experiments show that
the proposed SimAN significantly improves the representation quality and
achieves promising performance. Moreover, we surprisingly find that our
self-supervised generative network has impressive potential for data synthesis,
text image editing, and font interpolation, which suggests that the proposed
SimAN has a wide range of practical applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Depth Estimation by Combining Binocular Stereo and Monocular Structured-Light. (arXiv:2203.10493v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10493">
<div class="article-summary-box-inner">
<span><p>It is well known that the passive stereo system cannot adapt well to weak
texture objects, e.g., white walls. However, these weak texture targets are
very common in indoor environments. In this paper, we present a novel stereo
system, which consists of two cameras (an RGB camera and an IR camera) and an
IR speckle projector. The RGB camera is used both for depth estimation and
texture acquisition. The IR camera and the speckle projector can form a
monocular structured-light (MSL) subsystem, while the two cameras can form a
binocular stereo subsystem. The depth map generated by the MSL subsystem can
provide external guidance for the stereo matching networks, which can improve
the matching accuracy significantly. In order to verify the effectiveness of
the proposed system, we build a prototype and collect a test dataset in indoor
scenes. The evaluation results show that the Bad 2.0 error of the proposed
system is 28.2% of the passive stereo system when the network RAFT is used. The
dataset and trained models are available at
https://github.com/YuhuaXu/MonoStereoFusion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Single-image Human-body Reshaping with Deep Neural Networks. (arXiv:2203.10496v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10496">
<div class="article-summary-box-inner">
<span><p>In this paper, we present NeuralReshaper, a novel method for semantic
reshaping of human bodies in single images using deep generative networks. To
achieve globally coherent reshaping effects, our approach follows a
fit-then-reshape pipeline, which first fits a parametric 3D human model to a
source human image and then reshapes the fitted 3D model with respect to
user-specified semantic attributes. Previous methods rely on image warping to
transfer 3D reshaping effects to the entire image domain and thus often cause
distortions in both foreground and background. Instead, to achieve more
realistic reshaping results, we resort to generative adversarial nets
conditioned on the source image and a 2D warping field induced by the reshaped
3D model. Specifically, we separately encode the foreground and background
information in the source image using a two-headed U-net-like generator and
guide the information flow from the foreground branch to the background branch
via feature space warping. Furthermore, to deal with the lack-of-data problem
that no paired data exist (i.e., the same human bodies in varying shapes), we
introduce a novel weakly-supervised strategy to train our network. Besides,
unlike previous methods that often require manual efforts to correct
undesirable artifacts caused by incorrect body-to-image fitting, our method is
fully automatic. Extensive experiments on both indoor and outdoor datasets
demonstrate the superiority of our method over previous approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Soft-CP: A Credible and Effective Data Augmentation for Semantic Segmentation of Medical Lesions. (arXiv:2203.10507v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10507">
<div class="article-summary-box-inner">
<span><p>The medical datasets are usually faced with the problem of scarcity and data
imbalance. Moreover, annotating large datasets for semantic segmentation of
medical lesions is domain-knowledge and time-consuming. In this paper, we
propose a new object-blend method(short in soft-CP) that combines the
Copy-Paste augmentation method for semantic segmentation of medical lesions
offline, ensuring the correct edge information around the lession to solve the
issue above-mentioned. We proved the method's validity with several datasets in
different imaging modalities. In our experiments on the KiTS19[2] dataset,
Soft-CP outperforms existing medical lesions synthesis approaches. The Soft-CP
augementation provides gains of +26.5% DSC in the low data regime(10% of data)
and +10.2% DSC in the high data regime(all of data), In offline training data,
the ratio of real images to synthetic images is 3:1.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Whole Heart Mesh Generation From Patient Images For Computational Simulations. (arXiv:2203.10517v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10517">
<div class="article-summary-box-inner">
<span><p>Patient-specific cardiac modeling combines geometries of the heart derived
from medical images and biophysical simulations to predict various aspects of
cardiac function. However, generating simulation-suitable models of the heart
from patient image data often requires complicated procedures and significant
human effort. We present a fast and automated deep-learning method to construct
simulation-suitable models of the heart from medical images. The approach
constructs meshes from 3D patient images by learning to deform a small set of
deformation handles on a whole heart template. For both 3D CT and MR data, this
method achieves promising accuracy for whole heart reconstruction, consistently
outperforming prior methods in constructing simulation-suitable meshes of the
heart. When evaluated on time-series CT data, this method produced more
anatomically and temporally consistent geometries than prior methods, and was
able to produce geometries that better satisfy modeling requirements for
cardiac flow simulations. Our source code will be available on GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stochastic Video Prediction with Structure and Motion. (arXiv:2203.10528v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10528">
<div class="article-summary-box-inner">
<span><p>While stochastic video prediction models enable future prediction under
uncertainty, they mostly fail to model the complex dynamics of real-world
scenes. For example, they cannot provide reliable predictions for scenes with a
moving camera and independently moving foreground objects in driving scenarios.
The existing methods fail to fully capture the dynamics of the structured world
by only focusing on changes in pixels. In this paper, we assume that there is
an underlying process creating observations in a video and propose to factorize
it into static and dynamic components. We model the static part based on the
scene structure and the ego-motion of the vehicle, and the dynamic part based
on the remaining motion of the dynamic objects. By learning separate
distributions of changes in foreground and background, we can decompose the
scene into static and dynamic parts and separately model the change in each.
Our experiments demonstrate that disentangling structure and motion helps
stochastic video prediction, leading to better future predictions in complex
driving scenarios on two real-world driving datasets, KITTI and Cityscapes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Iwin: Human-Object Interaction Detection via Transformer with Irregular Windows. (arXiv:2203.10537v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10537">
<div class="article-summary-box-inner">
<span><p>This paper presents a new vision Transformer, named Iwin Transformer, which
is specifically designed for human-object interaction (HOI) detection, a
detailed scene understanding task involving a sequential process of
human/object detection and interaction recognition. Iwin Transformer is a
hierarchical Transformer which progressively performs token representation
learning and token agglomeration within irregular windows. The irregular
windows, achieved by augmenting regular grid locations with learned offsets, 1)
eliminate redundancy in token representation learning, which leads to efficient
human/object detection, and 2) enable the agglomerated tokens to align with
humans/objects with different shapes, which facilitates the acquisition of
highly-abstracted visual semantics for interaction recognition. The
effectiveness and efficiency of Iwin Transformer are verified on the two
standard HOI detection benchmark datasets, HICO-DET and V-COCO. Results show
our method outperforms existing Transformers-based methods by large margins
(3.7 mAP gain on HICO-DET and 2.0 mAP gain on V-COCO) with fewer training
epochs ($0.5 \times$).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Video Text Spotting with Transformer. (arXiv:2203.10539v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10539">
<div class="article-summary-box-inner">
<span><p>Recent video text spotting methods usually require the three-staged pipeline,
i.e., detecting text in individual images, recognizing localized text, tracking
text streams with post-processing to generate final results. These methods
typically follow the tracking-by-match paradigm and develop sophisticated
pipelines. In this paper, rooted in Transformer sequence modeling, we propose a
simple, but effective end-to-end video text DEtection, Tracking, and
Recognition framework (TransDETR). TransDETR mainly includes two advantages: 1)
Different from the explicit match paradigm in the adjacent frame, TransDETR
tracks and recognizes each text implicitly by the different query termed text
query over long-range temporal sequence (more than 7 frames). 2) TransDETR is
the first end-to-end trainable video text spotting framework, which
simultaneously addresses the three sub-tasks (e.g., text detection, tracking,
recognition). Extensive experiments in four video text datasets (i.e.,ICDAR2013
Video, ICDAR2015 Video, Minetto, and YouTube Video Text) are conducted to
demonstrate that TransDETR achieves state-of-the-art performance with up to
around 8.0% improvements on video text spotting tasks. The code of TransDETR
can be found at https://github.com/weijiawu/TransDETR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Domain Adaptation for Nighttime Aerial Tracking. (arXiv:2203.10541v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10541">
<div class="article-summary-box-inner">
<span><p>Previous advances in object tracking mostly reported on favorable
illumination circumstances while neglecting performance at nighttime, which
significantly impeded the development of related aerial robot applications.
This work instead develops a novel unsupervised domain adaptation framework for
nighttime aerial tracking (named UDAT). Specifically, a unique object discovery
approach is provided to generate training patches from raw nighttime tracking
videos. To tackle the domain discrepancy, we employ a Transformer-based
bridging layer post to the feature extractor to align image features from both
domains. With a Transformer day/night feature discriminator, the daytime
tracking model is adversarially trained to track at night. Moreover, we
construct a pioneering benchmark namely NAT2021 for unsupervised domain
adaptive nighttime tracking, which comprises a test set of 180 manually
annotated tracking sequences and a train set of over 276k unlabelled nighttime
tracking frames. Exhaustive experiments demonstrate the robustness and domain
adaptability of the proposed framework in nighttime aerial tracking. The code
and benchmark are available at https://github.com/vision4robotics/UDAT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Document Dewarping with Control Points. (arXiv:2203.10543v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10543">
<div class="article-summary-box-inner">
<span><p>Document images are now widely captured by handheld devices such as mobile
phones. The OCR performance on these images are largely affected due to
geometric distortion of the document paper, diverse camera positions and
complex backgrounds. In this paper, we propose a simple yet effective approach
to rectify distorted document image by estimating control points and reference
points. After that, we use interpolation method between control points and
reference points to convert sparse mappings to backward mapping, and remap the
original distorted document image to the rectified image. Furthermore, control
points are controllable to facilitate interaction or subsequent adjustment. We
can flexibly select post-processing methods and the number of vertices
according to different application scenarios. Experiments show that our
approach can rectify document images with various distortion types, and yield
state-of-the-art performance on real-world dataset. This paper also provides a
training dataset based on control points for document dewarping. Both the code
and the dataset are released at
https://github.com/gwxie/Document-Dewarping-with-Control-Points.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards 3D Scene Understanding by Referring Synthetic Models. (arXiv:2203.10546v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10546">
<div class="article-summary-box-inner">
<span><p>Promising performance has been achieved for visual perception on the point
cloud. However, the current methods typically rely on labour-extensive
annotations on the scene scans. In this paper, we explore how synthetic models
alleviate the real scene annotation burden, i.e., taking the labelled 3D
synthetic models as reference for supervision, the neural network aims to
recognize specific categories of objects on a real scene scan (without scene
annotation for supervision). The problem studies how to transfer knowledge from
synthetic 3D models to real 3D scenes and is named Referring Transfer Learning
(RTL). The main challenge is solving the model-to-scene (from a single model to
the scene) and synthetic-to-real (from synthetic model to real scene's object)
gap between the synthetic model and the real scene. To this end, we propose a
simple yet effective framework to perform two alignment operations. First,
physical data alignment aims to make the synthetic models cover the diversity
of the scene's objects with data processing techniques. Then a novel
\textbf{convex-hull regularized feature alignment} introduces learnable
prototypes to project the point features of both synthetic models and real
scenes to a unified feature space, which alleviates the domain gap. These
operations ease the model-to-scene and synthetic-to-real difficulty for a
network to recognize the target objects on a real unseen scene. Experiments
show that our method achieves the average mAP of 46.08\% and 55.49\% on the
ScanNet and S3DIS datasets by learning the synthetic models from the ModelNet
dataset. Code will be publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Human Pose Estimation Using M\"obius Graph Convolutional Networks. (arXiv:2203.10554v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10554">
<div class="article-summary-box-inner">
<span><p>3D human pose estimation is fundamental to understanding human behavior.
Recently, promising results have been achieved by graph convolutional networks
(GCNs), which achieve state-of-the-art performance and provide rather
light-weight architectures. However, a major limitation of GCNs is their
inability to encode all the transformations between joints explicitly. To
address this issue, we propose a novel spectral GCN using the M\"obius
transformation (M\"obiusGCN). In particular, this allows us to directly and
explicitly encode the transformation between joints, resulting in a
significantly more compact representation. Compared to even the lightest
architectures so far, our novel approach requires 90-98% fewer parameters, i.e.
our lightest M\"obiusGCN uses only 0.042M trainable parameters. Besides the
drastic parameter reduction, explicitly encoding the transformation of joints
also enables us to achieve state-of-the-art results. We evaluate our approach
on the two challenging pose estimation benchmarks, Human3.6M and MPI-INF-3DHP,
demonstrating both state-of-the-art results and the generalization capabilities
of M\"obiusGCN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CRISPnet: Color Rendition ISP Net. (arXiv:2203.10562v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10562">
<div class="article-summary-box-inner">
<span><p>Image signal processors (ISPs) are historically grown legacy software systems
for reconstructing color images from noisy raw sensor measurements. They are
usually composited of many heuristic blocks for denoising, demosaicking, and
color restoration. Color reproduction in this context is of particular
importance, since the raw colors are often severely distorted, and each smart
phone manufacturer has developed their own characteristic heuristics for
improving the color rendition, for example of skin tones and other visually
important colors.
</p>
<p>In recent years there has been strong interest in replacing the historically
grown ISP systems with deep learned pipelines. Much progress has been made in
approximating legacy ISPs with such learned models. However, so far the focus
of these efforts has been on reproducing the structural features of the images,
with less attention paid to color rendition.
</p>
<p>Here we present CRISPnet, the first learned ISP model to specifically target
color rendition accuracy relative to a complex, legacy smart phone ISP. We
achieve this by utilizing both image metadata (like a legacy ISP would), as
well as by learning simple global semantics based on image classification --
similar to what a legacy ISP does to determine the scene type. We also
contribute a new ISP image dataset consisting of both high dynamic range
monitor data, as well as real-world data, both captured with an actual cell
phone ISP pipeline under a variety of lighting conditions, exposure times, and
gain settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accelerating Integrated Task and Motion Planning with Neural Feasibility Checking. (arXiv:2203.10568v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10568">
<div class="article-summary-box-inner">
<span><p>As robots play an increasingly important role in the industrial, the
expectations about their applications for everyday living tasks are getting
higher. Robots need to perform long-horizon tasks that consist of several
sub-tasks that need to be accomplished. Task and Motion Planning (TAMP)
provides a hierarchical framework to handle the sequential nature of
manipulation tasks by interleaving a symbolic task planner that generates a
possible action sequence, with a motion planner that checks the kinematic
feasibility in the geometric world, generating robot trajectories if several
constraints are satisfied, e.g., a collision-free trajectory from one state to
another. Hence, the reasoning about the task plan's geometric grounding is
taken over by the motion planner. However, motion planning is computationally
intense and is usability as feasibility checker casts TAMP methods inapplicable
to real-world scenarios. In this paper, we introduce neural feasibility
classifier (NFC), a simple yet effective visual heuristic for classifying the
feasibility of proposed actions in TAMP. Namely, NFC will identify infeasible
actions of the task planner without the need for costly motion planning, hence
reducing planning time in multi-step manipulation tasks. NFC encodes the image
of the robot's workspace into a feature map thanks to convolutional neural
network (CNN). We train NFC using simulated data from TAMP problems and label
the instances based on IK feasibility checking. Our empirical results in
different simulated manipulation tasks show that our NFC generalizes to the
entire robot workspace and has high prediction accuracy even in scenes with
multiple obstructions. When combined with state-of-the-art integrated TAMP, our
NFC enhances its performance while reducing its planning time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Point Cloud Completion on Real Traffic Scenes via Scene-concerned Bottom-up Mechanism. (arXiv:2203.10569v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10569">
<div class="article-summary-box-inner">
<span><p>Real scans always miss partial geometries of objects due to the
self-occlusions, external-occlusions, and limited sensor resolutions. Point
cloud completion aims to refer the complete shapes for incomplete 3D scans of
objects. Current deep learning-based approaches rely on large-scale complete
shapes in the training process, which are usually obtained from synthetic
datasets. It is not applicable for real-world scans due to the domain gap. In
this paper, we propose a self-supervised point cloud completion method (TraPCC)
for vehicles in real traffic scenes without any complete data. Based on the
symmetry and similarity of vehicles, we make use of consecutive point cloud
frames to construct vehicle memory bank as reference. We design a bottom-up
mechanism to focus on both local geometry details and global shape features of
inputs. In addition, we design a scene-graph in the network to pay attention to
the missing parts by the aid of neighboring vehicles. Experiments show that
TraPCC achieve good performance for real-scan completion on KITTI and nuScenes
traffic datasets even without any complete data in training. We also show a
downstream application of 3D detection, which benefits from our completion
approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Point3D: tracking actions as moving points with 3D CNNs. (arXiv:2203.10584v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10584">
<div class="article-summary-box-inner">
<span><p>Spatio-temporal action recognition has been a challenging task that involves
detecting where and when actions occur. Current state-of-the-art action
detectors are mostly anchor-based, requiring sensitive anchor designs and huge
computations due to calculating large numbers of anchor boxes. Motivated by
nascent anchor-free approaches, we propose Point3D, a flexible and
computationally efficient network with high precision for spatio-temporal
action recognition. Our Point3D consists of a Point Head for action
localization and a 3D Head for action classification. Firstly, Point Head is
used to track center points and knot key points of humans to localize the
bounding box of an action. These location features are then piped into a
time-wise attention to learn long-range dependencies across frames. The 3D Head
is later deployed for the final action classification. Our Point3D achieves
state-of-the-art performance on the JHMDB, UCF101-24, and AVA benchmarks in
terms of frame-mAP and video-mAP. Comprehensive ablation studies also
demonstrate the effectiveness of each module proposed in our Point3D.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open-Vocabulary One-Stage Detection with Hierarchical Visual-Language Knowledge Distillation. (arXiv:2203.10593v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10593">
<div class="article-summary-box-inner">
<span><p>Open-vocabulary object detection aims to detect novel object categories
beyond the training set.
</p>
<p>The advanced open-vocabulary two-stage detectors employ instance-level
visual-to-visual knowledge distillation to align the visual space of the
detector with the semantic space of the Pre-trained Visual-Language Model
(PVLM).
</p>
<p>However, in the more efficient one-stage detector, the absence of
class-agnostic object proposals hinders the knowledge distillation on unseen
objects, leading to severe performance degradation.
</p>
<p>In this paper, we propose a hierarchical visual-language knowledge
distillation method, i.e., HierKD, for open-vocabulary one-stage detection.
</p>
<p>Specifically, a global-level knowledge distillation is explored to transfer
the knowledge of unseen categories from the PVLM to the detector.
</p>
<p>Moreover, we combine the proposed global-level knowledge distillation and the
common instance-level knowledge distillation to learn the knowledge of seen and
unseen categories simultaneously.
</p>
<p>Extensive experiments on MS-COCO show that our method significantly surpasses
the previous best one-stage detector with 11.9\% and 6.7\% $AP_{50}$ gains
under the zero-shot detection and generalized zero-shot detection settings, and
reduces the $AP_{50}$ performance gap from 14\% to 7.3\% compared to the best
two-stage detector.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Clinical Practice: Design and Implementation of Convolutional Neural Network-Based Assistive Diagnosis System for COVID-19 Case Detection from Chest X-Ray Images. (arXiv:2203.10596v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10596">
<div class="article-summary-box-inner">
<span><p>One of the critical tools for early detection and subsequent evaluation of
the incidence of lung diseases is chest radiography. This study presents a
real-world implementation of a convolutional neural network (CNN) based Carebot
Covid app to detect COVID-19 from chest X-ray (CXR) images. Our proposed model
takes the form of a simple and intuitive application. Used CNN can be deployed
as a STOW-RS prediction endpoint for direct implementation into DICOM viewers.
The results of this study show that the deep learning model based on DenseNet
and ResNet architecture can detect SARS-CoV-2 from CXR images with precision of
0.981, recall of 0.962 and AP of 0.993.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-Time Rotation-Invariant Face Detection with Progressive Calibration Networks. (arXiv:1804.06039v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1804.06039">
<div class="article-summary-box-inner">
<span><p>Rotation-invariant face detection, i.e. detecting faces with arbitrary
rotation-in-plane (RIP) angles, is widely required in unconstrained
applications but still remains as a challenging task, due to the large
variations of face appearances. Most existing methods compromise with speed or
accuracy to handle the large RIP variations. To address this problem more
efficiently, we propose Progressive Calibration Networks (PCN) to perform
rotation-invariant face detection in a coarse-to-fine manner. PCN consists of
three stages, each of which not only distinguishes the faces from non-faces,
but also calibrates the RIP orientation of each face candidate to upright
progressively. By dividing the calibration process into several progressive
steps and only predicting coarse orientations in early stages, PCN can achieve
precise and fast calibration. By performing binary classification of face vs.
non-face with gradually decreasing RIP ranges, PCN can accurately detect faces
with full $360^{\circ}$ RIP angles. Such designs lead to a real-time
rotation-invariant face detector. The experiments on multi-oriented FDDB and a
challenging subset of WIDER FACE containing rotated faces in the wild show that
our PCN achieves quite promising performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-time Semantic Segmentation via Spatial-detail Guided Context Propagation. (arXiv:2005.11034v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.11034">
<div class="article-summary-box-inner">
<span><p>Nowadays, vision-based computing tasks play an important role in various
real-world applications. However, many vision computing tasks, e.g. semantic
segmentation, are usually computationally expensive, posing a challenge to the
computing systems that are resource-constrained but require fast response
speed. Therefore, it is valuable to develop accurate and real-time vision
processing models that only require limited computational resources. To this
end, we propose the Spatial-detail Guided Context Propagation Network (SGCPNet)
for achieving real-time semantic segmentation. In SGCPNet, we propose the
strategy of spatial-detail guided context propagation. It uses the spatial
details of shallow layers to guide the propagation of the low-resolution global
contexts, in which the lost spatial information can be effectively
reconstructed. In this way, the need for maintaining high-resolution features
along the network is freed, therefore largely improving the model efficiency.
On the other hand, due to the effective reconstruction of spatial details, the
segmentation accuracy can be still preserved. In the experiments, we validate
the effectiveness and efficiency of the proposed SGCPNet model. On the
Citysacpes dataset, for example, our SGCPNet achieves 69.5% mIoU segmentation
accuracy, while its speed reaches 178.5 FPS on 768x1536 images on a GeForce GTX
1080 Ti GPU card. In addition, SGCPNet is very lightweight and only contains
0.61 M parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast Few-Shot Classification by Few-Iteration Meta-Learning. (arXiv:2010.00511v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.00511">
<div class="article-summary-box-inner">
<span><p>Autonomous agents interacting with the real world need to learn new concepts
efficiently and reliably. This requires learning in a low-data regime, which is
a highly challenging problem. We address this task by introducing a fast
optimization-based meta-learning method for few-shot classification. It
consists of an embedding network, providing a general representation of the
image, and a base learner module. The latter learns a linear classifier during
the inference through an unrolled optimization procedure. We design an inner
learning objective composed of (i) a robust classification loss on the support
set and (ii) an entropy loss, allowing transductive learning from unlabeled
query samples. By employing an efficient initialization module and a Steepest
Descent based optimization algorithm, our base learner predicts a powerful
classifier within only a few iterations. Further, our strategy enables
important aspects of the base learner objective to be learned during
meta-training. To the best of our knowledge, this work is the first to
integrate both induction and transduction into the base learner in an
optimization-based meta-learning framework. We perform a comprehensive
experimental analysis, demonstrating the speed and effectiveness of our
approach on four few-shot classification datasets. The Code is available at
\href{https://github.com/4rdhendu/FIML}{\textcolor{blue}{https://github.com/4rdhendu/FIML}}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Curriculum Learning: A Survey. (arXiv:2101.10382v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.10382">
<div class="article-summary-box-inner">
<span><p>Training machine learning models in a meaningful order, from the easy samples
to the hard ones, using curriculum learning can provide performance
improvements over the standard training approach based on random data
shuffling, without any additional computational costs. Curriculum learning
strategies have been successfully employed in all areas of machine learning, in
a wide range of tasks. However, the necessity of finding a way to rank the
samples from easy to hard, as well as the right pacing function for introducing
more difficult data can limit the usage of the curriculum approaches. In this
survey, we show how these limits have been tackled in the literature, and we
present different curriculum learning instantiations for various tasks in
machine learning. We construct a multi-perspective taxonomy of curriculum
learning approaches by hand, considering various classification criteria. We
further build a hierarchical tree of curriculum learning methods using an
agglomerative clustering algorithm, linking the discovered clusters with our
taxonomy. At the end, we provide some interesting directions for future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DOC2PPT: Automatic Presentation Slides Generation from Scientific Documents. (arXiv:2101.11796v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.11796">
<div class="article-summary-box-inner">
<span><p>Creating presentation materials requires complex multimodal reasoning skills
to summarize key concepts and arrange them in a logical and visually pleasing
manner. Can machines learn to emulate this laborious process? We present a
novel task and approach for document-to-slide generation. Solving this involves
document summarization, image and text retrieval, slide structure and layout
prediction to arrange key elements in a form suitable for presentation. We
propose a hierarchical sequence-to-sequence approach to tackle our task in an
end-to-end manner. Our approach exploits the inherent structures within
documents and slides and incorporates paraphrasing and layout prediction
modules to generate slides. To help accelerate research in this domain, we
release a dataset about 6K paired documents and slide decks used in our
experiments. We show that our approach outperforms strong baselines and
produces slides with rich content and aligned imagery.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Atlas Generative Models and Geodesic Interpolation. (arXiv:2102.00264v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.00264">
<div class="article-summary-box-inner">
<span><p>Generative neural networks have a well recognized ability to estimate
underlying manifold structure of high dimensional data. However, if a single
latent space is used, it is not possible to faithfully represent a manifold
with topology different from Euclidean space. In this work we define the
general class of Atlas Generative Models (AGMs), models with hybrid
discrete-continuous latent space that estimate an atlas on the underlying data
manifold together with a partition of unity on the data space. We identify
existing examples of models from various popular generative paradigms that fit
into this class. Due to the atlas interpretation, ideas from non-linear latent
space analysis and statistics, e.g. geodesic interpolation, which has
previously only been investigated for models with simply connected latent
spaces, may be extended to the entire class of AGMs in a natural way. We
exemplify this by generalizing an algorithm for graph based geodesic
interpolation to the setting of AGMs, and verify its performance
experimentally.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Selfie Periocular Verification using an Efficient Super-Resolution Approach. (arXiv:2102.08449v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.08449">
<div class="article-summary-box-inner">
<span><p>Selfie-based biometrics has great potential for a wide range of applications
since, e.g. periocular verification is contactless and is safe to use in
pandemics such as COVID-19, when a major portion of a face is covered by a
facial mask. Despite its advantages, selfie-based biometrics presents
challenges since there is limited control over data acquisition at different
distances. Therefore, Super-Resolution (SR) has to be used to increase the
quality of the eye images and to keep or improve the recognition performance.
We propose an Efficient Single Image Super-Resolution algorithm, which takes
into account a trade-off between the efficiency and the size of its filters. To
that end, the method implements a loss function based on the Sharpness metric
used to evaluate iris images quality. Our method drastically reduces the number
of parameters compared to the state-of-the-art: from 2,170,142 to 28,654. Our
best results on remote verification systems with no redimensioning reached an
EER of 8.89\% for FaceNet, 12.14% for VGGFace, and 12.81% for ArcFace. Then,
embedding vectors were extracted from SR images, the FaceNet-based system
yielded an EER of 8.92% for a resizing of x2, 8.85% for x3, and 9.32% for x4.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discriminative Semantic Transitive Consistency for Cross-Modal Learning. (arXiv:2103.14103v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.14103">
<div class="article-summary-box-inner">
<span><p>Cross-modal retrieval is generally performed by projecting and aligning the
data from two different modalities onto a shared representation space. This
shared space often also acts as a bridge for translating the modalities. We
address the problem of learning such representation space by proposing and
exploiting the property of Discriminative Semantic Transitive Consistency --
ensuring that the data points are correctly classified even after being
transferred to the other modality. Along with semantic transitive consistency,
we also enforce the traditional distance minimizing constraint which makes the
projections of the corresponding data points from both the modalities to come
closer in the representation space. We analyze and compare the contribution of
both the loss terms and their interaction, for the task. In addition, we
incorporate semantic cycle-consistency for each of the modality. We empirically
demonstrate better performance owing to the different components with clear
ablation studies. We also provide qualitative results to support the proposals.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text to Image Generation with Semantic-Spatial Aware GAN. (arXiv:2104.00567v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.00567">
<div class="article-summary-box-inner">
<span><p>Text-to-image synthesis (T2I) aims to generate photo-realistic images which
are semantically consistent with the text descriptions. Existing methods are
usually built upon conditional generative adversarial networks (GANs) and
initialize an image from noise with sentence embedding, and then refine the
features with fine-grained word embedding iteratively. A close inspection of
their generated images reveals a major limitation: even though the generated
image holistically matches the description, individual image regions or parts
of somethings are often not recognizable or consistent with words in the
sentence, e.g. "a white crown". To address this problem, we propose a novel
framework Semantic-Spatial Aware GAN for synthesizing images from input text.
Concretely, we introduce a simple and effective Semantic-Spatial Aware block,
which (1) learns semantic-adaptive transformation conditioned on text to
effectively fuse text features and image features, and (2) learns a semantic
mask in a weakly-supervised way that depends on the current text-image fusion
process in order to guide the transformation spatially. Experiments on the
challenging COCO and CUB bird datasets demonstrate the advantage of our method
over the recent state-of-the-art approaches, regarding both visual fidelity and
alignment with input text description. Code available at
https://github.com/wtliao/text2image.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">M3L: Language-based Video Editing via Multi-Modal Multi-Level Transformers. (arXiv:2104.01122v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01122">
<div class="article-summary-box-inner">
<span><p>Video editing tools are widely used nowadays for digital design. Although the
demand for these tools is high, the prior knowledge required makes it difficult
for novices to get started. Systems that could follow natural language
instructions to perform automatic editing would significantly improve
accessibility. This paper introduces the language-based video editing (LBVE)
task, which allows the model to edit, guided by text instruction, a source
video into a target video. LBVE contains two features: 1) the scenario of the
source video is preserved instead of generating a completely different video;
2) the semantic is presented differently in the target video, and all changes
are controlled by the given instruction. We propose a Multi-Modal Multi-Level
Transformer (M$^3$L) to carry out LBVE. M$^3$L dynamically learns the
correspondence between video perception and language semantic at different
levels, which benefits both the video understanding and video frame synthesis.
We build three new datasets for evaluation, including two diagnostic and one
from natural videos with human-labeled text. Extensive experimental results
show that M$^3$L is effective for video editing and that LBVE can lead to a new
field toward vision-and-language research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Well Does Self-Supervised Pre-Training Perform with Streaming Data?. (arXiv:2104.12081v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12081">
<div class="article-summary-box-inner">
<span><p>Prior works on self-supervised pre-training focus on the joint training
scenario, where massive unlabeled data are assumed to be given as input all at
once, and only then is a learner trained. Unfortunately, such a problem setting
is often impractical if not infeasible since many real-world tasks rely on
sequential learning, e.g., data are decentralized or collected in a streaming
fashion. In this paper, we conduct the first thorough and dedicated
investigation on self-supervised pre-training with streaming data, aiming to
shed light on the model behavior under this overlooked setup. Specifically, we
pre-train over 500 models on four categories of pre-training streaming data
from ImageNet and DomainNet and evaluate them on three types of downstream
tasks and 12 different downstream datasets. Our studies show that, somehow
beyond our expectation, with simple data replay or parameter regularization,
sequential self-supervised pre-training turns out to be an efficient
alternative for joint pre-training, as the performances of the former are
mostly on par with those of the latter. Moreover, catastrophic forgetting, a
common issue in sequential supervised learning, is much alleviated in
sequential self-supervised learning (SSL), which is well justified through our
comprehensive empirical analysis on representations and the sharpness of minima
in the loss landscape. Our findings, therefore, suggest that, in practice, for
SSL, the cumbersome joint training can be replaced mainly by sequential
learning, which in turn enables a much broader spectrum of potential
application scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Moving Towards Centers: Re-ranking with Attention and Memory for Re-identification. (arXiv:2105.01447v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.01447">
<div class="article-summary-box-inner">
<span><p>Re-ranking utilizes contextual information to optimize the initial ranking
list of person or vehicle re-identification (re-ID), which boosts the retrieval
performance at post-processing steps. This paper proposes a re-ranking network
to predict the correlations between the probe and top-ranked neighbor samples.
Specifically, all the feature embeddings of query and gallery images are
expanded and enhanced by a linear combination of their neighbors, with the
correlation prediction serving as discriminative combination weights. The
combination process is equivalent to moving independent embeddings toward the
identity centers, improving cluster compactness. For correlation prediction, we
first aggregate the contextual information for probe's k-nearest neighbors via
the Transformer encoder. Then, we distill and refine the probe-related features
into the Contextual Memory cell via attention mechanism. Like humans that
retrieve images by not only considering probe images but also memorizing the
retrieved ones, the Contextual Memory produces multi-view descriptions for each
instance. Finally, the neighbors are reconstructed with features fetched from
the Contextual Memory, and a binary classifier predicts their correlations with
the probe. Experiments on six widely-used person and vehicle re-ID benchmarks
demonstrate the effectiveness of the proposed method. Especially, our method
surpasses the state-of-the-art re-ranking approaches on large-scale datasets by
a significant margin, i.e., with an average 4.83% CMC@1 and 14.83% mAP
improvements on VERI-Wild, MSMT17, and VehicleID datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Shot Face Swapping on Megapixels. (arXiv:2105.04932v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.04932">
<div class="article-summary-box-inner">
<span><p>Face swapping has both positive applications such as entertainment,
human-computer interaction, etc., and negative applications such as DeepFake
threats to politics, economics, etc. Nevertheless, it is necessary to
understand the scheme of advanced methods for high-quality face swapping and
generate enough and representative face swapping images to train DeepFake
detection algorithms. This paper proposes the first Megapixel level method for
one shot Face Swapping (or MegaFS for short). Firstly, MegaFS organizes face
representation hierarchically by the proposed Hierarchical Representation Face
Encoder (HieRFE) in an extended latent space to maintain more facial details,
rather than compressed representation in previous face swapping methods.
Secondly, a carefully designed Face Transfer Module (FTM) is proposed to
transfer the identity from a source image to the target by a non-linear
trajectory without explicit feature disentanglement. Finally, the swapped faces
can be synthesized by StyleGAN2 with the benefits of its training stability and
powerful generative capability. Each part of MegaFS can be trained separately
so the requirement of our model for GPU memory can be satisfied for megapixel
face swapping. In summary, complete face representation, stable training, and
limited memory usage are the three novel contributions to the success of our
method. Extensive experiments demonstrate the superiority of MegaFS and the
first megapixel level face swapping database is released for research on
DeepFake detection and face image editing in the public domain. The dataset is
at this link.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language-Driven Image Style Transfer. (arXiv:2106.00178v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.00178">
<div class="article-summary-box-inner">
<span><p>Despite having promising results, style transfer, which requires preparing
style images in advance, may result in lack of creativity and accessibility.
Following human instruction, on the other hand, is the most natural way to
perform artistic style transfer that can significantly improve controllability
for visual effect applications. We introduce a new task, language-driven
artistic style transfer (LDAST), to manipulate the style of a content image,
guided by a text. We propose contrastive language visual artist (CLVA) that
learns to extract visual semantics from style instructions and accomplish LDAST
by the patch-wise style discriminator. The discriminator considers the
correlation between language and patches of style images or transferred results
to jointly embed style instructions. CLVA further compares contrastive pairs of
content images and style instructions to improve the mutual relativeness. The
results from the same content image can preserve consistent content structures.
Besides, they should present analogous style patterns from style instructions
that contain similar visual semantics. The experiments show that our CLVA is
effective and achieves superb transferred results on LDAST.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">More Than Meets the Eye: Self-Supervised Depth Reconstruction from Brain Activity. (arXiv:2106.05113v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05113">
<div class="article-summary-box-inner">
<span><p>In the past few years, significant advancements were made in reconstruction
of observed natural images from fMRI brain recordings using deep-learning
tools. Here, for the first time, we show that dense 3D depth maps of observed
2D natural images can also be recovered directly from fMRI brain recordings. We
use an off-the-shelf method to estimate the unknown depth maps of natural
images. This is applied to both: (i) the small number of images presented to
subjects in an fMRI scanner (images for which we have fMRI recordings -
referred to as "paired" data), and (ii) a very large number of natural images
with no fMRI recordings ("unpaired data"). The estimated depth maps are then
used as an auxiliary reconstruction criterion to train for depth reconstruction
directly from fMRI. We propose two main approaches: Depth-only recovery and
joint image-depth RGBD recovery. Because the number of available "paired"
training data (images with fMRI) is small, we enrich the training data via
self-supervised cycle-consistent training on many "unpaired" data (natural
images &amp; depth maps without fMRI). This is achieved using our newly defined and
trained Depth-based Perceptual Similarity metric as a reconstruction criterion.
We show that predicting the depth map directly from fMRI outperforms its
indirect sequential recovery from the reconstructed images. We further show
that activations from early cortical visual areas dominate our depth
reconstruction results, and propose means to characterize fMRI voxels by their
degree of depth-information tuning. This work adds an important layer of
decoded information, extending the current envelope of visual brain decoding
capabilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hidden Convexity of Wasserstein GANs: Interpretable Generative Models with Closed-Form Solutions. (arXiv:2107.05680v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.05680">
<div class="article-summary-box-inner">
<span><p>Generative Adversarial Networks (GANs) are commonly used for modeling complex
distributions of data. Both the generators and discriminators of GANs are often
modeled by neural networks, posing a non-transparent optimization problem which
is non-convex and non-concave over the generator and discriminator,
respectively. Such networks are often heuristically optimized with gradient
descent-ascent (GDA), but it is unclear whether the optimization problem
contains any saddle points, or whether heuristic methods can find them in
practice. In this work, we analyze the training of Wasserstein GANs with
two-layer neural network discriminators through the lens of convex duality, and
for a variety of generators expose the conditions under which Wasserstein GANs
can be solved exactly with convex optimization approaches, or can be
represented as convex-concave games. Using this convex duality interpretation,
we further demonstrate the impact of different activation functions of the
discriminator. Our observations are verified with numerical results
demonstrating the power of the convex interpretation, with applications in
progressive training of convex architectures corresponding to linear generators
and quadratic-activation discriminators for CelebA image generation. The code
for our experiments is available at https://github.com/ardasahiner/ProCoGAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards to Robust and Generalized Medical Image Segmentation Framework. (arXiv:2108.03823v7 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03823">
<div class="article-summary-box-inner">
<span><p>Deep learning-based computer-aided diagnosis is gradually deployed to review
and analyze medical images. However, this paradigm is restricted in real-world
clinical applications due to the poor robustness and generalization. The issue
is more sinister with a lack of training data. In this paper, we address the
challenge from the transfer learning point of view. Different from the common
setting that transferring knowledge from the natural image domain to the
medical image domain, we find the knowledge from the same domain further boosts
the model robustness and generalization. Therefore, we propose a novel
two-stage framework for robust generalized medical image segmentation. Firstly,
an unsupervised tile-wise autoencoder pretraining architecture is proposed to
learn local and global knowledge. Secondly, the downstream segmentation model
coupled with an auxiliary reconstruction network is designed. The
reconstruction branch encourages the model to capture more general semantic
features. Experiments of lung segmentation on multi chest X-ray datasets are
conducted. Comprehensive results demonstrate the superior robustness of the
proposed framework to corruption and high generalization performance on unseen
datasets, especially under the scenario of the limited training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SimCVD: Simple Contrastive Voxel-Wise Representation Distillation for Semi-Supervised Medical Image Segmentation. (arXiv:2108.06227v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06227">
<div class="article-summary-box-inner">
<span><p>Automated segmentation in medical image analysis is a challenging task that
requires a large amount of manually labeled data. However, most existing
learning-based approaches usually suffer from limited manually annotated
medical data, which poses a major practical problem for accurate and robust
medical image segmentation. In addition, most existing semi-supervised
approaches are usually not robust compared with the supervised counterparts,
and also lack explicit modeling of geometric structure and semantic
information, both of which limit the segmentation accuracy. In this work, we
present SimCVD, a simple contrastive distillation framework that significantly
advances state-of-the-art voxel-wise representation learning. We first describe
an unsupervised training strategy, which takes two views of an input volume and
predicts their signed distance maps of object boundaries in a contrastive
objective, with only two independent dropout as mask. This simple approach
works surprisingly well, performing on the same level as previous fully
supervised methods with much less labeled data. We hypothesize that dropout can
be viewed as a minimal form of data augmentation and makes the network robust
to representation collapse. Then, we propose to perform structural distillation
by distilling pair-wise similarities. We evaluate SimCVD on two popular
datasets: the Left Atrial Segmentation Challenge (LA) and the NIH pancreas CT
dataset. The results on the LA dataset demonstrate that, in two types of
labeled ratios (i.e., 20% and 10%), SimCVD achieves an average Dice score of
90.85% and 89.03% respectively, a 0.91% and 2.22% improvement compared to
previous best results. Our method can be trained in an end-to-end fashion,
showing the promise of utilizing SimCVD as a general framework for downstream
tasks, such as medical image synthesis, enhancement, and registration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CDTrans: Cross-domain Transformer for Unsupervised Domain Adaptation. (arXiv:2109.06165v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06165">
<div class="article-summary-box-inner">
<span><p>Unsupervised domain adaptation (UDA) aims to transfer knowledge learned from
a labeled source domain to a different unlabeled target domain. Most existing
UDA methods focus on learning domain-invariant feature representation, either
from the domain level or category level, using convolution neural networks
(CNNs)-based frameworks. One fundamental problem for the category level based
UDA is the production of pseudo labels for samples in target domain, which are
usually too noisy for accurate domain alignment, inevitably compromising the
UDA performance. With the success of Transformer in various tasks, we find that
the cross-attention in Transformer is robust to the noisy input pairs for
better feature alignment, thus in this paper Transformer is adopted for the
challenging UDA task. Specifically, to generate accurate input pairs, we design
a two-way center-aware labeling algorithm to produce pseudo labels for target
samples. Along with the pseudo labels, a weight-sharing triple-branch
transformer framework is proposed to apply self-attention and cross-attention
for source/target feature learning and source-target domain alignment,
respectively. Such design explicitly enforces the framework to learn
discriminative domain-specific and domain-invariant representations
simultaneously. The proposed method is dubbed CDTrans (cross-domain
transformer), and it provides one of the first attempts to solve UDA tasks with
a pure transformer solution. Experiments show that our proposed method achieves
the best performance on public UDA datasets, e.g. VisDA-2017 and DomainNet.
Code and models are available at https://github.com/CDTrans/CDTrans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ElasticFace: Elastic Margin Loss for Deep Face Recognition. (arXiv:2109.09416v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09416">
<div class="article-summary-box-inner">
<span><p>Learning discriminative face features plays a major role in building
high-performing face recognition models. The recent state-of-the-art face
recognition solutions proposed to incorporate a fixed penalty margin on
commonly used classification loss function, softmax loss, in the normalized
hypersphere to increase the discriminative power of face recognition models, by
minimizing the intra-class variation and maximizing the inter-class variation.
Marginal penalty softmax losses, such as ArcFace and CosFace, assume that the
geodesic distance between and within the different identities can be equally
learned using a fixed penalty margin. However, such a learning objective is not
realistic for real data with inconsistent inter-and intra-class variation,
which might limit the discriminative and generalizability of the face
recognition model. In this paper, we relax the fixed penalty margin constrain
by proposing elastic penalty margin loss (ElasticFace) that allows flexibility
in the push for class separability. The main idea is to utilize random margin
values drawn from a normal distribution in each training iteration. This aims
at giving the decision boundary chances to extract and retract to allow space
for flexible class separability learning. We demonstrate the superiority of our
ElasticFace loss over ArcFace and CosFace losses, using the same geometric
transformation, on a large set of mainstream benchmarks. From a wider
perspective, our ElasticFace has advanced the state-of-the-art face recognition
performance on seven out of nine mainstream benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Autoencoder Training Performance for Hyperspectral Unmixing with Network Reinitialisation. (arXiv:2109.13748v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13748">
<div class="article-summary-box-inner">
<span><p>Neural networks, in particular autoencoders, are one of the most promising
solutions for unmixing hyperspectral data, i.e. reconstructing the spectra of
observed substances (endmembers) and their relative mixing fractions
(abundances), which is needed for effective hyperspectral analysis and
classification. However, as we show in this paper, the training of autoencoders
for unmixing is highly dependent on weights initialisation; some sets of
weights lead to degenerate or low-performance solutions, introducing negative
bias in the expected performance. In this work, we experimentally investigate
autoencoders stability as well as network reinitialisation methods based on
coefficients of neurons' dead activations. We demonstrate that the proposed
techniques have a positive effect on autoencoder training in terms of
reconstruction, abundances and endmembers errors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Memory-Guided Semantic Reasoning Model for Image Inpainting. (arXiv:2110.00261v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00261">
<div class="article-summary-box-inner">
<span><p>Most existing methods for image inpainting focus on learning the intra-image
priors from the known regions of the current input image to infer the content
of the corrupted regions in the same image. While such methods perform well on
images with small corrupted regions, it is challenging for these methods to
deal with images with large corrupted area due to two potential limitations: 1)
such methods tend to overfit each single training pair of images relying solely
on the intra-image prior knowledge learned from the limited known area; 2) the
inter-image prior knowledge about the general distribution patterns of visual
semantics, which can be transferred across images sharing similar semantics, is
not exploited. In this paper, we propose the Generative Memory-Guided Semantic
Reasoning Model (GM-SRM), which not only learns the intra-image priors from the
known regions, but also distills the inter-image reasoning priors to infer the
content of the corrupted regions. In particular, the proposed GM-SRM first
pre-learns a generative memory from the whole training data to capture the
semantic distribution patterns in a global view. Then the learned memory are
leveraged to retrieve the matching inter-image priors for the current corrupted
image to perform semantic reasoning during image inpainting. While the
intra-image priors are used for guaranteeing the pixel-level content
consistency, the inter-image priors are favorable for performing high-level
semantic reasoning, which is particularly effective for inferring semantic
content for large corrupted area. Extensive experiments on Paris Street View,
CelebA-HQ, and Places2 benchmarks demonstrate that our GM-SRM outperforms the
state-of-the-art methods for image inpainting in terms of both the visual
quality and quantitative metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optical-Flow-Reuse-Based Bidirectional Recurrent Network for Space-Time Video Super-Resolution. (arXiv:2110.06786v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06786">
<div class="article-summary-box-inner">
<span><p>In this paper, we consider the task of space-time video super-resolution
(ST-VSR), which simultaneously increases the spatial resolution and frame rate
for a given video. However, existing methods typically suffer from difficulties
in how to efficiently leverage information from a large range of neighboring
frames or avoiding the speed degradation in the inference using deformable
ConvLSTM strategies for alignment. % Some recent LSTM-based ST-VSR methods have
achieved promising results. To solve the above problem of the existing methods,
we propose a coarse-to-fine bidirectional recurrent neural network instead of
using ConvLSTM to leverage knowledge between adjacent frames. Specifically, we
first use bi-directional optical flow to update the hidden state and then
employ a Feature Refinement Module (FRM) to refine the result. Since we could
fully utilize a large range of neighboring frames, our method leverages local
and global information more effectively. In addition, we propose an optical
flow-reuse strategy that can reuse the intermediate flow of adjacent frames,
which considerably reduces the computation burden of frame alignment compared
with existing LSTM-based designs. Extensive experiments demonstrate that our
optical-flow-reuse-based bidirectional recurrent network(OFR-BRN) is superior
to the state-of-the-art methods both in terms of accuracy and efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Online Continual Learning on Class Incremental Blurry Task Configuration with Anytime Inference. (arXiv:2110.10031v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10031">
<div class="article-summary-box-inner">
<span><p>Despite rapid advances in continual learning, a large body of research is
devoted to improving performance in the existing setups. While a handful of
work do propose new continual learning setups, they still lack practicality in
certain aspects. For better practicality, we first propose a novel continual
learning setup that is online, task-free, class-incremental, of blurry task
boundaries and subject to inference queries at any moment. We additionally
propose a new metric to better measure the performance of the continual
learning methods subject to inference queries at any moment. To address the
challenging setup and evaluation protocol, we propose an effective method that
employs a new memory management scheme and novel learning techniques. Our
empirical validation demonstrates that the proposed method outperforms prior
arts by large margins. Code and data splits are available at
https://github.com/naver-ai/i-Blurry.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ConAM: Confidence Attention Module for Convolutional Neural Networks. (arXiv:2110.14369v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14369">
<div class="article-summary-box-inner">
<span><p>The so-called "attention" is an efficient mechanism to improve the
performance of convolutional neural networks. It uses contextual information to
recalibrate the input to strengthen the propagation of informative features.
However, the majority of the attention mechanisms only consider either local or
global contextual information, which is singular to extract features. Moreover,
many existing mechanisms directly use the contextual information to recalibrate
the input, which unilaterally enhances the propagation of the informative
features, but does not suppress the useless ones. This paper proposes a new
attention mechanism module based on the correlation between local and global
contextual information and we name this correlation as confidence. The novel
attention mechanism extracts the local and global contextual information
simultaneously, and calculates the confidence between them, then uses this
confidence to recalibrate the input pixels. The extraction of local and global
contextual information increases the diversity of features. The recalibration
with confidence suppresses useless information while enhancing the informative
one with fewer parameters. We use CIFAR-10 and CIFAR-100 in our experiments and
explore the performance of our method's components by sufficient ablation
studies. Finally, we compare our method with a various state-of-the-art
convolutional neural networks and the results show that our method completely
surpasses these models. We implement ConAM with the Python library, Pytorch,
and the code and models will be publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Part Discovery from Contrastive Reconstruction. (arXiv:2111.06349v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.06349">
<div class="article-summary-box-inner">
<span><p>The goal of self-supervised visual representation learning is to learn
strong, transferable image representations, with the majority of research
focusing on object or scene level. On the other hand, representation learning
at part level has received significantly less attention. In this paper, we
propose an unsupervised approach to object part discovery and segmentation and
make three contributions. First, we construct a proxy task through a set of
objectives that encourages the model to learn a meaningful decomposition of the
image into its parts. Secondly, prior work argues for reconstructing or
clustering pre-computed features as a proxy to parts; we show empirically that
this alone is unlikely to find meaningful parts; mainly because of their low
resolution and the tendency of classification networks to spatially smear out
information. We suggest that image reconstruction at the level of pixels can
alleviate this problem, acting as a complementary cue. Lastly, we show that the
standard evaluation based on keypoint regression does not correlate well with
segmentation quality and thus introduce different metrics, NMI and ARI, that
better characterize the decomposition of objects into parts. Our method yields
semantic parts which are consistent across fine-grained but visually distinct
categories, outperforming the state of the art on three benchmark datasets.
Code is available at the project page:
https://www.robots.ox.ac.uk/~vgg/research/unsup-parts/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Color Mapping Functions For HDR Panorama Imaging: Weighted Histogram Averaging. (arXiv:2111.07283v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.07283">
<div class="article-summary-box-inner">
<span><p>It is challenging to stitch multiple images with different exposures due to
possible color distortion and loss of details in the brightest and darkest
regions of input images. In this paper, a novel color mapping algorithm is
first proposed by introducing a new concept of weighted histogram averaging
(WHA). The proposed WHA algorithm leverages the correspondence between the
histogram bins of two images which are built up by using the non-decreasing
property of the color mapping functions (CMFs). The WHA algorithm is then
adopted to synthesize a set of differently exposed panorama images. The
intermediate panorama images are finally fused via a state-of-the-art
multi-scale exposure fusion (MEF) algorithm to produce the final panorama
image. Extensive experiments indicate that the proposed WHA algorithm
significantly surpasses the related state-of-the-art color mapping methods. The
proposed high dynamic range (HDR) stitching algorithm based on MEF also
preserves details in the brightest and darkest regions of the input images
well. The related materials will be publicly accessible at
https://github.com/yilun-xu/WHA for reproducible research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mask-guided Spectral-wise Transformer for Efficient Hyperspectral Image Reconstruction. (arXiv:2111.07910v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.07910">
<div class="article-summary-box-inner">
<span><p>Hyperspectral image (HSI) reconstruction aims to recover the 3D
spatial-spectral signal from a 2D measurement in the coded aperture snapshot
spectral imaging (CASSI) system. The HSI representations are highly similar and
correlated across the spectral dimension. Modeling the inter-spectra
interactions is beneficial for HSI reconstruction. However, existing CNN-based
methods show limitations in capturing spectral-wise similarity and long-range
dependencies. Besides, the HSI information is modulated by a coded aperture
(physical mask) in CASSI. Nonetheless, current algorithms have not fully
explored the guidance effect of the mask for HSI restoration. In this paper, we
propose a novel framework, Mask-guided Spectral-wise Transformer (MST), for HSI
reconstruction. Specifically, we present a Spectral-wise Multi-head
Self-Attention (S-MSA) that treats each spectral feature as a token and
calculates self-attention along the spectral dimension. In addition, we
customize a Mask-guided Mechanism (MM) that directs S-MSA to pay attention to
spatial regions with high-fidelity spectral representations. Extensive
experiments show that our MST significantly outperforms state-of-the-art (SOTA)
methods on simulation and real HSI datasets while requiring dramatically
cheaper computational and memory costs. Code and pre-trained models are
available at https://github.com/caiyuanhao1998/MST/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Segment-level Semantics for Online Phase Recognition from Surgical Videos. (arXiv:2111.11044v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11044">
<div class="article-summary-box-inner">
<span><p>Automatic surgical phase recognition plays an important role in
robot-assisted surgeries. Existing methods ignored a pivotal problem that
surgical phases should be classified by learning segment-level semantics
instead of solely relying on frame-wise information. In this paper, we present
a segment-attentive hierarchical consistency network (SAHC) for surgical phase
recognition from videos. The key idea is to extract hierarchical high-level
semantic-consistent segments and use them to refine the erroneous predictions
caused by ambiguous frames. To achieve it, we design a temporal hierarchical
network to generate hierarchical high-level segments. Then, we introduce a
hierarchical segment-frame attention (SFA) module to capture relations between
the low-level frames and high-level segments. By regularizing the predictions
of frames and their corresponding segments via a consistency loss, the network
can generate semantic-consistent segments and then rectify the misclassified
predictions caused by ambiguous low-level frames. We validate SAHC on two
public surgical video datasets, i.e., the M2CAI16 challenge dataset and the
Cholec80 dataset. Experimental results show that our method outperforms
previous state-of-the-arts by a large margin, notably reaches 3.8% improvements
on M2CAI16.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GeoNeRF: Generalizing NeRF with Geometry Priors. (arXiv:2111.13539v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13539">
<div class="article-summary-box-inner">
<span><p>We present GeoNeRF, a generalizable photorealistic novel view synthesis
method based on neural radiance fields. Our approach consists of two main
stages: a geometry reasoner and a renderer. To render a novel view, the
geometry reasoner first constructs cascaded cost volumes for each nearby source
view. Then, using a Transformer-based attention mechanism and the cascaded cost
volumes, the renderer infers geometry and appearance, and renders detailed
images via classical volume rendering techniques. This architecture, in
particular, allows sophisticated occlusion reasoning, gathering information
from consistent source views. Moreover, our method can easily be fine-tuned on
a single scene, and renders competitive results with per-scene optimized neural
rendering methods with a fraction of computational cost. Experiments show that
GeoNeRF outperforms state-of-the-art generalizable neural rendering models on
various synthetic and real datasets. Lastly, with a slight modification to the
geometry reasoner, we also propose an alternative model that adapts to RGBD
images. This model directly exploits the depth information often available
thanks to depth sensors. The implementation code is available at
https://www.idiap.ch/paper/geonerf.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust and Accurate Superquadric Recovery: a Probabilistic Approach. (arXiv:2111.14517v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14517">
<div class="article-summary-box-inner">
<span><p>Interpreting objects with basic geometric primitives has long been studied in
computer vision. Among geometric primitives, superquadrics are well known for
their ability to represent a wide range of shapes with few parameters. However,
as the first and foremost step, recovering superquadrics accurately and
robustly from 3D data still remains challenging. The existing methods are
subject to local optima and sensitive to noise and outliers in real-world
scenarios, resulting in frequent failure in capturing geometric shapes. In this
paper, we propose the first probabilistic method to recover superquadrics from
point clouds. Our method builds a Gaussian-uniform mixture model (GUM) on the
parametric surface of a superquadric, which explicitly models the generation of
outliers and noise. The superquadric recovery is formulated as a Maximum
Likelihood Estimation (MLE) problem. We propose an algorithm, Expectation,
Maximization, and Switching (EMS), to solve this problem, where: (1) outliers
are predicted from the posterior perspective; (2) the superquadric parameter is
optimized by the trust-region reflective algorithm; and (3) local optima are
avoided by globally searching and switching among parameters encoding similar
superquadrics. We show that our method can be extended to the
multi-superquadrics recovery for complex objects. The proposed method
outperforms the state-of-the-art in terms of accuracy, efficiency, and
robustness on both synthetic and real-world datasets. The code is at
<a href="http://github.com/bmlklwx/EMS-superquadric_fitting.git.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FENeRF: Face Editing in Neural Radiance Fields. (arXiv:2111.15490v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.15490">
<div class="article-summary-box-inner">
<span><p>Previous portrait image generation methods roughly fall into two categories:
2D GANs and 3D-aware GANs. 2D GANs can generate high fidelity portraits but
with low view consistency. 3D-aware GAN methods can maintain view consistency
but their generated images are not locally editable. To overcome these
limitations, we propose FENeRF, a 3D-aware generator that can produce
view-consistent and locally-editable portrait images. Our method uses two
decoupled latent codes to generate corresponding facial semantics and texture
in a spatial aligned 3D volume with shared geometry. Benefiting from such
underlying 3D representation, FENeRF can jointly render the boundary-aligned
image and semantic mask and use the semantic mask to edit the 3D volume via GAN
inversion. We further show such 3D representation can be learned from widely
available monocular image and semantic mask pairs. Moreover, we reveal that
joint learning semantics and texture helps to generate finer geometry. Our
experiments demonstrate that FENeRF outperforms state-of-the-art methods in
various face editing tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open-Domain, Content-based, Multi-modal Fact-checking of Out-of-Context Images via Online Resources. (arXiv:2112.00061v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00061">
<div class="article-summary-box-inner">
<span><p>Misinformation is now a major problem due to its potential high risks to our
core democratic and societal values and orders. Out-of-context misinformation
is one of the easiest and effective ways used by adversaries to spread viral
false stories. In this threat, a real image is re-purposed to support other
narratives by misrepresenting its context and/or elements. The internet is
being used as the go-to way to verify information using different sources and
modalities. Our goal is an inspectable method that automates this
time-consuming and reasoning-intensive process by fact-checking the
image-caption pairing using Web evidence. To integrate evidence and cues from
both modalities, we introduce the concept of 'multi-modal cycle-consistency
check'; starting from the image/caption, we gather textual/visual evidence,
which will be compared against the other paired caption/image, respectively.
Moreover, we propose a novel architecture, Consistency-Checking Network (CCN),
that mimics the layered human reasoning across the same and different
modalities: the caption vs. textual evidence, the image vs. visual evidence,
and the image vs. caption. Our work offers the first step and benchmark for
open-domain, content-based, multi-modal fact-checking, and significantly
outperforms previous baselines that did not leverage external evidence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdaAfford: Learning to Adapt Manipulation Affordance for 3D Articulated Objects via Few-shot Interactions. (arXiv:2112.00246v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00246">
<div class="article-summary-box-inner">
<span><p>Perceiving and interacting with 3D articulated objects, such as cabinets,
doors, and faucets, pose particular challenges for future home-assistant robots
performing daily tasks in human environments. Besides parsing the articulated
parts and joint parameters, researchers recently advocate learning manipulation
affordance over the input shape geometry which is more task-aware and
geometrically fine-grained. However, taking only passive observations as
inputs, these methods ignore many hidden but important kinematic constraints
(e.g., joint location and limits) and dynamic factors (e.g., joint friction and
restitution), therefore losing significant accuracy for test cases with such
uncertainties. In this paper, we propose a novel framework, named AdaAfford,
that learns to perform very few test-time interactions for quickly adapting the
affordance priors to more accurate instance-specific posteriors. We conduct
large-scale experiments using the PartNet-Mobility dataset and prove that our
system performs better than baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Confidence Propagation Cluster: Unleash Full Potential of Object Detectors. (arXiv:2112.00342v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00342">
<div class="article-summary-box-inner">
<span><p>It has been a long history that most object detection methods obtain objects
by using the non-maximum suppression (NMS) and its improved versions like
Soft-NMS to remove redundant bounding boxes. We challenge those NMS-based
methods from three aspects: 1) The bounding box with highest confidence value
may not be the true positive having the biggest overlap with the ground-truth
box. 2) Not only suppression is required for redundant boxes, but also
confidence enhancement is needed for those true positives. 3) Sorting candidate
boxes by confidence values is not necessary so that full parallelism is
achievable.
</p>
<p>In this paper, inspired by belief propagation (BP), we propose the Confidence
Propagation Cluster (CP-Cluster) to replace NMS-based methods, which is fully
parallelizable as well as better in accuracy. In CP-Cluster, we borrow the
message passing mechanism from BP to penalize redundant boxes and enhance true
positives simultaneously in an iterative way until convergence. We verified the
effectiveness of CP-Cluster by applying it to various mainstream detectors such
as FasterRCNN, SSD, FCOS, YOLOv3, YOLOv5, Centernet etc. Experiments on MS COCO
show that our plug and play method, without retraining detectors, is able to
steadily improve average mAP of all those state-of-the-art models with a clear
margin from 0.3 to 1.9 respectively when compared with NMS-based methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLIPstyler: Image Style Transfer with a Single Text Condition. (arXiv:2112.00374v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00374">
<div class="article-summary-box-inner">
<span><p>Existing neural style transfer methods require reference style images to
transfer texture information of style images to content images. However, in
many practical situations, users may not have reference style images but still
be interested in transferring styles by just imagining them. In order to deal
with such applications, we propose a new framework that enables a style
transfer `without' a style image, but only with a text description of the
desired style. Using the pre-trained text-image embedding model of CLIP, we
demonstrate the modulation of the style of content images only with a single
text condition. Specifically, we propose a patch-wise text-image matching loss
with multiview augmentations for realistic texture transfer. Extensive
experimental results confirmed the successful image style transfer with
realistic textures that reflect semantic query texts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Video Transformer. (arXiv:2112.01514v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01514">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose self-supervised training for video transformers
using unlabeled video data. From a given video, we create local and global
spatiotemporal views with varying spatial sizes and frame rates. Our
self-supervised objective seeks to match the features of these different views
representing the same video, to be invariant to spatiotemporal variations in
actions. To the best of our knowledge, the proposed approach is the first to
alleviate the dependency on negative samples or dedicated memory banks in
Self-supervised Video Transformer (SVT). Further, owing to the flexibility of
Transformer models, SVT supports slow-fast video processing within a single
architecture using dynamically adjusted positional encoding and supports
long-term relationship modeling along spatiotemporal dimensions. Our approach
performs well on four action recognition benchmarks (Kinetics-400, UCF-101,
HMDB-51, and SSv2) and converges faster with small batch sizes. Code:
https://git.io/J1juJ
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting. (arXiv:2112.01518v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01518">
<div class="article-summary-box-inner">
<span><p>Recent progress has shown that large-scale pre-training using contrastive
image-text pairs can be a promising alternative for high-quality visual
representation learning from natural language supervision. Benefiting from a
broader source of supervision, this new paradigm exhibits impressive
transferability to downstream classification tasks and datasets. However, the
problem of transferring the knowledge learned from image-text pairs to more
complex dense prediction tasks has barely been visited. In this work, we
present a new framework for dense prediction by implicitly and explicitly
leveraging the pre-trained knowledge from CLIP. Specifically, we convert the
original image-text matching problem in CLIP to a pixel-text matching problem
and use the pixel-text score maps to guide the learning of dense prediction
models. By further using the contextual information from the image to prompt
the language model, we are able to facilitate our model to better exploit the
pre-trained knowledge. Our method is model-agnostic, which can be applied to
arbitrary dense prediction systems and various pre-trained visual backbones
including both CLIP models and ImageNet pre-trained models. Extensive
experiments demonstrate the superior performance of our methods on semantic
segmentation, object detection, and instance segmentation tasks. Code is
available at https://github.com/raoyongming/DenseCLIP
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Input-level Inductive Biases for 3D Reconstruction. (arXiv:2112.03243v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03243">
<div class="article-summary-box-inner">
<span><p>Much of the recent progress in 3D vision has been driven by the development
of specialized architectures that incorporate geometrical inductive biases. In
this paper we tackle 3D reconstruction using a domain agnostic architecture and
study how instead to inject the same type of inductive biases directly as extra
inputs to the model. This approach makes it possible to apply existing general
models, such as Perceivers, on this rich domain, without the need for
architectural changes, while simultaneously maintaining data efficiency of
bespoke models. In particular we study how to encode cameras, projective ray
incidence and epipolar geometry as model inputs, and demonstrate competitive
multi-view depth estimation performance on multiple benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Conditional Point Diffusion-Refinement Paradigm for 3D Point Cloud Completion. (arXiv:2112.03530v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03530">
<div class="article-summary-box-inner">
<span><p>3D point cloud is an important 3D representation for capturing real world 3D
objects. However, real-scanned 3D point clouds are often incomplete, and it is
important to recover complete point clouds for downstream applications. Most
existing point cloud completion methods use Chamfer Distance (CD) loss for
training. The CD loss estimates correspondences between two point clouds by
searching nearest neighbors, which does not capture the overall point density
distribution on the generated shape, and therefore likely leads to non-uniform
point cloud generation. To tackle this problem, we propose a novel Point
Diffusion-Refinement (PDR) paradigm for point cloud completion. PDR consists of
a Conditional Generation Network (CGNet) and a ReFinement Network (RFNet). The
CGNet uses a conditional generative model called the denoising diffusion
probabilistic model (DDPM) to generate a coarse completion conditioned on the
partial observation. DDPM establishes a one-to-one pointwise mapping between
the generated point cloud and the uniform ground truth, and then optimizes the
mean squared error loss to realize uniform generation. The RFNet refines the
coarse output of the CGNet and further improves quality of the completed point
cloud. Furthermore, we develop a novel dual-path architecture for both
networks. The architecture can (1) effectively and efficiently extract
multi-level features from partially observed point clouds to guide completion,
and (2) accurately manipulate spatial locations of 3D points to obtain smooth
surfaces and sharp details. Extensive experimental results on various benchmark
datasets show that our PDR paradigm outperforms previous state-of-the-art
methods for point cloud completion. Remarkably, with the help of the RFNet, we
can accelerate the iterative generation process of the DDPM by up to 50 times
without much performance drop.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GaTector: A Unified Framework for Gaze Object Prediction. (arXiv:2112.03549v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03549">
<div class="article-summary-box-inner">
<span><p>Gaze object prediction is a newly proposed task that aims to discover the
objects being stared at by humans. It is of great application significance but
still lacks a unified solution framework. An intuitive solution is to
incorporate an object detection branch into an existing gaze prediction method.
However, previous gaze prediction methods usually use two different networks to
extract features from scene image and head image, which would lead to heavy
network architecture and prevent each branch from joint optimization. In this
paper, we build a novel framework named GaTector to tackle the gaze object
prediction problem in a unified way. Particularly, a specific-general-specific
(SGS) feature extractor is firstly proposed to utilize a shared backbone to
extract general features for both scene and head images. To better consider the
specificity of inputs and tasks, SGS introduces two input-specific blocks
before the shared backbone and three task-specific blocks after the shared
backbone. Specifically, a novel Defocus layer is designed to generate
object-specific features for the object detection task without losing
information or requiring extra computations. Moreover, the energy aggregation
loss is introduced to guide the gaze heatmap to concentrate on the stared box.
In the end, we propose a novel wUoC metric that can reveal the difference
between boxes even when they share no overlapping area. Extensive experiments
on the GOO dataset verify the superiority of our method in all three tracks,
i.e. object detection, gaze estimation, and gaze object prediction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Come-Closer-Diffuse-Faster: Accelerating Conditional Diffusion Models for Inverse Problems through Stochastic Contraction. (arXiv:2112.05146v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05146">
<div class="article-summary-box-inner">
<span><p>Diffusion models have recently attained significant interest within the
community owing to their strong performance as generative models. Furthermore,
its application to inverse problems have demonstrated state-of-the-art
performance. Unfortunately, diffusion models have a critical downside - they
are inherently slow to sample from, needing few thousand steps of iteration to
generate images from pure Gaussian noise. In this work, we show that starting
from Gaussian noise is unnecessary. Instead, starting from a single forward
diffusion with better initialization significantly reduces the number of
sampling steps in the reverse conditional diffusion. This phenomenon is
formally explained by the contraction theory of the stochastic difference
equations like our conditional diffusion strategy - the alternating
applications of reverse diffusion followed by a non-expansive data consistency
step. The new sampling strategy, dubbed Come-Closer-Diffuse-Faster (CCDF), also
reveals a new insight on how the existing feed-forward neural network
approaches for inverse problems can be synergistically combined with the
diffusion models. Experimental results with super-resolution, image inpainting,
and compressed sensing MRI demonstrate that our method can achieve
state-of-the-art reconstruction performance at significantly reduced sampling
steps.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COMPOSER: Compositional Reasoning of Group Activity in Videos with Keypoint-Only Modality. (arXiv:2112.05892v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05892">
<div class="article-summary-box-inner">
<span><p>Group Activity Recognition detects the activity collectively performed by a
group of actors, which requires compositional reasoning of actors and objects.
We approach the task by modeling the video as tokens that represent the
multi-scale semantic concepts in the video. We propose COMPOSER, a Multiscale
Transformer based architecture that performs attention-based reasoning over
tokens at each scale and learns group activity compositionally. In addition,
prior works suffer from scene biases with privacy and ethical concerns. We only
use the keypoint modality which reduces scene biases and prevents acquiring
detailed visual data that may contain private or biased information of users.
We improve the multiscale representations in COMPOSER by clustering the
intermediate scale representations, while maintaining consistent cluster
assignments between scales. Finally, we use techniques such as auxiliary
prediction and data augmentations tailored to the keypoint signals to aid model
training. We demonstrate the model's strength and interpretability on two
widely-used datasets (Volleyball and Collective Activity). COMPOSER achieves up
to +5.4% improvement with just the keypoint modality. Our code will be made
publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stereoscopic Universal Perturbations across Different Architectures and Datasets. (arXiv:2112.06116v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06116">
<div class="article-summary-box-inner">
<span><p>We study the effect of adversarial perturbations of images on deep stereo
matching networks for the disparity estimation task. We present a method to
craft a single set of perturbations that, when added to any stereo image pair
in a dataset, can fool a stereo network to significantly alter the perceived
scene geometry. Our perturbation images are "universal" in that they not only
corrupt estimates of the network on the dataset they are optimized for, but
also generalize to different architectures trained on different datasets. We
evaluate our approach on multiple benchmark datasets where our perturbations
can increase the D1-error (akin to fooling rate) of state-of-the-art stereo
networks from 1% to as much as 87%. We investigate the effect of perturbations
on the estimated scene geometry and identify object classes that are most
vulnerable. Our analysis on the activations of registered points between left
and right images led us to find architectural components that can increase
robustness against adversaries. By simply designing networks with such
components, one can reduce the effect of adversaries by up to 60.5%, which
rivals the robustness of networks fine-tuned with costly adversarial data
augmentation. Our design principle also improves their robustness against
common image corruptions by an average of 70%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards General and Efficient Active Learning. (arXiv:2112.07963v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07963">
<div class="article-summary-box-inner">
<span><p>Active learning selects the most informative samples to exploit limited
annotation budgets. Existing work follows a cumbersome pipeline that repeats
the time-consuming model training and batch data selection multiple times. In
this paper, we challenge this status quo by proposing a novel general and
efficient active learning (GEAL) method following our designed new pipeline.
Utilizing a publicly available pretrained model, our method selects data from
different datasets with a single-pass inference of the same model without extra
training or supervision. To capture subtle local information, we propose
knowledge clusters extracted from intermediate features. Free from the
troublesome batch selection strategy, all data samples are selected in one-shot
through a distance-based sampling in the fine-grained knowledge cluster level.
This whole process is faster than prior arts by hundreds of times. Extensive
experiments verify the effectiveness of our method on object detection, image
classification, and semantic segmentation. Our code is publicly available in
https://github.com/yichen928/GEAL_active_learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CrossLoc: Scalable Aerial Localization Assisted by Multimodal Synthetic Data. (arXiv:2112.09081v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09081">
<div class="article-summary-box-inner">
<span><p>We present a visual localization system that learns to estimate camera poses
in the real world with the help of synthetic data. Despite significant progress
in recent years, most learning-based approaches to visual localization target
at a single domain and require a dense database of geo-tagged images to
function well. To mitigate the data scarcity issue and improve the scalability
of the neural localization models, we introduce TOPO-DataGen, a versatile
synthetic data generation tool that traverses smoothly between the real and
virtual world, hinged on the geographic camera viewpoint. New large-scale
sim-to-real benchmark datasets are proposed to showcase and evaluate the
utility of the said synthetic data. Our experiments reveal that synthetic data
generically enhances the neural network performance on real data. Furthermore,
we introduce CrossLoc, a cross-modal visual representation learning approach to
pose estimation that makes full use of the scene coordinate ground truth via
self-supervision. Without any extra data, CrossLoc significantly outperforms
the state-of-the-art methods and achieves substantially higher real-data sample
efficiency. Our code and datasets are all available at
https://crossloc.github.io/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Strong Scaling Through Burst Parallel Training. (arXiv:2112.10065v2 [cs.DC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10065">
<div class="article-summary-box-inner">
<span><p>As emerging deep neural network (DNN) models continue to grow in size, using
large GPU clusters to train DNNs is becoming an essential requirement to
achieving acceptable training times. In this paper, we consider the case where
future increases in cluster size will cause the global batch size that can be
used to train models to reach a fundamental limit: beyond a certain point,
larger global batch sizes cause sample efficiency to degrade, increasing
overall time to accuracy. As a result, to achieve further improvements in
training performance, we must instead consider "strong scaling" strategies that
hold the global batch size constant and allocate smaller batches to each GPU.
Unfortunately, this makes it significantly more difficult to use cluster
resources efficiently. We present DeepPool, a system that addresses this
efficiency challenge through two key ideas. First, burst parallelism allocates
large numbers of GPUs to foreground jobs in bursts to exploit the unevenness in
parallelism across layers. Second, GPU multiplexing prioritizes throughput for
foreground training jobs, while packing in background training jobs to reclaim
underutilized GPU resources, thereby improving cluster-wide utilization.
Together, these two ideas enable DeepPool to deliver a 1.2 - 2.3x improvement
in total cluster throughput over standard data parallelism with a single task
when the cluster scale is large.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Efficient Transformer-Based Image Pre-training for Low-Level Vision. (arXiv:2112.10175v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10175">
<div class="article-summary-box-inner">
<span><p>Pre-training has marked numerous state of the arts in high-level computer
vision, while few attempts have ever been made to investigate how pre-training
acts in image processing systems. In this paper, we tailor transformer-based
pre-training regimes that boost various low-level tasks. To comprehensively
diagnose the influence of pre-training, we design a whole set of principled
evaluation tools that uncover its effects on internal representations. The
observations demonstrate that pre-training plays strikingly different roles in
low-level tasks. For example, pre-training introduces more local information to
higher layers in super-resolution (SR), yielding significant performance gains,
while pre-training hardly affects internal feature representations in
denoising, resulting in limited gains. Further, we explore different methods of
pre-training, revealing that multi-related-task pre-training is more effective
and data-efficient than other alternatives. Finally, we extend our study to
varying data scales and model sizes, as well as comparisons between
transformers and CNNs-based architectures. Based on the study, we successfully
develop state-of-the-art models for multiple low-level tasks. Code is released
at https://github.com/fenglinglwb/EDT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Projected Sliced Wasserstein Autoencoder-based Hyperspectral Images Anomaly Detection. (arXiv:2112.11243v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.11243">
<div class="article-summary-box-inner">
<span><p>Anomaly detection (AD) has been an active research area in various domains.
Yet, the increasing data scale, complexity, and dimension turn the traditional
methods into challenging. Recently, the deep generative model, such as the
variational autoencoder (VAE), has sparked a renewed interest in the AD
problem. However, the probability distribution divergence used as the
regularization is too strong, which causes the model cannot capture the
manifold of the true data. In this paper, we propose the Projected Sliced
Wasserstein (PSW) autoencoder-based anomaly detection method. Rooted in the
optimal transportation, the PSW distance is a weaker distribution measure
compared with $f$-divergence. In particular, the computation-friendly
eigen-decomposition method is leveraged to find the principal component for
slicing the high-dimensional data. In this case, the Wasserstein distance can
be calculated with the closed-form, even the prior distribution is not
Gaussian. Comprehensive experiments conducted on various real-world
hyperspectral anomaly detection benchmarks demonstrate the superior performance
of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FedFR: Joint Optimization Federated Framework for Generic and Personalized Face Recognition. (arXiv:2112.12496v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.12496">
<div class="article-summary-box-inner">
<span><p>Current state-of-the-art deep learning based face recognition (FR) models
require a large number of face identities for central training. However, due to
the growing privacy awareness, it is prohibited to access the face images on
user devices to continually improve face recognition models. Federated Learning
(FL) is a technique to address the privacy issue, which can collaboratively
optimize the model without sharing the data between clients. In this work, we
propose a FL based framework called FedFR to improve the generic face
representation in a privacy-aware manner. Besides, the framework jointly
optimizes personalized models for the corresponding clients via the proposed
Decoupled Feature Customization module. The client-specific personalized model
can serve the need of optimized face recognition experience for registered
identities at the local device. To the best of our knowledge, we are the first
to explore the personalized face recognition in FL setup. The proposed
framework is validated to be superior to previous approaches on several generic
and personalized face recognition benchmarks with diverse FL scenarios. The
source codes and our proposed personalized FR benchmark under FL setup are
available at https://github.com/jackie840129/FedFR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Linear Variational State-Space Filtering. (arXiv:2201.01353v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01353">
<div class="article-summary-box-inner">
<span><p>We introduce Variational State-Space Filters (VSSF), a new method for
unsupervised learning, identification, and filtering of latent Markov state
space models from raw pixels. We present a theoretically sound framework for
latent state space inference under heterogeneous sensor configurations. The
resulting model can integrate an arbitrary subset of the sensor measurements
used during training, enabling the learning of semi-supervised state
representations, thus enforcing that certain components of the learned latent
state space to agree with interpretable measurements. From this framework we
derive L-VSSF, an explicit instantiation of this model with linear latent
dynamics and Gaussian distribution parameterizations. We experimentally
demonstrate L-VSSF's ability to filter in latent space beyond the sequence
length of the training dataset across several different test environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pyramid Fusion Transformer for Semantic Segmentation. (arXiv:2201.04019v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.04019">
<div class="article-summary-box-inner">
<span><p>The recently proposed MaskFormer gives a refreshed perspective on the task of
semantic segmentation: it shifts from the popular pixel-level classification
paradigm to a mask-level classification method. In essence, it generates paired
probabilities and masks corresponding to category segments and combines them
during inference for the segmentation maps. In our study, we find that per-mask
classification decoder on top of a single-scale feature is not effective enough
to extract reliable probability or mask. To mine for rich semantic information
across the feature pyramid, we propose a transformer-based Pyramid Fusion
Transformer (PFT) for per-mask approach semantic segmentation with multi-scale
features. The proposed transformer decoder performs cross-attention between the
learnable queries and each spatial feature from the feature pyramid in parallel
and uses cross-scale inter-query attention to exchange complimentary
information. We achieve competitive performance on three widely used semantic
segmentation datasets. In particular, on ADE20K validation set, our result with
Swin-B backbone surpasses that of MaskFormer's with a much larger Swin-L
backbone in both single-scale and multi-scale inference, achieving 54.1 mIoU
and 55.7 mIoU respectively. Using a Swin-L backbone, we achieve single-scale
56.1 mIoU and multi-scale 57.4 mIoU, obtaining state-of-the-art performance on
the dataset. Extensive experiments on three widely used semantic segmentation
datasets verify the effectiveness of our proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Adaptation in LiDAR Semantic Segmentation via Alternating Skip Connections and Hybrid Learning. (arXiv:2201.05585v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05585">
<div class="article-summary-box-inner">
<span><p>In this paper we address the challenging problem of domain adaptation in
LiDAR semantic segmentation. We consider the setting where we have a
fully-labeled data set from source domain and a target domain with a few
labeled and many unlabeled examples. We propose a domain adaption framework
that mitigates the issue of domain shift and produces appealing performance on
the target domain. To this end, we develop a GAN-based image-to-image
translation engine that has generators with alternating connections, and couple
it with a state-of-the-art LiDAR semantic segmentation network. Our framework
is hybrid in nature in the sense that our model learning is composed of
self-supervision, semi-supervision and unsupervised learning. Extensive
experiments on benchmark LiDAR semantic segmentation data sets demonstrate that
our method achieves superior performance in comparison to strong baselines and
prior arts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Variable Augmented Network for Invertible MR Coil Compression. (arXiv:2201.07428v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07428">
<div class="article-summary-box-inner">
<span><p>A large number of coils are able to provide enhanced signal-to-noise ratio
and improve imaging performance in parallel imaging. Nevertheless, the
increasing growth of coil number simultaneously aggravates the drawbacks of
data storage and reconstruction speed, especially in some iterative
reconstructions. Coil compression addresses these issues by generating fewer
virtual coils. In this work, a novel variable augmentation network for
invertible coil compression termed VAN-ICC is presented. It utilizes inherent
reversibility of normalizing flow-based models for high-precision compression
and invertible recovery. By employing the variable augmentation technology to
image/k-space variables from multi-coils, VAN-ICC trains invertible networks by
finding an invertible and bijective function, which can map the original data
to the compressed counterpart and vice versa. Experiments conducted on both
fully-sampled and under-sampled data verified the effectiveness and flexibility
of VAN-ICC. Quantitative and qualitative comparisons with traditional non-deep
learning-based approaches demonstrated that VAN-ICC can carry much higher
compression effects. Additionally, its performance is not susceptible to
different number of virtual coils.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Virtual Coil Augmentation Technology for MR Coil Extrapolation via Deep Learning. (arXiv:2201.07540v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07540">
<div class="article-summary-box-inner">
<span><p>Magnetic resonance imaging (MRI) is a widely used medical imaging modality.
However, due to the limitations in hardware, scan time, and throughput, it is
often clinically challenging to obtain high-quality MR images. In this article,
we propose a method of using artificial intelligence to expand the channel to
achieve the goal of generating the virtual coils. The main characteristic of
our work is utilizing dummy variable technology to expand/extrapolate the
receive coils in both image and k-space domains. The high-dimensional
information formed by channel expansion is used as the prior information to
improve the reconstruction effect of parallel imaging. Two main components are
incorporated into the network design, namely variable augmentation technology
and sum of squares (SOS) objective function. Variable augmentation provides the
network with more high-dimensional prior information, which is helpful for the
network to extract the deep feature information of the data. The SOS objective
function is employed to solve the deficiency of k-space data training while
speeding up convergence. Experimental results demonstrated its great potentials
in super-resolution of MR images and accelerated parallel imaging
reconstruction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Point-NeRF: Point-based Neural Radiance Fields. (arXiv:2201.08845v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08845">
<div class="article-summary-box-inner">
<span><p>Volumetric neural rendering methods like NeRF generate high-quality view
synthesis results but are optimized per-scene leading to prohibitive
reconstruction time. On the other hand, deep multi-view stereo methods can
quickly reconstruct scene geometry via direct network inference. Point-NeRF
combines the advantages of these two approaches by using neural 3D point
clouds, with associated neural features, to model a radiance field. Point-NeRF
can be rendered efficiently by aggregating neural point features near scene
surfaces, in a ray marching-based rendering pipeline. Moreover, Point-NeRF can
be initialized via direct inference of a pre-trained deep network to produce a
neural point cloud; this point cloud can be finetuned to surpass the visual
quality of NeRF with 30X faster training time. Point-NeRF can be combined with
other 3D reconstruction methods and handles the errors and outliers in such
methods via a novel pruning and growing mechanism. The experiments on the DTU,
the NeRF Synthetics , the ScanNet and the Tanks and Temples datasets
demonstrate Point-NeRF can surpass the existing methods and achieve the
state-of-the-art results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Object Counting with Similarity-Aware Feature Enhancement. (arXiv:2201.08959v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.08959">
<div class="article-summary-box-inner">
<span><p>This work studies the problem of few-shot object counting, which counts the
number of exemplar objects (i.e., described by one or several support images)
occurring in the query image. The major challenge lies in that the target
objects can be densely packed in the query image, making it hard to recognize
every single one. To tackle the obstacle, we propose a novel learning block,
equipped with a similarity comparison module (SCM) and a feature enhancement
module (FEM). Concretely, given a support image and a query image, we first
derive a score map by comparing their projected features at every spatial
position. The score maps regarding all support images are collected together
and normalized across both the exemplar dimension and the spatial dimensions,
producing a reliable similarity map. We then enhance the query feature with the
support features by employing the developed point-wise similarities as the
weighting coefficients. Such a design encourages the model to inspect the query
image by focusing more on the regions akin to the support images, leading to
much clearer boundaries between different objects. Extensive experiments on
various benchmarks and training setups suggest that our method surpasses the
state-of-the-art approaches by a sufficiently large margin. For instance, on
the very recent large-scale FSC-147 dataset, we beat the second competitor by
improving the mean absolute counting error from 22.08 to 14.32 (35%
$\uparrow$).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Infrastructure-Based Object Detection and Tracking for Cooperative Driving Automation: A Survey. (arXiv:2201.11871v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.11871">
<div class="article-summary-box-inner">
<span><p>Object detection plays a fundamental role in enabling Cooperative Driving
Automation (CDA), which is regarded as the revolutionary solution to addressing
safety, mobility, and sustainability issues of contemporary transportation
systems. Although current computer vision technologies could provide
satisfactory object detection results in occlusion-free scenarios, the
perception performance of onboard sensors could be inevitably limited by the
range and occlusion. Owing to flexible position and pose for sensor
installation, infrastructure-based detection and tracking systems can enhance
the perception capability for connected vehicles and thus quickly become one of
the most popular research topics. In this paper, we review the research
progress for infrastructure-based object detection and tracking systems.
Architectures of roadside perception systems based on different types of
sensors are reviewed to show a high-level description of the workflows for
infrastructure-based perception systems. Roadside sensors and different
perception methodologies are reviewed and analyzed with detailed literature to
provide a low-level explanation for specific methods followed by Datasets and
Simulators to draw an overall landscape of infrastructure-based object
detection and tracking methods. Discussions are conducted to point out current
opportunities, open problems, and anticipated future trends.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extracting Built Environment Features for Planning Research with Computer Vision: A Review and Discussion of State-of-the-Art Approaches. (arXiv:2201.12693v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.12693">
<div class="article-summary-box-inner">
<span><p>This is an extended abstract for a presentation at The 17th International
Conference on CUPUM - Computational Urban Planning and Urban Management in June
2021. This study presents an interdisciplinary synthesis of the
state-of-the-art approaches in computer vision technologies to extract built
environment features that could improve the robustness of empirical research in
planning. We discussed the findings from the review of studies in both planning
and computer science.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Features with Parameter-Free Layers. (arXiv:2202.02777v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.02777">
<div class="article-summary-box-inner">
<span><p>Trainable layers such as convolutional building blocks are the standard
network design choices by learning parameters to capture the global context
through successive spatial operations. When designing an efficient network,
trainable layers such as the depthwise convolution is the source of efficiency
in the number of parameters and FLOPs, but there was little improvement to the
model speed in practice. This paper argues that simple built-in parameter-free
operations can be a favorable alternative to the efficient trainable layers
replacing spatial operations in a network architecture. We aim to break the
stereotype of organizing the spatial operations of building blocks into
trainable layers. Extensive experimental analyses based on layer-level studies
with fully-trained models and neural architecture searches are provided to
investigate whether parameter-free operations such as the max-pool are
functional. The studies eventually give us a simple yet effective idea for
redesigning network architectures, where the parameter-free operations are
heavily used as the main building block without sacrificing the model accuracy
as much. Experimental results on the ImageNet dataset demonstrate that the
network architectures with parameter-free operations could enjoy the advantages
of further efficiency in terms of model speed, the number of the parameters,
and FLOPs. Code and ImageNet pretrained models are available at
https://github.com/naver-ai/PfLayer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HDAM: Heuristic Difference Attention Module for Convolutional Neural Networks. (arXiv:2202.09556v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09556">
<div class="article-summary-box-inner">
<span><p>The attention mechanism is one of the most important priori knowledge to
enhance convolutional neural networks. Most attention mechanisms are bound to
the convolutional layer and use local or global contextual information to
recalibrate the input. This is a popular attention strategy design method.
Global contextual information helps the network to consider the overall
distribution, while local contextual information is more general. The
contextual information makes the network pay attention to the mean or maximum
value of a particular receptive field. Different from the most attention
mechanism, this article proposes a novel attention mechanism with the heuristic
difference attention module, HDAM. HDAM's input recalibration is based on the
difference between the local and global contextual information instead of the
mean and maximum values. At the same time, to make different layers have a more
suitable local receptive field size and increase the exibility of the local
receptive field design, we use genetic algorithm to heuristically produce local
receptive fields. First, HDAM extracts the mean value of the global and local
receptive fields as the corresponding contextual information. Then the
difference between the global and local contextual information is calculated.
Finally HDAM uses this difference to recalibrate the input. In addition, we use
the heuristic ability of genetic algorithm to search for the local receptive
field size of each layer. Our experiments on CIFAR-10 and CIFAR-100 show that
HDAM can use fewer parameters than other attention mechanisms to achieve higher
accuracy. We implement HDAM with the Python library, Pytorch, and the code and
models will be publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tripartite: Tackle Noisy Labels by a More Precise Partition. (arXiv:2202.09579v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09579">
<div class="article-summary-box-inner">
<span><p>Samples in large-scale datasets may be mislabeled due to various reasons, and
Deep Neural Networks can easily over-fit to the noisy label data. To tackle
this problem, the key point is to alleviate the harm of these noisy labels.
Many existing methods try to divide training data into clean and noisy subsets
in terms of loss values, and then process the noisy label data varied. One of
the reasons hindering a better performance is the hard samples. As hard samples
always have relatively large losses whether their labels are clean or noisy,
these methods could not divide them precisely. Instead, we propose a Tripartite
solution to partition training data more precisely into three subsets: hard,
noisy, and clean. The partition criteria are based on the inconsistent
predictions of two networks, and the inconsistency between the prediction of a
network and the given label. To minimize the harm of noisy labels but maximize
the value of noisy label data, we apply a low-weight learning on hard data and
a self-supervised learning on noisy label data without using the given labels.
Extensive experiments demonstrate that Tripartite can filter out noisy label
data more precisely, and outperforms most state-of-the-art methods on five
benchmark datasets, especially on real-world datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Smoothing and Thresholding Image Segmentation Framework with Weighted Anisotropic-Isotropic Total Variation. (arXiv:2202.10115v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.10115">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a multi-stage image segmentation framework that
incorporates a weighted difference of anisotropic and isotropic total variation
(AITV). The segmentation framework generally consists of two stages: smoothing
and thresholding, thus referred to as SaT. In the first stage, a smoothed image
is obtained by an AITV-regularized Mumford-Shah (MS) model, which can be solved
efficiently by the alternating direction method of multipliers (ADMM) with a
closed-form solution of a proximal operator of the $\ell_1 -\alpha \ell_2$
regularizer. Convergence of the ADMM algorithm is analyzed. In the second
stage, we threshold the smoothed image by $k$-means clustering to obtain the
final segmentation result. Numerical experiments demonstrate that the proposed
segmentation framework is versatile for both grayscale and color images,
efficient in producing high-quality segmentation results within a few seconds,
and robust to input images that are corrupted with noise, blur, or both. We
compare the AITV method with its original convex and nonconvex TV$^p (0&lt;p&lt;1)$
counterparts, showcasing the qualitative and quantitative advantages of our
proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RIConv++: Effective Rotation Invariant Convolutions for 3D Point Clouds Deep Learning. (arXiv:2202.13094v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13094">
<div class="article-summary-box-inner">
<span><p>3D point clouds deep learning is a promising field of research that allows a
neural network to learn features of point clouds directly, making it a robust
tool for solving 3D scene understanding tasks. While recent works show that
point cloud convolutions can be invariant to translation and point permutation,
investigations of the rotation invariance property for point cloud convolution
has been so far scarce. Some existing methods perform point cloud convolutions
with rotation-invariant features, existing methods generally do not perform as
well as translation-invariant only counterpart. In this work, we argue that a
key reason is that compared to point coordinates, rotation-invariant features
consumed by point cloud convolution are not as distinctive. To address this
problem, we propose a simple yet effective convolution operator that enhances
feature distinction by designing powerful rotation invariant features from the
local regions. We consider the relationship between the point of interest and
its neighbors as well as the internal relationship of the neighbors to largely
improve the feature descriptiveness. Our network architecture can capture both
local and global context by simply tuning the neighborhood size in each
convolution layer. We conduct several experiments on synthetic and real-world
point cloud classifications, part segmentation, and shape retrieval to evaluate
our method, which achieves the state-of-the-art accuracy under challenging
rotations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Class-agnostic Tracking Using Feature Decorrelation in Point Clouds. (arXiv:2202.13524v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13524">
<div class="article-summary-box-inner">
<span><p>Single object tracking in point clouds has been attracting more and more
attention owing to the presence of LiDAR sensors in 3D vision. However, the
existing methods based on deep neural networks focus mainly on training
different models for different categories, which makes them unable to perform
well in real-world applications when encountering classes unseen during the
training phase. In this work, we investigate a more challenging task in the
LiDAR point clouds, class-agnostic tracking, where a general model is supposed
to be learned for any specified targets of both observed and unseen categories.
In particular, we first investigate the class-agnostic performances of the
state-of-the-art trackers via exposing the unseen categories to them during
testing, finding that a key factor for class-agnostic tracking is how to
constrain fused features between the template and search region to maintain
generalization when the distribution is shifted from observed to unseen
classes. Therefore, we propose a feature decorrelation method to address this
problem, which eliminates the spurious correlations of the fused features
through a set of learned weights and further makes the search region consistent
among foreground points and distinctive between foreground and background
points. Experiments on the KITTI and NuScenes demonstrate that the proposed
method can achieve considerable improvements by benchmarking against the
advanced trackers P2B and BAT, especially when tracking unseen objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Standardized Pipeline for Colon Nuclei Identification and Counting Challenge. (arXiv:2203.00171v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.00171">
<div class="article-summary-box-inner">
<span><p>Nuclear segmentation and classification is an essential step for
computational pathology. TIA lab from Warwick University organized a nuclear
segmentation and classification challenge (CoNIC) for H&amp;E stained
histopathology images in colorectal cancer with two highly correlated tasks,
nuclei segmentation and classification task and cellular composition task.
There are a few obstacles we have to address in this challenge, 1) limited
training samples, 2) color variation, 3) imbalanced annotations, 4) similar
morphological appearance among classes. To deal with these challenges, we
proposed a standardized pipeline for nuclear segmentation and classification by
integrating several pluggable components. First, we built a GAN-based model to
automatically generate pseudo images for data augmentation. Then we trained a
self-supervised stain normalization model to solve the color variation problem.
Next we constructed a baseline model HoVer-Net with cost-sensitive loss to
encourage the model pay more attention on the minority classes. According to
the results of the leaderboard, our proposed pipeline achieves 0.40665 mPQ+
(Rank 49th) and 0.62199 r2 (Rank 10th) in the preliminary test phase.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DenseUNets with feedback non-local attention for the segmentation of specular microscopy images of the corneal endothelium with guttae. (arXiv:2203.01882v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01882">
<div class="article-summary-box-inner">
<span><p>To estimate the corneal endothelial parameters from specular microscopy
images depicting cornea guttata (Fuchs dystrophy), we propose a new deep
learning methodology that includes a novel attention mechanism named feedback
non-local attention (fNLA). Our approach first infers the cell edges, then
selects the cells that are well detected, and finally applies a postprocessing
method to correct mistakes and provide the binary segmentation from which the
corneal parameters are estimated (cell density [ECD], coefficient of variation
[CV], and hexagonality [HEX]). In this study, we analyzed 1203 images acquired
with a Topcon SP-1P microscope, 500 of which contained guttae. Manual
segmentation was performed in all images. We compared the results of different
networks (UNet, ResUNeXt, DenseUNets, UNet++) and found that DenseUNets with
fNLA provided the best performance, with a mean absolute error of 23.16
[cells/mm$^{2}$] in ECD, 1.28 [%] in CV, and 3.13 [%] in HEX, which was 3-6
times smaller than the error obtained by Topcon's built-in software. Our
approach handled the cells affected by guttae remarkably well, detecting cell
edges occluded by small guttae while discarding areas covered by large guttae.
Overall, the proposed method obtained accurate estimations in extremely
challenging specular images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ACVNet: Attention Concatenation Volume for Accurate and Efficient Stereo Matching. (arXiv:2203.02146v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02146">
<div class="article-summary-box-inner">
<span><p>Stereo matching is a fundamental building block for many vision and robotics
applications. An informative and concise cost volume representation is vital
for stereo matching of high accuracy and efficiency. In this paper, we present
a novel cost volume construction method which generates attention weights from
correlation clues to suppress redundant information and enhance
matching-related information in the concatenation volume. To generate reliable
attention weights, we propose multi-level adaptive patch matching to improve
the distinctiveness of the matching cost at different disparities even for
textureless regions. The proposed cost volume is named attention concatenation
volume (ACV) which can be seamlessly embedded into most stereo matching
networks, the resulting networks can use a more lightweight aggregation network
and meanwhile achieve higher accuracy, e.g. using only 1/25 parameters of the
aggregation network can achieve higher accuracy for GwcNet. Furthermore, we
design a highly accurate network (ACVNet) based on our ACV, which achieves
state-of-the-art performance on several benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BoostMIS: Boosting Medical Image Semi-supervised Learning with Adaptive Pseudo Labeling and Informative Active Annotation. (arXiv:2203.02533v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02533">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel semi-supervised learning (SSL) framework
named BoostMIS that combines adaptive pseudo labeling and informative active
annotation to unleash the potential of medical image SSL models: (1) BoostMIS
can adaptively leverage the cluster assumption and consistency regularization
of the unlabeled data according to the current learning status. This strategy
can adaptively generate one-hot "hard" labels converted from task model
predictions for better task model training. (2) For the unselected unlabeled
images with low confidence, we introduce an Active learning (AL) algorithm to
find the informative samples as the annotation candidates by exploiting virtual
adversarial perturbation and model's density-aware entropy. These informative
candidates are subsequently fed into the next training cycle for better SSL
label propagation. Notably, the adaptive pseudo-labeling and informative active
annotation form a learning closed-loop that are mutually collaborative to boost
medical image SSL. To verify the effectiveness of the proposed method, we
collected a metastatic epidural spinal cord compression (MESCC) dataset that
aims to optimize MESCC diagnosis and classification for improved specialist
referral and treatment. We conducted an extensive experimental study of
BoostMIS on MESCC and another public dataset COVIDx. The experimental results
verify our framework's effectiveness and generalisability for different medical
image datasets with a significant improvement over various state-of-the-art
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UVCGAN: UNet Vision Transformer cycle-consistent GAN for unpaired image-to-image translation. (arXiv:2203.02557v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02557">
<div class="article-summary-box-inner">
<span><p>Image-to-image translation has broad applications in art, design, and
scientific simulations. The original CycleGAN model emphasizes one-to-one
mapping via a cycle-consistent loss, while more recent works promote
one-to-many mapping to boost the diversity of the translated images. With
scientific simulation and one-to-one needs in mind, this work examines if
equipping CycleGAN with a vision transformer (ViT) and employing advanced
generative adversarial network (GAN) training techniques can achieve better
performance. The resulting UNet ViT Cycle-consistent GAN (UVCGAN) model is
compared with previous best-performing models on open benchmark image-to-image
translation datasets, Selfie2Anime and CelebA. UVCGAN performs better and
retains a strong correlation between the original and translated images. An
accompanying ablation study shows that the gradient penalty and BERT-like
pre-training also contribute to the improvement.~To promote reproducibility and
open science, the source code, hyperparameter configurations, and pre-trained
model will be made available at: https://github.com/LS4GAN/uvcgan.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine Learning Applications in Diagnosis, Treatment and Prognosis of Lung Cancer. (arXiv:2203.02794v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02794">
<div class="article-summary-box-inner">
<span><p>The recent development of imaging and sequencing technologies enables
systematic advances in the clinical study of lung cancer. Meanwhile, the human
mind is limited in effectively handling and fully utilizing the accumulation of
such enormous amounts of data. Machine learning-based approaches play a
critical role in integrating and analyzing these large and complex datasets,
which have extensively characterized lung cancer through the use of different
perspectives from these accrued data. In this article, we provide an overview
of machine learning-based approaches that strengthen the varying aspects of
lung cancer diagnosis and therapy, including early detection, auxiliary
diagnosis, prognosis prediction and immunotherapy practice. Moreover, we
highlight the challenges and opportunities for future applications of machine
learning in lung cancer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differentially Private Federated Learning with Local Regularization and Sparsification. (arXiv:2203.03106v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03106">
<div class="article-summary-box-inner">
<span><p>User-level differential privacy (DP) provides certifiable privacy guarantees
to the information that is specific to any user's data in federated learning.
Existing methods that ensure user-level DP come at the cost of severe accuracy
decrease. In this paper, we study the cause of model performance degradation in
federated learning under user-level DP guarantee. We find the key to solving
this issue is to naturally restrict the norm of local updates before executing
operations that guarantee DP. To this end, we propose two techniques, Bounded
Local Update Regularization and Local Update Sparsification, to increase model
quality without sacrificing privacy. We provide theoretical analysis on the
convergence of our framework and give rigorous privacy guarantees. Extensive
experiments show that our framework significantly improves the privacy-utility
trade-off over the state-of-the-arts for federated learning with user-level DP
guarantee.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Rectangling for Image Stitching: A Learning Baseline. (arXiv:2203.03831v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03831">
<div class="article-summary-box-inner">
<span><p>Stitched images provide a wide field-of-view (FoV) but suffer from unpleasant
irregular boundaries. To deal with this problem, existing image rectangling
methods devote to searching an initial mesh and optimizing a target mesh to
form the mesh deformation in two stages. Then rectangular images can be
generated by warping stitched images. However, these solutions only work for
images with rich linear structures, leading to noticeable distortions for
portraits and landscapes with non-linear objects. In this paper, we address
these issues by proposing the first deep learning solution to image
rectangling. Concretely, we predefine a rigid target mesh and only estimate an
initial mesh to form the mesh deformation, contributing to a compact one-stage
solution. The initial mesh is predicted using a fully convolutional network
with a residual progressive regression strategy. To obtain results with high
content fidelity, a comprehensive objective function is proposed to
simultaneously encourage the boundary rectangular, mesh shape-preserving, and
content perceptually natural. Besides, we build the first image stitching
rectangling dataset with a large diversity in irregular boundaries and scenes.
Experiments demonstrate our superiority over traditional methods both
quantitatively and qualitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Motron: Multimodal Probabilistic Human Motion Forecasting. (arXiv:2203.04132v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04132">
<div class="article-summary-box-inner">
<span><p>Autonomous systems and humans are increasingly sharing the same space. Robots
work side by side or even hand in hand with humans to balance each other's
limitations. Such cooperative interactions are ever more sophisticated. Thus,
the ability to reason not just about a human's center of gravity position, but
also its granular motion is an important prerequisite for human-robot
interaction. Though, many algorithms ignore the multimodal nature of humans or
neglect uncertainty in their motion forecasts. We present Motron, a multimodal,
probabilistic, graph-structured model, that captures human's multimodality
using probabilistic methods while being able to output deterministic
maximum-likelihood motions and corresponding confidence values for each mode.
Our model aims to be tightly integrated with the robotic
planning-control-interaction loop; outputting physically feasible human motions
and being computationally efficient. We demonstrate the performance of our
model on several challenging real-world motion forecasting datasets,
outperforming a wide array of generative/variational methods while providing
state-of-the-art single-output motions if required. Both using significantly
less computational power than state-of-the art algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiscale Convolutional Transformer with Center Mask Pretraining for Hyperspectral Image Classification. (arXiv:2203.04771v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04771">
<div class="article-summary-box-inner">
<span><p>Hyperspectral images (HSI) not only have a broad macroscopic field of view
but also contain rich spectral information, and the types of surface objects
can be identified through spectral information, which is one of the main
applications in hyperspectral image related research.In recent years, more and
more deep learning methods have been proposed, among which convolutional neural
networks (CNN) are the most influential. However, CNN-based methods are
difficult to capture long-range dependencies, and also require a large amount
of labeled data for model training.Besides, most of the self-supervised
training methods in the field of HSI classification are based on the
reconstruction of input samples, and it is difficult to achieve effective use
of unlabeled samples. To address the shortcomings of CNN networks, we propose a
noval multi-scale convolutional embedding module for HSI to realize effective
extraction of spatial-spectral information, which can be better combined with
Transformer network.In order to make more efficient use of unlabeled data, we
propose a new self-supervised pretask. Similar to Mask autoencoder, but our
pre-training method only masks the corresponding token of the central pixel in
the encoder, and inputs the remaining token into the decoder to reconstruct the
spectral information of the central pixel.Such a pretask can better model the
relationship between the central feature and the domain feature, and obtain
more stable training results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-modal Map Learning for Vision and Language Navigation. (arXiv:2203.05137v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05137">
<div class="article-summary-box-inner">
<span><p>We consider the problem of Vision-and-Language Navigation (VLN). The majority
of current methods for VLN are trained end-to-end using either unstructured
memory such as LSTM, or using cross-modal attention over the egocentric
observations of the agent. In contrast to other works, our key insight is that
the association between language and vision is stronger when it occurs in
explicit spatial representations. In this work, we propose a cross-modal map
learning model for vision-and-language navigation that first learns to predict
the top-down semantics on an egocentric map for both observed and unobserved
regions, and then predicts a path towards the goal as a set of waypoints. In
both cases, the prediction is informed by the language through cross-modal
attention mechanisms. We experimentally test the basic hypothesis that
language-driven navigation can be solved given a map, and then show competitive
results on the full VLN-CE benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Frequency-driven Imperceptible Adversarial Attack on Semantic Similarity. (arXiv:2203.05151v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05151">
<div class="article-summary-box-inner">
<span><p>Current adversarial attack research reveals the vulnerability of
learning-based classifiers against carefully crafted perturbations. However,
most existing attack methods have inherent limitations in cross-dataset
generalization as they rely on a classification layer with a closed set of
categories. Furthermore, the perturbations generated by these methods may
appear in regions easily perceptible to the human visual system (HVS). To
circumvent the former problem, we propose a novel algorithm that attacks
semantic similarity on feature representations. In this way, we are able to
fool classifiers without limiting attacks to a specific dataset. For
imperceptibility, we introduce the low-frequency constraint to limit
perturbations within high-frequency components, ensuring perceptual similarity
between adversarial examples and originals. Extensive experiments on three
datasets (CIFAR-10, CIFAR-100, and ImageNet-1K) and three public online
platforms indicate that our attack can yield misleading and transferable
adversarial examples across architectures and datasets. Additionally,
visualization results and quantitative performance (in terms of four different
metrics) show that the proposed algorithm generates more imperceptible
perturbations than the state-of-the-art methods. Code is made available at.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PillarGrid: Deep Learning-based Cooperative Perception for 3D Object Detection from Onboard-Roadside LiDAR. (arXiv:2203.06319v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06319">
<div class="article-summary-box-inner">
<span><p>3D object detection plays a fundamental role in enabling autonomous driving,
which is regarded as the significant key to unlocking the bottleneck of
contemporary transportation systems from the perspectives of safety, mobility,
and sustainability. Most of the state-of-the-art (SOTA) object detection
methods from point clouds are developed based on a single onboard LiDAR, whose
performance will be inevitably limited by the range and occlusion, especially
in dense traffic scenarios. In this paper, we propose \textit{PillarGrid}, a
novel cooperative perception method fusing information from multiple 3D LiDARs
(both on-board and roadside), to enhance the situation awareness for connected
and automated vehicles (CAVs). PillarGrid consists of four main phases: 1)
cooperative preprocessing of point clouds, 2) pillar-wise voxelization and
feature extraction, 3) grid-wise deep fusion of features from multiple sensors,
and 4) convolutional neural network (CNN)-based augmented 3D object detection.
A novel cooperative perception platform is developed for model training and
testing. Extensive experimentation shows that PillarGrid outperforms the SOTA
single-LiDAR-based 3D object detection methods with respect to both accuracy
and range by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Visual-Prompt Temporal Answering Grounding in Medical Instructional Video. (arXiv:2203.06667v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06667">
<div class="article-summary-box-inner">
<span><p>The temporal answering grounding in the video (TAGV) is a new task naturally
deriving from temporal sentence grounding in the video (TSGV). Given an
untrimmed video and a text question, this task aims at locating the matching
span from the video that can semantically answer the question. Existing methods
tend to formulate the TAGV task with a visual span-based question answering
(QA) approach by matching the visual frame span queried by the text question.
However, due to the weak correlations and huge gaps in semantics in features
between the textual question and visual answer, existing methods adopting
visual span predictor fail to perform well in the TAGV task. In this work, we
propose a visual-prompt text span localizing (VPTSL) method, which enhances the
text span localization in the pre-trained language model (PLM) with the visual
highlight features. Specifically, the context query attention is utilized to
perform cross-modal modeling between the textual and visual features. Then, the
highlight features are obtained through the highlight module with a linear
layer to provide the visual prompt. To alleviate the differences in semantics
and correlations between textual and visual features, we design the text span
predictor by encoding the question, the subtitles, and the visual prompt in the
PLM. As a result, the TAGV task is formulated to predict the span of subtitles
matching the answering frame timeline. Extensive experiments on the medical
instructional dataset, namely MedVidQA, show that the proposed VPTSL
outperforms other state-of-the-art (SOTA) methods by 28.36 in mIOU score with a
large margin, which demonstrates the effectiveness of visual prompt and the
text span predictor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attention based Memory video portrait matting. (arXiv:2203.06890v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06890">
<div class="article-summary-box-inner">
<span><p>We proposed a novel trimap free video matting method based on the attention
mechanism. By the nature of the problem, most existing approaches use either
multiple computational expansive modules or complex algorithms to exploit
temporal information fully. We designed a temporal aggregation module to
compute the temporal coherence between the current frame and its two previous
frames.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Memory Learning for Fine-Grained Scene Graph Generation. (arXiv:2203.06907v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06907">
<div class="article-summary-box-inner">
<span><p>As far as Scene Graph Generation (SGG), coarse and fine predicates mix in the
dataset due to the crowd-sourced labeling, and the long-tail problem is also
pronounced. Given this tricky situation, many existing SGG methods treat the
predicates equally and learn the model under the supervision of
mixed-granularity predicates in one stage, leading to relatively coarse
predictions. In order to alleviate the negative impact of the suboptimum
mixed-granularity annotation and long-tail effect problems, this paper proposes
a novel Hierarchical Memory Learning (HML) framework to learn the model from
simple to complex, which is similar to the human beings' hierarchical memory
learning process. After the autonomous partition of coarse and fine predicates,
the model is first trained on the coarse predicates and then learns the fine
predicates. In order to realize this hierarchical learning pattern, this paper,
for the first time, formulates the HML framework using the new Concept
Reconstruction (CR) and Model Reconstruction (MR) constraints. It is worth
noticing that the HML framework can be taken as one general optimization
strategy to improve various SGG models, and significant improvement can be
achieved on the SGG benchmark (i.e., Visual Genome).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Progressive End-to-End Object Detection in Crowded Scenes. (arXiv:2203.07669v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07669">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a new query-based detection framework for crowd
detection. Previous query-based detectors suffer from two drawbacks: first,
multiple predictions will be inferred for a single object, typically in crowded
scenes; second, the performance saturates as the depth of the decoding stage
increases. Benefiting from the nature of the one-to-one label assignment rule,
we propose a progressive predicting method to address the above issues.
Specifically, we first select accepted queries prone to generate true positive
predictions, then refine the rest noisy queries according to the previously
accepted predictions. Experiments show that our method can significantly boost
the performance of query-based detectors in crowded scenes. Equipped with our
approach, Sparse RCNN achieves 92.0\% $\text{AP}$, 41.4\% $\text{MR}^{-2}$ and
83.2\% $\text{JI}$ on the challenging CrowdHuman \cite{shao2018crowdhuman}
dataset, outperforming the box-based method MIP \cite{chu2020detection} that
specifies in handling crowded scenarios. Moreover, the proposed method, robust
to crowdedness, can still obtain consistent improvements on moderately and
slightly crowded datasets like CityPersons \cite{zhang2017citypersons} and COCO
\cite{lin2014microsoft}. Code will be made publicly available at
https://github.com/megvii-model/Iter-E2EDET.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revitalize Region Feature for Democratizing Video-Language Pre-training. (arXiv:2203.07720v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07720">
<div class="article-summary-box-inner">
<span><p>Recent dominant methods for video-language pre-training (VLP) learn
transferable representations from the raw pixels in an end-to-end manner to
achieve advanced performance on downstream video-language tasks. Despite the
impressive results, VLP research becomes extremely expensive with the need for
massive data and a long training time, preventing further explorations. In this
work, we revitalize region features of sparsely sampled video clips to
significantly reduce both spatial and temporal visual redundancy towards
democratizing VLP research at the same time achieving state-of-the-art results.
Specifically, to fully explore the potential of region features, we introduce a
novel bidirectional region-word alignment regularization that properly
optimizes the fine-grained relations between regions and certain words in
sentences, eliminating the domain/modality disconnections between pre-extracted
region features and text. Extensive results of downstream text-to-video
retrieval and video question answering tasks on seven datasets demonstrate the
superiority of our method on both effectiveness and efficiency, e.g., our
method achieves competing results with 80\% fewer data and 85\% less
pre-training time compared to the most efficient VLP method so far. The code
will be available at \url{https://github.com/showlab/DemoVLP}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast Autofocusing using Tiny Networks for Digital Holographic Microscopy. (arXiv:2203.07772v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07772">
<div class="article-summary-box-inner">
<span><p>The numerical wavefront backpropagation principle of digital holography
confers unique extended focus capabilities, without mechanical displacements
along z-axis. However, the determination of the correct focusing distance is a
non-trivial and time consuming issue. A deep learning (DL) solution is proposed
to cast the autofocusing as a regression problem and tested over both
experimental and simulated holograms. Single wavelength digital holograms were
recorded by a Digital Holographic Microscope (DHM) with a 10$\mathrm{x}$
microscope objective from a patterned target moving in 3D over an axial range
of 92 $\mu$m. Tiny DL models are proposed and compared such as a tiny Vision
Transformer (TViT), tiny VGG16 (TVGG) and a tiny Swin-Transfomer (TSwinT). The
experiments show that the predicted focusing distance $Z_R^{\mathrm{Pred}}$ is
accurately inferred with an accuracy of 1.2 $\mu$m in average in comparison
with the DHM depth of field of 15 $\mu$m. Numerical simulations show that all
tiny models give the $Z_R^{\mathrm{Pred}}$ with an error below 0.3 $\mu$m. Such
a prospect would significantly improve the current capabilities of computer
vision position sensing in applications such as 3D microscopy for life sciences
or micro-robotics. Moreover, all models reach state of the art inference time
on CPU, less than 25 ms per inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scalable Penalized Regression for Noise Detection in Learning with Noisy Labels. (arXiv:2203.07788v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07788">
<div class="article-summary-box-inner">
<span><p>Noisy training set usually leads to the degradation of generalization and
robustness of neural networks. In this paper, we propose using a theoretically
guaranteed noisy label detection framework to detect and remove noisy data for
Learning with Noisy Labels (LNL). Specifically, we design a penalized
regression to model the linear relation between network features and one-hot
labels, where the noisy data are identified by the non-zero mean shift
parameters solved in the regression model. To make the framework scalable to
datasets that contain a large number of categories and training data, we
propose a split algorithm to divide the whole training set into small pieces
that can be solved by the penalized regression in parallel, leading to the
Scalable Penalized Regression (SPR) framework. We provide the non-asymptotic
probabilistic condition for SPR to correctly identify the noisy data. While SPR
can be regarded as a sample selection module for standard supervised training
pipeline, we further combine it with semi-supervised algorithm to further
exploit the support of noisy data as unlabeled data. Experimental results on
several benchmark datasets and real-world noisy datasets show the effectiveness
of our framework. Our code and pretrained models are released at
https://github.com/Yikai-Wang/SPR-LNL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robustness through Cognitive Dissociation Mitigation in Contrastive Adversarial Training. (arXiv:2203.08959v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08959">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce a novel neural network training framework that
increases model's adversarial robustness to adversarial attacks while
maintaining high clean accuracy by combining contrastive learning (CL) with
adversarial training (AT). We propose to improve model robustness to
adversarial attacks by learning feature representations that are consistent
under both data augmentations and adversarial perturbations. We leverage
contrastive learning to improve adversarial robustness by considering an
adversarial example as another positive example, and aim to maximize the
similarity between random augmentations of data samples and their adversarial
example, while constantly updating the classification head in order to avoid a
cognitive dissociation between the classification head and the embedding space.
This dissociation is caused by the fact that CL updates the network up to the
embedding space, while freezing the classification head which is used to
generate new positive adversarial examples. We validate our method, Contrastive
Learning with Adversarial Features(CLAF), on the CIFAR-10 dataset on which it
outperforms both robust accuracy and clean accuracy over alternative supervised
and self-supervised adversarial learning methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic-aligned Fusion Transformer for One-shot Object Detection. (arXiv:2203.09093v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09093">
<div class="article-summary-box-inner">
<span><p>One-shot object detection aims at detecting novel objects according to merely
one given instance. With extreme data scarcity, current approaches explore
various feature fusions to obtain directly transferable meta-knowledge. Yet,
their performances are often unsatisfactory. In this paper, we attribute this
to inappropriate correlation methods that misalign query-support semantics by
overlooking spatial structures and scale variances. Upon analysis, we leverage
the attention mechanism and propose a simple but effective architecture named
Semantic-aligned Fusion Transformer (SaFT) to resolve these issues.
Specifically, we equip SaFT with a vertical fusion module (VFM) for cross-scale
semantic enhancement and a horizontal fusion module (HFM) for cross-sample
feature fusion. Together, they broaden the vision for each feature point from
the support to a whole augmented feature pyramid from the query, facilitating
semantic-aligned associations. Extensive experiments on multiple benchmarks
demonstrate the superiority of our framework. Without fine-tuning on novel
classes, it brings significant performance gains to one-stage baselines,
lifting state-of-the-art results to a higher level.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TO-Scene: A Large-scale Dataset for Understanding 3D Tabletop Scenes. (arXiv:2203.09440v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09440">
<div class="article-summary-box-inner">
<span><p>Many basic indoor activities such as eating or writing are always conducted
upon different tabletops (e.g., coffee tables, writing desks). It is
indispensable to understanding tabletop scenes in 3D indoor scene parsing
applications. Unfortunately, it is hard to meet this demand by directly
deploying data-driven algorithms, since 3D tabletop scenes are rarely available
in current datasets. To remedy this defect, we introduce TO-Scene, a
large-scale dataset focusing on tabletop scenes, which contains 20,740 scenes
with three variants. To acquire the data, we design an efficient and scalable
framework, where a crowdsourcing UI is developed to transfer CAD objects onto
tables from ScanNet, then the output tabletop scenes are simulated into real
scans and annotated automatically.
</p>
<p>Further, a tabletop-aware learning strategy is proposed for better perceiving
the small-sized tabletop instances. Notably, we also provide a real scanned
test set TO-Real to verify the practical value of TO-Scene. Experiments show
that the algorithms trained on TO-Scene indeed work on the realistic test data,
and our proposed tabletop-aware learning strategy greatly improves the
state-of-the-art results on both 3D semantic segmentation and object detection
tasks. TO-Scene and TO-Real, plus Web UI, will all be publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FERV39k: A Large-Scale Multi-Scene Dataset for Facial Expression Recognition in Videos. (arXiv:2203.09463v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09463">
<div class="article-summary-box-inner">
<span><p>Current benchmarks for facial expression recognition (FER) mainly focus on
static images, while there are limited datasets for FER in videos. It is still
ambiguous to evaluate whether performances of existing methods remain
satisfactory in real-world application-oriented scenes. For example, the
"Happy" expression with high intensity in Talk-Show is more discriminating than
the same expression with low intensity in Official-Event. To fill this gap, we
build a large-scale multi-scene dataset, coined as FERV39k. We analyze the
important ingredients of constructing such a novel dataset in three aspects:
(1) multi-scene hierarchy and expression class, (2) generation of candidate
video clips, (3) trusted manual labelling process. Based on these guidelines,
we select 4 scenarios subdivided into 22 scenes, annotate 86k samples
automatically obtained from 4k videos based on the well-designed workflow, and
finally build 38,935 video clips labeled with 7 classic expressions. Experiment
benchmarks on four kinds of baseline frameworks were also provided and further
analysis on their performance across different scenes and some challenges for
future research were given. Besides, we systematically investigate key
components of DFER by ablation studies. The baseline framework and our project
will be available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Data-Efficient Detection Transformers. (arXiv:2203.09507v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09507">
<div class="article-summary-box-inner">
<span><p>Detection Transformers have achieved competitive performance on the
sample-rich COCO dataset. However, we show most of them suffer from significant
performance drops on small-size datasets, like Cityscapes. In other words, the
detection transformers are generally data-hungry. To tackle this problem, we
empirically analyze the factors that affect data efficiency, through a
step-by-step transition from a data-efficient RCNN variant to the
representative DETR. The empirical results suggest that sparse feature sampling
from local image areas holds the key. Based on this observation, we alleviate
the data-hungry issue of existing detection transformers by simply alternating
how key and value sequences are constructed in the cross-attention layer, with
minimum modifications to the original models. Besides, we introduce a simple
yet effective label augmentation method to provide richer supervision and
improve data efficiency. Experiments show that our method can be readily
applied to different detection transformers and improve their performance on
both small-size and sample-rich datasets. Code will be made publicly available
at \url{https://github.com/encounter1997/DE-DETRs}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HSC4D: Human-centered 4D Scene Capture in Large-scale Indoor-outdoor Space Using Wearable IMUs and LiDAR. (arXiv:2203.09215v1 [cs.CV] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09215">
<div class="article-summary-box-inner">
<span><p>We propose Human-centered 4D Scene Capture (HSC4D) to accurately and
efficiently create a dynamic digital world, containing large-scale
indoor-outdoor scenes, diverse human motions, and rich interactions between
humans and environments. Using only body-mounted IMUs and LiDAR, HSC4D is
space-free without any external devices' constraints and map-free without
pre-built maps. Considering that IMUs can capture human poses but always drift
for long-period use, while LiDAR is stable for global localization but rough
for local positions and orientations, HSC4D makes both sensors complement each
other by a joint optimization and achieves promising results for long-term
capture. Relationships between humans and environments are also explored to
make their interaction more realistic. To facilitate many down-stream tasks,
like AR, VR, robots, autonomous driving, etc., we propose a dataset containing
three large scenes (1k-5k $m^2$) with accurate dynamic human motions and
locations. Diverse scenarios (climbing gym, multi-story building, slope, etc.)
and challenging human activities (exercising, walking up/down stairs, climbing,
etc.) demonstrate the effectiveness and the generalization ability of HSC4D.
The dataset and code is available at https://github.com/climbingdaily/HSC4D.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-03-22 23:08:16.336140532 UTC">2022-03-22 23:08:16 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>