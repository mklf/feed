<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-01-05T01:30:00Z">01-05</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">AI & Racial Equity: Understanding Sentiment Analysis Artificial Intelligence, Data Security, and Systemic Theory in Criminal Justice Systems. (arXiv:2201.00855v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00855">
<div class="article-summary-box-inner">
<span><p>Various forms of implications of artificial intelligence that either
exacerbate or decrease racial systemic injustice have been explored in this
applied research endeavor. Taking each thematic area of identifying, analyzing,
and debating an systemic issue have been leveraged in investigating merits and
drawbacks of using algorithms to automate human decision making in racially
sensitive environments. It has been asserted through the analysis of historical
systemic patterns, implicit biases, existing algorithmic risks, and legal
implications that natural language processing based AI, such as risk assessment
tools, have racially disparate outcomes. It is concluded that more litigative
policies are needed to regulate and restrict how internal government
institutions and corporations utilize algorithms, privacy and security risks,
and auditing requirements in order to diverge from racially injustice outcomes
and practices of the past.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Adversarial Benchmark for Fake News Detection Models. (arXiv:2201.00912v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00912">
<div class="article-summary-box-inner">
<span><p>With the proliferation of online misinformation, fake news detection has
gained importance in the artificial intelligence community. In this paper, we
propose an adversarial benchmark that tests the ability of fake news detectors
to reason about real-world facts. We formulate adversarial attacks that target
three aspects of "understanding": compositional semantics, lexical relations,
and sensitivity to modifiers. We test our benchmark using BERT classifiers
fine-tuned on the LIAR <a href="/abs/arch-ive/1705648">arXiv:arch-ive/1705648</a> and Kaggle Fake-News datasets,
and show that both models fail to respond to changes in compositional and
lexical meaning. Our results strengthen the need for such models to be used in
conjunction with other fact checking methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantics-Preserved Distortion for Personal Privacy Protection. (arXiv:2201.00965v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00965">
<div class="article-summary-box-inner">
<span><p>Privacy protection is an important and concerning topic in Federated
Learning, especially for Natural Language Processing. In client devices, a
large number of texts containing personal information are produced by users
every day. As the direct application of information from users is likely to
invade personal privacy, many methods have been proposed in Federated Learning
to block the center model from the raw information in client devices. In this
paper, we try to do this more linguistically via distorting the text while
preserving the semantics. In practice, we leverage a recently proposed metric,
Neighboring Distribution Divergence, to evaluate the semantic preservation
during the distortion. Based on the metric, we propose two frameworks for
semantics-preserved distortion, a generative one and a substitutive one. Due to
the lack of privacy-related tasks in the current Natural Language Processing
field, we conduct experiments on named entity recognition and constituency
parsing. Results from our experiments show the plausibility and efficiency of
our distortion as a method for personal privacy protection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interactive Attention AI to translate low light photos to captions for night scene understanding in women safety. (arXiv:2201.00969v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00969">
<div class="article-summary-box-inner">
<span><p>There is amazing progress in Deep Learning based models for Image captioning
and Low Light image enhancement. For the first time in literature, this paper
develops a Deep Learning model that translates night scenes to sentences,
opening new possibilities for AI applications in the safety of visually
impaired women. Inspired by Image Captioning and Visual Question Answering, a
novel Interactive Image Captioning is developed. A user can make the AI focus
on any chosen person of interest by influencing the attention scoring.
Attention context vectors are computed from CNN feature vectors and
user-provided start word. The Encoder-Attention-Decoder neural network learns
to produce captions from low brightness images. This paper demonstrates how
women safety can be enabled by researching a novel AI capability in the
Interactive Vision-Language model for perception of the environment in the
night.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Submix: Practical Private Prediction for Large-Scale Language Models. (arXiv:2201.00971v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00971">
<div class="article-summary-box-inner">
<span><p>Recent data-extraction attacks have exposed that language models can memorize
some training samples verbatim. This is a vulnerability that can compromise the
privacy of the model's training data. In this work, we introduce SubMix: a
practical protocol for private next-token prediction designed to prevent
privacy violations by language models that were fine-tuned on a private corpus
after pre-training on a public corpus. We show that SubMix limits the leakage
of information that is unique to any individual user in the private corpus via
a relaxation of group differentially private prediction. Importantly, SubMix
admits a tight, data-dependent privacy accounting mechanism, which allows it to
thwart existing data-extraction attacks while maintaining the utility of the
language model. SubMix is the first protocol that maintains privacy even when
publicly releasing tens of thousands of next-token predictions made by large
transformer-based models such as GPT-2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StyleM: Stylized Metrics for Image Captioning Built with Contrastive N-grams. (arXiv:2201.00975v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00975">
<div class="article-summary-box-inner">
<span><p>In this paper, we build two automatic evaluation metrics for evaluating the
association between a machine-generated caption and a ground truth stylized
caption: OnlyStyle and StyleCIDEr.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Variational Stacked Local Attention Networks for Diverse Video Captioning. (arXiv:2201.00985v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00985">
<div class="article-summary-box-inner">
<span><p>While describing Spatio-temporal events in natural language, video captioning
models mostly rely on the encoder's latent visual representation. Recent
progress on the encoder-decoder model attends encoder features mainly in linear
interaction with the decoder. However, growing model complexity for visual data
encourages more explicit feature interaction for fine-grained information,
which is currently absent in the video captioning domain. Moreover, feature
aggregations methods have been used to unveil richer visual representation,
either by the concatenation or using a linear layer. Though feature sets for a
video semantically overlap to some extent, these approaches result in objective
mismatch and feature redundancy. In addition, diversity in captions is a
fundamental component of expressing one event from several meaningful
perspectives, currently missing in the temporal, i.e., video captioning domain.
To this end, we propose Variational Stacked Local Attention Network (VSLAN),
which exploits low-rank bilinear pooling for self-attentive feature interaction
and stacking multiple video feature streams in a discount fashion. Each feature
stack's learned attributes contribute to our proposed diversity encoding
module, followed by the decoding query stage to facilitate end-to-end diverse
and natural captions without any explicit supervision on attributes. We
evaluate VSLAN on MSVD and MSR-VTT datasets in terms of syntax and diversity.
The CIDEr score of VSLAN outperforms current off-the-shelf methods by $7.8\%$
on MSVD and $4.5\%$ on MSR-VTT, respectively. On the same datasets, VSLAN
achieves competitive results in caption diversity metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MDFEND: Multi-domain Fake News Detection. (arXiv:2201.00987v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00987">
<div class="article-summary-box-inner">
<span><p>Fake news spread widely on social media in various domains, which lead to
real-world threats in many aspects like politics, disasters, and finance. Most
existing approaches focus on single-domain fake news detection (SFND), which
leads to unsatisfying performance when these methods are applied to
multi-domain fake news detection. As an emerging field, multi-domain fake news
detection (MFND) is increasingly attracting attention. However, data
distributions, such as word frequency and propagation patterns, vary from
domain to domain, namely domain shift. Facing the challenge of serious domain
shift, existing fake news detection techniques perform poorly for multi-domain
scenarios. Therefore, it is demanding to design a specialized model for MFND.
In this paper, we first design a benchmark of fake news dataset for MFND with
domain label annotated, namely Weibo21, which consists of 4,488 fake news and
4,640 real news from 9 different domains. We further propose an effective
Multi-domain Fake News Detection Model (MDFEND) by utilizing a domain gate to
aggregate multiple representations extracted by a mixture of experts. The
experiments show that MDFEND can significantly improve the performance of
multi-domain fake news detection. Our dataset and code are available at
https://github.com/kennqiang/MDFEND-Weibo21.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DigNet: Digging Clues from Local-Global Interactive Graph for Aspect-level Sentiment Classification. (arXiv:2201.00989v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00989">
<div class="article-summary-box-inner">
<span><p>In aspect-level sentiment classification (ASC), state-of-the-art models
encode either syntax graph or relation graph to capture the local syntactic
information or global relational information. Despite the advantages of syntax
and relation graphs, they have respective shortages which are neglected,
limiting the representation power in the graph modeling process. To resolve
their limitations, we design a novel local-global interactive graph, which
marries their advantages by stitching the two graphs via interactive edges. To
model this local-global interactive graph, we propose a novel neural network
termed DigNet, whose core module is the stacked local-global interactive (LGI)
layers performing two processes: intra-graph message passing and cross-graph
message passing. In this way, the local syntactic and global relational
information can be reconciled as a whole in understanding the aspect-level
sentiment. Concretely, we design two variants of local-global interactive
graphs with different kinds of interactive edges and three variants of LGI
layers. We conduct experiments on several public benchmark datasets and the
results show that we outperform previous best scores by 3\%, 2.32\%, and 6.33\%
in terms of Macro-F1 on Lap14, Res14, and Res15 datasets, respectively,
confirming the effectiveness and superiority of the proposed local-global
interactive graph and DigNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting Influenza A Viral Host Using PSSM and Word Embeddings. (arXiv:2201.01140v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01140">
<div class="article-summary-box-inner">
<span><p>The rapid mutation of the influenza virus threatens public health.
Reassortment among viruses with different hosts can lead to a fatal pandemic.
However, it is difficult to detect the original host of the virus during or
after an outbreak as influenza viruses can circulate between different species.
Therefore, early and rapid detection of the viral host would help reduce the
further spread of the virus. We use various machine learning models with
features derived from the position-specific scoring matrix (PSSM) and features
learned from word embedding and word encoding to infer the origin host of
viruses. The results show that the performance of the PSSM-based model reaches
the MCC around 95%, and the F1 around 96%. The MCC obtained using the model
with word embedding is around 96%, and the F1 is around 97%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speech-to-SQL: Towards Speech-driven SQL Query Generation From Natural Language Question. (arXiv:2201.01209v1 [cs.DB])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01209">
<div class="article-summary-box-inner">
<span><p>Speech-based inputs have been gaining significant momentum with the
popularity of smartphones and tablets in our daily lives, since voice is the
most easiest and efficient way for human-computer interaction. This paper works
towards designing more effective speech-based interfaces to query the
structured data in relational databases. We first identify a new task named
Speech-to-SQL, which aims to understand the information conveyed by human
speech and directly translate it into structured query language (SQL)
statements. A naive solution to this problem can work in a cascaded manner,
that is, an automatic speech recognition (ASR) component followed by a
text-to-SQL component. However, it requires a high-quality ASR system and also
suffers from the error compounding problem between the two components,
resulting in limited performance. To handle these challenges, we further
propose a novel end-to-end neural architecture named SpeechSQLNet to directly
translate human speech into SQL queries without an external ASR step.
SpeechSQLNet has the advantage of making full use of the rich linguistic
information presented in speech. To the best of our knowledge, this is the
first attempt to directly synthesize SQL based on arbitrary natural language
questions, rather than a natural language-based version of SQL or its variants
with a limited SQL grammar. To validate the effectiveness of the proposed
problem and model, we further construct a dataset named SpeechQL, by
piggybacking the widely-used text-to-SQL datasets. Extensive experimental
evaluations on this dataset show that SpeechSQLNet can directly synthesize
high-quality SQL queries from human speech, outperforming various competitive
counterparts as well as the cascaded methods in terms of exact match
accuracies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Stage Episodic Control for Strategic Exploration in Text Games. (arXiv:2201.01251v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01251">
<div class="article-summary-box-inner">
<span><p>Text adventure games present unique challenges to reinforcement learning
methods due to their combinatorially large action spaces and sparse rewards.
The interplay of these two factors is particularly demanding because large
action spaces require extensive exploration, while sparse rewards provide
limited feedback. This work proposes to tackle the explore-vs-exploit dilemma
using a multi-stage approach that explicitly disentangles these two strategies
within each episode. Our algorithm, called eXploit-Then-eXplore (XTX), begins
each episode using an exploitation policy that imitates a set of promising
trajectories from the past, and then switches over to an exploration policy
aimed at discovering novel actions that lead to unseen state spaces. This
policy decomposition allows us to combine global decisions about which parts of
the game space to return to with curiosity-based local exploration in that
space, motivated by how a human may approach these games. Our method
significantly outperforms prior approaches by 27% and 11% average normalized
score over 12 games from the Jericho benchmark (Hausknecht et al., 2020) in
both deterministic and stochastic settings, respectively. On the game of Zork1,
in particular, XTX obtains a score of 103, more than a 2x improvement over
prior methods, and pushes past several known bottlenecks in the game that have
plagued previous state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topic Model Supervised by Understanding Map. (arXiv:2110.06043v6 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06043">
<div class="article-summary-box-inner">
<span><p>Inspired by the notion of Center of Mass in physics, an extension called
Semantic Center of Mass (SCOM) is proposed, and used to discover the abstract
"topic" of a document. The notion is under a framework model called
Understanding Map Supervised Topic Model (UM-S-TM). The devise aim of UM-S-TM
is to let both the document content and a semantic network -- specifically,
Understanding Map -- play a role, in interpreting the meaning of a document.
Based on different justifications, three possible methods are devised to
discover the SCOM of a document. Some experiments on artificial documents and
Understanding Maps are conducted to test their outcomes. In addition, its
ability of vectorization of documents and capturing sequential information are
tested. We also compared UM-S-TM with probabilistic topic models like Latent
Dirichlet Allocation (LDA) and probabilistic Latent Semantic Analysis (pLSA).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Enactivist account of "Mind Reading" in Natural Language Understanding. (arXiv:2111.06179v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.06179">
<div class="article-summary-box-inner">
<span><p>In this paper we apply our understanding of the radical enactivist agenda to
the classic AI-hard problem of Natural Language Understanding. When Turing
devised his famous test the assumption was that a computer could use language
and the challenge would be to mimic human intelligence. It turned out playing
chess and formal logic were easy compared to understanding what people say. The
techniques of good old-fashioned AI (GOFAI) assume symbolic representation is
the core of reasoning and by that paradigm human communication consists of
transferring representations from one mind to another. However, one finds that
representations appear in another's mind, without appearing in the intermediary
language. People communicate by mind reading it seems. Systems with speech
interfaces such as Alexa and Siri are of course common, but they are limited.
Rather than adding mind reading skills, we introduced a "cheat" that enabled
our systems to fake it. The cheat is simple and only slightly interesting to
computer scientists and not at all interesting to philosophers. However,
reading about the enactivist idea that we "directly perceive" the intentions of
others, our cheat took on a new light and in this paper look again at how
natural language understanding might actually work between humans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Building a great multi-lingual teacher with sparsely-gated mixture of experts for speech recognition. (arXiv:2112.05820v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.05820">
<div class="article-summary-box-inner">
<span><p>The sparsely-gated Mixture of Experts (MoE) can magnify a network capacity
with a little computational complexity. In this work, we investigate how
multi-lingual Automatic Speech Recognition (ASR) networks can be scaled up with
a simple routing algorithm in order to achieve better accuracy. More
specifically, we apply the sparsely-gated MoE technique to two types of
networks: Sequence-to-Sequence Transformer (S2S-T) and Transformer Transducer
(T-T). We demonstrate through a set of ASR experiments on multiple language
data that the MoE networks can reduce the relative word error rates by 16.3%
and 4.6% with the S2S-T and T-T, respectively. Moreover, we thoroughly
investigate the effect of the MoE on the T-T architecture in various
conditions: streaming mode, non-streaming mode, the use of language ID and the
label decoder with the MoE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Graph-Aware Reinforcement Learning to Identify Winning Strategies in Diplomacy Games (Student Abstract). (arXiv:2112.15331v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15331">
<div class="article-summary-box-inner">
<span><p>This abstract proposes an approach towards goal-oriented modeling of the
detection and modeling complex social phenomena in multiparty discourse in an
online political strategy game. We developed a two-tier approach that first
encodes sociolinguistic behavior as linguistic features then use reinforcement
learning to estimate the advantage afforded to any player. In the first tier,
sociolinguistic behavior, such as Friendship and Reasoning, that speakers use
to influence others are encoded as linguistic features to identify the
persuasive strategies applied by each player in simultaneous two-party
dialogues. In the second tier, a reinforcement learning approach is used to
estimate a graph-aware reward function to quantify the advantage afforded to
each player based on their standing in this multiparty setup. We apply this
technique to the game Diplomacy, using a dataset comprising of over 15,000
messages exchanged between 78 users. Our graph-aware approach shows robust
performance compared to a context-agnostic setup.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topical Classification of Food Safety Publications with a Knowledge Base. (arXiv:2201.00374v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00374">
<div class="article-summary-box-inner">
<span><p>The vast body of scientific publications presents an increasing challenge of
finding those that are relevant to a given research question, and making
informed decisions on their basis. This becomes extremely difficult without the
use of automated tools. Here, one possible area for improvement is automatic
classification of publication abstracts according to their topic. This work
introduces a novel, knowledge base-oriented publication classifier. The
proposed method focuses on achieving scalability and easy adaptability to other
domains. Classification speed and accuracy are shown to be satisfactory, in the
very demanding field of food safety. Further development and evaluation of the
method is needed, as the proposed approach shows much potential.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Low dosage 3D volume fluorescence microscopy imaging using compressive sensing. (arXiv:2201.00820v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00820">
<div class="article-summary-box-inner">
<span><p>Fluorescence microscopy has been a significant tool to observe long-term
imaging of embryos (in vivo) growth over time. However, cumulative exposure is
phototoxic to such sensitive live samples. While techniques like light-sheet
fluorescence microscopy (LSFM) allow for reduced exposure, it is not well
suited for deep imaging models. Other computational techniques are
computationally expensive and often lack restoration quality. To address this
challenge, one can use various low-dosage imaging techniques that are developed
to achieve the 3D volume reconstruction using a few slices in the axial
direction (z-axis); however, they often lack restoration quality. Also,
acquiring dense images (with small steps) in the axial direction is
computationally expensive. To address this challenge, we present a compressive
sensing (CS) based approach to fully reconstruct 3D volumes with the same
signal-to-noise ratio (SNR) with less than half of the excitation dosage. We
present the theory and experimentally validate the approach. To demonstrate our
technique, we capture a 3D volume of the RFP labeled neurons in the zebrafish
embryo spinal cord (30um thickness) with the axial sampling of 0.1um using a
confocal microscope. From the results, we observe the CS-based approach
achieves accurate 3D volume reconstruction from less than 20% of the entire
stack optical sections. The developed CS-based methodology in this work can be
easily applied to other deep imaging modalities such as two-photon and
light-sheet microscopy, where reducing sample photo-toxicity is a critical
challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Runway Extraction and Improved Mapping from Space Imagery. (arXiv:2201.00848v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00848">
<div class="article-summary-box-inner">
<span><p>Change detection methods applied to monitoring key infrastructure like
airport runways represent an important capability for disaster relief and urban
planning. The present work identifies two generative adversarial networks (GAN)
architectures that translate reversibly between plausible runway maps and
satellite imagery. We illustrate the training capability using paired images
(satellite-map) from the same point of view and using the Pix2Pix architecture
or conditional GANs. In the absence of available pairs, we likewise show that
CycleGAN architectures with four network heads (discriminator-generator pairs)
can also provide effective style transfer from raw image pixels to outline or
feature maps. To emphasize the runway and tarmac boundaries, we experimentally
show that the traditional grey-tan map palette is not a required training input
but can be augmented by higher contrast mapping palettes (red-black) for
sharper runway boundaries. We preview a potentially novel use case (called
"sketch2satellite") where a human roughly draws the current runway boundaries
and automates the machine output of plausible satellite images. Finally, we
identify examples of faulty runway maps where the published satellite and
mapped runways disagree but an automated update renders the correct map using
GANs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Delving into Sample Loss Curve to Embrace Noisy and Imbalanced Data. (arXiv:2201.00849v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00849">
<div class="article-summary-box-inner">
<span><p>Corrupted labels and class imbalance are commonly encountered in practically
collected training data, which easily leads to over-fitting of deep neural
networks (DNNs). Existing approaches alleviate these issues by adopting a
sample re-weighting strategy, which is to re-weight sample by designing
weighting function. However, it is only applicable for training data containing
only either one type of data biases. In practice, however, biased samples with
corrupted labels and of tailed classes commonly co-exist in training data. How
to handle them simultaneously is a key but under-explored problem. In this
paper, we find that these two types of biased samples, though have similar
transient loss, have distinguishable trend and characteristics in loss curves,
which could provide valuable priors for sample weight assignment. Motivated by
this, we delve into the loss curves and propose a novel probe-and-allocate
training strategy: In the probing stage, we train the network on the whole
biased training data without intervention, and record the loss curve of each
sample as an additional attribute; In the allocating stage, we feed the
resulting attribute to a newly designed curve-perception network, named
CurveNet, to learn to identify the bias type of each sample and assign proper
weights through meta-learning adaptively. The training speed of meta learning
also blocks its application. To solve it, we propose a method named skip layer
meta optimization (SLMO) to accelerate training speed by skipping the bottom
layers. Extensive synthetic and real experiments well validate the proposed
method, which achieves state-of-the-art performance on multiple challenging
benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gaussian-Hermite Moment Invariants of General Vector Functions to Rotation-Affine Transform. (arXiv:2201.00877v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00877">
<div class="article-summary-box-inner">
<span><p>With the development of data acquisition technology, multi-channel data is
collected and widely used in many fields. Most of them can be expressed as
various types of vector functions. Feature extraction of vector functions for
identifying certain patterns of interest is a critical but challenging task. In
this paper, we focus on constructing moment invariants of general vector
functions. Specifically, we define rotation-affine transform to describe real
deformations of general vector functions, and then design a structural frame to
systematically generate Gaussian-Hermite moment invariants to this transform
model. This is the first time that a uniform frame has been proposed in the
literature to construct orthogonal moment invariants of general vector
functions. Given a certain type of multi-channel data, we demonstrate how to
utilize the new method to derive all possible invariants and to eliminate
various dependences among them. For RGB images, 2D and 3D flow fields, we
obtain the complete and independent sets of the invariants with low orders and
low degrees. Based on synthetic and popular datasets of vector-valued data, the
experiments are carried out to evaluate the stability and discriminability of
these invariants, and also their robustness to noise. The results clearly show
that the moment invariants proposed in our paper have better performance than
other previously used moment invariants of vector functions in RGB image
classification, vortex detection in 2D vector fields and template matching for
3D flow fields.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rice Diseases Detection and Classification Using Attention Based Neural Network and Bayesian Optimization. (arXiv:2201.00893v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00893">
<div class="article-summary-box-inner">
<span><p>In this research, an attention-based depthwise separable neural network with
Bayesian optimization (ADSNN-BO) is proposed to detect and classify rice
disease from rice leaf images. Rice diseases frequently result in 20 to 40 \%
corp production loss in yield and is highly related to the global economy.
Rapid disease identification is critical to plan treatment promptly and reduce
the corp losses. Rice disease diagnosis is still mainly performed manually. To
achieve AI assisted rapid and accurate disease detection, we proposed the
ADSNN-BO model based on MobileNet structure and augmented attention mechanism.
Moreover, Bayesian optimization method is applied to tune hyper-parameters of
the model. Cross-validated classification experiments are conducted based on a
public rice disease dataset with four categories in total. The experimental
results demonstrate that our mobile compatible ADSNN-BO model achieves a test
accuracy of 94.65\%, which outperforms all of the state-of-the-art models
tested. To check the interpretability of our proposed model, feature analysis
including activation map and filters visualization approach are also conducted.
Results show that our proposed attention-based mechanism can more effectively
guide the ADSNN-BO model to learn informative features. The outcome of this
research will promote the implementation of artificial intelligence for fast
plant disease diagnosis and control in the agricultural field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Gradient Mapping Guided Explainable Deep Neural Network for Extracapsular Extension Identification in 3D Head and Neck Cancer Computed Tomography Images. (arXiv:2201.00895v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00895">
<div class="article-summary-box-inner">
<span><p>Diagnosis and treatment management for head and neck squamous cell carcinoma
(HNSCC) is guided by routine diagnostic head and neck computed tomography (CT)
scans to identify tumor and lymph node features. Extracapsular extension (ECE)
is a strong predictor of patients' survival outcomes with HNSCC. It is
essential to detect the occurrence of ECE as it changes staging and management
for the patients. Current clinical ECE detection relies on visual
identification and pathologic confirmation conducted by radiologists. Machine
learning (ML)-based ECE diagnosis has shown high potential in the recent years.
However, manual annotation of lymph node region is a required data
preprocessing step in most of the current ML-based ECE diagnosis studies. In
addition, this manual annotation process is time-consuming, labor-intensive,
and error-prone. Therefore, in this paper, we propose a Gradient Mapping Guided
Explainable Network (GMGENet) framework to perform ECE identification
automatically without requiring annotated lymph node region information. The
gradient-weighted class activation mapping (Grad-CAM) technique is proposed to
guide the deep learning algorithm to focus on the regions that are highly
related to ECE. Informative volumes of interest (VOIs) are extracted without
labeled lymph node region information. In evaluation, the proposed method is
well-trained and tested using cross validation, achieving test accuracy and AUC
of 90.2% and 91.1%, respectively. The presence or absence of ECE has been
analyzed and correlated with gold standard histopathological findings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">External Attention Assisted Multi-Phase Splenic Vascular Injury Segmentation with Limited Data. (arXiv:2201.00942v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00942">
<div class="article-summary-box-inner">
<span><p>The spleen is one of the most commonly injured solid organs in blunt
abdominal trauma. The development of automatic segmentation systems from
multi-phase CT for splenic vascular injury can augment severity grading for
improving clinical decision support and outcome prediction. However, accurate
segmentation of splenic vascular injury is challenging for the following
reasons: 1) Splenic vascular injury can be highly variant in shape, texture,
size, and overall appearance; and 2) Data acquisition is a complex and
expensive procedure that requires intensive efforts from both data scientists
and radiologists, which makes large-scale well-annotated datasets hard to
acquire in general.
</p>
<p>In light of these challenges, we hereby design a novel framework for
multi-phase splenic vascular injury segmentation, especially with limited data.
On the one hand, we propose to leverage external data to mine pseudo splenic
masks as the spatial attention, dubbed external attention, for guiding the
segmentation of splenic vascular injury. On the other hand, we develop a
synthetic phase augmentation module, which builds upon generative adversarial
networks, for populating the internal data by fully leveraging the relation
between different phases. By jointly enforcing external attention and
populating internal data representation during training, our proposed method
outperforms other competing methods and substantially improves the popular
DeepLab-v3+ baseline by more than 7% in terms of average DSC, which confirms
its effectiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HWRCNet: Handwritten Word Recognition in JPEG Compressed Domain using CNN-BiLSTM Network. (arXiv:2201.00947v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00947">
<div class="article-summary-box-inner">
<span><p>The handwritten word recognition from images using deep learning is an active
research area with promising performance. It practical scenario, it might be
required to process the handwritten images in the compressed domain due to due
to security reasons. However, the utilization of deep learning is still very
limited for the processing of compressed images. Motivated by the need of
processing document images in the compressed domain using recent developments
in deep learning, we propose a HWRCNet model for handwritten word recognition
in JPEG compressed domain. The proposed model combines the Convolutional Neural
Network (CNN) and Bi-Directional Long Short Term Memory (BiLSTM) based
Recurrent Neural Network (RNN). Basically, we train the model using compressed
domain images and observe a very appealing performance with 89.05% word
recognition accuracy and 13.37% character error rate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stain Normalized Breast Histopathology Image Recognition using Convolutional Neural Networks for Cancer Detection. (arXiv:2201.00957v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00957">
<div class="article-summary-box-inner">
<span><p>Computer assisted diagnosis in digital pathology is becoming ubiquitous as it
can provide more efficient and objective healthcare diagnostics. Recent
advances have shown that the convolutional Neural Network (CNN) architectures,
a well-established deep learning paradigm, can be used to design a Computer
Aided Diagnostic (CAD) System for breast cancer detection. However, the
challenges due to stain variability and the effect of stain normalization with
such deep learning frameworks are yet to be well explored. Moreover,
performance analysis with arguably more efficient network models, which may be
important for high throughput screening, is also not well explored.To address
this challenge, we consider some contemporary CNN models for binary
classification of breast histopathology images that involves (1) the data
preprocessing with stain normalized images using an adaptive colour
deconvolution (ACD) based color normalization algorithm to handle the stain
variabilities; and (2) applying transfer learning based training of some
arguably more efficient CNN models, namely Visual Geometry Group Network
(VGG16), MobileNet and EfficientNet. We have validated the trained CNN networks
on a publicly available BreaKHis dataset, for 200x and 400x magnified
histopathology images. The experimental analysis shows that pretrained networks
in most cases yield better quality results on data augmented breast
histopathology images with stain normalization, than the case without stain
normalization. Further, we evaluated the performance and efficiency of popular
lightweight networks using stain normalized images and found that EfficientNet
outperforms VGG16 and MobileNet in terms of test accuracy and F1 Score. We
observed that efficiency in terms of test time is better in EfficientNet than
other networks; VGG Net, MobileNet, without much drop in the classification
accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AI visualization in Nanoscale Microscopy. (arXiv:2201.00966v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00966">
<div class="article-summary-box-inner">
<span><p>Artificial Intelligence &amp; Nanotechnology are promising areas for the future
of humanity. While Deep Learning based Computer Vision has found applications
in many fields from medicine to automotive, its application in nanotechnology
can open doors for new scientific discoveries. Can we apply AI to explore
objects that our eyes can't see such as nano scale sized objects? An AI
platform to visualize nanoscale patterns learnt by a Deep Learning neural
network can open new frontiers for nanotechnology. The objective of this paper
is to develop a Deep Learning based visualization system on images of
nanomaterials obtained by scanning electron microscope. This paper contributes
an AI platform to enable any nanoscience researcher to use AI in visual
exploration of nanoscale morphologies of nanomaterials. This AI is developed by
a technique of visualizing intermediate activations of a Convolutional
AutoEncoder. In this method, a nano scale specimen image is transformed into
its feature representations by a Convolution Neural Network. The Convolutional
AutoEncoder is trained on 100% SEM dataset, and then CNN visualization is
applied. This AI generates various conceptual feature representations of the
nanomaterial.
</p>
<p>While Deep Learning based image classification of SEM images are widely
published in literature, there are not much publications that have visualized
Deep neural networks of nanomaterials. There is a significant opportunity to
gain insights from the learnings extracted by machine learning. This paper
unlocks the potential of applying Deep Learning based Visualization on electron
microscopy to offer AI extracted features and architectural patterns of various
nanomaterials. This is a contribution in Explainable AI in nano scale objects.
This paper contributes an open source AI with reproducible results at URL
(https://sites.google.com/view/aifornanotechnology)
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interactive Attention AI to translate low light photos to captions for night scene understanding in women safety. (arXiv:2201.00969v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00969">
<div class="article-summary-box-inner">
<span><p>There is amazing progress in Deep Learning based models for Image captioning
and Low Light image enhancement. For the first time in literature, this paper
develops a Deep Learning model that translates night scenes to sentences,
opening new possibilities for AI applications in the safety of visually
impaired women. Inspired by Image Captioning and Visual Question Answering, a
novel Interactive Image Captioning is developed. A user can make the AI focus
on any chosen person of interest by influencing the attention scoring.
Attention context vectors are computed from CNN feature vectors and
user-provided start word. The Encoder-Attention-Decoder neural network learns
to produce captions from low brightness images. This paper demonstrates how
women safety can be enabled by researching a novel AI capability in the
Interactive Vision-Language model for perception of the environment in the
night.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StyleM: Stylized Metrics for Image Captioning Built with Contrastive N-grams. (arXiv:2201.00975v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00975">
<div class="article-summary-box-inner">
<span><p>In this paper, we build two automatic evaluation metrics for evaluating the
association between a machine-generated caption and a ground truth stylized
caption: OnlyStyle and StyleCIDEr.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Underwater Object Classification and Detection: first results and open challenges. (arXiv:2201.00977v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00977">
<div class="article-summary-box-inner">
<span><p>This work reviews the problem of object detection in underwater environments.
We analyse and quantify the shortcomings of conventional state-of-the-art
(SOTA) algorithms in the computer vision community when applied to this
challenging environment, as well as providing insights and general guidelines
for future research efforts. First, we assessed if pretraining with the
conventional ImageNet is beneficial when the object detector needs to be
applied to environments that may be characterised by a different feature
distribution. We then investigate whether two-stage detectors yields to better
performance with respect to single-stage detectors, in terms of accuracy,
intersection of union (IoU), floating operation per second (FLOPS), and
inference time. Finally, we assessed the generalisation capability of each
model to a lower quality dataset to simulate performance on a real scenario, in
which harsher conditions ought to be expected. Our experimental results provide
evidence that underwater object detection requires searching for "ad-hoc"
architectures than merely training SOTA architectures on new data, and that
pretraining is not beneficial.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PyramidTNT: Improved Transformer-in-Transformer Baselines with Pyramid Architecture. (arXiv:2201.00978v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00978">
<div class="article-summary-box-inner">
<span><p>Transformer networks have achieved great progress for computer vision tasks.
Transformer-in-Transformer (TNT) architecture utilizes inner transformer and
outer transformer to extract both local and global representations. In this
work, we present new TNT baselines by introducing two advanced designs: 1)
pyramid architecture, and 2) convolutional stem. The new "PyramidTNT"
significantly improves the original TNT by establishing hierarchical
representations. PyramidTNT achieves better performances than the previous
state-of-the-art vision transformers such as Swin Transformer. We hope this new
baseline will be helpful to the further research and application of vision
transformer. Code will be available at
https://github.com/huawei-noah/CV-Backbones/tree/master/tnt_pytorch.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Variational Stacked Local Attention Networks for Diverse Video Captioning. (arXiv:2201.00985v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00985">
<div class="article-summary-box-inner">
<span><p>While describing Spatio-temporal events in natural language, video captioning
models mostly rely on the encoder's latent visual representation. Recent
progress on the encoder-decoder model attends encoder features mainly in linear
interaction with the decoder. However, growing model complexity for visual data
encourages more explicit feature interaction for fine-grained information,
which is currently absent in the video captioning domain. Moreover, feature
aggregations methods have been used to unveil richer visual representation,
either by the concatenation or using a linear layer. Though feature sets for a
video semantically overlap to some extent, these approaches result in objective
mismatch and feature redundancy. In addition, diversity in captions is a
fundamental component of expressing one event from several meaningful
perspectives, currently missing in the temporal, i.e., video captioning domain.
To this end, we propose Variational Stacked Local Attention Network (VSLAN),
which exploits low-rank bilinear pooling for self-attentive feature interaction
and stacking multiple video feature streams in a discount fashion. Each feature
stack's learned attributes contribute to our proposed diversity encoding
module, followed by the decoding query stage to facilitate end-to-end diverse
and natural captions without any explicit supervision on attributes. We
evaluate VSLAN on MSVD and MSR-VTT datasets in terms of syntax and diversity.
The CIDEr score of VSLAN outperforms current off-the-shelf methods by $7.8\%$
on MSVD and $4.5\%$ on MSR-VTT, respectively. On the same datasets, VSLAN
achieves competitive results in caption diversity metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attention Mechanism Meets with Hybrid Dense Network for Hyperspectral Image Classification. (arXiv:2201.01001v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01001">
<div class="article-summary-box-inner">
<span><p>Convolutional Neural Networks (CNN) are more suitable, indeed. However, fixed
kernel sizes make traditional CNN too specific, neither flexible nor conducive
to feature learning, thus impacting on the classification accuracy. The
convolution of different kernel size networks may overcome this problem by
capturing more discriminating and relevant information. In light of this, the
proposed solution aims at combining the core idea of 3D and 2D Inception net
with the Attention mechanism to boost the HSIC CNN performance in a hybrid
scenario. The resulting \textit{attention-fused hybrid network} (AfNet) is
based on three attention-fused parallel hybrid sub-nets with different kernels
in each block repeatedly using high-level features to enhance the final
ground-truth maps. In short, AfNet is able to selectively filter out the
discriminative features critical for classification. Several tests on HSI
datasets provided competitive results for AfNet compared to state-of-the-art
models. The proposed pipeline achieved, indeed, an overall accuracy of 97\% for
the Indian Pines, 100\% for Botswana, 99\% for Pavia University, Pavia Center,
and Salinas datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Representation Adaptation Network for Cross-domain Image Classification. (arXiv:2201.01002v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01002">
<div class="article-summary-box-inner">
<span><p>In image classification, it is often expensive and time-consuming to acquire
sufficient labels. To solve this problem, domain adaptation often provides an
attractive option given a large amount of labeled data from a similar nature
but different domain. Existing approaches mainly align the distributions of
representations extracted by a single structure and the representations may
only contain partial information, e.g., only contain part of the saturation,
brightness, and hue information. Along this line, we propose
Multi-Representation Adaptation which can dramatically improve the
classification accuracy for cross-domain image classification and specially
aims to align the distributions of multiple representations extracted by a
hybrid structure named Inception Adaptation Module (IAM). Based on this, we
present Multi-Representation Adaptation Network (MRAN) to accomplish the
cross-domain image classification task via multi-representation alignment which
can capture the information from different aspects. In addition, we extend
Maximum Mean Discrepancy (MMD) to compute the adaptation loss. Our approach can
be easily implemented by extending most feed-forward models with IAM, and the
network can be trained efficiently via back-propagation. Experiments conducted
on three benchmark image datasets demonstrate the effectiveness of MRAN. The
code has been available at https://github.com/easezyc/deep-transfer-learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Aligning Domain-specific Distribution and Classifier for Cross-domain Classification from Multiple Sources. (arXiv:2201.01003v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01003">
<div class="article-summary-box-inner">
<span><p>While Unsupervised Domain Adaptation (UDA) algorithms, i.e., there are only
labeled data from source domains, have been actively studied in recent years,
most algorithms and theoretical results focus on Single-source Unsupervised
Domain Adaptation (SUDA). However, in the practical scenario, labeled data can
be typically collected from multiple diverse sources, and they might be
different not only from the target domain but also from each other. Thus,
domain adapters from multiple sources should not be modeled in the same way.
Recent deep learning based Multi-source Unsupervised Domain Adaptation (MUDA)
algorithms focus on extracting common domain-invariant representations for all
domains by aligning distribution of all pairs of source and target domains in a
common feature space. However, it is often very hard to extract the same
domain-invariant representations for all domains in MUDA. In addition, these
methods match distributions without considering domain-specific decision
boundaries between classes. To solve these problems, we propose a new framework
with two alignment stages for MUDA which not only respectively aligns the
distributions of each pair of source and target domains in multiple specific
feature spaces, but also aligns the outputs of classifiers by utilizing the
domain-specific decision boundaries. Extensive experiments demonstrate that our
method can achieve remarkable results on popular benchmark datasets for image
classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Generate Novel Classes for Deep Metric Learning. (arXiv:2201.01008v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01008">
<div class="article-summary-box-inner">
<span><p>Deep metric learning aims to learn an embedding space where the distance
between data reflects their class equivalence, even when their classes are
unseen during training. However, the limited number of classes available in
training precludes generalization of the learned embedding space. Motivated by
this, we introduce a new data augmentation approach that synthesizes novel
classes and their embedding vectors. Our approach can provide rich semantic
information to an embedding model and improve its generalization by augmenting
training data with novel classes unavailable in the original data. We implement
this idea by learning and exploiting a conditional generative model, which,
given a class label and a noise, produces a random embedding vector of the
class. Our proposed generator allows the loss to use richer class relations by
augmenting realistic and diverse classes, resulting in better generalization to
unseen samples. Experimental results on public benchmark datasets demonstrate
that our method clearly enhances the performance of proxy-based losses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MoCoPnet: Exploring Local Motion and Contrast Priors for Infrared Small Target Super-Resolution. (arXiv:2201.01014v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01014">
<div class="article-summary-box-inner">
<span><p>Infrared small target super-resolution (SR) aims to recover reliable and
detailed high-resolution image with highcontrast targets from its
low-resolution counterparts. Since the infrared small target lacks color and
fine structure information, it is significant to exploit the supplementary
information among sequence images to enhance the target. In this paper, we
propose the first infrared small target SR method named local motion and
contrast prior driven deep network (MoCoPnet) to integrate the domain knowledge
of infrared small target into deep network, which can mitigate the intrinsic
feature scarcity of infrared small targets. Specifically, motivated by the
local motion prior in the spatio-temporal dimension, we propose a local
spatiotemporal attention module to perform implicit frame alignment and
incorporate the local spatio-temporal information to enhance the local features
(especially for small targets). Motivated by the local contrast prior in the
spatial dimension, we propose a central difference residual group to
incorporate the central difference convolution into the feature extraction
backbone, which can achieve center-oriented gradient-aware feature extraction
to further improve the target contrast. Extensive experiments have demonstrated
that our method can recover accurate spatial dependency and improve the target
contrast. Comparative results show that MoCoPnet can outperform the
state-of-the-art video SR and single image SR methods in terms of both SR
performance and target enhancement. Based on the SR results, we further
investigate the influence of SR on infrared small target detection and the
experimental results demonstrate that MoCoPnet promotes the detection
performance. The code is available at https://github.com/XinyiYing/MoCoPnet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detailed Facial Geometry Recovery from Multi-view Images by Learning an Implicit Function. (arXiv:2201.01016v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01016">
<div class="article-summary-box-inner">
<span><p>Recovering detailed facial geometry from a set of calibrated multi-view
images is valuable for its wide range of applications. Traditional multi-view
stereo (MVS) methods adopt optimization methods to regularize the matching
cost. Recently, learning-based methods integrate all these into an end-to-end
neural network and show superiority of efficiency. In this paper, we propose a
novel architecture to recover extremely detailed 3D faces in roughly 10
seconds. Unlike previous learning-based methods that regularize the cost volume
via 3D CNN, we propose to learn an implicit function for regressing the
matching cost. By fitting a 3D morphable model from multi-view images, the
features of multiple images are extracted and aggregated in the mesh-attached
UV space, which makes the implicit function more effective in recovering
detailed facial shape. Our method outperforms SOTA learning-based MVS in
accuracy by a large margin on the FaceScape dataset. The code and data will be
released soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly-supervised continual learning for class-incremental segmentation. (arXiv:2201.01029v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01029">
<div class="article-summary-box-inner">
<span><p>Transfer learning is a powerful way to adapt existing deep learning models to
new emerging use-cases in remote sensing. Starting from a neural network
already trained for semantic segmentation, we propose to modify its label space
to swiftly adapt it to new classes under weak supervision. To alleviate the
background shift and the catastrophic forgetting problems inherent to this form
of continual learning, we compare different regularization terms and leverage a
pseudo-label strategy. We experimentally show the relevance of our approach on
three public remote sensing datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Robust Visual Sampling Model Inspired by Receptive Field. (arXiv:2201.01030v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01030">
<div class="article-summary-box-inner">
<span><p>Spike camera mimicking the retina fovea can report per-pixel luminance
intensity accumulation by firing spikes. As a bio-inspired vision sensor with
high temporal resolution, it has a huge potential for computer vision. However,
the sampling model in current Spike camera is so susceptible to quantization
and noise that it cannot capture the texture details of objects effectively. In
this work, a robust visual sampling model inspired by receptive field (RVSM) is
proposed where wavelet filter generated by difference of Gaussian (DoG) and
Gaussian filter are used to simulate receptive field. Using corresponding
method similar to inverse wavelet transform, spike data from RVSM can be
converted into images. To test the performance, we also propose a high-speed
motion spike dataset (HMD) including a variety of motion scenes. By comparing
reconstructed images in HMD, we find RVSM can improve the ability of capturing
information of Spike camera greatly. More importantly, due to mimicking
receptive field mechanism to collect regional information, RVSM can filter high
intensity noise effectively and improves the problem that Spike camera is
sensitive to noise largely. Besides, due to the strong generalization of
sampling structure, RVSM is also suitable for other neuromorphic vision sensor.
Above experiments are finished in a Spike camera simulator.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Hinders Perceptual Quality of PSNR-oriented Methods?. (arXiv:2201.01034v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01034">
<div class="article-summary-box-inner">
<span><p>In this paper, we discover two factors that inhibit POMs from achieving high
perceptual quality: 1) center-oriented optimization (COO) problem and 2)
model's low-frequency tendency. First, POMs tend to generate an SR image whose
position in the feature space is closest to the distribution center of all
potential high-resolution (HR) images, resulting in such POMs losing
high-frequency details. Second, $90\%$ area of an image consists of
low-frequency signals; in contrast, human perception relies on an image's
high-frequency details. However, POMs apply the same calculation to process
different-frequency areas, so that POMs tend to restore the low-frequency
regions. Based on these two factors, we propose a Detail Enhanced Contrastive
Loss (DECLoss), by combining a high-frequency enhancement module and spatial
contrastive learning module, to reduce the influence of the COO problem and
low-Frequency tendency. Experimental results show the efficiency and
effectiveness when applying DECLoss on several regular SR models. E.g, in EDSR,
our proposed method achieves 3.60$\times$ faster learning speed compared to a
GAN-based method with a subtle degradation in visual quality. In addition, our
final results show that an SR network equipped with our DECLoss generates more
realistic and visually pleasing textures compared to state-of-the-art methods.
%The source code of the proposed method is included in the supplementary
material and will be made publicly available in the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sound and Visual Representation Learning with Multiple Pretraining Tasks. (arXiv:2201.01046v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01046">
<div class="article-summary-box-inner">
<span><p>Different self-supervised tasks (SSL) reveal different features from the
data. The learned feature representations can exhibit different performance for
each downstream task. In this light, this work aims to combine Multiple SSL
tasks (Multi-SSL) that generalizes well for all downstream tasks. Specifically,
for this study, we investigate binaural sounds and image data in isolation. For
binaural sounds, we propose three SSL tasks namely, spatial alignment, temporal
synchronization of foreground objects and binaural audio and temporal gap
prediction. We investigate several approaches of Multi-SSL and give insights
into the downstream task performance on video retrieval, spatial sound super
resolution, and semantic prediction on the OmniAudio dataset. Our experiments
on binaural sound representations demonstrate that Multi-SSL via incremental
learning (IL) of SSL tasks outperforms single SSL task models and fully
supervised models in the downstream task performance. As a check of
applicability on other modality, we also formulate our Multi-SSL models for
image representation learning and we use the recently proposed SSL tasks,
MoCov2 and DenseCL. Here, Multi-SSL surpasses recent methods such as MoCov2,
DenseCL and DetCo by 2.06%, 3.27% and 1.19% on VOC07 classification and +2.83,
+1.56 and +1.61 AP on COCO detection. Code will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DIAL: Deep Interactive and Active Learning for Semantic Segmentation in Remote Sensing. (arXiv:2201.01047v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01047">
<div class="article-summary-box-inner">
<span><p>We propose in this article to build up a collaboration between a deep neural
network and a human in the loop to swiftly obtain accurate segmentation maps of
remote sensing images. In a nutshell, the agent iteratively interacts with the
network to correct its initially flawed predictions. Concretely, these
interactions are annotations representing the semantic labels. Our
methodological contribution is twofold. First, we propose two interactive
learning schemes to integrate user inputs into deep neural networks. The first
one concatenates the annotations with the other network's inputs. The second
one uses the annotations as a sparse ground-truth to retrain the network.
Second, we propose an active learning strategy to guide the user towards the
most relevant areas to annotate. To this purpose, we compare different
state-of-the-art acquisition functions to evaluate the neural network
uncertainty such as ConfidNet, entropy or ODIN. Through experiments on three
remote sensing datasets, we show the effectiveness of the proposed methods.
Notably, we show that active learning based on uncertainty estimation enables
to quickly lead the user towards mistakes and that it is thus relevant to guide
the user interventions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Unsupervised Open World Semantic Segmentation. (arXiv:2201.01073v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01073">
<div class="article-summary-box-inner">
<span><p>For the semantic segmentation of images, state-of-the-art deep neural
networks (DNNs) achieve high segmentation accuracy if that task is restricted
to a closed set of classes. However, as of now DNNs have limited ability to
operate in an open world, where they are tasked to identify pixels belonging to
unknown objects and eventually to learn novel classes, incrementally. Humans
have the capability to say: I don't know what that is, but I've already seen
something like that. Therefore, it is desirable to perform such an incremental
learning task in an unsupervised fashion. We introduce a method where unknown
objects are clustered based on visual similarity. Those clusters are utilized
to define new classes and serve as training data for unsupervised incremental
learning. More precisely, the connected components of a predicted semantic
segmentation are assessed by a segmentation quality estimate. connected
components with a low estimated prediction quality are candidates for a
subsequent clustering. Additionally, the component-wise quality assessment
allows for obtaining predicted segmentation masks for the image regions
potentially containing unknown objects. The respective pixels of such masks are
pseudo-labeled and afterwards used for re-training the DNN, i.e., without the
use of ground truth generated by humans. In our experiments we demonstrate
that, without access to ground truth and even with few data, a DNN's class
space can be extended by a novel class, achieving considerable segmentation
accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Understanding and Harnessing the Effect of Image Transformation in Adversarial Detection. (arXiv:2201.01080v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01080">
<div class="article-summary-box-inner">
<span><p>Deep neural networks (DNNs) are under threat from adversarial examples.
Adversarial detection is a fundamental work for robust DNNs-based service,
which distinguishes adversarial images from benign images. Image transformation
is one of the most effective approaches to detect adversarial examples. During
the last few years, a variety of image transformations have been studied and
discussed to design reliable adversarial detectors. In this paper, we
systematically review the recent progress on adversarial detection via image
transformations with a novel taxonomy. Then we conduct an extensive set of
experiments to test the detection performance of image transformations towards
the state-of-the-art adversarial attacks. Furthermore, we reveal that the
single transformation is not capable of detecting robust adversarial examples,
and propose an improved approach by combining multiple image transformations.
The results show that the joint approach achieves significant improvement in
detection accuracy and recall. We suggest that the joint detector is a more
effective tool to detect adversarial examples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying the exterior image of buildings on a 3D map and extracting elevation information using deep learning and digital image processing. (arXiv:2201.01081v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01081">
<div class="article-summary-box-inner">
<span><p>Despite the fact that architectural administration information in Korea has
been providing high-quality information for a long period of time, the level of
utility of the information is not high because it focuses on administrative
information. While this is the case, a three-dimensional (3D) map with higher
resolution has emerged along with the technological development. However, it
cannot function better than visual transmission, as it includes only image
information focusing on the exterior of the building. If information related to
the exterior of the building can be extracted or identified from a 3D map, it
is expected that the utility of the information will be more valuable as the
national architectural administration information can then potentially be
extended to include such information regarding the building exteriors to the
level of BIM(Building Information Modeling). This study aims to present and
assess a basic method of extracting information related to the appearance of
the exterior of a building for the purpose of 3D mapping using deep learning
and digital image processing. After extracting and preprocessing images from
the map, information was identified using the Fast R-CNN(Regions with
Convolutional Neuron Networks) model. The information was identified using the
Faster R-CNN model after extracting and preprocessing images from the map. As a
result, it showed approximately 93% and 91% accuracy in terms of detecting the
elevation and window parts of the building, respectively, as well as excellent
performance in an experiment aimed at extracting the elevation information of
the building. Nonetheless, it is expected that improved results will be
obtained by supplementing the probability of mixing the false detection rate or
noise data caused by the misunderstanding of experimenters in relation to the
unclear boundaries of windows.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Quality-aware Representation for Multi-person Pose Regression. (arXiv:2201.01087v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01087">
<div class="article-summary-box-inner">
<span><p>Off-the-shelf single-stage multi-person pose regression methods generally
leverage the instance score (i.e., confidence of the instance localization) to
indicate the pose quality for selecting the pose candidates. We consider that
there are two gaps involved in existing paradigm:~1) The instance score is not
well interrelated with the pose regression quality.~2) The instance feature
representation, which is used for predicting the instance score, does not
explicitly encode the structural pose information to predict the reasonable
score that represents pose regression quality. To address the aforementioned
issues, we propose to learn the pose regression quality-aware representation.
Concretely, for the first gap, instead of using the previous instance
confidence label (e.g., discrete {1,0} or Gaussian representation) to denote
the position and confidence for person instance, we firstly introduce the
Consistent Instance Representation (CIR) that unifies the pose regression
quality score of instance and the confidence of background into a pixel-wise
score map to calibrates the inconsistency between instance score and pose
regression quality. To fill the second gap, we further present the Query
Encoding Module (QEM) including the Keypoint Query Encoding (KQE) to encode the
positional and semantic information for each keypoint and the Pose Query
Encoding (PQE) which explicitly encodes the predicted structural pose
information to better fit the Consistent Instance Representation (CIR). By
using the proposed components, we significantly alleviate the above gaps. Our
method outperforms previous single-stage regression-based even bottom-up
methods and achieves the state-of-the-art result of 71.7 AP on MS COCO test-dev
set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Short Range Correlation Transformer for Occluded Person Re-Identification. (arXiv:2201.01090v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01090">
<div class="article-summary-box-inner">
<span><p>Occluded person re-identification is one of the challenging areas of computer
vision, which faces problems such as inefficient feature representation and low
recognition accuracy. Convolutional neural network pays more attention to the
extraction of local features, therefore it is difficult to extract features of
occluded pedestrians and the effect is not so satisfied. Recently, vision
transformer is introduced into the field of re-identification and achieves the
most advanced results by constructing the relationship of global features
between patch sequences. However, the performance of vision transformer in
extracting local features is inferior to that of convolutional neural network.
Therefore, we design a partial feature transformer-based person
re-identification framework named PFT. The proposed PFT utilizes three modules
to enhance the efficiency of vision transformer. (1) Patch full dimension
enhancement module. We design a learnable tensor with the same size as patch
sequences, which is full-dimensional and deeply embedded in patch sequences to
enrich the diversity of training samples. (2) Fusion and reconstruction module.
We extract the less important part of obtained patch sequences, and fuse them
with original patch sequence to reconstruct the original patch sequences. (3)
Spatial Slicing Module. We slice and group patch sequences from spatial
direction, which can effectively improve the short-range correlation of patch
sequences. Experimental results over occluded and holistic re-identification
datasets demonstrate that the proposed PFT network achieves superior
performance consistently and outperforms the state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Transferable Unrestricted Adversarial Examples with Minimum Changes. (arXiv:2201.01102v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01102">
<div class="article-summary-box-inner">
<span><p>Transfer-based adversarial example is one of the most important classes of
black-box attacks. However, there is a trade-off between transferability and
imperceptibility of the adversarial perturbation. Prior work in this direction
often requires a fixed but large $\ell_p$-norm perturbation budget to reach a
good transfer success rate, leading to perceptible adversarial perturbations.
On the other hand, most of the current unrestricted adversarial attacks that
aim to generate semantic-preserving perturbations suffer from weaker
transferability to the target model. In this work, we propose a geometry-aware
framework to generate transferable adversarial examples with minimum changes.
Analogous to model selection in statistical machine learning, we leverage a
validation model to select the optimal perturbation budget for each image under
both the $\ell_{\infty}$-norm and unrestricted threat models. Extensive
experiments verify the effectiveness of our framework on balancing
imperceptibility and transferability of the crafted adversarial examples. The
methodology is the foundation of our entry to the CVPR'21 Security AI
Challenger: Unrestricted Adversarial Attacks on ImageNet, in which we ranked
1st place out of 1,559 teams and surpassed the runner-up submissions by 4.59%
and 23.91% in terms of final score and average image quality level,
respectively. Code is available at https://github.com/Equationliu/GA-Attack.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Augmentation for Depression Detection Using Skeleton-Based Gait Information. (arXiv:2201.01115v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01115">
<div class="article-summary-box-inner">
<span><p>In recent years, the incidence of depression is rising rapidly worldwide, but
large-scale depression screening is still challenging. Gait analysis provides a
non-contact, low-cost, and efficient early screening method for depression.
However, the early screening of depression based on gait analysis lacks
sufficient effective sample data. In this paper, we propose a skeleton data
augmentation method for assessing the risk of depression. First, we propose
five techniques to augment skeleton data and apply them to depression and
emotion datasets. Then, we divide augmentation methods into two types
(non-noise augmentation and noise augmentation) based on the mutual information
and the classification accuracy. Finally, we explore which augmentation
strategies can capture the characteristics of human skeleton data more
effectively. Experimental results show that the augmented training data set
that retains more of the raw skeleton data properties determines the
performance of the detection model. Specifically, rotation augmentation and
channel mask augmentation make the depression detection accuracy reach 92.15%
and 91.34%, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepVisualInsight: Time-Travelling Visualization for Spatio-Temporal Causality of Deep Classification Training. (arXiv:2201.01155v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01155">
<div class="article-summary-box-inner">
<span><p>Understanding how the predictions of deep learning models are formed during
the training process is crucial to improve model performance and fix model
defects, especially when we need to investigate nontrivial training strategies
such as active learning, and track the root cause of unexpected training
results such as performance degeneration.
</p>
<p>In this work, we propose a time-travelling visual solution DeepVisualInsight
(DVI), aiming to manifest the spatio-temporal causality while training a deep
learning image classifier. The spatio-temporal causality demonstrates how the
gradient-descent algorithm and various training data sampling techniques can
influence and reshape the layout of learnt input representation and the
classification boundaries in consecutive epochs. Such causality allows us to
observe and analyze the whole learning process in the visible low dimensional
space. Technically, we propose four spatial and temporal properties and design
our visualization solution to satisfy them. These properties preserve the most
important information when inverse-)projecting input samples between the
visible low-dimensional and the invisible high-dimensional space, for causal
analyses. Our extensive experiments show that, comparing to baseline
approaches, we achieve the best visualization performance regarding the
spatial/temporal properties and visualization efficiency. Moreover, our case
study shows that our visual solution can well reflect the characteristics of
various training scenarios, showing good potential of DVI as a debugging tool
for analyzing deep learning training processes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepFGS: Fine-Grained Scalable Coding for Learned Image Compression. (arXiv:2201.01173v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01173">
<div class="article-summary-box-inner">
<span><p>Scalable coding, which can adapt to channel bandwidth variation, performs
well in today's complex network environment. However, the existing scalable
compression methods face two challenges: reduced compression performance and
insufficient scalability. In this paper, we propose the first learned
fine-grained scalable image compression model (DeepFGS) to overcome the above
two shortcomings. Specifically, we introduce a feature separation backbone to
divide the image information into basic and scalable features, then
redistribute the features channel by channel through an information
rearrangement strategy. In this way, we can generate a continuously scalable
bitstream via one-pass encoding. In addition, we reuse the decoder to reduce
the parameters and computational complexity of DeepFGS. Experiments demonstrate
that our DeepFGS outperforms all learning-based scalable image compression
models and conventional scalable image codecs in PSNR and MS-SSIM metrics. To
the best of our knowledge, our DeepFGS is the first exploration of learned
fine-grained scalable coding, which achieves the finest scalability compared
with learning-based methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated 3D reconstruction of LoD2 and LoD1 models for all 10 million buildings of the Netherlands. (arXiv:2201.01191v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01191">
<div class="article-summary-box-inner">
<span><p>In this paper we present our workflow to automatically reconstruct 3D
building models based on 2D building polygons and a LiDAR point cloud. The
workflow generates models at different levels of detail (LoDs) to support data
requirements of different applications from one consistent source. Specific
attention has been paid to make the workflow robust to quickly run a new
iteration in case of improvements in an algorithm or in case new input data
become available. The quality of the reconstructed data highly depends on the
quality of the input data and is monitored in several steps of the process. A
3D viewer has been developed to view and download the openly available 3D data
at different LoDs in different formats. The workflow has been applied to all 10
million buildings of The Netherlands. The 3D service will be updated after new
input data becomes available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The cluster structure function. (arXiv:2201.01222v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01222">
<div class="article-summary-box-inner">
<span><p>For each partition of a data set into a given number of parts there is a
partition such that every part is as much as possible a good model (an
"algorithmic sufficient statistic") for the data in that part. Since this can
be done for every number between one and the number of data, the result is a
function, the cluster structure function. It maps the number of parts of a
partition to values related to the deficiencies of being good models by the
parts. Such a function starts with a value at least zero for no partition of
the data set and descents to zero for the partition of the data set into
singleton parts. The optimal clustering is the one chosen to minimize the
cluster structure function. The theory behind the method is expressed in
algorithmic information theory (Kolmogorov complexity). In practice the
Kolmogorov complexities involved are approximated by a concrete compressor. We
give examples using real data sets: the MNIST handwritten digits and the
segmentation of real cells as used in stem cell research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Semi-supervised Federated Learning for Images Automatic Recognition in Internet of Drones. (arXiv:2201.01230v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01230">
<div class="article-summary-box-inner">
<span><p>Air access networks have been recognized as a significant driver of various
Internet of Things (IoT) services and applications. In particular, the aerial
computing network infrastructure centered on the Internet of Drones has set off
a new revolution in automatic image recognition. This emerging technology
relies on sharing ground truth labeled data between Unmanned Aerial Vehicle
(UAV) swarms to train a high-quality automatic image recognition model.
However, such an approach will bring data privacy and data availability
challenges. To address these issues, we first present a Semi-supervised
Federated Learning (SSFL) framework for privacy-preserving UAV image
recognition. Specifically, we propose model parameters mixing strategy to
improve the naive combination of FL and semi-supervised learning methods under
two realistic scenarios (labels-at-client and labels-at-server), which is
referred to as Federated Mixing (FedMix). Furthermore, there are significant
differences in the number, features, and distribution of local data collected
by UAVs using different camera modules in different environments, i.e.,
statistical heterogeneity. To alleviate the statistical heterogeneity problem,
we propose an aggregation rule based on the frequency of the client's
participation in training, namely the FedFreq aggregation rule, which can
adjust the weight of the corresponding local model according to its frequency.
Numerical results demonstrate that the performance of our proposed method is
significantly better than those of the current baseline and is robust to
different non-IID levels of client data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transfer Learning for Retinal Vascular Disease Detection: A Pilot Study with Diabetic Retinopathy and Retinopathy of Prematurity. (arXiv:2201.01250v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01250">
<div class="article-summary-box-inner">
<span><p>Retinal vascular diseases affect the well-being of human body and sometimes
provide vital signs of otherwise undetected bodily damage. Recently, deep
learning techniques have been successfully applied for detection of diabetic
retinopathy (DR). The main obstacle of applying deep learning techniques to
detect most other retinal vascular diseases is the limited amount of data
available. In this paper, we propose a transfer learning technique that aims to
utilize the feature similarities for detecting retinal vascular diseases. We
choose the well-studied DR detection as a source task and identify the early
detection of retinopathy of prematurity (ROP) as the target task. Our
experimental results demonstrate that our DR-pretrained approach dominates in
all metrics the conventional ImageNet-pretrained transfer learning approach,
currently adopted in medical image analysis. Moreover, our approach is more
robust with respect to the stochasticity in the training process and with
respect to reduced training samples. This study suggests the potential of our
proposed transfer learning approach for a broad range of retinal vascular
diseases or pathologies, where data is limited.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images. (arXiv:2201.01266v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01266">
<div class="article-summary-box-inner">
<span><p>Semantic segmentation of brain tumors is a fundamental medical image analysis
task involving multiple MRI imaging modalities that can assist clinicians in
diagnosing the patient and successively studying the progression of the
malignant entity. In recent years, Fully Convolutional Neural Networks (FCNNs)
approaches have become the de facto standard for 3D medical image segmentation.
The popular "U-shaped" network architecture has achieved state-of-the-art
performance benchmarks on different 2D and 3D semantic segmentation tasks and
across various imaging modalities. However, due to the limited kernel size of
convolution layers in FCNNs, their performance of modeling long-range
information is sub-optimal, and this can lead to deficiencies in the
segmentation of tumors with variable sizes. On the other hand, transformer
models have demonstrated excellent capabilities in capturing such long-range
information in multiple domains, including natural language processing and
computer vision. Inspired by the success of vision transformers and their
variants, we propose a novel segmentation model termed Swin UNEt TRansformers
(Swin UNETR). Specifically, the task of 3D brain tumor semantic segmentation is
reformulated as a sequence to sequence prediction problem wherein multi-modal
input data is projected into a 1D sequence of embedding and used as an input to
a hierarchical Swin transformer as the encoder. The swin transformer encoder
extracts features at five different resolutions by utilizing shifted windows
for computing self-attention and is connected to an FCNN-based decoder at each
resolution via skip connections. We have participated in BraTS 2021
segmentation challenge, and our proposed model ranks among the top-performing
approaches in the validation phase. Code: https://monai.io/research/swin-unetr
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Local Quadruple Pattern: A Novel Descriptor for Facial Image Recognition and Retrieval. (arXiv:2201.01275v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01275">
<div class="article-summary-box-inner">
<span><p>In this paper a novel hand crafted local quadruple pattern (LQPAT) is
proposed for facial image recognition and retrieval. Most of the existing
hand-crafted descriptors encodes only a limited number of pixels in the local
neighbourhood. Under unconstrained environment the performance of these
descriptors tends to degrade drastically. The major problem in increasing the
local neighbourhood is that, it also increases the feature length of the
descriptor. The proposed descriptor try to overcome these problems by defining
an efficient encoding structure with optimal feature length. The proposed
descriptor encodes relations amongst the neighbours in quadruple space. Two
micro patterns are computed from the local relationships to form the
descriptor. The retrieval and recognition accuracies of the proposed descriptor
has been compared with state of the art hand crafted descriptors on bench mark
databases namely; Caltech-face, LFW, Colour-FERET, and CASIA-face-v5. Result
analysis shows that the proposed descriptor performs well under uncontrolled
variations in pose, illumination, background and expressions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Local Directional Gradient Pattern: A Local Descriptor for Face Recognition. (arXiv:2201.01276v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01276">
<div class="article-summary-box-inner">
<span><p>In this paper a local pattern descriptor in high order derivative space is
proposed for face recognition. The proposed local directional gradient pattern
(LDGP) is a 1D local micropattern computed by encoding the relationships
between the higher order derivatives of the reference pixel in four distinct
directions. The proposed descriptor identifies the relationship between the
high order derivatives of the referenced pixel in four different directions to
compute the micropattern which corresponds to the local feature. Proposed
descriptor considerably reduces the length of the micropattern which
consequently reduces the extraction time and matching time while maintaining
the recognition rate. Results of the extensive experiments conducted on
benchmark databases AT&amp;T, Extended Yale B and CMU-PIE show that the proposed
descriptor significantly reduces the extraction as well as matching time while
the recognition rate is almost similar to the existing state of the art
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Learning from 100 Million Medical Images. (arXiv:2201.01283v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01283">
<div class="article-summary-box-inner">
<span><p>Building accurate and robust artificial intelligence systems for medical
image assessment requires not only the research and design of advanced deep
learning models but also the creation of large and curated sets of annotated
training examples. Constructing such datasets, however, is often very costly --
due to the complex nature of annotation tasks and the high level of expertise
required for the interpretation of medical images (e.g., expert radiologists).
To counter this limitation, we propose a method for self-supervised learning of
rich image features based on contrastive learning and online feature
clustering. For this purpose we leverage large training datasets of over
100,000,000 medical images of various modalities, including radiography,
computed tomography (CT), magnetic resonance (MR) imaging and ultrasonography.
We propose to use these features to guide model training in supervised and
hybrid self-supervised/supervised regime on various downstream tasks. We
highlight a number of advantages of this strategy on challenging image
assessment problems in radiography, CT and MR: 1) Significant increase in
accuracy compared to the state-of-the-art (e.g., AUC boost of 3-7% for
detection of abnormalities from chest radiography scans and hemorrhage
detection on brain CT); 2) Acceleration of model convergence during training by
up to 85% compared to using no pretraining (e.g., 83% when training a model for
detection of brain metastases in MR scans); 3) Increase in robustness to
various image augmentations, such as intensity variations, rotations or scaling
reflective of data variation seen in the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Transformer-Based Siamese Network for Change Detection. (arXiv:2201.01293v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01293">
<div class="article-summary-box-inner">
<span><p>This paper presents a transformer-based Siamese network architecture
(abbreviated by ChangeFormer) for Change Detection (CD) from a pair of
co-registered remote sensing images. Different from recent CD frameworks, which
are based on fully convolutional networks (ConvNets), the proposed method
unifies hierarchically structured transformer encoder with Multi-Layer
Perception (MLP) decoder in a Siamese network architecture to efficiently
render multi-scale long-range details required for accurate CD. Experiments on
two CD datasets show that the proposed end-to-end trainable ChangeFormer
architecture achieves better CD performance than previous counterparts. Our
code is available at https://github.com/wgcban/ChangeFormer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3DVSR: 3D EPI Volume-based Approach for Angular and Spatial Light field Image Super-resolution. (arXiv:2201.01294v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01294">
<div class="article-summary-box-inner">
<span><p>Light field (LF) imaging, which captures both spatial and angular information
of a scene, is undoubtedly beneficial to numerous applications. Although
various techniques have been proposed for LF acquisition, achieving both
angularly and spatially high-resolution LF remains a technology challenge. In
this paper, a learning-based approach applied to 3D epipolar image (EPI) is
proposed to reconstruct high-resolution LF. Through a 2-stage super-resolution
framework, the proposed approach effectively addresses various LF
super-resolution (SR) problems, i.e., spatial SR, angular SR, and
angular-spatial SR. While the first stage provides flexible options to
up-sample EPI volume to the desired resolution, the second stage, which
consists of a novel EPI volume-based refinement network (EVRN), substantially
enhances the quality of the high-resolution EPI volume. An extensive evaluation
on 90 challenging synthetic and real-world light field scenes from 7 published
datasets shows that the proposed approach outperforms state-of-the-art methods
to a large extend for both spatial and angular super-resolution problem, i.e.,
an average peak signal to noise ratio improvement of more than 2.0 dB, 1.4 dB,
and 3.14 dB in spatial SR $\times 2$, spatial SR $\times 4$, and angular SR
respectively. The reconstructed 4D light field demonstrates a balanced
performance distribution across all perspective images and presents superior
visual quality compared to the previous works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Online Multi-Object Tracking with Unsupervised Re-Identification Learning and Occlusion Estimation. (arXiv:2201.01297v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01297">
<div class="article-summary-box-inner">
<span><p>Occlusion between different objects is a typical challenge in Multi-Object
Tracking (MOT), which often leads to inferior tracking results due to the
missing detected objects. The common practice in multi-object tracking is
re-identifying the missed objects after their reappearance. Though tracking
performance can be boosted by the re-identification, the annotation of identity
is required to train the model. In addition, such practice of re-identification
still can not track those highly occluded objects when they are missed by the
detector. In this paper, we focus on online multi-object tracking and design
two novel modules, the unsupervised re-identification learning module and the
occlusion estimation module, to handle these problems. Specifically, the
proposed unsupervised re-identification learning module does not require any
(pseudo) identity information nor suffer from the scalability issue. The
proposed occlusion estimation module tries to predict the locations where
occlusions happen, which are used to estimate the positions of missed objects
by the detector. Our study shows that, when applied to state-of-the-art MOT
methods, the proposed unsupervised re-identification learning is comparable to
supervised re-identification learning, and the tracking performance is further
improved by the proposed occlusion estimation module.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Object Removal and Spatio-Temporal RGB-D Inpainting via Geometry-Aware Adversarial Learning. (arXiv:2008.05058v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.05058">
<div class="article-summary-box-inner">
<span><p>Dynamic objects have a significant impact on the robot's perception of the
environment which degrades the performance of essential tasks such as
localization and mapping. In this work, we address this problem by synthesizing
plausible color, texture and geometry in regions occluded by dynamic objects.
We propose the novel geometry-aware DynaFill architecture that follows a
coarse-to-fine topology and incorporates our gated recurrent feedback mechanism
to adaptively fuse information from previous timesteps. We optimize our
architecture using adversarial training to synthesize fine realistic textures
which enables it to hallucinate color and depth structure in occluded regions
online in a spatially and temporally coherent manner, without relying on future
frame information. Casting our inpainting problem as an image-to-image
translation task, our model also corrects regions correlated with the presence
of dynamic objects in the scene, such as shadows or reflections. We introduce a
large-scale hyperrealistic dataset with RGB-D images, semantic segmentation
labels, camera poses as well as groundtruth RGB-D information of occluded
regions. Extensive quantitative and qualitative evaluations show that our
approach achieves state-of-the-art performance, even in challenging weather
conditions. Furthermore, we present results for retrieval-based visual
localization with the synthesized images that demonstrate the utility of our
approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accelerated Zeroth-Order and First-Order Momentum Methods from Mini to Minimax Optimization. (arXiv:2008.08170v5 [math.OC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.08170">
<div class="article-summary-box-inner">
<span><p>In the paper, we propose a class of accelerated zeroth-order and first-order
momentum methods for both nonconvex mini-optimization and minimax-optimization.
Specifically, we propose a new accelerated zeroth-order momentum (Acc-ZOM)
method for black-box mini-optimization. Moreover, we prove that our Acc-ZOM
method achieves a lower query complexity of $\tilde{O}(d^{3/4}\epsilon^{-3})$
for finding an $\epsilon$-stationary point, which improves the best known
result by a factor of $O(d^{1/4})$ where $d$ denotes the variable dimension. In
particular, the Acc-ZOM does not require large batches required in the existing
zeroth-order stochastic algorithms. Meanwhile, we propose an accelerated
\textbf{zeroth-order} momentum descent ascent (Acc-ZOMDA) method for
\textbf{black-box} minimax-optimization, which obtains a query complexity of
$\tilde{O}((d_1+d_2)^{3/4}\kappa_y^{4.5}\epsilon^{-3})$ without large batches
for finding an $\epsilon$-stationary point, where $d_1$ and $d_2$ denote
variable dimensions and $\kappa_y$ is condition number. Moreover, we propose an
accelerated \textbf{first-order} momentum descent ascent (Acc-MDA) method for
\textbf{white-box} minimax optimization, which has a gradient complexity of
$\tilde{O}(\kappa_y^{4.5}\epsilon^{-3})$ without large batches for finding an
$\epsilon$-stationary point. In particular, our Acc-MDA can obtain a lower
gradient complexity of $\tilde{O}(\kappa_y^{2.5}\epsilon^{-3})$ with a batch
size $O(\kappa_y^4)$. Extensive experimental results on the black-box
adversarial attack to deep neural networks (DNNs) and poisoning attack
demonstrate efficiency of our algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Convolutional Normalization: Improving Deep Convolutional Network Robustness and Training. (arXiv:2103.00673v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.00673">
<div class="article-summary-box-inner">
<span><p>Normalization techniques have become a basic component in modern
convolutional neural networks (ConvNets). In particular, many recent works
demonstrate that promoting the orthogonality of the weights helps train deep
models and improve robustness. For ConvNets, most existing methods are based on
penalizing or normalizing weight matrices derived from concatenating or
flattening the convolutional kernels. These methods often destroy or ignore the
benign convolutional structure of the kernels; therefore, they are often
expensive or impractical for deep ConvNets. In contrast, we introduce a simple
and efficient "Convolutional Normalization" (ConvNorm) method that can fully
exploit the convolutional structure in the Fourier domain and serve as a simple
plug-and-play module to be conveniently incorporated into any ConvNets. Our
method is inspired by recent work on preconditioning methods for convolutional
sparse coding and can effectively promote each layer's channel-wise isometry.
Furthermore, we show that our ConvNorm can reduce the layerwise spectral norm
of the weight matrices and hence improve the Lipschitzness of the network,
leading to easier training and improved robustness for deep ConvNets. Applied
to classification under noise corruptions and generative adversarial network
(GAN), we show that the ConvNorm improves the robustness of common ConvNets
such as ResNet and the performance of GAN. We verify our findings via numerical
experiments on CIFAR and ImageNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey On Universal Adversarial Attack. (arXiv:2103.01498v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.01498">
<div class="article-summary-box-inner">
<span><p>The intriguing phenomenon of adversarial examples has attracted significant
attention in machine learning and what might be more surprising to the
community is the existence of universal adversarial perturbations (UAPs), i.e.
a single perturbation to fool the target DNN for most images. With the focus on
UAP against deep classifiers, this survey summarizes the recent progress on
universal adversarial attacks, discussing the challenges from both the attack
and defense sides, as well as the reason for the existence of UAP. We aim to
extend this work as a dynamic survey that will regularly update its content to
follow new works regarding UAP or universal attack in a wide range of domains,
such as image, audio, video, text, etc. Relevant updates will be discussed at:
https://bit.ly/2SbQlLG. We welcome authors of future works in this field to
contact us for including your new finding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the effectiveness of adversarial training against common corruptions. (arXiv:2103.02325v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.02325">
<div class="article-summary-box-inner">
<span><p>The literature on robustness towards common corruptions shows no consensus on
whether adversarial training can improve the performance in this setting.
First, we show that, when used with an appropriately selected perturbation
radius, $\ell_p$ adversarial training can serve as a strong baseline against
common corruptions improving both accuracy and calibration. Then we explain why
adversarial training performs better than data augmentation with simple
Gaussian noise which has been observed to be a meaningful baseline on common
corruptions. Related to this, we identify the $\sigma$-overfitting phenomenon
when Gaussian augmentation overfits to a particular standard deviation used for
training which has a significant detrimental effect on common corruption
accuracy. We discuss how to alleviate this problem and then how to further
enhance $\ell_p$ adversarial training by introducing an efficient relaxation of
adversarial training with learned perceptual image patch similarity as the
distance metric. Through experiments on CIFAR-10 and ImageNet-100, we show that
our approach does not only improve the $\ell_p$ adversarial training baseline
but also has cumulative gains with data augmentation methods such as AugMix,
DeepAugment, ANT, and SIN, leading to state-of-the-art performance on common
corruptions.
</p>
<p>The code of our experiments is publicly available at
https://github.com/tml-epfl/adv-training-corruptions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Neural Networks Learn Meta-Structures from Noisy Labels in Semantic Segmentation. (arXiv:2103.11594v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.11594">
<div class="article-summary-box-inner">
<span><p>How deep neural networks (DNNs) learn from noisy labels has been studied
extensively in image classification but much less in image segmentation. So
far, our understanding of the learning behavior of DNNs trained by noisy
segmentation labels remains limited. In this study, we address this deficiency
in both binary segmentation of biological microscopy images and multi-class
segmentation of natural images. We generate extremely noisy labels by randomly
sampling a small fraction (e.g., 10%) or flipping a large fraction (e.g., 90%)
of the ground truth labels. When trained with these noisy labels, DNNs provide
largely the same segmentation performance as trained by the original ground
truth. This indicates that DNNs learn structures hidden in labels rather than
pixel-level labels per se in their supervised training for semantic
segmentation. We refer to these hidden structures in labels as meta-structures.
When DNNs are trained by labels with different perturbations to the
meta-structure, we find consistent degradation in their segmentation
performance. In contrast, incorporation of meta-structure information
substantially improves performance of an unsupervised segmentation model
developed for binary semantic segmentation. We define meta-structures
mathematically as spatial density distributions and show both theoretically and
experimentally how this formulation explains key observed learning behavior of
DNNs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Implicit Statistical Shape Models for 3D Medical Image Delineation. (arXiv:2104.02847v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.02847">
<div class="article-summary-box-inner">
<span><p>3D delineation of anatomical structures is a cardinal goal in medical imaging
analysis. Prior to deep learning, statistical shape models that imposed
anatomical constraints and produced high quality surfaces were a core
technology. Prior to deep learning, statistical shape models that imposed
anatomical constraints and produced high quality surfaces were a core
technology. Today fully-convolutional networks (FCNs), while dominant, do not
offer these capabilities. We present deep implicit statistical shape models
(DISSMs), a new approach to delineation that marries the representation power
of convolutional neural networks (CNNs) with the robustness of SSMs. DISSMs use
a deep implicit surface representation to produce a compact and descriptive
shape latent space that permits statistical models of anatomical variance. To
reliably fit anatomically plausible shapes to an image, we introduce a novel
rigid and non-rigid pose estimation pipeline that is modelled as a Markov
decision process(MDP). We outline a training regime that includes inverted
episodic training and a deep realization of marginal space learning (MSL).
Intra-dataset experiments on the task of pathological liver segmentation
demonstrate that DISSMs can perform more robustly than three leading FCN
models, including nnU-Net: reducing the mean Hausdorff distance (HD) by
7.7-14.3mm and improving the worst case Dice-Sorensen coefficient (DSC) by
1.2-2.3%. More critically, cross-dataset experiments on a dataset directly
reflecting clinical deployment scenarios demonstrate that DISSMs improve the
mean DSC and HD by 3.5-5.9% and 12.3-24.5mm, respectively, and the worst-case
DSC by 5.4-7.3%. These improvements are over and above any benefits from
representing delineations with high-quality surface.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shadow Generation for Composite Image in Real-world Scenes. (arXiv:2104.10338v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.10338">
<div class="article-summary-box-inner">
<span><p>Image composition targets at inserting a foreground object into a background
image. Most previous image composition methods focus on adjusting the
foreground to make it compatible with background while ignoring the shadow
effect of foreground on the background. In this work, we focus on generating
plausible shadow for the foreground object in the composite image. First, we
contribute a real-world shadow generation dataset DESOBA by generating
synthetic composite images based on paired real images and deshadowed images.
Then, we propose a novel shadow generation network SGRNet, which consists of a
shadow mask prediction stage and a shadow filling stage. In the shadow mask
prediction stage, foreground and background information are thoroughly
interacted to generate foreground shadow mask. In the shadow filling stage,
shadow parameters are predicted to fill the shadow area. Extensive experiments
on our DESOBA dataset and real composite images demonstrate the effectiveness
of our proposed method. Our dataset and code are available at
https://github.com/bcmi/Object-Shadow-Generation-Dataset-DESOBA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Domain and Disentangled Face Manipulation with 3D Guidance. (arXiv:2104.11228v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.11228">
<div class="article-summary-box-inner">
<span><p>Face image manipulation via three-dimensional guidance has been widely
applied in various interactive scenarios due to its semantically-meaningful
understanding and user-friendly controllability. However, existing
3D-morphable-model-based manipulation methods are not directly applicable to
out-of-domain faces, such as non-photorealistic paintings, cartoon portraits,
or even animals, mainly due to the formidable difficulties in building the
model for each specific face domain. To overcome this challenge, we propose, as
far as we know, the first method to manipulate faces in arbitrary domains using
human 3DMM. This is achieved through two major steps: 1) disentangled mapping
from 3DMM parameters to the latent space embedding of a pre-trained StyleGAN2
that guarantees disentangled and precise controls for each semantic attribute;
and 2) cross-domain adaptation that bridges domain discrepancies and makes
human 3DMM applicable to out-of-domain faces by enforcing a consistent latent
space embedding. Experiments and comparisons demonstrate the superiority of our
high-quality semantic manipulation method on a variety of face domains with all
major 3D facial attributes controllable-pose, expression, shape, albedo, and
illumination. Moreover, we develop an intuitive editing interface to support
user-friendly control and instant feedback. Our project page is
https://cassiepython.github.io/cddfm3d/index.html
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Actor: Neural Free-view Synthesis of Human Actors with Pose Control. (arXiv:2106.02019v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02019">
<div class="article-summary-box-inner">
<span><p>We propose Neural Actor (NA), a new method for high-quality synthesis of
humans from arbitrary viewpoints and under arbitrary controllable poses. Our
method is built upon recent neural scene representation and rendering works
which learn representations of geometry and appearance from only 2D images.
While existing works demonstrated compelling rendering of static scenes and
playback of dynamic scenes, photo-realistic reconstruction and rendering of
humans with neural implicit methods, in particular under user-controlled novel
poses, is still difficult. To address this problem, we utilize a coarse body
model as the proxy to unwarp the surrounding 3D space into a canonical pose. A
neural radiance field learns pose-dependent geometric deformations and pose-
and view-dependent appearance effects in the canonical space from multi-view
video input. To synthesize novel views of high fidelity dynamic geometry and
appearance, we leverage 2D texture maps defined on the body model as latent
variables for predicting residual deformations and the dynamic appearance.
Experiments demonstrate that our method achieves better quality than the
state-of-the-arts on playback as well as novel pose synthesis, and can even
generalize well to new poses that starkly differ from the training poses.
Furthermore, our method also supports body shape control of the synthesized
results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tensor feature hallucination for few-shot learning. (arXiv:2106.05321v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05321">
<div class="article-summary-box-inner">
<span><p>Few-shot learning addresses the challenge of learning how to address novel
tasks given not just limited supervision but limited data as well. An
attractive solution is synthetic data generation. However, most such methods
are overly sophisticated, focusing on high-quality, realistic data in the input
space. It is unclear whether adapting them to the few-shot regime and using
them for the downstream task of classification is the right approach. Previous
works on synthetic data generation for few-shot classification focus on
exploiting complex models, e.g. a Wasserstein GAN with multiple regularizers or
a network that transfers latent diversities from known to novel classes.
</p>
<p>We follow a different approach and investigate how a simple and
straightforward synthetic data generation method can be used effectively. We
make two contributions, namely we show that: (1) using a simple loss function
is more than enough for training a feature generator in the few-shot setting;
and (2) learning to generate tensor features instead of vector features is
superior. Extensive experiments on miniImagenet, CUB and CIFAR-FS datasets show
that our method sets a new state of the art, outperforming more sophisticated
few-shot data augmentation methods. The source code can be found at
https://github.com/MichalisLazarou/TFH_fewshot.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VisBuddy -- A Smart Wearable Assistant for the Visually Challenged. (arXiv:2108.07761v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07761">
<div class="article-summary-box-inner">
<span><p>Vision plays a crucial role in comprehending the world around us. More than
85% of the external information is obtained through the vision system. It
influences our mobility, cognition, information access, and interaction with
the environment and other people. Blindness prevents a person from gaining
knowledge of the surrounding environment and makes unassisted navigation,
object recognition, obstacle avoidance, and reading tasks significant
challenges. Many existing systems are often limited by cost and complexity. To
help the visually challenged overcome these difficulties faced in everyday
life, we propose VisBuddy, a smart assistant to help the visually challenged
with their day-to-day activities. VisBuddy is a voice-based assistant where the
user can give voice commands to perform specific tasks. It uses the techniques
of image captioning for describing the user's surroundings, optical character
recognition (OCR) for reading the text in the user's view, object detection to
search and find the objects in a room and web scraping to give the user the
latest news. VisBuddy has been built by combining the concepts from Deep
Learning and the Internet of Things. Thus, VisBuddy serves as a cost-efficient,
powerful, all-in-one assistant for the visually challenged by helping them with
their day-to-day activities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anchor DETR: Query Design for Transformer-Based Object Detection. (arXiv:2109.07107v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07107">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel query design for the transformer-based
object detection. In previous transformer-based detectors, the object queries
are a set of learned embeddings. However, each learned embedding does not have
an explicit physical meaning and we cannot explain where it will focus on. It
is difficult to optimize as the prediction slot of each object query does not
have a specific mode. In other words, each object query will not focus on a
specific region. To solved these problems, in our query design, object queries
are based on anchor points, which are widely used in CNN-based detectors. So
each object query focuses on the objects near the anchor point. Moreover, our
query design can predict multiple objects at one position to solve the
difficulty: "one region, multiple objects". In addition, we design an attention
variant, which can reduce the memory cost while achieving similar or better
performance than the standard attention in DETR. Thanks to the query design and
the attention variant, the proposed detector that we called Anchor DETR, can
achieve better performance and run faster than the DETR with 10$\times$ fewer
training epochs. For example, it achieves 44.2 AP with 19 FPS on the MSCOCO
dataset when using the ResNet50-DC5 feature for training 50 epochs. Extensive
experiments on the MSCOCO benchmark prove the effectiveness of the proposed
methods. Code is available at
\url{https://github.com/megvii-research/AnchorDETR}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Architectural Ingredients of Adversarially Robust Deep Neural Networks. (arXiv:2110.03825v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03825">
<div class="article-summary-box-inner">
<span><p>Deep neural networks (DNNs) are known to be vulnerable to adversarial
attacks. A range of defense methods have been proposed to train adversarially
robust DNNs, among which adversarial training has demonstrated promising
results. However, despite preliminary understandings developed for adversarial
training, it is still not clear, from the architectural perspective, what
configurations can lead to more robust DNNs. In this paper, we address this gap
via a comprehensive investigation on the impact of network width and depth on
the robustness of adversarially trained DNNs. Specifically, we make the
following key observations: 1) more parameters (higher model capacity) does not
necessarily help adversarial robustness; 2) reducing capacity at the last stage
(the last group of blocks) of the network can actually improve adversarial
robustness; and 3) under the same parameter budget, there exists an optimal
architectural configuration for adversarial robustness. We also provide a
theoretical analysis explaning why such network configuration can help
robustness. These architectural insights can help design adversarially robust
DNNs. Code is available at \url{https://github.com/HanxunH/RobustWRN}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Salt and pepper noise removal method based on stationary Framelet transform with non-convex sparsity regularization. (arXiv:2110.09113v6 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09113">
<div class="article-summary-box-inner">
<span><p>Salt and pepper noise removal is a common inverse problem in image
processing. Traditional denoising methods have two limitations. First, noise
characteristics are often not described accurately. For example, the noise
location information is often ignored and the sparsity of the salt and pepper
noise is often described by L1 norm, which cannot illustrate the sparse
variables clearly. Second, conventional methods separate the contaminated image
into a recovered image and a noise part, thus resulting in recovering an image
with unsatisfied smooth parts and detail parts. In this study, we introduce a
noise detection strategy to determine the position of the noise, and a
non-convex sparsity regularization depicted by Lp quasi-norm is employed to
describe the sparsity of the noise, thereby addressing the first limitation.
The morphological component analysis framework with stationary Framelet
transform is adopted to decompose the processed image into cartoon, texture,
and noise parts to resolve the second limitation. Then, the alternating
direction method of multipliers (ADMM) is employed to solve the proposed model.
Finally, experiments are conducted to verify the proposed method and compare it
with some current state-of-the-art denoising methods. The experimental results
show that the proposed method can remove salt and pepper noise while preserving
the details of the processed image.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Multi-Organ Segmentation Using SpatialConfiguration-Net with Low GPU Memory Requirements. (arXiv:2111.13630v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13630">
<div class="article-summary-box-inner">
<span><p>Even though many semantic segmentation methods exist that are able to perform
well on many medical datasets, often, they are not designed for direct use in
clinical practice. The two main concerns are generalization to unseen data with
a different visual appearance, e.g., images acquired using a different scanner,
and efficiency in terms of computation time and required Graphics Processing
Unit (GPU) memory. In this work, we employ a multi-organ segmentation model
based on the SpatialConfiguration-Net (SCN), which integrates prior knowledge
of the spatial configuration among the labelled organs to resolve spurious
responses in the network outputs. Furthermore, we modified the architecture of
the segmentation model to reduce its memory footprint as much as possible
without drastically impacting the quality of the predictions. Lastly, we
implemented a minimal inference script for which we optimized both, execution
time and required GPU memory.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decomposing the Deep: Finding Class Specific Filters in Deep CNNs. (arXiv:2112.07719v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.07719">
<div class="article-summary-box-inner">
<span><p>Interpretability of Deep Neural Networks has become a major area of
exploration. Although these networks have achieved state of the art accuracy in
many tasks, it is extremely difficult to interpret and explain their decisions.
In this work we analyze the final and penultimate layers of Deep Convolutional
Networks and provide an efficient method for identifying subsets of features
that contribute most towards the network's decision for a class. We demonstrate
that the number of such features per class is much lower in comparison to the
dimension of the final layer and therefore the decision surface of Deep CNNs
lies on a low dimensional manifold and is proportional to the network depth.
Our methods allow to decompose the final layer into separate subspaces which is
far more interpretable and has a lower computational cost as compared to the
final layer of the full network.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Positional Encoding Augmented GAN for the Assessment of Wind Flow for Pedestrian Comfort in Urban Areas. (arXiv:2112.08447v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08447">
<div class="article-summary-box-inner">
<span><p>Approximating wind flows using computational fluid dynamics (CFD) methods can
be time-consuming. Creating a tool for interactively designing prototypes while
observing the wind flow change requires simpler models to simulate faster.
Instead of running numerical approximations resulting in detailed calculations,
data-driven methods and deep learning might be able to give similar results in
a fraction of the time. This work rephrases the problem from computing 3D flow
fields using CFD to a 2D image-to-image translation-based problem on the
building footprints to predict the flow field at pedestrian height level. We
investigate the use of generative adversarial networks (GAN), such as Pix2Pix
[1] and CycleGAN [2] representing state-of-the-art for image-to-image
translation task in various domains as well as U-Net autoencoder [3]. The
models can learn the underlying distribution of a dataset in a data-driven
manner, which we argue can help the model learn the underlying
Reynolds-averaged Navier-Stokes (RANS) equations from CFD. We experiment on
novel simulated datasets on various three-dimensional bluff-shaped buildings
with and without height information. Moreover, we present an extensive
qualitative and quantitative evaluation of the generated images for a selection
of models and compare their performance with the simulations delivered by CFD.
We then show that adding positional data to the input can produce more accurate
results by proposing a general framework for injecting such information on the
different architectures. Furthermore, we show that the models performances
improve by applying attention mechanisms and spectral normalization to
facilitate stable training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Microfossil Identification via Deep Metric Learning. (arXiv:2112.09490v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09490">
<div class="article-summary-box-inner">
<span><p>We apply deep metric learning for the first time to the prob-lem of
classifying planktic foraminifer shells on microscopic images. This species
recognition task is an important information source and scientific pillar for
reconstructing past climates. All foraminifer CNN recognition pipelines in the
literature produce black-box classifiers that lack visualisation options for
human experts and cannot be applied to open set problems. Here, we benchmark
metric learning against these pipelines, produce the first scientific
visualisation of the phenotypic planktic foraminifer morphology space, and
demonstrate that metric learning can be used to cluster species unseen during
training. We show that metric learning out-performs all published CNN-based
state-of-the-art benchmarks in this domain. We evaluate our approach on the
34,640 expert-annotated images of the Endless Forams public library of 35
modern planktic foraminifera species. Our results on this data show leading 92%
accuracy (at 0.84 F1-score) in reproducing expert labels on withheld test data,
and 66.5% accuracy (at 0.70 F1-score) when clustering species never encountered
in training. We conclude that metric learning is highly effective for this
domain and serves as an important tool towards expert-in-the-loop automation of
microfossil identification. Key code, network weights, and data splits are
published with this paper for full reproducibility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual Path Structural Contrastive Embeddings for Learning Novel Objects. (arXiv:2112.12359v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.12359">
<div class="article-summary-box-inner">
<span><p>Learning novel classes from a very few labeled samples has attracted
increasing attention in machine learning areas. Recent research on either
meta-learning based or transfer-learning based paradigm demonstrates that
gaining information on a good feature space can be an effective solution to
achieve favorable performance on few-shot tasks. In this paper, we propose a
simple but effective paradigm that decouples the tasks of learning feature
representations and classifiers and only learns the feature embedding
architecture from base classes via the typical transfer-learning training
strategy. To maintain both the generalization ability across base and novel
classes and discrimination ability within each class, we propose a dual path
feature learning scheme that effectively combines structural similarity with
contrastive feature construction. In this way, both inner-class alignment and
inter-class uniformity can be well balanced, and result in improved
performance. Experiments on three popular benchmarks show that when
incorporated with a simple prototype based classifier, our method can still
achieve promising results for both standard and generalized few-shot problems
in either an inductive or transductive inference setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generation of Synthetic Rat Brain MRI scans with a 3D Enhanced Alpha-GAN. (arXiv:2112.13626v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13626">
<div class="article-summary-box-inner">
<span><p>Translational brain research using Magnetic Resonance Imaging (MRI) is
becoming increasingly popular as animal models are an essential part of
scientific studies and more ultra-high-field scanners are becoming available.
Some disadvantages of MRI are the availability of MRI scanners and the time
required for a full scanning session (it usually takes over 30 minutes).
Privacy laws and the 3Rs ethics rule also make it difficult to create large
datasets for training deep learning models. Generative Adversarial Networks
(GANs) can perform data augmentation with higher quality than other techniques.
In this work, the alpha-GAN architecture is used to test its ability to produce
realistic 3D MRI scans of the rat brain. As far as the authors are aware, this
is the first time that a GAN-based approach has been used for data augmentation
in preclinical data. The generated scans are evaluated using various
qualitative and quantitative metrics. A Turing test conducted by 4 experts has
shown that the generated scans can trick almost any expert. The generated scans
were also used to evaluate their impact on the performance of an existing deep
learning model developed for segmenting the rat brain into white matter, grey
matter and cerebrospinal fluid. The models were compared using the Dice score.
The best results for whole brain and white matter segmentation were obtained
when 174 real scans and 348 synthetic scans were used, with improvements of
0.0172 and 0.0129, respectively. Using 174 real scans and 87 synthetic scans
resulted in improvements of 0.0038 and 0.0764 for grey matter and CSF
segmentation, respectively. Thus, by using the proposed new normalisation layer
and loss functions, it was possible to improve the realism of the generated rat
MRI scans and it was shown that using the generated data improved the
segmentation model more than using the conventional data augmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Associative Adversarial Learning Based on Selective Attack. (arXiv:2112.13989v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.13989">
<div class="article-summary-box-inner">
<span><p>A human's attention can intuitively adapt to corrupted areas of an image by
recalling a similar uncorrupted image they have previously seen. This
observation motivates us to improve the attention of adversarial images by
considering their clean counterparts. To accomplish this, we introduce
Associative Adversarial Learning (AAL) into adversarial learning to guide a
selective attack. We formulate the intrinsic relationship between attention and
attack (perturbation) as a coupling optimization problem to improve their
interaction. This leads to an attention backtracking algorithm that can
effectively enhance the attention's adversarial robustness. Our method is
generic and can be used to address a variety of tasks by simply choosing
different kernels for the associative attention that select other regions for a
specific attack. Experimental results show that the selective attack improves
the model's performance. We show that our method improves the recognition
accuracy of adversarial training on ImageNet by 8.32% compared with the
baseline. It also increases object detection mAP on PascalVOC by 2.02% and
recognition accuracy of few-shot learning on miniImageNet by 1.63%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Calibrated Hyperspectral Image Reconstruction via Graph-based Self-Tuning Network. (arXiv:2112.15362v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15362">
<div class="article-summary-box-inner">
<span><p>Recently, hyperspectral imaging (HSI) has attracted increasing research
attention, especially for the ones based on a coded aperture snapshot spectral
imaging (CASSI) system. Existing deep HSI reconstruction models are generally
trained on paired data to retrieve original signals upon 2D compressed
measurements given by a particular optical hardware mask in CASSI, during which
the mask largely impacts the reconstruction performance and could work as a
"model hyperparameter" governing on data augmentations. This mask-specific
training style will lead to a hardware miscalibration issue, which sets up
barriers to deploying deep HSI models among different hardware and noisy
environments. To address this challenge, we introduce mask uncertainty for HSI
with a complete variational Bayesian learning treatment and explicitly model it
through a mask decomposition inspired by real hardware. Specifically, we
propose a novel Graph-based Self-Tuning (GST) network to reason uncertainties
adapting to varying spatial structures of masks among different hardware.
Moreover, we develop a bilevel optimization framework to balance HSI
reconstruction and uncertainty estimation, accounting for the hyperparameter
property of masks. Extensive experimental results and model discussions
validate the effectiveness (over 33/30 dB) of the proposed GST method under two
miscalibration scenarios and demonstrate a highly competitive performance
compared with the state-of-the-art well-calibrated methods. Our code and
pre-trained model are available at
https://github.com/Jiamian-Wang/mask_uncertainty_spectral_SCI
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Cross-dataset Generalization in License Plate Recognition. (arXiv:2201.00267v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00267">
<div class="article-summary-box-inner">
<span><p>Automatic License Plate Recognition (ALPR) systems have shown remarkable
performance on license plates (LPs) from multiple regions due to advances in
deep learning and the increasing availability of datasets. The evaluation of
deep ALPR systems is usually done within each dataset; therefore, it is
questionable if such results are a reliable indicator of generalization
ability. In this paper, we propose a traditional-split versus
leave-one-dataset-out experimental setup to empirically assess the
cross-dataset generalization of 12 Optical Character Recognition (OCR) models
applied to LP recognition on nine publicly available datasets with a great
variety in several aspects (e.g., acquisition settings, image resolution, and
LP layouts). We also introduce a public dataset for end-to-end ALPR that is the
first to contain images of vehicles with Mercosur LPs and the one with the
highest number of motorcycle images. The experimental results shed light on the
limitations of the traditional-split protocol for evaluating approaches in the
ALPR context, as there are significant drops in performance for most datasets
when training and testing the models in a leave-one-dataset-out fashion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast and High-Quality Image Denoising via Malleable Convolutions. (arXiv:2201.00392v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00392">
<div class="article-summary-box-inner">
<span><p>Many image processing networks apply a single set of static convolutional
kernels across the entire input image, which is sub-optimal for natural images,
as they often consist of heterogeneous visual patterns. Recent works in
classification, segmentation, and image restoration have demonstrated that
dynamic kernels outperform static kernels at modeling local image statistics.
However, these works often adopt per-pixel convolution kernels, which introduce
high memory and computation costs. To achieve spatial-varying processing
without significant overhead, we present Malleable Convolution (MalleConv), as
an efficient variant of dynamic convolution. The weights of MalleConv are
dynamically produced by an efficient predictor network capable of generating
content-dependent outputs at specific spatial locations. Unlike previous works,
MalleConv generates a much smaller set of spatially-varying kernels from input,
which enlarges the network's receptive field and significantly reduces
computational and memory costs. These kernels are then applied to a
full-resolution feature map through an efficient slice-and-conv operator with
minimum memory overhead. We further build an efficient denoising network using
MalleConv, coined as MalleNet. It achieves high quality results without very
deep architecture, e.g., reaching 8.91x faster speed compared to the best
performed denoising algorithms (SwinIR), while maintaining similar performance.
We also show that a single MalleConv added to a standard convolution-based
backbone can contribute significantly to reducing the computational cost or
boosting image quality at a similar cost. Project page:
https://yifanjiang.net/MalleConv.html
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Open World Object Detection. (arXiv:2201.00471v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.00471">
<div class="article-summary-box-inner">
<span><p>Open World Object Detection (OWOD), simulating the real dynamic world where
knowledge grows continuously, attempts to detect both known and unknown classes
and incrementally learn the identified unknown ones. We find that although the
only previous OWOD work constructively puts forward to the OWOD definition, the
experimental settings are unreasonable with the illogical benchmark, confusing
metric calculation, and inappropriate method. In this paper, we rethink the
OWOD experimental setting and propose five fundamental benchmark principles to
guide the OWOD benchmark construction. Moreover, we design two fair evaluation
protocols specific to the OWOD problem, filling the void of evaluating from the
perspective of unknown classes. Furthermore, we introduce a novel and effective
OWOD framework containing an auxiliary Proposal ADvisor (PAD) and a
Class-specific Expelling Classifier (CEC). The non-parametric PAD could assist
the RPN in identifying accurate unknown proposals without supervision, while
CEC calibrates the over-confident activation boundary and filters out confusing
predictions through a class-specific expelling function. Comprehensive
experiments conducted on our fair benchmark demonstrate that our method
outperforms other state-of-the-art object detection approaches in terms of both
existing and our new metrics. Our benchmark and code are available at
https://github.com/RE-OWOD/RE-OWOD.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-01-05 23:07:17.237048670 UTC">2022-01-05 23:07:17 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>