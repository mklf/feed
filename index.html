<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-03-24T01:30:00Z">03-24</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Text Transformations in Contrastive Self-Supervised Learning: A Review. (arXiv:2203.12000v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12000">
<div class="article-summary-box-inner">
<span><p>Contrastive self-supervised learning has become a prominent technique in
representation learning. The main step in these methods is to contrast
semantically similar and dissimilar pairs of samples. However, in the domain of
Natural Language, the augmentation methods used in creating similar pairs with
regard to contrastive learning assumptions are challenging. This is because,
even simply modifying a word in the input might change the semantic meaning of
the sentence, and hence, would violate the distributional hypothesis. In this
review paper, we formalize the contrastive learning framework in the domain of
natural language processing. We emphasize the considerations that need to be
addressed in the data transformation step and review the state-of-the-art
methods and evaluations for contrastive representation learning in NLP.
Finally, we describe some challenges and potential directions for learning
better text representations using contrastive methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Building Robust Spoken Language Understanding by Cross Attention between Phoneme Sequence and ASR Hypothesis. (arXiv:2203.12067v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12067">
<div class="article-summary-box-inner">
<span><p>Building Spoken Language Understanding (SLU) robust to Automatic Speech
Recognition (ASR) errors is an essential issue for various voice-enabled
virtual assistants. Considering that most ASR errors are caused by phonetic
confusion between similar-sounding expressions, intuitively, leveraging the
phoneme sequence of speech can complement ASR hypothesis and enhance the
robustness of SLU. This paper proposes a novel model with Cross Attention for
SLU (denoted as CASLU). The cross attention block is devised to catch the
fine-grained interactions between phoneme and word embeddings in order to make
the joint representations catch the phonetic and semantic features of input
simultaneously and for overcoming the ASR errors in downstream natural language
understanding (NLU) tasks. Extensive experiments are conducted on three
datasets, showing the effectiveness and competitiveness of our approach.
Additionally, We also validate the universality of CASLU and prove its
complementarity when combining with other robust SLU techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Study on Learning and Improving the Search Objective for Unsupervised Paraphrasing. (arXiv:2203.12106v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12106">
<div class="article-summary-box-inner">
<span><p>Research in unsupervised text generation has been gaining attention over the
years. One recent approach is local search towards a heuristically defined
objective, which specifies language fluency, semantic meanings, and other
task-specific attributes. Search in the sentence space is realized by
word-level edit operations including insertion, replacement, and deletion.
However, such objective function is manually designed with multiple components.
Although previous work has shown maximizing this objective yields good
performance in terms of true measure of success (i.e. BLEU and iBLEU), the
objective landscape is considered to be non-smooth with significant noises,
posing challenges for optimization. In this dissertation, we address the
research problem of smoothing the noise in the heuristic search objective by
learning to model the search dynamics. Then, the learned model is combined with
the original objective function to guide the search in a bootstrapping fashion.
Experimental results show that the learned models combined with the original
search objective can indeed provide a smoothing effect, improving the search
performance by a small margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ALT: um software para an\'alise de legibilidade de textos em L\'ingua Portuguesa. (arXiv:2203.12135v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12135">
<div class="article-summary-box-inner">
<span><p>In the initial stage of human life, communication, seen as a process of
social interaction, was always the best way to reach consensus between the
parties. Understanding and credibility in this process are essential for the
mutual agreement to be validated. But, how to do it so that this communication
reaches the great mass? This is the main challenge when what is sought is the
dissemination of information and its approval. In this context, this study
presents the ALT software, developed from original readability metrics adapted
to the Portuguese language, available on the web, to reduce communication
difficulties. The development of the software was motivated by the theory of
communicative action of Habermas, which uses a multidisciplinary style to
measure the credibility of the discourse in the communication channels used to
build and maintain a safe and healthy relationship with the public.
</p>
<p>--
</p>
<p>No est\'agio inicial da vida humana a comunica\c{c}\~ao, vista como um
processo de intera\c{c}\~ao social, foi sempre o melhor caminho para o consenso
entre as partes. O entendimento e a credibilidade nesse processo s\~ao
fundamentais para que o acordo m\'utuo seja validado. Mas, como faz\^e-lo de
forma que essa comunica\c{c}\~ao alcance a grande massa? Esse \'e o principal
desafio quando o que se busca \'e a difus\~ao da informa\c{c}\~ao e a sua
aprova\c{c}\~ao. Nesse contexto, este estudo apresenta o software ALT,
desenvolvido a partir de m\'etricas de legibilidade originais adaptadas para a
L\'ingua Portuguesa, dispon\'ivel na web, para reduzir as dificuldades na
comunica\c{c}\~ao. O desenvolvimento do software foi motivado pela teoria do
agir comunicativo de Habermas, que faz uso de um estilo multidisciplinar para
medir a credibilidade do discurso nos canais de comunica\c{c}\~ao utilizados
para construir e manter uma rela\c{c}\~ao segura e saud\'avel com o p\'ublico.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Study of Memorization in NLP. (arXiv:2203.12171v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12171">
<div class="article-summary-box-inner">
<span><p>A recent study by Feldman (2020) proposed a long-tail theory to explain the
memorization behavior of deep learning models. However, memorization has not
been empirically verified in the context of NLP, a gap addressed by this work.
In this paper, we use three different NLP tasks to check if the long-tail
theory holds. Our experiments demonstrate that top-ranked memorized training
instances are likely atypical, and removing the top-memorized training
instances leads to a more serious drop in test accuracy compared with removing
training instances randomly. Furthermore, we develop an attribution method to
better understand why a training instance is memorized. We empirically show
that our memorization attribution method is faithful, and share our interesting
finding that the top-memorized parts of a training instance tend to be features
negatively correlated with the class label.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Theoretically Grounded Benchmark for Evaluating Machine Commonsense. (arXiv:2203.12184v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12184">
<div class="article-summary-box-inner">
<span><p>Programming machines with commonsense reasoning (CSR) abilities is a
longstanding challenge in the Artificial Intelligence community. Current CSR
benchmarks use multiple-choice (and in relatively fewer cases, generative)
question-answering instances to evaluate machine commonsense. Recent progress
in transformer-based language representation models suggest that considerable
progress has been made on existing benchmarks. However, although tens of CSR
benchmarks currently exist, and are growing, it is not evident that the full
suite of commonsense capabilities have been systematically evaluated.
Furthermore, there are doubts about whether language models are 'fitting' to a
benchmark dataset's training partition by picking up on subtle, but normatively
irrelevant (at least for CSR), statistical features to achieve good performance
on the testing partition. To address these challenges, we propose a benchmark
called Theoretically-Grounded Commonsense Reasoning (TG-CSR) that is also based
on discriminative question answering, but with questions designed to evaluate
diverse aspects of commonsense, such as space, time, and world states. TG-CSR
is based on a subset of commonsense categories first proposed as a viable
theory of commonsense by Gordon and Hobbs. The benchmark is also designed to be
few-shot (and in the future, zero-shot), with only a few training and
validation examples provided. This report discusses the structure and
construction of the benchmark. Preliminary results suggest that the benchmark
is challenging even for advanced language representation models designed for
discriminative CSR question answering tasks.
</p>
<p>Benchmark access and leaderboard:
https://codalab.lisn.upsaclay.fr/competitions/3080 Benchmark website:
https://usc-isi-i2.github.io/TGCSR/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AbductionRules: Training Transformers to Explain Unexpected Inputs. (arXiv:2203.12186v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12186">
<div class="article-summary-box-inner">
<span><p>Transformers have recently been shown to be capable of reliably performing
logical reasoning over facts and rules expressed in natural language, but
abductive reasoning - inference to the best explanation of an unexpected
observation - has been underexplored despite significant applications to
scientific discovery, common-sense reasoning, and model interpretability.
</p>
<p>We present AbductionRules, a group of natural language datasets designed to
train and test generalisable abduction over natural-language knowledge bases.
We use these datasets to finetune pretrained Transformers and discuss their
performance, finding that our models learned generalisable abductive techniques
but also learned to exploit the structure of our data. Finally, we discuss the
viability of this approach to abductive reasoning and ways in which it may be
improved in future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Converse -- A Tree-Based Modular Task-Oriented Dialogue System. (arXiv:2203.12187v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12187">
<div class="article-summary-box-inner">
<span><p>Creating a system that can have meaningful conversations with humans to help
accomplish tasks is one of the ultimate goals of Artificial Intelligence (AI).
It has defined the meaning of AI since the beginning. A lot has been
accomplished in this area recently, with voice assistant products entering our
daily lives and chat bot systems becoming commonplace in customer service. At
first glance there seems to be no shortage of options for dialogue systems.
However, the frequently deployed dialogue systems today seem to all struggle
with a critical weakness - they are hard to build and harder to maintain. At
the core of the struggle is the need to script every single turn of
interactions between the bot and the human user. This makes the dialogue
systems more difficult to maintain as the tasks become more complex and more
tasks are added to the system. In this paper, we propose Converse, a flexible
tree-based modular task-oriented dialogue system. Converse uses an and-or tree
structure to represent tasks and offers powerful multi-task dialogue
management. Converse supports task dependency and task switching, which are
unique features compared to other open-source dialogue frameworks. At the same
time, Converse aims to make the bot building process easy and simple, for both
professional and non-professional software developers. The code is available at
https://github.com/salesforce/Converse.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Expressive Speaking Style Modelling with Hierarchical Context Information for Mandarin Speech Synthesis. (arXiv:2203.12201v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12201">
<div class="article-summary-box-inner">
<span><p>Previous works on expressive speech synthesis mainly focus on current
sentence. The context in adjacent sentences is neglected, resulting in
inflexible speaking style for the same text, which lacks speech variations. In
this paper, we propose a hierarchical framework to model speaking style from
context. A hierarchical context encoder is proposed to explore a wider range of
contextual information considering structural relationship in context,
including inter-phrase and inter-sentence relations. Moreover, to encourage
this encoder to learn style representation better, we introduce a novel
training strategy with knowledge distillation, which provides the target for
encoder training. Both objective and subjective evaluations on a Mandarin
lecture dataset demonstrate that the proposed method can significantly improve
the naturalness and expressiveness of the synthesized speech.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Integrating Vectorized Lexical Constraints for Neural Machine Translation. (arXiv:2203.12210v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12210">
<div class="article-summary-box-inner">
<span><p>Lexically constrained neural machine translation (NMT), which controls the
generation of NMT models with pre-specified constraints, is important in many
practical scenarios. Due to the representation gap between discrete constraints
and continuous vectors in NMT models, most existing works choose to construct
synthetic data or modify the decoding algorithm to impose lexical constraints,
treating the NMT model as a black box. In this work, we propose to open this
black box by directly integrating the constraints into NMT models.
Specifically, we vectorize source and target constraints into continuous keys
and values, which can be utilized by the attention modules of NMT models. The
proposed integration method is based on the assumption that the correspondence
between keys and values in attention modules is naturally suitable for modeling
constraint pairs. Experimental results show that our method consistently
outperforms several representative baselines on four language pairs,
demonstrating the superiority of integrating vectorized lexical constraints.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Geometry-Aware Supertagging with Heterogeneous Dynamic Convolutions. (arXiv:2203.12235v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12235">
<div class="article-summary-box-inner">
<span><p>The syntactic categories of categorial grammar formalisms are structured
units made of smaller, indivisible primitives, bound together by the underlying
grammar's category formation rules. In the trending approach of constructive
supertagging, neural models are increasingly made aware of the internal
category structure, which in turn enables them to more reliably predict rare
and out-of-vocabulary categories, with significant implications for grammars
previously deemed too complex to find practical use. In this work, we revisit
constructive supertagging from a graph-theoretic perspective, and propose a
framework based on heterogeneous dynamic graph convolutions aimed at exploiting
the distinctive structure of a supertagger's output space. We test our approach
on a number of categorial grammar datasets spanning different languages and
grammar formalisms, achieving substantial improvements over previous state of
the art scores. Code will be made available at
https://github.com/konstantinosKokos/dynamic-graph-supertagging
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Named Entity Recognition with Self-describing Networks. (arXiv:2203.12252v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12252">
<div class="article-summary-box-inner">
<span><p>Few-shot NER needs to effectively capture information from limited instances
and transfer useful knowledge from external resources. In this paper, we
propose a self-describing mechanism for few-shot NER, which can effectively
leverage illustrative instances and precisely transfer knowledge from external
resources by describing both entity types and mentions using a universal
concept set. Specifically, we design Self-describing Networks (SDNet), a
Seq2Seq generation model which can universally describe mentions using
concepts, automatically map novel entity types to concepts, and adaptively
recognize entities on-demand. We pre-train SDNet with large-scale corpus, and
conduct experiments on 8 benchmarks from different domains. Experiments show
that SDNet achieves competitive performances on all benchmarks and achieves the
new state-of-the-art on 6 benchmarks, which demonstrates its effectiveness and
robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chat-Capsule: A Hierarchical Capsule for Dialog-level Emotion Analysis. (arXiv:2203.12254v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12254">
<div class="article-summary-box-inner">
<span><p>Many studies on dialog emotion analysis focus on utterance-level emotion
only. These models hence are not optimized for dialog-level emotion detection,
i.e. to predict the emotion category of a dialog as a whole. More importantly,
these models cannot benefit from the context provided by the whole dialog. In
real-world applications, annotations to dialog could fine-grained, including
both utterance-level tags (e.g. speaker type, intent category, and emotion
category), and dialog-level tags (e.g. user satisfaction, and emotion curve
category). In this paper, we propose a Context-based Hierarchical Attention
Capsule~(Chat-Capsule) model, which models both utterance-level and
dialog-level emotions and their interrelations. On a dialog dataset collected
from customer support of an e-commerce platform, our model is also able to
predict user satisfaction and emotion curve category. Emotion curve refers to
the change of emotions along the development of a conversation. Experiments
show that the proposed Chat-Capsule outperform state-of-the-art baselines on
both benchmark dataset and proprietary dataset. Source code will be released
upon acceptance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IAM: A Comprehensive and Large-Scale Dataset for Integrated Argument Mining Tasks. (arXiv:2203.12257v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12257">
<div class="article-summary-box-inner">
<span><p>Traditionally, a debate usually requires a manual preparation process,
including reading plenty of articles, selecting the claims, identifying the
stances of the claims, seeking the evidence for the claims, etc. As the AI
debate attracts more attention these years, it is worth exploring the methods
to automate the tedious process involved in the debating system. In this work,
we introduce a comprehensive and large dataset named IAM, which can be applied
to a series of argument mining tasks, including claim extraction, stance
classification, evidence extraction, etc. Our dataset is collected from over 1k
articles related to 123 topics. Near 70k sentences in the dataset are fully
annotated based on their argument properties (e.g., claims, stances, evidence,
etc.). We further propose two new integrated argument mining tasks associated
with the debate preparation process: (1) claim extraction with stance
classification (CESC) and (2) claim-evidence pair extraction (CEPE). We adopt a
pipeline approach and an end-to-end method for each integrated task separately.
Promising experimental results are reported to show the values and challenges
of our proposed tasks, and motivate future research on argument mining.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Prompt Probe Pretrained Language Models? Understanding the Invisible Risks from a Causal View. (arXiv:2203.12258v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12258">
<div class="article-summary-box-inner">
<span><p>Prompt-based probing has been widely used in evaluating the abilities of
pretrained language models (PLMs). Unfortunately, recent studies have
discovered such an evaluation may be inaccurate, inconsistent and unreliable.
Furthermore, the lack of understanding its inner workings, combined with its
wide applicability, has the potential to lead to unforeseen risks for
evaluating and applying PLMs in real-world applications. To discover,
understand and quantify the risks, this paper investigates the prompt-based
probing from a causal view, highlights three critical biases which could induce
biased results and conclusions, and proposes to conduct debiasing via causal
intervention. This paper provides valuable insights for the design of unbiased
datasets, better probing frameworks and more reliable evaluations of pretrained
language models. Furthermore, our conclusions also echo that we need to rethink
the criteria for identifying better pretrained language models. We openly
released the source code and data at https://github.com/c-box/causalEval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ECO v1: Towards Event-Centric Opinion Mining. (arXiv:2203.12264v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12264">
<div class="article-summary-box-inner">
<span><p>Events are considered as the fundamental building blocks of the world. Mining
event-centric opinions can benefit decision making, people communication, and
social good. Unfortunately, there is little literature addressing event-centric
opinion mining, although which significantly diverges from the well-studied
entity-centric opinion mining in connotation, structure, and expression. In
this paper, we propose and formulate the task of event-centric opinion mining
based on event-argument structure and expression categorizing theory. We also
benchmark this task by constructing a pioneer corpus and designing a two-step
benchmark framework. Experiment results show that event-centric opinion mining
is feasible and challenging, and the proposed task, dataset, and baselines are
beneficial for future studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pre-training to Match for Unified Low-shot Relation Extraction. (arXiv:2203.12274v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12274">
<div class="article-summary-box-inner">
<span><p>Low-shot relation extraction~(RE) aims to recognize novel relations with very
few or even no samples, which is critical in real scenario application.
Few-shot and zero-shot RE are two representative low-shot RE tasks, which seem
to be with similar target but require totally different underlying abilities.
In this paper, we propose Multi-Choice Matching Networks to unify low-shot
relation extraction. To fill in the gap between zero-shot and few-shot RE, we
propose the triplet-paraphrase meta-training, which leverages triplet
paraphrase to pre-train zero-shot label matching ability and uses meta-learning
paradigm to learn few-shot instance summarizing ability. Experimental results
on three different low-shot RE tasks show that the proposed method outperforms
strong baselines by a large margin, and achieve the best performance on
few-shot RE leaderboard.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ERNIE-SPARSE: Learning Hierarchical Efficient Transformer Through Regularized Self-Attention. (arXiv:2203.12276v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12276">
<div class="article-summary-box-inner">
<span><p>Sparse Transformer has recently attracted a lot of attention since the
ability for reducing the quadratic dependency on the sequence length. We argue
that two factors, information bottleneck sensitivity and inconsistency between
different attention topologies, could affect the performance of the Sparse
Transformer. This paper proposes a well-designed model named ERNIE-Sparse. It
consists of two distinctive parts: (i) Hierarchical Sparse Transformer (HST) to
sequentially unify local and global information. (ii) Self-Attention
Regularization (SAR) method, a novel regularization designed to minimize the
distance for transformers with different attention topologies. To evaluate the
effectiveness of ERNIE-Sparse, we perform extensive evaluations. Firstly, we
perform experiments on a multi-modal long sequence modeling task benchmark,
Long Range Arena (LRA). Experimental results demonstrate that ERNIE-Sparse
significantly outperforms a variety of strong baseline methods including the
dense attention and other efficient sparse attention methods and achieves
improvements by 2.77% (57.78% vs. 55.01%). Secondly, to further show the
effectiveness of our method, we pretrain ERNIE-Sparse and verified it on 3 text
classification and 2 QA downstream tasks, achieve improvements on
classification benchmark by 0.83% (92.46% vs. 91.63%), on QA benchmark by 3.24%
(74.67% vs. 71.43%). Experimental results continue to demonstrate its superior
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified Structure Generation for Universal Information Extraction. (arXiv:2203.12277v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12277">
<div class="article-summary-box-inner">
<span><p>Information extraction suffers from its varying targets, heterogeneous
structures, and demand-specific schemas. In this paper, we propose a unified
text-to-structure generation framework, namely UIE, which can universally model
different IE tasks, adaptively generate targeted structures, and
collaboratively learn general IE abilities from different knowledge sources.
Specifically, UIE uniformly encodes different extraction structures via a
structured extraction language, adaptively generates target extractions via a
schema-based prompt mechanism - structural schema instructor, and captures the
common IE abilities via a large-scale pre-trained text-to-structure model.
Experiments show that UIE achieved the state-of-the-art performance on 4 IE
tasks, 13 datasets, and on all supervised, low-resource, and few-shot settings
for a wide range of entity, relation, event and sentiment extraction tasks and
their unification. These results verified the effectiveness, universality, and
transferability of UIE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Input-specific Attention Subnetworks for Adversarial Detection. (arXiv:2203.12298v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12298">
<div class="article-summary-box-inner">
<span><p>Self-attention heads are characteristic of Transformer models and have been
well studied for interpretability and pruning. In this work, we demonstrate an
altogether different utility of attention heads, namely for adversarial
detection. Specifically, we propose a method to construct input-specific
attention subnetworks (IAS) from which we extract three features to
discriminate between authentic and adversarial inputs. The resultant detector
significantly improves (by over 7.5%) the state-of-the-art adversarial
detection accuracy for the BERT encoder on 10 NLU datasets with 11 different
adversarial attack types. We also demonstrate that our method (a) is more
accurate for larger models which are likely to have more spurious correlations
and thus vulnerable to adversarial attack, and (b) performs well even with
modest training sets of adversarial examples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Framework for Fast Polarity Labelling of Massive Data Streams. (arXiv:2203.12368v1 [cs.DB])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12368">
<div class="article-summary-box-inner">
<span><p>Many of the existing sentiment analysis techniques are based on supervised
learning, and they demand the availability of valuable training datasets to
train their models. When dataset freshness is critical, the annotating of high
speed unlabelled data streams becomes critical but remains an open problem. In
this paper, we propose PLStream, a novel Apache Flink-based framework for fast
polarity labelling of massive data streams, like Twitter tweets or online
product reviews. We address the associated implementation challenges and
propose a list of techniques including both algorithmic improvements and system
optimizations. A thorough empirical validation with two real-world workloads
demonstrates that PLStream is able to generate high quality labels (almost 80%
accuracy) in the presence of high-speed continuous unlabelled data streams
(almost 16,000 tuples/sec) without any manual efforts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey On Semantic Steganography Systems. (arXiv:2203.12425v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12425">
<div class="article-summary-box-inner">
<span><p>Steganography is the practice of concealing a message within some other
carrier or cover message. It is used to allow the sending of hidden information
through communication channels where third parties would only be aware of the
explicit information in the carrier message. With the growth of internet
surveillance and the increased need for secret communication, steganography
systems continue to find new applications. In semantic steganography, the
redundancies in the semantics of a language are used to send a text
steganographic message. In this article we go over the concepts behind semantic
steganography and propose a hierarchy for classifying systems within the
context of text steganography and steganography in general. After laying this
groundwork we list systems for semantic steganography that have been published
in the past and review their properties. Finally, we comment on and briefly
compare the described systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The VoicePrivacy 2022 Challenge Evaluation Plan. (arXiv:2203.12468v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12468">
<div class="article-summary-box-inner">
<span><p>For new participants - Executive summary: (1) The task is to develop a voice
anonymization system for speech data which conceals the speaker's voice
identity while protecting linguistic content, paralinguistic attributes,
intelligibility and naturalness. (2) Training, development and evaluation
datasets are provided in addition to 3 different baseline anonymization
systems, evaluation scripts, and metrics. Participants apply their developed
anonymization systems, run evaluation scripts and submit objective evaluation
results and anonymized speech data to the organizers. (3) Results will be
presented at a workshop held in conjunction with INTERSPEECH 2022 to which all
participants are invited to present their challenge systems and to submit
additional workshop papers.
</p>
<p>For readers familiar with the VoicePrivacy Challenge - Changes w.r.t. 2020:
(1) A stronger, semi-informed attack model in the form of an automatic speaker
verification (ASV) system trained on anonymized (per-utterance) speech data.
(2) Complementary metrics comprising the equal error rate (EER) as a privacy
metric, the word error rate (WER) as a primary utility metric, and the pitch
correlation and gain of voice distinctiveness as secondary utility metrics. (3)
A new ranking policy based upon a set of minimum target privacy requirements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt-based Pre-trained Model for Personality and Interpersonal Reactivity Prediction. (arXiv:2203.12481v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12481">
<div class="article-summary-box-inner">
<span><p>This paper describes the LingJing team's method to the Workshop on
Computational Approaches to Subjectivity, Sentiment &amp; Social Media Analysis
(WASSA) 2022 shared task on Personality Prediction (PER) and Reactivity Index
Prediction (IRI). In this paper, we adopt the prompt-based method with the
pre-trained language model to accomplish these tasks. Specifically, the prompt
is designed to provide the extra knowledge for enhancing the pre-trained model.
Data augmentation and model ensemble are adopted for obtaining better results.
Extensive experiments are performed, which shows the effectiveness of the
proposed method. On the final submission, our system achieves a Pearson
Correlation Coefficient of 0.2301 and 0.2546 on Track 3 and Track 4
respectively. We ranked Top-1 on both sub-tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Context-Aware Feature Fusion Framework for Punctuation Restoration. (arXiv:2203.12487v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12487">
<div class="article-summary-box-inner">
<span><p>To accomplish the punctuation restoration task, most existing approaches
focused on leveraging extra information (e.g., part-of-speech tags) or
addressing the class imbalance problem. Recent works have widely applied the
transformer-based language models and significantly improved their
effectiveness. To the best of our knowledge, an inherent issue has remained
neglected: the attention of individual heads in the transformer will be diluted
or powerless while feeding the long non-punctuation utterances. Since those
previous contexts, not the followings, are comparatively more valuable to the
current position, it's hard to achieve a good balance by independent attention.
In this paper, we propose a novel Feature Fusion framework based on two-type
Attentions (FFA) to alleviate the shortage. It introduces a two-stream
architecture. One module involves interaction between attention heads to
encourage the communication, and another masked attention module captures the
dependent feature representation. Then, it aggregates two feature embeddings to
fuse information and enhances context-awareness. The experiments on the popular
benchmark dataset IWSLT demonstrate that our approach is effective. Without
additional data, it obtains comparable performance to the current
state-of-the-art models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Cross-Lingual Summarization. (arXiv:2203.12515v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12515">
<div class="article-summary-box-inner">
<span><p>Cross-lingual summarization is the task of generating a summary in one
language (e.g., English) for the given document(s) in a different language
(e.g., Chinese). Under the globalization background, this task has attracted
increasing attention of the computational linguistics community. Nevertheless,
there still remains a lack of comprehensive review for this task. Therefore, we
present the first systematic critical review on the datasets, approaches and
challenges in this field. Specifically, we carefully organize existing datasets
and approaches according to different construction methods and solution
paradigms, respectively. For each type of datasets or approaches, we thoroughly
introduce and summarize previous efforts and further compare them with each
other to provide deeper analyses. In the end, we also discuss promising
directions and offer our thoughts to facilitate future research. This survey is
for both beginners and experts in cross-lingual summarization, and we hope it
will serve as a starting point as well as a source of new ideas for researchers
and engineers interested in this area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Computational historical linguistics and language diversity in South Asia. (arXiv:2203.12524v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12524">
<div class="article-summary-box-inner">
<span><p>South Asia is home to a plethora of languages, many of which severely lack
access to new language technologies. This linguistic diversity also results in
a research environment conducive to the study of comparative, contact, and
historical linguistics -- fields which necessitate the gathering of extensive
data from many languages. We claim that data scatteredness (rather than
scarcity) is the primary obstacle in the development of South Asian language
technology, and suggest that the study of language history is uniquely aligned
with surmounting this obstacle. We review recent developments in and at the
intersection of South Asian NLP and historical-comparative linguistics,
describing our and others' current efforts in this area. We also offer new
strategies towards breaking the data barrier.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamically Refined Regularization for Improving Cross-corpora Hate Speech Detection. (arXiv:2203.12536v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12536">
<div class="article-summary-box-inner">
<span><p>Hate speech classifiers exhibit substantial performance degradation when
evaluated on datasets different from the source. This is due to learning
spurious correlations between words that are not necessarily relevant to
hateful language, and hate speech labels from the training corpus. Previous
work has attempted to mitigate this problem by regularizing specific terms from
pre-defined static dictionaries. While this has been demonstrated to improve
the generalizability of classifiers, the coverage of such methods is limited
and the dictionaries require regular manual updates from human experts. In this
paper, we propose to automatically identify and reduce spurious correlations
using attribution methods with dynamic refinement of the list of terms that
need to be regularized during training. Our approach is flexible and improves
the cross-corpora performance over previous work independently and in
combination with pre-defined dictionaries.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mitigating Gender Bias in Distilled Language Models via Counterfactual Role Reversal. (arXiv:2203.12574v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12574">
<div class="article-summary-box-inner">
<span><p>Language models excel at generating coherent text, and model compression
techniques such as knowledge distillation have enabled their use in
resource-constrained settings. However, these models can be biased in multiple
ways, including the unfounded association of male and female genders with
gender-neutral professions. Therefore, knowledge distillation without any
fairness constraints may preserve or exaggerate the teacher model's biases onto
the distilled model. To this end, we present a novel approach to mitigate
gender disparity in text generation by learning a fair model during knowledge
distillation. We propose two modifications to the base knowledge distillation
based on counterfactual role reversal$\unicode{x2014}$modifying teacher
probabilities and augmenting the training set. We evaluate gender polarity
across professions in open-ended text generated from the resulting distilled
and finetuned GPT$\unicode{x2012}$2 models and demonstrate a substantial
reduction in gender disparity with only a minor compromise in utility. Finally,
we observe that language models that reduce gender polarity in language
generation do not improve embedding fairness or downstream classification
fairness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tokenization Repair in the Presence of Spelling Errors. (arXiv:2010.07878v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.07878">
<div class="article-summary-box-inner">
<span><p>We consider the following tokenization repair problem: Given a natural
language text with any combination of missing or spurious spaces, correct
these. Spelling errors can be present, but it's not part of the problem to
correct them. For example, given: "Tispa per isabout token izaionrep air",
compute "Tis paper is about tokenizaion repair". We identify three key
ingredients of high-quality tokenization repair, all missing from previous
work: deep language models with a bidirectional component, training the models
on text with spelling errors, and making use of the space information already
present. Our methods also improve existing spell checkers by fixing not only
more tokenization errors but also more spelling errors: once it is clear which
characters form a word, it is much easier for them to figure out the correct
word. We provide six benchmarks that cover three use cases (OCR errors, text
extraction from PDF, human errors) and the cases of partially correct space
information and all spaces missing. We evaluate our methods against the best
existing methods and a non-trivial baseline. We provide full reproducibility
under https://ad.cs.uni-freiburg.de/publications .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta-Learning for Fast Cross-Lingual Adaptation in Dependency Parsing. (arXiv:2104.04736v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.04736">
<div class="article-summary-box-inner">
<span><p>Meta-learning, or learning to learn, is a technique that can help to overcome
resource scarcity in cross-lingual NLP problems, by enabling fast adaptation to
new tasks. We apply model-agnostic meta-learning (MAML) to the task of
cross-lingual dependency parsing. We train our model on a diverse set of
languages to learn a parameter initialization that can adapt quickly to new
languages. We find that meta-learning with pre-training can significantly
improve upon the performance of language transfer and standard supervised
learning baselines for a variety of unseen, typologically diverse, and
low-resource languages, in a few-shot learning setup.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Packed Levitated Marker for Entity and Relation Extraction. (arXiv:2109.06067v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06067">
<div class="article-summary-box-inner">
<span><p>Recent entity and relation extraction works focus on investigating how to
obtain a better span representation from the pre-trained encoder. However, a
major limitation of existing works is that they ignore the interrelation
between spans (pairs). In this work, we propose a novel span representation
approach, named Packed Levitated Markers (PL-Marker), to consider the
interrelation between the spans (pairs) by strategically packing the markers in
the encoder. In particular, we propose a neighborhood-oriented packing
strategy, which considers the neighbor spans integrally to better model the
entity boundary information. Furthermore, for those more complicated span pair
classification tasks, we design a subject-oriented packing strategy, which
packs each subject and all its objects to model the interrelation between the
same-subject span pairs. The experimental results show that, with the enhanced
marker feature, our model advances baselines on six NER benchmarks, and obtains
a 4.1%-4.3% strict relation F1 improvement with higher speed over previous
state-of-the-art models on ACE04 and ACE05.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical prosody modeling and control in non-autoregressive parallel neural TTS. (arXiv:2110.02952v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02952">
<div class="article-summary-box-inner">
<span><p>Neural text-to-speech (TTS) synthesis can generate speech that is
indistinguishable from natural speech. However, the synthetic speech often
represents the average prosodic style of the database instead of having more
versatile prosodic variation. Moreover, many models lack the ability to control
the output prosody, which does not allow for different styles for the same text
input. In this work, we train a non-autoregressive parallel neural TTS
front-end model hierarchically conditioned on both coarse and fine-grained
acoustic speech features to learn a latent prosody space with intuitive and
meaningful dimensions. Experiments show that a non-autoregressive TTS model
hierarchically conditioned on utterance-wise pitch, pitch range, duration,
energy, and spectral tilt can effectively control each prosodic dimension,
generate a wide variety of speaking styles, and provide word-wise emphasis
control, while maintaining equal or better quality to the baseline model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topic Model Supervised by Understanding Map. (arXiv:2110.06043v7 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06043">
<div class="article-summary-box-inner">
<span><p>Inspired by the notion of Center of Mass in physics, an extension called
Semantic Center of Mass (SCOM) is proposed, and used to discover the abstract
"topic" of a document. The notion is under a framework model called
Understanding Map Supervised Topic Model (UM-S-TM). The devise aim of UM-S-TM
is to let both the document content and a semantic network -- specifically,
Understanding Map -- play a role, in interpreting the meaning of a document.
Based on different justifications, three possible methods are devised to
discover the SCOM of a document. Some experiments on artificial documents and
Understanding Maps are conducted to test their outcomes. In addition, its
ability of vectorization of documents and capturing sequential information are
tested. We also compared UM-S-TM with probabilistic topic models like Latent
Dirichlet Allocation (LDA) and probabilistic Latent Semantic Analysis (pLSA).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning. (arXiv:2110.07577v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07577">
<div class="article-summary-box-inner">
<span><p>Recent parameter-efficient language model tuning (PELT) methods manage to
match the performance of fine-tuning with much fewer trainable parameters and
perform especially well when training data is limited. However, different PELT
methods may perform rather differently on the same task, making it nontrivial
to select the most appropriate method for a specific task, especially
considering the fast-growing number of new PELT methods and tasks. In light of
model diversity and the difficulty of model selection, we propose a unified
framework, UniPELT, which incorporates different PELT methods as submodules and
learns to activate the ones that best suit the current data or task setup via
gating mechanism. On the GLUE benchmark, UniPELT consistently achieves 1~4%
gains compared to the best individual PELT method that it incorporates and even
outperforms fine-tuning under different setups. Moreover, UniPELT generally
surpasses the upper bound that takes the best performance of all its submodules
used individually on each task, indicating that a mixture of multiple PELT
methods may be inherently more effective than single methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models. (arXiv:2110.08527v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08527">
<div class="article-summary-box-inner">
<span><p>Recent work has shown pre-trained language models capture social biases from
the large amounts of text they are trained on. This has attracted attention to
developing techniques that mitigate such biases. In this work, we perform an
empirical survey of five recently proposed bias mitigation techniques:
Counterfactual Data Augmentation (CDA), Dropout, Iterative Nullspace
Projection, Self-Debias, and SentenceDebias. We quantify the effectiveness of
each technique using three intrinsic bias benchmarks while also measuring the
impact of these techniques on a model's language modeling ability, as well as
its performance on downstream NLU tasks. We experimentally find that: (1)
Self-Debias is the strongest debiasing technique, obtaining improved scores on
all bias benchmarks; (2) Current debiasing techniques perform less consistently
when mitigating non-gender biases; And (3) improvements on bias benchmarks such
as StereoSet and CrowS-Pairs by using debiasing strategies are often
accompanied by a decrease in language modeling ability, making it difficult to
determine whether the bias mitigation was effective.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Triples-to-Text Generation with Reinforcement Learning Based Graph-augmented Neural Networks. (arXiv:2111.10545v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10545">
<div class="article-summary-box-inner">
<span><p>Considering a collection of RDF triples, the RDF-to-text generation task aims
to generate a text description. Most previous methods solve this task using a
sequence-to-sequence model or using a graph-based model to encode RDF triples
and to generate a text sequence. Nevertheless, these approaches fail to clearly
model the local and global structural information between and within RDF
triples. Moreover, the previous methods also face the non-negligible problem of
low faithfulness of the generated text, which seriously affects the overall
performance of these models. To solve these problems, we propose a model
combining two new graph-augmented structural neural encoders to jointly learn
both local and global structural information in the input RDF triples. To
further improve text faithfulness, we innovatively introduce a reinforcement
learning (RL) reward based on information extraction (IE). We first extract
triples from the generated text using a pretrained IE model and regard the
correct number of the extracted triples as the additional RL reward.
Experimental results on two benchmark datasets demonstrate that our proposed
model outperforms the state-of-the-art baselines, and the additional
reinforcement learning reward does help to improve the faithfulness of the
generated text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">L-Verse: Bidirectional Generation Between Image and Text. (arXiv:2111.11133v10 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11133">
<div class="article-summary-box-inner">
<span><p>Far beyond learning long-range interactions of natural language, transformers
are becoming the de-facto standard for many vision tasks with their power and
scalability. Especially with cross-modal tasks between image and text, vector
quantized variational autoencoders (VQ-VAEs) are widely used to make a raw RGB
image into a sequence of feature vectors. To better leverage the correlation
between image and text, we propose L-Verse, a novel architecture consisting of
feature-augmented variational autoencoder (AugVAE) and bidirectional
auto-regressive transformer (BiART) for image-to-text and text-to-image
generation. Our AugVAE shows the state-of-the-art reconstruction performance on
ImageNet1K validation set, along with the robustness to unseen images in the
wild. Unlike other models, BiART can distinguish between image (or text) as a
conditional reference and a generation target. L-Verse can be directly used for
image-to-text or text-to-image generation without any finetuning or extra
object detection framework. In quantitative and qualitative experiments,
L-Verse shows impressive results against previous methods in both image-to-text
and text-to-image generation on MS-COCO Captions. We furthermore assess the
scalability of L-Verse architecture on Conceptual Captions and present the
initial result of bidirectional vision-language representation learning on
general domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Bounded Context-Free-Grammar via LSTM and the Transformer:Difference and Explanations. (arXiv:2112.09174v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09174">
<div class="article-summary-box-inner">
<span><p>Long Short-Term Memory (LSTM) and Transformers are two popular neural
architectures used for natural language processing tasks. Theoretical results
show that both are Turing-complete and can represent any context-free language
(CFL).In practice, it is often observed that Transformer models have better
representation power than LSTM. But the reason is barely understood. We study
such practical differences between LSTM and Transformer and propose an
explanation based on their latent space decomposition patterns. To achieve this
goal, we introduce an oracle training paradigm, which forces the decomposition
of the latent representation of LSTM and the Transformer and supervises with
the transitions of the Pushdown Automaton (PDA) of the corresponding CFL. With
the forced decomposition, we show that the performance upper bounds of LSTM and
Transformer in learning CFL are close: both of them can simulate a stack and
perform stack operation along with state transitions. However, the absence of
forced decomposition leads to the failure of LSTM models to capture the stack
and stack operations, while having a marginal impact on the Transformer model.
Lastly, we connect the experiment on the prototypical PDA to a real-world
parsing task to re-verify the conclusions
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NoisyTune: A Little Noise Can Help You Finetune Pretrained Language Models Better. (arXiv:2202.12024v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12024">
<div class="article-summary-box-inner">
<span><p>Effectively finetuning pretrained language models (PLMs) is critical for
their success in downstream tasks. However, PLMs may have risks in overfitting
the pretraining tasks and data, which usually have gap with the target
downstream tasks. Such gap may be difficult for existing PLM finetuning methods
to overcome and lead to suboptimal performance. In this paper, we propose a
very simple yet effective method named NoisyTune to help better finetune PLMs
on downstream tasks by adding some noise to the parameters of PLMs before
fine-tuning. More specifically, we propose a matrix-wise perturbing method
which adds different uniform noises to different parameter matrices based on
their standard deviations. In this way, the varied characteristics of different
types of parameters in PLMs can be considered. Extensive experiments on both
GLUE English benchmark and XTREME multilingual benchmark show NoisyTune can
consistently empower the finetuning of different PLMs on different downstream
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simple but Effective Pluggable Entity Lookup Table for Pre-trained Language Models. (arXiv:2202.13392v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13392">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (PLMs) cannot well recall rich factual knowledge
of entities exhibited in large-scale corpora, especially those rare entities.
In this paper, we propose to build a simple but effective Pluggable Entity
Lookup Table (PELT) on demand by aggregating the entity's output
representations of multiple occurrences in the corpora. PELT can be compatibly
plugged as inputs to infuse supplemental entity knowledge into PLMs. Compared
to previous knowledge-enhanced PLMs, PELT only requires 0.2%-5% pre-computation
with capability of acquiring knowledge from out-of-domain corpora for domain
adaptation scenario. The experiments on knowledge-related tasks demonstrate
that our method, PELT, can flexibly and effectively transfer entity knowledge
from related corpora into PLMs with different architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Overlap-based Vocabulary Generation Improves Cross-lingual Transfer Among Related Languages. (arXiv:2203.01976v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01976">
<div class="article-summary-box-inner">
<span><p>Pre-trained multilingual language models such as mBERT and XLM-R have
demonstrated great potential for zero-shot cross-lingual transfer to low
web-resource languages (LRL). However, due to limited model capacity, the large
difference in the sizes of available monolingual corpora between high
web-resource languages (HRL) and LRLs does not provide enough scope of
co-embedding the LRL with the HRL, thereby affecting downstream task
performance of LRLs. In this paper, we argue that relatedness among languages
in a language family along the dimension of lexical overlap may be leveraged to
overcome some of the corpora limitations of LRLs. We propose Overlap BPE
(OBPE), a simple yet effective modification to the BPE vocabulary generation
algorithm which enhances overlap across related languages. Through extensive
experiments on multiple NLP tasks and datasets, we observe that OBPE generates
a vocabulary that increases the representation of LRLs via tokens shared with
HRLs. This results in improved zero-shot transfer from related HRLs to LRLs
without reducing HRL representation and accuracy. Unlike previous studies that
dismissed the importance of token-overlap, we show that in the low-resource
related language setting, token overlap matters. Synthetically reducing the
overlap to zero can cause as much as a four-fold drop in zero-shot transfer
accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incorporating Hierarchy into Text Encoder: a Contrastive Learning Approach for Hierarchical Text Classification. (arXiv:2203.03825v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03825">
<div class="article-summary-box-inner">
<span><p>Hierarchical text classification is a challenging subtask of multi-label
classification due to its complex label hierarchy. Existing methods encode text
and label hierarchy separately and mix their representations for
classification, where the hierarchy remains unchanged for all input text.
Instead of modeling them separately, in this work, we propose Hierarchy-guided
Contrastive Learning (HGCLR) to directly embed the hierarchy into a text
encoder. During training, HGCLR constructs positive samples for input text
under the guidance of the label hierarchy. By pulling together the input text
and its positive sample, the text encoder can learn to generate the
hierarchy-aware text representation independently. Therefore, after training,
the HGCLR enhanced text encoder can dispense with the redundant hierarchy.
Extensive experiments on three benchmark datasets verify the effectiveness of
HGCLR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Visual-Prompt Temporal Answering Grounding in Medical Instructional Video. (arXiv:2203.06667v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06667">
<div class="article-summary-box-inner">
<span><p>The temporal answering grounding in the video (TAGV) is a new task naturally
deriving from temporal sentence grounding in the video (TSGV). Given an
untrimmed video and a text question, this task aims at locating the matching
span from the video that can semantically answer the question. Existing methods
tend to formulate the TAGV task with a visual span-based question answering
(QA) approach by matching the visual frame span queried by the text question.
However, due to the weak correlations and huge gaps in semantics in features
between the textual question and visual answer, existing methods adopting
visual span predictor fail to perform well in the TAGV task. In this work, we
propose a visual-prompt text span localizing (VPTSL) method, which enhances the
text span localization in the pre-trained language model (PLM) with the visual
highlight features. Specifically, the context query attention is utilized to
perform cross-modal modeling between the textual and visual features. Then, the
highlight features are obtained through the highlight module with a linear
layer to provide the visual prompt. To alleviate the differences in semantics
and correlations between textual and visual features, we design the text span
predictor by encoding the question, the subtitles, and the visual prompt in the
PLM. As a result, the TAGV task is formulated to predict the span of subtitles
matching the visual answer. Extensive experiments on the medical instructional
dataset, namely MedVidQA, show that the proposed VPTSL outperforms other
state-of-the-art (SOTA) methods by 28.36 in mIOU score with a large margin,
which demonstrates the effectiveness of visual prompt and the text span
predictor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Word Translation via Two-Stage Contrastive Learning. (arXiv:2203.08307v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08307">
<div class="article-summary-box-inner">
<span><p>Word translation or bilingual lexicon induction (BLI) is a key cross-lingual
task, aiming to bridge the lexical gap between different languages. In this
work, we propose a robust and effective two-stage contrastive learning
framework for the BLI task. At Stage C1, we propose to refine standard
cross-lingual linear maps between static word embeddings (WEs) via a
contrastive learning objective; we also show how to integrate it into the
self-learning procedure for even more refined cross-lingual maps. In Stage C2,
we conduct BLI-oriented contrastive fine-tuning of mBERT, unlocking its word
translation capability. We also show that static WEs induced from the
`C2-tuned' mBERT complement static WEs from Stage C1. Comprehensive experiments
on standard BLI datasets for diverse languages and different experimental
setups demonstrate substantial gains achieved by our framework. While the BLI
method from Stage C1 already yields substantial gains over all state-of-the-art
BLI methods in our comparison, even stronger improvements are met with the full
two-stage framework: e.g., we report gains for 112/112 BLI setups, spanning 28
language pairs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridging the Data Gap between Training and Inference for Unsupervised Neural Machine Translation. (arXiv:2203.08394v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.08394">
<div class="article-summary-box-inner">
<span><p>Back-translation is a critical component of Unsupervised Neural Machine
Translation (UNMT), which generates pseudo parallel data from target
monolingual data. A UNMT model is trained on the pseudo parallel data with
translated source, and translates natural source sentences in inference. The
source discrepancy between training and inference hinders the translation
performance of UNMT models. By carefully designing experiments, we identify two
representative characteristics of the data gap in source: (1) style gap (i.e.,
translated vs. natural text style) that leads to poor generalization
capability; (2) content gap that induces the model to produce hallucination
content biased towards the target language. To narrow the data gap, we propose
an online self-training approach, which simultaneously uses the pseudo parallel
data {natural source, translated target} to mimic the inference scenario.
Experimental results on several widely-used language pairs show that our
approach outperforms two strong baselines (XLM and MASS) by remedying the style
and content gaps.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reducing Position Bias in Simultaneous Machine Translation with Length-Aware Framework. (arXiv:2203.09053v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09053">
<div class="article-summary-box-inner">
<span><p>Simultaneous machine translation (SiMT) starts translating while receiving
the streaming source inputs, and hence the source sentence is always incomplete
during translating. Different from the full-sentence MT using the conventional
seq-to-seq architecture, SiMT often applies prefix-to-prefix architecture,
which forces each target word to only align with a partial source prefix to
adapt to the incomplete source in streaming inputs. However, the source words
in the front positions are always illusoryly considered more important since
they appear in more prefixes, resulting in position bias, which makes the model
pay more attention on the front source positions in testing. In this paper, we
first analyze the phenomenon of position bias in SiMT, and develop a
Length-Aware Framework to reduce the position bias by bridging the structural
gap between SiMT and full-sentence MT. Specifically, given the streaming
inputs, we first predict the full-sentence length and then fill the future
source position with positional encoding, thereby turning the streaming inputs
into a pseudo full-sentence. The proposed framework can be integrated into most
existing SiMT methods to further improve performance. Experiments on two
representative SiMT methods, including the state-of-the-art adaptive policy,
show that our method successfully reduces the position bias and thereby
achieves better SiMT performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GRS: Combining Generation and Revision in Unsupervised Sentence Simplification. (arXiv:2203.09742v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09742">
<div class="article-summary-box-inner">
<span><p>We propose GRS: an unsupervised approach to sentence simplification that
combines text generation and text revision. We start with an iterative
framework in which an input sentence is revised using explicit edit operations,
and add paraphrasing as a new edit operation. This allows us to combine the
advantages of generative and revision-based approaches: paraphrasing captures
complex edit operations, and the use of explicit edit operations in an
iterative manner provides controllability and interpretability. We demonstrate
these advantages of GRS compared to existing methods on the Newsela and ASSET
datasets.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">CM-GAN: Image Inpainting with Cascaded Modulation GAN and Object-Aware Training. (arXiv:2203.11947v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11947">
<div class="article-summary-box-inner">
<span><p>Recent image inpainting methods have made great progress but often struggle
to generate plausible image structures when dealing with large holes in complex
images. This is partially due to the lack of effective network structures that
can capture both the long-range dependency and high-level semantics of an
image. To address these problems, we propose cascaded modulation GAN (CM-GAN),
a new network design consisting of an encoder with Fourier convolution blocks
that extract multi-scale feature representations from the input image with
holes and a StyleGAN-like decoder with a novel cascaded global-spatial
modulation block at each scale level. In each decoder block, global modulation
is first applied to perform coarse semantic-aware structure synthesis, then
spatial modulation is applied on the output of global modulation to further
adjust the feature map in a spatially adaptive fashion. In addition, we design
an object-aware training scheme to prevent the network from hallucinating new
objects inside holes, fulfilling the needs of object removal tasks in
real-world scenarios. Extensive experiments are conducted to show that our
method significantly outperforms existing methods in both quantitative and
qualitative evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Patch-to-Cluster Attention in Vision Transformer. (arXiv:2203.11987v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11987">
<div class="article-summary-box-inner">
<span><p>The Vision Transformer (ViT) model is built on the assumption of treating
image patches as "visual tokens" and learning patch-to-patch attention. The
patch embedding based tokenizer is a workaround in practice and has a semantic
gap with respect to its counterpart, the textual tokenizer. The patch-to-patch
attention suffers from the quadratic complexity issue, and also makes it
non-trivial to explain learned ViT models. To address these issues in ViT
models, this paper proposes to learn patch-to-cluster attention (PaCa) based
ViT models. Queries in our PaCaViT are based on patches, while keys and values
are based on clustering (with a predefined small number of clusters). The
clusters are learned end-to-end, leading to better tokenizers and realizing
joint clustering-for-attention and attention-for-clustering when deployed in
ViT models. The quadratic complexity is relaxed to linear complexity. Also,
directly visualizing the learned clusters can reveal how a trained ViT model
learns to perform a task (e.g., object detection). In experiments, the proposed
PaCa-ViT is tested on CIFAR-100 and ImageNet-1000 image classification, and
MS-COCO object detection and instance segmentation. Compared with prior arts,
it obtains better performance in classification and comparable performance in
detection and segmentation. It is significantly more efficient in COCO due to
the linear complexity. The learned clusters are also semantically meaningful
and shed light on designing more discriminative yet interpretable ViT models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Feature Learning and Relation Modeling for Tracking: A One-Stream Framework. (arXiv:2203.11991v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11991">
<div class="article-summary-box-inner">
<span><p>The current popular two-stream, two-stage tracking framework extracts the
template and the search region features separately and then performs relation
modeling, thus the extracted features lack the awareness of the target and have
limited target-background discriminability. To tackle the above issue, we
propose a novel one-stream tracking (OSTrack) framework that unifies feature
learning and relation modeling by bridging the template-search image pairs with
bidirectional information flows. In this way, discriminative target-oriented
features can be dynamically extracted by mutual guidance. Since no extra heavy
relation modeling module is needed and the implementation is highly
parallelized, the proposed tracker runs at a fast speed. To further improve the
inference efficiency, an in-network candidate early elimination module is
proposed based on the strong similarity prior calculated in the one-stream
framework. As a unified framework, OSTrack achieves state-of-the-art
performance on multiple benchmarks, in particular, it shows impressive results
on the one-shot tracking benchmark GOT-10k, i.e., achieving 73.7% AO, improving
the existing best result (SwinTrack) by 4.3%. Besides, our method maintains a
good performance-speed trade-off and shows faster convergence. The code and
models will be available at https://github.com/botaoye/OSTrack.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Geodesic-Aware Local Features from RGB-D Images. (arXiv:2203.12016v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12016">
<div class="article-summary-box-inner">
<span><p>Most of the existing handcrafted and learning-based local descriptors are
still at best approximately invariant to affine image transformations, often
disregarding deformable surfaces. In this paper, we take one step further by
proposing a new approach to compute descriptors from RGB-D images (where RGB
refers to the pixel color brightness and D stands for depth information) that
are invariant to isometric non-rigid deformations, as well as to scale changes
and rotation. Our proposed description strategies are grounded on the key idea
of learning feature representations on undistorted local image patches using
surface geodesics. We design two complementary local descriptors strategies to
compute geodesic-aware features efficiently: one efficient binary descriptor
based on handcrafted binary tests (named GeoBit), and one learning-based
descriptor (GeoPatch) with convolutional neural networks (CNNs) to compute
features. In different experiments using real and publicly available RGB-D data
benchmarks, they consistently outperforms state-of-the-art handcrafted and
learning-based image and RGB-D descriptors in matching scores, as well as in
object retrieval and non-rigid surface tracking experiments, with comparable
processing times. We also provide to the community a new dataset with accurate
matching annotations of RGB-D images of different objects (shirts, cloths,
paintings, bags), subjected to strong non-rigid deformations, for evaluation
benchmark of deformable surface correspondence algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Modeling Helps Weak Supervision (and Vice Versa). (arXiv:2203.12023v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12023">
<div class="article-summary-box-inner">
<span><p>Many promising applications of supervised machine learning face hurdles in
the acquisition of labeled data in sufficient quantity and quality, creating an
expensive bottleneck. To overcome such limitations, techniques that do not
depend on ground truth labels have been developed, including weak supervision
and generative modeling. While these techniques would seem to be usable in
concert, improving one another, how to build an interface between them is not
well-understood. In this work, we propose a model fusing weak supervision and
generative adversarial networks. It captures discrete variables in the data
alongside the weak supervision derived label estimate. Their alignment allows
for better modeling of sample-dependent accuracies of the weak supervision
sources, improving the unobserved ground truth estimate. It is the first
approach to enable data augmentation through weakly supervised synthetic images
and pseudolabels. Additionally, its learned discrete variables can be inspected
qualitatively. The model outperforms baseline weak supervision label models on
a number of multiclass classification datasets, improves the quality of
generated images, and further improves end-model performance through data
augmentation with synthetic samples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervision through Random Segments with Autoregressive Coding (RandSAC). (arXiv:2203.12054v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12054">
<div class="article-summary-box-inner">
<span><p>Inspired by the success of self-supervised autoregressive representation
learning in natural language (GPT and its variants), and advances in recent
visual architecture design with Vision Transformers (ViTs), in this paper, we
explore the effects various design choices have on the success of applying such
training strategies for visual feature learning. Specifically, we introduce a
novel strategy that we call Random Segments with Autoregressive Coding
(RandSAC). In RandSAC, we group patch representations (image tokens) into
hierarchically arranged segments; within each segment, tokens are predicted in
parallel, similar to BERT, while across segment predictions are sequential,
similar to GPT. We illustrate that randomized serialization of the segments
significantly improves the performance and results in distribution over
spatially-long (across-segments) and -short (within-segment) predictions which
are effective for feature learning. We illustrate the pertinence of these
design choices and explore alternatives on a number of datasets (e.g., CIFAR10,
ImageNet). While our pre-training strategy works with vanilla Transformer, we
also propose a conceptually simple, but highly effective, addition to the
decoder that allows learnable skip-connections to encoder feature layers, which
further improves the performance. Our final model, trained on ImageNet,
achieves new state-of-the-art linear probing performance 68.3% among
comparative predictive self-supervised learning approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WayFAST: Traversability Predictive Navigation for Field Robots. (arXiv:2203.12071v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12071">
<div class="article-summary-box-inner">
<span><p>We present a self-supervised approach for learning to predict traversable
paths for wheeled mobile robots that require good traction to navigate. Our
algorithm, termed WayFAST (Waypoint Free Autonomous Systems for
Traversability), uses RGB and depth data, along with navigation experience, to
autonomously generate traversable paths in outdoor unstructured environments.
Our key inspiration is that traction can be estimated for rolling robots using
kinodynamic models. Using traction estimates provided by an online receding
horizon estimator, we are able to train a traversability prediction neural
network in a self-supervised manner, without requiring heuristics utilized by
previous methods. We demonstrate the effectiveness of WayFAST through extensive
field testing in varying environments, ranging from sandy dry beaches to forest
canopies and snow covered grass fields. Our results clearly demonstrate that
WayFAST can learn to avoid geometric obstacles as well as untraversable
terrain, such as snow, which would be difficult to avoid with sensors that
provide only geometric data, such as LiDAR. Furthermore, we show that our
training pipeline based on online traction estimates is more data-efficient
than other heuristic-based methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DTFD-MIL: Double-Tier Feature Distillation Multiple Instance Learning for Histopathology Whole Slide Image Classification. (arXiv:2203.12081v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12081">
<div class="article-summary-box-inner">
<span><p>Multiple instance learning (MIL) has been increasingly used in the
classification of histopathology whole slide images (WSIs). However, MIL
approaches for this specific classification problem still face unique
challenges, particularly those related to small sample cohorts. In these, there
are limited number of WSI slides (bags), while the resolution of a single WSI
is huge, which leads to a large number of patches (instances) cropped from this
slide. To address this issue, we propose to virtually enlarge the number of
bags by introducing the concept of pseudo-bags, on which a double-tier MIL
framework is built to effectively use the intrinsic features. Besides, we also
contribute to deriving the instance probability under the framework of
attention-based MIL, and utilize the derivation to help construct and analyze
the proposed framework. The proposed method outperforms other latest methods on
the CAMELYON-16 by substantially large margins, and is also better in
performance on the TCGA lung cancer dataset. The proposed framework is ready to
be extended for wider MIL applications. The code is available at:
https://github.com/hrzhang1123/DTFD-MIL
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PlaneMVS: 3D Plane Reconstruction from Multi-View Stereo. (arXiv:2203.12082v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12082">
<div class="article-summary-box-inner">
<span><p>We present a novel framework named PlaneMVS for 3D plane reconstruction from
multiple input views with known camera poses. Most previous learning-based
plane reconstruction methods reconstruct 3D planes from single images, which
highly rely on single-view regression and suffer from depth scale ambiguity. In
contrast, we reconstruct 3D planes with a multi-view-stereo (MVS) pipeline that
takes advantage of multi-view geometry. We decouple plane reconstruction into a
semantic plane detection branch and a plane MVS branch. The semantic plane
detection branch is based on a single-view plane detection framework but with
differences. The plane MVS branch adopts a set of slanted plane hypotheses to
replace conventional depth hypotheses to perform plane sweeping strategy and
finally learns pixel-level plane parameters and its planar depth map. We
present how the two branches are learned in a balanced way, and propose a
soft-pooling loss to associate the outputs of the two branches and make them
benefit from each other. Extensive experiments on various indoor datasets show
that PlaneMVS significantly outperforms state-of-the-art (SOTA) single-view
plane reconstruction methods on both plane detection and 3D geometry metrics.
Our method even outperforms a set of SOTA learning-based MVS methods thanks to
the learned plane priors. To the best of our knowledge, this is the first work
on 3D plane reconstruction within an end-to-end MVS framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Portrait Delighting. (arXiv:2203.12088v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12088">
<div class="article-summary-box-inner">
<span><p>We present a deep neural network for removing undesirable shading features
from an unconstrained portrait image, recovering the underlying texture. Our
training scheme incorporates three regularization strategies: masked loss, to
emphasize high-frequency shading features; soft-shadow loss, which improves
sensitivity to subtle changes in lighting; and shading-offset estimation, to
supervise separation of shading and texture. Our method demonstrates improved
delighting quality and generalization when compared with the state-of-the-art.
We further demonstrate how our delighting method can enhance the performance of
light-sensitive computer vision tasks such as face relighting and semantic
parsing, allowing them to handle extreme lighting conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FxP-QNet: A Post-Training Quantizer for the Design of Mixed Low-Precision DNNs with Dynamic Fixed-Point Representation. (arXiv:2203.12091v1 [cs.NE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12091">
<div class="article-summary-box-inner">
<span><p>Deep neural networks (DNNs) have demonstrated their effectiveness in a wide
range of computer vision tasks, with the state-of-the-art results obtained
through complex and deep structures that require intensive computation and
memory. Now-a-days, efficient model inference is crucial for consumer
applications on resource-constrained platforms. As a result, there is much
interest in the research and development of dedicated deep learning (DL)
hardware to improve the throughput and energy efficiency of DNNs. Low-precision
representation of DNN data-structures through quantization would bring great
benefits to specialized DL hardware. However, the rigorous quantization leads
to a severe accuracy drop. As such, quantization opens a large hyper-parameter
space at bit-precision levels, the exploration of which is a major challenge.
In this paper, we propose a novel framework referred to as the Fixed-Point
Quantizer of deep neural Networks (FxP-QNet) that flexibly designs a mixed
low-precision DNN for integer-arithmetic-only deployment. Specifically, the
FxP-QNet gradually adapts the quantization level for each data-structure of
each layer based on the trade-off between the network accuracy and the
low-precision requirements. Additionally, it employs post-training
self-distillation and network prediction error statistics to optimize the
quantization of floating-point values into fixed-point numbers. Examining
FxP-QNet on state-of-the-art architectures and the benchmark ImageNet dataset,
we empirically demonstrate the effectiveness of FxP-QNet in achieving the
accuracy-compression trade-off without the need for training. The results show
that FxP-QNet-quantized AlexNet, VGG-16, and ResNet-18 reduce the overall
memory requirements of their full-precision counterparts by 7.16x, 10.36x, and
6.44x with less than 0.95%, 0.95%, and 1.99% accuracy drop, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast on-line signature recognition based on VQ with time modeling. (arXiv:2203.12104v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12104">
<div class="article-summary-box-inner">
<span><p>This paper proposes a multi-section vector quantization approach for on-line
signature recognition. We have used the MCYT database, which consists of 330
users and 25 skilled forgeries per person performed by 5 different impostors.
This database is larger than those typically used in the literature.
Nevertheless, we also provide results from the SVC database.
</p>
<p>Our proposed system outperforms the winner of SVC with a reduced
computational requirement, which is around 47 times lower than DTW. In
addition, our system improves the database storage requirements due to vector
compression, and is more privacy-friendly as it is not possible to recover the
original signature using the codebooks. Experimental results with MCYT provide
a 99.76% identification rate and 2.46% EER (skilled forgeries and individual
threshold). Experimental results with SVC are 100% of identification rate and
0% (individual threshold) and 0.31% (general threshold) when using a
two-section VQ approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lymphocyte Classification in Hyperspectral Images of Ovarian Cancer Tissue Biopsy Samples. (arXiv:2203.12112v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12112">
<div class="article-summary-box-inner">
<span><p>Current methods for diagnosing the progression of multiple types of cancer
within patients rely on interpreting stained needle biopsies. This process is
time-consuming and susceptible to error throughout the paraffinization,
Hematoxylin and Eosin (H&amp;E) staining, deparaffinization, and annotation stages.
Fourier Transform Infrared (FTIR) imaging has been shown to be a promising
alternative to staining for appropriately annotating biopsy cores without the
need for deparaffinization or H&amp;E staining with the use of Fourier Transform
Infrared (FTIR) images when combined with machine learning to interpret the
dense spectral information. We present a machine learning pipeline to segment
white blood cell (lymphocyte) pixels in hyperspectral images of biopsy cores.
These cells are clinically important for diagnosis, but some prior work has
struggled to incorporate them due to difficulty obtaining precise pixel labels.
Evaluated methods include Support Vector Machine (SVM), Gaussian Naive Bayes,
and Multilayer Perceptron (MLP), as well as analyzing the comparatively modern
convolutional neural network (CNN).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GOSS: Towards Generalized Open-set Semantic Segmentation. (arXiv:2203.12116v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12116">
<div class="article-summary-box-inner">
<span><p>In this paper, we present and study a new image segmentation task, called
Generalized Open-set Semantic Segmentation (GOSS). Previously, with the
well-known open-set semantic segmentation (OSS), the intelligent agent only
detects the unknown regions without further processing, limiting their
perception of the environment. It stands to reason that a further analysis of
the detected unknown pixels would be beneficial. Therefore, we propose GOSS,
which unifies the abilities of two well-defined segmentation tasks, OSS and
generic segmentation (GS), in a holistic way. Specifically, GOSS classifies
pixels as belonging to known classes, and clusters (or groups) of pixels of
unknown class are labelled as such. To evaluate this new expanded task, we
further propose a metric which balances the pixel classification and clustering
aspects. Moreover, we build benchmark tests on top of existing datasets and
propose a simple neural architecture as a baseline, which jointly predicts
pixel classification and clustering under open-set settings. Our experiments on
multiple benchmarks demonstrate the effectiveness of our baseline. We believe
our new GOSS task can produce an expressive image understanding for future
research. Code will be made available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual Prompt Tuning. (arXiv:2203.12119v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12119">
<div class="article-summary-box-inner">
<span><p>The current modus operandi in adapting pre-trained models involves updating
all the backbone parameters, ie, full fine-tuning. This paper introduces Visual
Prompt Tuning (VPT) as an efficient and effective alternative to full
fine-tuning for large-scale Transformer models in vision. Taking inspiration
from recent advances in efficiently tuning large language models, VPT
introduces only a small amount (less than 1% of model parameters) of trainable
parameters in the input space while keeping the model backbone frozen. Via
extensive experiments on a wide variety of downstream recognition tasks, we
show that VPT achieves significant performance gains compared to other
parameter efficient tuning protocols. Most importantly, VPT even outperforms
full fine-tuning in many cases across model capacities and training data
scales, while reducing per-task storage cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Transformer-based Multiple Instance Learning for Weakly Supervised Polyp Frame Detection. (arXiv:2203.12121v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12121">
<div class="article-summary-box-inner">
<span><p>Current polyp detection methods from colonoscopy videos use exclusively
normal (i.e., healthy) training images, which i) ignore the importance of
temporal information in consecutive video frames, and ii) lack knowledge about
the polyps. Consequently, they often have high detection errors, especially on
challenging polyp cases (e.g., small, flat, or partially visible polyps). In
this work, we formulate polyp detection as a weakly-supervised anomaly
detection task that uses video-level labelled training data to detect
frame-level polyps. In particular, we propose a novel convolutional
transformer-based multiple instance learning method designed to identify
abnormal frames (i.e., frames with polyps) from anomalous videos (i.e., videos
containing at least one frame with polyp). In our method, local and global
temporal dependencies are seamlessly captured while we simultaneously optimise
video and snippet-level anomaly scores. A contrastive snippet mining method is
also proposed to enable an effective modelling of the challenging polyp cases.
The resulting method achieves a detection accuracy that is substantially better
than current state-of-the-art approaches on a new large-scale colonoscopy video
dataset introduced in this work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pixel VQ-VAEs for Improved Pixel Art Representation. (arXiv:2203.12130v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12130">
<div class="article-summary-box-inner">
<span><p>Machine learning has had a great deal of success in image processing.
However, the focus of this work has largely been on realistic images, ignoring
more niche art styles such as pixel art. Additionally, many traditional machine
learning models that focus on groups of pixels do not work well with pixel art,
where individual pixels are important. We propose the Pixel VQ-VAE, a
specialized VQ-VAE model that learns representations of pixel art. We show that
it outperforms other models in both the quality of embeddings as well as
performance on downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Supervised Hybrid Spine Network for Segmentation of Spine MR Images. (arXiv:2203.12151v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12151">
<div class="article-summary-box-inner">
<span><p>Automatic segmentation of vertebral bodies (VBs) and intervertebral discs
(IVDs) in 3D magnetic resonance (MR) images is vital in diagnosing and treating
spinal diseases. However, segmenting the VBs and IVDs simultaneously is not
trivial. Moreover, problems exist, including blurry segmentation caused by
anisotropy resolution, high computational cost, inter-class similarity and
intra-class variability, and data imbalances. We proposed a two-stage
algorithm, named semi-supervised hybrid spine network (SSHSNet), to address
these problems by achieving accurate simultaneous VB and IVD segmentation. In
the first stage, we constructed a 2D semi-supervised DeepLabv3+ by using cross
pseudo supervision to obtain intra-slice features and coarse segmentation. In
the second stage, a 3D full-resolution patch-based DeepLabv3+ was built. This
model can be used to extract inter-slice information and combine the coarse
segmentation and intra-slice features provided from the first stage. Moreover,
a cross tri-attention module was applied to compensate for the loss of
inter-slice and intra-slice information separately generated from 2D and 3D
networks, thereby improving feature representation ability and achieving
satisfactory segmentation results. The proposed SSHSNet was validated on a
publicly available spine MR image dataset, and remarkable segmentation
performance was achieved. Moreover, results show that the proposed method has
great potential in dealing with the data imbalance problem. Based on previous
reports, few studies have incorporated a semi-supervised learning strategy with
a cross attention mechanism for spine segmentation. Therefore, the proposed
method may provide a useful tool for spine segmentation and aid clinically in
spinal disease diagnoses and treatments. Codes are publicly available at:
https://github.com/Meiyan88/SSHSNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comprehensive Benchmark Datasets for Amharic Scene Text Detection and Recognition. (arXiv:2203.12165v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12165">
<div class="article-summary-box-inner">
<span><p>Ethiopic/Amharic script is one of the oldest African writing systems, which
serves at least 23 languages (e.g., Amharic, Tigrinya) in East Africa for more
than 120 million people. The Amharic writing system, Abugida, has 282
syllables, 15 punctuation marks, and 20 numerals. The Amharic syllabic matrix
is derived from 34 base graphemes/consonants by adding up to 12 appropriate
diacritics or vocalic markers to the characters. The syllables with a common
consonant or vocalic markers are likely to be visually similar and challenge
text recognition tasks. In this work, we presented the first comprehensive
public datasets named HUST-ART, HUST-AST, ABE, and Tana for Amharic script
detection and recognition in the natural scene. We have also conducted
extensive experiments to evaluate the performance of the state of art methods
in detecting and recognizing Amharic scene text on our datasets. The evaluation
results demonstrate the robustness of our datasets for benchmarking and its
potential of promoting the development of robust Amharic script detection and
recognition algorithms. Consequently, the outcome will benefit people in East
Africa, including diplomats from several countries and international
communities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Transformers for Robust Few-shot Cross-domain Face Anti-spoofing. (arXiv:2203.12175v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12175">
<div class="article-summary-box-inner">
<span><p>While recent face anti-spoofing methods perform well under the intra-domain
setups, an effective approach needs to account for much larger appearance
variations of images acquired in complex scenes with different sensors for
robust performance. In this paper, we present adaptive vision transformers
(ViT) for robust cross-domain face anti-spoofing. Specifically, we adopt ViT as
a backbone to exploit its strength to account for long-range dependencies among
pixels. We further introduce the ensemble adapters module and feature-wise
transformation layers in the ViT to adapt to different domains for robust
performance with a few samples. Experiments on several benchmark datasets show
that the proposed models achieve both robust and competitive performance
against the state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unifying Motion Deblurring and Frame Interpolation with Events. (arXiv:2203.12178v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12178">
<div class="article-summary-box-inner">
<span><p>Slow shutter speed and long exposure time of frame-based cameras often cause
visual blur and loss of inter-frame information, degenerating the overall
quality of captured videos. To this end, we present a unified framework of
event-based motion deblurring and frame interpolation for blurry video
enhancement, where the extremely low latency of events is leveraged to
alleviate motion blur and facilitate intermediate frame prediction.
Specifically, the mapping relation between blurry frames and sharp latent
images is first predicted by a learnable double integral network, and a fusion
network is then proposed to refine the coarse results via utilizing the
information from consecutive blurry inputs and the concurrent events. By
exploring the mutual constraints among blurry frames, latent images, and event
streams, we further propose a self-supervised learning framework to enable
network training with real-world blurry videos and events. Extensive
experiments demonstrate that our method compares favorably against the
state-of-the-art approaches and achieves remarkable performance on both
synthetic and real-world datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Censor by Noisy Sampling. (arXiv:2203.12192v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12192">
<div class="article-summary-box-inner">
<span><p>Point clouds are an increasingly ubiquitous input modality and the raw signal
can be efficiently processed with recent progress in deep learning. This signal
may, often inadvertently, capture sensitive information that can leak semantic
and geometric properties of the scene which the data owner does not want to
share. The goal of this work is to protect sensitive information when learning
from point clouds; by censoring the sensitive information before the point
cloud is released for downstream tasks. Specifically, we focus on preserving
utility for perception tasks while mitigating attribute leakage attacks. The
key motivating insight is to leverage the localized saliency of perception
tasks on point clouds to provide good privacy-utility trade-offs. We realize
this through a mechanism called Censoring by Noisy Sampling (CBNS), which is
composed of two modules: i) Invariant Sampler: a differentiable point-cloud
sampler which learns to remove points invariant to utility and ii) Noisy
Distorter: which learns to distort sampled points to decouple the sensitive
information from utility, and mitigate privacy leakage. We validate the
effectiveness of CBNS through extensive comparisons with state-of-the-art
baselines and sensitivity analyses of key design choices. Results show that
CBNS achieves superior privacy-utility trade-offs on multiple datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Robust Scene Flow Estimation via the Alignment of Probability Density Functions. (arXiv:2203.12193v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12193">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a new self-supervised scene flow estimation
approach for a pair of consecutive point clouds. The key idea of our approach
is to represent discrete point clouds as continuous probability density
functions using Gaussian mixture models. Scene flow estimation is therefore
converted into the problem of recovering motion from the alignment of
probability density functions, which we achieve using a closed-form expression
of the classic Cauchy-Schwarz divergence. Unlike existing
nearest-neighbor-based approaches that use hard pairwise correspondences, our
proposed approach establishes soft and implicit point correspondences between
point clouds and generates more robust and accurate scene flow in the presence
of missing correspondences and outliers. Comprehensive experiments show that
our method makes noticeable gains over the Chamfer Distance and the Earth
Mover's Distance in real-world environments and achieves state-of-the-art
performance among self-supervised learning methods on FlyingThings3D and KITTI,
even outperforming some supervised methods with ground truth annotations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Biceph-Net: A robust and lightweight framework for the diagnosis of Alzheimer's disease using 2D-MRI scans and deep similarity learning. (arXiv:2203.12197v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12197">
<div class="article-summary-box-inner">
<span><p>Alzheimer's Disease (AD) is a neurodegenerative disease that is one of the
significant causes of death in the elderly population. Many deep learning
techniques have been proposed to diagnose AD using Magnetic Resonance Imaging
(MRI) scans. Predicting AD using 2D slices extracted from 3D MRI scans is
challenging as the inter-slice information gets lost. To this end, we propose a
novel and lightweight framework termed 'Biceph-Net' for AD diagnosis using 2D
MRI scans that model both the intra-slice and inter-slice information.
Biceph-Net has been experimentally shown to perform similar to other
Spatio-temporal neural networks while being computationally more efficient.
Biceph-Net is also superior in performance compared to vanilla 2D convolutional
neural networks (CNN) for AD diagnosis using 2D MRI slices. Biceph-Net also has
an inbuilt neighbourhood-based model interpretation feature that can be
exploited to understand the classification decision taken by the network.
Biceph-Net experimentally achieves a test accuracy of 100% in the
classification of Cognitively Normal (CN) vs AD, 98.16% for Mild Cognitive
Impairment (MCI) vs AD, and 97.80% for CN vs MCI vs AD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Frequency Filtering for Domain Generalization. (arXiv:2203.12198v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12198">
<div class="article-summary-box-inner">
<span><p>Improving the generalization capability of Deep Neural Networks (DNNs) is
critical for their practical uses, which has been a longstanding challenge.
Some theoretical studies have revealed that DNNs have preferences to different
frequency components in the learning process and indicated that this may affect
the robustness of learned features. In this paper, we propose Deep Frequency
Filtering (DFF) for learning domain-generalizable features, which is the first
endeavour to explicitly modulate frequency components of different transfer
difficulties across domains during training. To achieve this, we perform Fast
Fourier Transform (FFT) on feature maps at different layers, then adopt a
light-weight module to learn the attention masks from frequency representations
after FFT to enhance transferable frequency components while suppressing the
components not conductive to generalization. Further, we empirically compare
different types of attention for implementing our conceptualized DFF. Extensive
experiments demonstrate the effectiveness of the proposed DFF and show that
applying DFF on a plain baseline outperforms the state-of-the-art methods on
different domain generalization tasks, including close-set classification and
open-set retrieval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpretable Prediction of Lung Squamous Cell Carcinoma Recurrence With Self-supervised Learning. (arXiv:2203.12204v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12204">
<div class="article-summary-box-inner">
<span><p>Lung squamous cell carcinoma (LSCC) has a high recurrence and metastasis
rate. Factors influencing recurrence and metastasis are currently unknown and
there are no distinct histopathological or morphological features indicating
the risks of recurrence and metastasis in LSCC. Our study focuses on the
recurrence prediction of LSCC based on H&amp;E-stained histopathological
whole-slide images (WSI). Due to the small size of LSCC cohorts in terms of
patients with available recurrence information, standard end-to-end learning
with various convolutional neural networks for this task tends to overfit.
Also, the predictions made by these models are hard to interpret.
Histopathology WSIs are typically very large and are therefore processed as a
set of smaller tiles. In this work, we propose a novel conditional
self-supervised learning (SSL) method to learn representations of WSI at the
tile level first, and leverage clustering algorithms to identify the tiles with
similar histopathological representations. The resulting representations and
clusters from self-supervision are used as features of a survival model for
recurrence prediction at the patient level. Using two publicly available
datasets from TCGA and CPTAC, we show that our LSCC recurrence prediction
survival model outperforms both LSCC pathological stage-based approach and
machine learning baselines such as multiple instance learning. The proposed
method also enables us to explain the recurrence histopathological risk factors
via the derived clusters. This can help pathologists derive new hypotheses
regarding morphological features associated with LSCC recurrence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Learning of Adversarial Example: Towards Good Generalizations for Deepfake Detection. (arXiv:2203.12208v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12208">
<div class="article-summary-box-inner">
<span><p>Recent studies in deepfake detection have yielded promising results when the
training and testing face forgeries are from the same dataset. However, the
problem remains challenging when one tries to generalize the detector to
forgeries created by unseen methods in the training dataset. This work
addresses the generalizable deepfake detection from a simple principle: a
generalizable representation should be sensitive to diverse types of forgeries.
Following this principle, we propose to enrich the "diversity" of forgeries by
synthesizing augmented forgeries with a pool of forgery configurations and
strengthen the "sensitivity" to the forgeries by enforcing the model to predict
the forgery configurations. To effectively explore the large forgery
augmentation space, we further propose to use the adversarial training strategy
to dynamically synthesize the most challenging forgeries to the current model.
Through extensive experiments, we show that the proposed strategies are
surprisingly effective (see Figure 1), and they could achieve superior
performance than the current state-of-the-art methods. Code is available at
\url{https://github.com/liangchen527/SLADD}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Physics-Driven Deep Learning for Computational Magnetic Resonance Imaging. (arXiv:2203.12215v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12215">
<div class="article-summary-box-inner">
<span><p>Physics-driven deep learning methods have emerged as a powerful tool for
computational magnetic resonance imaging (MRI) problems, pushing reconstruction
performance to new limits. This article provides an overview of the recent
developments in incorporating physics information into learning-based MRI
reconstruction. We consider inverse problems with both linear and non-linear
forward models for computational MRI, and review the classical approaches for
solving these. We then focus on physics-driven deep learning approaches,
covering physics-driven loss functions, plug-and-play methods, generative
models, and unrolled networks. We highlight domain-specific challenges such as
real- and complex-valued building blocks of neural networks, and translational
applications in MRI with linear and non-linear forward models. Finally, we
discuss common issues and open challenges, and draw connections to the
importance of physics-driven learning when combined with other downstream tasks
in the medical imaging pipeline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training-free Transformer Architecture Search. (arXiv:2203.12217v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12217">
<div class="article-summary-box-inner">
<span><p>Recently, Vision Transformer (ViT) has achieved remarkable success in several
computer vision tasks. The progresses are highly relevant to the architecture
design, then it is worthwhile to propose Transformer Architecture Search (TAS)
to search for better ViTs automatically. However, current TAS methods are
time-consuming and existing zero-cost proxies in CNN do not generalize well to
the ViT search space according to our experimental observations. In this paper,
for the first time, we investigate how to conduct TAS in a training-free manner
and devise an effective training-free TAS (TF-TAS) scheme. Firstly, we observe
that the properties of multi-head self-attention (MSA) and multi-layer
perceptron (MLP) in ViTs are quite different and that the synaptic diversity of
MSA affects the performance notably. Secondly, based on the observation, we
devise a modular strategy in TF-TAS that evaluates and ranks ViT architectures
from two theoretical perspectives: synaptic diversity and synaptic saliency,
termed as DSS-indicator. With DSS-indicator, evaluation results are strongly
correlated with the test accuracies of ViT models. Experimental results
demonstrate that our TF-TAS achieves a competitive performance against the
state-of-the-art manually or automatically design ViT architectures, and it
promotes the searching efficiency in ViT search space greatly: from about $24$
GPU days to less than $0.5$ GPU days. Moreover, the proposed DSS-indicator
outperforms the existing cutting-edge zero-cost approaches (e.g., TE-score and
NASWOT).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Few-Shot Object Detection via Knowledge Inheritance. (arXiv:2203.12224v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12224">
<div class="article-summary-box-inner">
<span><p>Few-shot object detection (FSOD), which aims at learning a generic detector
that can adapt to unseen tasks with scarce training samples, has witnessed
consistent improvement recently. However, most existing methods ignore the
efficiency issues, e.g., high computational complexity and slow adaptation
speed. Notably, efficiency has become an increasingly important evaluation
metric for few-shot techniques due to an emerging trend toward embedded AI. To
this end, we present an efficient pretrain-transfer framework (PTF) baseline
with no computational increment, which achieves comparable results with
previous state-of-the-art (SOTA) methods. Upon this baseline, we devise an
initializer named knowledge inheritance (KI) to reliably initialize the novel
weights for the box classifier, which effectively facilitates the knowledge
transfer process and boosts the adaptation speed. Within the KI initializer, we
propose an adaptive length re-scaling (ALR) strategy to alleviate the vector
length inconsistency between the predicted novel weights and the pretrained
base weights. Finally, our approach not only achieves the SOTA results across
three public benchmarks, i.e., PASCAL VOC, COCO and LVIS, but also exhibits
high efficiency with 1.8-9.0x faster adaptation speed against the other methods
on COCO/LVIS benchmark during few-shot transfer. To our best knowledge, this is
the first work to consider the efficiency problem in FSOD. We hope to motivate
a trend toward powerful yet efficient few-shot technique development. The codes
are publicly available at https://github.com/Ze-Yang/Efficient-FSOD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Negative Selection by Clustering for Contrastive Learning in Human Activity Recognition. (arXiv:2203.12230v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12230">
<div class="article-summary-box-inner">
<span><p>Contrastive learning has been applied to Human Activity Recognition (HAR)
based on sensor data owing to its ability to achieve performance comparable to
supervised learning with a large amount of unlabeled data and a small amount of
labeled data. The pre-training task for contrastive learning is generally
instance discrimination, which specifies that each instance belongs to a single
class, but this will consider the same class of samples as negative examples.
Such a pre-training task is not conducive to human activity recognition tasks,
which are mainly classification tasks. To address this problem, we follow
SimCLR to propose a new contrastive learning framework that negative selection
by clustering in HAR, which is called ClusterCLHAR. Compared with SimCLR, it
redefines the negative pairs in the contrastive loss function by using
unsupervised clustering methods to generate soft labels that mask other samples
of the same cluster to avoid regarding them as negative samples. We evaluate
ClusterCLHAR on three benchmark datasets, USC-HAD, MotionSense, and UCI-HAR,
using mean F1-score as the evaluation metric. The experiment results show that
it outperforms all the state-of-the-art methods applied to HAR in
self-supervised learning and semi-supervised learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multi-Characteristic Learning Method with Micro-Doppler Signatures for Pedestrian Identification. (arXiv:2203.12236v1 [eess.SP])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12236">
<div class="article-summary-box-inner">
<span><p>The identification of pedestrians using radar micro-Doppler signatures has
become a hot topic in recent years. In this paper, we propose a
multi-characteristic learning (MCL) model with clusters to jointly learn
discrepant pedestrian micro-Doppler signatures and fuse the knowledge learned
from each cluster into final decisions. Time-Doppler spectrogram (TDS) and
signal statistical features extracted from FMCW radar, as two categories of
micro-Doppler signatures, are used in MCL to learn the micro-motion information
inside pedestrians' free walking patterns. The experimental results show that
our model achieves a higher accuracy rate and is more stable for pedestrian
identification than other studies, which make our model more practical.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Method of Data Augmentation to Train a Small Area Fingerprint Recognition Deep Neural Network with a Normal Fingerprint Database. (arXiv:2203.12241v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12241">
<div class="article-summary-box-inner">
<span><p>Fingerprints are popular among the biometric based systems due to ease of
acquisition, uniqueness and availability. Nowadays it is used in smart phone
security, digital payment and digital locker. The traditional fingerprint
matching methods based on minutiae are mainly applicable for large-area
fingerprint and the accuracy rate would reduce significantly when dealing with
small-area fingerprint from smart phone. There are many attempts to using deep
learning for small-area fingerprint recognition, and there are many successes.
But training deep neural network needs a lot of datasets for training. There is
no well-known dataset for small-area, so we have to make datasets ourselves. In
this paper, we propose a method of data augmentation to train a small-area
fingerprint recognition deep neural network with a normal fingerprint database
(such as FVC2002) and verify it via tests. The experimental results showed the
efficiency of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scale-Equivalent Distillation for Semi-Supervised Object Detection. (arXiv:2203.12244v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12244">
<div class="article-summary-box-inner">
<span><p>Recent Semi-Supervised Object Detection (SS-OD) methods are mainly based on
self-training, i.e., generating hard pseudo-labels by a teacher model on
unlabeled data as supervisory signals. Although they achieved certain success,
the limited labeled data in semi-supervised learning scales up the challenges
of object detection. We analyze the challenges these methods meet with the
empirical experiment results. We find that the massive False Negative samples
and inferior localization precision lack consideration. Besides, the large
variance of object sizes and class imbalance (i.e., the extreme ratio between
background and object) hinder the performance of prior arts. Further, we
overcome these challenges by introducing a novel approach, Scale-Equivalent
Distillation (SED), which is a simple yet effective end-to-end knowledge
distillation framework robust to large object size variance and class
imbalance. SED has several appealing benefits compared to the previous works.
(1) SED imposes a consistency regularization to handle the large scale variance
problem. (2) SED alleviates the noise problem from the False Negative samples
and inferior localization precision. (3) A re-weighting strategy can implicitly
screen the potential foreground regions of the unlabeled data to reduce the
effect of class imbalance. Extensive experiments show that SED consistently
outperforms the recent state-of-the-art methods on different datasets with
significant margins. For example, it surpasses the supervised counterpart by
more than 10 mAP when using 5% and 10% labeled data on MS-COCO.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ev-TTA: Test-Time Adaptation for Event-Based Object Recognition. (arXiv:2203.12247v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12247">
<div class="article-summary-box-inner">
<span><p>We introduce Ev-TTA, a simple, effective test-time adaptation algorithm for
event-based object recognition. While event cameras are proposed to provide
measurements of scenes with fast motions or drastic illumination changes, many
existing event-based recognition algorithms suffer from performance
deterioration under extreme conditions due to significant domain shifts. Ev-TTA
mitigates the severe domain gaps by fine-tuning the pre-trained classifiers
during the test phase using loss functions inspired by the spatio-temporal
characteristics of events. Since the event data is a temporal stream of
measurements, our loss function enforces similar predictions for adjacent
events to quickly adapt to the changed environment online. Also, we utilize the
spatial correlations between two polarities of events to handle noise under
extreme illumination, where different polarities of events exhibit distinctive
noise distributions. Ev-TTA demonstrates a large amount of performance gain on
a wide range of event-based object recognition tasks without extensive
additional training. Our formulation can be successfully applied regardless of
input representations and further extended into regression tasks. We expect
Ev-TTA to provide the key technique to deploy event-based vision algorithms in
challenging real-world applications where significant domain shift is
inevitable.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Event-Based Dense Reconstruction Pipeline. (arXiv:2203.12270v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12270">
<div class="article-summary-box-inner">
<span><p>Event cameras are a new type of sensors that are different from traditional
cameras. Each pixel is triggered asynchronously by event. The trigger event is
the change of the brightness irradiated on the pixel. If the increment or
decrement of brightness is higher than a certain threshold, an event is output.
Compared with traditional cameras, event cameras have the advantages of high
dynamic range and no motion blur. Since events are caused by the apparent
motion of intensity edges, the majority of 3D reconstructed maps consist only
of scene edges, i.e., semi-dense maps, which is not enough for some
applications. In this paper, we propose a pipeline to realize event-based dense
reconstruction. First, deep learning is used to reconstruct intensity images
from events. And then, structure from motion (SfM) is used to estimate camera
intrinsic, extrinsic and sparse point cloud. Finally, multi-view stereo (MVS)
is used to complete dense reconstruction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DAN: a Segmentation-free Document Attention Network for Handwritten Document Recognition. (arXiv:2203.12273v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12273">
<div class="article-summary-box-inner">
<span><p>Unconstrained handwritten document recognition is a challenging computer
vision task. It is traditionally handled by a two-step approach combining line
segmentation followed by text line recognition. For the first time, we propose
an end-to-end segmentation-free architecture for the task of handwritten
document recognition: the Document Attention Network. In addition to the text
recognition, the model is trained to label text parts using begin and end tags
in an XML-like fashion. This model is made up of an FCN encoder for feature
extraction and a stack of transformer decoder layers for a recurrent
token-by-token prediction process. It takes whole text documents as input and
sequentially outputs characters, as well as logical layout tokens. Contrary to
the existing segmentation-based approaches, the model is trained without using
any segmentation label. We achieve competitive results on the READ dataset at
page level, as well as double-page level with a CER of 3.53% and 3.69%,
respectively. We also provide results for the RIMES dataset at page level,
reaching 4.54% of CER.
</p>
<p>We provide all source code and pre-trained model weights at
https://github.com/FactoDeepLearning/DAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cell segmentation from telecentric bright-field transmitted light microscopic images using a Residual Attention U-Net: a case study on HeLa line. (arXiv:2203.12290v1 [q-bio.QM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12290">
<div class="article-summary-box-inner">
<span><p>Living cell segmentation from bright-field light microscopic images is
challenging due to the image complexity and temporal changes in the living
cells. Recently developed deep learning (DL)-based methods became popular in
medical and microscopic image segmentation tasks due to their success and
promising outcomes. The main objective of this paper is to develop a deep
learning, UNet-based method to segment the living cells of the HeLa line in
bright-field transmitted light microscopy. To find the most suitable
architecture for our datasets, we have proposed a residual attention U-Net and
compared it with an attention and a simple U-Net architecture. The attention
mechanism highlights the remarkable features and suppresses activations in the
irrelevant image regions. The residual mechanism overcomes with vanishing
gradient problem. The Mean-IoU score for our datasets reaches 0.9505, 0.9524,
and 0.9530 for the simple, attention, and residual attention U-Net,
respectively. We achieved the most accurate semantic segmentation results in
the Mean-IoU and Dice metrics by applying the residual and attention mechanisms
together. The watershed method applied to this best - Residual Attention -
semantic segmentation result gave the segmentation with the specific
information for each cell.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lane detection with Position Embedding. (arXiv:2203.12301v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12301">
<div class="article-summary-box-inner">
<span><p>Recently, lane detection has made great progress in autonomous driving. RESA
(REcurrent Feature-Shift Aggregator) is based on image segmentation. It
presents a novel module to enrich lane feature after preliminary feature
extraction with an ordinary CNN. For Tusimple dataset, there is not too
complicated scene and lane has more prominent spatial features. On the basis of
RESA, we introduce the method of position embedding to enhance the spatial
features. The experimental results show that this method has achieved the best
accuracy 96.93% on Tusimple dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain-Generalized Textured Surface Anomaly Detection. (arXiv:2203.12304v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12304">
<div class="article-summary-box-inner">
<span><p>Anomaly detection aims to identify abnormal data that deviates from the
normal ones, while typically requiring a sufficient amount of normal data to
train the model for performing this task. Despite the success of recent anomaly
detection methods, performing anomaly detection in an unseen domain remain a
challenging task. In this paper, we address the task of domain-generalized
textured surface anomaly detection. By observing normal and abnormal surface
data across multiple source domains, our model is expected to be generalized to
an unseen textured surface of interest, in which only a small number of normal
data can be observed during testing. Although with only image-level labels
observed in the training data, our patch-based meta-learning model exhibits
promising generalization ability: not only can it generalize to unseen image
domains, but it can also localize abnormal regions in the query image. Our
experiments verify that our model performs favorably against state-of-the-art
anomaly detection and domain generalization approaches in various settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised HDR Imaging from Motion and Exposure Cues. (arXiv:2203.12311v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12311">
<div class="article-summary-box-inner">
<span><p>Recent High Dynamic Range (HDR) techniques extend the capabilities of current
cameras where scenes with a wide range of illumination can not be accurately
captured with a single low-dynamic-range (LDR) image. This is generally
accomplished by capturing several LDR images with varying exposure values whose
information is then incorporated into a merged HDR image. While such approaches
work well for static scenes, dynamic scenes pose several challenges, mostly
related to the difficulty of finding reliable pixel correspondences.
Data-driven approaches tackle the problem by learning an end-to-end mapping
with paired LDR-HDR training data, but in practice generating such HDR
ground-truth labels for dynamic scenes is time-consuming and requires complex
procedures that assume control of certain dynamic elements of the scene (e.g.
actor pose) and repeatable lighting conditions (stop-motion capturing). In this
work, we propose a novel self-supervised approach for learnable HDR estimation
that alleviates the need for HDR ground-truth labels. We propose to leverage
the internal statistics of LDR images to create HDR pseudo-labels. We
separately exploit static and well-exposed parts of the input images, which in
conjunction with synthetic illumination clipping and motion augmentation
provide high quality training examples. Experimental results show that the HDR
models trained using our proposed self-supervision approach achieve performance
competitive with those trained under full supervision, and are to a large
extent superior to previous methods that equally do not require any
supervision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Autofocus for Event Cameras. (arXiv:2203.12321v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12321">
<div class="article-summary-box-inner">
<span><p>Focus control (FC) is crucial for cameras to capture sharp images in
challenging real-world scenarios. The autofocus (AF) facilitates the FC by
automatically adjusting the focus settings. However, due to the lack of
effective AF methods for the recently introduced event cameras, their FC still
relies on naive AF like manual focus adjustments, leading to poor adaptation in
challenging real-world conditions. In particular, the inherent differences
between event and frame data in terms of sensing modality, noise, temporal
resolutions, etc., bring many challenges in designing an effective AF method
for event cameras. To address these challenges, we develop a novel event-based
autofocus framework consisting of an event-specific focus measure called event
rate (ER) and a robust search strategy called event-based golden search (EGS).
To verify the performance of our method, we have collected an event-based
autofocus dataset (EAD) containing well-synchronized frames, events, and focal
positions in a wide variety of challenging scenes with severe lighting and
motion conditions. The experiments on this dataset and additional real-world
scenarios demonstrated the superiority of our method over state-of-the-art
approaches in terms of efficiency and accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DR.VIC: Decomposition and Reasoning for Video Individual Counting. (arXiv:2203.12335v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12335">
<div class="article-summary-box-inner">
<span><p>Pedestrian counting is a fundamental tool for understanding pedestrian
patterns and crowd flow analysis. Existing works (e.g., image-level pedestrian
counting, crossline crowd counting et al.) either only focus on the image-level
counting or are constrained to the manual annotation of lines. In this work, we
propose to conduct the pedestrian counting from a new perspective - Video
Individual Counting (VIC), which counts the total number of individual
pedestrians in the given video (a person is only counted once). Instead of
relying on the Multiple Object Tracking (MOT) techniques, we propose to solve
the problem by decomposing all pedestrians into the initial pedestrians who
existed in the first frame and the new pedestrians with separate identities in
each following frame. Then, an end-to-end Decomposition and Reasoning Network
(DRNet) is designed to predict the initial pedestrian count with the density
estimation method and reason the new pedestrian's count of each frame with the
differentiable optimal transport. Extensive experiments are conducted on two
datasets with congested pedestrians and diverse scenes, demonstrating the
effectiveness of our method over baselines with great superiority in counting
the individual pedestrians. Code: https://github.com/taohan10200/DRNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Binary Morphological Neural Network. (arXiv:2203.12337v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12337">
<div class="article-summary-box-inner">
<span><p>In the last ten years, Convolutional Neural Networks (CNNs) have formed the
basis of deep-learning architectures for most computer vision tasks. However,
they are not necessarily optimal. For example, mathematical morphology is known
to be better suited to deal with binary images. In this work, we create a
morphological neural network that handles binary inputs and outputs. We propose
their construction inspired by CNNs to formulate layers adapted to such images
by replacing convolutions with erosions and dilations. We give explainable
theoretical results on whether or not the resulting learned networks are indeed
morphological operators. We present promising experimental results designed to
learn basic binary operators, and we have made our code publicly available
online.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-time Object Detection for Streaming Perception. (arXiv:2203.12338v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12338">
<div class="article-summary-box-inner">
<span><p>Autonomous driving requires the model to perceive the environment and (re)act
within a low latency for safety. While past works ignore the inevitable changes
in the environment after processing, streaming perception is proposed to
jointly evaluate the latency and accuracy into a single metric for video online
perception. In this paper, instead of searching trade-offs between accuracy and
speed like previous works, we point out that endowing real-time models with the
ability to predict the future is the key to dealing with this problem. We build
a simple and effective framework for streaming perception. It equips a novel
DualFlow Perception module (DFP), which includes dynamic and static flows to
capture the moving trend and basic detection feature for streaming prediction.
Further, we introduce a Trend-Aware Loss (TAL) combined with a trend factor to
generate adaptive weights for objects with different moving speeds. Our simple
method achieves competitive performance on Argoverse-HD dataset and improves
the AP by 4.9% compared to the strong baseline, validating its effectiveness.
Our code will be made available at https://github.com/yancie-yjr/StreamYOLO.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Semi-Supervised Deep Facial Expression Recognition with An Adaptive Confidence Margin. (arXiv:2203.12341v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12341">
<div class="article-summary-box-inner">
<span><p>Only parts of unlabeled data are selected to train models for most
semi-supervised learning methods, whose confidence scores are usually higher
than the pre-defined threshold (i.e., the confidence margin). We argue that the
recognition performance should be further improved by making full use of all
unlabeled data. In this paper, we learn an Adaptive Confidence Margin (Ada-CM)
to fully leverage all unlabeled data for semi-supervised deep facial expression
recognition. All unlabeled samples are partitioned into two subsets by
comparing their confidence scores with the adaptively learned confidence margin
at each training epoch: (1) subset I including samples whose confidence scores
are no lower than the margin; (2) subset II including samples whose confidence
scores are lower than the margin. For samples in subset I, we constrain their
predictions to match pseudo labels. Meanwhile, samples in subset II participate
in the feature-level contrastive objective to learn effective facial expression
features. We extensively evaluate Ada-CM on four challenging datasets, showing
that our method achieves state-of-the-art performance, especially surpassing
fully-supervised baselines in a semi-supervised manner. Ablation study further
proves the effectiveness of our method. The source code is available at
https://github.com/hangyu94/Ada-CM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Do You Do It? Fine-Grained Action Understanding with Pseudo-Adverbs. (arXiv:2203.12344v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12344">
<div class="article-summary-box-inner">
<span><p>We aim to understand how actions are performed and identify subtle
differences, such as 'fold firmly' vs. 'fold gently'. To this end, we propose a
method which recognizes adverbs across different actions. However, such
fine-grained annotations are difficult to obtain and their long-tailed nature
makes it challenging to recognize adverbs in rare action-adverb compositions.
Our approach therefore uses semi-supervised learning with multiple adverb
pseudo-labels to leverage videos with only action labels. Combined with
adaptive thresholding of these pseudo-adverbs we are able to make efficient use
of the available data while tackling the long-tailed distribution.
Additionally, we gather adverb annotations for three existing video retrieval
datasets, which allows us to introduce the new tasks of recognizing adverbs in
unseen action-adverb compositions and unseen domains. Experiments demonstrate
the effectiveness of our method, which outperforms prior work in recognizing
adverbs and semi-supervised works adapted for adverb recognition. We also show
how adverbs can relate fine-grained actions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Text Line Detection in Historical Documents: Learning and Evaluation Methods. (arXiv:2203.12346v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12346">
<div class="article-summary-box-inner">
<span><p>Text line segmentation is one of the key steps in historical document
understanding. It is challenging due to the variety of fonts, contents, writing
styles and the quality of documents that have degraded through the years.
</p>
<p>In this paper, we address the limitations that currently prevent people from
building line segmentation models with a high generalization capacity. We
present a study conducted using three state-of-the-art systems Doc-UFCN,
dhSegment and ARU-Net and show that it is possible to build generic models
trained on a wide variety of historical document datasets that can correctly
segment diverse unseen pages. This paper also highlights the importance of the
annotations used during training: each existing dataset is annotated
differently. We present a unification of the annotations and show its positive
impact on the final text recognition results. In this end, we present a
complete evaluation strategy using standard pixel-level metrics, object-level
ones and introducing goal-oriented metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hyper-Spectral Imaging for Overlapping Plastic Flakes Segmentation. (arXiv:2203.12350v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12350">
<div class="article-summary-box-inner">
<span><p>Given the hyper-spectral imaging unique potentials in grasping the polymer
characteristics of different materials, it is commonly used in sorting
procedures. In a practical plastic sorting scenario, multiple plastic flakes
may overlap which depending on their characteristics, the overlap can be
reflected in their spectral signature. In this work, we use hyper-spectral
imaging for the segmentation of three types of plastic flakes and their
possible overlapping combinations. We propose an intuitive and simple
multi-label encoding approach, bitfield encoding, to account for the
overlapping regions. With our experiments, we show that the bitfield encoding
improves over the baseline single-label approach and we further demonstrate its
potential in predicting multiple labels for overlapping classes even when the
model is only trained with non-overlapping classes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MONAI Label: A framework for AI-assisted Interactive Labeling of 3D Medical Images. (arXiv:2203.12362v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12362">
<div class="article-summary-box-inner">
<span><p>The lack of annotated datasets is a major challenge in training new
task-specific supervised AI algorithms as manual annotation is expensive and
time-consuming. To address this problem, we present MONAI Label, a free and
open-source platform that facilitates the development of AI-based applications
that aim at reducing the time required to annotate 3D medical image datasets.
Through MONAI Label researchers can develop annotation applications focusing on
their domain of expertise. It allows researchers to readily deploy their apps
as services, which can be made available to clinicians via their preferred
user-interface. Currently, MONAI Label readily supports locally installed
(3DSlicer) and web-based (OHIF) frontends, and offers two Active learning
strategies to facilitate and speed up the training of segmentation algorithms.
MONAI Label allows researchers to make incremental improvements to their
labeling apps by making them available to other researchers and clinicians
alike. Lastly, MONAI Label provides sample labeling apps, namely DeepEdit and
DeepGrow, demonstrating dramatically reduced annotation times.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer-based Multimodal Information Fusion for Facial Expression Analysis. (arXiv:2203.12367v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12367">
<div class="article-summary-box-inner">
<span><p>Facial expression analysis has been a crucial research problem in the
computer vision area. With the recent development of deep learning techniques
and large-scale in-the-wild annotated datasets, facial expression analysis is
now aimed at challenges in real world settings. In this paper, we introduce our
submission to CVPR2022 Competition on Affective Behavior Analysis in-the-wild
(ABAW) that defines four competition tasks, including expression
classification, action unit detection, valence-arousal estimation, and a
multi-task-learning. The available multimodal information consist of spoken
words, speech prosody, and visual expression in videos. Our work proposes four
unified transformer-based network frameworks to create the fusion of the above
multimodal information. The preliminary results on the official Aff-Wild2
dataset are reported and demonstrate the effectiveness of our proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the (Limited) Generalization of MasterFace Attacks and Its Relation to the Capacity of Face Representations. (arXiv:2203.12387v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12387">
<div class="article-summary-box-inner">
<span><p>A MasterFace is a face image that can successfully match against a large
portion of the population. Since their generation does not require access to
the information of the enrolled subjects, MasterFace attacks represent a
potential security risk for widely-used face recognition systems. Previous
works proposed methods for generating such images and demonstrated that these
attacks can strongly compromise face recognition. However, previous works
followed evaluation settings consisting of older recognition models, limited
cross-dataset and cross-model evaluations, and the use of low-scale testing
data. This makes it hard to state the generalizability of these attacks. In
this work, we comprehensively analyse the generalizability of MasterFace
attacks in empirical and theoretical investigations. The empirical
investigations include the use of six state-of-the-art FR models, cross-dataset
and cross-model evaluation protocols, and utilizing testing datasets of
significantly higher size and variance. The results indicate a low
generalizability when MasterFaces are training on a different face recognition
model than the one used for testing. In these cases, the attack performance is
similar to zero-effort imposter attacks. In the theoretical investigations, we
define and estimate the face capacity and the maximum MasterFace coverage under
the assumption that identities in the face space are well separated. The
current trend of increasing the fairness and generalizability in face
recognition indicates that the vulnerability of future systems might further
decrease. We conclude that MasterFaces should not be seen as a threat to face
recognition systems but, on the contrary, seen as a tool to understand and
enhance the robustness of face recognition models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">U-Boost NAS: Utilization-Boosted Differentiable Neural Architecture Search. (arXiv:2203.12412v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12412">
<div class="article-summary-box-inner">
<span><p>Optimizing resource utilization in target platforms is key to achieving high
performance during DNN inference. While optimizations have been proposed for
inference latency, memory footprint, and energy consumption, prior
hardware-aware neural architecture search (NAS) methods have omitted resource
utilization, preventing DNNs to take full advantage of the target inference
platforms. Modeling resource utilization efficiently and accurately is
challenging, especially for widely-used array-based inference accelerators such
as Google TPU. In this work, we propose a novel hardware-aware NAS framework
that does not only optimize for task accuracy and inference latency, but also
for resource utilization. We also propose and validate a new computational
model for resource utilization in inference accelerators. By using the proposed
NAS framework and the proposed resource utilization model, we achieve 2.8 - 4x
speedup for DNN inference compared to prior hardware-aware NAS methods while
attaining similar or improved accuracy in image classification on CIFAR-10 and
Imagenet-100 datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Attention-based Method for Action Unit Detection at the 3rd ABAW Competition. (arXiv:2203.12428v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12428">
<div class="article-summary-box-inner">
<span><p>Facial Action Coding System is an approach for modeling the complexity of
human emotional expression. Automatic action unit (AU) detection is a crucial
research area in human-computer interaction. This paper describes our
submission to the third Affective Behavior Analysis in-the-wild (ABAW)
competition 2022. We proposed a method for detecting facial action units in the
video. At the first stage, a lightweight CNN-based feature extractor is
employed to extract the feature map from each video frame. Then, an attention
module is applied to refine the attention map. The attention encoded vector is
derived using a weighted sum of the feature map and the attention scores later.
Finally, the sigmoid function is used at the output layer to make the
prediction suitable for multi-label AUs detection. We achieved a macro F1 score
of 0.48 on the ABAW challenge validation set compared to 0.39 from the baseline
model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SMEMO: Social Memory for Trajectory Forecasting. (arXiv:2203.12446v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12446">
<div class="article-summary-box-inner">
<span><p>Effective modeling of human interactions is of utmost importance when
forecasting behaviors such as future trajectories. Each individual, with its
motion, influences surrounding agents since everyone obeys to social
non-written rules such as collision avoidance or group following. In this paper
we model such interactions, which constantly evolve through time, by looking at
the problem from an algorithmic point of view, i.e. as a data manipulation
task. We present a neural network based on an end-to-end trainable working
memory, which acts as an external storage where information about each agent
can be continuously written, updated and recalled. We show that our method is
capable of learning explainable cause-effect relationships between motions of
different agents, obtaining state-of-the-art results on multiple trajectory
forecasting datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MT-UDA: Towards Unsupervised Cross-modality Medical Image Segmentation with Limited Source Labels. (arXiv:2203.12454v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12454">
<div class="article-summary-box-inner">
<span><p>The success of deep convolutional neural networks (DCNNs) benefits from high
volumes of annotated data. However, annotating medical images is laborious,
expensive, and requires human expertise, which induces the label scarcity
problem. Especially when encountering the domain shift, the problem becomes
more serious. Although deep unsupervised domain adaptation (UDA) can leverage
well-established source domain annotations and abundant target domain data to
facilitate cross-modality image segmentation and also mitigate the label
paucity problem on the target domain, the conventional UDA methods suffer from
severe performance degradation when source domain annotations are scarce. In
this paper, we explore a challenging UDA setting - limited source domain
annotations. We aim to investigate how to efficiently leverage unlabeled data
from the source and target domains with limited source annotations for
cross-modality image segmentation. To achieve this, we propose a new
label-efficient UDA framework, termed MT-UDA, in which the student model
trained with limited source labels learns from unlabeled data of both domains
by two teacher models respectively in a semi-supervised manner. More
specifically, the student model not only distills the intra-domain semantic
knowledge by encouraging prediction consistency but also exploits the
inter-domain anatomical information by enforcing structural consistency.
Consequently, the student model can effectively integrate the underlying
knowledge beneath available data resources to mitigate the impact of source
label scarcity and yield improved cross-modality segmentation performance. We
evaluate our method on MM-WHS 2017 dataset and demonstrate that our approach
outperforms the state-of-the-art methods by a large margin under the
source-label scarcity scenario.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Activation-Based Sampling for Pixel- to Image-Level Aggregation in Weakly-Supervised Segmentation. (arXiv:2203.12459v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12459">
<div class="article-summary-box-inner">
<span><p>Classification networks can be used to localize and segment objects in images
by means of class activation maps (CAMs). However, without pixel-level
annotations, they are known to (1) mainly focus on discriminative regions, and
(2) to produce diffuse CAMs without well-defined prediction contours. In this
work, we approach both problems with two contributions for improving CAM
learning. First, we incorporate importance sampling based on the class-wise
probability mass function induced by the CAMs to produce stochastic image-level
class predictions. This results in CAMs which activate over a larger extent of
the objects. Second, we formulate a feature similarity loss term which aims to
match the prediction contours with edges in the image. As a third contribution,
we conduct experiments on the PASCAL VOC and MS-COCO benchmark datasets to
demonstrate that these modifications significantly increase the performance in
terms of contour accuracy, while being comparable to current state-of-the-art
methods in terms of region similarity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Adapted Random Forest Vision (3DARFV) for Untangling Heterogeneous-Fabric Exceeding Deep Learning Semantic Segmentation Efficiency at the Utmost Accuracy. (arXiv:2203.12469v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12469">
<div class="article-summary-box-inner">
<span><p>Planetary exploration depends heavily on 3D image data to characterize the
static and dynamic properties of the rock and environment. Analyzing 3D images
requires many computations, causing efficiency to suffer lengthy processing
time alongside large energy consumption. High-Performance Computing (HPC)
provides apparent efficiency at the expense of energy consumption. However, for
remote explorations, the conveyed surveillance and the robotized sensing need
faster data analysis with ultimate accuracy to make real-time decisions. In
such environments, access to HPC and energy is limited. Therefore, we realize
that reducing the number of computations to optimal and maintaining the desired
accuracy leads to higher efficiency. This paper demonstrates the semantic
segmentation capability of a probabilistic decision tree algorithm, 3D Adapted
Random Forest Vision (3DARFV), exceeding deep learning algorithm efficiency at
the utmost accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptively Re-weighting Multi-Loss Untrained Transformer for Sparse-View Cone-Beam CT Reconstruction. (arXiv:2203.12476v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12476">
<div class="article-summary-box-inner">
<span><p>Cone-Beam Computed Tomography (CBCT) has been proven useful in diagnosis, but
how to shorten scanning time with lower radiation dosage and how to efficiently
reconstruct 3D image remain as the main issues for clinical practice. The
recent development of tomographic image reconstruction on sparse-view
measurements employs deep neural networks in a supervised way to tackle such
issues, whereas the success of model training requires quantity and quality of
the given paired measurements/images. We propose a novel untrained Transformer
to fit the CBCT inverse solver without training data. It is mainly comprised of
an untrained 3D Transformer of billions of network weights and a multi-level
loss function with variable weights. Unlike conventional deep neural networks
(DNNs), there is no requirement of training steps in our approach. Upon
observing the hardship of optimising Transformer, the variable weights within
the loss function are designed to automatically update together with the
iteration process, ultimately stabilising its optimisation. We evaluate the
proposed approach on two publicly available datasets: SPARE and Walnut. The
results show a significant performance improvement on image quality metrics
with streak artefact reduction in the visualisation. We also provide a clinical
report by an experienced radiologist to assess our reconstructed images in a
diagnosis point of view. The source code and the optimised models are available
from the corresponding author on request at the moment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Deep Learning Framework to Reconstruct Face under Mask. (arXiv:2203.12482v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12482">
<div class="article-summary-box-inner">
<span><p>While deep learning-based image reconstruction methods have shown significant
success in removing objects from pictures, they have yet to achieve acceptable
results for attributing consistency to gender, ethnicity, expression, and other
characteristics like the topological structure of the face. The purpose of this
work is to extract the mask region from a masked image and rebuild the area
that has been detected. This problem is complex because (i) it is difficult to
determine the gender of an image hidden behind a mask, which causes the network
to become confused and reconstruct the male face as a female or vice versa;
(ii) we may receive images from multiple angles, making it extremely difficult
to maintain the actual shape, topological structure of the face and a natural
image; and (iii) there are problems with various mask forms because, in some
cases, the area of the mask cannot be anticipated precisely; certain parts of
the mask remain on the face after completion. To solve this complex task, we
split the problem into three phases: landmark detection, object detection for
the targeted mask area, and inpainting the addressed mask region. To begin, to
solve the first problem, we have used gender classification, which detects the
actual gender behind a mask, then we detect the landmark of the masked facial
image. Second, we identified the non-face item, i.e., the mask, and used the
Mask R-CNN network to create the binary mask of the observed mask area.
Thirdly, we developed an inpainting network that uses anticipated landmarks to
create realistic images. To segment the mask, this article uses a mask R-CNN
and offers a binary segmentation map for identifying the mask area.
Additionally, we generated the image utilizing landmarks as structural guidance
through a GAN-based network. The studies presented in this paper use the FFHQ
and CelebA datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CroMo: Cross-Modal Learning for Monocular Depth Estimation. (arXiv:2203.12485v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12485">
<div class="article-summary-box-inner">
<span><p>Learning-based depth estimation has witnessed recent progress in multiple
directions; from self-supervision using monocular video to supervised methods
offering highest accuracy. Complementary to supervision, further boosts to
performance and robustness are gained by combining information from multiple
signals. In this paper we systematically investigate key trade-offs associated
with sensor and modality design choices as well as related model training
strategies. Our study leads us to a new method, capable of connecting
modality-specific advantages from polarisation, Time-of-Flight and
structured-light inputs. We propose a novel pipeline capable of estimating
depth from monocular polarisation for which we evaluate various training
signals. The inversion of differentiable analytic models thereby connects scene
geometry with polarisation and ToF signals and enables self-supervised and
cross-modal learning. In the absence of existing multimodal datasets, we
examine our approach with a custom-made multi-modal camera rig and collect
CroMo; the first dataset to consist of synchronized stereo polarisation,
indirect ToF and structured-light depth, captured at video rates. Extensive
experiments on challenging video scenes confirm both qualitative and
quantitative pipeline advantages where we are able to outperform competitive
monocular depth estimation method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Refine-Net: Normal Refinement Neural Network for Noisy Point Clouds. (arXiv:2203.12514v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12514">
<div class="article-summary-box-inner">
<span><p>Point normal, as an intrinsic geometric property of 3D objects, not only
serves conventional geometric tasks such as surface consolidation and
reconstruction, but also facilitates cutting-edge learning-based techniques for
shape analysis and generation. In this paper, we propose a normal refinement
network, called Refine-Net, to predict accurate normals for noisy point clouds.
Traditional normal estimation wisdom heavily depends on priors such as surface
shapes or noise distributions, while learning-based solutions settle for single
types of hand-crafted features. Differently, our network is designed to refine
the initial normal of each point by extracting additional information from
multiple feature representations. To this end, several feature modules are
developed and incorporated into Refine-Net by a novel connection module.
Besides the overall network architecture of Refine-Net, we propose a new
multi-scale fitting patch selection scheme for the initial normal estimation,
by absorbing geometry domain knowledge. Also, Refine-Net is a generic normal
estimation framework: 1) point normals obtained from other methods can be
further refined, and 2) any feature module related to the surface geometric
structures can be potentially integrated into the framework. Qualitative and
quantitative evaluations demonstrate the clear superiority of Refine-Net over
the state-of-the-arts on both synthetic and real-scanned datasets. Our code is
available at https://github.com/hrzhou2/refinenet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-label Transformer for Action Unit Detection. (arXiv:2203.12531v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12531">
<div class="article-summary-box-inner">
<span><p>Action Unit (AU) Detection is the branch of affective computing that aims at
recognizing unitary facial muscular movements. It is key to unlock unbiaised
computational face representations and has therefore aroused great interest in
the past few years. One of main obstacles toward building efficient deep
learning based AU detection system facial images database annotated by AU
experts. In that extent the ABAW challenge paves the way toward better AU
detection as it involves a ~2M frames AU annotated dataset. In this paper, we
present our submission to the ABAW3 challenge. In a nutshell, we applied a
multi-label detection transformer that leverage multi-head attention to learn
which part of the face image is the most relevant to predict each AU.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GriTS: Grid table similarity metric for table structure recognition. (arXiv:2203.12555v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12555">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a new class of evaluation metric for table
structure recognition, grid table similarity (GriTS). Unlike prior metrics,
GriTS evaluates the correctness of a predicted table directly in its natural
form as a matrix. To create a similarity measure between matrices, we
generalize the two-dimensional largest common substructure (2D-LCS) problem,
which is NP-hard, to the 2D most similar substructures (2D-MSS) problem and
propose a polynomial-time heuristic for solving it. We validate empirically
using the PubTables-1M dataset that comparison between matrices exhibits more
desirable behavior than alternatives for table structure recognition
evaluation. GriTS also unifies all three subtasks of cell topology recognition,
cell location recognition, and cell content recognition within the same
framework, which simplifies the evaluation and enables more meaningful
comparisons across different types of structure recognition approaches. Code
will be released at https://github.com/microsoft/table-transformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DynamicEarthNet: Daily Multi-Spectral Satellite Dataset for Semantic Change Segmentation. (arXiv:2203.12560v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12560">
<div class="article-summary-box-inner">
<span><p>Earth observation is a fundamental tool for monitoring the evolution of land
use in specific areas of interest. Observing and precisely defining change, in
this context, requires both time-series data and pixel-wise segmentations. To
that end, we propose the DynamicEarthNet dataset that consists of daily,
multi-spectral satellite observations of 75 selected areas of interest
distributed over the globe with imagery from Planet Labs. These observations
are paired with pixel-wise monthly semantic segmentation labels of 7 land use
and land cover (LULC) classes. DynamicEarthNet is the first dataset that
provides this unique combination of daily measurements and high-quality labels.
In our experiments, we compare several established baselines that either
utilize the daily observations as additional training data (semi-supervised
learning) or multiple observations at once (spatio-temporal learning) as a
point of reference for future research. Finally, we propose a new evaluation
metric SCS that addresses the specific challenges associated with time-series
semantic change segmentation. The data is available at:
https://mediatum.ub.tum.<a href="/abs/de/1650201">de/1650201</a>.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Your "Attention" Deserves Attention: A Self-Diversified Multi-Channel Attention for Facial Action Analysis. (arXiv:2203.12570v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12570">
<div class="article-summary-box-inner">
<span><p>Visual attention has been extensively studied for learning fine-grained
features in both facial expression recognition (FER) and Action Unit (AU)
detection. A broad range of previous research has explored how to use attention
modules to localize detailed facial parts (e,g. facial action units), learn
discriminative features, and learn inter-class correlation. However, few
related works pay attention to the robustness of the attention module itself.
Through experiments, we found neural attention maps initialized with different
feature maps yield diverse representations when learning to attend the
identical Region of Interest (ROI). In other words, similar to general feature
learning, the representational quality of attention maps also greatly affects
the performance of a model, which means unconstrained attention learning has
lots of randomnesses. This uncertainty lets conventional attention learning
fall into sub-optimal. In this paper, we propose a compact model to enhance the
representational and focusing power of neural attention maps and learn the
"inter-attention" correlation for refined attention maps, which we term the
"Self-Diversified Multi-Channel Attention Network (SMA-Net)". The proposed
method is evaluated on two benchmark databases (BP4D and DISFA) for AU
detection and four databases (CK+, MMI, BU-3DFE, and BP4D+) for facial
expression recognition. It achieves superior performance compared to the
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeuMan: Neural Human Radiance Field from a Single Video. (arXiv:2203.12575v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12575">
<div class="article-summary-box-inner">
<span><p>Photorealistic rendering and reposing of humans is important for enabling
augmented reality experiences. We propose a novel framework to reconstruct the
human and the scene that can be rendered with novel human poses and views from
just a single in-the-wild video. Given a video captured by a moving camera, we
train two NeRF models: a human NeRF model and a scene NeRF model. To train
these models, we rely on existing methods to estimate the rough geometry of the
human and the scene. Those rough geometry estimates allow us to create a
warping field from the observation space to the canonical pose-independent
space, where we train the human model in. Our method is able to learn subject
specific details, including cloth wrinkles and accessories, from just a 10
seconds video clip, and to provide high quality renderings of the human under
novel poses, from novel views, together with the background.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">R3M: A Universal Visual Representation for Robot Manipulation. (arXiv:2203.12601v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12601">
<div class="article-summary-box-inner">
<span><p>We study how visual representations pre-trained on diverse human video data
can enable data-efficient learning of downstream robotic manipulation tasks.
Concretely, we pre-train a visual representation using the Ego4D human video
dataset using a combination of time-contrastive learning, video-language
alignment, and an L1 penalty to encourage sparse and compact representations.
The resulting representation, R3M, can be used as a frozen perception module
for downstream policy learning. Across a suite of 12 simulated robot
manipulation tasks, we find that R3M improves task success by over 20% compared
to training from scratch and by over 10% compared to state-of-the-art visual
representations like CLIP and MoCo. Furthermore, R3M enables a Franka Emika
Panda arm to learn a range of manipulation tasks in a real, cluttered apartment
given just 20 demonstrations. Code and pre-trained models are available at
https://tinyurl.com/robotr3m.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training. (arXiv:2203.12602v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12602">
<div class="article-summary-box-inner">
<span><p>Pre-training video transformers on extra large-scale datasets is generally
required to achieve premier performance on relatively small datasets. In this
paper, we show that video masked autoencoders (VideoMAE) are data-efficient
learners for self-supervised video pre-training (SSVP). We are inspired by the
recent ImageMAE and propose customized video tube masking and reconstruction.
These simple designs turn out to be effective for overcoming information
leakage caused by the temporal correlation during video reconstruction. We
obtain three important findings on SSVP: (1) An extremely high proportion of
masking ratio (i.e., 90% to 95%) still yields favorable performance of
VideoMAE. The temporally redundant video content enables higher masking ratio
than that of images. (2) VideoMAE achieves impressive results on very small
datasets (i.e., around 3k-4k videos) without using any extra data. This is
partially ascribed to the challenging task of video reconstruction to enforce
high-level structure learning. (3) VideoMAE shows that data quality is more
important than data quantity for SSVP. Domain shift between pre-training and
target datasets are important issues in SSVP. Notably, our VideoMAE with the
vanilla ViT backbone can achieve 83.9% on Kinects-400, 75.3% on
Something-Something V2, 90.8% on UCF101, and 61.1% on HMDB51 without using any
extra data. Code will be released at https://github.com/MCG-NJU/VideoMAE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving the Fairness of Chest X-ray Classifiers. (arXiv:2203.12609v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12609">
<div class="article-summary-box-inner">
<span><p>Deep learning models have reached or surpassed human-level performance in the
field of medical imaging, especially in disease diagnosis using chest x-rays.
However, prior work has found that such classifiers can exhibit biases in the
form of gaps in predictive performance across protected groups. In this paper,
we question whether striving to achieve zero disparities in predictive
performance (i.e. group fairness) is the appropriate fairness definition in the
clinical setting, over minimax fairness, which focuses on maximizing the
performance of the worst-case group. We benchmark the performance of nine
methods in improving classifier fairness across these two definitions. We find,
consistent with prior work on non-clinical data, that methods which strive to
achieve better worst-group performance do not outperform simple data balancing.
We also find that methods which achieve group fairness do so by worsening
performance for all groups. In light of these results, we discuss the utility
of fairness definitions in the clinical setting, advocating for an
investigation of the bias-inducing mechanisms in the underlying data generating
process whenever possible.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StructToken : Rethinking Semantic Segmentation with Structural Prior. (arXiv:2203.12612v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12612">
<div class="article-summary-box-inner">
<span><p>In this paper, we present structure token (StructToken), a new paradigm for
semantic segmentation. From a perspective on semantic segmentation as per-pixel
classification, the previous deep learning-based methods learn the per-pixel
representation first through an encoder and a decoder head and then classify
each pixel representation to a specific category to obtain the semantic masks.
Differently, we propose a structure-aware algorithm that takes structural
information as prior to predict semantic masks directly without per-pixel
classification. Specifically, given an input image, the learnable structure
token interacts with the image representations to reason the final semantic
masks. Three interaction approaches are explored and the results not only
outperform the state-of-the-art methods but also contain more structural
information. Experiments are conducted on three widely used datasets including
ADE20k, Cityscapes, and COCO-Stuff 10K. We hope that structure token could
serve as an alternative for semantic segmentation and inspire future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Hybrid Mesh-neural Representation for 3D Transparent Object Reconstruction. (arXiv:2203.12613v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12613">
<div class="article-summary-box-inner">
<span><p>We propose a novel method to reconstruct the 3D shapes of transparent objects
using hand-held captured images under natural light conditions. It combines the
advantage of explicit mesh and multi-layer perceptron (MLP) network, a hybrid
representation, to simplify the capture setting used in recent contributions.
After obtaining an initial shape through the multi-view silhouettes, we
introduce surface-based local MLPs to encode the vertex displacement field
(VDF) for the reconstruction of surface details. The design of local MLPs
allows to represent the VDF in a piece-wise manner using two layer MLP
networks, which is beneficial to the optimization algorithm. Defining local
MLPs on the surface instead of the volume also reduces the searching space.
Such a hybrid representation enables us to relax the ray-pixel correspondences
that represent the light path constraint to our designed ray-cell
correspondences, which significantly simplifies the implementation of
single-image based environment matting algorithm. We evaluate our
representation and reconstruction algorithm on several transparent objects with
ground truth models. Our experiments show that our method can produce
high-quality reconstruction results superior to state-of-the-art methods using
a simplified data acquisition setup.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Salient Object Detection with Spectral Cluster Voting. (arXiv:2203.12614v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.12614">
<div class="article-summary-box-inner">
<span><p>In this paper, we tackle the challenging task of unsupervised salient object
detection (SOD) by leveraging spectral clustering on self-supervised features.
We make the following contributions: (i) We revisit spectral clustering and
demonstrate its potential to group the pixels of salient objects; (ii) Given
mask proposals from multiple applications of spectral clustering on image
features computed from various self-supervised models, e.g., MoCov2, SwAV,
DINO, we propose a simple but effective winner-takes-all voting mechanism for
selecting the salient masks, leveraging object priors based on framing and
distinctiveness; (iii) Using the selected object segmentation as pseudo
groundtruth masks, we train a salient object detector, dubbed SelfMask, which
outperforms prior approaches on three unsupervised SOD benchmarks. Code is
publicly available at https://github.com/NoelShin/selfmask.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inferring Restaurant Styles by Mining Crowd Sourced Photos from User-Review Websites. (arXiv:1611.06301v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1611.06301">
<div class="article-summary-box-inner">
<span><p>When looking for a restaurant online, user uploaded photos often give people
an immediate and tangible impression about a restaurant. Due to their
informativeness, such user contributed photos are leveraged by restaurant
review websites to provide their users an intuitive and effective search
experience. In this paper, we present a novel approach to inferring restaurant
types or styles (ambiance, dish styles, suitability for different occasions)
from user uploaded photos on user-review websites. To that end, we first
collect a novel restaurant photo dataset associating the user contributed
photos with the restaurant styles from TripAdvior. We then propose a deep
multi-instance multi-label learning (MIML) framework to deal with the unique
problem setting of the restaurant style classification task. We employ a
two-step bootstrap strategy to train a multi-label convolutional neural network
(CNN). The multi-label CNN is then used to compute the confidence scores of
restaurant styles for all the images associated with a restaurant. The computed
confidence scores are further used to train a final binary classifier for each
restaurant style tag. Upon training, the styles of a restaurant can be profiled
by analyzing restaurant photos with the trained multi-label CNN and SVM models.
Experimental evaluation has demonstrated that our crowd sourcing-based approach
can effectively infer the restaurant style when there are a sufficient number
of user uploaded photos for a given restaurant.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-Volume Preserving-based Fusion to Group-Level Emotion Recognition on Crowd Videos. (arXiv:1811.11849v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1811.11849">
<div class="article-summary-box-inner">
<span><p>Group-level emotion recognition (ER) is a growing research area as the
demands for assessing crowds of all sizes are becoming an interest in both the
security arena as well as social media. This work extends the earlier ER
investigations, which focused on either group-level ER on single images or
within a video, by fully investigating group-level expression recognition on
crowd videos. In this paper, we propose an effective deep feature level fusion
mechanism to model the spatial-temporal information in the crowd videos. In our
approach, the fusing process is performed on the deep feature domain by a
generative probabilistic model, Non-Volume Preserving Fusion (NVPF), that
models spatial information relationships. Furthermore, we extend our proposed
spatial NVPF approach to the spatial-temporal NVPF approach to learn the
temporal information between frames. To demonstrate the robustness and
effectiveness of each component in the proposed approach, three experiments
were conducted: (i) evaluation on AffectNet database to benchmark the proposed
EmoNet for recognizing facial expression; (ii) evaluation on EmotiW2018 to
benchmark the proposed deep feature level fusion mechanism NVPF; and, (iii)
examine the proposed TNVPF on an innovative Group-level Emotion on Crowd Videos
(GECV) dataset composed of 627 videos collected from publicly available
sources. GECV dataset is a collection of videos containing crowds of people.
Each video is labeled with emotion categories at three levels: individual
faces, group of people, and the entire video frame.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Face Completion with Semantic Knowledge and Collaborative Adversarial Learning. (arXiv:1812.03252v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1812.03252">
<div class="article-summary-box-inner">
<span><p>Unlike a conventional background inpainting approach that infers a missing
area from image patches similar to the background, face completion requires
semantic knowledge about the target object for realistic outputs. Current image
inpainting approaches utilize generative adversarial networks (GANs) to achieve
such semantic understanding. However, in adversarial learning, the semantic
knowledge is learned implicitly and hence good semantic understanding is not
always guaranteed. In this work, we propose a collaborative adversarial
learning approach to face completion to explicitly induce the training process.
Our method is formulated under a novel generative framework called
collaborative GAN (collaGAN), which allows better semantic understanding of a
target object through collaborative learning of multiple tasks including face
completion, landmark detection, and semantic segmentation. Together with the
collaGAN, we also introduce an inpainting concentrated scheme such that the
model emphasizes more on inpainting instead of autoencoding. Extensive
experiments show that the proposed designs are indeed effective and
collaborative adversarial learning provides better feature representations of
the faces. In comparison with other generative image inpainting models and
single task learning methods, our solution produces superior performances on
all tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">More Knowledge is Better: Cross-Modality Volume Completion and 3D+2D Segmentation for Intracardiac Echocardiography Contouring. (arXiv:1812.03507v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1812.03507">
<div class="article-summary-box-inner">
<span><p>Using catheter ablation to treat atrial fibrillation increasingly relies on
intracardiac echocardiography (ICE) for an anatomical delineation of the left
atrium and the pulmonary veins that enter the atrium. However, it is a
challenge to build an automatic contouring algorithm because ICE is noisy and
provides only a limited 2D view of the 3D anatomy. This work provides the first
automatic solution to segment the left atrium and the pulmonary veins from ICE.
In this solution, we demonstrate the benefit of building a cross-modality
framework that can leverage a database of diagnostic images to supplement the
less available interventional images. To this end, we develop a novel deep
neural network approach that uses the (i) 3D geometrical information provided
by a position sensor embedded in the ICE catheter and the (ii) 3D image
appearance information from a set of computed tomography cardiac volumes. We
evaluate the proposed approach over 11,000 ICE images collected from 150
clinical patients. Experimental results show that our model is significantly
better than a direct 2D image-to-image deep neural network segmentation,
especially for less-observed structures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Skin Disease Classification versus Skin Lesion Characterization: Achieving Robust Diagnosis using Multi-label Deep Neural Networks. (arXiv:1812.03520v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1812.03520">
<div class="article-summary-box-inner">
<span><p>In this study, we investigate what a practically useful approach is in order
to achieve robust skin disease diagnosis. A direct approach is to target the
ground truth diagnosis labels, while an alternative approach instead focuses on
determining skin lesion characteristics that are more visually consistent and
discernible. We argue that, for computer-aided skin disease diagnosis, it is
both more realistic and more useful that lesion type tags should be considered
as the target of an automated diagnosis system such that the system can first
achieve a high accuracy in describing skin lesions, and in turn facilitate
disease diagnosis using lesion characteristics in conjunction with other
evidence. To further meet such an objective, we employ convolutional neural
networks (CNNs) for both the disease-targeted and lesion-targeted
classifications. We have collected a large-scale and diverse dataset of 75,665
skin disease images from six publicly available dermatology atlantes. Then we
train and compare both disease-targeted and lesion-targeted classifiers,
respectively. For disease-targeted classification, only 27.6% top-1 accuracy
and 57.9% top-5 accuracy are achieved with a mean average precision (mAP) of
0.42. In contrast, for lesion-targeted classification, we can achieve a much
higher mAP of 0.70.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Deep Multi-task Learning Approach to Skin Lesion Classification. (arXiv:1812.03527v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1812.03527">
<div class="article-summary-box-inner">
<span><p>Skin lesion identification is a key step toward dermatological diagnosis.
When describing a skin lesion, it is very important to note its body site
distribution as many skin diseases commonly affect particular parts of the
body. To exploit the correlation between skin lesions and their body site
distributions, in this study, we investigate the possibility of improving skin
lesion classification using the additional context information provided by body
location. Specifically, we build a deep multi-task learning (MTL) framework to
jointly optimize skin lesion classification and body location classification
(the latter is used as an inductive bias). Our MTL framework uses the
state-of-the-art ImageNet pretrained model with specialized loss functions for
the two related tasks. Our experiments show that the proposed MTL based method
performs more robustly than its standalone (single-task) counterpart.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Mask Pyramid Network for CT/CBCT Metal Artifact Reduction with Joint Projection-Sinogram Correction. (arXiv:1907.00294v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1907.00294">
<div class="article-summary-box-inner">
<span><p>A conventional approach to computed tomography (CT) or cone beam CT (CBCT)
metal artifact reduction is to replace the X-ray projection data within the
metal trace with synthesized data. However, existing projection or sinogram
completion methods cannot always produce anatomically consistent information to
fill the metal trace, and thus, when the metallic implant is large, significant
secondary artifacts are often introduced. In this work, we propose to replace
metal artifact affected regions with anatomically consistent content through
joint projection-sinogram correction as well as adversarial learning. To handle
the metallic implants of diverse shapes and large sizes, we also propose a
novel mask pyramid network that enforces the mask information across the
network's encoding layers and a mask fusion loss that reduces early saturation
of adversarial training. Our experimental results show that the proposed
projection-sinogram correction designs are effective and our method recovers
information from the metal traces better than the state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Arbitrary-Oriented Object Detection: Classification based Approaches Revisited. (arXiv:2003.05597v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.05597">
<div class="article-summary-box-inner">
<span><p>Arbitrary-oriented object detection has been a building block for rotation
sensitive tasks. We first show that the boundary problem suffered in existing
dominant regression-based rotation detectors, is caused by angular periodicity
or corner ordering, according to the parameterization protocol. We also show
that the root cause is that the ideal predictions can be out of the defined
range. Accordingly, we transform the angular prediction task from a regression
problem to a classification one. For the resulting circularly distributed angle
classification problem, we first devise a Circular Smooth Label technique to
handle the periodicity of angle and increase the error tolerance to adjacent
angles. To reduce the excessive model parameters by Circular Smooth Label, we
further design a Densely Coded Labels, which greatly reduces the length of the
encoding. Finally, we further develop an object heading detection module, which
can be useful when the exact heading orientation information is needed e.g. for
ship and plane heading detection. We release our OHD-SJTU dataset and OHDet
detector for heading detection. Extensive experimental results on three
large-scale public datasets for aerial images i.e. DOTA, HRSC2016, OHD-SJTU,
and face dataset FDDB, as well as scene text dataset ICDAR2015 and MLT, show
the effectiveness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incremental Few-Shot Object Detection for Robotics. (arXiv:2005.02641v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.02641">
<div class="article-summary-box-inner">
<span><p>Incremental few-shot learning is highly expected for practical robotics
applications. On one hand, robot is desired to learn new tasks quickly and
flexibly using only few annotated training samples; on the other hand, such new
additional tasks should be learned in a continuous and incremental manner
without forgetting the previous learned knowledge dramatically. In this work,
we propose a novel Class-Incremental Few-Shot Object Detection (CI-FSOD)
framework that enables deep object detection network to perform effective
continual learning from just few-shot samples without re-accessing the previous
training data. We achieve this by equipping the widely-used Faster-RCNN
detector with three elegant components. Firstly, to best preserve performance
on the pre-trained base classes, we propose a novel Dual-Embedding-Space (DES)
architecture which decouples the representation learning of base and novel
categories into different spaces. Secondly, to mitigate the catastrophic
forgetting on the accumulated novel classes, we propose a Sequential Model
Fusion (SMF) method, which is able to achieve long-term memory without
additional storage cost. Thirdly, to promote inter-task class separation in
feature space, we propose a novel regularization technique that extends the
classification boundary further away from the previous classes to avoid
misclassification. Overall, our framework is simple yet effective and
outperforms the previous SOTA with a significant margin of 2.4 points in AP
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Anticipation Tasks: Uncertainty-aware Anticipation of Sparse Surgical Instrument Usage for Context-aware Assistance. (arXiv:2007.00548v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.00548">
<div class="article-summary-box-inner">
<span><p>Intra-operative anticipation of instrument usage is a necessary component for
context-aware assistance in surgery, e.g. for instrument preparation or
semi-automation of robotic tasks. However, the sparsity of instrument
occurrences in long videos poses a challenge. Current approaches are limited as
they assume knowledge on the timing of future actions or require dense temporal
segmentations during training and inference. We propose a novel learning task
for anticipation of instrument usage in laparoscopic videos that overcomes
these limitations. During training, only sparse instrument annotations are
required and inference is done solely on image data. We train a probabilistic
model to address the uncertainty associated with future events. Our approach
outperforms several baselines and is competitive to a variant using richer
annotations. We demonstrate the model's ability to quantify task-relevant
uncertainties. To the best of our knowledge, we are the first to propose a
method for anticipating instruments in surgery.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Representation Decomposition for Image Manipulation and Beyond. (arXiv:2011.00788v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.00788">
<div class="article-summary-box-inner">
<span><p>Representation disentanglement aims at learning interpretable features, so
that the output can be recovered or manipulated accordingly. While existing
works like infoGAN and AC-GAN exist, they choose to derive disjoint attribute
code for feature disentanglement, which is not applicable for existing/trained
generative models. In this paper, we propose a decomposition-GAN (dec-GAN),
which is able to achieve the decomposition of an existing latent representation
into content and attribute features. Guided by the classifier pre-trained on
the attributes of interest, our dec-GAN decomposes the attributes of interest
from the latent representation, while data recovery and feature consistency
objectives enforce the learning of our proposed method. Our experiments on
multiple image datasets confirm the effectiveness and robustness of our dec-GAN
over recent representation disentanglement models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PLAD: Learning to Infer Shape Programs with Pseudo-Labels and Approximate Distributions. (arXiv:2011.13045v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.13045">
<div class="article-summary-box-inner">
<span><p>Inferring programs which generate 2D and 3D shapes is important for reverse
engineering, editing, and more. Training models to perform this task is
complicated because paired (shape, program) data is not readily available for
many domains, making exact supervised learning infeasible. However, it is
possible to get paired data by compromising the accuracy of either the assigned
program labels or the shape distribution. Wake-sleep methods use samples from a
generative model of shape programs to approximate the distribution of real
shapes. In self-training, shapes are passed through a recognition model, which
predicts programs that are treated as "pseudo-labels" for those shapes. Related
to these approaches, we introduce a novel self-training variant unique to
program inference, where program pseudo-labels are paired with their executed
output shapes, avoiding label mismatch at the cost of an approximate shape
distribution. We propose to group these regimes under a single conceptual
framework, where training is performed with maximum likelihood updates sourced
from either Pseudo-Labels or an Approximate Distribution (PLAD). We evaluate
these techniques on multiple 2D and 3D shape program inference domains.
Compared with policy gradient reinforcement learning, we show that PLAD
techniques infer more accurate shape programs and converge significantly
faster. Finally, we propose to combine updates from different PLAD methods
within the training of a single model, and find that this approach outperforms
any individual technique.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GAN Inversion: A Survey. (arXiv:2101.05278v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.05278">
<div class="article-summary-box-inner">
<span><p>GAN inversion aims to invert a given image back into the latent space of a
pretrained GAN model, for the image to be faithfully reconstructed from the
inverted code by the generator. As an emerging technique to bridge the real and
fake image domains, GAN inversion plays an essential role in enabling the
pretrained GAN models such as StyleGAN and BigGAN to be used for real image
editing applications. Meanwhile, GAN inversion also provides insights on the
interpretation of GAN's latent space and how the realistic images can be
generated. In this paper, we provide an overview of GAN inversion with a focus
on its recent algorithms and applications. We cover important techniques of GAN
inversion and their applications to image restoration and image manipulation.
We further elaborate on some trends and challenges for future directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Countering Malicious DeepFakes: Survey, Battleground, and Horizon. (arXiv:2103.00218v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.00218">
<div class="article-summary-box-inner">
<span><p>The creation or manipulation of facial appearance through deep generative
approaches, known as DeepFake, have achieved significant progress and promoted
a wide range of benign and malicious applications, e.g., visual effect
assistance in movie and misinformation generation by faking famous persons. The
evil side of this new technique poses another popular study, i.e., DeepFake
detection aiming to identify the fake faces from the real ones. With the rapid
development of the DeepFake-related studies in the community, both sides have
formed the relationship of battleground, pushing the improvements of each other
and inspiring new directions, e.g., the evasion of DeepFake detection.
Nevertheless, the overview of such battleground and the new direction is
unclear and neglected by recent surveys due to the rapid increase of related
publications, limiting the in-depth understanding of the tendency and future
works. To fill this gap, in this paper, we provide a comprehensive overview and
detailed analysis of the research work on the topic of DeepFake generation,
DeepFake detection as well as evasion of DeepFake detection, with more than 318
research papers carefully surveyed. We present the taxonomy of various DeepFake
generation methods and the categorization of various DeepFake detection
methods, and more importantly, we showcase the battleground between the two
parties with detailed interactions between the adversaries (DeepFake
generation) and the defenders (DeepFake detection). The battleground allows
fresh perspective into the latest landscape of the DeepFake research and can
provide valuable analysis towards the research challenges and opportunities as
well as research trends and future directions. We also elaborately design
interactive diagrams (<a href="http://www.xujuefei.com/dfsurvey">this http URL</a>) to allow researchers to
explore their own interests on popular DeepFake generators or detectors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FIBER: Fill-in-the-Blanks as a Challenging Video Understanding Evaluation Framework. (arXiv:2104.04182v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.04182">
<div class="article-summary-box-inner">
<span><p>We propose fill-in-the-blanks as a video understanding evaluation framework
and introduce FIBER -- a novel dataset consisting of 28,000 videos and
descriptions in support of this evaluation framework. The fill-in-the-blanks
setting tests a model's understanding of a video by requiring it to predict a
masked noun phrase in the caption of the video, given the video and the
surrounding text. The FIBER benchmark does not share the weaknesses of the
current state-of-the-art language-informed video understanding tasks, namely:
(1) video question answering using multiple-choice questions, where models
perform relatively well because they exploit linguistic biases in the task
formulation, thus making our framework challenging for the current
state-of-the-art systems to solve; and (2) video captioning, which relies on an
open-ended evaluation framework that is often inaccurate because system answers
may be perceived as incorrect if they differ in form from the ground truth. The
FIBER dataset and our code are available at https://lit.eecs.umich.edu/fiber/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SQN: Weakly-Supervised Semantic Segmentation of Large-Scale 3D Point Clouds. (arXiv:2104.04891v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.04891">
<div class="article-summary-box-inner">
<span><p>Labelling point clouds fully is highly time-consuming and costly. As larger
point cloud datasets with billions of points become more common, we ask whether
the full annotation is even necessary, demonstrating that existing baselines
designed under a fully annotated assumption only degrade slightly even when
faced with 1% random point annotations. However, beyond this point, e.g., at
0.1% annotations, segmentation accuracy is unacceptably low. We observe that,
as point clouds are samples of the 3D world, the distribution of points in a
local neighborhood is relatively homogeneous, exhibiting strong semantic
similarity. Motivated by this, we propose a new weak supervision method to
implicitly augment highly sparse supervision signals. Extensive experiments
demonstrate the proposed Semantic Query Network (SQN) achieves promising
performance on seven large-scale open datasets under weak supervision schemes,
while requiring only 0.1% randomly annotated points for training, greatly
reducing annotation cost and effort. The code is available at
https://github.com/QingyongHu/SQN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ShapeMOD: Macro Operation Discovery for 3D Shape Programs. (arXiv:2104.06392v3 [cs.GR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06392">
<div class="article-summary-box-inner">
<span><p>A popular way to create detailed yet easily controllable 3D shapes is via
procedural modeling, i.e. generating geometry using programs. Such programs
consist of a series of instructions along with their associated parameter
values. To fully realize the benefits of this representation, a shape program
should be compact and only expose degrees of freedom that allow for meaningful
manipulation of output geometry. One way to achieve this goal is to design
higher-level macro operators that, when executed, expand into a series of
commands from the base shape modeling language. However, manually authoring
such macros, much like shape programs themselves, is difficult and largely
restricted to domain experts. In this paper, we present ShapeMOD, an algorithm
for automatically discovering macros that are useful across large datasets of
3D shape programs. ShapeMOD operates on shape programs expressed in an
imperative, statement-based language. It is designed to discover macros that
make programs more compact by minimizing the number of function calls and free
parameters required to represent an input shape collection. We run ShapeMOD on
multiple collections of programs expressed in a domain-specific language for 3D
shape structures. We show that it automatically discovers a concise set of
macros that abstract out common structural and parametric patterns that
generalize over large shape collections. We also demonstrate that the macros
found by ShapeMOD improve performance on downstream tasks including shape
generative modeling and inferring programs from point clouds. Finally, we
conduct a user study that indicates that ShapeMOD's discovered macros make
interactive shape editing more efficient.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Programmable 3D snapshot microscopy with Fourier convolutional networks. (arXiv:2104.10611v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.10611">
<div class="article-summary-box-inner">
<span><p>3D snapshot microscopy enables fast volumetric imaging by capturing a 3D
volume in a single 2D camera image and performing computational reconstruction.
Fast volumetric imaging has a variety of biological applications such as whole
brain imaging of rapid neural activity in larval zebrafish. The optimal
microscope design for this optical 3D-to-2D encoding is both sample- and
task-dependent, with no general solution known. Deep learning based decoders
can be combined with a differentiable simulation of an optical encoder for
end-to-end optimization of both the deep learning decoder and optical encoder.
This technique has been used to engineer local optical encoders for other
problems such as depth estimation, 3D particle localization, and lensless
photography. However, 3D snapshot microscopy is known to require a highly
non-local optical encoder which existing UNet-based decoders are not able to
engineer. We show that a neural network architecture based on global kernel
Fourier convolutional neural networks can efficiently decode information from
multiple depths in a volume, globally encoded across a 3D snapshot image. We
show in simulation that our proposed networks succeed in engineering and
reconstructing optical encoders for 3D snapshot microscopy where the existing
state-of-the-art UNet architecture fails. We also show that our networks
outperform the state-of-the-art learned reconstruction algorithms for a
computational photography dataset collected on a prototype lensless camera
which also uses a highly non-local optical encoding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep 3D-to-2D Watermarking: Embedding Messages in 3D Meshes and Extracting Them from 2D Renderings. (arXiv:2104.13450v6 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.13450">
<div class="article-summary-box-inner">
<span><p>Digital watermarking is widely used for copyright protection. Traditional 3D
watermarking approaches or commercial software are typically designed to embed
messages into 3D meshes, and later retrieve the messages directly from
distorted/undistorted watermarked 3D meshes. However, in many cases, users only
have access to rendered 2D images instead of 3D meshes. Unfortunately,
retrieving messages from 2D renderings of 3D meshes is still challenging and
underexplored. We introduce a novel end-to-end learning framework to solve this
problem through: 1) an encoder to covertly embed messages in both mesh geometry
and textures; 2) a differentiable renderer to render watermarked 3D objects
from different camera angles and under varied lighting conditions; 3) a decoder
to recover the messages from 2D rendered images. From our experiments, we show
that our model can learn to embed information visually imperceptible to humans,
and to retrieve the embedded information from 2D renderings that undergo 3D
distortions. In addition, we demonstrate that our method can also work with
other renderers, such as ray tracers and real-time renderers with and without
fine-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Neurally-Guided Shape Parser: Grammar-based Labeling of 3D Shape Regions with Approximate Inference. (arXiv:2106.12026v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.12026">
<div class="article-summary-box-inner">
<span><p>We propose the Neurally-Guided Shape Parser (NGSP), a method that learns how
to assign fine-grained semantic labels to regions of a 3D shape. NGSP solves
this problem via MAP inference, modeling the posterior probability of a label
assignment conditioned on an input shape with a learned likelihood function. To
make this search tractable, NGSP employs a neural guide network that learns to
approximate the posterior. NGSP finds high-probability label assignments by
first sampling proposals with the guide network and then evaluating each
proposal under the full likelihood. We evaluate NGSP on the task of
fine-grained semantic segmentation of manufactured 3D shapes from PartNet,
where shapes have been decomposed into regions that correspond to part instance
over-segmentations. We find that NGSP delivers significant performance
improvements over comparison methods that (i) use regions to group per-point
predictions, (ii) use regions as a self-supervisory signal or (iii) assign
labels to regions under alternative formulations. Further, we show that NGSP
maintains strong performance even with limited labeled data or noisy input
shape regions. Finally, we demonstrate that NGSP can be directly applied to CAD
shapes found in online repositories and validate its effectiveness with a
perceptual study.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FastSHAP: Real-Time Shapley Value Estimation. (arXiv:2107.07436v3 [stat.ML] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.07436">
<div class="article-summary-box-inner">
<span><p>Shapley values are widely used to explain black-box models, but they are
costly to calculate because they require many model evaluations. We introduce
FastSHAP, a method for estimating Shapley values in a single forward pass using
a learned explainer model. FastSHAP amortizes the cost of explaining many
inputs via a learning approach inspired by the Shapley value's weighted least
squares characterization, and it can be trained using standard stochastic
gradient optimization. We compare FastSHAP to existing estimation approaches,
revealing that it generates high-quality explanations with orders of magnitude
speedup.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human Pose and Shape Estimation from Single Polarization Images. (arXiv:2108.06834v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06834">
<div class="article-summary-box-inner">
<span><p>This paper focuses on a new problem of estimating human pose and shape from
single polarization images. Polarization camera is known to be able to capture
the polarization of reflected lights that preserves rich geometric cues of an
object surface. Inspired by the recent applications in surface normal
reconstruction from polarization images, in this paper, we attempt to estimate
human pose and shape from single polarization images by leveraging the
polarization-induced geometric cues. A dedicated two-stage pipeline is
proposed: given a single polarization image, stage one (Polar2Normal) focuses
on the fine detailed human body surface normal estimation; stage two
(Polar2Shape) then reconstructs clothed human shape from the polarization image
and the estimated surface normal. To empirically validate our approach, a
dedicated dataset (PHSPD) is constructed, consisting of over 500K frames with
accurate pose and parametric shape annotations. Empirical evaluations on this
real-world dataset as well as a synthetic dataset, SURREAL, demonstrate the
effectiveness of our approach. It suggests polarization camera as a promising
alternative to the more conventional RGB camera for human pose and shape
estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sensor Data Augmentation by Resampling for Contrastive Learning in Human Activity Recognition. (arXiv:2109.02054v2 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02054">
<div class="article-summary-box-inner">
<span><p>While deep learning has contributed to the advancement of sensor-based Human
Activity Recognition (HAR), it is usually a costly and challenging supervised
task with the needs of a large amount of labeled data. To alleviate this issue,
contrastive learning has been applied for sensor-based HAR. Data augmentation
is an essential part of contrastive learning and has a significant impact on
the performance of downstream tasks. However, current popular augmentation
methods do not achieve competitive performance in contrastive learning for
sensor-based HAR. Motivated by this issue, we propose a new sensor data
augmentation method by resampling, which simulates more realistic activity data
by varying the sampling frequency to maximize the coverage of the sampling
space. In addition, we extend MoCo, a popular contrastive learning framework,
to MoCoHAR for HAR. The resampling augmentation method will be evaluated on two
contrastive learning frameworks, SimCLRHAR and MoCoHAR, using UCI-HAR,
MotionSensor, and USC-HAD datasets. The experiment results show that the
resampling augmentation method outperforms all state-of-the-art methods under a
small amount of labeled data, on SimCLRHAR and MoCoHAR, with mean F1-score as
the evaluation metric. The results also demonstrate that not all data
augmentation methods have positive effects in the contrastive learning
framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Autonomous Crop-Agnostic Visual Navigation in Arable Fields. (arXiv:2109.11936v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11936">
<div class="article-summary-box-inner">
<span><p>Autonomous navigation of a robot in agricultural fields is essential for
every task from crop monitoring to weed management and fertilizer application.
Many current approaches rely on accurate GPS, however, such technology is
expensive and also prone to failure (e.g. through lack of coverage). As such,
autonomous navigation through sensors that can interpret their environment
(such as cameras) is important to achieve the goal of autonomy in agriculture.
In this paper, we introduce a purely vision-based navigation scheme that is
able to reliably guide the robot through row-crop fields without manual
intervention. Independent of any global localization or mapping, this approach
is able to accurately follow the crop-rows and switch between the rows, only
using onboard cameras. With the help of a novel crop-row detection and a novel
crop-row switching technique, our navigation scheme can be deployed in a wide
range of fields with different canopy types in various growth stages with
limited parameter tuning, creating a crop agnostic navigation approach. We have
extensively evaluated our approach in three different fields under various
illumination conditions using our agricultural robotic platform (BonnBot-I).
For navigation, our approach is evaluated on five crop types and achieves an
average navigation accuracy of 3.82cm relative to manual teleoperation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explainability-Aware One Point Attack for Point Cloud Neural Networks. (arXiv:2110.04158v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04158">
<div class="article-summary-box-inner">
<span><p>With the proposition of neural networks for point clouds, deep learning has
started to shine in the field of 3D object recognition while researchers have
shown an increased interest to investigate the reliability of point cloud
networks by adversarial attacks. However, most of the existing studies aim to
deceive humans or defense algorithms, while the few that address the operation
principles of the models themselves remain flawed in terms of critical point
selection. In this work, we propose two adversarial methods: One Point Attack
(OPA) and Critical Traversal Attack (CTA), which incorporate the explainability
technologies and aim to explore the intrinsic operating principle of point
cloud networks and their sensitivity against critical points perturbations. Our
results show that popular point cloud networks can be deceived with almost
$100\%$ success rate by shifting only one point from the input instance. In
addition, we show the interesting impact of different point attribution
distributions on the adversarial robustness of point cloud networks. Finally,
we discuss how our approaches facilitate the explainability study for point
cloud networks. To the best of our knowledge, this is the first
point-cloud-based adversarial approach concerning explainability. Our code is
available at https://github.com/Explain3D/Exp-One-Point-Atk-PC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UBnormal: New Benchmark for Supervised Open-Set Video Anomaly Detection. (arXiv:2111.08644v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.08644">
<div class="article-summary-box-inner">
<span><p>Detecting abnormal events in video is commonly framed as a one-class
classification task, where training videos contain only normal events, while
test videos encompass both normal and abnormal events. In this scenario,
anomaly detection is an open-set problem. However, some studies assimilate
anomaly detection to action recognition. This is a closed-set scenario that
fails to test the capability of systems at detecting new anomaly types. To this
end, we propose UBnormal, a new supervised open-set benchmark composed of
multiple virtual scenes for video anomaly detection. Unlike existing data sets,
we introduce abnormal events annotated at the pixel level at training time, for
the first time enabling the use of fully-supervised learning methods for
abnormal event detection. To preserve the typical open-set formulation, we make
sure to include disjoint sets of anomaly types in our training and test
collections of videos. To our knowledge, UBnormal is the first video anomaly
detection benchmark to allow a fair head-to-head comparison between one-class
open-set models and supervised closed-set models, as shown in our experiments.
Moreover, we provide empirical evidence showing that UBnormal can enhance the
performance of a state-of-the-art anomaly detection framework on two prominent
data sets, Avenue and ShanghaiTech. Our benchmark is freely available at
https://github.com/lilygeorgescu/UBnormal.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Local Texture Estimator for Implicit Representation Function. (arXiv:2111.08918v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.08918">
<div class="article-summary-box-inner">
<span><p>Recent works with an implicit neural function shed light on representing
images in arbitrary resolution. However, a standalone multi-layer perceptron
shows limited performance in learning high-frequency components. In this paper,
we propose a Local Texture Estimator (LTE), a dominant-frequency estimator for
natural images, enabling an implicit function to capture fine details while
reconstructing images in a continuous manner. When jointly trained with a deep
super-resolution (SR) architecture, LTE is capable of characterizing image
textures in 2D Fourier space. We show that an LTE-based neural function
achieves favorable performance against existing deep SR methods within an
arbitrary-scale factor. Furthermore, we demonstrate that our implementation
takes the shortest running time compared to previous works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">L-Verse: Bidirectional Generation Between Image and Text. (arXiv:2111.11133v10 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11133">
<div class="article-summary-box-inner">
<span><p>Far beyond learning long-range interactions of natural language, transformers
are becoming the de-facto standard for many vision tasks with their power and
scalability. Especially with cross-modal tasks between image and text, vector
quantized variational autoencoders (VQ-VAEs) are widely used to make a raw RGB
image into a sequence of feature vectors. To better leverage the correlation
between image and text, we propose L-Verse, a novel architecture consisting of
feature-augmented variational autoencoder (AugVAE) and bidirectional
auto-regressive transformer (BiART) for image-to-text and text-to-image
generation. Our AugVAE shows the state-of-the-art reconstruction performance on
ImageNet1K validation set, along with the robustness to unseen images in the
wild. Unlike other models, BiART can distinguish between image (or text) as a
conditional reference and a generation target. L-Verse can be directly used for
image-to-text or text-to-image generation without any finetuning or extra
object detection framework. In quantitative and qualitative experiments,
L-Verse shows impressive results against previous methods in both image-to-text
and text-to-image generation on MS-COCO Captions. We furthermore assess the
scalability of L-Verse architecture on Conceptual Captions and present the
initial result of bidirectional vision-language representation learning on
general domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">U-shape Transformer for Underwater Image Enhancement. (arXiv:2111.11843v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.11843">
<div class="article-summary-box-inner">
<span><p>The light absorption and scattering of underwater impurities lead to poor
underwater imaging quality. The existing data-driven based underwater image
enhancement (UIE) techniques suffer from the lack of a large-scale dataset
containing various underwater scenes and high-fidelity reference images.
Besides, the inconsistent attenuation in different color channels and space
areas is not fully considered for boosted enhancement. In this work, we
constructed a large-scale underwater image (LSUI) dataset including 5004 image
pairs, and reported an U-shape Transformer network where the transformer model
is for the first time introduced to the UIE task. The U-shape Transformer is
integrated with a channel-wise multi-scale feature fusion transformer (CMSFFT)
module and a spatial-wise global feature modeling transformer (SGFMT) module,
which reinforce the network's attention to the color channels and space areas
with more serious attenuation. Meanwhile, in order to further improve the
contrast and saturation, a novel loss function combining RGB, LAB and LCH color
spaces is designed following the human vision principle. The extensive
experiments on available datasets validate the state-of-the-art performance of
the reported technique with more than 2dB superiority.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extracting Triangular 3D Models, Materials, and Lighting From Images. (arXiv:2111.12503v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12503">
<div class="article-summary-box-inner">
<span><p>We present an efficient method for joint optimization of topology, materials
and lighting from multi-view image observations. Unlike recent multi-view
reconstruction approaches, which typically produce entangled 3D representations
encoded in neural networks, we output triangle meshes with spatially-varying
materials and environment lighting that can be deployed in any traditional
graphics engine unmodified. We leverage recent work in differentiable
rendering, coordinate-based networks to compactly represent volumetric
texturing, alongside differentiable marching tetrahedrons to enable
gradient-based optimization directly on the surface mesh. Finally, we introduce
a differentiable formulation of the split sum approximation of environment
lighting to efficiently recover all-frequency lighting. Experiments show our
extracted models used in advanced scene editing, material decomposition, and
high quality view interpolation, all running at interactive rates in
triangle-based renderers (rasterizers and path tracers). Project website:
https://nvlabs.github.io/nvdiffrec/ .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QMagFace: Simple and Accurate Quality-Aware Face Recognition. (arXiv:2111.13475v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.13475">
<div class="article-summary-box-inner">
<span><p>Face recognition systems have to deal with large variabilities (such as
different poses, illuminations, and expressions) that might lead to incorrect
matching decisions. These variabilities can be measured in terms of face image
quality which is defined over the utility of a sample for recognition. Previous
works on face recognition either do not employ this valuable information or
make use of non-inherently fit quality estimates. In this work, we propose a
simple and effective face recognition solution (QMagFace) that combines a
quality-aware comparison score with a recognition model based on a
magnitude-aware angular margin loss. The proposed approach includes
model-specific face image qualities in the comparison process to enhance the
recognition performance under unconstrained circumstances. Exploiting the
linearity between the qualities and their comparison scores induced by the
utilized loss, our quality-aware comparison function is simple and highly
generalizable. The experiments conducted on several face recognition databases
and benchmarks demonstrate that the introduced quality-awareness leads to
consistent improvements in the recognition performance. Moreover, the proposed
QMagFace approach performs especially well under challenging circumstances,
such as cross-pose, cross-age, or cross-quality. Consequently, it leads to
state-of-the-art performances on several face recognition benchmarks, such as
98.50% on AgeDB, 83.95% on XQLFQ, and 98.74% on CFP-FP. The code for QMagFace
is publicly available
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ranking Distance Calibration for Cross-Domain Few-Shot Learning. (arXiv:2112.00260v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00260">
<div class="article-summary-box-inner">
<span><p>Recent progress in few-shot learning promotes a more realistic cross-domain
setting, where the source and target datasets are from different domains. Due
to the domain gap and disjoint label spaces between source and target datasets,
their shared knowledge is extremely limited. This encourages us to explore more
information in the target domain rather than to overly elaborate training
strategies on the source domain as in many existing methods. Hence, we start
from a generic representation pre-trained by a cross-entropy loss and a
conventional distance-based classifier, along with an image retrieval view, to
employ a re-ranking process for calibrating a target distance matrix by
discovering the reciprocal k-nearest neighbours within the task. Assuming the
pre-trained representation is biased towards the source, we construct a
non-linear subspace to minimise task-irrelevant features therewithin while keep
more transferrable discriminative information by a hyperbolic tangent
transformation. The calibrated distance in this target-aware non-linear
subspace is complementary to that in the pre-trained representation. To impose
such distance calibration information onto the pre-trained representation, a
Kullback-Leibler divergence loss is employed to gradually guide the model
towards the calibrated distance-based distribution. Extensive evaluations on
eight target domains show that this target ranking calibration process can
improve conventional distance-based classifiers in few-shot learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Transformer Features for Image Quality Assessment. (arXiv:2112.00485v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00485">
<div class="article-summary-box-inner">
<span><p>Objective image quality evaluation is a challenging task, which aims to
measure the quality of a given image automatically. According to the
availability of the reference images, there are Full-Reference and No-Reference
IQA tasks, respectively. Most deep learning approaches use regression from deep
features extracted by Convolutional Neural Networks. For the FR task, another
option is conducting a statistical comparison on deep features. For all these
methods, non-local information is usually neglected. In addition, the
relationship between FR and NR tasks is less explored. Motivated by the recent
success of transformers in modeling contextual information, we propose a
unified IQA framework that utilizes CNN backbone and transformer encoder to
extract features. The proposed framework is compatible with both FR and NR
modes and allows for a joint training scheme. Evaluation experiments on three
standard IQA datasets, i.e., LIVE, CSIQ and TID2013, and KONIQ-10K, show that
our proposed model can achieve state-of-the-art FR performance. In addition,
comparable NR performance is achieved in extensive experiments, and the results
show that the NR performance can be leveraged by the joint training scheme.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PartImageNet: A Large, High-Quality Dataset of Parts. (arXiv:2112.00933v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00933">
<div class="article-summary-box-inner">
<span><p>It is natural to represent objects in terms of their parts. This has the
potential to improve the performance of algorithms for object recognition and
segmentation but can also help for downstream tasks like activity recognition.
Research on part-based models, however, is hindered by the lack of datasets
with per-pixel part annotations. This is partly due to the difficulty and high
cost of annotating object parts so it has rarely been done except for humans
(where there exists a big literature on part-based models). To help address
this problem, we propose PartImageNet, a large, high-quality dataset with part
segmentation annotations. It consists of $158$ classes from ImageNet with
approximately $24,000$ images. PartImageNet is unique because it offers
part-level annotations on a general set of classes including non-rigid,
articulated objects, while having an order of magnitude larger size compared to
existing part datasets (excluding datasets of humans). It can be utilized for
many vision tasks including Object Segmentation, Semantic Part Segmentation,
Few-shot Learning and Part Discovery. We conduct comprehensive experiments
which study these tasks and set up a set of baselines. The dataset and scripts
are released at https://github.com/TACJu/PartImageNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bootstrapping ViTs: Towards Liberating Vision Transformers from Pre-training. (arXiv:2112.03552v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.03552">
<div class="article-summary-box-inner">
<span><p>Recently, vision Transformers (ViTs) are developing rapidly and starting to
challenge the domination of convolutional neural networks (CNNs) in the realm
of computer vision (CV). With the general-purpose Transformer architecture
replacing the hard-coded inductive biases of convolution, ViTs have surpassed
CNNs, especially in data-sufficient circumstances. However, ViTs are prone to
over-fit on small datasets and thus rely on large-scale pre-training, which
expends enormous time. In this paper, we strive to liberate ViTs from
pre-training by introducing CNNs' inductive biases back to ViTs while
preserving their network architectures for higher upper bound and setting up
more suitable optimization objectives. To begin with, an agent CNN is designed
based on the given ViT with inductive biases. Then a bootstrapping training
algorithm is proposed to jointly optimize the agent and ViT with weight
sharing, during which the ViT learns inductive biases from the intermediate
features of the agent. Extensive experiments on CIFAR-10/100 and ImageNet-1k
with limited training data have shown encouraging results that the inductive
biases help ViTs converge significantly faster and outperform conventional CNNs
with even fewer parameters. Our code is publicly available at
https://github.com/zhfeing/Bootstrapping-ViTs-pytorch.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Watermarking Images in Self-Supervised Latent Spaces. (arXiv:2112.09581v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.09581">
<div class="article-summary-box-inner">
<span><p>We revisit watermarking techniques based on pre-trained deep networks, in the
light of self-supervised approaches. We present a way to embed both marks and
binary messages into their latent spaces, leveraging data augmentation at
marking time. Our method can operate at any resolution and creates watermarks
robust to a broad range of transformations (rotations, crops, JPEG, contrast,
etc). It significantly outperforms the previous zero-bit methods, and its
performance on multi-bit watermarking is on par with state-of-the-art
encoder-decoder architectures trained end-to-end for watermarking. The code is
available at github.com/facebookresearch/ssl_watermarking
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalized Few-Shot Semantic Segmentation: All You Need is Fine-Tuning. (arXiv:2112.10982v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10982">
<div class="article-summary-box-inner">
<span><p>Generalized few-shot semantic segmentation was introduced to move beyond only
evaluating few-shot segmentation models on novel classes to include testing
their ability to remember base classes. While the current state-of-the-art
approach is based on meta-learning, it performs poorly and saturates in
learning after observing only a few shots. We propose the first fine-tuning
solution, and demonstrate that it addresses the saturation problem while
achieving state-of-the-art results on two datasets, PASCAL-5i and COCO-20i. We
also show that it outperforms existing methods, whether fine-tuning multiple
final layers or only the final layer. Finally, we present a triplet loss
regularization that shows how to redistribute the balance of performance
between novel and base categories so that there is a smaller gap between them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Omni-Seg: A Single Dynamic Network for Multi-label Renal Pathology Image Segmentation using Partially Labeled Data. (arXiv:2112.12665v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.12665">
<div class="article-summary-box-inner">
<span><p>Computer-assisted quantitative analysis on Giga-pixel pathology images has
provided a new avenue in histology examination. The innovations have been
largely focused on cancer pathology (i.e., tumor segmentation and
characterization). In non-cancer pathology, the learning algorithms can be
asked to examine more comprehensive tissue types simultaneously, as a
multi-label setting. The prior arts typically needed to train multiple
segmentation networks in order to match the domain-specific knowledge for
heterogeneous tissue types (e.g., glomerular tuft, glomerular unit, proximal
tubular, distal tubular, peritubular capillaries, and arteries). In this paper,
we propose a dynamic single segmentation network (Omni-Seg) that learns to
segment multiple tissue types using partially labeled images (i.e., only one
tissue type is labeled for each training image) for renal pathology. By
learning from ~150,000 patch-wise pathological images from six tissue types,
the proposed Omni-Seg network achieved superior segmentation accuracy and less
resource consumption when compared to the previous the multiple-network and
multi-head design. In the testing stage, the proposed method obtains
"completely labeled" tissue segmentation results using only "partially labeled"
training images. The source code is available at
https://github.com/ddrrnn123/Omni-Seg
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving the Behaviour of Vision Transformers with Token-consistent Stochastic Layers. (arXiv:2112.15111v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.15111">
<div class="article-summary-box-inner">
<span><p>We introduce token-consistent stochastic layers in vision transformers,
without causing any severe drop in performance. The added stochasticity
improves network calibration, robustness and strengthens privacy. We use linear
layers with token-consistent stochastic parameters inside the multilayer
perceptron blocks, without altering the architecture of the transformer. The
stochastic parameters are sampled from the uniform distribution, both during
training and inference. The applied linear operations preserve the topological
structure, formed by the set of tokens passing through the shared multilayer
perceptron. This operation encourages the learning of the recognition task to
rely on the topological structures of the tokens, instead of their values,
which in turn offers the desired robustness and privacy of the visual features.
The effectiveness of the token-consistent stochasticity is demonstrated on
three different applications, namely, network calibration, adversarial
robustness, and feature privacy, by boosting the performance of the respective
established baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransVPR: Transformer-based place recognition with multi-level attention aggregation. (arXiv:2201.02001v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.02001">
<div class="article-summary-box-inner">
<span><p>Visual place recognition is a challenging task for applications such as
autonomous driving navigation and mobile robot localization. Distracting
elements presenting in complex scenes often lead to deviations in the
perception of visual place. To address this problem, it is crucial to integrate
information from only task-relevant regions into image representations. In this
paper, we introduce a novel holistic place recognition model, TransVPR, based
on vision Transformers. It benefits from the desirable property of the
self-attention operation in Transformers which can naturally aggregate
task-relevant features. Attentions from multiple levels of the Transformer,
which focus on different regions of interest, are further combined to generate
a global image representation. In addition, the output tokens from Transformer
layers filtered by the fused attention mask are considered as key-patch
descriptors, which are used to perform spatial matching to re-rank the
candidates retrieved by the global image features. The whole model allows
end-to-end training with a single objective and image-level supervision.
TransVPR achieves state-of-the-art performance on several real-world benchmarks
while maintaining low computational time and storage requirements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ImpliCity: City Modeling from Satellite Images with Deep Implicit Occupancy Fields. (arXiv:2201.09968v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.09968">
<div class="article-summary-box-inner">
<span><p>High-resolution optical satellite sensors, combined with dense stereo
algorithms, have made it possible to reconstruct 3D city models from space.
However, these models are, in practice, rather noisy and tend to miss small
geometric features that are clearly visible in the images. We argue that one
reason for the limited quality may be a too early, heuristic reduction of the
triangulated 3D point cloud to an explicit height field or surface mesh. To
make full use of the point cloud and the underlying images, we introduce
ImpliCity, a neural representation of the 3D scene as an implicit, continuous
occupancy field, driven by learned embeddings of the point cloud and a stereo
pair of ortho-photos. We show that this representation enables the extraction
of high-quality DSMs: with image resolution 0.5$\,$m, ImpliCity reaches a
median height error of $\approx\,$0.7$\,$m and outperforms competing methods,
especially w.r.t. building reconstruction, featuring intricate roof details,
smooth surfaces, and straight, regular outlines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anomaly Detection via Reverse Distillation from One-Class Embedding. (arXiv:2201.10703v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.10703">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation (KD) achieves promising results on the challenging
problem of unsupervised anomaly detection (AD).The representation discrepancy
of anomalies in the teacher-student (T-S) model provides essential evidence for
AD. However, using similar or identical architectures to build the teacher and
student models in previous studies hinders the diversity of anomalous
representations. To tackle this problem, we propose a novel T-S model
consisting of a teacher encoder and a student decoder and introduce a simple
yet effective "reverse distillation" paradigm accordingly. Instead of receiving
raw images directly, the student network takes teacher model's one-class
embedding as input and targets to restore the teacher's multiscale
representations. Inherently, knowledge distillation in this study starts from
abstract, high-level presentations to low-level features. In addition, we
introduce a trainable one-class bottleneck embedding (OCBE) module in our T-S
model. The obtained compact embedding effectively preserves essential
information on normal patterns, but abandons anomaly perturbations. Extensive
experimentation on AD and one-class novelty detection benchmarks shows that our
method surpasses SOTA performance, demonstrating our proposed approach's
effectiveness and generalizability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Robust Convolutional Neural Networks with Relevant Feature Focusing via Explanations. (arXiv:2202.04237v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04237">
<div class="article-summary-box-inner">
<span><p>Existing image recognition techniques based on convolutional neural networks
(CNNs) basically assume that the training and test datasets are sampled from
i.i.d distributions. However, this assumption is easily broken in the real
world because of the distribution shift that occurs when the co-occurrence
relations between objects and backgrounds in input images change. Under this
type of distribution shift, CNNs learn to focus on features that are not
task-relevant, such as backgrounds from the training data, and degrade their
accuracy on the test data. To tackle this problem, we propose relevant feature
focusing (ReFF). ReFF detects task-relevant features and regularizes CNNs via
explanation outputs (e.g., Grad-CAM). Since ReFF is composed of post-hoc
explanation modules, it can be easily applied to off-the-shelf CNNs.
Furthermore, ReFF requires no additional inference cost at test time because it
is only used for regularization while training. We demonstrate that CNNs
trained with ReFF focus on features relevant to the target task and that ReFF
improves the test-time accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Fine-tuning for Backdoor Defense: Connecting Backdoor Attacks to Adversarial Attacks. (arXiv:2202.06312v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.06312">
<div class="article-summary-box-inner">
<span><p>Deep neural networks (DNNs) are known to be vulnerable to both backdoor
attacks as well as adversarial attacks. In the literature, these two types of
attacks are commonly treated as distinct problems and solved separately, since
they belong to training-time and inference-time attacks respectively. However,
in this paper we find an intriguing connection between them: for a model
planted with backdoors, we observe that its adversarial examples have similar
behaviors as its triggered samples, i.e., both activate the same subset of DNN
neurons. It indicates that planting a backdoor into a model will significantly
affect the model's adversarial examples. Based on this observations, we design
a new Adversarial Fine-Tuning (AFT) algorithm to defend against backdoor
attacks. We empirically show that, against 5 state-of-the-art backdoor attacks,
our AFT can effectively erase the backdoor triggers without obvious performance
degradation on clean samples and significantly outperforms existing defense
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Constrained Least Squares for Blind Image Super-Resolution. (arXiv:2202.07508v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07508">
<div class="article-summary-box-inner">
<span><p>In this paper, we tackle the problem of blind image super-resolution(SR) with
a reformulated degradation model and two novel modules. Following the common
practices of blind SR, our method proposes to improve both the kernel
estimation as well as the kernel-based high-resolution image restoration. To be
more specific, we first reformulate the degradation model such that the
deblurring kernel estimation can be transferred into the low-resolution space.
On top of this, we introduce a dynamic deep linear filter module. Instead of
learning a fixed kernel for all images, it can adaptively generate deblurring
kernel weights conditional on the input and yield a more robust kernel
estimation. Subsequently, a deep constrained least square filtering module is
applied to generate clean features based on the reformulation and estimated
kernel. The deblurred feature and the low input image feature are then fed into
a dual-path structured SR network and restore the final high-resolution result.
To evaluate our method, we further conduct evaluations on several benchmarks,
including Gaussian8 and DIV2KRK. Our experiments demonstrate that the proposed
method achieves better accuracy and visual improvements against
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ditto: Building Digital Twins of Articulated Objects from Interaction. (arXiv:2202.08227v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.08227">
<div class="article-summary-box-inner">
<span><p>Digitizing physical objects into the virtual world has the potential to
unlock new research and applications in embodied AI and mixed reality. This
work focuses on recreating interactive digital twins of real-world articulated
objects, which can be directly imported into virtual environments. We introduce
Ditto to learn articulation model estimation and 3D geometry reconstruction of
an articulated object through interactive perception. Given a pair of visual
observations of an articulated object before and after interaction, Ditto
reconstructs part-level geometry and estimates the articulation model of the
object. We employ implicit neural representations for joint geometry and
articulation modeling. Our experiments show that Ditto effectively builds
digital twins of articulated objects in a category-agnostic way. We also apply
Ditto to real-world objects and deploy the recreated digital twins in physical
simulation. Code and additional results are available at
https://ut-austin-rpl.github.io/Ditto
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comprehensive Review of Computer Vision in Sports: Open Issues, Future Trends and Research Directions. (arXiv:2203.02281v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02281">
<div class="article-summary-box-inner">
<span><p>Recent developments in video analysis of sports and computer vision
techniques have achieved significant improvements to enable a variety of
critical operations. To provide enhanced information, such as detailed complex
analysis in sports like soccer, basketball, cricket, badminton, etc., studies
have focused mainly on computer vision techniques employed to carry out
different tasks. This paper presents a comprehensive review of sports video
analysis for various applications high-level analysis such as detection and
classification of players, tracking player or ball in sports and predicting the
trajectories of player or ball, recognizing the teams strategies, classifying
various events in sports. The paper further discusses published works in a
variety of application-specific tasks related to sports and the present
researchers views regarding them. Since there is a wide research scope in
sports for deploying computer vision techniques in various sports, some of the
publicly available datasets related to a particular sport have been provided.
This work reviews a detailed discussion on some of the artificial
intelligence(AI)applications in sports vision, GPU-based work stations, and
embedded platforms. Finally, this review identifies the research directions,
probable challenges, and future trends in the area of visual recognition in
sports.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shadows can be Dangerous: Stealthy and Effective Physical-world Adversarial Attack by Natural Phenomenon. (arXiv:2203.03818v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03818">
<div class="article-summary-box-inner">
<span><p>Estimating the risk level of adversarial examples is essential for safely
deploying machine learning models in the real world. One popular approach for
physical-world attacks is to adopt the "sticker-pasting" strategy, which
however suffers from some limitations, including difficulties in access to the
target or printing by valid colors. A new type of non-invasive attacks emerged
recently, which attempt to cast perturbation onto the target by optics based
tools, such as laser beam and projector. However, the added optical patterns
are artificial but not natural. Thus, they are still conspicuous and
attention-grabbed, and can be easily noticed by humans. In this paper, we study
a new type of optical adversarial examples, in which the perturbations are
generated by a very common natural phenomenon, shadow, to achieve naturalistic
and stealthy physical-world adversarial attack under the black-box setting. We
extensively evaluate the effectiveness of this new attack on both simulated and
real-world environments. Experimental results on traffic sign recognition
demonstrate that our algorithm can generate adversarial examples effectively,
reaching 98.23% and 90.47% success rates on LISA and GTSRB test sets
respectively, while continuously misleading a moving camera over 95% of the
time in real-world scenarios. We also offer discussions about the limitations
and the defense mechanism of this attack.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Visual-Prompt Temporal Answering Grounding in Medical Instructional Video. (arXiv:2203.06667v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06667">
<div class="article-summary-box-inner">
<span><p>The temporal answering grounding in the video (TAGV) is a new task naturally
deriving from temporal sentence grounding in the video (TSGV). Given an
untrimmed video and a text question, this task aims at locating the matching
span from the video that can semantically answer the question. Existing methods
tend to formulate the TAGV task with a visual span-based question answering
(QA) approach by matching the visual frame span queried by the text question.
However, due to the weak correlations and huge gaps in semantics in features
between the textual question and visual answer, existing methods adopting
visual span predictor fail to perform well in the TAGV task. In this work, we
propose a visual-prompt text span localizing (VPTSL) method, which enhances the
text span localization in the pre-trained language model (PLM) with the visual
highlight features. Specifically, the context query attention is utilized to
perform cross-modal modeling between the textual and visual features. Then, the
highlight features are obtained through the highlight module with a linear
layer to provide the visual prompt. To alleviate the differences in semantics
and correlations between textual and visual features, we design the text span
predictor by encoding the question, the subtitles, and the visual prompt in the
PLM. As a result, the TAGV task is formulated to predict the span of subtitles
matching the visual answer. Extensive experiments on the medical instructional
dataset, namely MedVidQA, show that the proposed VPTSL outperforms other
state-of-the-art (SOTA) methods by 28.36 in mIOU score with a large margin,
which demonstrates the effectiveness of visual prompt and the text span
predictor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Memory Learning for Fine-Grained Scene Graph Generation. (arXiv:2203.06907v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06907">
<div class="article-summary-box-inner">
<span><p>As far as Scene Graph Generation (SGG), coarse and fine predicates mix in the
dataset due to the crowd-sourced labeling, and the long-tail problem is also
pronounced. Given this tricky situation, many existing SGG methods treat the
predicates equally and learn the model under the supervision of
mixed-granularity predicates in one stage, leading to relatively coarse
predictions. In order to alleviate the negative impact of the suboptimum
mixed-granularity annotation and long-tail effect problems, this paper proposes
a novel Hierarchical Memory Learning (HML) framework to learn the model from
simple to complex, which is similar to the human beings' hierarchical memory
learning process. After the autonomous partition of coarse and fine predicates,
the model is first trained on the coarse predicates and then learns the fine
predicates. In order to realize this hierarchical learning pattern, this paper,
for the first time, formulates the HML framework using the new Concept
Reconstruction (CR) and Model Reconstruction (MR) constraints. It is worth
noticing that the HML framework can be taken as one general optimization
strategy to improve various SGG models, and significant improvement can be
achieved on the SGG benchmark (i.e., Visual Genome).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distribution-Aware Single-Stage Models for Multi-Person 3D Pose Estimation. (arXiv:2203.07697v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07697">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a novel Distribution-Aware Single-stage (DAS) model
for tackling the challenging multi-person 3D pose estimation problem. Different
from existing top-down and bottom-up methods, the proposed DAS model
simultaneously localizes person positions and their corresponding body joints
in the 3D camera space in a one-pass manner. This leads to a simplified
pipeline with enhanced efficiency. In addition, DAS learns the true
distribution of body joints for the regression of their positions, rather than
making a simple Laplacian or Gaussian assumption as previous works. This
provides valuable priors for model prediction and thus boosts the
regression-based scheme to achieve competitive performance with volumetric-base
ones. Moreover, DAS exploits a recursive update strategy for progressively
approaching to regression target, alleviating the optimization difficulty and
further lifting the regression performance. DAS is implemented with a fully
Convolutional Neural Network and end-to-end learnable. Comprehensive
experiments on benchmarks CMU Panoptic and MuPoTS-3D demonstrate the superior
efficiency of the proposed DAS model, specifically 1.5x speedup over previous
best model, and its stat-of-the-art accuracy for multi-person 3D pose
estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Intrinsic Neural Fields: Learning Functions on Manifolds. (arXiv:2203.07967v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.07967">
<div class="article-summary-box-inner">
<span><p>Neural fields have gained significant attention in the computer vision
community due to their excellent performance in novel view synthesis, geometry
reconstruction, and generative modeling. Some of their advantages are a sound
theoretic foundation and an easy implementation in current deep learning
frameworks. While neural fields have been applied to signals on manifolds,
e.g., for texture reconstruction, their representation has been limited to
extrinsically embedding the shape into Euclidean space. The extrinsic embedding
ignores known intrinsic manifold properties and is inflexible wrt. transfer of
the learned function. To overcome these limitations, this work introduces
intrinsic neural fields, a novel and versatile representation for neural fields
on manifolds. Intrinsic neural fields combine the advantages of neural fields
with the spectral properties of the Laplace-Beltrami operator. We show
theoretically that intrinsic neural fields inherit many desirable properties of
the extrinsic neural field framework but exhibit additional intrinsic
qualities, like isometry invariance. In experiments, we show intrinsic neural
fields can reconstruct high-fidelity textures from images with state-of-the-art
quality and are robust to the discretization of the underlying manifold. We
demonstrate the versatility of intrinsic neural fields by tackling various
applications: texture transfer between deformed shapes &amp; different shapes,
texture reconstruction from real-world images with view dependence, and
discretization-agnostic learning on meshes and point clouds.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One-Shot Adaptation of GAN in Just One CLIP. (arXiv:2203.09301v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09301">
<div class="article-summary-box-inner">
<span><p>There are many recent research efforts to fine-tune a pre-trained generator
with a few target images to generate images of a novel domain. Unfortunately,
these methods often suffer from overfitting or under-fitting when fine-tuned
with a single target image. To address this, here we present a novel
single-shot GAN adaptation method through unified CLIP space manipulations.
Specifically, our model employs a two-step training strategy: reference image
search in the source generator using a CLIP-guided latent optimization,
followed by generator fine-tuning with a novel loss function that imposes CLIP
space consistency between the source and adapted generators. To further improve
the adapted model to produce spatially consistent samples with respect to the
source generator, we also propose contrastive regularization for patchwise
relationships in the CLIP space. Experimental results show that our model
generates diverse outputs with the target texture and outperforms the baseline
models both qualitatively and quantitatively. Furthermore, we show that our
CLIP space manipulation strategy allows more effective attribute editing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-similarity based Hyperrelation Network for few-shot segmentation. (arXiv:2203.09550v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09550">
<div class="article-summary-box-inner">
<span><p>Few-shot semantic segmentation aims at recognizing the object regions of
unseen categories with only a few annotated examples as supervision. The key to
few-shot segmentation is to establish a robust semantic relationship between
the support and query images and to prevent overfitting. In this paper, we
propose an effective Multi-similarity Hyperrelation Network (MSHNet) to tackle
the few-shot semantic segmentation problem. In MSHNet, we propose a new
Generative Prototype Similarity (GPS), which together with cosine similarity
can establish a strong semantic relation between the support and query images.
The locally generated prototype similarity based on global feature is logically
complementary to the global cosine similarity based on local feature, and the
relationship between the query image and the supported image can be expressed
more comprehensively by using the two similarities simultaneously. In addition,
we propose a Symmetric Merging Block (SMB) in MSHNet to efficiently merge
multi-layer, multi-shot and multi-similarity hyperrelational features. MSHNet
is built on the basis of similarity rather than specific category features,
which can achieve more general unity and effectively reduce overfitting. On two
benchmark semantic segmentation datasets Pascal-5i and COCO-20i, MSHNet
achieves new state-of-the-art performances on 1-shot and 5-shot semantic
segmentation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Regional Semantic Contrast and Aggregation for Weakly Supervised Semantic Segmentation. (arXiv:2203.09653v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09653">
<div class="article-summary-box-inner">
<span><p>Learning semantic segmentation from weakly-labeled (e.g., image tags only)
data is challenging since it is hard to infer dense object regions from sparse
semantic tags. Despite being broadly studied, most current efforts directly
learn from limited semantic annotations carried by individual image or image
pairs, and struggle to obtain integral localization maps. Our work alleviates
this from a novel perspective, by exploring rich semantic contexts
synergistically among abundant weakly-labeled training data for network
learning and inference. In particular, we propose regional semantic contrast
and aggregation (RCA) . RCA is equipped with a regional memory bank to store
massive, diverse object patterns appearing in training data, which acts as
strong support for exploration of dataset-level semantic structure.
Particularly, we propose i) semantic contrast to drive network learning by
contrasting massive categorical object regions, leading to a more holistic
object pattern understanding, and ii) semantic aggregation to gather diverse
relational contexts in the memory to enrich semantic representations. In this
manner, RCA earns a strong capability of fine-grained semantic understanding,
and eventually establishes new state-of-the-art results on two popular
benchmarks, i.e., PASCAL VOC 2012 and COCO 2014.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Modal Masked Pre-Training for Monocular Panoramic Depth Completion. (arXiv:2203.09855v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.09855">
<div class="article-summary-box-inner">
<span><p>In this paper, we formulate a potentially valuable panoramic depth completion
(PDC) task as panoramic 3D cameras often produce 360{\deg} depth with missing
data in complex scenes. Its goal is to recover dense panoramic depths from raw
sparse ones and panoramic RGB images. To deal with the PDC task, we train a
deep network that takes both depth and image as inputs for the dense panoramic
depth recovery. However, it needs to face a challenging optimization problem of
the network parameters due to its non-convex objective function. To address
this problem, we propose a simple yet effective approach termed M{^3}PT:
multi-modal masked pre-training. Specifically, during pre-training, we
simultaneously cover up patches of the panoramic RGB image and sparse depth by
shared random mask, then reconstruct the sparse depth in the masked regions. To
our best knowledge, it is the first time that we show the effectiveness of
masked pre-training in a multi-modal vision task, instead of the single-modal
task resolved by masked autoencoders (MAE). Different from MAE where
fine-tuning completely discards the decoder part of pre-training, there is no
architectural difference between the pre-training and fine-tuning stages in our
M$^{3}$PT as they only differ in the prediction density, which potentially
makes the transfer learning more convenient and effective. Extensive
experiments verify the effectiveness of M{^3}PT on three panoramic datasets.
Notably, we improve the state-of-the-art baselines by averagely 26.2% in RMSE,
51.7% in MRE, 49.7% in MAE, and 37.5% in RMSElog on three benchmark datasets.
Codes and pre-trained models are available at
https://github.com/anonymoustbd/MMMPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unidirectional Thin Adapter for Efficient Adaptation of Deep Neural Networks. (arXiv:2203.10463v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.10463">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a new adapter network for adapting a pre-trained
deep neural network to a target domain with minimal computation. The proposed
model, unidirectional thin adapter (UDTA), helps the classifier adapt to new
data by providing auxiliary features that complement the backbone network. UDTA
takes outputs from multiple layers of the backbone as input features but does
not transmit any feature to the backbone. As a result, UDTA can learn without
computing the gradient of the backbone, which saves computation for training
significantly. In addition, since UDTA learns the target task without modifying
the backbone, a single backbone can adapt to multiple tasks by learning only
UDTAs separately. In experiments on five fine-grained classification datasets
consisting of a small number of samples, UDTA significantly reduced computation
and training time required for backpropagation while showing comparable or even
improved accuracy compared with conventional adapter models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLIP meets GamePhysics: Towards bug identification in gameplay videos using zero-shot transfer learning. (arXiv:2203.11096v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11096">
<div class="article-summary-box-inner">
<span><p>Gameplay videos contain rich information about how players interact with the
game and how the game responds. Sharing gameplay videos on social media
platforms, such as Reddit, has become a common practice for many players.
Often, players will share gameplay videos that showcase video game bugs. Such
gameplay videos are software artifacts that can be utilized for game testing,
as they provide insight for bug analysis. Although large repositories of
gameplay videos exist, parsing and mining them in an effective and structured
fashion has still remained a big challenge. In this paper, we propose a search
method that accepts any English text query as input to retrieve relevant videos
from large repositories of gameplay videos. Our approach does not rely on any
external information (such as video metadata); it works solely based on the
content of the video. By leveraging the zero-shot transfer capabilities of the
Contrastive Language-Image Pre-Training (CLIP) model, our approach does not
require any data labeling or training. To evaluate our approach, we present the
$\texttt{GamePhysics}$ dataset consisting of 26,954 videos from 1,873 games,
that were collected from the GamePhysics section on the Reddit website. Our
approach shows promising results in our extensive analysis of simple queries,
compound queries, and bug queries, indicating that our approach is useful for
object and event detection in gameplay videos. An example application of our
approach is as a gameplay video search engine to aid in reproducing video game
bugs. Please visit the following link for the code and the data:
https://asgaardlab.github.io/CLIPxGamePhysics/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">No Pain, Big Gain: Classify Dynamic Point Cloud Sequences with Static Models by Fitting Feature-level Space-time Surfaces. (arXiv:2203.11113v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11113">
<div class="article-summary-box-inner">
<span><p>Scene flow is a powerful tool for capturing the motion field of 3D point
clouds. However, it is difficult to directly apply flow-based models to dynamic
point cloud classification since the unstructured points make it hard or even
impossible to efficiently and effectively trace point-wise correspondences. To
capture 3D motions without explicitly tracking correspondences, we propose a
kinematics-inspired neural network (Kinet) by generalizing the kinematic
concept of ST-surfaces to the feature space. By unrolling the normal solver of
ST-surfaces in the feature space, Kinet implicitly encodes feature-level
dynamics and gains advantages from the use of mature backbones for static point
cloud processing. With only minor changes in network structures and low
computing overhead, it is painless to jointly train and deploy our framework
with a given static model. Experiments on NvGesture, SHREC'17, MSRAction-3D,
and NTU-RGBD demonstrate its efficacy in performance, efficiency in both the
number of parameters and computational complexity, as well as its versatility
to various static backbones. Noticeably, Kinet achieves the accuracy of 93.27%
on MSRAction-3D with only 3.20M parameters and 10.35G FLOPS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Channel Self-Supervision for Online Knowledge Distillation. (arXiv:2203.11660v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11660">
<div class="article-summary-box-inner">
<span><p>Recently, researchers have shown an increased interest in the online
knowledge distillation. Adopting an one-stage and end-to-end training fashion,
online knowledge distillation uses aggregated intermediated predictions of
multiple peer models for training. However, the absence of a powerful teacher
model may result in the homogeneity problem between group peers, affecting the
effectiveness of group distillation adversely. In this paper, we propose a
novel online knowledge distillation method, \textbf{C}hannel
\textbf{S}elf-\textbf{S}upervision for Online Knowledge Distillation (CSS),
which structures diversity in terms of input, target, and network to alleviate
the homogenization problem. Specifically, we construct a dual-network
multi-branch structure and enhance inter-branch diversity through
self-supervised learning, adopting the feature-level transformation and
augmenting the corresponding labels. Meanwhile, the dual network structure has
a larger space of independent parameters to resist the homogenization problem
during distillation. Extensive quantitative experiments on CIFAR-100 illustrate
that our method provides greater diversity than OKDDip and we also give pretty
performance improvement, even over the state-of-the-art such as PCL. The
results on three fine-grained datasets (StanfordDogs, StanfordCars,
CUB-200-211) also show the significant generalization capability of our
approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring and Evaluating Image Restoration Potential in Dynamic Scenes. (arXiv:2203.11754v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.11754">
<div class="article-summary-box-inner">
<span><p>In dynamic scenes, images often suffer from dynamic blur due to superposition
of motions or low signal-noise ratio resulted from quick shutter speed when
avoiding motions. Recovering sharp and clean results from the captured images
heavily depends on the ability of restoration methods and the quality of the
input. Although existing research on image restoration focuses on developing
models for obtaining better restored results, fewer have studied to evaluate
how and which input image leads to superior restored quality. In this paper, to
better study an image's potential value that can be explored for restoration,
we propose a novel concept, referring to image restoration potential (IRP).
Specifically, We first establish a dynamic scene imaging dataset containing
composite distortions and applied image restoration processes to validate the
rationality of the existence to IRP. Based on this dataset, we investigate
several properties of IRP and propose a novel deep model to accurately predict
IRP values. By gradually distilling and selective fusing the degradation
features, the proposed model shows its superiority in IRP prediction. Thanks to
the proposed model, we are then able to validate how various image restoration
related applications are benefited from IRP prediction. We show the potential
usages of IRP as a filtering principle to select valuable frames, an auxiliary
guidance to improve restoration models, and even an indicator to optimize
camera settings for capturing better images under dynamic scenarios.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-03-24 23:08:10.680019978 UTC">2022-03-24 23:08:10 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>