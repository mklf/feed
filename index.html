<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-03-14T01:30:00Z">03-14</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">A new approach to calculating BERTScore for automatic assessment of translation quality. (arXiv:2203.05598v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05598">
<div class="article-summary-box-inner">
<span><p>The study of the applicability of the BERTScore metric was conducted to
translation quality assessment at the sentence level for English -&gt; Russian
direction. Experiments were performed with a pre-trained multilingual BERT as
well as with a pair of monolingual BERT models. To align the monolingual
embeddings, an orthogonal transformation based on anchor tokens was used. It
was demonstrated that such transformation helps to prevent mismatching issue
and shown that this approach gives better results than using embeddings of the
multilingual model. To improve the token matching process it is proposed to
combine all incomplete WorkPiece tokens into meaningful words and use simple
averaging of corresponding vectors and to calculate BERTScore based on anchor
tokens only. Such modifications allowed us to achieve a better correlation of
the model predictions with human estimates. In addition to evaluating machine
translation, several versions of human translation were evaluated as well, the
problems of this approach were listed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Parameter-Free Attentive Scoring for Speaker Verification. (arXiv:2203.05642v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05642">
<div class="article-summary-box-inner">
<span><p>This paper presents a novel study of parameter-free attentive scoring for
speaker verification. Parameter-free scoring provides the flexibility of
comparing speaker representations without the need of an accompanying
parametric scoring model. Inspired by the attention component in Transformer
neural networks, we propose a variant of the scaled dot product attention
mechanism to compare enrollment and test segment representations. In addition,
this work explores the effect on performance of (i) different types of
normalization, (ii) independent versus tied query/key estimation, (iii) varying
the number of key-value pairs and (iv) pooling multiple enrollment utterance
statistics. Experimental results for a 4 task average show that a simple
parameter-free attentive scoring mechanism can improve the average EER by 10%
over the best cosine similarity baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextualized Sensorimotor Norms: multi-dimensional measures of sensorimotor strength for ambiguous English words, in context. (arXiv:2203.05648v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05648">
<div class="article-summary-box-inner">
<span><p>Most large language models are trained on linguistic input alone, yet humans
appear to ground their understanding of words in sensorimotor experience. A
natural solution is to augment LM representations with human judgments of a
word's sensorimotor associations (e.g., the Lancaster Sensorimotor Norms), but
this raises another challenge: most words are ambiguous, and judgments of words
in isolation fail to account for this multiplicity of meaning (e.g., "wooden
table" vs. "data table"). We attempted to address this problem by building a
new lexical resource of contextualized sensorimotor judgments for 112 English
words, each rated in four different contexts (448 sentences total). We show
that these ratings encode overlapping but distinct information from the
Lancaster Sensorimotor Norms, and that they also predict other measures of
interest (e.g., relatedness), above and beyond measures derived from BERT.
Beyond shedding light on theoretical questions, we suggest that these ratings
could be of use as a "challenge set" for researchers building grounded language
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NELA-GT-2021: A Large Multi-Labelled News Dataset for The Study of Misinformation in News Articles. (arXiv:2203.05659v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05659">
<div class="article-summary-box-inner">
<span><p>In this paper, we present the fourth installment of the NELA-GT datasets,
NELA-GT-2021. The dataset contains 1.8M articles from 367 outlets between
January 1st, 2021 and December 31st, 2021. Just as in past releases of the
dataset, NELA-GT-2021 includes outlet-level veracity labels from Media
Bias/Fact Check and tweets embedded in collected news articles. The
NELA-GT-2021 dataset can be found at: https://doi.org/10.7910/DVN/RBKVBM
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synopses of Movie Narratives: a Video-Language Dataset for Story Understanding. (arXiv:2203.05711v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05711">
<div class="article-summary-box-inner">
<span><p>Despite recent advances of AI, story understanding remains an open and
under-investigated problem. We collect, preprocess, and publicly release a
video-language story dataset, Synopses of Movie Narratives(SyMoN), containing
5,193 video summaries of popular movies and TV series. SyMoN captures
naturalistic storytelling videos for human audience made by human creators, and
has higher story coverage and more frequent mental-state references than
similar video-language story datasets. Differing from most existing video-text
datasets, SyMoN features large semantic gaps between the visual and the textual
modalities due to the prevalence of reporting bias and mental state
descriptions. We establish benchmarks on video-text retrieval and zero-shot
alignment on movie summary videos. With SyMoN, we hope to lay the groundwork
for progress in multimodal story understanding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-constraint Optimal Transport for Entity Alignment with Dangling Cases. (arXiv:2203.05744v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05744">
<div class="article-summary-box-inner">
<span><p>Entity alignment (EA) merges knowledge graphs (KGs) by identifying the
equivalent entities in different graphs, which can effectively enrich knowledge
representations of KGs. However, in practice, different KGs often include
dangling entities whose counterparts cannot be found in the other graph, which
limits the performance of EA methods. To improve EA with dangling entities, we
propose an unsupervised method called Semi-constraint Optimal Transport for
Entity Alignment in Dangling cases (SoTead). Our main idea is to model the
entity alignment between two KGs as an optimal transport problem from one KG's
entities to the others. First, we set pseudo entity pairs between KGs based on
pretrained word embeddings. Then, we conduct contrastive metric learning to
obtain the transport cost between each entity pair. Finally, we introduce a
virtual entity for each KG to "align" the dangling entities from the other KGs,
which relaxes the optimization constraints and leads to a semi-constraint
optimal transport. In the experimental part, we first show the superiority of
SoTead on a commonly-used entity alignment dataset. Besides, to analyze the
ability for dangling entity detection with other baselines, we construct a
medical cross-lingual knowledge graph dataset, MedED, where our SoTead also
reaches state-of-the-art performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tevatron: An Efficient and Flexible Toolkit for Dense Retrieval. (arXiv:2203.05765v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05765">
<div class="article-summary-box-inner">
<span><p>Recent rapid advancements in deep pre-trained language models and the
introductions of large datasets have powered research in embedding-based dense
retrieval. While several good research papers have emerged, many of them come
with their own software stacks. These stacks are typically optimized for some
particular research goals instead of efficiency or code structure. In this
paper, we present Tevatron, a dense retrieval toolkit optimized for efficiency,
flexibility, and code simplicity. Tevatron provides a standardized pipeline for
dense retrieval including text processing, model training, corpus/query
encoding, and search. This paper presents an overview of Tevatron and
demonstrates its effectiveness and efficiency across several IR and QA data
sets. We also show how Tevatron's flexible design enables easy generalization
across datasets, model architectures, and accelerator platforms(GPU/TPU). We
believe Tevatron can serve as an effective software foundation for dense
retrieval system research including design, modeling, and optimization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERTopic: Neural topic modeling with a class-based TF-IDF procedure. (arXiv:2203.05794v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05794">
<div class="article-summary-box-inner">
<span><p>Topic models can be useful tools to discover latent topics in collections of
documents. Recent studies have shown the feasibility of approach topic modeling
as a clustering task. We present BERTopic, a topic model that extends this
process by extracting coherent topic representation through the development of
a class-based variation of TF-IDF. More specifically, BERTopic generates
document embedding with pre-trained transformer-based language models, clusters
these embeddings, and finally, generates topic representations with the
class-based TF-IDF procedure. BERTopic generates coherent topics and remains
competitive across a variety of benchmarks involving classical models and those
that follow the more recent clustering approach of topic modeling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Long Time No See! Open-Domain Conversation with Long-Term Persona Memory. (arXiv:2203.05797v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05797">
<div class="article-summary-box-inner">
<span><p>Most of the open-domain dialogue models tend to perform poorly in the setting
of long-term human-bot conversations. The possible reason is that they lack the
capability of understanding and memorizing long-term dialogue history
information. To address this issue, we present a novel task of Long-term Memory
Conversation (LeMon) and then build a new dialogue dataset DuLeMon and a
dialogue generation framework with Long-Term Memory (LTM) mechanism (called
PLATO-LTM). This LTM mechanism enables our system to accurately extract and
continuously update long-term persona memory without requiring multiple-session
dialogue datasets for model training. To our knowledge, this is the first
attempt to conduct real-time dynamic management of persona information of both
parties, including the user and the bot. Results on DuLeMon indicate that
PLATO-LTM can significantly outperform baselines in terms of long-term dialogue
consistency, leading to better dialogue engagingness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Open Intent Detection. (arXiv:2203.05823v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05823">
<div class="article-summary-box-inner">
<span><p>The open intent detection problem is presented in this paper, which aims to
identify known intents and detect open intent in natural language
understanding. Current methods have two core challenges. On the one hand, the
existing methods have limitations in learning robust representations to detect
the open intent without any prior knowledge. On the other hand, there lacks an
effective approach to learning the specific and compact decision boundary to
distinguish the known intents and the open intent. This paper introduces an
original pipeline framework, DA-ADB, to address these issues, which
successively learns discriminative intent features with distance-aware strategy
and appropriate decision boundaries adaptive to the feature space for open
intent detection. The proposed method first leverages distance information to
enhance the distinguishing capability of the intent representations. Then, it
obtains discriminative decision boundaries adaptive to the known intent feature
space by balancing both the empirical and open space risks. Extensive
experiments show the effectiveness of distance-aware and boundary learning
strategies. Compared with the state-of-the-art methods, our method achieves
substantial improvements on three benchmark intent datasets. It also yields
robust performance with different proportions of labeled data and known
categories.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Identification and Classification of Bragging in Social Media. (arXiv:2203.05840v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05840">
<div class="article-summary-box-inner">
<span><p>Bragging is a speech act employed with the goal of constructing a favorable
self-image through positive statements about oneself. It is widespread in daily
communication and especially popular in social media, where users aim to build
a positive image of their persona directly or indirectly. In this paper, we
present the first large scale study of bragging in computational linguistics,
building on previous research in linguistics and pragmatics. To facilitate
this, we introduce a new publicly available data set of tweets annotated for
bragging and their types. We empirically evaluate different transformer-based
models injected with linguistic information in (a) binary bragging
classification, i.e., if tweets contain bragging statements or not; and (b)
multi-class bragging type prediction including not bragging. Our results show
that our models can predict bragging with macro F1 up to 72.42 and 35.95 in the
binary and multi-class classification tasks respectively. Finally, we present
an extensive linguistic and error analysis of bragging prediction to guide
future research on this topic.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Interpretable Neuro-Symbolic Reasoning Framework for Task-Oriented Dialogue Generation. (arXiv:2203.05843v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05843">
<div class="article-summary-box-inner">
<span><p>We study the interpretability issue of task-oriented dialogue systems in this
paper. Previously, most neural-based task-oriented dialogue systems employ an
implicit reasoning strategy that makes the model predictions uninterpretable to
humans. To obtain a transparent reasoning process, we introduce neuro-symbolic
to perform explicit reasoning that justifies model decisions by reasoning
chains. Since deriving reasoning chains requires multi-hop reasoning for
task-oriented dialogues, existing neuro-symbolic approaches would induce error
propagation due to the one-phase design. To overcome this, we propose a
two-phase approach that consists of a hypothesis generator and a reasoner. We
first obtain multiple hypotheses, i.e., potential operations to perform the
desired task, through the hypothesis generator. Each hypothesis is then
verified by the reasoner, and the valid one is selected to conduct the final
prediction. The whole system is trained by exploiting raw textual dialogues
without using any reasoning chain annotations. Experimental studies on two
public benchmark datasets demonstrate that the proposed approach not only
achieves better results, but also introduces an interpretable decision process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Word Embeddings to Analyze Protests News. (arXiv:2203.05875v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05875">
<div class="article-summary-box-inner">
<span><p>The first two tasks of the CLEF 2019 ProtestNews events focused on
distinguishing between protest and non-protest related news articles and
sentences in a binary classification task. Among the submissions, two well
performing models have been chosen in order to replace the existing word
embeddings word2vec and FastTest with ELMo and DistilBERT. Unlike bag of words
or earlier vector approaches, ELMo and DistilBERT represent words as a sequence
of vectors by capturing the meaning based on contextual information in the
text. Without changing the architecture of the original models other than the
word embeddings, the implementation of DistilBERT improved the performance
measured on the F1-Score of 0.66 compared to the FastText implementation.
DistilBERT also outperformed ELMo in both tasks and models. Cleaning the
datasets by removing stopwords and lemmatizing the words has been shown to make
the models more generalizable across different contexts when training on a
dataset with Indian news articles and evaluating the models on a dataset with
news articles from China.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Sentence is Worth 128 Pseudo Tokens: A Semantic-Aware Contrastive Learning Framework for Sentence Embeddings. (arXiv:2203.05877v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05877">
<div class="article-summary-box-inner">
<span><p>Contrastive learning has shown great potential in unsupervised sentence
embedding tasks, e.g., SimCSE. However, We find that these existing solutions
are heavily affected by superficial features like the length of sentences or
syntactic structures. In this paper, we propose a semantics-aware contrastive
learning framework for sentence embeddings, termed Pseudo-Token BERT (PT-BERT),
which is able to exploit the pseudo-token space (i.e., latent semantic space)
representation of a sentence while eliminating the impact of superficial
features such as sentence length and syntax. Specifically, we introduce an
additional pseudo token embedding layer independent of the BERT encoder to map
each sentence into a sequence of pseudo tokens in a fixed length. Leveraging
these pseudo sequences, we are able to construct same-length positive and
negative pairs based on the attention mechanism to perform contrastive
learning. In addition, we utilize both the gradient-updating and
momentum-updating encoders to encode instances while dynamically maintaining an
additional queue to store the representation of sentence embeddings, enhancing
the encoder's learning performance for negative examples. Experiments show that
our model outperforms the state-of-the-art baselines on six standard semantic
textual similarity (STS) tasks. Furthermore, experiments on alignments and
uniformity losses, as well as hard examples with different sentence lengths and
syntax, consistently verify the effectiveness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Achieving Reliable Human Assessment of Open-Domain Dialogue Systems. (arXiv:2203.05899v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05899">
<div class="article-summary-box-inner">
<span><p>Evaluation of open-domain dialogue systems is highly challenging and
development of better techniques is highlighted time and again as desperately
needed. Despite substantial efforts to carry out reliable live evaluation of
systems in recent competitions, annotations have been abandoned and reported as
too unreliable to yield sensible results. This is a serious problem since
automatic metrics are not known to provide a good indication of what may or may
not be a high-quality conversation. Answering the distress call of competitions
that have emphasized the urgent need for better evaluation techniques in
dialogue, we present the successful development of human evaluation that is
highly reliable while still remaining feasible and low cost. Self-replication
experiments reveal almost perfectly repeatable results with a correlation of
$r=0.969$. Furthermore, due to the lack of appropriate methods of statistical
significance testing, the likelihood of potential improvements to systems
occurring due to chance is rarely taken into account in dialogue evaluation,
and the evaluation we propose facilitates application of standard tests. Since
we have developed a highly reliable evaluation method, new insights into system
performance can be revealed. We therefore include a comparison of
state-of-the-art models (i) with and without personas, to measure the
contribution of personas to conversation quality, as well as (ii) prescribed
versus freely chosen topics. Interestingly with respect to personas, results
indicate that personas do not positively contribute to conversation quality as
expected.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Integrating Dependency Tree Into Self-attention for Sentence Representation. (arXiv:2203.05918v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05918">
<div class="article-summary-box-inner">
<span><p>Recent progress on parse tree encoder for sentence representation learning is
notable. However, these works mainly encode tree structures recursively, which
is not conducive to parallelization. On the other hand, these works rarely take
into account the labels of arcs in dependency trees. To address both issues, we
propose Dependency-Transformer, which applies a relation-attention mechanism
that works in concert with the self-attention mechanism. This mechanism aims to
encode the dependency and the spatial positional relations between nodes in the
dependency tree of sentences. By a score-based method, we successfully inject
the syntax information without affecting Transformer's parallelizability. Our
model outperforms or is comparable to the state-of-the-art methods on four
tasks for sentence representation and has obvious advantages in computational
efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are discrete units necessary for Spoken Language Modeling?. (arXiv:2203.05936v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05936">
<div class="article-summary-box-inner">
<span><p>Recent work in spoken language modeling shows the possibility of learning a
language unsupervisedly from raw audio without any text labels. The approach
relies first on transforming the audio into a sequence of discrete units (or
pseudo-text) and then training a language model directly on such pseudo-text.
Is such a discrete bottleneck necessary, potentially introducing irreversible
errors in the encoding of the speech signal, or could we learn a language model
without discrete units at all? In this work, show that discretization is indeed
essential for good results in spoken language modeling, but that can omit the
discrete bottleneck if we use using discrete target features from a higher
level than the input features. We also show that an end-to-end model trained
with discrete target like HuBERT achieves similar results as the best language
model trained on pseudo-text on a set of zero-shot spoken language modeling
metrics from the Zero Resource Speech Challenge 2021.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Block-Sparse Adversarial Attack to Fool Transformer-Based Text Classifiers. (arXiv:2203.05948v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05948">
<div class="article-summary-box-inner">
<span><p>Recently, it has been shown that, in spite of the significant performance of
deep neural networks in different fields, those are vulnerable to adversarial
examples. In this paper, we propose a gradient-based adversarial attack against
transformer-based text classifiers. The adversarial perturbation in our method
is imposed to be block-sparse so that the resultant adversarial example differs
from the original sentence in only a few words. Due to the discrete nature of
textual data, we perform gradient projection to find the minimizer of our
proposed optimization problem. Experimental results demonstrate that, while our
adversarial attack maintains the semantics of the sentence, it can reduce the
accuracy of GPT-2 to less than 5% on different datasets (AG News, MNLI, and
Yelp Reviews). Furthermore, the block-sparsity constraint of the proposed
optimization problem results in small perturbations in the adversarial example.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Weibo Dataset for the 2022 Russo-Ukrainian Crisis. (arXiv:2203.05967v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05967">
<div class="article-summary-box-inner">
<span><p>Online social networks such as Twitter and Weibo play an important role in
how people stay informed and exchange reactions. Each crisis encompasses a new
opportunity to study the portability of models for various tasks (e.g.,
information extraction, complex event understanding, misinformation detection,
etc.), due to differences in domain, entities, and event types. We present the
Russia-Ukraine Crisis Weibo (RUW) dataset, with over 3.5M user posts and
comments in the first release. Our data is available at
https://github.com/yrf1/RussiaUkraine_weibo_dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active Evaluation: Efficient NLG Evaluation with Few Pairwise Comparisons. (arXiv:2203.06063v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06063">
<div class="article-summary-box-inner">
<span><p>Recent studies have shown the advantages of evaluating NLG systems using
pairwise comparisons as opposed to direct assessment. Given $k$ systems, a
naive approach for identifying the top-ranked system would be to uniformly
obtain pairwise comparisons from all ${k \choose 2}$ pairs of systems. However,
this can be very expensive as the number of human annotations required would
grow quadratically with $k$. In this work, we introduce Active Evaluation, a
framework to efficiently identify the top-ranked system by actively choosing
system pairs for comparison using dueling bandit algorithms. We perform
extensive experiments with 13 dueling bandits algorithms on 13 NLG evaluation
datasets spanning 5 tasks and show that the number of human annotations can be
reduced by 80%. To further reduce the number of human annotations, we propose
model-based dueling bandit algorithms which combine automatic evaluation
metrics with human evaluations. Specifically, we eliminate sub-optimal systems
even before the human annotation process and perform human evaluations only on
test examples where the automatic metric is highly uncertain. This reduces the
number of human annotations required further by 89%. In effect, we show that
identifying the top-ranked system requires only a few hundred human
annotations, which grow linearly with $k$. Lastly, we provide practical
recommendations and best practices to identify the top-ranked system
efficiently. Our code has been made publicly available at
https://github.com/akashkm99/duelnlg
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WLASL-LEX: a Dataset for Recognising Phonological Properties in American Sign Language. (arXiv:2203.06096v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06096">
<div class="article-summary-box-inner">
<span><p>Signed Language Processing (SLP) concerns the automated processing of signed
languages, the main means of communication of Deaf and hearing impaired
individuals. SLP features many different tasks, ranging from sign recognition
to translation and production of signed speech, but has been overlooked by the
NLP community thus far. In this paper, we bring to attention the task of
modelling the phonology of sign languages. We leverage existing resources to
construct a large-scale dataset of American Sign Language signs annotated with
six different phonological properties. We then conduct an extensive empirical
study to investigate whether data-driven end-to-end and feature-based
approaches can be optimised to automatically recognise these properties. We
find that, despite the inherent challenges of the task, graph-based neural
networks that operate over skeleton features extracted from raw videos are able
to succeed at the task to a varying degree. Most importantly, we show that this
performance pertains even on signs unobserved during training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LaPraDoR: Unsupervised Pretrained Dense Retriever for Zero-Shot Text Retrieval. (arXiv:2203.06169v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06169">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose LaPraDoR, a pretrained dual-tower dense retriever
that does not require any supervised data for training. Specifically, we first
present Iterative Contrastive Learning (ICoL) that iteratively trains the query
and document encoders with a cache mechanism. ICoL not only enlarges the number
of negative instances but also keeps representations of cached examples in the
same hidden space. We then propose Lexicon-Enhanced Dense Retrieval (LEDR) as a
simple yet effective way to enhance dense retrieval with lexical matching. We
evaluate LaPraDoR on the recently proposed BEIR benchmark, including 18
datasets of 9 zero-shot text retrieval tasks. Experimental results show that
LaPraDoR achieves state-of-the-art performance compared with supervised dense
retrieval models, and further analysis reveals the effectiveness of our
training strategy and objectives. Compared to re-ranking, our lexicon-enhanced
approach can be run in milliseconds (22.5x faster) while achieving superior
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Contextual Word-level Style Relevance for Unsupervised Style Transfer. (arXiv:2005.02049v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.02049">
<div class="article-summary-box-inner">
<span><p>Unsupervised style transfer aims to change the style of an input sentence
while preserving its original content without using parallel training data. In
current dominant approaches, owing to the lack of fine-grained control on the
influence from the target style,they are unable to yield desirable output
sentences. In this paper, we propose a novel attentional sequence-to-sequence
(Seq2seq) model that dynamically exploits the relevance of each output word to
the target style for unsupervised style transfer. Specifically, we first
pretrain a style classifier, where the relevance of each input word to the
original style can be quantified via layer-wise relevance propagation. In a
denoising auto-encoding manner, we train an attentional Seq2seq model to
reconstruct input sentences and repredict word-level previously-quantified
style relevance simultaneously. In this way, this model is endowed with the
ability to automatically predict the style relevance of each output word. Then,
we equip the decoder of this model with a neural style component to exploit the
predicted wordlevel style relevance for better style transfer. Particularly, we
fine-tune this model using a carefully-designed objective function involving
style transfer, style relevance consistency, content preservation and fluency
modeling loss terms. Experimental results show that our proposed model achieves
state-of-the-art performance in terms of both transfer accuracy and content
preservation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dr-Vectors: Decision Residual Networks and an Improved Loss for Speaker Recognition. (arXiv:2104.01989v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01989">
<div class="article-summary-box-inner">
<span><p>Many neural network speaker recognition systems model each speaker using a
fixed-dimensional embedding vector. These embeddings are generally compared
using either linear or 2nd-order scoring and, until recently, do not handle
utterance-specific uncertainty. In this work we propose scoring these
representations in a way that can capture uncertainty, enroll/test asymmetry
and additional non-linear information. This is achieved by incorporating a
2nd-stage neural network (known as a decision network) as part of an end-to-end
training regimen. In particular, we propose the concept of decision residual
networks which involves the use of a compact decision network to leverage
cosine scores and to model the residual signal that's needed. Additionally, we
present a modification to the generalized end-to-end softmax loss function to
target the separation of same/different speaker scores. We observed significant
performance gains for the two techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training Heterogeneous Features in Sequence to Sequence Tasks: Latent Enhanced Multi-filter Seq2Seq Model. (arXiv:2105.08840v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.08840">
<div class="article-summary-box-inner">
<span><p>In language processing, training data with extremely large variance may lead
to difficulty of language model's convergence. It is difficult for the network
parameters to adapt sentences with largely varied semantics or grammatical
structures. To resolve this problem, we introduce a model that concentrates the
each of the heterogeneous features in the input sentences. Build upon the
encoder-decoder architecture, we design a latent-enhanced multi-filter seq2seq
model (LEMS) that analyzes the input representations by introducing a latent
space transformation and clustering. The representations are extracted from the
final hidden state of the encoder and lie in the latent space. A latent space
transformation is applied for enhancing the quality of the representations.
Thus the clustering algorithm can easily separate samples based on the features
of these representations. Multiple filters are trained by the features from
their corresponding clusters, the heterogeneity of the training data can be
resolved accordingly. We conduct two sets of comparative experiments on
semantic parsing and machine translation, using the Geo-query dataset and
Multi30k English-French to demonstrate the enhancement our model has made
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Zero-shot Cross-lingual Transfer between Closely Related Languages by injecting Character-level Noise. (arXiv:2109.06772v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06772">
<div class="article-summary-box-inner">
<span><p>Cross-lingual transfer between a high-resource language and its dialects or
closely related language varieties should be facilitated by their similarity.
However, current approaches that operate in the embedding space do not take
surface similarity into account. This work presents a simple yet effective
strategy to imrove cross-lingual transfer between closely related varieties. We
propose to augment the data of the high-resource source language with
character-level noise to make the model more robust towards spelling
variations. Our strategy shows consistent improvements over several languages
and tasks: Zero-shot transfer of POS tagging and topic identification between
language varieties from the Finnic, West and North Germanic, and Western
Romance language branches. Our work provides evidence for the usefulness of
simple surface-level noise in improving transfer between language varieties.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interactively Providing Explanations for Transformer Language Models. (arXiv:2110.02058v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02058">
<div class="article-summary-box-inner">
<span><p>Transformer language models are state of the art in a multitude of NLP tasks.
Despite these successes, their opaqueness remains problematic. Recent methods
aiming to provide interpretability and explainability to black-box models
primarily focus on post-hoc explanations of (sometimes spurious) input-output
correlations. Instead, we emphasize using prototype networks directly
incorporated into the model architecture and hence explain the reasoning
process behind the network's decisions. Our architecture performs on par with
several language models and, moreover, enables learning from user interactions.
This not only offers a better understanding of language models but uses human
capabilities to incorporate knowledge outside of the rigid range of purely
data-driven approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document Understanding. (arXiv:2110.08518v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08518">
<div class="article-summary-box-inner">
<span><p>Multimodal pre-training with text, layout, and image has made significant
progress for Visually Rich Document Understanding (VRDU), especially the
fixed-layout documents such as scanned document images. While, there are still
a large number of digital documents where the layout information is not fixed
and needs to be interactively and dynamically rendered for visualization,
making existing layout-based pre-training approaches not easy to apply. In this
paper, we propose MarkupLM for document understanding tasks with markup
languages as the backbone, such as HTML/XML-based documents, where text and
markup information is jointly pre-trained. Experiment results show that the
pre-trained MarkupLM significantly outperforms the existing strong baseline
models on several document understanding tasks. The pre-trained model and code
will be publicly available at https://aka.ms/markuplm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Choice of Knowledge Base in Automated Claim Checking. (arXiv:2111.07795v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.07795">
<div class="article-summary-box-inner">
<span><p>Automated claim checking is the task of determining the veracity of a claim
given evidence found in a knowledge base of trustworthy facts. While previous
work has taken the knowledge base as given and optimized the claim-checking
pipeline, we take the opposite approach - taking the pipeline as given, we
explore the choice of knowledge base. Our first insight is that a
claim-checking pipeline can be transferred to a new domain of claims with
access to a knowledge base from the new domain. Second, we do not find a
"universally best" knowledge base - higher domain overlap of a task dataset and
a knowledge base tends to produce better label accuracy. Third, combining
multiple knowledge bases does not tend to improve performance beyond using the
closest-domain knowledge base. Finally, we show that the claim-checking
pipeline's confidence score for selecting evidence can be used to assess
whether a knowledge base will perform well for a new set of claims, even in the
absence of ground-truth labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Candidate Retrieval with Entity Profile Generation for Wikidata Entity Linking. (arXiv:2202.13404v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.13404">
<div class="article-summary-box-inner">
<span><p>Entity linking (EL) is the task of linking entity mentions in a document to
referent entities in a knowledge base (KB). Many previous studies focus on
Wikipedia-derived KBs. There is little work on EL over Wikidata, even though it
is the most extensive crowdsourced KB. The scale of Wikidata can open up many
new real-world applications, but its massive number of entities also makes EL
challenging. To effectively narrow down the search space, we propose a novel
candidate retrieval paradigm based on entity profiling. Wikidata entities and
their textual fields are first indexed into a text search engine (e.g.,
Elasticsearch). During inference, given a mention and its context, we use a
sequence-to-sequence (seq2seq) model to generate the profile of the target
entity, which consists of its title and description. We use the profile to
query the indexed search engine to retrieve candidate entities. Our approach
complements the traditional approach of using a Wikipedia anchor-text
dictionary, enabling us to further design a highly effective hybrid method for
candidate retrieval. Combined with a simple cross-attention reranker, our
complete EL framework achieves state-of-the-art results on three Wikidata-based
datasets and strong performance on TACKBP-2010.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Focus on the Target's Vocabulary: Masked Label Smoothing for Machine Translation. (arXiv:2203.02889v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02889">
<div class="article-summary-box-inner">
<span><p>Label smoothing and vocabulary sharing are two widely used techniques in
neural machine translation models. However, we argue that simply applying both
techniques can be conflicting and even leads to sub-optimal performance. When
allocating smoothed probability, original label smoothing treats the
source-side words that would never appear in the target language equally to the
real target-side words, which could bias the translation model. To address this
issue, we propose Masked Label Smoothing (MLS), a new mechanism that masks the
soft label probability of source-side words to zero. Simple yet effective, MLS
manages to better integrate label smoothing with vocabulary sharing. Our
extensive experiments show that MLS consistently yields improvement over
original label smoothing on different datasets, including bilingual and
multilingual translation from both translation quality and model's calibration.
Our code is released at https://github.com/PKUnlp-icler/MLS
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Evaluation of Answer-Agnostic Paragraph-level Multi-Question Generation. (arXiv:2203.04464v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04464">
<div class="article-summary-box-inner">
<span><p>We study the task of predicting a set of salient questions from a given
paragraph without any prior knowledge of the precise answer. We make two main
contributions. First, we propose a new method to evaluate a set of predicted
questions against the set of references by using the Hungarian algorithm to
assign predicted questions to references before scoring the assigned pairs. We
show that our proposed evaluation strategy has better theoretical and practical
properties compared to prior methods because it can properly account for the
coverage of references. Second, we compare different strategies to utilize a
pre-trained seq2seq model to generate and select a set of questions related to
a given paragraph. The code is available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Look Backward and Forward: Self-Knowledge Distillation with Bidirectional Decoder for Neural Machine Translation. (arXiv:2203.05248v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05248">
<div class="article-summary-box-inner">
<span><p>Neural Machine Translation(NMT) models are usually trained via unidirectional
decoder which corresponds to optimizing one-step-ahead prediction. However,
this kind of unidirectional decoding framework may incline to focus on local
structure rather than global coherence. To alleviate this problem, we propose a
novel method, Self-Knowledge Distillation with Bidirectional Decoder for Neural
Machine Translation(SBD-NMT). We deploy a backward decoder which can act as an
effective regularization method to the forward decoder. By leveraging the
backward decoder's information about the longer-term future, distilling
knowledge learned in the backward decoder can encourage auto-regressive NMT
models to plan ahead. Experiments show that our method is significantly better
than the strong Transformer baselines on multiple machine translation data
sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BEAT: A Large-Scale Semantic and Emotional Multi-Modal Dataset for Conversational Gestures Synthesis. (arXiv:2203.05297v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05297">
<div class="article-summary-box-inner">
<span><p>Achieving realistic, vivid, and human-like synthesized conversational
gestures conditioned on multi-modal data is still an unsolved problem, due to
the lack of available datasets, models and standard evaluation metrics. To
address this, we build Body-Expression-Audio-Text dataset, BEAT, which has i)
76 hours, high-quality, multi-modal data captured from 30 speakers talking with
eight different emotions and in four different languages, ii) 32 millions
frame-level emotion and semantic relevance annotations.Our statistical analysis
on BEAT demonstrates the correlation of conversational gestures with facial
expressions, emotions, and semantics, in addition to the known correlation with
audio, text, and speaker identity. Qualitative and quantitative experiments
demonstrate metrics' validness, ground truth data quality, and baseline's
state-of-the-art performance. To the best of our knowledge, BEAT is the largest
motion capture dataset for investigating the human gestures, which may
contribute to a number of different research fields including controllable
gesture synthesis, cross-modality analysis, emotional gesture recognition. The
data, code and model will be released for research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StyleBabel: Artistic Style Tagging and Captioning. (arXiv:2203.05321v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05321">
<div class="article-summary-box-inner">
<span><p>We present StyleBabel, a unique open access dataset of natural language
captions and free-form tags describing the artistic style of over 135K digital
artworks, collected via a novel participatory method from experts studying at
specialist art and design schools. StyleBabel was collected via an iterative
method, inspired by `Grounded Theory': a qualitative approach that enables
annotation while co-evolving a shared language for fine-grained artistic style
attribute description. We demonstrate several downstream tasks for StyleBabel,
adapting the recent ALADIN architecture for fine-grained style similarity, to
train cross-modal embeddings for: 1) free-form tag generation; 2) natural
language description of artistic style; 3) fine-grained text search of style.
To do so, we extend ALADIN with recent advances in Visual Transformer (ViT) and
cross-modal representation learning, achieving a state of the art accuracy in
fine-grained style retrieval.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Artificial Intelligence Solution for Effective Treatment Planning for Glioblastoma Patients. (arXiv:2203.05563v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05563">
<div class="article-summary-box-inner">
<span><p>Glioblastomas are the most common malignant brain tumors in adults.
Approximately 200000 people die each year from Glioblastoma in the world.
Glioblastoma patients have a median survival of 12 months with optimal therapy
and about 4 months without treatment. Glioblastomas appear as heterogeneous
necrotic masses with irregular peripheral enhancement, surrounded by vasogenic
edema. The current standard of care includes surgical resection, radiotherapy
and chemotherapy, which require accurate segmentation of brain tumor
subregions. For effective treatment planning, it is vital to identify the
methylation status of the promoter of Methylguanine Methyltransferase (MGMT), a
positive prognostic factor for chemotherapy. However, current methods for brain
tumor segmentation are tedious, subjective and not scalable, and current
techniques to determine the methylation status of MGMT promoter involve
surgically invasive procedures, which are expensive and time consuming. Hence
there is a pressing need to develop automated tools to segment brain tumors and
non-invasive methods to predict methylation status of MGMT promoter, to
facilitate better treatment planning and improve survival rate. I created an
integrated diagnostics solution powered by Artificial Intelligence to
automatically segment brain tumor subregions and predict MGMT promoter
methylation status, using brain MRI scans. My AI solution is proven on large
datasets with performance exceeding current standards and field tested with
data from teaching files of local neuroradiologists. With my solution,
physicians can submit brain MRI images, and get segmentation and methylation
predictions in minutes, and guide brain tumor patients with effective treatment
planning and ultimately improve survival time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HDL: Hybrid Deep Learning for the Synthesis of Myocardial Velocity Maps in Digital Twins for Cardiac Analysis. (arXiv:2203.05564v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05564">
<div class="article-summary-box-inner">
<span><p>Synthetic digital twins based on medical data accelerate the acquisition,
labelling and decision making procedure in digital healthcare. A core part of
digital healthcare twins is model-based data synthesis, which permits the
generation of realistic medical signals without requiring to cope with the
modelling complexity of anatomical and biochemical phenomena producing them in
reality. Unfortunately, algorithms for cardiac data synthesis have been so far
scarcely studied in the literature. An important imaging modality in the
cardiac examination is three-directional CINE multi-slice myocardial velocity
mapping (3Dir MVM), which provides a quantitative assessment of cardiac motion
in three orthogonal directions of the left ventricle. The long acquisition time
and complex acquisition produce make it more urgent to produce synthetic
digital twins of this imaging modality. In this study, we propose a hybrid deep
learning (HDL) network, especially for synthetic 3Dir MVM data. Our algorithm
is featured by a hybrid UNet and a Generative Adversarial Network with a
foreground-background generation scheme. The experimental results show that
from temporally down-sampled magnitude CINE images (six times), our proposed
algorithm can still successfully synthesise high temporal resolution 3Dir MVM
CMR data (PSNR=42.32) with precise left ventricle segmentation (DICE=0.92).
These performance scores indicate that our proposed HDL algorithm can be
implemented in real-world digital twins for myocardial velocity mapping data
simulation. To the best of our knowledge, this work is the first one in the
literature investigating digital twins of the 3Dir MVM CMR, which has shown
great potential for improving the efficiency of clinical studies via
synthesised cardiac data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LiftReg: Limited Angle 2D/3D Deformable Registration. (arXiv:2203.05565v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05565">
<div class="article-summary-box-inner">
<span><p>We propose LiftReg, a 2D/3D deformable registration approach. LiftReg is a
deep registration framework which is trained using sets of digitally
reconstructed radiographs (DRR) and computed tomography (CT) image pairs. By
using simulated training data, LiftReg can use a high-quality CT-CT image
similarity measure, which helps the network to learn a high-quality deformation
space. To further improve registration quality and to address the inherent
depth ambiguities of very limited angle acquisitions, we propose to use
features extracted from the backprojected 2D images and a statistical
deformation model. We test our approach on the DirLab lung registration dataset
and show that it outperforms an existing learning-based pairwise registration
approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recovering medical images from CT film photos. (arXiv:2203.05567v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05567">
<div class="article-summary-box-inner">
<span><p>While medical images such as computed tomography (CT) are stored in DICOM
format in hospital PACS, it is still quite routine in many countries to print a
film as a transferable medium for the purposes of self-storage and secondary
consultation. Also, with the ubiquitousness of mobile phone cameras, it is
quite common to take pictures of CT films, which unfortunately suffer from
geometric deformation and illumination variation. In this work, we study the
problem of recovering a CT film, which marks \textbf{the first attempt} in the
literature, to the best of our knowledge. We start with building a large-scale
head CT film database CTFilm20K, consisting of approximately 20,000 pictures,
using the widely used computer graphics software Blender. We also record all
accompanying information related to the geometric deformation (such as 3D
coordinate, depth, normal, and UV maps) and illumination variation (such as
albedo map). Then we propose a deep framework called \textbf{F}ilm
\textbf{I}mage \textbf{Re}covery \textbf{Net}work (\textbf{FIReNet}) to tackle
geometric deformation and illumination variation using the multiple maps
extracted from the CT films to collaboratively guide the recovery process.
Finally, we convert the dewarped images to DICOM files with our cascade model
for further analysis such as radiomics feature extraction. Extensive
experiments demonstrate the superiority of our approach over the previous
approaches. We plan to open source the simulated images and deep models for
promoting the research on CT film image analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unfolded Deep Kernel Estimation for Blind Image Super-resolution. (arXiv:2203.05568v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05568">
<div class="article-summary-box-inner">
<span><p>Blind image super-resolution (BISR) aims to reconstruct a high-resolution
image from its low-resolution counterpart degraded by unknown blur kernel and
noise. Many deep neural network based methods have been proposed to tackle this
challenging problem without considering the image degradation model. However,
they largely rely on the training sets and often fail to handle images with
unseen blur kernels during inference. Deep unfolding methods have also been
proposed to perform BISR by utilizing the degradation model. Nonetheless, the
existing deep unfolding methods cannot explicitly solve the data term of the
unfolding objective function, limiting their capability in blur kernel
estimation. In this work, we propose a novel unfolded deep kernel estimation
(UDKE) method, which, for the first time to our best knowledge, explicitly
solves the data term with high efficiency. The UDKE based BISR method can
jointly learn image and kernel priors in an end-to-end manner, and it can
effectively exploit the information in both training data and image degradation
model. Experiments on benchmark datasets and real-world data demonstrate that
the proposed UDKE method could well predict complex unseen non-Gaussian blur
kernels in inference, achieving significantly better BISR performance than
state-of-the-art. The source code of UDKE is available at:
https://github.com/natezhenghy/UDKE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Autofocusing+: Noise-Resilient Motion Correction in Magnetic Resonance Imaging. (arXiv:2203.05569v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05569">
<div class="article-summary-box-inner">
<span><p>Image corruption by motion artifacts is an ingrained problem in Magnetic
Resonance Imaging (MRI). In this work, we propose a neural network-based
regularization term to enhance Autofocusing, a classic optimization-based
method to remove motion artifacts. The method takes the best of both worlds:
the optimization-based routine iteratively executes the blind demotion and deep
learning-based prior penalizes for unrealistic restorations and speeds up the
convergence. We validate the method on three models of motion trajectories,
using synthetic and real noisy data. The method proves resilient to noise and
anatomic structure variation, outperforming the state-of-the-art demotion
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Convolutional Neural Networks for Molecular Subtyping of Gliomas Using Magnetic Resonance Imaging. (arXiv:2203.05571v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05571">
<div class="article-summary-box-inner">
<span><p>Knowledge of molecular subtypes of gliomas can provide valuable information
for tailored therapies. This study aimed to investigate the use of deep
convolutional neural networks (DCNNs) for noninvasive glioma subtyping with
radiological imaging data according to the new taxonomy announced by the World
Health Organization in 2016. Methods: A DCNN model was developed for the
prediction of the five glioma subtypes based on a hierarchical classification
paradigm. This model used three parallel, weight-sharing, deep residual
learning networks to process 2.5-dimensional input of trimodal MRI data,
including T1-weighted, T1-weighted with contrast enhancement, and T2-weighted
images. A data set comprising 1,016 real patients was collected for evaluation
of the developed DCNN model. The predictive performance was evaluated via the
area under the curve (AUC) from the receiver operating characteristic analysis.
For comparison, the performance of a radiomics-based approach was also
evaluated. Results: The AUCs of the DCNN model for the four classification
tasks in the hierarchical classification paradigm were 0.89, 0.89, 0.85, and
0.66, respectively, as compared to 0.85, 0.75, 0.67, and 0.59 of the radiomics
approach. Conclusion: The results showed that the developed DCNN model can
predict glioma subtypes with promising performance, given sufficient,
non-ill-balanced training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self Pre-training with Masked Autoencoders for Medical Image Analysis. (arXiv:2203.05573v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05573">
<div class="article-summary-box-inner">
<span><p>Masked Autoencoder (MAE) has recently been shown to be effective in
pre-training Vision Transformers (ViT) for natural image analysis. By
performing the pretext task of reconstructing the original image from only
partial observations, the encoder, which is a ViT, is encouraged to aggregate
contextual information to infer content in masked image regions. We believe
that this context aggregation ability is also essential to the medical image
domain where each anatomical structure is functionally and mechanically
connected to other structures and regions. However, there is no ImageNet-scale
medical image dataset for pre-training. Thus, in this paper, we investigate a
self pre-training paradigm with MAE for medical images, i.e., models are
pre-trained on the same target dataset. To validate the MAE self pre-training,
we consider three diverse medical image tasks including chest X-ray disease
classification, CT abdomen multi-organ segmentation and MRI brain tumor
segmentation. It turns out MAE self pre-training benefits all the tasks
markedly. Specifically, the mAUC on lung disease classification is increased by
9.4%. The average DSC on brain tumor segmentation is improved from 77.4% to
78.9%. Most interestingly, on the small-scale multi-organ segmentation dataset
(N=30), the average DSC improves from 78.8% to 83.5% and the HD95 is reduced by
60%, indicating its effectiveness in limited data scenarios. The segmentation
and classification results reveal the promising potential of MAE self
pre-training for medical image analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On-the-Fly Test-time Adaptation for Medical Image Segmentation. (arXiv:2203.05574v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05574">
<div class="article-summary-box-inner">
<span><p>One major problem in deep learning-based solutions for medical imaging is the
drop in performance when a model is tested on a data distribution different
from the one that it is trained on. Adapting the source model to target data
distribution at test-time is an efficient solution for the data-shift problem.
Previous methods solve this by adapting the model to target distribution by
using techniques like entropy minimization or regularization. In these methods,
the models are still updated by back-propagation using an unsupervised loss on
complete test data distribution. In real-world clinical settings, it makes more
sense to adapt a model to a new test image on-the-fly and avoid model update
during inference due to privacy concerns and lack of computing resource at
deployment. To this end, we propose a new setting - On-the-Fly Adaptation which
is zero-shot and episodic (i.e., the model is adapted to a single image at a
time and also does not perform any back-propagation during test-time). To
achieve this, we propose a new framework called Adaptive UNet where each
convolutional block is equipped with an adaptive batch normalization layer to
adapt the features with respect to a domain code. The domain code is generated
using a pre-trained encoder trained on a large corpus of medical images. During
test-time, the model takes in just the new test image and generates a domain
code to adapt the features of source model according to the test data. We
validate the performance on both 2D and 3D data distribution shifts where we
get a better performance compared to previous test-time adaptation methods.
Code is available at https://github.com/jeya-maria-jose/On-The-Fly-Adaptation
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human Face Recognition from Part of a Facial Image based on Image Stitching. (arXiv:2203.05601v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05601">
<div class="article-summary-box-inner">
<span><p>Most of the current techniques for face recognition require the presence of a
full face of the person to be recognized, and this situation is difficult to
achieve in practice, the required person may appear with a part of his face,
which requires prediction of the part that did not appear. Most of the current
forecasting processes are done by what is known as image interpolation, which
does not give reliable results, especially if the missing part is large. In
this work, we adopted the process of stitching the face by completing the
missing part with the flipping of the part shown in the picture, depending on
the fact that the human face is characterized by symmetry in most cases. To
create a complete model, two facial recognition methods were used to prove the
efficiency of the algorithm. The selected face recognition algorithms that are
applied here are Eigenfaces and geometrical methods. Image stitching is the
process during which distinctive photographic images are combined to make a
complete scene or a high-resolution image. Several images are integrated to
form a wide-angle panoramic image. The quality of the image stitching is
determined by calculating the similarity among the stitched image and original
images and by the presence of the seam lines through the stitched images. The
Eigenfaces approach utilizes PCA calculation to reduce the feature vector
dimensions. It provides an effective approach for discovering the
lower-dimensional space. In addition, to enable the proposed algorithm to
recognize the face, it also ensures a fast and effective way of classifying
faces. The phase of feature extraction is followed by the classifier phase.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gesture based Arabic Sign Language Recognition for Impaired People based on Convolution Neural Network. (arXiv:2203.05602v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05602">
<div class="article-summary-box-inner">
<span><p>The Arabic Sign Language has endorsed outstanding research achievements for
identifying gestures and hand signs using the deep learning methodology. The
term "forms of communication" refers to the actions used by hearing-impaired
people to communicate. These actions are difficult for ordinary people to
comprehend. The recognition of Arabic Sign Language (ArSL) has become a
difficult study subject due to variations in Arabic Sign Language (ArSL) from
one territory to another and then within states. The Convolution Neural Network
has been encapsulated in the proposed system which is based on the machine
learning technique. For the recognition of the Arabic Sign Language, the
wearable sensor is utilized. This approach has been used a different system
that could suit all Arabic gestures. This could be used by the impaired people
of the local Arabic community. The research method has been used with
reasonable and moderate accuracy. A deep Convolutional network is initially
developed for feature extraction from the data gathered by the sensing devices.
These sensors can reliably recognize the Arabic sign language's 30 hand sign
letters. The hand movements in the dataset were captured using DG5-V hand
gloves with wearable sensors. For categorization purposes, the CNN technique is
used. The suggested system takes Arabic sign language hand gestures as input
and outputs vocalized speech as output. The results were recognized by 90% of
the people.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning-Based Perceptual Stimulus Encoder for Bionic Vision. (arXiv:2203.05604v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05604">
<div class="article-summary-box-inner">
<span><p>Retinal implants have the potential to treat incurable blindness, yet the
quality of the artificial vision they produce is still rudimentary. An
outstanding challenge is identifying electrode activation patterns that lead to
intelligible visual percepts (phosphenes). Here we propose a PSE based on CNN
that is trained in an end-to-end fashion to predict the electrode activation
patterns required to produce a desired visual percept. We demonstrate the
effectiveness of the encoder on MNIST using a psychophysically validated
phosphene model tailored to individual retinal implant users. The present work
constitutes an essential first step towards improving the quality of the
artificial vision provided by retinal implants.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">City-wide Street-to-Satellite Image Geolocalization of a Mobile Ground Agent. (arXiv:2203.05612v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05612">
<div class="article-summary-box-inner">
<span><p>Cross-view image geolocalization provides an estimate of an agent's global
position by matching a local ground image to an overhead satellite image
without the need for GPS. It is challenging to reliably match a ground image to
the correct satellite image since the images have significant viewpoint
differences. Existing works have demonstrated localization in constrained
scenarios over small areas but have not demonstrated wider-scale localization.
Our approach, called Wide-Area Geolocalization (WAG), combines a neural network
with a particle filter to achieve global position estimates for agents moving
in GPS-denied environments, scaling efficiently to city-scale regions. WAG
introduces a trinomial loss function for a Siamese network to robustly match
non-centered image pairs and thus enables the generation of a smaller satellite
image database by coarsely discretizing the search area. A modified particle
filter weighting scheme is also presented to improve localization accuracy and
convergence. Taken together, WAG's network training and particle filter
weighting approach achieves city-scale position estimation accuracies on the
order of 20 meters, a 98% reduction compared to a baseline training and
weighting approach. Applied to a smaller-scale testing area, WAG reduces the
final position estimation error by 64% compared to a state-of-the-art baseline
from the literature. WAG's search space discretization additionally
significantly reduces storage and processing requirements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PETR: Position Embedding Transformation for Multi-View 3D Object Detection. (arXiv:2203.05625v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05625">
<div class="article-summary-box-inner">
<span><p>In this paper, we develop position embedding transformation (PETR) for
multi-view 3D object detection. PETR encodes the position information of 3D
coordinates into image features, producing the 3D position-aware features.
Object query can perceive the 3D position-aware features and perform end-to-end
object detection. PETR achieves state-of-the-art performance (50.4% NDS and
44.1% mAP) on standard nuScenes dataset and ranks 1st place on the benchmark.
It can serve as a simple yet strong baseline for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">High Definition, Inexpensive, Underwater Mapping. (arXiv:2203.05640v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05640">
<div class="article-summary-box-inner">
<span><p>In this paper we present a complete framework for Underwater SLAM utilizing a
single inexpensive sensor. Over the recent years, imaging technology of action
cameras is producing stunning results even under the challenging conditions of
the underwater domain. The GoPro 9 camera provides high definition video in
synchronization with an Inertial Measurement Unit (IMU) data stream encoded in
a single mp4 file. The visual inertial SLAM framework is augmented to adjust
the map after each loop closure. Data collected at an artificial wreck of the
coast of South Carolina and in caverns and caves in Florida demonstrate the
robustness of the proposed approach in a variety of conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attack Analysis of Face Recognition Authentication Systems Using Fast Gradient Sign Method. (arXiv:2203.05653v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05653">
<div class="article-summary-box-inner">
<span><p>Biometric authentication methods, representing the "something you are"
scheme, are considered the most secure approach for gaining access to protected
resources. Recent attacks using Machine Learning techniques demand a serious
systematic reevaluation of biometric authentication. This paper analyzes and
presents the Fast Gradient Sign Method (FGSM) attack using face recognition for
biometric authentication. Machine Learning techniques have been used to train
and test the model, which can classify and identify different people's faces
and which will be used as a target for carrying out the attack. Furthermore,
the case study will analyze the implementation of the FGSM and the level of
performance reduction that the model will have by applying this method in
attacking. The test results were performed with the change of parameters both
in terms of training and attacking the model, thus showing the efficiency of
applying the FGSM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Point Density-Aware Voxels for LiDAR 3D Object Detection. (arXiv:2203.05662v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05662">
<div class="article-summary-box-inner">
<span><p>LiDAR has become one of the primary 3D object detection sensors in autonomous
driving. However, LiDAR's diverging point pattern with increasing distance
results in a non-uniform sampled point cloud ill-suited to discretized
volumetric feature extraction. Current methods either rely on voxelized point
clouds or use inefficient farthest point sampling to mitigate detrimental
effects caused by density variation but largely ignore point density as a
feature and its predictable relationship with distance from the LiDAR sensor.
Our proposed solution, Point Density-Aware Voxel network (PDV), is an
end-to-end two stage LiDAR 3D object detection architecture that is designed to
account for these point density variations. PDV efficiently localizes voxel
features from the 3D sparse convolution backbone through voxel point centroids.
The spatially localized voxel features are then aggregated through a
density-aware RoI grid pooling module using kernel density estimation (KDE) and
self-attention with point density positional encoding. Finally, we exploit
LiDAR's point density to distance relationship to refine our final bounding box
confidences. PDV outperforms all state-of-the-art methods on the Waymo Open
Dataset and achieves competitive results on the KITTI dataset. We provide a
code release for PDV which is available at https://github.com/TRAILab/PDV.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Overlooked Classifier in Human-Object Interaction Recognition. (arXiv:2203.05676v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05676">
<div class="article-summary-box-inner">
<span><p>Human-Object Interaction (HOI) recognition is challenging due to two factors:
(1) significant imbalance across classes and (2) requiring multiple labels per
image. This paper shows that these two challenges can be effectively addressed
by improving the classifier with the backbone architecture untouched. Firstly,
we encode the semantic correlation among classes into the classification head
by initializing the weights with language embeddings of HOIs. As a result, the
performance is boosted significantly, especially for the few-shot subset.
Secondly, we propose a new loss named LSE-Sign to enhance multi-label learning
on a long-tailed dataset. Our simple yet effective method enables
detection-free HOI classification, outperforming the state-of-the-arts that
require object detection and human pose by a clear margin. Moreover, we
transfer the classification model to instance-level HOI detection by connecting
it with an off-the-shelf object detector. We achieve state-of-the-art without
additional fine-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Labeling Representations in Uncertainty-based Semi-supervised Segmentation. (arXiv:2203.05682v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05682">
<div class="article-summary-box-inner">
<span><p>Semi-supervised segmentation tackles the scarcity of annotations by
leveraging unlabeled data with a small amount of labeled data. A prominent way
to utilize the unlabeled data is by consistency training which commonly uses a
teacher-student network, where a teacher guides a student segmentation. The
predictions of unlabeled data are not reliable, therefore, uncertainty-aware
methods have been proposed to gradually learn from meaningful and reliable
predictions. Uncertainty estimation, however, relies on multiple inferences
from model predictions that need to be computed for each training step, which
is computationally expensive. This work proposes a novel method to estimate the
pixel-level uncertainty by leveraging the labeling representation of
segmentation masks. On the one hand, a labeling representation is learnt to
represent the available segmentation masks. The learnt labeling representation
is used to map the prediction of the segmentation into a set of plausible
masks. Such a reconstructed segmentation mask aids in estimating the
pixel-level uncertainty guiding the segmentation network. The proposed method
estimates the uncertainty with a single inference from the labeling
representation, thereby reducing the total computation. We evaluate our method
on the 3D segmentation of left atrium in MRI, and we show that our uncertainty
estimates from our labeling representation improve the segmentation accuracy
over state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Multimodal Guidance for Medical Image Classification. (arXiv:2203.05683v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05683">
<div class="article-summary-box-inner">
<span><p>Medical imaging is a cornerstone of therapy and diagnosis in modern medicine.
However, the choice of imaging modality for a particular theranostic task
typically involves trade-offs between the feasibility of using a particular
modality (e.g., short wait times, low cost, fast acquisition, reduced
radiation/invasiveness) and the expected performance on a clinical task (e.g.,
diagnostic accuracy, efficacy of treatment planning and guidance). In this
work, we aim to apply the knowledge learned from the less feasible but
better-performing (superior) modality to guide the utilization of the
more-feasible yet under-performing (inferior) modality and steer it towards
improved performance. We focus on the application of deep learning for
image-based diagnosis. We develop a light-weight guidance model that leverages
the latent representation learned from the superior modality, when training a
model that consumes only the inferior modality. We examine the advantages of
our method in the context of two clinical applications: multi-task skin lesion
classification from clinical and dermoscopic images and brain tumor
classification from multi-sequence magnetic resonance imaging (MRI) and
histopathology images. For both these scenarios we show a boost in diagnostic
performance of the inferior modality without requiring the superior modality.
Furthermore, in the case of brain tumor classification, our method outperforms
the model trained on the superior modality while producing comparable results
to the model that uses both modalities during inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PC-SwinMorph: Patch Representation for Unsupervised Medical Image Registration and Segmentation. (arXiv:2203.05684v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05684">
<div class="article-summary-box-inner">
<span><p>Medical image registration and segmentation are critical tasks for several
clinical procedures. Manual realisation of those tasks is time-consuming and
the quality is highly dependent on the level of expertise of the physician. To
mitigate that laborious task, automatic tools have been developed where the
majority of solutions are supervised techniques. However, in medical domain,
the strong assumption of having a well-representative ground truth is far from
being realistic. To overcome this challenge, unsupervised techniques have been
investigated. However, they are still limited in performance and they fail to
produce plausible results. In this work, we propose a novel unified
unsupervised framework for image registration and segmentation that we called
PC-SwinMorph. The core of our framework is two patch-based strategies, where we
demonstrate that patch representation is key for performance gain. We first
introduce a patch-based contrastive strategy that enforces locality conditions
and richer feature representation. Secondly, we utilise a 3D
window/shifted-window multi-head self-attention module as a patch stitching
strategy to eliminate artifacts from the patch splitting. We demonstrate,
through a set of numerical and visual results, that our technique outperforms
current state-of-the-art unsupervised techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward Efficient Hyperspectral Image Processing inside Camera Pixels. (arXiv:2203.05696v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05696">
<div class="article-summary-box-inner">
<span><p>Hyperspectral cameras generate a large amount of data due to the presence of
hundreds of spectral bands as opposed to only three channels (red, green, and
blue) in traditional cameras. This requires a significant amount of data
transmission between the hyperspectral image sensor and a processor used to
classify/detect/track the images, frame by frame, expending high energy and
causing bandwidth and security bottlenecks. To mitigate this problem, we
propose a form of processing-in-pixel (PIP) that leverages advanced CMOS
technologies to enable the pixel array to perform a wide range of complex
operations required by the modern convolutional neural networks (CNN) for
hyperspectral image recognition (HSI). Consequently, our PIP-optimized custom
CNN layers effectively compress the input data, significantly reducing the
bandwidth required to transmit the data downstream to the HSI processing unit.
This reduces the average energy consumption associated with pixel array of
cameras and the CNN processing unit by 25.06x and 3.90x respectively, compared
to existing hardware implementations. Our custom models yield average test
accuracies within 0.56% of the baseline models for the standard HSI benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning-based Localizability Estimation for Robust LiDAR Localization. (arXiv:2203.05698v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05698">
<div class="article-summary-box-inner">
<span><p>LiDAR-based localization and mapping is one of the core components in many
modern robotic systems due to the direct integration of range and geometry,
allowing for precise motion estimation and generation of high quality maps in
real-time. Yet, as a consequence of insufficient environmental constraints
present in the scene, this dependence on geometry can result in localization
failure, happening in self-symmetric surroundings such as tunnels. This work
addresses precisely this issue by proposing a neural network-based estimation
approach for detecting (non-)localizability during robot operation. Special
attention is given to the localizability of scan-to-scan registration, as it is
a crucial component in many LiDAR odometry estimation pipelines. In contrast to
previous, mostly traditional detection approaches, the proposed method enables
early detection of failure by estimating the localizability on raw sensor
measurements without evaluating the underlying registration optimization.
Moreover, previous approaches remain limited in their ability to generalize
across environments and sensor types, as heuristic-tuning of degeneracy
detection thresholds is required. The proposed approach avoids this problem by
learning from a corpus of different environments, allowing the network to
function over various scenarios. Furthermore, the network is trained
exclusively on simulated data, avoiding arduous data collection in challenging
and degenerate, often hard-to-access, environments. The presented method is
tested during field experiments conducted across challenging environments and
on two different sensor types without any modifications. The observed detection
performance is on par with state-of-the-art methods after environment-specific
threshold tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">6-DoF Pose Estimation of Household Objects for Robotic Manipulation: An Accessible Dataset and Benchmark. (arXiv:2203.05701v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05701">
<div class="article-summary-box-inner">
<span><p>We present a new dataset for 6-DoF pose estimation of known objects, with a
focus on robotic manipulation research. We propose a set of toy grocery
objects, whose physical instantiations are readily available for purchase and
are appropriately sized for robotic grasping and manipulation. We provide 3D
scanned textured models of these objects, suitable for generating synthetic
training data, as well as RGBD images of the objects in challenging, cluttered
scenes exhibiting partial occlusion, extreme lighting variations, multiple
instances per image, and a large variety of poses. Using semi-automated
RGBD-to-model texture correspondences, the images are annotated with ground
truth poses that were verified empirically to be accurate to within a few
millimeters. We also propose a new pose evaluation metric called {ADD-H} based
upon the Hungarian assignment algorithm that is robust to symmetries in object
geometry without requiring their explicit enumeration. We share pre-trained
pose estimators for all the toy grocery objects, along with their baseline
performance on both validation and test sets. We offer this dataset to the
community to help connect the efforts of computer vision researchers with the
needs of roboticists.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Geometric Synthesis: A Free lunch for Large-scale Palmprint Recognition Model Pretraining. (arXiv:2203.05703v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05703">
<div class="article-summary-box-inner">
<span><p>Palmprints are private and stable information for biometric recognition. In
the deep learning era, the development of palmprint recognition is limited by
the lack of sufficient training data. In this paper, by observing that palmar
creases are the key information to deep-learning-based palmprint recognition,
we propose to synthesize training data by manipulating palmar creases.
Concretely, we introduce an intuitive geometric model which represents palmar
creases with parameterized B\'ezier curves. By randomly sampling B\'ezier
parameters, we can synthesize massive training samples of diverse identities,
which enables us to pretrain large-scale palmprint recognition models.
Experimental results demonstrate that such synthetically pretrained models have
a very strong generalization ability: they can be efficiently transferred to
real datasets, leading to significant performance improvements on palmprint
recognition. For example, under the open-set protocol, our method improves the
strong ArcFace baseline by more than 10\% in terms of TAR@1e-6. And under the
closed-set protocol, our method reduces the equal error rate (EER) by an order
of magnitude.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Bi-directional Skip Connections in Encoder-Decoder Architectures and Beyond. (arXiv:2203.05709v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05709">
<div class="article-summary-box-inner">
<span><p>U-Net, as an encoder-decoder architecture with forward skip connections, has
achieved promising results in various medical image analysis tasks. Many recent
approaches have also extended U-Net with more complex building blocks, which
typically increase the number of network parameters considerably. Such
complexity makes the inference stage highly inefficient for clinical
applications. Towards an effective yet economic segmentation network design, in
this work, we propose backward skip connections that bring decoded features
back to the encoder. Our design can be jointly adopted with forward skip
connections in any encoder-decoder architecture forming a recurrence structure
without introducing extra parameters. With the backward skip connections, we
propose a U-Net based network family, namely Bi-directional O-shape networks,
which set new benchmarks on multiple public medical imaging segmentation
datasets. On the other hand, with the most plain architecture (BiO-Net),
network computations inevitably increase along with the pre-set recurrence
time. We have thus studied the deficiency bottleneck of such recurrent design
and propose a novel two-phase Neural Architecture Search (NAS) algorithm,
namely BiX-NAS, to search for the best multi-scale bi-directional skip
connections. The ineffective skip connections are then discarded to reduce
computational costs and speed up network inference. The finally searched
BiX-Net yields the least network complexity and outperforms other
state-of-the-art counterparts by large margins. We evaluate our methods on both
2D and 3D segmentation tasks in a total of six datasets. Extensive ablation
studies have also been conducted to provide a comprehensive analysis for our
proposed methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synopses of Movie Narratives: a Video-Language Dataset for Story Understanding. (arXiv:2203.05711v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05711">
<div class="article-summary-box-inner">
<span><p>Despite recent advances of AI, story understanding remains an open and
under-investigated problem. We collect, preprocess, and publicly release a
video-language story dataset, Synopses of Movie Narratives(SyMoN), containing
5,193 video summaries of popular movies and TV series. SyMoN captures
naturalistic storytelling videos for human audience made by human creators, and
has higher story coverage and more frequent mental-state references than
similar video-language story datasets. Differing from most existing video-text
datasets, SyMoN features large semantic gaps between the visual and the textual
modalities due to the prevalence of reporting bias and mental state
descriptions. We establish benchmarks on video-text retrieval and zero-shot
alignment on movie summary videos. With SyMoN, we hope to lay the groundwork
for progress in multimodal story understanding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Scale Consistent Monocular Visual Odometry by Learning from the Virtual World. (arXiv:2203.05712v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05712">
<div class="article-summary-box-inner">
<span><p>Monocular visual odometry (VO) has attracted extensive research attention by
providing real-time vehicle motion from cost-effective camera images. However,
state-of-the-art optimization-based monocular VO methods suffer from the scale
inconsistency problem for long-term predictions. Deep learning has recently
been introduced to address this issue by leveraging stereo sequences or
ground-truth motions in the training dataset. However, it comes at an
additional cost for data collection, and such training data may not be
available in all datasets. In this work, we propose VRVO, a novel framework for
retrieving the absolute scale from virtual data that can be easily obtained
from modern simulation environments, whereas in the real domain no stereo or
ground-truth data are required in either the training or inference phases.
Specifically, we first train a scale-aware disparity network using both
monocular real images and stereo virtual data. The virtual-to-real domain gap
is bridged by using an adversarial training strategy to map images from both
domains into a shared feature space. The resulting scale-consistent disparities
are then integrated with a direct VO system by constructing a virtual stereo
objective that ensures the scale consistency over long trajectories.
Additionally, to address the suboptimality issue caused by the separate
optimization backend and the learning process, we further propose a mutual
reinforcement pipeline that allows bidirectional information flow between
learning and optimization, which boosts the robustness and accuracy of each
other. We demonstrate the effectiveness of our framework on the KITTI and
vKITTI2 datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image-based Stroke Assessment for Multi-site Preclinical Evaluation of Cerebroprotectants. (arXiv:2203.05714v1 [q-bio.QM])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05714">
<div class="article-summary-box-inner">
<span><p>Ischemic stroke is a leading cause of death worldwide, but there has been
little success translating putative cerebroprotectants from preclinical trials
to patients. We investigated computational image-based assessment tools for
practical improvement of the quality, scalability, and outlook for large scale
preclinical screening for potential therapeutic interventions. We developed,
evaluated, and deployed a pipeline for image-based stroke outcome
quantification for the Stroke Prelinical Assessment Network (SPAN), which is a
multi-site, multi-arm, multi-stage study evaluating a suite of
cerebroprotectant interventions. Our fully automated pipeline combines
state-of-the-art algorithmic and data analytic approaches to assess stroke
outcomes from multi-parameter MRI data collected longitudinally from a rodent
model of middle cerebral artery occlusion (MCAO), including measures of infarct
volume, brain atrophy, midline shift, and data quality. We tested our approach
with 1,368 scans and report population level results of lesion extent and
longitudinal changes from injury. We validated our system by comparison with
manual annotations of coronal MRI slices and tissue sections from the same
brain, using crowdsourcing from blinded stroke experts from the network. Our
results demonstrate the efficacy and robustness of our image-based stroke
assessments. The pipeline may provide a promising resource for ongoing
preclinical studies conducted by SPAN and other networks in the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating U-net Brain Extraction for Multi-site and Longitudinal Preclinical Stroke Imaging. (arXiv:2203.05716v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05716">
<div class="article-summary-box-inner">
<span><p>Rodent stroke models are important for evaluating treatments and
understanding the pathophysiology and behavioral changes of brain ischemia, and
magnetic resonance imaging (MRI) is a valuable tool for measuring outcome in
preclinical studies. Brain extraction is an essential first step in most
neuroimaging pipelines; however, it can be challenging in the presence of
severe pathology and when dataset quality is highly variable. Convolutional
neural networks (CNNs) can improve accuracy and reduce operator time,
facilitating high throughput preclinical studies. As part of an ongoing
preclinical stroke imaging study, we developed a deep-learning mouse brain
extraction tool by using a U-net CNN. While previous studies have evaluated
U-net architectures, we sought to evaluate their practical performance across
data types. We ask how performance is affected with data across: six imaging
centers, two time points after experimental stroke, and across four MRI
contrasts. We trained, validated, and tested a typical U-net model on 240
multimodal MRI datasets including quantitative multi-echo T2 and apparent
diffusivity coefficient (ADC) maps, and performed qualitative evaluation with a
large preclinical stroke database (N=1,368). We describe the design and
development of this system, and report our findings linking data
characteristics to segmentation performance. We consistently found high
accuracy and ability of the U-net architecture to generalize performance in a
range of 95-97% accuracy, with only modest reductions in performance based on
lower fidelity imaging hardware and brain pathology. This work can help inform
the design of future preclinical rodent imaging studies and improve their
scalability and reliability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Information-Theoretic Odometry Learning. (arXiv:2203.05724v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05724">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a unified information theoretic framework for
learning-motivated methods aimed at odometry estimation, a crucial component of
many robotics and vision tasks such as navigation and virtual reality where
relative camera poses are required in real time. We formulate this problem as
optimizing a variational information bottleneck objective function, which
eliminates pose-irrelevant information from the latent representation. The
proposed framework provides an elegant tool for performance evaluation and
understanding in information-theoretic language. Specifically, we bound the
generalization errors of the deep information bottleneck framework and the
predictability of the latent representation. These provide not only a
performance guarantee but also practical guidance for model design, sample
collection, and sensor selection. Furthermore, the stochastic latent
representation provides a natural uncertainty measure without the needs for
extra structures or computations. Experiments on two well-known odometry
datasets demonstrate the effectiveness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual-Domain Reconstruction Networks with V-Net and K-Net for fast MRI. (arXiv:2203.05725v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05725">
<div class="article-summary-box-inner">
<span><p>Partial scan is a common approach for accelerating Magnetic Resonance Imaging
(MRI) data acquisition. However, it is challenging to accurately reconstruct
images from partial scan data (i.e., incomplete k-space matrices). Most
state-of-the-art reconstruction methods apply U-Net (a classical
encoder-decoder form of convolutional neural network) or cascaded U-Nets in
image domain and/or k-space domain. These methods have great advantages over
traditional methods where deep learning is not involved in. Nevertheless, these
methods have following problems: (1) Directly applying U-Net in k-space domain
is not optimal for extracting features in k-space domain; (2) Classical
image-domain oriented U-Net is heavy-weight and hence is inefficient to be
cascaded many times for yielding good reconstruction accuracy; (3) Classical
image-domain oriented U-Net does not fully make use information of encoder
network for extracting features in decoder network; and (4) Existing methods
are ineffective in simultaneously extracting and fusing features in image
domain and its dual k-space domain. To tackle these problems, we propose in
this paper (1) an image-domain encoder-decoder sub-network called V-Net which
is more light-weight for cascading and effective in fully utilizing features in
the encoder for decoding, (2) a k-space domain sub-network called K-Net which
is more suitable for extracting hierarchical features in k-space domain, and
(3) a dual-domain reconstruction network where V-Nets and K-Nets are parallelly
and effectively combined and cascaded. The effectiveness of KV-Net is
demonstrated on the challenging fastMRI dataset where large-scale raw k-space
training data are available and ground truth of test data is not released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Surface Defect Detection of Industrial Products Based on A Small Number of Labeled Data. (arXiv:2203.05733v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05733">
<div class="article-summary-box-inner">
<span><p>The surface defect detection method based on visual perception has been
widely used in industrial quality inspection. Because defect data are not easy
to obtain and the annotation of a large number of defect data will waste a lot
of manpower and material resources. Therefore, this paper reviews the methods
of surface defect detection of industrial products based on a small number of
labeled data, and this method is divided into traditional image
processing-based industrial product surface defect detection methods and deep
learning-based industrial product surface defect detection methods suitable for
a small number of labeled data. The traditional image processing-based
industrial product surface defect detection methods are divided into
statistical methods, spectral methods and model methods. Deep learning-based
industrial product surface defect detection methods suitable for a small number
of labeled data are divided into based on data augmentation, based on transfer
learning, model-based fine-tuning, semi-supervised, weak supervised and
unsupervised.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Distinctive Margin toward Active Domain Adaptation. (arXiv:2203.05738v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05738">
<div class="article-summary-box-inner">
<span><p>Despite plenty of efforts focusing on improving the domain adaptation ability
(DA) under unsupervised or few-shot semi-supervised settings, recently the
solution of active learning started to attract more attention due to its
suitability in transferring model in a more practical way with limited
annotation resource on target data. Nevertheless, most active learning methods
are not inherently designed to handle domain gap between data distribution, on
the other hand, some active domain adaptation methods (ADA) usually requires
complicated query functions, which is vulnerable to overfitting. In this work,
we propose a concise but effective ADA method called
Select-by-Distinctive-Margin (SDM), which consists of a maximum margin loss and
a margin sampling algorithm for data selection. We provide theoretical analysis
to show that SDM works like a Support Vector Machine, storing hard examples
around decision boundaries and exploiting them to find informative and
transferable data. In addition, we propose two variants of our method, one is
designed to adaptively adjust the gradient from margin loss, the other boosts
the selectivity of margin sampling by taking the gradient direction into
account. We benchmark SDM with standard active learning setting, demonstrating
our algorithm achieves competitive results with good data scalability. Code is
available at https://github.com/TencentYoutuResearch/ActiveLearning-SDM
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QDrop: Randomly Dropping Quantization for Extremely Low-bit Post-Training Quantization. (arXiv:2203.05740v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05740">
<div class="article-summary-box-inner">
<span><p>Recently, post-training quantization (PTQ) has driven much attention to
produce efficient neural networks without long-time retraining. Despite its low
cost, current PTQ works tend to fail under the extremely low-bit setting. In
this study, we pioneeringly confirm that properly incorporating activation
quantization into the PTQ reconstruction benefits the final accuracy. To deeply
understand the inherent reason, a theoretical framework is established,
indicating that the flatness of the optimized low-bit model on calibration and
test data is crucial. Based on the conclusion, a simple yet effective approach
dubbed as QDROP is proposed, which randomly drops the quantization of
activations during PTQ. Extensive experiments on various tasks including
computer vision (image classification, object detection) and natural language
processing (text classification and question answering) prove its superiority.
With QDROP, the limit of PTQ is pushed to the 2-bit activation for the first
time and the accuracy boost can be up to 51.49%. Without bells and whistles,
QDROP establishes a new state of the art for PTQ. Our code is available at
https://github.com/wimh966/QDrop and has been integrated into MQBench
(https://github.com/ModelTC/MQBench)
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active Phase-Encode Selection for Slice-Specific Fast MR Scanning Using a Transformer-Based Deep Reinforcement Learning Framework. (arXiv:2203.05756v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05756">
<div class="article-summary-box-inner">
<span><p>Purpose: Long scan time in phase encoding for forming complete K-space
matrices is a critical drawback of MRI, making patients uncomfortable and
wasting important time for diagnosing emergent diseases. This paper aims to
reducing the scan time by actively and sequentially selecting partial phases in
a short time so that a slice can be accurately reconstructed from the resultant
slice-specific incomplete K-space matrix. Methods: A transformer based deep
reinforcement learning framework is proposed for actively determining a
sequence of partial phases according to reconstruction-quality based Q-value (a
function of reward), where the reward is the improvement degree of
reconstructed image quality. The Q-value is efficiently predicted from binary
phase-indicator vectors, incomplete K-space matrices and their corresponding
undersampled images with a light-weight transformer so that the sequential
information of phases and global relationship in images can be used. The
inverse Fourier transform is employed for efficiently computing the
undersampled images and hence gaining the rewards of selecting phases. Results:
Experimental results on the fastMRI dataset with original K-space data
accessible demonstrate the efficiency and accuracy superiorities of proposed
method. Compared with the state-of-the-art reinforcement learning based method
proposed by Pineda et al., the proposed method is roughly 150 times faster and
achieves significant improvement in reconstruction accuracy. Conclusions: We
have proposed a light-weight transformer based deep reinforcement learning
framework for generating high-quality slice-specific trajectory consisting of a
small number of phases. The proposed method, called TITLE (Transformer Involved
Trajectory LEarning), has remarkable superiority in phase-encode selection
efficiency and image reconstruction accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Federated Remote Physiological Measurement with Imperfect Data. (arXiv:2203.05759v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05759">
<div class="article-summary-box-inner">
<span><p>The growing need for technology that supports remote healthcare is being
acutely highlighted by an aging population and the COVID-19 pandemic. In
health-related machine learning applications the ability to learn predictive
models without data leaving a private device is attractive, especially when
these data might contain features (e.g., photographs or videos of the body)
that make identifying a subject trivial and/or the training data volume is
large (e.g., uncompressed video). Camera-based remote physiological sensing
facilitates scalable and low-cost measurement, but is a prime example of a task
that involves analysing high bit-rate videos containing identifiable images and
sensitive health information. Federated learning enables privacy-preserving
decentralized training which has several properties beneficial for camera-based
sensing. We develop the first mobile federated learning camera-based sensing
system and show that it can perform competitively with traditional
state-of-the-art supervised approaches. However, in the presence of corrupted
data (e.g., video or label noise) from a few devices the performance of weight
averaging quickly degrades. To address this, we leverage knowledge about the
expected noise profile within the video to intelligently adjust how the model
weights are averaged on the server. Our results show that this significantly
improves upon the robustness of models even when the signal-to-noise ratio is
low
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Physics-informed Reinforcement Learning for Perception and Reasoning about Fluids. (arXiv:2203.05775v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05775">
<div class="article-summary-box-inner">
<span><p>Learning and reasoning about physical phenomena is still a challenge in
robotics development, and computational sciences play a capital role in the
search for accurate methods able to provide explanations for past events and
rigorous forecasts of future situations. We propose a physics-informed
reinforcement learning strategy for fluid perception and reasoning from
observations. As a model problem, we take the sloshing phenomena of different
fluids contained in a glass. Starting from full-field and high-resolution
synthetic data for a particular fluid, we develop a method for the tracking
(perception) and analysis (reasoning) of any previously unseen liquid whose
free surface is observed with a commodity camera. This approach demonstrates
the importance of physics and knowledge not only in data-driven (grey box)
modeling but also in the correction for real physics adaptation in low data
regimes and partial observations of the dynamics. The method here presented is
extensible to other domains such as the development of cognitive digital twins,
able to learn from observation of phenomena for which they have not been
trained explicitly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AI-enabled Automatic Multimodal Fusion of Cone-Beam CT and Intraoral Scans for Intelligent 3D Tooth-Bone Reconstruction and Clinical Applications. (arXiv:2203.05784v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05784">
<div class="article-summary-box-inner">
<span><p>A critical step in virtual dental treatment planning is to accurately
delineate all tooth-bone structures from CBCT with high fidelity and accurate
anatomical information. Previous studies have established several methods for
CBCT segmentation using deep learning. However, the inherent resolution
discrepancy of CBCT and the loss of occlusal and dentition information largely
limited its clinical applicability. Here, we present a Deep Dental Multimodal
Analysis (DDMA) framework consisting of a CBCT segmentation model, an intraoral
scan (IOS) segmentation model (the most accurate digital dental model), and a
fusion model to generate 3D fused crown-root-bone structures with high fidelity
and accurate occlusal and dentition information. Our model was trained with a
large-scale dataset with 503 CBCT and 28,559 IOS meshes manually annotated by
experienced human experts. For CBCT segmentation, we use a five-fold cross
validation test, each with 50 CBCT, and our model achieves an average Dice
coefficient and IoU of 93.99% and 88.68%, respectively, significantly
outperforming the baselines. For IOS segmentations, our model achieves an mIoU
of 93.07% and 95.70% on the maxillary and mandible on a test set of 200 IOS
meshes, which are 1.77% and 3.52% higher than the state-of-art method. Our DDMA
framework takes about 20 to 25 minutes to generate the fused 3D mesh model
following the sequential processing order, compared to over 5 hours by human
experts. Notably, our framework has been incorporated into a software by a
clear aligner manufacturer, and real-world clinical cases demonstrate that our
model can visualize crown-root-bone structures during the entire orthodontic
treatment and can predict risks like dehiscence and fenestration. These
findings demonstrate the potential of multi-modal deep learning to improve the
quality of digital dental models and help dentists make better clinical
decisions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Democracy Does Matter: Comprehensive Feature Mining for Co-Salient Object Detection. (arXiv:2203.05787v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05787">
<div class="article-summary-box-inner">
<span><p>Co-salient object detection, with the target of detecting co-existed salient
objects among a group of images, is gaining popularity. Recent works use the
attention mechanism or extra information to aggregate common co-salient
features, leading to incomplete even incorrect responses for target objects. In
this paper, we aim to mine comprehensive co-salient features with democracy and
reduce background interference without introducing any extra information. To
achieve this, we design a democratic prototype generation module to generate
democratic response maps, covering sufficient co-salient regions and thereby
involving more shared attributes of co-salient objects. Then a comprehensive
prototype based on the response maps can be generated as a guide for final
prediction. To suppress the noisy background information in the prototype, we
propose a self-contrastive learning module, where both positive and negative
pairs are formed without relying on additional classification information.
Besides, we also design a democratic feature enhancement module to further
strengthen the co-salient features by readjusting attention values. Extensive
experiments show that our model obtains better performance than previous
state-of-the-art methods, especially on challenging real-world cases (e.g., for
CoCA, we obtain a gain of 2.0% for MAE, 5.4% for maximum F-measure, 2.3% for
maximum E-measure, and 3.7% for S-measure) under the same settings. Code will
be released soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FLAG: Flow-based 3D Avatar Generation from Sparse Observations. (arXiv:2203.05789v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05789">
<div class="article-summary-box-inner">
<span><p>To represent people in mixed reality applications for collaboration and
communication, we need to generate realistic and faithful avatar poses.
However, the signal streams that can be applied for this task from head-mounted
devices (HMDs) are typically limited to head pose and hand pose estimates.
While these signals are valuable, they are an incomplete representation of the
human body, making it challenging to generate a faithful full-body avatar. We
address this challenge by developing a flow-based generative model of the 3D
human body from sparse observations, wherein we learn not only a conditional
distribution of 3D human pose, but also a probabilistic mapping from
observations to the latent space from which we can generate a plausible pose
along with uncertainty estimates for the joints. We show that our approach is
not only a strong predictive model, but can also act as an efficient pose prior
in different optimization settings where a good initial latent code plays a
major role.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Democratizing Contrastive Language-Image Pre-training: A CLIP Benchmark of Data, Model, and Supervision. (arXiv:2203.05796v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05796">
<div class="article-summary-box-inner">
<span><p>Contrastive Language-Image Pretraining (CLIP) has emerged as a novel paradigm
to learn visual models from language supervision. While researchers continue to
push the frontier of CLIP, reproducing these works remains challenging. This is
because researchers do not choose consistent training recipes and even use
different data, hampering the fair comparison between different methods. In
this work, we propose CLIP-benchmark, a first attempt to evaluate, analyze, and
benchmark CLIP and its variants. We conduct a comprehensive analysis of three
key factors: data, supervision, and model architecture. We find considerable
intuitive or counter-intuitive insights: (1). Data quality has a significant
impact on performance. (2). Certain supervision has different effects for
Convolutional Networks (ConvNets) and Vision Transformers (ViT). Applying more
proper supervision can effectively improve the performance of CLIP. (3).
Curtailing the text encoder reduces the training cost but not much affect the
final performance. Moreover, we further combine DeCLIP with FILIP, bringing us
the strongest variant DeFILIP. The CLIP-benchmark would be released at:
https://github.com/Sense-GVT/DeCLIP for future CLIP research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improve Convolutional Neural Network Pruning by Maximizing Filter Variety. (arXiv:2203.05807v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05807">
<div class="article-summary-box-inner">
<span><p>Neural network pruning is a widely used strategy for reducing model storage
and computing requirements. It allows to lower the complexity of the network by
introducing sparsity in the weights. Because taking advantage of sparse
matrices is still challenging, pruning is often performed in a structured way,
i.e. removing entire convolution filters in the case of ConvNets, according to
a chosen pruning criteria. Common pruning criteria, such as l1-norm or
movement, usually do not consider the individual utility of filters, which may
lead to: (1) the removal of filters exhibiting rare, thus important and
discriminative behaviour, and (2) the retaining of filters with redundant
information. In this paper, we present a technique solving those two issues,
and which can be appended to any pruning criteria. This technique ensures that
the criteria of selection focuses on redundant filters, while retaining the
rare ones, thus maximizing the variety of remaining filters. The experimental
results, carried out on different datasets (CIFAR-10, CIFAR-100 and
CALTECH-101) and using different architectures (VGG-16 and ResNet-18)
demonstrate that it is possible to achieve similar sparsity levels while
maintaining a higher performance when appending our filter selection technique
to pruning criteria. Moreover, we assess the quality of the found sparse
sub-networks by applying the Lottery Ticket Hypothesis and find that the
addition of our method allows to discover better performing tickets in most
cases
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Font Shape-to-Impression Translation. (arXiv:2203.05808v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05808">
<div class="article-summary-box-inner">
<span><p>Different fonts have different impressions, such as elegant, scary, and cool.
This paper tackles part-based shape-impression analysis based on the
Transformer architecture, which is able to handle the correlation among local
parts by its self-attention mechanism. This ability will reveal how
combinations of local parts realize a specific impression of a font. The
versatility of Transformer allows us to realize two very different approaches
for the analysis, i.e., multi-label classification and translation. A
quantitative evaluation shows that our Transformer-based approaches estimate
the font impressions from a set of local parts more accurately than other
approaches. A qualitative evaluation then indicates the important local parts
for a specific impression.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">aiWave: Volumetric Image Compression with 3-D Trained Affine Wavelet-like Transform. (arXiv:2203.05822v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05822">
<div class="article-summary-box-inner">
<span><p>Volumetric image compression has become an urgent task to effectively
transmit and store images produced in biological research and clinical
practice. At present, the most commonly used volumetric image compression
methods are based on wavelet transform, such as JP3D. However, JP3D employs an
ideal, separable, global, and fixed wavelet basis to convert input images from
pixel domain to frequency domain, which seriously limits its performance. In
this paper, we first design a 3-D trained wavelet-like transform to enable
signal-dependent and non-separable transform. Then, an affine wavelet basis is
introduced to capture the various local correlations in different regions of
volumetric images. Furthermore, we embed the proposed wavelet-like transform to
an end-to-end compression framework called aiWave to enable an adaptive
compression scheme for various datasets. Last but not least, we introduce the
weight sharing strategies of the affine wavelet-like transform according to the
volumetric data characteristics in the axial direction to reduce the amount of
parameters. The experimental results show that: 1) when cooperating our trained
3-D affine wavelet-like transform with a simple factorized entropy module,
aiWave performs better than JP3D and is comparable in terms of encoding and
decoding complexities; 2) when adding a context module to further remove signal
redundancy, aiWave can achieve a much better performance than HEVC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WiCV 2021: The Eighth Women In Computer Vision Workshop. (arXiv:2203.05825v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05825">
<div class="article-summary-box-inner">
<span><p>In this paper, we present the details of Women in Computer Vision Workshop -
WiCV 2021, organized alongside the virtual CVPR 2021. It provides a voice to a
minority (female) group in the computer vision community and focuses on
increasing the visibility of these researchers, both in academia and industry.
WiCV believes that such an event can play an important role in lowering the
gender imbalance in the field of computer vision. WiCV is organized each year
where it provides a)~opportunity for collaboration between researchers from
minority groups, b)~mentorship to female junior researchers, c)~financial
support to presenters to overcome monetary burden and d)~large and diverse
choice of role models, who can serve as examples to younger researchers at the
beginning of their careers. In this paper, we present a report on the workshop
program, trends over the past years, a summary of statistics regarding
presenters, attendees, and sponsorship for the WiCV 2021 workshop.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Flexible Amortized Variational Inference in qBOLD MRI. (arXiv:2203.05845v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05845">
<div class="article-summary-box-inner">
<span><p>Streamlined qBOLD acquisitions enable experimentally straightforward
observations of brain oxygen metabolism. $R_2^\prime$ maps are easily inferred;
however, the Oxygen extraction fraction (OEF) and deoxygenated blood volume
(DBV) are more ambiguously determined from the data. As such, existing
inference methods tend to yield very noisy and underestimated OEF maps, while
overestimating DBV.
</p>
<p>This work describes a novel probabilistic machine learning approach that can
infer plausible distributions of OEF and DBV. Initially, we create a model that
produces informative voxelwise prior distribution based on synthetic training
data. Contrary to prior work, we model the joint distribution of OEF and DBV
through a scaled multivariate logit-Normal distribution, which enables the
values to be constrained within a plausible range. The prior distribution model
is used to train an efficient amortized variational Bayesian inference model.
This model learns to infer OEF and DBV by predicting real image data, with few
training data required, using the signal equations as a forward model.
</p>
<p>We demonstrate that our approach enables the inference of smooth OEF and DBV
maps, with a physiologically plausible distribution that can be adapted through
specification of an informative prior distribution. Other benefits include
model comparison (via the evidence lower bound) and uncertainty quantification
for identifying image artefacts. Results are demonstrated on a small study
comparing subjects undergoing hyperventilation and at rest. We illustrate that
the proposed approach allows measurement of gray matter differences in OEF and
DBV and enables voxelwise comparison between conditions, where we observe
significant increases in OEF and $R_2^\prime$ during hyperventilation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Fine-grained Glomerular Lesion Recognition in Kidney Pathology. (arXiv:2203.05847v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05847">
<div class="article-summary-box-inner">
<span><p>Recognition of glomeruli lesions is the key for diagnosis and treatment
planning in kidney pathology; however, the coexisting glomerular structures
such as mesangial regions exacerbate the difficulties of this task. In this
paper, we introduce a scheme to recognize fine-grained glomeruli lesions from
whole slide images. First, a focal instance structural similarity loss is
proposed to drive the model to locate all types of glomeruli precisely. Then an
Uncertainty Aided Apportionment Network is designed to carry out the
fine-grained visual classification without bounding-box annotations. This
double branch-shaped structure extracts common features of the child class from
the parent class and produces the uncertainty factor for reconstituting the
training dataset. Results of slide-wise evaluation illustrate the effectiveness
of the entire scheme, with an 8-22% improvement of the mean Average Precision
compared with remarkable detection methods. The comprehensive results clearly
demonstrate the effectiveness of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human Silhouette and Skeleton Video Synthesis through Wi-Fi signals. (arXiv:2203.05864v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05864">
<div class="article-summary-box-inner">
<span><p>The increasing availability of wireless access points (APs) is leading
towards human sensing applications based on Wi-Fi signals as support or
alternative tools to the widespread visual sensors, where the signals enable to
address well-known vision-related problems such as illumination changes or
occlusions. Indeed, using image synthesis techniques to translate radio
frequencies to the visible spectrum can become essential to obtain otherwise
unavailable visual data. This domain-to-domain translation is feasible because
both objects and people affect electromagnetic waves, causing radio and optical
frequencies variations. In literature, models capable of inferring
radio-to-visual features mappings have gained momentum in the last few years
since frequency changes can be observed in the radio domain through the channel
state information (CSI) of Wi-Fi APs, enabling signal-based feature extraction,
e.g., amplitude. On this account, this paper presents a novel two-branch
generative neural network that effectively maps radio data into visual
features, following a teacher-student design that exploits a cross-modality
supervision strategy. The latter conditions signal-based features in the visual
domain to completely replace visual data. Once trained, the proposed method
synthesizes human silhouette and skeleton videos using exclusively Wi-Fi
signals. The approach is evaluated on publicly available data, where it obtains
remarkable results for both silhouette and skeleton videos generation,
demonstrating the effectiveness of the proposed cross-modality supervision
strategy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-modal Graph Learning for Disease Prediction. (arXiv:2203.05880v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05880">
<div class="article-summary-box-inner">
<span><p>Benefiting from the powerful expressive capability of graphs, graph-based
approaches have been popularly applied to handle multi-modal medical data and
achieved impressive performance in various biomedical applications. For disease
prediction tasks, most existing graph-based methods tend to define the graph
manually based on specified modality (e.g., demographic information), and then
integrated other modalities to obtain the patient representation by Graph
Representation Learning (GRL). However, constructing an appropriate graph in
advance is not a simple matter for these methods. Meanwhile, the complex
correlation between modalities is ignored. These factors inevitably yield the
inadequacy of providing sufficient information about the patient's condition
for a reliable diagnosis. To this end, we propose an end-to-end Multi-modal
Graph Learning framework (MMGL) for disease prediction with multi-modality. To
effectively exploit the rich information across multi-modality associated with
the disease, modality-aware representation learning is proposed to aggregate
the features of each modality by leveraging the correlation and complementarity
between the modalities. Furthermore, instead of defining the graph manually,
the latent graph structure is captured through an effective way of adaptive
graph learning. It could be jointly optimized with the prediction model, thus
revealing the intrinsic connections among samples. Our model is also applicable
to the scenario of inductive learning for those unseen data. An extensive group
of experiments on two disease prediction tasks demonstrates that the proposed
MMGL achieves more favorable performance. The code of MMGL is available at
\url{https://github.com/SsGood/MMGL}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video Coding for Machines with Feature-Based Rate-Distortion Optimization. (arXiv:2203.05890v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05890">
<div class="article-summary-box-inner">
<span><p>Common state-of-the-art video codecs are optimized to deliver a low bitrate
by providing a certain quality for the final human observer, which is achieved
by rate-distortion optimization (RDO). But, with the steady improvement of
neural networks solving computer vision tasks, more and more multimedia data is
not observed by humans anymore, but directly analyzed by neural networks. In
this paper, we propose a standard-compliant feature-based RDO (FRDO) that is
designed to increase the coding performance, when the decoded frame is analyzed
by a neural network in a video coding for machine scenario. To that extent, we
replace the pixel-based distortion metrics in conventional RDO of VTM-8.0 with
distortion metrics calculated in the feature space created by the first layers
of a neural network. Throughout several tests with the segmentation network
Mask R-CNN and single images from the Cityscapes dataset, we compare the
proposed FRDO and its hybrid version HFRDO with different distortion measures
in the feature space against the conventional RDO. With HFRDO, up to 5.49 %
bitrate can be saved compared to the VTM-8.0 implementation in terms of
Bj{\o}ntegaard Delta Rate and using the weighted average precision as quality
metric. Additionally, allowing the encoder to vary the quantization parameter
results in coding gains for the proposed HFRDO of up 9.95 % compared to
conventional VTM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DRTAM: Dual Rank-1 Tensor Attention Module. (arXiv:2203.05893v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05893">
<div class="article-summary-box-inner">
<span><p>Recently, attention mechanisms have been extensively investigated in computer
vision, but few of them show excellent performance on both large and mobile
networks. This paper proposes Dual Rank-1 Tensor Attention Module (DRTAM), a
novel residual-attention-learning-guided attention module for feed-forward
convolutional neural networks. Given a 3D feature tensor map, DRTAM firstly
generates three 2D feature descriptors along three axes. Then, using three
descriptors, DRTAM sequentially infers two rank-1 tensor attention maps, the
initial attention map and the complement attention map, combines and multiplied
them to the input feature map for adaptive feature refinement(see Fig.1(c)). To
generate two attention maps, DRTAM introduces rank-1 tensor attention module
(RTAM) and residual descriptors extraction module (RDEM): RTAM divides each 2D
feature descriptors into several chunks, and generate three factor vectors of a
rank-1 tensor attention map by employing strip pooling on each chunk so that
local and long-range contextual information can be captured along three
dimension respectively; RDEM generates three 2D feature descriptors of the
residual feature to produce the complement attention map, using three factor
vectors of the initial attention map and three descriptors of the input
feature. Extensive experimental results on ImageNet-1K, MS COCO and PASCAL VOC
demonstrate that DRTAM achieves competitive performance on both large and
mobile networks compare with other state-of-the-art attention modules.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hyperbolic Image Segmentation. (arXiv:2203.05898v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05898">
<div class="article-summary-box-inner">
<span><p>For image segmentation, the current standard is to perform pixel-level
optimization and inference in Euclidean output embedding spaces through linear
hyperplanes. In this work, we show that hyperbolic manifolds provide a valuable
alternative for image segmentation and propose a tractable formulation of
hierarchical pixel-level classification in hyperbolic space. Hyperbolic Image
Segmentation opens up new possibilities and practical benefits for
segmentation, such as uncertainty estimation and boundary information for free,
zero-label generalization, and increased performance in low-dimensional output
embeddings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BabyNet: Reconstructing 3D faces of babies from uncalibrated photographs. (arXiv:2203.05908v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05908">
<div class="article-summary-box-inner">
<span><p>We present a 3D face reconstruction system that aims at recovering the 3D
facial geometry of babies from uncalibrated photographs, BabyNet. Since the 3D
facial geometry of babies differs substantially from that of adults,
baby-specific facial reconstruction systems are needed. BabyNet consists of two
stages: 1) a 3D graph convolutional autoencoder learns a latent space of the
baby 3D facial shape; and 2) a 2D encoder that maps photographs to the 3D
latent space based on representative features extracted using transfer
learning. In this way, using the pre-trained 3D decoder, we can recover a 3D
face from 2D images. We evaluate BabyNet and show that 1) methods based on
adult datasets cannot model the 3D facial geometry of babies, which proves the
need for a baby-specific method, and 2) BabyNet outperforms classical
model-fitting methods even when a baby-specific 3D morphable model, such as
BabyFM, is used.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visualizing and Understanding Patch Interactions in Vision Transformer. (arXiv:2203.05922v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05922">
<div class="article-summary-box-inner">
<span><p>Vision Transformer (ViT) has become a leading tool in various computer vision
tasks, owing to its unique self-attention mechanism that learns visual
representations explicitly through cross-patch information interactions.
Despite having good success, the literature seldom explores the explainability
of vision transformer, and there is no clear picture of how the attention
mechanism with respect to the correlation across comprehensive patches will
impact the performance and what is the further potential. In this work, we
propose a novel explainable visualization approach to analyze and interpret the
crucial attention interactions among patches for vision transformer.
Specifically, we first introduce a quantification indicator to measure the
impact of patch interaction and verify such quantification on attention window
design and indiscriminative patches removal. Then, we exploit the effective
responsive field of each patch in ViT and devise a window-free transformer
architecture accordingly. Extensive experiments on ImageNet demonstrate that
the exquisitely designed quantitative method is shown able to facilitate ViT
model learning, leading the top-1 accuracy by 4.28% at most. Moreover, the
results on downstream fine-grained recognition tasks further validate the
generalization of our proposal.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TFCNet: Temporal Fully Connected Networks for Static Unbiased Temporal Reasoning. (arXiv:2203.05928v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05928">
<div class="article-summary-box-inner">
<span><p>Temporal Reasoning is one important functionality for vision intelligence. In
computer vision research community, temporal reasoning is usually studied in
the form of video classification, for which many state-of-the-art Neural
Network structures and dataset benchmarks are proposed in recent years,
especially 3D CNNs and Kinetics. However, some recent works found that current
video classification benchmarks contain strong biases towards static features,
thus cannot accurately reflect the temporal modeling ability. New video
classification benchmarks aiming to eliminate static biases are proposed, with
experiments on these new benchmarks showing that the current clip-based 3D CNNs
are outperformed by RNN structures and recent video transformers.
</p>
<p>In this paper, we find that 3D CNNs and their efficient depthwise variants,
when video-level sampling strategy is used, are actually able to beat RNNs and
recent vision transformers by significant margins on static-unbiased temporal
reasoning benchmarks. Further, we propose Temporal Fully Connected Block (TFC
Block), an efficient and effective component, which approximates fully
connected layers along temporal dimension to obtain video-level receptive
field, enhancing the spatiotemporal reasoning ability. With TFC blocks inserted
into Video-level 3D CNNs (V3D), our proposed TFCNets establish new
state-of-the-art results on synthetic temporal reasoning benchmark, CATER, and
real world static-unbiased dataset, Diving48, surpassing all previous methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PD-Flow: A Point Cloud Denoising Framework with Normalizing Flows. (arXiv:2203.05940v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05940">
<div class="article-summary-box-inner">
<span><p>Point cloud denoising aims to restore clean point clouds from raw
observations corrupted by noise and outliers while preserving the fine-grained
details. We present a novel deep learning-based denoising model, that
incorporates normalizing flows and noise disentanglement techniques to achieve
high denoising accuracy. Unlike existing works that extract features of point
clouds for point-wise correction, we formulate the denoising process from the
perspective of distribution learning and feature disentanglement. By
considering noisy point clouds as a joint distribution of clean points and
noise, the denoised results can be derived from disentangling the noise
counterpart from latent point representation, and the mapping between Euclidean
and latent spaces is modeled by normalizing flows. We evaluate our method on
synthesized 3D models and real-world datasets with various noise settings.
Qualitative and quantitative results show that our method outperforms previous
state-of-the-art deep learning-based approaches. %in terms of detail
preservation and distribution uniformity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Saliency-Driven Versatile Video Coding for Neural Object Detection. (arXiv:2203.05944v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05944">
<div class="article-summary-box-inner">
<span><p>Saliency-driven image and video coding for humans has gained importance in
the recent past. In this paper, we propose such a saliency-driven coding
framework for the video coding for machines task using the latest video coding
standard Versatile Video Coding (VVC). To determine the salient regions before
encoding, we employ the real-time-capable object detection network You Only
Look Once~(YOLO) in combination with a novel decision criterion. To measure the
coding quality for a machine, the state-of-the-art object segmentation network
Mask R-CNN was applied to the decoded frame. From extensive simulations we find
that, compared to the reference VVC with a constant quality, up to 29 % of
bitrate can be saved with the same detection accuracy at the decoder side by
applying the proposed saliency-driven framework. Besides, we compare YOLO
against other, more traditional saliency detection methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Peng Cheng Object Detection Benchmark for Smart City. (arXiv:2203.05949v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05949">
<div class="article-summary-box-inner">
<span><p>Object detection is an algorithm that recognizes and locates the objects in
the image and has a wide range of applications in the visual understanding of
complex urban scenes. Existing object detection benchmarks mainly focus on a
single specific scenario and their annotation attributes are not rich enough,
these make the object detection model is not generalized for the smart city
scenes. Considering the diversity and complexity of scenes in intelligent city
governance, we build a large-scale object detection benchmark for the smart
city. Our benchmark contains about 500K images and includes three scenarios:
intelligent transportation, intelligent security, and drones. For the
complexity of the real scene in the smart city, the diversity of weather,
occlusion, and other complex environment diversity attributes of the images in
the three scenes are annotated. The characteristics of the benchmark are
analyzed and extensive experiments of the current state-of-the-art target
detection algorithm are conducted based on our benchmark to show their
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Label-efficient Hybrid-supervised Learning for Medical Image Segmentation. (arXiv:2203.05956v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05956">
<div class="article-summary-box-inner">
<span><p>Due to the lack of expertise for medical image annotation, the investigation
of label-efficient methodology for medical image segmentation becomes a heated
topic. Recent progresses focus on the efficient utilization of weak annotations
together with few strongly-annotated labels so as to achieve comparable
segmentation performance in many unprofessional scenarios. However, these
approaches only concentrate on the supervision inconsistency between strongly-
and weakly-annotated instances but ignore the instance inconsistency inside the
weakly-annotated instances, which inevitably leads to performance degradation.
To address this problem, we propose a novel label-efficient hybrid-supervised
framework, which considers each weakly-annotated instance individually and
learns its weight guided by the gradient direction of the strongly-annotated
instances, so that the high-quality prior in the strongly-annotated instances
is better exploited and the weakly-annotated instances are depicted more
precisely. Specially, our designed dynamic instance indicator (DII) realizes
the above objectives, and is adapted to our dynamic co-regularization (DCR)
framework further to alleviate the erroneous accumulation from distortions of
weak annotations. Extensive experiments on two hybrid-supervised medical
segmentation datasets demonstrate that with only 10% strong labels, the
proposed framework can leverage the weak labels efficiently and achieve
competitive performance against the 100% strong-label supervised scenario.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anti-Oversmoothing in Deep Vision Transformers via the Fourier Domain Analysis: From Theory to Practice. (arXiv:2203.05962v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05962">
<div class="article-summary-box-inner">
<span><p>Vision Transformer (ViT) has recently demonstrated promise in computer vision
problems. However, unlike Convolutional Neural Networks (CNN), it is known that
the performance of ViT saturates quickly with depth increasing, due to the
observed attention collapse or patch uniformity. Despite a couple of empirical
solutions, a rigorous framework studying on this scalability issue remains
elusive. In this paper, we first establish a rigorous theory framework to
analyze ViT features from the Fourier spectrum domain. We show that the
self-attention mechanism inherently amounts to a low-pass filter, which
indicates when ViT scales up its depth, excessive low-pass filtering will cause
feature maps to only preserve their Direct-Current (DC) component. We then
propose two straightforward yet effective techniques to mitigate the
undesirable low-pass limitation. The first technique, termed AttnScale,
decomposes a self-attention block into low-pass and high-pass components, then
rescales and combines these two filters to produce an all-pass self-attention
matrix. The second technique, termed FeatScale, re-weights feature maps on
separate frequency bands to amplify the high-frequency signals. Both techniques
are efficient and hyperparameter-free, while effectively overcoming relevant
ViT training artifacts such as attention collapse and patch uniformity. By
seamlessly plugging in our techniques to multiple ViT variants, we demonstrate
that they consistently help ViTs benefit from deeper architectures, bringing up
to 1.1% performance gains "for free" (e.g., with little parameter overhead). We
publicly release our codes and pre-trained models at
https://github.com/VITA-Group/ViT-Anti-Oversmoothing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Channel Convolutional Analysis Operator Learning for Dual-Energy CT Reconstruction. (arXiv:2203.05968v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05968">
<div class="article-summary-box-inner">
<span><p>Objective. Dual-energy computed tomography (DECT) has the potential to
improve contrast, reduce artifacts and the ability to perform material
decomposition in advanced imaging applications. The increased number or
measurements results with a higher radiation dose and it is therefore essential
to reduce either number of projections per energy or the source X-ray
intensity, but this makes tomographic reconstruction more ill-posed.
</p>
<p>Approach. We developed the multi-channel convolutional analysis operator
learning (MCAOL) method to exploit common spatial features within attenuation
images at different energies and we propose an optimization method which
jointly reconstructs the attenuation images at low and high energies with a
mixed norm regularization on the sparse features obtained by pre-trained
convolutional filters through the convolutional analysis operator learning
(CAOL) algorithm.
</p>
<p>Main results. Extensive experiments with simulated and real computed
tomography (CT) data were performed to validate the effectiveness of the
proposed methods and we reported increased reconstruction accuracy compared to
CAOL and iterative methods with single and joint total-variation (TV)
regularization.
</p>
<p>Significance. Qualitative and quantitative results on sparse-views and
low-dose DECT demonstrate that the proposed MCAOL method outperforms both CAOL
applied on each energy independently and several existing state-of-the-art
model-based iterative reconstruction (MBIR) techniques, thus paving the way for
dose reduction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FExGAN-Meta: Facial Expression Generation with Meta Humans. (arXiv:2203.05975v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05975">
<div class="article-summary-box-inner">
<span><p>The subtleness of human facial expressions and a large degree of variation in
the level of intensity to which a human expresses them is what makes it
challenging to robustly classify and generate images of facial expressions.
Lack of good quality data can hinder the performance of a deep learning model.
In this article, we have proposed a Facial Expression Generation method for
Meta-Humans (FExGAN-Meta) that works robustly with the images of Meta-Humans.
We have prepared a large dataset of facial expressions exhibited by ten
Meta-Humans when placed in a studio environment and then we have evaluated
FExGAN-Meta on the collected images. The results show that FExGAN-Meta robustly
generates and classifies the images of Meta-Humans for the simple as well as
the complex facial expressions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PseudoProp: Robust Pseudo-Label Generation for Semi-Supervised Object Detection in Autonomous Driving Systems. (arXiv:2203.05983v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05983">
<div class="article-summary-box-inner">
<span><p>Semi-supervised object detection methods are widely used in autonomous
driving systems, where only a fraction of objects are labeled. To propagate
information from the labeled objects to the unlabeled ones, pseudo-labels for
unlabeled objects must be generated. Although pseudo-labels have proven to
improve the performance of semi-supervised object detection significantly, the
applications of image-based methods to video frames result in numerous miss or
false detections using such generated pseudo-labels. In this paper, we propose
a new approach, PseudoProp, to generate robust pseudo-labels by leveraging
motion continuity in video frames. Specifically, PseudoProp uses a novel
bidirectional pseudo-label propagation approach to compensate for misdetection.
A feature-based fusion technique is also used to suppress inference noise.
Extensive experiments on the large-scale Cityscapes dataset demonstrate that
our method outperforms the state-of-the-art semi-supervised object detection
methods by 7.4% on mAP75.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Class Incremental Learning from Decentralized Data. (arXiv:2203.05984v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05984">
<div class="article-summary-box-inner">
<span><p>In this paper, we focus on a new and challenging decentralized machine
learning paradigm in which there are continuous inflows of data to be addressed
and the data are stored in multiple repositories. We initiate the study of data
decentralized class-incremental learning (DCIL) by making the following
contributions. Firstly, we formulate the DCIL problem and develop the
experimental protocol. Secondly, we introduce a paradigm to create a basic
decentralized counterpart of typical (centralized) class-incremental learning
approaches, and as a result, establish a benchmark for the DCIL study. Thirdly,
we further propose a Decentralized Composite knowledge Incremental Distillation
framework (DCID) to transfer knowledge from historical models and multiple
local sites to the general model continually. DCID consists of three main
components namely local class-incremental learning, collaborated knowledge
distillation among local models, and aggregated knowledge distillation from
local models to the general one. We comprehensively investigate our DCID
framework by using different implementations of the three components. Extensive
experimental results demonstrate the effectiveness of our DCID framework. The
codes of the baseline methods and the proposed DCIL will be released at
https://github.com/zxxxxh/DCIL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph Neural Networks for Relational Inductive Bias in Vision-based Deep Reinforcement Learning of Robot Control. (arXiv:2203.05985v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05985">
<div class="article-summary-box-inner">
<span><p>State-of-the-art reinforcement learning algorithms predominantly learn a
policy from either a numerical state vector or images. Both approaches
generally do not take structural knowledge of the task into account, which is
especially prevalent in robotic applications and can benefit learning if
exploited. This work introduces a neural network architecture that combines
relational inductive bias and visual feedback to learn an efficient position
control policy for robotic manipulation. We derive a graph representation that
models the physical structure of the manipulator and combines the robot's
internal state with a low-dimensional description of the visual scene generated
by an image encoding network. On this basis, a graph neural network trained
with reinforcement learning predicts joint velocities to control the robot. We
further introduce an asymmetric approach of training the image encoder
separately from the policy using supervised learning. Experimental results
demonstrate that, for a 2-DoF planar robot in a geometrically simplistic 2D
environment, a learned representation of the visual scene can replace access to
the explicit coordinates of the reaching target without compromising on the
quality and sample efficiency of the policy. We further show the ability of the
model to improve sample efficiency for a 6-DoF robot arm in a visually
realistic 3D environment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Self-Supervised Learning of Global and Object-Centric Representations. (arXiv:2203.05997v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05997">
<div class="article-summary-box-inner">
<span><p>Self-supervision allows learning meaningful representations of natural images
which usually contain one central object. How well does it transfer to
multi-entity scenes? We discuss key aspects of learning structured
object-centric representations with self-supervision and validate our insights
through several experiments on the CLEVR dataset. Regarding the architecture,
we confirm the importance of competition for attention-based object discovery,
where each image patch is exclusively attended by one object. For training, we
show that contrastive losses equipped with matching can be applied directly in
a latent space, avoiding pixel-based reconstruction. However, such an
optimization objective is sensitive to false negatives (recurring objects) and
false positives (matching errors). Thus, careful consideration is required
around data augmentation and negative sample selection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Polar Transformation Based Multiple Instance Learning Assisting Weakly Supervised Image Segmentation With Loose Bounding Box Annotations. (arXiv:2203.06000v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06000">
<div class="article-summary-box-inner">
<span><p>This study investigates weakly supervised image segmentation using loose
bounding box supervision. It presents a multiple instance learning strategy
based on polar transformation to assist image segmentation when loose bounding
boxes are employed as supervision. In this strategy, weighted smooth maximum
approximation is introduced to incorporate the observation that pixels closer
to the origin of the polar transformation are more likely to belong to the
object in the bounding box. The proposed approach was evaluated on a public
medical dataset using Dice coefficient. The results demonstrate its superior
performance. The codes are available at
\url{https://github.com/wangjuan313/wsis-polartransform}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An error correction scheme for improved air-tissue boundary in real-time MRI video for speech production. (arXiv:2203.06004v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06004">
<div class="article-summary-box-inner">
<span><p>The best performance in Air-tissue boundary (ATB) segmentation of real-time
Magnetic Resonance Imaging (rtMRI) videos in speech production is known to be
achieved by a 3-dimensional convolutional neural network (3D-CNN) model.
However, the evaluation of this model, as well as other ATB segmentation
techniques reported in the literature, is done using Dynamic Time Warping (DTW)
distance between the entire original and predicted contours. Such an evaluation
measure may not capture local errors in the predicted contour. Careful analysis
of predicted contours reveals errors in regions like the velum part of contour1
(ATB comprising of upper lip, hard palate, and velum) and tongue base section
of contour2 (ATB covering jawline, lower lip, tongue base, and epiglottis),
which are not captured in a global evaluation metric like DTW distance. In this
work, we automatically detect such errors and propose a correction scheme for
the same. We also propose two new evaluation metrics for ATB segmentation
separately in contour1 and contour2 to explicitly capture two types of errors
in these contours. The proposed detection and correction strategies result in
an improvement of these two evaluation metrics by 61.8% and 61.4% for contour1
and by 67.8% and 28.4% for contour2. Traditional DTW distance, on the other
hand, improves by 44.6% for contour1 and 4.0% for contour2.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Role of ImageNet Classes in Fr\'echet Inception Distance. (arXiv:2203.06026v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06026">
<div class="article-summary-box-inner">
<span><p>Fr\'echet Inception Distance (FID) is a metric for quantifying the distance
between two distributions of images. Given its status as a standard yardstick
for ranking models in data-driven generative modeling research, it seems
important that the distance is computed from general, "vision-related"
features. But is it? We observe that FID is essentially a distance between sets
of ImageNet class probabilities. We trace the reason to the fact that the
standard feature space, the penultimate "pre-logit" layer of a particular
Inception-V3 classifier network, is only one affine transform away from the
logits, i.e., ImageNet classes, and thus, the features are necessarily highly
specialized to them. This has unintuitive consequences for the metric's
sensitivity. For example, when evaluating a model for human faces, we observe
that, on average, FID is actually very insensitive to the facial region, and
that the probabilities of classes like "bow tie" or "seat belt" play a much
larger role. Further, we show that FID can be significantly reduced -- without
actually improving the quality of results -- by an attack that first generates
a slightly larger set of candidates, and then chooses a subset that happens to
match the histogram of such "fringe features" in the real data. We then
demonstrate that this observation has practical relevance in case of ImageNet
pre-training of GANs, where a part of the observed FID improvement turns out
not to be real. Our results suggest caution against over-interpreting FID
improvements, and underline the need for distribution metrics that are more
perceptually uniform.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Embedding Earth: Self-supervised contrastive pre-training for dense land cover classification. (arXiv:2203.06041v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06041">
<div class="article-summary-box-inner">
<span><p>In training machine learning models for land cover semantic segmentation
there is a stark contrast between the availability of satellite imagery to be
used as inputs and ground truth data to enable supervised learning. While
thousands of new satellite images become freely available on a daily basis,
getting ground truth data is still very challenging, time consuming and costly.
In this paper we present Embedding Earth a self-supervised contrastive
pre-training method for leveraging the large availability of satellite imagery
to improve performance on downstream dense land cover classification tasks.
Performing an extensive experimental evaluation spanning four countries and two
continents we use models pre-trained with our proposed method as initialization
points for supervised land cover semantic segmentation and observe significant
improvements up to 25% absolute mIoU. In every case tested we outperform random
initialization, especially so when ground truth data are scarse. Through a
series of ablation studies we explore the qualities of the proposed approach
and find that learnt features can generalize between disparate regions opening
up the possibility of using the proposed pre-training scheme as a replacement
to random initialization for Earth observation tasks. Code will be uploaded
soon at https://github.com/michaeltrs/DeepSatModels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ROOD-MRI: Benchmarking the robustness of deep learning segmentation models to out-of-distribution and corrupted data in MRI. (arXiv:2203.06060v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06060">
<div class="article-summary-box-inner">
<span><p>Deep artificial neural networks (DNNs) have moved to the forefront of medical
image analysis due to their success in classification, segmentation, and
detection challenges. A principal challenge in large-scale deployment of DNNs
in neuroimage analysis is the potential for shifts in signal-to-noise ratio,
contrast, resolution, and presence of artifacts from site to site due to
variances in scanners and acquisition protocols. DNNs are famously susceptible
to these distribution shifts in computer vision. Currently, there are no
benchmarking platforms or frameworks to assess the robustness of new and
existing models to specific distribution shifts in MRI, and accessible
multi-site benchmarking datasets are still scarce or task-specific. To address
these limitations, we propose ROOD-MRI: a platform for benchmarking the
Robustness of DNNs to Out-Of-Distribution (OOD) data, corruptions, and
artifacts in MRI. The platform provides modules for generating benchmarking
datasets using transforms that model distribution shifts in MRI,
implementations of newly derived benchmarking metrics for image segmentation,
and examples for using the methodology with new models and tasks. We apply our
methodology to hippocampus, ventricle, and white matter hyperintensity
segmentation in several large studies, providing the hippocampus dataset as a
publicly available benchmark. By evaluating modern DNNs on these datasets, we
demonstrate that they are highly susceptible to distribution shifts and
corruptions in MRI. We show that while data augmentation strategies can
substantially improve robustness to OOD data for anatomical segmentation tasks,
modern DNNs using augmentation still lack robustness in more challenging
lesion-based segmentation tasks. We finally benchmark U-Nets and
transformer-based models, finding consistent differences in robustness to
particular classes of transforms across architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TAPE: Task-Agnostic Prior Embedding for Image Restoration. (arXiv:2203.06074v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06074">
<div class="article-summary-box-inner">
<span><p>Learning an generalized prior for natural image restoration is an important
yet challenging task. Early methods mostly involved handcrafted priors
including normalized sparsity, L0 gradients, dark channel priors, etc.
Recently, deep neural networks have been used to learn various image priors but
do not guarantee to generalize. In this paper, we propose a novel approach that
embeds a task-agnostic prior into a transformer. Our approach, named
Task-Agnostic Prior Embedding (TAPE), consists of three stages, namely,
task-agnostic pre-training, task-agnostic fine-tuning, and task-specific
fine-tuning, where the first one embeds prior knowledge about natural images
into the transformer and the latter two extracts the knowledge to assist
downstream image restoration. Experiments on various types of degradation
validate the effectiveness of TAPE. The image restoration performance in terms
of PSNR is improved by as much as 1.45 dB and even outperforms task-specific
algorithms. More importantly, TAPE shows the ability of disentangling
generalized image priors from degraded images, which enjoys favorable transfer
ability to unknown downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LFW-Beautified: A Dataset of Face Images with Beautification and Augmented Reality Filters. (arXiv:2203.06082v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06082">
<div class="article-summary-box-inner">
<span><p>Selfie images enjoy huge popularity in social media. The same platforms
centered around sharing this type of images offer filters to beautify them or
incorporate augmented reality effects. Studies suggests that filtered images
attract more views and engagement. Selfie images are also in increasing use in
security applications due to mobiles becoming data hubs for many transactions.
Also, video conference applications, boomed during the pandemic, include such
filters.
</p>
<p>Such filters may destroy biometric features that would allow person
recognition or even detection of the face itself, even if such commodity
applications are not necessarily used to compromise facial systems. This could
also affect subsequent investigations like crimes in social media, where
automatic analysis is usually necessary given the amount of information posted
in social sites or stored in devices or cloud repositories.
</p>
<p>To help in counteracting such issues, we contribute with a database of facial
images that includes several manipulations. It includes image enhancement
filters (which mostly modify contrast and lightning) and augmented reality
filters that incorporate items like animal noses or glasses. Additionally,
images with sunglasses are processed with a reconstruction network trained to
learn to reverse such modifications. This is because obfuscating the eye region
has been observed in the literature to have the highest impact on the accuracy
of face detection or recognition.
</p>
<p>We start from the popular Labeled Faces in the Wild (LFW) database, to which
we apply different modifications, generating 8 datasets. Each dataset contains
4,324 images of size 64 x 64, with a total of 34,592 images. The use of a
public and widely employed face dataset allows for replication and comparison.
</p>
<p>The created database is available at
https://github.com/HalmstadUniversityBiometrics/LFW-Beautified
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WLASL-LEX: a Dataset for Recognising Phonological Properties in American Sign Language. (arXiv:2203.06096v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06096">
<div class="article-summary-box-inner">
<span><p>Signed Language Processing (SLP) concerns the automated processing of signed
languages, the main means of communication of Deaf and hearing impaired
individuals. SLP features many different tasks, ranging from sign recognition
to translation and production of signed speech, but has been overlooked by the
NLP community thus far. In this paper, we bring to attention the task of
modelling the phonology of sign languages. We leverage existing resources to
construct a large-scale dataset of American Sign Language signs annotated with
six different phonological properties. We then conduct an extensive empirical
study to investigate whether data-driven end-to-end and feature-based
approaches can be optimised to automatically recognise these properties. We
find that, despite the inherent challenges of the task, graph-based neural
networks that operate over skeleton features extracted from raw videos are able
to succeed at the task to a varying degree. Most importantly, we show that this
performance pertains even on signs unobserved during training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">REX: Reasoning-aware and Grounded Explanation. (arXiv:2203.06107v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06107">
<div class="article-summary-box-inner">
<span><p>Effectiveness and interpretability are two essential properties for
trustworthy AI systems. Most recent studies in visual reasoning are dedicated
to improving the accuracy of predicted answers, and less attention is paid to
explaining the rationales behind the decisions. As a result, they commonly take
advantage of spurious biases instead of actually reasoning on the
visual-textual data, and have yet developed the capability to explain their
decision making by considering key information from both modalities. This paper
aims to close the gap from three distinct perspectives: first, we define a new
type of multi-modal explanations that explain the decisions by progressively
traversing the reasoning process and grounding keywords in the images. We
develop a functional program to sequentially execute different reasoning steps
and construct a new dataset with 1,040,830 multi-modal explanations. Second, we
identify the critical need to tightly couple important components across the
visual and textual modalities for explaining the decisions, and propose a novel
explanation generation method that explicitly models the pairwise
correspondence between words and regions of interest. It improves the visual
grounding capability by a considerable margin, resulting in enhanced
interpretability and reasoning performance. Finally, with our new data and
method, we perform extensive analyses to study the effectiveness of our
explanation under different settings, including multi-task learning and
transfer learning. Our code and data are available at
https://github.com/szzexpoi/rex.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ActiveMLP: An MLP-like Architecture with Active Token Mixer. (arXiv:2203.06108v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06108">
<div class="article-summary-box-inner">
<span><p>This paper presents ActiveMLP, a general MLP-like backbone for computer
vision. The three existing dominant network families, i.e., CNNs, Transformers
and MLPs, differ from each other mainly in the ways to fuse contextual
information into a given token, leaving the design of more effective
token-mixing mechanisms at the core of backbone architecture development. In
ActiveMLP, we propose an innovative token-mixer, dubbed Active Token Mixer
(ATM), to actively incorporate contextual information from other tokens in the
global scope into the given one. This fundamental operator actively predicts
where to capture useful contexts and learns how to fuse the captured contexts
with the original information of the given token at channel levels. In this
way, the spatial range of token-mixing is expanded and the way of token-mixing
is reformed. With this design, ActiveMLP is endowed with the merits of global
receptive fields and more flexible content-adaptive information fusion.
Extensive experiments demonstrate that ActiveMLP is generally applicable and
comprehensively surpasses different families of SOTA vision backbones by a
clear margin on a broad range of vision tasks, including visual recognition and
dense prediction tasks. The code and models will be available at
https://github.com/microsoft/ActiveMLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-sensor large-scale dataset for multi-view 3D reconstruction. (arXiv:2203.06111v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06111">
<div class="article-summary-box-inner">
<span><p>We present a new multi-sensor dataset for 3D surface reconstruction. It
includes registered RGB and depth data from sensors of different resolutions
and modalities: smartphones, Intel RealSense, Microsoft Kinect, industrial
cameras, and structured-light scanner. The data for each scene is obtained
under a large number of lighting conditions, and the scenes are selected to
emphasize a diverse set of material properties challenging for existing
algorithms. In the acquisition process, we aimed to maximize high-resolution
depth data quality for challenging cases, to provide reliable ground truth for
learning algorithms. Overall, we provide over 1.4 million images of 110
different scenes acquired at 14 lighting conditions from 100 viewing
directions. We expect our dataset will be useful for evaluation and training of
3D reconstruction algorithms of different types and for other related tasks.
Our dataset and accompanying software will be available online.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detection of multiple retinal diseases in ultra-widefield fundus images using deep learning: data-driven identification of relevant regions. (arXiv:2203.06113v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06113">
<div class="article-summary-box-inner">
<span><p>Ultra-widefield (UWF) imaging is a promising modality that captures a larger
retinal field of view compared to traditional fundus photography. Previous
studies showed that deep learning (DL) models are effective for detecting
retinal disease in UWF images, but primarily considered individual diseases
under less-than-realistic conditions (excluding images with other diseases,
artefacts, comorbidities, or borderline cases; and balancing healthy and
diseased images) and did not systematically investigate which regions of the
UWF images are relevant for disease detection. We first improve on the state of
the field by proposing a DL model that can recognise multiple retinal diseases
under more realistic conditions. We then use global explainability methods to
identify which regions of the UWF images the model generally attends to. Our
model performs very well, separating between healthy and diseased retinas with
an area under the curve (AUC) of 0.9206 on an internal test set, and an AUC of
0.9841 on a challenging, external test set. When diagnosing specific diseases,
the model attends to regions where we would expect those diseases to occur. We
further identify the posterior pole as the most important region in a purely
data-driven fashion. Surprisingly, 10% of the image around the posterior pole
is sufficient for achieving comparable performance to having the full images
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning the Shape of the Brain Connectome. (arXiv:2203.06122v1 [q-bio.NC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06122">
<div class="article-summary-box-inner">
<span><p>To statistically study the variability and differences between normal and
abnormal brain connectomes, a mathematical model of the neural connections is
required. In this paper, we represent the brain connectome as a Riemannian
manifold, which allows us to model neural connections as geodesics. We show for
the first time how one can leverage deep neural networks to estimate a
Riemannian metric of the brain that can accommodate fiber crossings and is a
natural modeling tool to infer the shape of the brain from DWMRI. Our method
achieves excellent performance in geodesic-white-matter-pathway alignment and
tackles the long-standing issue in previous methods: the inability to recover
the crossing fibers with high fidelity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatial Consistency Loss for Training Multi-Label Classifiers from Single-Label Annotations. (arXiv:2203.06127v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06127">
<div class="article-summary-box-inner">
<span><p>As natural images usually contain multiple objects, multi-label image
classification is more applicable "in the wild" than single-label
classification. However, exhaustively annotating images with every object of
interest is costly and time-consuming. We aim to train multi-label classifiers
from single-label annotations only. We show that adding a consistency loss,
ensuring that the predictions of the network are consistent over consecutive
training epochs, is a simple yet effective method to train multi-label
classifiers in a weakly supervised setting. We further extend this approach
spatially, by ensuring consistency of the spatial feature maps produced over
consecutive training epochs, maintaining per-class running-average heatmaps for
each training image. We show that this spatial consistency loss further
improves the multi-label mAP of the classifiers. In addition, we show that this
method overcomes shortcomings of the "crop" data-augmentation by recovering
correct supervision signal even when most of the single ground truth object is
cropped out of the input image by the data augmentation. We demonstrate gains
of the consistency and spatial consistency losses over the binary cross-entropy
baseline, and over competing methods, on MS-COCO and Pascal VOC. We also
demonstrate improved multi-label classification mAP on ImageNet-1K using the
ReaL multi-label validation set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neuromorphic Data Augmentation for Training Spiking Neural Networks. (arXiv:2203.06145v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06145">
<div class="article-summary-box-inner">
<span><p>Developing neuromorphic intelligence on event-based datasets with spiking
neural networks (SNNs) has recently attracted much research attention. However,
the limited size of event-based datasets makes SNNs prone to overfitting and
unstable convergence. This issue remains unexplored by previous academic works.
In an effort to minimize this generalization gap, we propose neuromorphic data
augmentation (NDA), a family of geometric augmentations specifically designed
for event-based datasets with the goal of significantly stabilizing the SNN
training and reducing the generalization gap between training and test
performance. The proposed method is simple and compatible with existing SNN
training pipelines. Using the proposed augmentation, for the first time, we
demonstrate the feasibility of unsupervised contrastive learning for SNNs. We
conduct comprehensive experiments on prevailing neuromorphic vision benchmarks
and show that NDA yields substantial improvements over previous
state-of-the-art results. For example, NDA-based SNN achieves accuracy gain on
CIFAR10-DVS and N-Caltech 101 by 10.1% and 13.7%, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep AutoAugment. (arXiv:2203.06172v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06172">
<div class="article-summary-box-inner">
<span><p>While recent automated data augmentation methods lead to state-of-the-art
results, their design spaces and the derived data augmentation strategies still
incorporate strong human priors. In this work, instead of fixing a set of
hand-picked default augmentations alongside the searched data augmentations, we
propose a fully automated approach for data augmentation search named Deep
AutoAugment (DeepAA). DeepAA progressively builds a multi-layer data
augmentation pipeline from scratch by stacking augmentation layers one at a
time until reaching convergence. For each augmentation layer, the policy is
optimized to maximize the cosine similarity between the gradients of the
original and augmented data along the direction with low variance. Our
experiments show that even without default augmentations, we can learn an
augmentation policy that achieves strong performance with that of previous
works. Extensive ablation studies show that the regularized gradient matching
is an effective search method for data augmentation policies. Our code is
available at: https://github.com/MSU-MLSys-Lab/DeepAA .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Masked Visual Pre-training for Motor Control. (arXiv:2203.06173v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.06173">
<div class="article-summary-box-inner">
<span><p>This paper shows that self-supervised visual pre-training from real-world
images is effective for learning motor control tasks from pixels. We first
train the visual representations by masked modeling of natural images. We then
freeze the visual encoder and train neural network controllers on top with
reinforcement learning. We do not perform any task-specific fine-tuning of the
encoder; the same visual representations are used for all motor control tasks.
To the best of our knowledge, this is the first self-supervised model to
exploit real-world images at scale for motor control. To accelerate progress in
learning from pixels, we contribute a benchmark suite of hand-designed tasks
varying in movements, scenes, and robots. Without relying on labels,
state-estimation, or expert demonstrations, we consistently outperform
supervised encoders by up to 80% absolute success rate, sometimes even matching
the oracle state performance. We also find that in-the-wild images, e.g., from
YouTube or Egocentric videos, lead to better visual representations for various
manipulation tasks than ImageNet images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse Black-box Video Attack with Reinforcement Learning. (arXiv:2001.03754v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2001.03754">
<div class="article-summary-box-inner">
<span><p>Adversarial attacks on video recognition models have been explored recently.
However, most existing works treat each video frame equally and ignore their
temporal interactions. To overcome this drawback, a few methods try to select
some key frames and then perform attacks based on them. Unfortunately, their
selection strategy is independent of the attacking step, therefore the
resulting performance is limited. Instead, we argue the frame selection phase
is closely relevant with the attacking phase. The key frames should be adjusted
according to the attacking results. For that, we formulate the black-box video
attacks into a Reinforcement Learning (RL) framework. Specifically, the
environment in RL is set as the recognition model, and the agent in RL plays
the role of frame selecting. By continuously querying the recognition models
and receiving the attacking feedback, the agent gradually adjusts its frame
selection strategy and adversarial perturbations become smaller and smaller. We
conduct a series of experiments with two mainstream video recognition models:
C3D and LRCN on the public UCF-101 and HMDB-51 datasets. The results
demonstrate that the proposed method can significantly reduce the adversarial
perturbations with efficient query times.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Monocular Real-time Hand Shape and Motion Capture using Multi-modal Data. (arXiv:2003.09572v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.09572">
<div class="article-summary-box-inner">
<span><p>We present a novel method for monocular hand shape and pose estimation at
unprecedented runtime performance of 100fps and at state-of-the-art accuracy.
This is enabled by a new learning based architecture designed such that it can
make use of all the sources of available hand training data: image data with
either 2D or 3D annotations, as well as stand-alone 3D animations without
corresponding image data. It features a 3D hand joint detection module and an
inverse kinematics module which regresses not only 3D joint positions but also
maps them to joint rotations in a single feed-forward pass. This output makes
the method more directly usable for applications in computer vision and
graphics compared to only regressing 3D joint positions. We demonstrate that
our architectural design leads to a significant quantitative and qualitative
improvement over the state of the art on several challenging benchmarks. Our
model is publicly available for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Training for Class-Incremental Semantic Segmentation. (arXiv:2012.03362v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.03362">
<div class="article-summary-box-inner">
<span><p>In class-incremental semantic segmentation, we have no access to the labeled
data of previous tasks. Therefore, when incrementally learning new classes,
deep neural networks suffer from catastrophic forgetting of previously learned
knowledge. To address this problem, we propose to apply a self-training
approach that leverages unlabeled data, which is used for the rehearsal of
previous knowledge. Specifically, we first learn a temporary model for the
current task, and then pseudo labels for the unlabeled data are computed by
fusing information from the old model of the previous task and the current
temporary model. Additionally, conflict reduction is proposed to resolve the
conflicts of pseudo labels generated from both the old and temporary models. We
show that maximizing self-entropy can further improve results by smoothing the
overconfident predictions. Interestingly, in the experiments we show that the
auxiliary data can be different from the training data and that even
general-purpose but diverse auxiliary data can lead to large performance gains.
The experiments demonstrate state-of-the-art results: obtaining a relative gain
of up to 114% on Pascal-VOC 2012 and 8.5% on the more challenging ADE20K
compared to previous state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Style Normalization and Restitution for Domain Generalization and Adaptation. (arXiv:2101.00588v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00588">
<div class="article-summary-box-inner">
<span><p>For many practical computer vision applications, the learned models usually
have high performance on the datasets used for training but suffer from
significant performance degradation when deployed in new environments, where
there are usually style differences between the training images and the testing
images. An effective domain generalizable model is expected to be able to learn
feature representations that are both generalizable and discriminative. In this
paper, we design a novel Style Normalization and Restitution module (SNR) to
simultaneously ensure both high generalization and discrimination capability of
the networks. In the SNR module, particularly, we filter out the style
variations (e.g, illumination, color contrast) by performing Instance
Normalization (IN) to obtain style normalized features, where the discrepancy
among different samples and domains is reduced. However, such a process is
task-ignorant and inevitably removes some task-relevant discriminative
information, which could hurt the performance. To remedy this, we propose to
distill task-relevant discriminative features from the residual (i.e, the
difference between the original feature and the style normalized feature) and
add them back to the network to ensure high discrimination. Moreover, for
better disentanglement, we enforce a dual causality loss constraint in the
restitution step to encourage the better separation of task-relevant and
task-irrelevant features. We validate the effectiveness of our SNR on different
computer vision tasks, including classification, semantic segmentation, and
object detection. Experiments demonstrate that our SNR module is capable of
improving the performance of networks for domain generalization (DG) and
unsupervised domain adaptation (UDA) on many tasks. Code are available at
https://github.com/microsoft/SNR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Action Quality Assessment using Weighted Aggregation. (arXiv:2102.10555v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.10555">
<div class="article-summary-box-inner">
<span><p>Action quality assessment (AQA) aims at automatically judging human action
based on a video of the said action and assigning a performance score to it.
The majority of works in the existing literature on AQA divide RGB videos into
short clips, transform these clips to higher-level representations using
Convolutional 3D (C3D) networks, and aggregate them through averaging. These
higher-level representations are used to perform AQA. We find that the current
clip level feature aggregation technique of averaging is insufficient to
capture the relative importance of clip level features. In this work, we
propose a learning-based weighted-averaging technique. Using this technique,
better performance can be obtained without sacrificing too much computational
resources. We call this technique Weight-Decider(WD). We also experiment with
ResNets for learning better representations for action quality assessment. We
assess the effects of the depth and input clip size of the convolutional neural
network on the quality of action score predictions. We achieve a new
state-of-the-art Spearman's rank correlation of 0.9315 (an increase of 0.45%)
on the MTL-AQA dataset using a 34 layer (2+1)D ResNet with the capability of
processing 32 frame clips, with WD aggregation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Content-Aware Detection of Temporal Metadata Manipulation. (arXiv:2103.04736v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.04736">
<div class="article-summary-box-inner">
<span><p>Most pictures shared online are accompanied by temporal metadata (i.e., the
day and time they were taken), which makes it possible to associate an image
content with real-world events. Maliciously manipulating this metadata can
convey a distorted version of reality. In this work, we present the emerging
problem of detecting timestamp manipulation. We propose an end-to-end approach
to verify whether the purported time of capture of an outdoor image is
consistent with its content and geographic location. We consider manipulations
done in the hour and/or month of capture of a photograph. The central idea is
the use of supervised consistency verification, in which we predict the
probability that the image content, capture time, and geographical location are
consistent. We also include a pair of auxiliary tasks, which can be used to
explain the network decision. Our approach improves upon previous work on a
large benchmark dataset, increasing the classification accuracy from 59.0% to
81.1%. We perform an ablation study that highlights the importance of various
components of the method, showing what types of tampering are detectable using
our approach. Finally, we demonstrate how the proposed method can be employed
to estimate a possible time-of-capture in scenarios in which the timestamp is
missing from the metadata.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InfinityGAN: Towards Infinite-Pixel Image Synthesis. (arXiv:2104.03963v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.03963">
<div class="article-summary-box-inner">
<span><p>We present a novel framework, InfinityGAN, for arbitrary-sized image
generation. The task is associated with several key challenges. First, scaling
existing models to an arbitrarily large image size is resource-constrained, in
terms of both computation and availability of large-field-of-view training
data. InfinityGAN trains and infers in a seamless patch-by-patch manner with
low computational resources. Second, large images should be locally and
globally consistent, avoid repetitive patterns, and look realistic. To address
these, InfinityGAN disentangles global appearances, local structures, and
textures. With this formulation, we can generate images with spatial size and
level of details not attainable before. Experimental evaluation validates that
InfinityGAN generates images with superior realism compared to baselines and
features parallelizable inference. Finally, we show several applications
unlocked by our approach, such as spatial style fusion, multi-modal
outpainting, and image inbetweening. All applications can be operated with
arbitrary input and output sizes. Please find the full version of the paper at
https://openreview.net/forum?id=ufGMqIM0a4b .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FLEX: Extrinsic Parameter-free Multi-view 3D Human Motion Reconstruction. (arXiv:2105.01937v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.01937">
<div class="article-summary-box-inner">
<span><p>The increasing availability of video recordings made by multiple cameras has
offered new means for mitigating occlusion and depth ambiguities in pose and
motion reconstruction methods. Yet, multi-view algorithms strongly depend on
camera parameters; particularly, the relative transformations between the
cameras. Such a dependency becomes a hurdle once shifting to dynamic capture in
uncontrolled settings. We introduce FLEX (Free muLti-view rEconstruXion), an
end-to-end extrinsic parameter-free multi-view model. FLEX is extrinsic
parameter-free (dubbed ep-free) in the sense that it does not require extrinsic
camera parameters. Our key idea is that the 3D angles between skeletal parts,
as well as bone lengths, are invariant to the camera position. Hence, learning
3D rotations and bone lengths rather than locations allows predicting common
values for all camera views. Our network takes multiple video streams, learns
fused deep features through a novel multi-view fusion layer, and reconstructs a
single consistent skeleton with temporally coherent joint rotations. We
demonstrate quantitative and qualitative results on three public datasets, and
on synthetic multi-person video streams captured by dynamic cameras. We compare
our model to state-of-the-art methods that are not ep-free and show that in the
absence of camera parameters, we outperform them by a large margin while
obtaining comparable results when camera parameters are available. Code,
trained models, and other materials are available on our project page.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RLCorrector: Reinforced Proofreading for Cell-level Microscopy Image Segmentation. (arXiv:2106.05487v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05487">
<div class="article-summary-box-inner">
<span><p>Segmentation of nanoscale electron microscopy (EM) images is crucial but
still challenging in connectomics research. One reason for this is that none of
the existing segmentation methods are error-free, so they require proofreading,
which is typically implemented as an interactive, semi-automatic process via
manual intervention. Herein, we propose a fully automatic proofreading method
based on reinforcement learning that mimics the human decision process of
detection, classification, and correction of segmentation errors. We
systematically design the proposed system by combining multiple reinforcement
learning agents in a hierarchical manner, where each agent focuses only on a
specific task while preserving dependency between agents. Furthermore, we
demonstrate that the episodic task setting of reinforcement learning can
efficiently manage a combination of merge and split errors concurrently
presented in the input. We demonstrate the efficacy of the proposed system by
comparing it with conventional proofreading methods over various testing cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">To fit or not to fit: Model-based Face Reconstruction and Occlusion Segmentation from Weak Supervision. (arXiv:2106.09614v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.09614">
<div class="article-summary-box-inner">
<span><p>3D face reconstruction under occlusions is highly challenging due to the
large variability of occluders. Currently, the most successful methods fit a 3D
face model through inverse rendering and assume a given segmentation of the
occluder to avoid fitting the occluder. However, training an occlusion
segmentation model requires large amounts of annotated data. In this work, we
introduce a model-based approach for 3D face reconstruction that is highly
robust to occlusions but does not require any occlusion annotations for
training. In our approach, we exploit the fact that generative face models can
only synthesize human faces, but not the occluders. We use this property to
guide the decision-making process of an occlusion segmentation network and
resulting in unsupervised training. The main challenge is that the model
fitting and the occlusion segmentation are mutually dependent on each other,
and need to be inferred jointly. We resolve this chicken-and-egg problem with
an EM-type training strategy. This leads to a synergistic effect, in which the
segmentation network prevents the face encoder from fitting to the occlusion,
enhancing the reconstruction quality. The improved 3D face reconstruction, in
turn, enables the segmentation network to better predict the occlusion.
Qualitative and quantitative experiments on the CelebA-HQ, the AR databases,
and the NoW challenge demonstrate that the proposed pipeline achieves the
state-of-the-art 3D face reconstruction under occlusion. Moreover, the
segmentation network localizes occlusions accurately despite being trained
without any occlusion annotation. The code is available at
https://github.com/unibas-gravis/Occlusion-Robust-MoFA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pseudo-LiDAR Based Road Detection. (arXiv:2107.13279v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.13279">
<div class="article-summary-box-inner">
<span><p>Road detection is a critically important task for self-driving cars. By
employing LiDAR data, recent works have significantly improved the accuracy of
road detection. Relying on LiDAR sensors limits the wide application of those
methods when only cameras are available. In this paper, we propose a novel road
detection approach with RGB being the only input during inference.
Specifically, we exploit pseudo-LiDAR using depth estimation, and propose a
feature fusion network where RGB and learned depth information are fused for
improved road detection. To further optimize the network structure and improve
the efficiency of the network. we search for the network structure of the
feature fusion module using NAS techniques. Finally, be aware of that
generating pseudo-LiDAR from RGB via depth estimation introduces extra
computational costs and relies on depth estimation networks, we design a
modality distillation strategy and leverage it to further free our network from
these extra computational cost and dependencies during inference. The proposed
method achieves state-of-the-art performance on two challenging benchmarks,
KITTI and R2D.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RAMA: A Rapid Multicut Algorithm on GPU. (arXiv:2109.01838v3 [cs.DC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01838">
<div class="article-summary-box-inner">
<span><p>We propose a highly parallel primal-dual algorithm for the multicut (a.k.a.
correlation clustering) problem, a classical graph clustering problem widely
used in machine learning and computer vision. Our algorithm consists of three
steps executed recursively: (1) Finding conflicted cycles that correspond to
violated inequalities of the underlying multicut relaxation, (2) Performing
message passing between the edges and cycles to optimize the Lagrange
relaxation coming from the found violated cycles producing reduced costs and
(3) Contracting edges with high reduced costs through matrix-matrix
multiplications. Our algorithm produces primal solutions and lower bounds that
estimate the distance to optimum. We implement our algorithm on GPUs and show
resulting one to two orders-of-magnitudes improvements in execution speed
without sacrificing solution quality compared to traditional sequential
algorithms that run on CPUs. We can solve very large scale benchmark problems
with up to $\mathcal{O}(10^8)$ variables in a few seconds with small
primal-dual gaps. Our code is available at
https://github.com/pawelswoboda/RAMA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DAFNe: A One-Stage Anchor-Free Approach for Oriented Object Detection. (arXiv:2109.06148v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06148">
<div class="article-summary-box-inner">
<span><p>We present DAFNe, a Dense one-stage Anchor-Free deep Network for oriented
object detection. As a one-stage model, it performs bounding box predictions on
a dense grid over the input image, being architecturally simpler in design, as
well as easier to optimize than its two-stage counterparts. Furthermore, as an
anchor-free model, it reduces the prediction complexity by refraining from
employing bounding box anchors. With DAFNe we introduce an orientation-aware
generalization of the center-ness function for arbitrarily oriented bounding
boxes to down-weight low-quality predictions and a center-to-corner bounding
box prediction strategy that improves object localization performance. Our
experiments show that DAFNe outperforms all previous one-stage anchor-free
models on DOTA 1.0, DOTA 1.5, and UCAS-AOD and is on par with the best models
on HRSC2016.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FathomNet: A global image database for enabling artificial intelligence in the ocean. (arXiv:2109.14646v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14646">
<div class="article-summary-box-inner">
<span><p>The ocean is experiencing unprecedented rapid change, and visually monitoring
marine biota at the spatiotemporal scales needed for responsible stewardship is
a formidable task. As baselines are sought by the research community, the
volume and rate of this required data collection rapidly outpaces our abilities
to process and analyze them. Recent advances in machine learning enables fast,
sophisticated analysis of visual data, but have had limited success in the
ocean due to lack of data standardization, insufficient formatting, and demand
for large, labeled datasets. To address this need, we built FathomNet, an
open-source image database that standardizes and aggregates expertly curated
labeled data. FathomNet has been seeded with existing iconic and non-iconic
imagery of marine animals, underwater equipment, debris, and other concepts,
and allows for future contributions from distributed data sources. We
demonstrate how FathomNet data can be used to train and deploy models on other
institutional video to reduce annotation effort, and enable automated tracking
of underwater concepts when integrated with robotic vehicles. As FathomNet
continues to grow and incorporate more labeled data from the community, we can
accelerate the processing of visual data to achieve a healthy and sustainable
global ocean.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Instance Segmentation with Automotive Radar Detection Points. (arXiv:2110.01775v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01775">
<div class="article-summary-box-inner">
<span><p>Automotive radar provides reliable environmental perception in all-weather
conditions with affordable cost, but it hardly supplies semantic and geometry
information due to the sparsity of radar detection points. With the development
of automotive radar technologies in recent years, instance segmentation becomes
possible by using automotive radar. Its data contain contexts such as radar
cross section and micro-Doppler effects, and sometimes can provide detection
when the field of view is obscured. The outcome from instance segmentation
could be potentially used as the input of trackers for tracking targets. The
existing methods often utilize a clustering based classification framework,
which fits the need of real-time processing but has limited performance due to
minimum information provided by sparse radar detection points. In this paper,
we propose an efficient method based on clustering of estimated semantic
information to achieve instance segmentation for the sparse radar detection
points. In addition, we show that the performance of the proposed approach can
be further enhanced by incorporating the visual multi-layer perceptron. The
effectiveness of the proposed method is verified by experimental results on the
popular RadarScenes dataset, achieving 89.53% mCov and 86.97% mAP0.5, which is
the best comparing to other approaches in the literature. More significantly,
the proposed algorithm consumes memory around 1MB, and the inference time is
less than 40ms. These two criteria ensure the practicality of the proposed
method in real-world systems
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Effect of Selfie Beautification Filters on Face Detection and Recognition. (arXiv:2110.08934v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08934">
<div class="article-summary-box-inner">
<span><p>Beautification and augmented reality filters are very popular in applications
that use selfie images captured with smartphones or personal devices. However,
they can distort or modify biometric features, severely affecting the
capability of recognizing individuals' identity or even detecting the face.
Accordingly, we address the effect of such filters on the accuracy of automated
face detection and recognition. The social media image filters studied either
modify the image contrast or illumination or occlude parts of the face with for
example artificial glasses or animal noses. We observe that the effect of some
of these filters is harmful both to face detection and identity recognition,
specially if they obfuscate the eye or (to a lesser extent) the nose. To
counteract such effect, we develop a method to reconstruct the applied
manipulation with a modified version of the U-NET segmentation network. This is
observed to contribute to a better face detection and recognition accuracy.
From a recognition perspective, we employ distance measures and trained machine
learning algorithms applied to features extracted using a ResNet-34 network
trained to recognize faces. We also evaluate if incorporating filtered images
to the training set of machine learning approaches are beneficial for identity
recognition. Our results show good recognition when filters do not occlude
important landmarks, specially the eyes (identification accuracy &gt;99%, EER&lt;2%).
The combined effect of the proposed approaches also allow to mitigate the
effect produced by filters that occlude parts of the face, achieving an
identification accuracy of &gt;92% with the majority of perturbations evaluated,
and an EER &lt;8%. Although there is room for improvement, when neither U-NET
reconstruction nor training with filtered images is applied, the accuracy with
filters that severely occlude the eye is &lt;72% (identification) and &gt;12% (EER)
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Restormer: Efficient Transformer for High-Resolution Image Restoration. (arXiv:2111.09881v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.09881">
<div class="article-summary-box-inner">
<span><p>Since convolutional neural networks (CNNs) perform well at learning
generalizable image priors from large-scale data, these models have been
extensively applied to image restoration and related tasks. Recently, another
class of neural architectures, Transformers, have shown significant performance
gains on natural language and high-level vision tasks. While the Transformer
model mitigates the shortcomings of CNNs (i.e., limited receptive field and
inadaptability to input content), its computational complexity grows
quadratically with the spatial resolution, therefore making it infeasible to
apply to most image restoration tasks involving high-resolution images. In this
work, we propose an efficient Transformer model by making several key designs
in the building blocks (multi-head attention and feed-forward network) such
that it can capture long-range pixel interactions, while still remaining
applicable to large images. Our model, named Restoration Transformer
(Restormer), achieves state-of-the-art results on several image restoration
tasks, including image deraining, single-image motion deblurring, defocus
deblurring (single-image and dual-pixel data), and image denoising (Gaussian
grayscale/color denoising, and real image denoising). The source code and
pre-trained models are available at https://github.com/swz30/Restormer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FastDOG: Fast Discrete Optimization on GPU. (arXiv:2111.10270v2 [math.OC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10270">
<div class="article-summary-box-inner">
<span><p>We present a massively parallel Lagrange decomposition method for solving
0--1 integer linear programs occurring in structured prediction. We propose a
new iterative update scheme for solving the Lagrangean dual and a perturbation
technique for decoding primal solutions. For representing subproblems we follow
Lange et al. (2021) and use binary decision diagrams (BDDs). Our primal and
dual algorithms require little synchronization between subproblems and
optimization over BDDs needs only elementary operations without complicated
control flow. This allows us to exploit the parallelism offered by GPUs for all
components of our method. We present experimental results on combinatorial
problems from MAP inference for Markov Random Fields, quadratic assignment and
cell tracking for developmental biology. Our highly parallel GPU implementation
improves upon the running times of the algorithms from Lange et al. (2021) by
up to an order of magnitude. In particular, we come close to or outperform some
state-of-the-art specialized heuristics while being problem agnostic. Our
implementation is available at https://github.com/LPMP/BDD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Image Patch is a Wave: Quantum Inspired Vision MLP. (arXiv:2111.12294v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.12294">
<div class="article-summary-box-inner">
<span><p>In the field of computer vision, recent works show that a pure MLP
architecture mainly stacked by fully-connected layers can achieve competing
performance with CNN and transformer. An input image of vision MLP is usually
split into multiple tokens (patches), while the existing MLP models directly
aggregate them with fixed weights, neglecting the varying semantic information
of tokens from different images. To dynamically aggregate tokens, we propose to
represent each token as a wave function with two parts, amplitude and phase.
Amplitude is the original feature and the phase term is a complex value
changing according to the semantic contents of input images. Introducing the
phase term can dynamically modulate the relationship between tokens and fixed
weights in MLP. Based on the wave-like token representation, we establish a
novel Wave-MLP architecture for vision tasks. Extensive experiments demonstrate
that the proposed Wave-MLP is superior to the state-of-the-art MLP
architectures on various vision tasks such as image classification, object
detection and semantic segmentation. The source code will be available at
https://github.com/huawei-noah/CV-Backbones/tree/master/wavemlp_pytorch and
https://gitee.com/mindspore/models/tree/master/research/cv/wave_mlp.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep MAGSAC++. (arXiv:2111.14093v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.14093">
<div class="article-summary-box-inner">
<span><p>We propose Deep MAGSAC++ combining the advantages of traditional and deep
robust estimators. We introduce a novel loss function that exploits the
orientation and scale from partially affine covariant features, e.g., SIFT, in
a geometrically justifiable manner. The new loss helps in learning higher-order
information about the underlying scene geometry. Moreover, we propose a new
sampler for RANSAC that always selects the sample with the highest probability
of consisting only of inliers. After every unsuccessful iteration, the
probabilities are updated in a principled way via a Bayesian approach. The
prediction of the deep network is exploited as prior inside the sampler.
Benefiting from the new loss, the proposed sampler and a number of technical
advancements, Deep MAGSAC++ is superior to the state-of-the-art both in terms
of accuracy and run-time on thousands of image pairs from publicly available
real-world datasets for essential and fundamental matrix estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">No-Reference Point Cloud Quality Assessment via Domain Adaptation. (arXiv:2112.02851v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.02851">
<div class="article-summary-box-inner">
<span><p>We present a novel no-reference quality assessment metric, the image
transferred point cloud quality assessment (IT-PCQA), for 3D point clouds. For
quality assessment, deep neural network (DNN) has shown compelling performance
on no-reference metric design. However, the most challenging issue for
no-reference PCQA is that we lack large-scale subjective databases to drive
robust networks. Our motivation is that the human visual system (HVS) is the
decision-maker regardless of the type of media for quality assessment.
Leveraging the rich subjective scores of the natural images, we can quest the
evaluation criteria of human perception via DNN and transfer the capability of
prediction to 3D point clouds. In particular, we treat natural images as the
source domain and point clouds as the target domain, and infer point cloud
quality via unsupervised adversarial domain adaptation. To extract effective
latent features and minimize the domain discrepancy, we propose a hierarchical
feature encoder and a conditional-discriminative network. Considering that the
ultimate purpose is regressing objective score, we introduce a novel
conditional cross entropy loss in the conditional-discriminative network to
penalize the negative samples which hinder the convergence of the quality
regression network. Experimental results show that the proposed method can
achieve higher performance than traditional no-reference metrics, even
comparable results with full-reference metrics. The proposed method also
suggests the feasibility of assessing the quality of specific media content
without the expensive and cumbersome subjective evaluations. Code is available
at https://github.com/Qi-Yangsjtu/IT-PCQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interactive Visualization and Representation Analysis Applied to Glacier Segmentation. (arXiv:2112.08184v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08184">
<div class="article-summary-box-inner">
<span><p>Interpretability has attracted increasing attention in earth observation
problems. We apply interactive visualization and representation analysis to
guide interpretation of glacier segmentation models. We visualize the
activations from a U-Net to understand and evaluate the model performance. We
build an online interface using the Shiny R package to provide comprehensive
error analysis of the predictions. Users can interact with the panels and
discover model failure modes. Further, we discuss how visualization can provide
sanity checks during data preprocessing and model training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Deep Learning Based Workflow for Detection of Lung Nodules With Chest Radiograph. (arXiv:2112.10184v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.10184">
<div class="article-summary-box-inner">
<span><p>PURPOSE: This study aimed to develop a deep learning-based tool to detect and
localize lung nodules with chest radiographs(CXRs). We expected it to enhance
the efficiency of interpreting CXRs and reduce the possibilities of delayed
diagnosis of lung cancer.
</p>
<p>MATERIALS AND METHODS: We collected CXRs from NCKUH database and VBD, an
open-source medical image dataset, as our training and validation data. A
number of CXRs from the Ministry of Health and Welfare(MOHW) database served as
our test data. We built a segmentation model to identify lung areas from CXRs,
and sliced them into 16 patches. Physicians labeled the CXRs by clicking the
patches. These labeled patches were then used to train and fine-tune a deep
neural network(DNN) model, classifying the patches as positive or negative.
Finally, we test the DNN model with the lung patches of CXRs from MOHW.
</p>
<p>RESULTS: Our segmentation model identified the lung regions well from the
whole CXR. The Intersection over Union(IoU) between the ground truth and the
segmentation result was 0.9228. In addition, our DNN model achieved a
sensitivity of 0.81, specificity of 0.82, and AUROC of 0.869 in 98 of 125
cases. For the other 27 difficult cases, the sensitivity was 0.54, specificity
0.494, and AUROC 0.682. Overall, we obtained a sensitivity of 0.78, specificity
of 0.79, and AUROC 0.837.
</p>
<p>CONCLUSIONS: Our two-step workflow is comparable to state-of-the-art
algorithms in the sensitivity and specificity of localizing lung nodules from
CXRs. Notably, our workflow provides an efficient way for specialists to label
the data, which is valuable for relevant researches because of the relative
rarity of labeled medical image data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Depth Estimation for Multi-View Stereo: A Unified Representation. (arXiv:2201.01501v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.01501">
<div class="article-summary-box-inner">
<span><p>Depth estimation is solved as a regression or classification problem in
existing learning-based multi-view stereo methods. Although these two
representations have recently demonstrated their excellent performance, they
still have apparent shortcomings, e.g., regression methods tend to overfit due
to the indirect learning cost volume, and classification methods cannot
directly infer the exact depth due to its discrete prediction. In this paper,
we propose a novel representation, termed Unification, to unify the advantages
of regression and classification. It can directly constrain the cost volume
like classification methods, but also realize the sub-pixel depth prediction
like regression methods. To excavate the potential of unification, we design a
new loss function named Unified Focal Loss, which is more uniform and
reasonable to combat the challenge of sample imbalance. Combining these two
unburdened modules, we present a coarse-to-fine framework, that we call
UniMVSNet. The results of ranking first on both DTU and Tanks and Temples
benchmarks verify that our model not only performs the best but also has the
best generalization ability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Adversarial Robustness of Trajectory Prediction for Autonomous Vehicles. (arXiv:2201.05057v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05057">
<div class="article-summary-box-inner">
<span><p>Trajectory prediction is a critical component for autonomous vehicles (AVs)
to perform safe planning and navigation. However, few studies have analyzed the
adversarial robustness of trajectory prediction or investigated whether the
worst-case prediction can still lead to safe planning. To bridge this gap, we
study the adversarial robustness of trajectory prediction models by proposing a
new adversarial attack that perturbs normal vehicle trajectories to maximize
the prediction error. Our experiments on three models and three datasets show
that the adversarial prediction increases the prediction error by more than
150%. Our case studies show that if an adversary drives a vehicle close to the
target AV following the adversarial trajectory, the AV may make an inaccurate
prediction and even make unsafe driving decisions. We also explore possible
mitigation techniques via data augmentation and trajectory smoothing. The
implementation is open source at
https://github.com/zqzqz/AdvTrajectoryPrediction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Structural Sparsity in Neural Image Compression. (arXiv:2202.04595v4 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.04595">
<div class="article-summary-box-inner">
<span><p>Neural image compression have reached or out-performed traditional methods
(such as JPEG, BPG, WebP). However,their sophisticated network structures with
cascaded convolution layers bring heavy computational burden for practical
deployment. In this paper, we explore the structural sparsity in neural image
compression network to obtain real-time acceleration without any specialized
hardware design or algorithm. We propose a simple plug-in adaptive binary
channel masking(ABCM) to judge the importance of each convolution channel and
introduce sparsity during training. During inference, the unimportant channels
are pruned to obtain slimmer network and less computation. We implement our
method into three neural image compression networks with different entropy
models to verify its effectiveness and generalization, the experiment results
show that up to 7x computation reduction and 3x acceleration can be achieved
with negligible performance drop.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bench-Marking And Improving Arabic Automatic Image Captioning Through The Use Of Multi-Task Learning Paradigm. (arXiv:2202.05474v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.05474">
<div class="article-summary-box-inner">
<span><p>The continuous increase in the use of social media and the visual content on
the internet have accelerated the research in computer vision field in general
and the image captioning task in specific. The process of generating a caption
that best describes an image is a useful task for various applications such as
it can be used in image indexing and as a hearing aid for the visually
impaired. In recent years, the image captioning task has witnessed remarkable
advances regarding both datasets and architectures, and as a result, the
captioning quality has reached an astounding performance. However, the majority
of these advances especially in datasets are targeted for English, which left
other languages such as Arabic lagging behind. Although Arabic language, being
spoken by more than 450 million people and being the most growing language on
the internet, lacks the fundamental pillars it needs to advance its image
captioning research, such as benchmarks or unified datasets. This works is an
attempt to expedite the synergy in this task by providing unified datasets and
benchmarks, while also exploring methods and techniques that could enhance the
performance of Arabic image captioning. The use of multi-task learning is
explored, alongside exploring various word representations and different
features. The results showed that the use of multi-task learning and
pre-trained word embeddings noticeably enhanced the quality of image
captioning, however the presented results shows that Arabic captioning still
lags behind when compared to the English language. The used dataset and code
are available at this link.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Deterministic Translation for Unsupervised Domain Adaptation. (arXiv:2202.07778v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.07778">
<div class="article-summary-box-inner">
<span><p>In this work we challenge the common approach of using a one-to-one mapping
('translation') between the source and target domains in unsupervised domain
adaptation (UDA). Instead, we rely on stochastic translation to capture
inherent translation ambiguities. This allows us to (i) train more accurate
target networks by generating multiple outputs conditioned on the same source
image, leveraging both accurate translation and data augmentation for
appearance variability, (ii) impute robust pseudo-labels for the target data by
averaging the predictions of a source network on multiple translated versions
of a single target image and (iii) train and ensemble diverse networks in the
target domain by modulating the degree of stochasticity in the translations. We
report improvements over strong recent baselines, leading to state-of-the-art
UDA results on two challenging semantic segmentation benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TableFormer: Table Structure Understanding with Transformers. (arXiv:2203.01017v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.01017">
<div class="article-summary-box-inner">
<span><p>Tables organize valuable content in a concise and compact representation.
This content is extremely valuable for systems such as search engines,
Knowledge Graph's, etc, since they enhance their predictive capabilities.
Unfortunately, tables come in a large variety of shapes and sizes. Furthermore,
they can have complex column/row-header configurations, multiline rows,
different variety of separation lines, missing entries, etc. As such, the
correct identification of the table-structure from an image is a non-trivial
task. In this paper, we present a new table-structure identification model. The
latter improves the latest end-to-end deep learning model (i.e.
encoder-dual-decoder from PubTabNet) in two significant ways. First, we
introduce a new object detection decoder for table-cells. In this way, we can
obtain the content of the table-cells from programmatic PDF's directly from the
PDF source and avoid the training of the custom OCR decoders. This
architectural change leads to more accurate table-content extraction and allows
us to tackle non-english tables. Second, we replace the LSTM decoders with
transformer based decoders. This upgrade improves significantly the previous
state-of-the-art tree-editing-distance-score (TEDS) from 91% to 98.5% on simple
tables and from 88.7% to 95% on complex tables.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Supervised Temporal Action Localization via Representative Snippet Knowledge Propagation. (arXiv:2203.02925v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.02925">
<div class="article-summary-box-inner">
<span><p>Weakly supervised temporal action localization aims to localize temporal
boundaries of actions and simultaneously identify their categories with only
video-level category labels. Many existing methods seek to generate pseudo
labels for bridging the discrepancy between classification and localization,
but usually only make use of limited contextual information for pseudo label
generation. To alleviate this problem, we propose a representative snippet
summarization and propagation framework. Our method seeks to mine the
representative snippets in each video for propagating information between video
snippets to generate better pseudo labels. For each video, its own
representative snippets and the representative snippets from a memory bank are
propagated to update the input features in an intra- and inter-video manner.
The pseudo labels are generated from the temporal class activation maps of the
updated features to rectify the predictions of the main branch. Our method
obtains superior performance in comparison to the existing methods on two
benchmarks, THUMOS14 and ActivityNet1.3, achieving gains as high as 1.2% in
terms of average mAP on THUMOS14.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differentially Private Federated Learning with Local Regularization and Sparsification. (arXiv:2203.03106v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03106">
<div class="article-summary-box-inner">
<span><p>User-level differential privacy (DP) provides certifiable privacy guarantees
to the information that is specific to any user's data in federated learning.
Existing methods that ensure user-level DP come at the cost of severe accuracy
decrease. In this paper, we study the cause of model performance degradation in
federated learning under user-level DP guarantee. We find the key to solving
this issue is to naturally restrict the norm of local updates before executing
operations that guarantee DP. To this end, we propose two techniques, Bounded
Local Update Regularization and Local Update Sparsification, to increase model
quality without sacrificing privacy. We provide theoretical analysis on the
convergence of our framework and give rigorous privacy guarantees. Extensive
experiments show that our framework significantly improves the privacy-utility
trade-off over the state-of-the-arts for federated learning with user-level DP
guarantee.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Biometric recognition: why not massively adopted yet?. (arXiv:2203.03719v2 [cs.CY] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.03719">
<div class="article-summary-box-inner">
<span><p>Although there has been a dramatically reduction on the prices of capturing
devices and an increase on computing power in the last decade, it seems that
biometric systems are still far from massive adoption for civilian
applications. This paper deals with the causes of this phenomenon, as well as
some misconceptions regarding biometric identification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards performant and reliable undersampled MR reconstruction via diffusion model sampling. (arXiv:2203.04292v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04292">
<div class="article-summary-box-inner">
<span><p>Magnetic Resonance (MR) image reconstruction from under-sampled acquisition
promises faster scanning time. To this end, current State-of-The-Art (SoTA)
approaches leverage deep neural networks and supervised training to learn a
recovery model. While these approaches achieve impressive performances, the
learned model can be fragile on unseen degradation, e.g. when given a different
acceleration factor. These methods are also generally deterministic and provide
a single solution to an ill-posed problem; as such, it can be difficult for
practitioners to understand the reliability of the reconstruction. We introduce
DiffuseRecon, a novel diffusion model-based MR reconstruction method.
DiffuseRecon guides the generation process based on the observed signals and a
pre-trained diffusion model, and does not require additional training on
specific acceleration factors. DiffuseRecon is stochastic in nature and
generates results from a distribution of fully-sampled MR images; as such, it
allows us to explicitly visualize different potential reconstruction solutions.
Lastly, DiffuseRecon proposes an accelerated, coarse-to-fine Monte-Carlo
sampling scheme to approximate the most likely reconstruction candidate. The
proposed DiffuseRecon achieves SoTA performances reconstructing from raw
acquisition signals in fastMRI and SKM-TEA. Code will be open-sourced at
www.github.com/cpeng93/DiffuseRecon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Source-free Domain Adaptation for Multi-site and Lifespan Brain Skull Stripping. (arXiv:2203.04299v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04299">
<div class="article-summary-box-inner">
<span><p>Skull stripping is a crucial prerequisite step in the analysis of brain
magnetic resonance (MR) images. Although many excellent works or tools have
been proposed, they suffer from low generalization capability. For instance,
the model trained on a dataset with specific imaging parameters (source domain)
cannot be well applied to other datasets with different imaging parameters
(target domain). Especially, for the lifespan datasets, the model trained on an
adult dataset is not applicable to an infant dataset due to the large domain
difference. To address this issue, numerous domain adaptation (DA) methods have
been proposed to align the extracted features between the source and target
domains, requiring concurrent access to the input images of both domains.
Unfortunately, it is problematic to share the images due to privacy. In this
paper, we design a source-free domain adaptation framework (SDAF) for
multi-site and lifespan skull stripping that can accomplish domain adaptation
without access to source domain images. Our method only needs to share the
source labels as shape dictionaries and the weights trained on the source data,
without disclosing private information from source domain subjects. To deal
with the domain shift between multi-site lifespan datasets, we take advantage
of the brain shape prior which is invariant to imaging parameters and ages.
Experiments demonstrate that our framework can significantly outperform the
state-of-the-art methods on multi-site lifespan datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structure-Aware Flow Generation for Human Body Reshaping. (arXiv:2203.04670v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04670">
<div class="article-summary-box-inner">
<span><p>Body reshaping is an important procedure in portrait photo retouching. Due to
the complicated structure and multifarious appearance of human bodies, existing
methods either fall back on the 3D domain via body morphable model or resort to
keypoint-based image deformation, leading to inefficiency and unsatisfied
visual quality. In this paper, we address these limitations by formulating an
end-to-end flow generation architecture under the guidance of body structural
priors, including skeletons and Part Affinity Fields, and achieve
unprecedentedly controllable performance under arbitrary poses and garments. A
compositional attention mechanism is introduced for capturing both visual
perceptual correlations and structural associations of the human body to
reinforce the manipulation consistency among related parts. For a comprehensive
evaluation, we construct the first large-scale body reshaping dataset, namely
BR-5K, which contains 5,000 portrait photos as well as professionally retouched
targets. Extensive experiments demonstrate that our approach significantly
outperforms existing state-of-the-art methods in terms of visual performance,
controllability, and efficiency. The dataset is available at our website:
https://github.com/JianqiangRen/FlowBasedBodyReshaping.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unified Transformer Framework for Group-based Segmentation: Co-Segmentation, Co-Saliency Detection and Video Salient Object Detection. (arXiv:2203.04708v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.04708">
<div class="article-summary-box-inner">
<span><p>Humans tend to mine objects by learning from a group of images or several
frames of video since we live in a dynamic world. In the computer vision area,
many researches focus on co-segmentation (CoS), co-saliency detection (CoSD)
and video salient object detection (VSOD) to discover the co-occurrent objects.
However, previous approaches design different networks on these similar tasks
separately, and they are difficult to apply to each other, which lowers the
upper bound of the transferability of deep learning frameworks. Besides, they
fail to take full advantage of the cues among inter- and intra-feature within a
group of images. In this paper, we introduce a unified framework to tackle
these issues, term as UFO (Unified Framework for Co-Object Segmentation).
Specifically, we first introduce a transformer block, which views the image
feature as a patch token and then captures their long-range dependencies
through the self-attention mechanism. This can help the network to excavate the
patch structured similarities among the relevant objects. Furthermore, we
propose an intra-MLP learning module to produce self-mask to enhance the
network to avoid partial activation. Extensive experiments on four CoS
benchmarks (PASCAL, iCoseg, Internet and MSRC), three CoSD benchmarks
(Cosal2015, CoSOD3k, and CocA) and four VSOD benchmarks (DAVIS16, FBMS, ViSal
and SegV2) show that our method outperforms other state-of-the-arts on three
different tasks in both accuracy and speed by using the same network
architecture , which can reach 140 FPS in real-time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Boundary Learning for Point Cloud Segmentation. (arXiv:2203.05272v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05272">
<div class="article-summary-box-inner">
<span><p>Point cloud segmentation is fundamental in understanding 3D environments.
However, current 3D point cloud segmentation methods usually perform poorly on
scene boundaries, which degenerates the overall segmentation performance. In
this paper, we focus on the segmentation of scene boundaries. Accordingly, we
first explore metrics to evaluate the segmentation performance on scene
boundaries. To address the unsatisfactory performance on boundaries, we then
propose a novel contrastive boundary learning (CBL) framework for point cloud
segmentation. Specifically, the proposed CBL enhances feature discrimination
between points across boundaries by contrasting their representations with the
assistance of scene contexts at multiple scales. By applying CBL on three
different baseline methods, we experimentally show that CBL consistently
improves different baselines and assists them to achieve compelling performance
on boundaries, as well as the overall performance, eg in mIoU. The experimental
results demonstrate the effectiveness of our method and the importance of
boundaries for 3D point cloud segmentation. Code and model will be made
publicly available at https://github.com/LiyaoTang/contrastBoundary.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BEAT: A Large-Scale Semantic and Emotional Multi-Modal Dataset for Conversational Gestures Synthesis. (arXiv:2203.05297v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05297">
<div class="article-summary-box-inner">
<span><p>Achieving realistic, vivid, and human-like synthesized conversational
gestures conditioned on multi-modal data is still an unsolved problem, due to
the lack of available datasets, models and standard evaluation metrics. To
address this, we build Body-Expression-Audio-Text dataset, BEAT, which has i)
76 hours, high-quality, multi-modal data captured from 30 speakers talking with
eight different emotions and in four different languages, ii) 32 millions
frame-level emotion and semantic relevance annotations.Our statistical analysis
on BEAT demonstrates the correlation of conversational gestures with facial
expressions, emotions, and semantics, in addition to the known correlation with
audio, text, and speaker identity. Qualitative and quantitative experiments
demonstrate metrics' validness, ground truth data quality, and baseline's
state-of-the-art performance. To the best of our knowledge, BEAT is the largest
motion capture dataset for investigating the human gestures, which may
contribute to a number of different research fields including controllable
gesture synthesis, cross-modality analysis, emotional gesture recognition. The
data, code and model will be released for research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StyleBabel: Artistic Style Tagging and Captioning. (arXiv:2203.05321v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05321">
<div class="article-summary-box-inner">
<span><p>We present StyleBabel, a unique open access dataset of natural language
captions and free-form tags describing the artistic style of over 135K digital
artworks, collected via a novel participatory method from experts studying at
specialist art and design schools. StyleBabel was collected via an iterative
method, inspired by `Grounded Theory': a qualitative approach that enables
annotation while co-evolving a shared language for fine-grained artistic style
attribute description. We demonstrate several downstream tasks for StyleBabel,
adapting the recent ALADIN architecture for fine-grained style similarity, to
train cross-modal embeddings for: 1) free-form tag generation; 2) natural
language description of artistic style; 3) fine-grained text search of style.
To do so, we extend ALADIN with recent advances in Visual Transformer (ViT) and
cross-modal representation learning, achieving a state of the art accuracy in
fine-grained style retrieval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TrueType Transformer: Character and Font Style Recognition in Outline Format. (arXiv:2203.05338v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05338">
<div class="article-summary-box-inner">
<span><p>We propose TrueType Transformer (T3), which can perform character and font
style recognition in an outline format. The outline format, such as TrueType,
represents each character as a sequence of control points of stroke contours
and is frequently used in born-digital documents. T3 is organized by a deep
neural network, so-called Transformer. Transformer is originally proposed for
sequential data, such as text, and therefore appropriate for handling the
outline data. In other words, T3 directly accepts the outline data without
converting it into a bitmap image. Consequently, T3 realizes a
resolution-independent classification. Moreover, since the locations of the
control points represent the fine and local structures of the font style, T3 is
suitable for font style classification, where such structures are very
important. In this paper, we experimentally show the applicability of T3 in
character and font style recognition tasks, while observing how the individual
control points contribute to classification results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain Generalization via Shuffled Style Assembly for Face Anti-Spoofing. (arXiv:2203.05340v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05340">
<div class="article-summary-box-inner">
<span><p>With diverse presentation attacks emerging continually, generalizable face
anti-spoofing (FAS) has drawn growing attention. Most existing methods
implement domain generalization (DG) on the complete representations. However,
different image statistics may have unique properties for the FAS tasks. In
this work, we separate the complete representation into content and style ones.
A novel Shuffled Style Assembly Network (SSAN) is proposed to extract and
reassemble different content and style features for a stylized feature space.
Then, to obtain a generalized representation, a contrastive learning strategy
is developed to emphasize liveness-related style information while suppress the
domain-specific one. Finally, the representations of the correct assemblies are
used to distinguish between living and spoofing during the inferring. On the
other hand, despite the decent performance, there still exists a gap between
academia and industry, due to the difference in data quantity and distribution.
Thus, a new large-scale benchmark for FAS is built up to further evaluate the
performance of algorithms in reality. Both qualitative and quantitative results
on existing and proposed benchmarks demonstrate the effectiveness of our
methods. The codes will be available at https://github.com/wangzhuo2019/SSAN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AGCN: Augmented Graph Convolutional Network for Lifelong Multi-label Image Recognition. (arXiv:2203.05534v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2203.05534">
<div class="article-summary-box-inner">
<span><p>The Lifelong Multi-Label (LML) image recognition builds an online
class-incremental classifier in a sequential multi-label image recognition data
stream. The key challenges of LML image recognition are the construction of
label relationships on Partial Labels of training data and the Catastrophic
Forgetting on old classes, resulting in poor generalization. To solve the
problems, the study proposes an Augmented Graph Convolutional Network (AGCN)
model that can construct the label relationships across the sequential
recognition tasks and sustain the catastrophic forgetting. First, we build an
Augmented Correlation Matrix (ACM) across all seen classes, where the
intra-task relationships derive from the hard label statistics while the
inter-task relationships leverage both hard and soft labels from data and a
constructed expert network. Then, based on the ACM, the proposed AGCN captures
label dependencies with dynamic augmented structure and yields effective class
representations. Last, to suppress the forgetting of label dependencies across
old tasks, we propose a relationship-preserving loss as a constraint to the
construction of label relationships. The proposed method is evaluated using two
multi-label image benchmarks and the experimental results show that the
proposed method is effective for LML image recognition and can build convincing
correlation across tasks even if the labels of previous tasks are missing. Our
code is available at https://github.com/Kaile-Du/AGCN.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-03-14 23:08:39.612796784 UTC">2022-03-14 23:08:39 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>