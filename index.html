<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-10-19T01:30:00Z">10-19</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Inconsistent Few-Shot Relation Classification via Cross-Attentional Prototype Networks with Contrastive Learning. (arXiv:2110.08254v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08254">
<div class="article-summary-box-inner">
<span><p>Standard few-shot relation classification (RC) is designed to learn a robust
classifier with only few labeled data for each class. However, previous works
rarely investigate the effects of a different number of classes (i.e., $N$-way)
and number of labeled data per class (i.e., $K$-shot) during training vs.
testing. In this work, we define a new task, \textit{inconsistent few-shot RC},
where the model needs to handle the inconsistency of $N$ and $K$ between
training and testing. To address this new task, we propose Prototype
Network-based cross-attention contrastive learning (ProtoCACL) to capture the
rich mutual interactions between the support set and query set. Experimental
results demonstrate that our ProtoCACL can outperform the state-of-the-art
baseline model under both inconsistent $K$ and inconsistent $N$ settings, owing
to its more robust and discriminate representations. Moreover, we identify that
in the inconsistent few-shot learning setting, models can achieve better
performance with \textit{less data} than the standard few-shot setting with
carefully-selected $N$ and $K$. In the end of the paper, we provide further
analyses and suggestions to systematically guide the selection of $N$ and $K$
under different scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Multimodal to Unimodal Attention in Transformers using Knowledge Distillation. (arXiv:2110.08270v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08270">
<div class="article-summary-box-inner">
<span><p>Multimodal Deep Learning has garnered much interest, and transformers have
triggered novel approaches, thanks to the cross-attention mechanism. Here we
propose an approach to deal with two key existing challenges: the high
computational resource demanded and the issue of missing modalities. We
introduce for the first time the concept of knowledge distillation in
transformers to use only one modality at inference time. We report a full study
analyzing multiple student-teacher configurations, levels at which distillation
is applied, and different methodologies. With the best configuration, we
improved the state-of-the-art accuracy by 3%, we reduced the number of
parameters by 2.5 times and the inference time by 22%. Such
performance-computation tradeoff can be exploited in many applications and we
aim at opening a new research area where the deployment of complex models with
limited resources is demanded.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting coherence of language models. (arXiv:2110.08294v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08294">
<div class="article-summary-box-inner">
<span><p>Naturality of long-term information structure -- coherence -- remains a
challenge in language generation. Large language models have insufficiently
learned such structure, as their long-form generations differ from natural text
in measures of coherence. To alleviate this divergence, we propose coherence
boosting, an inference procedure that increases the effect of distant context
on next-token prediction. We show the benefits of coherence boosting with
pretrained models by distributional analyses of generated ordinary text and
dialog responses. We also find that coherence boosting with state-of-the-art
models for various zero-shot NLP tasks yields performance gains with no
additional training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Aspect-Oriented Summarization through Query-Focused Extraction. (arXiv:2110.08296v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08296">
<div class="article-summary-box-inner">
<span><p>A reader interested in a particular topic might be interested in summarizing
documents on that subject with a particular focus, rather than simply seeing
generic summaries produced by most summarization systems. While query-focused
summarization has been explored in prior work, this is often approached from
the standpoint of document-specific questions or on synthetic data. Real users'
needs often fall more closely into aspects, broad topics in a dataset the user
is interested in rather than specific queries. In this paper, we collect a
dataset of realistic aspect-oriented test cases, AspectNews, which covers
different subtopics about articles in news sub-domains. We then investigate how
query-focused methods, for which we can construct synthetic data, can handle
this aspect-oriented setting: we benchmark extractive query-focused training
schemes, and propose a contrastive augmentation approach to train the model. We
evaluate on two aspect-oriented datasets and find this approach yields (a)
focused summaries, better than those from a generic summarization system, which
go beyond simple keyword matching; (b) a system sensitive to the choice of
keywords.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When Combating Hype, Proceed with Caution. (arXiv:2110.08300v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08300">
<div class="article-summary-box-inner">
<span><p>In an effort to avoid reinforcing widespread hype about the capabilities of
state-of-the-art language technology, researchers have developed practices in
framing and citation that serve to deemphasize the field's successes. Though
well-meaning, these practices often yield misleading or even false claims about
the limits of our best technology. This is a problem, and it may be more
serious than it looks: It limits our ability to mitigate short-term harms from
NLP deployments and it limits our ability to prepare for the potentially
enormous impacts of more distant future advances. This paper urges researchers
to be careful about these claims and suggests some research directions and
communication strategies that will make it easier to avoid or rebut them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Learning the Transformer Kernel. (arXiv:2110.08323v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08323">
<div class="article-summary-box-inner">
<span><p>In this work we introduce KERNELIZED TRANSFORMER, a generic, scalable, data
driven framework for learning the kernel function in Transformers. Our
framework approximates the Transformer kernel as a dot product between spectral
feature maps and learns the kernel by learning the spectral distribution. This
not only helps in learning a generic kernel end-to-end, but also reduces the
time and space complexity of Transformers from quadratic to linear. We show
that KERNELIZED TRANSFORMERS achieve performance comparable to existing
efficient Transformer architectures, both in terms of accuracy as well as
computational efficiency. Our study also demonstrates that the choice of the
kernel has a substantial impact on performance, and kernel learning variants
are competitive alternatives to fixed kernel Transformers, both in long as well
as short sequence tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Control Prefixes for Text Generation. (arXiv:2110.08329v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08329">
<div class="article-summary-box-inner">
<span><p>Prompt learning methods adapt pre-trained language models to downstream
applications by using a task-specific prompt together with the input. Most of
the current work on prompt learning in text generation relies on a shared
dataset-level prompt for all examples in the dataset. We extend this approach
and propose a dynamic method, Control Prefixes, which allows for the inclusion
of conditional input-dependent information in each prompt. Control Prefixes is
at the intersection of prompt learning and controlled generation, empowering
the model to have finer-grained control during text generation. The method
incorporates attribute-level learnable representations into different layers of
a pre-trained transformer, allowing for the generated text to be guided in a
particular direction. We provide a systematic evaluation of the technique and
apply it to five datasets from the GEM benchmark for natural language
generation (NLG). We present state-of-the-art results on several data-to-text
datasets, including WebNLG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Transparent Interactive Semantic Parsing via Step-by-Step Correction. (arXiv:2110.08345v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08345">
<div class="article-summary-box-inner">
<span><p>Existing studies on semantic parsing focus primarily on mapping a
natural-language utterance to a corresponding logical form in one turn.
However, because natural language can contain a great deal of ambiguity and
variability, this is a difficult challenge. In this work, we investigate an
interactive semantic parsing framework that explains the predicted logical form
step by step in natural language and enables the user to make corrections
through natural-language feedback for individual steps. We focus on question
answering over knowledge bases (KBQA) as an instantiation of our framework,
aiming to increase the transparency of the parsing process and help the user
appropriately trust the final answer. To do so, we construct INSPIRED, a
crowdsourced dialogue dataset derived from the ComplexWebQuestions dataset. Our
experiments show that the interactive framework with human feedback has the
potential to greatly improve overall parse accuracy. Furthermore, we develop a
pipeline for dialogue simulation to evaluate our framework w.r.t. a variety of
state-of-the-art KBQA models without involving further crowdsourcing effort.
The results demonstrate that our interactive semantic parsing framework
promises to be effective across such models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Omni-sparsity DNN: Fast Sparsity Optimization for On-Device Streaming E2E ASR via Supernet. (arXiv:2110.08352v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08352">
<div class="article-summary-box-inner">
<span><p>From wearables to powerful smart devices, modern automatic speech recognition
(ASR) models run on a variety of edge devices with different computational
budgets. To navigate the Pareto front of model accuracy vs model size,
researchers are trapped in a dilemma of optimizing model accuracy by training
and fine-tuning models for each individual edge device while keeping the
training GPU-hours tractable. In this paper, we propose Omni-sparsity DNN,
where a single neural network can be pruned to generate optimized model for a
large range of model sizes. We develop training strategies for Omni-sparsity
DNN that allows it to find models along the Pareto front of word-error-rate
(WER) vs model size while keeping the training GPU-hours to no more than that
of training one singular model. We demonstrate the Omni-sparsity DNN with
streaming E2E ASR models. Our results show great saving on training time and
resources with similar or better accuracy on LibriSpeech compared to
individually pruned sparse models: 2%-6.6% better WER on Test-other.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning with Noisy Labels by Targeted Relabeling. (arXiv:2110.08355v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08355">
<div class="article-summary-box-inner">
<span><p>Crowdsourcing platforms are often used to collect datasets for training deep
neural networks, despite higher levels of inaccurate labeling compared to
expert labeling. There are two common strategies to manage the impact of this
noise, the first involves aggregating redundant annotations, but comes at the
expense of labeling substantially fewer examples. Secondly, prior works have
also considered using the entire annotation budget to label as many examples as
possible and subsequently apply denoising algorithms to implicitly clean up the
dataset. We propose an approach which instead reserves a fraction of
annotations to explicitly relabel highly probable labeling errors. In
particular, we allocate a large portion of the labeling budget to form an
initial dataset used to train a model. This model is then used to identify
specific examples that appear most likely to be incorrect, which we spend the
remaining budget to relabel. Experiments across three model variations and four
natural language processing tasks show our approach outperforms both label
aggregation and advanced denoising methods designed to handle noisy labels when
allocated the same annotation budget.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training Dynamics for Text Summarization Models. (arXiv:2110.08370v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08370">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (e.g. BART) have shown impressive results when
fine-tuned on large summarization datasets. However, little is understood about
this fine-tuning process, including what knowledge is retained from
pre-training models or how content selection and generation strategies are
learnt across iterations. In this work, we analyze the training dynamics for
generation models, focusing on news summarization. Across different datasets
(CNN/DM, XSum, MediaSum) and summary properties, such as abstractiveness and
hallucination, we study what the model learns at different stages of its
fine-tuning process. We find that properties such as copy behavior are learnt
earlier in the training process and these observations are robust across
domains. On the other hand, factual errors, such as hallucination of
unsupported facts, are learnt in the later stages, and this behavior is more
varied across domains. Based on these observations, we explore complementary
approaches for modifying training: first, disregarding high-loss tokens that
are challenging to learn and second, disregarding low-loss tokens that are
learnt very quickly. This simple training modification allows us to configure
our model to achieve different goals, such as improving factuality or improving
abstractiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On The Ingredients of an Effective Zero-shot Semantic Parser. (arXiv:2110.08381v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08381">
<div class="article-summary-box-inner">
<span><p>Semantic parsers map natural language utterances into meaning representations
(e.g., programs). Such models are typically bottlenecked by the paucity of
training data due to the required laborious annotation efforts. Recent studies
have performed zero-shot learning by synthesizing training examples of
canonical utterances and programs from a grammar, and further paraphrasing
these utterances to improve linguistic diversity. However, such synthetic
examples cannot fully capture patterns in real data. In this paper we analyze
zero-shot parsers through the lenses of the language and logical gaps (Herzig
and Berant, 2019), which quantify the discrepancy of language and programmatic
patterns between the canonical examples and real-world user-issued ones. We
propose bridging these gaps using improved grammars, stronger paraphrasers, and
efficient learning methods using canonical examples that most likely reflect
real user intents. Our model achieves strong performance on two semantic
parsing benchmarks (Scholar, Geo) with zero labeled data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training Conversational Agents with Generative Conversational Networks. (arXiv:2110.08383v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08383">
<div class="article-summary-box-inner">
<span><p>Rich, open-domain textual data available on the web resulted in great
advancements for language processing. However, while that data may be suitable
for language processing tasks, they are mostly non-conversational, lacking many
phenomena that appear in human interactions and this is one of the reasons why
we still have many unsolved challenges in conversational AI. In this work, we
attempt to address this by using Generative Conversational Networks to
automatically generate data and train social conversational agents. We evaluate
our approach on TopicalChat with automatic metrics and human evaluators,
showing that with 10% of seed data it performs close to the baseline that uses
100% of the data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generated Knowledge Prompting for Commonsense Reasoning. (arXiv:2110.08387v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08387">
<div class="article-summary-box-inner">
<span><p>Despite their ability to capture large amount of knowledge during
pretraining, large-scale language models often benefit from incorporating
external knowledge bases, especially on commonsense reasoning tasks. This
motivates us to explore how we can best leverage knowledge elicited from
language models themselves. We propose generating knowledge statements directly
from a language model with a generic prompt format, then selecting the
knowledge which maximizes prediction probability. Despite its simplicity, this
approach improves performance of both off-the-shelf and finetuned language
models on four commonsense reasoning tasks, improving the state-of-the-art on
numerical commonsense (NumerSense), general commonsense (CommonsenseQA 2.0),
and scientific commonsense (QASC) benchmarks. Notably, we find that a model's
predictions can improve when using its own generated knowledge, demonstrating
the importance of symbolic knowledge representation in neural reasoning
processes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing as Quantifying the Inductive Bias of Pre-trained Representations. (arXiv:2110.08388v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08388">
<div class="article-summary-box-inner">
<span><p>Pre-trained contextual representations have led to dramatic performance
improvements on a range of downstream tasks. This has motivated researchers to
quantify and understand the linguistic information encoded in them. In general,
this is done by probing, which consists of training a supervised model to
predict a linguistic property from said representations. Unfortunately, this
definition of probing has been subject to extensive criticism, and can lead to
paradoxical or counter-intuitive results. In this work, we present a novel
framework for probing where the goal is to evaluate the inductive bias of
representations for a particular task, and provide a practical avenue to do
this using Bayesian inference. We apply our framework to a series of token-,
arc-, and sentence-level tasks. Our results suggest that our framework solves
problems of previous approaches and that fastText can offer a better inductive
bias than BERT in certain situations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DS-TOD: Efficient Domain Specialization for Task Oriented Dialog. (arXiv:2110.08395v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08395">
<div class="article-summary-box-inner">
<span><p>Recent work has shown that self-supervised dialog-specific pretraining on
large conversational datasets yields substantial gains over traditional
language modeling (LM) pretraining in downstream task-oriented dialog (TOD).
These approaches, however, exploit general dialogic corpora (e.g., Reddit) and
thus presumably fail to reliably embed domain-specific knowledge useful for
concrete downstream TOD domains. In this work, we investigate the effects of
domain specialization of pretrained language models (PLMs) for task-oriented
dialog. Within our DS-TOD framework, we first automatically extract salient
domain-specific terms, and then use them to construct DomainCC and DomainReddit
-- resources that we leverage for domain-specific pretraining, based on (i)
masked language modeling (MLM) and (ii) response selection (RS) objectives,
respectively. We further propose a resource-efficient and modular domain
specialization by means of domain adapters -- additional parameter-light layers
in which we encode the domain knowledge. Our experiments with two prominent TOD
tasks -- dialog state tracking (DST) and response retrieval (RR) --
encompassing five domains from the MultiWOZ TOD benchmark demonstrate the
effectiveness of our domain specialization approach. Moreover, we show that the
light-weight adapter-based specialization (1) performs comparably to full
fine-tuning in single-domain setups and (2) is particularly suitable for
multi-domain specialization, in which, besides advantageous computational
footprint, it can offer better downstream performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating the Faithfulness of Importance Measures in NLP by Recursively Masking Allegedly Important Tokens and Retraining. (arXiv:2110.08412v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08412">
<div class="article-summary-box-inner">
<span><p>To explain NLP models, many methods inform which inputs tokens are important
for a prediction. However, an open question is if these methods accurately
reflect the model's logic, a property often called faithfulness. In this work,
we adapt and improve a recently proposed faithfulness benchmark from computer
vision called ROAR (RemOve And Retrain), by Hooker et al. (2019).
</p>
<p>We improve ROAR by recursively removing dataset redundancies, which otherwise
interfere with ROAR. We adapt and apply ROAR, to popular NLP importance
measures, namely attention, gradient, and integrated gradients. Additionally,
we use mutual information as an additional baseline. Evaluation is done on a
suite of classification tasks often used in the faithfulness of attention
literature. Finally, we propose a scalar faithfulness metric, which makes it
easy to compare results across papers.
</p>
<p>We find that, importance measures considered to be unfaithful for computer
vision tasks perform favorably for NLP tasks, the faithfulness of an importance
measure is task-dependent, and the computational overhead of integrated
gradient is rarely justified.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Invariant Language Modeling. (arXiv:2110.08413v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08413">
<div class="article-summary-box-inner">
<span><p>Modern pretrained language models are critical components of NLP pipelines.
Yet, they suffer from spurious correlations, poor out-of-domain generalization,
and biases. Inspired by recent progress in causal machine learning, in
particular the invariant risk minimization (IRM) paradigm, we propose invariant
language modeling, a framework for learning invariant representations that
generalize better across multiple environments. In particular, we adapt a
game-theoretic implementation of IRM (IRM-games) to language models, where the
invariance emerges from a specific training schedule in which all the
environments compete to optimize their own environment-specific loss by
updating subsets of the model in a round-robin fashion. In a series of
controlled experiments, we demonstrate the ability of our method to (i) remove
structured noise, (ii) ignore specific spurious correlations without affecting
global performance, and (iii) achieve better out-of-domain generalization.
These benefits come with a negligible computational overhead compared to
standard training, do not require changing the local loss, and can be applied
to any language model architecture. We believe this framework is promising to
help mitigate spurious correlations and biases in language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual unsupervised sequence segmentation transfers to extremely low-resource languages. (arXiv:2110.08415v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08415">
<div class="article-summary-box-inner">
<span><p>We show that unsupervised sequence-segmentation performance can be
transferred to extremely low-resource languages by pre-training a Masked
Segmental Language Model (Downey et al., 2021) multilingually. Further, we show
that this transfer can be achieved by training over a collection of
low-resource languages that are typologically similar (but phylogenetically
unrelated) to the target language. In our experiments, we transfer from a
collection of 10 Indigenous American languages (AmericasNLP, Mager et al.,
2021) to K'iche', a Mayan language. We compare our model to a monolingual
baseline, and show that the multilingual pre-trained approach yields much more
consistent segmentation quality across target dataset sizes, including a
zero-shot performance of 20.6 F1, and exceeds the monolingual performance in
9/10 experimental settings. These results have promising implications for
low-resource NLP pipelines involving human-like linguistic units, such as the
sparse transcription framework proposed by Bird (2020).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open Domain Question Answering over Virtual Documents: A Unified Approach for Data and Text. (arXiv:2110.08417v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08417">
<div class="article-summary-box-inner">
<span><p>Due to its potential for a universal interface over both data and text,
data-to-text generation is becoming increasingly popular recently. However, few
previous work has focused on its application to downstream tasks, e.g. using
the converted data for grounding or reasoning. In this work, we aim to bridge
this gap and use the data-to-text method as a means for encoding structured
knowledge for knowledge-intensive applications, i.e. open-domain question
answering (QA). Specifically, we propose a verbalizer-retriever-reader
framework for open-domain QA over data and text where verbalized tables from
Wikipedia and triples from Wikidata are used as augmented knowledge sources. We
show that our Unified Data and Text QA, UDT-QA, can effectively benefit from
the expanded knowledge index, leading to large gains over text-only baselines.
Notably, our approach sets the single-model state-of-the-art on Natural
Questions. Furthermore, our analyses indicate that verbalized knowledge is
preferred for answer reasoning for both adapted and hot-swap settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What do Compressed Large Language Models Forget? Robustness Challenges in Model Compression. (arXiv:2110.08419v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08419">
<div class="article-summary-box-inner">
<span><p>Recent works have focused on compressing pre-trained language models (PLMs)
like BERT where the major focus has been to improve the compressed model
performance for downstream tasks. However, there has been no study in analyzing
the impact of compression on the generalizability and robustness of these
models. Towards this end, we study two popular model compression techniques
including knowledge distillation and pruning and show that compressed models
are significantly less robust than their PLM counterparts on adversarial test
sets although they obtain similar performance on in-distribution development
sets for a task. Further analysis indicates that the compressed models overfit
on the easy samples and generalize poorly on the hard ones. We further leverage
this observation to develop a regularization strategy for model compression
based on sample uncertainty. Experimental results on several natural language
understanding tasks demonstrate our mitigation framework to improve both the
adversarial generalization as well as in-distribution task performance of the
compressed models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Information-Theoretic Measures of Dataset Difficulty. (arXiv:2110.08420v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08420">
<div class="article-summary-box-inner">
<span><p>Estimating the difficulty of a dataset typically involves comparing
state-of-the-art models to humans; the bigger the performance gap, the harder
the dataset is said to be. Not only is this framework informal, but it also
provides little understanding of how difficult each instance is, or what
attributes make it difficult for a given model. To address these problems, we
propose an information-theoretic perspective, framing dataset difficulty as the
absence of $\textit{usable information}$. Measuring usable information is as
easy as measuring performance, but has certain theoretical advantages. While
the latter only allows us to compare different models w.r.t the same dataset,
the former also allows us to compare different datasets w.r.t the same model.
We then introduce $\textit{pointwise}$ $\mathcal{V}-$$\textit{information}$
(PVI) for measuring the difficulty of individual instances, where instances
with higher PVI are easier for model $\mathcal{V}$. By manipulating the input
before measuring usable information, we can understand $\textit{why}$ a dataset
is easy or difficult for a given model, which we use to discover annotation
artefacts in widely-used benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EncT5: Fine-tuning T5 Encoder for Non-autoregressive Tasks. (arXiv:2110.08426v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08426">
<div class="article-summary-box-inner">
<span><p>Encoder-decoder transformer architectures have become popular recently with
the advent of T5 models. It is also more favorable over architectures like BERT
for pre-training on language model task when it comes to large scale models
which could take months to train given it's generality. While being able to
generalize to more tasks, it is not evident if the proposed encoder-decoder
architecture is the most efficient for fine-tuning on classification and
regression tasks given the pre-trained model. In this work, we study
fine-tuning pre-trained encoder-decoder models such as T5. Particularly, we
propose \textbf{EncT5} as a way to efficiently fine-tune pre-trained
encoder-decoder T5 models for classification and regression tasks by using the
encoder layers. Our experimental results show that \textbf{EncT5} with less
than half of the parameters of T5 performs similarly to T5 models on GLUE
benchmark. We believe our proposed approach can be easily applied to any
pre-trained encoder-decoder model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Metadata Shaping: Natural Language Annotations for the Tail. (arXiv:2110.08430v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08430">
<div class="article-summary-box-inner">
<span><p>Language models (LMs) have made remarkable progress, but still struggle to
generalize beyond the training data to rare linguistic patterns. Since rare
entities and facts are prevalent in the queries users submit to popular
applications such as search and personal assistant systems, improving the
ability of LMs to reliably capture knowledge over rare entities is a pressing
challenge studied in significant prior work. Noticing that existing approaches
primarily modify the LM architecture or introduce auxiliary objectives to
inject useful entity knowledge, we ask to what extent we could match the
quality of these architectures using a base LM architecture, and only changing
the data? We propose metadata shaping, a method in which readily available
metadata, such as entity descriptions and categorical tags, are appended to
examples based on information theoretic metrics. Intuitively, if metadata
corresponding to popular entities overlap with metadata for rare entities, the
LM may be able to better reason about the rare entities using patterns learned
from similar popular entities. On standard entity-rich tasks (TACRED, FewRel,
OpenEntity), with no changes to the LM whatsoever, metadata shaping exceeds the
BERT-baseline by up to 5.3 F1 points, and achieves or competes with
state-of-the-art results. We further show the improvements are up to 10x larger
on examples containing tail versus popular entities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Natural Language Inference Using PHL Triplet Generation. (arXiv:2110.08438v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08438">
<div class="article-summary-box-inner">
<span><p>Transformer-based models have achieved impressive performance on various
Natural Language Inference (NLI) benchmarks, when trained on respective
training datasets. However, in certain cases, training samples may not be
available or collecting them could be time-consuming and resource-intensive. In
this work, we address this challenge and present an explorative study on
unsupervised NLI, a paradigm in which no human-annotated training samples are
available. We investigate NLI under three challenging settings: PH, P, and NPH
that differ in the extent of unlabeled data available for learning. As a
solution, we propose a procedural data generation approach that leverages a set
of sentence transformations to collect PHL (Premise, Hypothesis, Label)
triplets for training NLI models, bypassing the need for human-annotated
training datasets. Comprehensive experiments show that this approach results in
accuracies of 66.75%, 65.9%, 65.39% in PH, P, NPH settings respectively,
outperforming all existing baselines. Furthermore, fine-tuning our models with
as little as ~0.1% of the training dataset (500 samples) leads to 12.2% higher
accuracy than the model trained from scratch on the same 500 instances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prix-LM: Pretraining for Multilingual Knowledge Base Construction. (arXiv:2110.08443v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08443">
<div class="article-summary-box-inner">
<span><p>Knowledge bases (KBs) contain plenty of structured world and commonsense
knowledge. As such, they often complement distributional text-based information
and facilitate various downstream tasks. Since their manual construction is
resource- and time-intensive, recent efforts have tried leveraging large
pretrained language models (PLMs) to generate additional monolingual knowledge
facts for KBs. However, such methods have not been attempted for building and
enriching multilingual KBs. Besides wider application, such multilingual KBs
can provide richer combined knowledge than monolingual (e.g., English) KBs.
Knowledge expressed in different languages may be complementary and unequally
distributed: this implies that the knowledge available in high-resource
languages can be transferred to low-resource ones. To achieve this, it is
crucial to represent multilingual knowledge in a shared/unified space. To this
end, we propose a unified framework, Prix-LM, for multilingual KB construction
and completion. We leverage two types of knowledge, monolingual triples and
cross-lingual links, extracted from existing multilingual KBs, and tune a
multilingual language encoder XLM-R via a causal language modeling objective.
Prix-LM integrates useful multilingual and KB-based factual knowledge into a
single model. Experiments on standard entity-related tasks, such as link
prediction in multiple languages, cross-lingual entity linking and bilingual
lexicon induction, demonstrate its effectiveness, with gains reported over
strong task-specialised baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Well Do You Know Your Audience? Reader-aware Question Generation. (arXiv:2110.08445v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08445">
<div class="article-summary-box-inner">
<span><p>When writing, a person may need to anticipate questions from their readers,
but different types of readers may ask very different types of questions. If
someone is writing for advice about a problem, what question will a domain
expert ask, and is this different from how a novice might react? In this paper,
we address the task of reader-aware question generation. We collect a new data
set of questions and posts from social media, augmented with background
information about the post readers. Based on predictive analysis and
descriptive differences, we find that different readers, such as experts and
novices, consistently ask different types of questions. We next develop several
text generation models that incorporate different types of reader background,
including discrete and continuous reader representations based on the readers'
prior behavior. We demonstrate that reader-aware models can perform on par or
slightly better than the text-only model in some cases, particularly in cases
where a post attracts very different questions from readers of different
groups. Our work has the potential to help writers anticipate the information
needs of different readers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Good Examples Make A Faster Learner: Simple Demonstration-based Learning for Low-resource NER. (arXiv:2110.08454v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08454">
<div class="article-summary-box-inner">
<span><p>Recent advances in prompt-based learning have shown impressive results on
few-shot text classification tasks by using cloze-style language prompts. There
have been attempts on prompt-based learning for NER which use manually designed
templates to predict entity types. However, these two-step methods may suffer
from error propagation (from entity span detection), need to prompt for all
possible text spans which is costly, and neglect the interdependency when
predicting labels for different spans in a sentence. In this paper, we present
a simple demonstration-based learning method for NER, which augments the prompt
(learning context) with a few task demonstrations. Such demonstrations help the
model learn the task better under low-resource settings and allow for span
detection and classification over all tokens jointly. Here, we explore
entity-oriented demonstration which selects an appropriate entity example per
each entity type, and instance-oriented demonstration which retrieves a similar
instance example. Through extensive experiments, we find empirically that
showing entity example per each entity type, along with its example sentence,
can improve the performance both in in-domain and cross-domain settings by 1-3
F1 score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Enhanced Pretrained Language Models: A Compreshensive Survey. (arXiv:2110.08455v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08455">
<div class="article-summary-box-inner">
<span><p>Pretrained Language Models (PLM) have established a new paradigm through
learning informative contextualized representations on large-scale text corpus.
This new paradigm has revolutionized the entire field of natural language
processing, and set the new state-of-the-art performance for a wide variety of
NLP tasks. However, though PLMs could store certain knowledge/facts from
training corpus, their knowledge awareness is still far from satisfactory. To
address this issue, integrating knowledge into PLMs have recently become a very
active research area and a variety of approaches have been developed. In this
paper, we provide a comprehensive survey of the literature on this emerging and
fast-growing field - Knowledge Enhanced Pretrained Language Models (KE-PLMs).
We introduce three taxonomies to categorize existing work. Besides, we also
survey the various NLU and NLG applications on which KE-PLM has demonstrated
superior performance over vanilla PLMs. Finally, we discuss challenges that
face KE-PLMs and also promising directions for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controllable Semantic Parsing via Retrieval Augmentation. (arXiv:2110.08458v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08458">
<div class="article-summary-box-inner">
<span><p>In practical applications of semantic parsing, we often want to rapidly
change the behavior of the parser, such as enabling it to handle queries in a
new domain, or changing its predictions on certain targeted queries. While we
can introduce new training examples exhibiting the target behavior, a mechanism
for enacting such behavior changes without expensive model re-training would be
preferable. To this end, we propose ControllAble Semantic Parser via Exemplar
Retrieval (CASPER). Given an input query, the parser retrieves related
exemplars from a retrieval index, augments them to the query, and then applies
a generative seq2seq model to produce an output parse. The exemplars act as a
control mechanism over the generic generative model: by manipulating the
retrieval index or how the augmented query is constructed, we can manipulate
the behavior of the parser. On the MTOP dataset, in addition to achieving
state-of-the-art on the standard setup, we show that CASPER can parse queries
in a new domain, adapt the prediction toward the specified patterns, or adapt
to new semantic schemas without having to further re-train the model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Short Study on Compressing Decoder-Based Language Models. (arXiv:2110.08460v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08460">
<div class="article-summary-box-inner">
<span><p>Pre-trained Language Models (PLMs) have been successful for a wide range of
natural language processing (NLP) tasks. The state-of-the-art of PLMs, however,
are extremely large to be used on edge devices. As a result, the topic of model
compression has attracted increasing attention in the NLP community. Most of
the existing works focus on compressing encoder-based models (tiny-BERT,
distilBERT, distilRoBERTa, etc), however, to the best of our knowledge, the
compression of decoder-based models (such as GPT-2) has not been investigated
much. Our paper aims to fill this gap. Specifically, we explore two directions:
1) we employ current state-of-the-art knowledge distillation techniques to
improve fine-tuning of DistilGPT-2. 2) we pre-train a compressed GPT-2 model
using layer truncation and compare it against the distillation-based method
(DistilGPT2). The training time of our compressed model is significantly less
than DistilGPT-2, but it can achieve better performance when fine-tuned on
downstream tasks. We also demonstrate the impact of data cleaning on model
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Knowledge in Multilingual Commonsense Reasoning. (arXiv:2110.08462v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08462">
<div class="article-summary-box-inner">
<span><p>Commonsense reasoning (CSR) requires the model to be equipped with general
world knowledge. While CSR is a language-agnostic process, most comprehensive
knowledge sources are in few popular languages, especially English. Thus, it
remains unclear how to effectively conduct multilingual commonsense reasoning
(XCSR) for various languages. In this work, we propose to utilize English
knowledge sources via a translate-retrieve-translate (TRT) strategy. For
multilingual commonsense questions and choices, we collect related knowledge
via translation and retrieval from the knowledge sources. The retrieved
knowledge is then translated into the target language and integrated into a
pre-trained multilingual language model via visible knowledge attention. Then
we utilize a diverse of 4 English knowledge sources to provide more
comprehensive coverage of knowledge in different formats. Extensive results on
the XCSR benchmark demonstrate that TRT with external knowledge can
significantly improve multilingual commonsense reasoning in both zero-shot and
translate-train settings, outperforming 3.3 and 3.6 points over the previous
state-of-the-art on XCSR benchmark datasets (X-CSQA and X-CODAH).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Seeking Patterns, Not just Memorizing Procedures: Contrastive Learning for Solving Math Word Problems. (arXiv:2110.08464v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08464">
<div class="article-summary-box-inner">
<span><p>Math Word Problem (MWP) solving needs to discover the quantitative
relationships over natural language narratives. Recent work shows that existing
models memorize procedures from context and rely on shallow heuristics to solve
MWPs. In this paper, we look at this issue and argue that the cause is a lack
of overall understanding of MWP patterns. We first investigate how a neural
network understands patterns only from semantics, and observe that, if the
prototype equations are the same, most problems get closer representations and
those representations apart from them or close to other prototypes tend to
produce wrong solutions. Inspired by it, we propose a contrastive learning
approach, where the neural network perceives the divergence of patterns. We
collect contrastive examples by converting the prototype equation into a tree
and seeking similar tree structures. The solving model is trained with an
auxiliary objective on the collected examples, resulting in the representations
of problems with similar prototypes being pulled closer. We conduct experiments
on the Chinese dataset Math23k and the English dataset MathQA. Our method
greatly improves the performance in monolingual and multilingual settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Safety of Conversational Models: Taxonomy, Dataset, and Benchmark. (arXiv:2110.08466v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08466">
<div class="article-summary-box-inner">
<span><p>Dialogue safety problems severely limit the real-world deployment of neural
conversational models and attract great research interests recently. We propose
a taxonomy for dialogue safety specifically designed to capture unsafe
behaviors that are unique in human-bot dialogue setting, with focuses on
context-sensitive unsafety, which is under-explored in prior works. To spur
research in this direction, we compile DiaSafety, a dataset of 6 unsafe
categories with rich context-sensitive unsafe examples. Experiments show that
existing utterance-level safety guarding tools fail catastrophically on our
dataset. As a remedy, we train a context-level dialogue safety classifier to
provide a strong baseline for context-sensitive dialogue unsafety detection.
With our classifier, we perform safety evaluations on popular conversational
models and show that existing dialogue systems are still stuck in
context-sensitive safety problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Compositional Generalization with Self-Training for Data-to-Text Generation. (arXiv:2110.08467v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08467">
<div class="article-summary-box-inner">
<span><p>Data-to-text generation focuses on generating fluent natural language
responses from structured semantic representations. Such representations are
compositional, allowing for the combination of atomic meaning schemata in
various ways to express the rich semantics in natural language. Recently,
pretrained language models (LMs) have achieved impressive results on
data-to-text tasks, though it remains unclear the extent to which these LMs
generalize to new semantic representations. In this work, we systematically
study the compositional generalization of current state-of-the-art generation
models in data-to-text tasks. By simulating structural shifts in the
compositional Weather dataset, we show that T5 models fail to generalize to
unseen structures. Next, we show that template-based input representations
greatly improve the model performance and model scale does not trivially solve
the lack of generalization. To further improve the model's performance, we
propose an approach based on self-training using finetuned BLEURT for
pseudo-response selection. Extensive experiments on the few-shot Weather and
multi-domain SGD datasets demonstrate strong gains of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Case-based Reasoning for Better Generalization in Text-Adventure Games. (arXiv:2110.08470v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08470">
<div class="article-summary-box-inner">
<span><p>Text-based games (TBG) have emerged as promising environments for driving
research in grounded language understanding and studying problems like
generalization and sample efficiency. Several deep reinforcement learning (RL)
methods with varying architectures and learning schemes have been proposed for
TBGs. However, these methods fail to generalize efficiently, especially under
distributional shifts. In a departure from deep RL approaches, in this paper,
we propose a general method inspired by case-based reasoning to train agents
and generalize out of the training distribution. The case-based reasoner
collects instances of positive experiences from the agent's interaction with
the world in the past and later reuses the collected experiences to act
efficiently. The method can be applied in conjunction with any existing
on-policy neural agent in the literature for TBGs. Our experiments show that
the proposed approach consistently improves existing methods, obtains good
out-of-distribution generalization, and achieves new state-of-the-art results
on widely used environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Good Prompt Is Worth Millions of Parameters? Low-resource Prompt-based Learning for Vision-Language Models. (arXiv:2110.08484v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08484">
<div class="article-summary-box-inner">
<span><p>Large pretrained vision-language (VL) models can learn a new task with a
handful of examples or generalize to a new task without fine-tuning. However,
these gigantic VL models are hard to deploy for real-world applications due to
their impractically huge model size and slow inference speed. In this work, we
propose FewVLM, a few-shot prompt-based learner on vision-language tasks. We
pretrain a sequence-to-sequence Transformer model with both prefix language
modeling (PrefixLM) and masked language modeling (MaskedLM), and introduce
simple prompts to improve zero-shot and few-shot performance on VQA and image
captioning. Experimental results on five VQA and captioning datasets show that
\method\xspace outperforms Frozen which is 31 times larger than ours by 18.2%
point on zero-shot VQAv2 and achieves comparable results to a 246$\times$
larger model, PICa. We observe that (1) prompts significantly affect zero-shot
performance but marginally affect few-shot performance, (2) MaskedLM helps
few-shot VQA tasks while PrefixLM boosts captioning performance, and (3)
performance significantly increases when training set size is small.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Procedural Knowledge by Sequencing Multimodal Instructional Manuals. (arXiv:2110.08486v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08486">
<div class="article-summary-box-inner">
<span><p>The ability to sequence unordered events is an essential skill to comprehend
and reason about real world task procedures, which often requires thorough
understanding of temporal common sense and multimodal information, as these
procedures are often communicated through a combination of texts and images.
Such capability is essential for applications such as sequential task planning
and multi-source instruction summarization. While humans are capable of
reasoning about and sequencing unordered multimodal procedural instructions,
whether current machine learning models have such essential capability is still
an open question. In this work, we benchmark models' capability of reasoning
over and sequencing unordered multimodal instructions by curating datasets from
popular online instructional manuals and collecting comprehensive human
annotations. We find models not only perform significantly worse than humans
but also seem incapable of efficiently utilizing the multimodal information. To
improve machines' performance on multimodal event sequencing, we propose
sequentiality-aware pretraining techniques that exploit the sequential
alignment properties of both texts and images, resulting in &gt; 5% significant
improvements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PRIMER: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization. (arXiv:2110.08499v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08499">
<div class="article-summary-box-inner">
<span><p>Recently proposed pre-trained generation models achieve strong performance on
single-document summarization benchmarks. However, most of them are pre-trained
with general-purpose objectives and mainly aim to process single document
inputs. In this paper, we propose PRIMER, a pre-trained model for
multi-document representation with focus on summarization that reduces the need
for dataset-specific architectures and large amounts of fine-tuning labeled
data. Specifically, we adopt the Longformer architecture with proper input
transformation and global attention to fit for multi-document inputs, and we
use Gap Sentence Generation objective with a new strategy to select salient
sentences for the whole cluster, called Entity Pyramid, to teach the model to
select and aggregate information across a cluster of related documents. With
extensive experiments on 6 multi-document summarization datasets from 3
different domains on the zero-shot, few-shot, and full-supervised settings, our
model, PRIMER, outperforms current state-of-the-art models on most of these
settings with large margins. Code and pre-trained models are released at
https://github.com/allenai/PRIMER
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Think Before You Speak: Using Self-talk to Generate Implicit Commonsense Knowledge for Response Generation. (arXiv:2110.08501v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08501">
<div class="article-summary-box-inner">
<span><p>Implicit knowledge, such as common sense, is key to fluid human
conversations. Current neural response generation (RG) models are trained
end-to-end, omitting unstated implicit knowledge. In this paper, we present a
self-talk approach that first generates the implicit commonsense knowledge and
then generates response by referencing the externalized knowledge, all using
one generative model. We analyze different choices to collect knowledge-aligned
dialogues, represent implicit knowledge, and elicit knowledge and responses. We
introduce three evaluation aspects: knowledge quality, knowledge-response
connection, and response quality and perform extensive human evaluations. Our
experimental results show that compared with end-to-end RG models, self-talk
models that externalize the knowledge grounding process by explicitly
generating implicit knowledge also produce responses that are more informative,
specific, and follow common sense. We also find via human evaluation that
self-talk models generate high-quality knowledge around 75% of the time. We
hope that our findings encourage further work on different approaches to
modeling implicit commonsense knowledge and training knowledgeable RG models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing Dynamic Adversarial Training Data in the Limit. (arXiv:2110.08514v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08514">
<div class="article-summary-box-inner">
<span><p>To create models that are robust across a wide range of test inputs, training
datasets should include diverse examples that span numerous phenomena. Dynamic
adversarial data collection (DADC), where annotators craft examples that
challenge continually improving models, holds promise as an approach for
generating such diverse training sets. Prior work has shown that running DADC
over 1-3 rounds can help models fix some error types, but it does not
necessarily lead to better generalization beyond adversarial test data. We
argue that running DADC over many rounds maximizes its training-time benefits,
as the different rounds can together cover many of the task-relevant phenomena.
We present the first study of longer-term DADC, where we collect 20 rounds of
NLI examples for a small set of premise paragraphs, with both adversarial and
non-adversarial approaches. Models trained on DADC examples make 26% fewer
errors on our expert-curated test set compared to models trained on
non-adversarial data. Our analysis shows that DADC yields examples that are
more difficult, more lexically and syntactically diverse, and contain fewer
annotation artifacts compared to non-adversarial examples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Dialogue Response Generation. (arXiv:2110.08515v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08515">
<div class="article-summary-box-inner">
<span><p>Responsing with image has been recognized as an important capability for an
intelligent conversational agent. Yet existing works only focus on exploring
the multimodal dialogue models which depend on retrieval-based methods, but
neglecting generation methods. To fill in the gaps, we first present a
multimodal dialogue generation model, which takes the dialogue history as
input, then generates a textual sequence or an image as response. Learning such
a model often requires multimodal dialogues containing both texts and images
which are difficult to obtain. Motivated by the challenge in practice, we
consider multimodal dialogue generation under a natural assumption that only
limited training examples are available. In such a low-resource setting, we
devise a novel conversational agent, Divter, in order to isolate parameters
that depend on multimodal dialogues from the entire generation model. By this
means, the major part of the model can be learned from a large number of
text-only dialogues and text-image pairs respectively, then the whole
parameters can be well fitted using the limited training examples. Extensive
experiments demonstrate our method achieves state-of-the-art results in both
automatic and human evaluation, and can generate informative text and
high-resolution image responses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document Understanding. (arXiv:2110.08518v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08518">
<div class="article-summary-box-inner">
<span><p>Multimodal pre-training with text, layout, and image has made significant
progress for Visually-rich Document Understanding (VrDU), especially the
fixed-layout documents such as scanned document images. While, there are still
a large number of digital documents where the layout information is not fixed
and needs to be interactively and dynamically rendered for visualization,
making existing layout-based pre-training approaches not easy to apply. In this
paper, we propose MarkupLM for document understanding tasks with markup
languages as the backbone such as HTML/XML-based documents, where text and
markup information is jointly pre-trained. Experiment results show that the
pre-trained MarkupLM significantly outperforms the existing strong baseline
models on several document understanding tasks. The pre-trained model and code
will be publicly available at https://aka.ms/markuplm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Dataset for Discourse Structure in Peer Review Discussions. (arXiv:2110.08520v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08520">
<div class="article-summary-box-inner">
<span><p>At the foundation of scientific evaluation is the labor-intensive process of
peer review. This critical task requires participants to consume and interpret
vast amounts of highly technical text. We show that discourse cues from
rebuttals can shed light on the quality and interpretation of reviews. Further,
an understanding of the argumentative strategies employed by the reviewers and
authors provides useful signal for area chairs and other decision makers.
</p>
<p>This paper presents a new labeled dataset of 20k sentences contained in 506
review-rebuttal pairs in English, annotated by experts. While existing datasets
annotate a subset of review sentences using various schemes, ours synthesizes
existing label sets and extends them to include fine-grained annotation of the
rebuttal sentences, characterizing the authors' stance towards the reviewers'
criticisms and their commitment to addressing them. Further, we annotate
\textit{every} sentence in both the review and the rebuttal, including a
description of the context for each rebuttal sentence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Power of Prompt Tuning for Low-Resource Semantic Parsing. (arXiv:2110.08525v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08525">
<div class="article-summary-box-inner">
<span><p>Prompt tuning has recently emerged as an effective method for adapting
pre-trained language models to a number of language tasks. In this paper, we
investigate prompt tuning for semantic parsing, the task of mapping natural
language utterances onto formal meaning representations. For large T5 models we
find (i) that prompt tuning significantly outperforms fine-tuning in the low
data regime and (ii) that canonicalization -- i.e. naturalizing the meaning
representations -- barely improves performance. This last result is surprising
as it suggests that large T5 models can be modulated to generate sequences that
are far from the pre-training distribution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-Trained Language Models. (arXiv:2110.08527v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08527">
<div class="article-summary-box-inner">
<span><p>Recent work has shown that pre-trained language models capture social biases
from the text corpora they are trained on. This has attracted attention to
developing techniques that mitigate such biases. In this work, we perform a
empirical survey of five recently proposed debiasing techniques: Counterfactual
Data Augmentation (CDA), Dropout, Iterative Nullspace Projection, Self-Debias,
and SentenceDebias. We quantify the effectiveness of each technique using three
different bias benchmarks while also measuring the impact of these techniques
on a model's language modeling ability, as well as its performance on
downstream NLU tasks. We experimentally find that: (1) CDA and Self-Debias are
the strongest of the debiasing techniques, obtaining improved scores on most of
the bias benchmarks (2) Current debiasing techniques do not generalize well
beyond gender bias; And (3) improvements on bias benchmarks such as StereoSet
and CrowS-Pairs by using debiasing strategies are usually accompanied by a
decrease in language modeling ability, making it difficult to determine whether
the bias mitigation is effective.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sharpness-Aware Minimization Improves Language Model Generalization. (arXiv:2110.08529v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08529">
<div class="article-summary-box-inner">
<span><p>The allure of superhuman-level capabilities has led to considerable interest
in language models like GPT-3 and T5, wherein the research has, by and large,
revolved around new model architectures, training tasks, and loss objectives,
along with substantial engineering efforts to scale up model capacity and
dataset size. Comparatively little work has been done to improve the
generalization of these models through better optimization. In this work, we
show that Sharpness-Aware Minimization (SAM), a recently proposed optimization
procedure that encourages convergence to flatter minima, can substantially
improve the generalization of language models without much computational
overhead. We show that SAM is able to boost performance on SuperGLUE, GLUE, Web
Questions, Natural Questions, Trivia QA, and TyDiQA, with particularly large
gains when training data for these tasks is limited.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pro-KD: Progressive Distillation by Following the Footsteps of the Teacher. (arXiv:2110.08532v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08532">
<div class="article-summary-box-inner">
<span><p>With ever growing scale of neural models, knowledge distillation (KD)
attracts more attention as a prominent tool for neural model compression.
However, there are counter intuitive observations in the literature showing
some challenging limitations of KD. A case in point is that the best performing
checkpoint of the teacher might not necessarily be the best teacher for
training the student in KD. Therefore, one important question would be how to
find the best checkpoint of the teacher for distillation? Searching through the
checkpoints of the teacher would be a very tedious and computationally
expensive process, which we refer to as the \textit{checkpoint-search problem}.
Moreover, another observation is that larger teachers might not necessarily be
better teachers in KD which is referred to as the \textit{capacity-gap}
problem. To address these challenging problems, in this work, we introduce our
progressive knowledge distillation (Pro-KD) technique which defines a smoother
training path for the student by following the training footprints of the
teacher instead of solely relying on distilling from a single mature
fully-trained teacher. We demonstrate that our technique is quite effective in
mitigating the capacity-gap problem and the checkpoint search problem. We
evaluate our technique using a comprehensive set of experiments on different
tasks such as image classification (CIFAR-10 and CIFAR-100), natural language
understanding tasks of the GLUE benchmark, and question answering (SQuAD 1.1
and 2.0) using BERT-based models and consistently got superior results over
state-of-the-art techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora. (arXiv:2110.08534v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08534">
<div class="article-summary-box-inner">
<span><p>Pretrained language models (PTLMs) are typically learned over a large, static
corpus and further fine-tuned for various downstream tasks. However, when
deployed in the real world, a PTLM-based model must deal with data from a new
domain that deviates from what the PTLM was initially trained on, or newly
emerged data that contains out-of-distribution information. In this paper, we
study a lifelong language model pretraining challenge where a PTLM is
continually updated so as to adapt to emerging data. Over a domain-incremental
research paper stream and a chronologically ordered tweet stream, we
incrementally pretrain a PTLM with different continual learning algorithms, and
keep track of the downstream task performance (after fine-tuning) to analyze
its ability of acquiring new knowledge and preserving learned knowledge. Our
experiments show continual learning algorithms improve knowledge preservation,
with logit distillation being the most effective approach. We further show that
continual pretraining improves generalization when training and testing data of
downstream tasks are drawn from different time steps, but do not improve when
they are from the same time steps. We believe our problem formulation, methods,
and analysis will inspire future studies towards continual pretraining of
language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse Distillation: Speeding Up Text Classification by Using Bigger Models. (arXiv:2110.08536v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08536">
<div class="article-summary-box-inner">
<span><p>Distilling state-of-the-art transformer models into lightweight student
models is an effective way to reduce computation cost at inference time.
However, the improved inference speed may be still unsatisfactory for certain
time-sensitive applications. In this paper, we aim to further push the limit of
inference speed by exploring a new area in the design space of the student
model. More specifically, we consider distilling a transformer-based text
classifier into a billion-parameter, sparsely-activated student model with a
embedding-averaging architecture. Our experiments show that the student models
retain 97% of the RoBERTa-Large teacher performance on a collection of six text
classification tasks. Meanwhile, the student model achieves up to 600x speed-up
on both GPUs and CPUs, compared to the teacher models. Further investigation
shows that our pipeline is also effective in privacy-preserving and domain
generalization settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Substructure Distribution Projection for Zero-Shot Cross-Lingual Dependency Parsing. (arXiv:2110.08538v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08538">
<div class="article-summary-box-inner">
<span><p>We present substructure distribution projection (SubDP), a technique that
projects a distribution over structures in one domain to another, by projecting
substructure distributions separately. Models for the target domains can be
then trained, using the projected distributions as soft silver labels. We
evaluate SubDP on zero-shot cross-lingual dependency parsing, taking dependency
arcs as substructures: we project the predicted dependency arc distributions in
the source language(s) to target language(s), and train a target language
parser to fit the resulting distributions. When an English treebank is the only
annotation that involves human effort, SubDP achieves better unlabeled
attachment score than all prior work on the Universal Dependencies v2.2 (Nivre
et al., 2020) test set across eight diverse target languages, as well as the
best labeled attachment score on six out of eight languages. In addition, SubDP
improves zero-shot cross-lingual dependency parsing with very few (e.g., 50)
supervised bitext pairs, across a broader range of target languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Solve Complex Tasks by Talking to Agents. (arXiv:2110.08542v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08542">
<div class="article-summary-box-inner">
<span><p>Humans often solve complex problems by interacting (in natural language) with
existing agents, such as AI assistants, that can solve simpler sub-tasks. These
agents themselves can be powerful systems built using extensive resources and
privately held data. In contrast, common NLP benchmarks aim for the development
of self-sufficient models for every task. To address this gap and facilitate
research towards ``green'' AI systems that build upon existing agents, we
propose a new benchmark called CommaQA that contains three kinds of complex
reasoning tasks that are designed to be solved by ``talking'' to four agents
with different capabilities. We demonstrate that state-of-the-art black-box
models, which are unable to leverage existing agents, struggle on CommaQA
(exact match score only reaches 40pts) even when given access to the agents'
internal knowledge and gold fact supervision. On the other hand, models using
gold question decomposition supervision can indeed solve CommaQA to a high
accuracy (over 96\% exact match) by learning to utilize the agents. Even these
additional supervision models, however, do not solve our compositional
generalization test set. Finally the end-goal of learning to solve complex
tasks by communicating with existing agents \emph{without relying on any
additional supervision} remains unsolved and we hope CommaQA serves as a novel
benchmark to enable the development of such systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tackling Multi-Answer Open-Domain Questions via a Recall-then-Verify Framework. (arXiv:2110.08544v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08544">
<div class="article-summary-box-inner">
<span><p>Open domain questions are likely to be open-ended and ambiguous, leading to
multiple valid answers. Existing approaches typically adopt the
rerank-then-read framework, where a reader reads top-ranking evidence to
predict answers. According to our empirical analyses, this framework is faced
with three problems: to leverage the power of a large reader, the reranker is
forced to select only a few relevant passages that cover diverse answers, which
is non-trivial due to unknown effect on the reader's performance; the small
reading budget also prevents the reader from making use of valuable retrieved
evidence filtered out by the reranker; besides, as the reader generates
predictions all at once based on all selected evidence, it may learn
pathological dependencies among answers, i.e., whether to predict an answer may
also depend on evidence of the other answers. To avoid these problems, we
propose to tackle multi-answer open-domain questions with a recall-then-verify
framework, which separates the reasoning process of each answer so that we can
make better use of retrieved evidence while also leveraging the power of large
models under the same memory constraint. Our framework achieves new
state-of-the-art results on two multi-answer datasets, and predicts
significantly more gold answers than a rerank-then-read system with an oracle
reranker.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unified Speaker Adaptation Approach for ASR. (arXiv:2110.08545v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08545">
<div class="article-summary-box-inner">
<span><p>Transformer models have been used in automatic speech recognition (ASR)
successfully and yields state-of-the-art results. However, its performance is
still affected by speaker mismatch between training and test data. Further
finetuning a trained model with target speaker data is the most natural
approach for adaptation, but it takes a lot of compute and may cause
catastrophic forgetting to the existing speakers. In this work, we propose a
unified speaker adaptation approach consisting of feature adaptation and model
adaptation. For feature adaptation, we employ a speaker-aware persistent memory
model which generalizes better to unseen test speakers by making use of speaker
i-vectors to form a persistent memory. For model adaptation, we use a novel
gradual pruning method to adapt to target speakers without changing the model
architecture, which to the best of our knowledge, has never been explored in
ASR. Specifically, we gradually prune less contributing parameters on model
encoder to a certain sparsity level, and use the pruned parameters for
adaptation, while freezing the unpruned parameters to keep the original model
performance. We conduct experiments on the Librispeech dataset. Our proposed
approach brings relative 2.74-6.52% word error rate (WER) reduction on general
speaker adaptation. On target speaker adaptation, our method outperforms the
baseline with up to 20.58% relative WER reduction, and surpasses the finetuning
method by up to relative 2.54%. Besides, with extremely low-resource adaptation
data (e.g., 1 utterance), our method could improve the WER by relative 6.53%
with only a few epochs of training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Making the Most of Multilingual Pretraining for Zero-Shot Neural Machine Translation. (arXiv:2110.08547v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08547">
<div class="article-summary-box-inner">
<span><p>This paper demonstrates that multilingual pretraining, a proper fine-tuning
method and a large-scale parallel dataset from multiple auxiliary languages are
all critical for zero-shot translation, where the NMT model is tested on source
languages unseen during supervised training. Following this idea, we present
SixT++, a strong many-to-English NMT model that supports 100 source languages
but is trained once with a parallel dataset from only six source languages.
SixT++ initializes the decoder embedding and the full encoder with XLM-R large,
and then trains the encoder and decoder layers with a simple two-stage training
strategy. SixT++ achieves impressive performance on many-to-English
translation. It significantly outperforms CRISS and m2m-100, two strong
multilingual NMT systems, with an average gain of 7.2 and 5.0 BLEU
respectively. Additionally, SixT++ offers a set of model parameters that can be
further fine-tuned to develop unsupervised NMT models for low-resource
languages. With back-translation on monolingual data of low-resource language,
it outperforms all current state-of-the-art unsupervised methods on Nepali and
Sinhal for both translating into and from English.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HRKD: Hierarchical Relational Knowledge Distillation for Cross-domain Language Model Compression. (arXiv:2110.08551v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08551">
<div class="article-summary-box-inner">
<span><p>On many natural language processing tasks, large pre-trained language models
(PLMs) have shown overwhelming performances compared with traditional neural
network methods. Nevertheless, their huge model size and low inference speed
have hindered the deployment on resource-limited devices in practice. In this
paper, we target to compress PLMs with knowledge distillation, and propose a
hierarchical relational knowledge distillation (HRKD) method to capture both
hierarchical and domain relational information. Specifically, to enhance the
model capability and transferability, we leverage the idea of meta-learning and
set up domain-relational graphs to capture the relational information across
different domains. And to dynamically select the most representative prototypes
for each domain, we propose a hierarchical compare-aggregate mechanism to
capture hierarchical relationships. Extensive experiments on public
multi-domain datasets demonstrate the superior performance of our HRKD method
as well as its strong few-shot learning ability. For reproducibility, we
release the code at https://github.com/cheneydon/hrkd.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Virtual Augmentation Supported Contrastive Learning of Sentence Representations. (arXiv:2110.08552v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08552">
<div class="article-summary-box-inner">
<span><p>Despite profound successes, contrastive representation learning relies on
carefully designed data augmentations using domain specific knowledge. This
challenge is magnified in natural language processing where no general rules
exist for data augmentation due to the discrete nature of natural language. We
tackle this challenge by presenting a Virtual augmentation Supported
Contrastive Learning of sentence representations (VaSCL). Originating from the
interpretation that data augmentation essentially constructs the neighborhoods
of each training instance, we in turn utilize the neighborhood to generate
effective data augmentations. Leveraging the large training batch size of
contrastive learning, we approximate the neighborhood of an instance via its
K-nearest in-batch neighbors in the representation space. We then define an
instance discrimination task within this neighborhood, and generate the virtual
augmentation in an adversarial training manner. We access the performance of
VaSCL on a wide range of downstream tasks, and set a new state-of-the-art for
unsupervised sentence representation learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PAGnol: An Extra-Large French Generative Model. (arXiv:2110.08554v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08554">
<div class="article-summary-box-inner">
<span><p>Access to large pre-trained models of varied architectures, in many different
languages, is central to the democratization of NLP. We introduce PAGnol, a
collection of French GPT models. Using scaling laws, we efficiently train
PAGnol-XL (1.5B parameters) with the same computational budget as CamemBERT, a
model 13 times smaller. PAGnol-XL is the largest model trained to date for the
French language. We plan to train increasingly large and performing versions of
PAGnol, exploring the capabilities of French extreme-scale models.
</p>
<p>For this first release, we focus on the pre-training and scaling calculations
underlining PAGnol. We fit a scaling law for compute for the French language,
and compare it with its English counterpart. We find the pre-training dataset
significantly conditions the quality of the outputs, with common datasets such
as OSCAR leading to low-quality offensive text. We evaluate our models on
discriminative and generative tasks in French, comparing to other
state-of-the-art French and multilingual models, and reaching the state of the
art in the abstract summarization task. Our research was conducted on the
public GENCI Jean Zay supercomputer, and our models up to the Large are made
publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Robustness of Reading Comprehension Models to Entity Renaming. (arXiv:2110.08555v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08555">
<div class="article-summary-box-inner">
<span><p>We study the robustness of machine reading comprehension (MRC) models to
entity renaming -- do models make more wrong predictions when answer entities
have different names? Such failures would indicate that models are overly
reliant on entity knowledge to answer questions, and therefore may generalize
poorly when facts about the world change or questions are asked about novel
entities. To systematically audit model robustness, we propose a general and
scalable method to replace person names with names from a variety of sources,
ranging from common English names to names from other languages to arbitrary
strings. Across four datasets and three pretrained model architectures, MRC
models consistently perform worse when entities are renamed, with particularly
large accuracy drops on datasets constructed via distant supervision. We also
find large differences between models: SpanBERT, which is pretrained with
span-level masking, is more robust than RoBERTa, despite having similar
accuracy on unperturbed test data. Inspired by this, we experiment with
span-level and entity-level masking as a continual pretraining objective and
find that they can further improve the robustness of MRC models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FrugalScore: Learning Cheaper, Lighter and Faster Evaluation Metricsfor Automatic Text Generation. (arXiv:2110.08559v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08559">
<div class="article-summary-box-inner">
<span><p>Fast and reliable evaluation metrics are key to R&amp;D progress. While
traditional natural language generation metrics are fast, they are not very
reliable. Conversely, new metrics based on large pretrained language models are
much more reliable, but require significant computational resources. In this
paper, we propose FrugalScore, an approach to learn a fixed, low cost version
of any expensive NLG metric, while retaining most of its original performance.
Experiments with BERTScore and MoverScore on summarization and translation show
that FrugalScore is on par with the original metrics (and sometimes better),
while having several orders of magnitude less parameters and running several
times faster. On average over all learned metrics, tasks, and variants,
FrugalScore retains 96.8% of the performance, runs 24 times faster, and has 35
times less parameters than the original metrics. We make our trained metrics
publicly available, to benefit the entire NLP community and in particular
researchers and practitioners with limited resources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ASR4REAL: An extended benchmark for speech models. (arXiv:2110.08583v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08583">
<div class="article-summary-box-inner">
<span><p>Popular ASR benchmarks such as Librispeech and Switchboard are limited in the
diversity of settings and speakers they represent. We introduce a set of
benchmarks matching real-life conditions, aimed at spotting possible biases and
weaknesses in models. We have found out that even though recent models do not
seem to exhibit a gender bias, they usually show important performance
discrepancies by accent, and even more important ones depending on the
socio-economic status of the speakers. Finally, all tested models show a strong
performance drop when tested on conversational speech, and in this precise
context even a language model trained on a dataset as big as Common Crawl does
not seem to have significant positive effect which reiterates the importance of
developing conversational language models
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">n-stage Latent Dirichlet Allocation: A Novel Approach for LDA. (arXiv:2110.08591v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08591">
<div class="article-summary-box-inner">
<span><p>Nowadays, data analysis has become a problem as the amount of data is
constantly increasing. In order to overcome this problem in textual data, many
models and methods are used in natural language processing. The topic modeling
field is one of these methods. Topic modeling allows determining the semantic
structure of a text document. Latent Dirichlet Allocation (LDA) is the most
common method among topic modeling methods. In this article, the proposed
n-stage LDA method, which can enable the LDA method to be used more
effectively, is explained in detail. The positive effect of the method has been
demonstrated by the applied English and Turkish studies. Since the method
focuses on reducing the word count in the dictionary, it can be used
language-independently. You can access the open-source code of the method and
the example: https://github.com/anil1055/n-stage_LDA
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Back to Reality: Leveraging Pattern-driven Modeling to Enable Affordable Sentiment Dependency Learning. (arXiv:2110.08604v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08604">
<div class="article-summary-box-inner">
<span><p>Aspect-based Sentiment Classification (ABSC) is a challenging sub-task of
traditional sentiment analysis. Due to the difficulty of handling potential
correlations among sentiment polarities of multiple aspects, i.e., sentiment
dependency, recent popular works tend to exploit syntactic information guiding
sentiment dependency parsing. However, syntax information (e.g., syntactic
dependency trees) usually occupies expensive computational resources in terms
of the operation of the adjacent matrix. Instead, we define the consecutive
aspects with the same sentiment as the sentiment cluster in the case that we
find that most sentiment dependency occurs between adjacent aspects. Motivated
by this finding, we propose the sentiment patterns (SP) to guide the model
dependency learning. Thereafter, we introduce the local sentiment aggregating
(LSA) mechanism to focus on learning the sentiment dependency in the sentiment
cluster. The LSA is more efficient than existing dependency tree-based models
due to the absence of additional dependency matrix constructing and modeling.
Furthermore, we propose differential weighting for aggregation window building
to measure the importance of sentiment dependency. Experiments on four public
datasets show that our models achieve state-of-the-art performance with
especially improvement on learning sentiment cluster.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Position-Aware Self-Attention based Neural Sequence Labeling. (arXiv:1908.09128v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1908.09128">
<div class="article-summary-box-inner">
<span><p>Sequence labeling is a fundamental task in natural language processing and
has been widely studied. Recently, RNN-based sequence labeling models have
increasingly gained attentions. Despite superior performance achieved by
learning the long short-term (i.e., successive) dependencies, the way of
sequentially processing inputs might limit the ability to capture the
non-continuous relations over tokens within a sentence. To tackle the problem,
we focus on how to effectively model successive and discrete dependencies of
each token for enhancing the sequence labeling performance. Specifically, we
propose an innovative attention-based model (called position-aware
selfattention, i.e., PSA) as well as a well-designed self-attentional context
fusion layer within a neural network architecture, to explore the positional
information of an input sequence for capturing the latent relations among
tokens. Extensive experiments on three classical tasks in sequence labeling
domain, i.e., partof-speech (POS) tagging, named entity recognition (NER) and
phrase chunking, demonstrate our proposed model outperforms the
state-of-the-arts without any external knowledge, in terms of various metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modulating Bottom-Up and Top-Down Visual Processing via Language-Conditional Filters. (arXiv:2003.12739v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.12739">
<div class="article-summary-box-inner">
<span><p>How to best integrate linguistic and perceptual processing in multi-modal
tasks that involve language and vision is an important open problem. In this
work, we argue that the common practice of using language in a top-down manner,
to direct visual attention over high-level visual features, may not be optimal.
We hypothesize that the use of language to also condition the bottom-up
processing from pixels to high-level features can provide benefits to the
overall performance. To support our claim, we propose a model for
language-vision problems involving dense prediction, and perform experiments on
two different multi-modal tasks: image segmentation from referring expressions
and language-guided image colorization. We compare results where either one or
both of the top-down and bottom-up visual branches are conditioned on language.
Our experiments reveal that using language to control the filters for bottom-up
visual processing in addition to top-down attention leads to better results on
both tasks and achieves state-of-the-art performance. Our analysis of different
word types in input expressions suggest that the bottom-up conditioning is
especially helpful in the presence of low level visual concepts like color.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simulated Chats for Building Dialog Systems: Learning to Generate Conversations from Instructions. (arXiv:2010.10216v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.10216">
<div class="article-summary-box-inner">
<span><p>Popular dialog datasets such as MultiWOZ are created by providing crowd
workers an instruction, expressed in natural language, that describes the task
to be accomplished. Crowd workers play the role of a user and an agent to
generate dialogs to accomplish tasks involving booking restaurant tables,
calling a taxi etc. In this paper, we present a data creation strategy that
uses the pre-trained language model, GPT2, to simulate the interaction between
crowd workers by creating a user bot and an agent bot. We train the simulators
using a smaller percentage of actual crowd-generated conversations and their
corresponding instructions. We demonstrate that by using the simulated data, we
achieve significant improvements in low-resource settings on two publicly
available datasets - the MultiWOZ dataset and the Persona chat dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LSTM Based Sentiment Analysis for Cryptocurrency Prediction. (arXiv:2103.14804v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.14804">
<div class="article-summary-box-inner">
<span><p>Recent studies in big data analytics and natural language processing develop
automatic techniques in analyzing sentiment in the social media information. In
addition, the growing user base of social media and the high volume of posts
also provide valuable sentiment information to predict the price fluctuation of
the cryptocurrency. This research is directed to predicting the volatile price
movement of cryptocurrency by analyzing the sentiment in social media and
finding the correlation between them. While previous work has been developed to
analyze sentiment in English social media posts, we propose a method to
identify the sentiment of the Chinese social media posts from the most popular
Chinese social media platform Sina-Weibo. We develop the pipeline to capture
Weibo posts, describe the creation of the crypto-specific sentiment dictionary,
and propose a long short-term memory (LSTM) based recurrent neural network
along with the historical cryptocurrency price movement to predict the price
trend for future time frames. The conducted experiments demonstrate the
proposed approach outperforms the state of the art auto regressive based model
by 18.5% in precision and 15.4% in recall.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Will it Take to Fix Benchmarking in Natural Language Understanding?. (arXiv:2104.02145v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.02145">
<div class="article-summary-box-inner">
<span><p>Evaluation for many natural language understanding (NLU) tasks is broken:
Unreliable and biased systems score so highly on standard benchmarks that there
is little room for researchers who develop better systems to demonstrate their
improvements. The recent trend to abandon IID benchmarks in favor of
adversarially-constructed, out-of-distribution test sets ensures that current
models will perform poorly, but ultimately only obscures the abilities that we
want our benchmarks to measure. In this position paper, we lay out four
criteria that we argue NLU benchmarks should meet. We argue most current
benchmarks fail at these criteria, and that adversarial data collection does
not meaningfully address the causes of these failures. Instead, restoring a
healthy evaluation ecosystem will require significant progress in the design of
benchmark datasets, the reliability with which they are annotated, their size,
and the ways they handle social bias.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Family of Origin and Family of Choice: Massively Parallel Lexiconized Iterative Pretraining for Severely Low Resource Machine Translation. (arXiv:2104.05848v7 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05848">
<div class="article-summary-box-inner">
<span><p>We translate a closed text that is known in advance into a severely low
resource language by leveraging massive source parallelism. In other words,
given a text in 124 source languages, we translate it into a severely low
resource language using only ~1,000 lines of low resource data without any
external help. Firstly, we propose a systematic method to rank and choose
source languages that are close to the low resource language. We call the
linguistic definition of language family Family of Origin (FAMO), and we call
the empirical definition of higher-ranked languages using our metrics Family of
Choice (FAMC). Secondly, we build an Iteratively Pretrained Multilingual
Order-preserving Lexiconized Transformer (IPML) to train on ~1,000 lines
(~3.5%) of low resource data. To translate named entities correctly, we build a
massive lexicon table for 2,939 Bible named entities in 124 source languages,
and include many that occur once and covers more than 66 severely low resource
languages. Moreover, we also build a novel method of combining translations
from different source languages into one. Using English as a hypothetical low
resource language, we get a +23.9 BLEU increase over a multilingual baseline,
and a +10.3 BLEU increase over our asymmetric baseline in the Bible dataset. We
get a 42.8 BLEU score for Portuguese-English translation on the medical EMEA
dataset. We also have good results for a real severely low resource Mayan
language, Eastern Pokomchi.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ExplaGraphs: An Explanation Graph Generation Task for Structured Commonsense Reasoning. (arXiv:2104.07644v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07644">
<div class="article-summary-box-inner">
<span><p>Recent commonsense-reasoning tasks are typically discriminative in nature,
where a model answers a multiple-choice question for a certain context.
Discriminative tasks are limiting because they fail to adequately evaluate the
model's ability to reason and explain predictions with underlying commonsense
knowledge. They also allow such models to use reasoning shortcuts and not be
"right for the right reasons". In this work, we present ExplaGraphs, a new
generative and structured commonsense-reasoning task (and an associated
dataset) of explanation graph generation for stance prediction. Specifically,
given a belief and an argument, a model has to predict if the argument supports
or counters the belief and also generate a commonsense-augmented graph that
serves as non-trivial, complete, and unambiguous explanation for the predicted
stance. We collect explanation graphs through a novel Create-Verify-And-Refine
graph collection framework that improves the graph quality (up to 90%) via
multiple rounds of verification and refinement. A significant 79% of our graphs
contain external commonsense nodes with diverse structures and reasoning
depths. Next, we propose a multi-level evaluation framework, consisting of
automatic metrics and human evaluation, that check for the structural and
semantic correctness of the generated graphs and their degree of match with
ground-truth graphs. Finally, we present several structured,
commonsense-augmented, and text generation models as strong starting points for
this explanation graph generation task, and observe that there is a large gap
with human performance, thereby encouraging future work for this new
challenging task. ExplaGraphs will be publicly available at
https://explagraphs.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Task Generalization via Natural Language Crowdsourcing Instructions. (arXiv:2104.08773v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08773">
<div class="article-summary-box-inner">
<span><p>Humans (e.g., crowdworkers) have a remarkable ability in solving different
tasks, by simply reading textual instructions that define them and looking at a
few examples. NLP models built with the conventional paradigm, however, often
struggle with generalization across tasks (e.g., a question-answering system
cannot solve classification tasks). A long-standing challenge in AI is to build
a model that learns a new task by understanding the human-readable instructions
that define it. To study this, we introduce NATURAL INSTRUCTIONS, a dataset of
61 distinct tasks, their human-authored instructions and 193k task instances.
The instructions are obtained from crowdsourcing instructions used to create
existing NLP datasets and mapped to a unified schema. We adopt generative
pre-trained language models to encode task-specific instructions along with
input and generate task output. Our results indicate that models benefit from
instructions when evaluated in terms of generalization to unseen tasks. These
models, however, are far behind supervised task-specific models, indicating
significant room for more progress in this direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Misinfo Reaction Frames: Reasoning about Readers' Reactions to News Headlines. (arXiv:2104.08790v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08790">
<div class="article-summary-box-inner">
<span><p>Even to a simple and short news headline, readers react in a multitude of
ways: cognitively (e.g., inferring the writer's intent), emotionally (e.g.,
feeling distrust), and behaviorally (e.g., sharing the news with their
friends). Such reactions are instantaneous and yet complex, as they rely on
factors that go beyond interpreting the factual content of the news headline.
Instead, understanding reactions requires pragmatic understanding of the news
headline, including broader background knowledge about contentious news topics
as well as commonsense reasoning about people's intents and emotional
reactions. We propose Misinfo Reaction Frames, a pragmatic formalism for
modeling how readers might react to a news headline cognitively, emotionally,
and behaviorally. We also introduce a Misinfo Reaction Frames corpus, a dataset
of over 200k news headline/annotated dimension pairs with crowdsourced
reactions focusing on global crises: the Covid-19 pandemic, climate change, and
cancer. Empirical results confirm that it is indeed possible to learn the
prominent patterns of readers' reactions to news headlines. We also find a
potentially positive use case of our model; When we present our model generated
inferences to people, we find that the machine inferences can increase readers'
trust in real news while decreasing their trust in misinformation. Our work
demonstrates the feasibility and the importance of pragmatic inferences of news
to help enhance AI-guided misinformation detection and mitigation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SUPERB: Speech processing Universal PERformance Benchmark. (arXiv:2105.01051v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.01051">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning (SSL) has proven vital for advancing research in
natural language processing (NLP) and computer vision (CV). The paradigm
pretrains a shared model on large volumes of unlabeled data and achieves
state-of-the-art (SOTA) for various tasks with minimal adaptation. However, the
speech processing community lacks a similar setup to systematically explore the
paradigm. To bridge this gap, we introduce Speech processing Universal
PERformance Benchmark (SUPERB). SUPERB is a leaderboard to benchmark the
performance of a shared model across a wide range of speech processing tasks
with minimal architecture changes and labeled data. Among multiple usages of
the shared model, we especially focus on extracting the representation learned
from SSL due to its preferable re-usability. We present a simple framework to
solve SUPERB tasks by learning task-specialized lightweight prediction heads on
top of the frozen shared model. Our results demonstrate that the framework is
promising as SSL representations show competitive generalizability and
accessibility across SUPERB tasks. We release SUPERB as a challenge with a
leaderboard and a benchmark toolkit to fuel the research in representation
learning and general speech processing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic-WER: A Unified Metric for the Evaluation of ASR Transcript for End Usability. (arXiv:2106.02016v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02016">
<div class="article-summary-box-inner">
<span><p>Recent advances in supervised, semi-supervised and self-supervised deep
learning algorithms have shown significant improvement in the performance of
automatic speech recognition(ASR) systems. The state-of-the-art systems have
achieved a word error rate (WER) less than 5%. However, in the past,
researchers have argued the non-suitability of the WER metric for the
evaluation of ASR systems for downstream tasks such as spoken language
understanding (SLU) and information retrieval. The reason is that the WER works
at the surface level and does not include any syntactic and semantic
knowledge.The current work proposes Semantic-WER (SWER), a metric to evaluate
the ASR transcripts for downstream applications in general. The SWER can be
easily customized for any down-stream task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine Translation into Low-resource Language Varieties. (arXiv:2106.06797v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06797">
<div class="article-summary-box-inner">
<span><p>State-of-the-art machine translation (MT) systems are typically trained to
generate the "standard" target language; however, many languages have multiple
varieties (regional varieties, dialects, sociolects, non-native varieties) that
are different from the standard language. Such varieties are often
low-resource, and hence do not benefit from contemporary NLP solutions, MT
included. We propose a general framework to rapidly adapt MT systems to
generate language varieties that are close to, but different from, the standard
target language, using no parallel (source--variety) data. This also includes
adaptation of MT systems to low-resource typologically-related target
languages. We experiment with adapting an English--Russian MT system to
generate Ukrainian and Belarusian, an English--Norwegian Bokm{\aa}l system to
generate Nynorsk, and an English--Arabic system to generate four Arabic
dialects, obtaining significant improvements over competitive baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LoRA: Low-Rank Adaptation of Large Language Models. (arXiv:2106.09685v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.09685">
<div class="article-summary-box-inner">
<span><p>An important paradigm of natural language processing consists of large-scale
pre-training on general domain data and adaptation to particular tasks or
domains. As we pre-train larger models, full fine-tuning, which retrains all
model parameters, becomes less feasible. Using GPT-3 175B as an example --
deploying independent instances of fine-tuned models, each with 175B
parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or
LoRA, which freezes the pre-trained model weights and injects trainable rank
decomposition matrices into each layer of the Transformer architecture, greatly
reducing the number of trainable parameters for downstream tasks. Compared to
GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable
parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA
performs on-par or better than fine-tuning in model quality on RoBERTa,
DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher
training throughput, and, unlike adapters, no additional inference latency. We
also provide an empirical investigation into rank-deficiency in language model
adaptation, which sheds light on the efficacy of LoRA. We release a package
that facilitates the integration of LoRA with PyTorch models and provide our
implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at
https://github.com/microsoft/LoRA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Investigation of the (In)effectiveness of Counterfactually Augmented Data. (arXiv:2107.00753v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.00753">
<div class="article-summary-box-inner">
<span><p>While pretrained language models achieve excellent performance on natural
language understanding benchmarks, they tend to rely on spurious correlations
and generalize poorly to out-of-distribution (OOD) data. Recent work has
explored using counterfactually-augmented data (CAD) -- data generated by
minimally perturbing examples to flip the ground-truth label -- to identify
robust features that are invariant under distribution shift. However, empirical
results using CAD for OOD generalization have been mixed. To explain this
discrepancy, we draw insights from a linear Gaussian model and demonstrate the
pitfalls of CAD. Specifically, we show that (a) while CAD is effective at
identifying robust features, it may prevent the model from learning unperturbed
robust features; and (b) CAD may exacerbate existing spurious correlations in
the data. On two crowdsourced CAD datasets, our results show that the lack of
perturbation diversity limits their effectiveness on OOD generalization,
calling for innovative crowdsourcing procedures to elicit diverse perturbation
of examples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved Language Identification Through Cross-Lingual Self-Supervised Learning. (arXiv:2107.04082v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.04082">
<div class="article-summary-box-inner">
<span><p>Language identification greatly impacts the success of downstream tasks such
as automatic speech recognition. Recently, self-supervised speech
representations learned by wav2vec 2.0 have been shown to be very effective for
a range of speech tasks. We extend previous self-supervised work on language
identification by experimenting with pre-trained models which were learned on
real-world unconstrained speech in multiple languages and not just on English.
We show that models pre-trained on many languages perform better and enable
language identification systems that require very little labeled data to
perform well. Results on a 26 languages setup show that with only 10 minutes of
labeled data per language, a cross-lingually pre-trained model can achieve over
89.2% accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MuSiQue: Multi-hop Questions via Single-hop Question Composition. (arXiv:2108.00573v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.00573">
<div class="article-summary-box-inner">
<span><p>Can we create a question answering (QA) dataset that, by construction,
requires proper multi-hop reasoning? This goal has been surprisingly elusive.
We introduce a bottom-up approach that systematically selects composable pairs
of single-hop questions that are connected, i.e., where one reasoning step
requires information from the other. This bottom-up approach allows greater
control over the properties of the resulting $k$-hop questions. We add
stringent filters and other mechanisms targeting connected reasoning, including
minimizing many forms of train-test leakage, improved distractor contexts, and
contrasting unanswerable questions at the sub-question level. We use this
process to construct MuSiQue-Ans, a new multihop QA dataset with 25K 2-4 hop
questions, built using seed questions from 5 existing single-hop datasets. Our
experiments demonstrate that MuSiQue-Ans is challenging for state-of-the-art QA
models significantly harder than existing datasets (3x human-machine gap in a
comparable setting), and substantially less cheatable (e.g., a single-hop model
is worse by 30 F1 pts). We also build a more challenging dataset, MuSiQue-Full,
consisting of answerable and unanswerable contrast question pairs, where model
performance drops further by 14 F1 pts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Monolingual versus Multilingual BERTology for Vietnamese Extractive Multi-Document Summarization. (arXiv:2108.13741v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13741">
<div class="article-summary-box-inner">
<span><p>Recent researches have demonstrated that BERT shows potential in a wide range
of natural language processing tasks. It is adopted as an encoder for many
state-of-the-art automatic summarizing systems, which achieve excellent
performance. However, so far, there is not much work done for Vietnamese. In
this paper, we showcase how BERT can be implemented for extractive text
summarization in Vietnamese on multi-document. We introduce a novel comparison
between different multilingual and monolingual BERT models. The experiment
results indicate that monolingual models produce promising results compared to
other multilingual models and previous text summarizing models for Vietnamese.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequence-to-Sequence Learning with Latent Neural Grammars. (arXiv:2109.01135v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01135">
<div class="article-summary-box-inner">
<span><p>Sequence-to-sequence learning with neural networks has become the de facto
standard for sequence prediction tasks. This approach typically models the
local distribution over the next word with a powerful neural network that can
condition on arbitrary context. While flexible and performant, these models
often require large datasets for training and can fail spectacularly on
benchmarks designed to test for compositional generalization. This work
explores an alternative, hierarchical approach to sequence-to-sequence learning
with quasi-synchronous grammars, where each node in the target tree is
transduced by a node in the source tree. Both the source and target trees are
treated as latent and induced during training. We develop a neural
parameterization of the grammar which enables parameter sharing over the
combinatorial space of derivation rules without the need for manual feature
engineering. We apply this latent neural grammar to various domains -- a
diagnostic language navigation task designed to test for compositional
generalization (SCAN), style transfer, and small-scale machine translation --
and find that it performs respectably compared to standard baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multitask Balanced and Recalibrated Network for Medical Code Prediction. (arXiv:2109.02418v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02418">
<div class="article-summary-box-inner">
<span><p>Human coders assign standardized medical codes to clinical documents
generated during patients' hospitalization, which is error-prone and
labor-intensive. Automated medical coding approaches have been developed using
machine learning methods such as deep neural networks. Nevertheless, automated
medical coding is still challenging because of the imbalanced class problem,
complex code association, and noise in lengthy documents. To solve these
issues, we propose a novel neural network called Multitask Balanced and
Recalibrated Neural Network. Significantly, the multitask learning scheme
shares the relationship knowledge between different code branches to capture
the code association. A recalibrated aggregation module is developed by
cascading convolutional blocks to extract high-level semantic features that
mitigate the impact of noise in documents. Also, the cascaded structure of the
recalibrated module can benefit the learning from lengthy notes. To solve the
class imbalanced problem, we deploy the focal loss to redistribute the
attention of low and high-frequency medical codes. Experimental results show
that our proposed model outperforms competitive baselines on a real-world
clinical dataset MIMIC-III.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Balancing Methods for Multi-label Text Classification with Long-Tailed Class Distribution. (arXiv:2109.04712v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04712">
<div class="article-summary-box-inner">
<span><p>Multi-label text classification is a challenging task because it requires
capturing label dependencies. It becomes even more challenging when class
distribution is long-tailed. Resampling and re-weighting are common approaches
used for addressing the class imbalance problem, however, they are not
effective when there is label dependency besides class imbalance because they
result in oversampling of common labels. Here, we introduce the application of
balancing loss functions for multi-label text classification. We perform
experiments on a general domain dataset with 90 labels (Reuters-21578) and a
domain-specific dataset from PubMed with 18211 labels. We find that a
distribution-balanced loss function, which inherently addresses both the class
imbalance and label linkage problems, outperforms commonly used loss functions.
Distribution balancing methods have been successfully used in the image
recognition field. Here, we show their effectiveness in natural language
processing. Source code is available at
https://github.com/Roche/BalancedLossNLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Contrastive Learning via Novel Data Augmentation and Curriculum Learning. (arXiv:2109.05941v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05941">
<div class="article-summary-box-inner">
<span><p>We introduce EfficientCL, a memory-efficient continual pretraining method
that applies contrastive learning with novel data augmentation and curriculum
learning. For data augmentation, we stack two types of operation sequentially:
cutoff and PCA jittering. While pretraining steps proceed, we apply curriculum
learning by incrementing the augmentation degree for each difficulty step.
After data augmentation is finished, contrastive learning is applied on
projected embeddings of original and augmented examples. When finetuned on GLUE
benchmark, our model outperforms baseline models, especially for sentence-level
tasks. Additionally, this improvement is capable with only 70% of computational
memory compared to the baseline model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Jointly Modeling Aspect and Polarity for Aspect-based Sentiment Analysis in Persian Reviews. (arXiv:2109.07680v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07680">
<div class="article-summary-box-inner">
<span><p>Identification of user's opinions from natural language text has become an
exciting field of research due to its growing applications in the real world.
The research field is known as sentiment analysis and classification, where
aspect category detection (ACD) and aspect category polarity (ACP) are two
important sub-tasks of aspect-based sentiment analysis. The goal in ACD is to
specify which aspect of the entity comes up in opinion while ACP aims to
specify the polarity of each aspect category from the ACD task. The previous
works mostly propose separate solutions for these two sub-tasks. This paper
focuses on the ACD and ACP sub-tasks to solve both problems simultaneously. The
proposed method carries out multi-label classification where four different
deep models were employed and comparatively evaluated to examine their
performance. A dataset of Persian reviews was collected from CinemaTicket
website including 2200 samples from 14 categories. The developed models were
evaluated using the collected dataset in terms of example-based and label-based
metrics. The results indicate the high applicability and preference of the CNN
and GRU models in comparison to LSTM and Bi-LSTM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reframing Instructional Prompts to GPTk's Language. (arXiv:2109.07830v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07830">
<div class="article-summary-box-inner">
<span><p>How can model designers turn task instructions into effective prompts for
language models? Backed by extensive empirical analysis on GPT3, we observe
important features for successful instructional prompts, and propose several
reframing techniques for model designers to create such prompts. For example, a
complex task can be decomposed into multiple simpler tasks. We experiment over
12 NLP tasks across 6 diverse categories (question generation, classification,
etc.). Our results show that reframing improves few-shot and zero-shot learning
performance by 14% and 17% respectively while reducing sample complexity over
other recent few-shot baselines. The performance gains are particularly
important on large language models, such as GPT3 where tuning models or prompts
on large datasets is not feasible. Furthermore, we observe that such gains are
not limited to GPT3; the reframed tasks remain superior over raw instructions
across different model architectures, underscoring the cross-model generality
of these guidelines. We hope these empirical-driven techniques will pave way
for more effective ways to prompt LMs in the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FastCorrect 2: Fast Error Correction on Multiple Candidates for Automatic Speech Recognition. (arXiv:2109.14420v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14420">
<div class="article-summary-box-inner">
<span><p>Error correction is widely used in automatic speech recognition (ASR) to
post-process the generated sentence, and can further reduce the word error rate
(WER). Although multiple candidates are generated by an ASR system through beam
search, current error correction approaches can only correct one sentence at a
time, failing to leverage the voting effect from multiple candidates to better
detect and correct error tokens. In this work, we propose FastCorrect 2, an
error correction model that takes multiple ASR candidates as input for better
correction accuracy. FastCorrect 2 adopts non-autoregressive generation for
fast inference, which consists of an encoder that processes multiple source
sentences and a decoder that generates the target sentence in parallel from the
adjusted source sentence, where the adjustment is based on the predicted
duration of each source token. However, there are some issues when handling
multiple source sentences. First, it is non-trivial to leverage the voting
effect from multiple source sentences since they usually vary in length. Thus,
we propose a novel alignment algorithm to maximize the degree of token
alignment among multiple sentences in terms of token and pronunciation
similarity. Second, the decoder can only take one adjusted source sentence as
input, while there are multiple source sentences. Thus, we develop a candidate
predictor to detect the most suitable candidate for the decoder. Experiments on
our inhouse dataset and AISHELL-1 show that FastCorrect 2 can further reduce
the WER over the previous correction model with single candidate by 3.2% and
2.6%, demonstrating the effectiveness of leveraging multiple candidates in ASR
error correction. FastCorrect 2 achieves better performance than the cascaded
re-scoring and correction pipeline and can serve as a unified post-processing
module for ASR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Factorized Neural Transducer for Efficient Language Model Adaptation. (arXiv:2110.01500v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01500">
<div class="article-summary-box-inner">
<span><p>In recent years, end-to-end (E2E) based automatic speech recognition (ASR)
systems have achieved great success due to their simplicity and promising
performance. Neural Transducer based models are increasingly popular in
streaming E2E based ASR systems and have been reported to outperform the
traditional hybrid system in some scenarios. However, the joint optimization of
acoustic model, lexicon and language model in neural Transducer also brings
about challenges to utilize pure text for language model adaptation. This
drawback might prevent their potential applications in practice. In order to
address this issue, in this paper, we propose a novel model, factorized neural
Transducer, by factorizing the blank and vocabulary prediction, and adopting a
standalone language model for the vocabulary prediction. It is expected that
this factorization can transfer the improvement of the standalone language
model to the Transducer for speech recognition, which allows various language
model adaptation techniques to be applied. We demonstrate that the proposed
factorized neural Transducer yields 15% to 20% WER improvements when
out-of-domain text data is used for language model adaptation, at the cost of a
minor degradation in WER on a general test set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WenetSpeech: A 10000+ Hours Multi-domain Mandarin Corpus for Speech Recognition. (arXiv:2110.03370v3 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03370">
<div class="article-summary-box-inner">
<span><p>In this paper, we present WenetSpeech, a multi-domain Mandarin corpus
consisting of 10000+ hours high-quality labeled speech, 2400+ hours weakly
labeled speech, and about 10000 hours unlabeled speech, with 22400+ hours in
total. We collect the data from YouTube and Podcast, which covers a variety of
speaking styles, scenarios, domains, topics, and noisy conditions. An optical
character recognition (OCR) based method is introduced to generate the
audio/text segmentation candidates for the YouTube data on its corresponding
video captions, while a high-quality ASR transcription system is used to
generate audio/text pair candidates for the Podcast data. Then we propose a
novel end-to-end label error detection approach to further validate and filter
the candidates. We also provide three manually labelled high-quality test sets
along with WenetSpeech for evaluation -- Dev for cross-validation purpose in
training, Test_Net, collected from Internet for matched test, and
Test\_Meeting, recorded from real meetings for more challenging mismatched
test. Baseline systems trained with WenetSpeech are provided for three popular
speech recognition toolkits, namely Kaldi, ESPnet, and WeNet, and recognition
results on the three test sets are also provided as benchmarks. To the best of
our knowledge, WenetSpeech is the current largest open-sourced Mandarin speech
corpus with transcriptions, which benefits research on production-level speech
recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relation-aware Video Reading Comprehension for Temporal Language Grounding. (arXiv:2110.05717v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05717">
<div class="article-summary-box-inner">
<span><p>Temporal language grounding in videos aims to localize the temporal span
relevant to the given query sentence. Previous methods treat it either as a
boundary regression task or a span extraction task. This paper will formulate
temporal language grounding into video reading comprehension and propose a
Relation-aware Network (RaNet) to address it. This framework aims to select a
video moment choice from the predefined answer set with the aid of
coarse-and-fine choice-query interaction and choice-choice relation
construction. A choice-query interactor is proposed to match the visual and
textual information simultaneously in sentence-moment and token-moment levels,
leading to a coarse-and-fine cross-modal interaction. Moreover, a novel
multi-choice relation constructor is introduced by leveraging graph convolution
to capture the dependencies among video moment choices for the best choice
selection. Extensive experiments on ActivityNet-Captions, TACoS, and
Charades-STA demonstrate the effectiveness of our solution. Codes will be
released soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting IPA-based Cross-lingual Text-to-speech. (arXiv:2110.07187v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07187">
<div class="article-summary-box-inner">
<span><p>International Phonetic Alphabet (IPA) has been widely used in cross-lingual
text-to-speech (TTS) to achieve cross-lingual voice cloning (CL VC). However,
IPA itself has been understudied in cross-lingual TTS. In this paper, we report
some empirical findings of building a cross-lingual TTS model using IPA as
inputs. Experiments show that the way to process the IPA and suprasegmental
sequence has a negligible impact on the CL VC performance. Furthermore, we find
that using a dataset including one speaker per language to build an IPA-based
TTS system would fail CL VC since the language-unique IPA and tone/stress
symbols could leak the speaker information. In addition, we experiment with
different combinations of speakers in the training dataset to further
investigate the effect of the number of speakers on the CL VC performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks. (arXiv:2110.07602v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07602">
<div class="article-summary-box-inner">
<span><p>Prompt tuning, which only tunes continuous prompts with a frozen language
model, substantially reduces per-task storage and memory usage at training.
However, in the context of NLU, prior work reveals that prompt tuning does not
perform well for normal-sized pre-trained models. We also find that existing
methods of prompt tuning cannot handle hard sequence tagging tasks, indicating
a lack of universality. We present a novel empirical finding that properly
optimized prompt tuning can be universally effective across a wide range of
model scales and NLU tasks. It matches the performance of fine-tuning while
having only 0.1\%-3\% tuned parameters. Our method P-Tuning v2 is not a new
method, but a version of prefix-tuning \cite{li2021prefix} optimized and
adapted for NLU. Given the universality and simplicity of P-Tuning v2, we
believe it can serve as an alternative to fine-tuning and a strong baseline for
future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Core Challenges in Embodied Vision-Language Planning. (arXiv:2106.13948v2 [cs.LG] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.13948">
<div class="article-summary-box-inner">
<span><p>Recent advances in the areas of multimodal machine learning and artificial
intelligence (AI) have led to the development of challenging tasks at the
intersection of Computer Vision, Natural Language Processing, and Embodied AI.
Whereas many approaches and previous survey pursuits have characterised one or
two of these dimensions, there has not been a holistic analysis at the center
of all three. Moreover, even when combinations of these topics are considered,
more focus is placed on describing, e.g., current architectural methods, as
opposed to also illustrating high-level challenges and opportunities for the
field. In this survey paper, we discuss Embodied Vision-Language Planning
(EVLP) tasks, a family of prominent embodied navigation and manipulation
problems that jointly use computer vision and natural language. We propose a
taxonomy to unify these tasks and provide an in-depth analysis and comparison
of the new and current algorithmic approaches, metrics, simulated environments,
as well as the datasets used for EVLP tasks. Finally, we present the core
challenges that we believe new EVLP works should seek to address, and we
advocate for task construction that enables model generalizability and furthers
real-world deployment.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">FlexMatch: Boosting Semi-Supervised Learning with Curriculum Pseudo Labeling. (arXiv:2110.08263v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08263">
<div class="article-summary-box-inner">
<span><p>The recently proposed FixMatch achieved state-of-the-art results on most
semi-supervised learning (SSL) benchmarks. However, like other modern SSL
algorithms, FixMatch uses a pre-defined constant threshold for all classes to
select unlabeled data that contribute to the training, thus failing to consider
different learning status and learning difficulties of different classes. To
address this issue, we propose Curriculum Pseudo Labeling (CPL), a curriculum
learning approach to leverage unlabeled data according to the model's learning
status. The core of CPL is to flexibly adjust thresholds for different classes
at each time step to let pass informative unlabeled data and their pseudo
labels. CPL does not introduce additional parameters or computations (forward
or backward propagation). We apply CPL to FixMatch and call our improved
algorithm FlexMatch. FlexMatch achieves state-of-the-art performance on a
variety of SSL benchmarks, with especially strong performances when the labeled
data are extremely limited or when the task is challenging. For example,
FlexMatch outperforms FixMatch by 14.32% and 24.55% on CIFAR-100 and STL-10
datasets respectively, when there are only 4 labels per class. CPL also
significantly boosts the convergence speed, e.g., FlexMatch can use only 1/5
training time of FixMatch to achieve even better performance. Furthermore, we
show that CPL can be easily adapted to other SSL algorithms and remarkably
improve their performances. We open source our code at
https://github.com/TorchSSL/TorchSSL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training Deep Neural Networks with Joint Quantization and Pruning of Weights and Activations. (arXiv:2110.08271v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08271">
<div class="article-summary-box-inner">
<span><p>Quantization and pruning are core techniques used to reduce the inference
costs of deep neural networks. State-of-the-art quantization techniques are
currently applied to both the weights and activations; however, pruning is most
often applied to only the weights of the network. In this work, we jointly
apply novel uniform quantization and unstructured pruning methods to both the
weights and activations of deep neural networks during training. Using our
methods, we empirically evaluate the currently accepted prune-then-quantize
paradigm across a wide range of computer vision tasks and observe a
non-commutative nature when applied to both the weights and activations of deep
neural networks. Informed by these observations, we articulate the
non-commutativity hypothesis: for a given deep neural network being trained for
a specific task, there exists an exact training schedule in which quantization
and pruning can be introduced to optimize network performance. We identify that
this optimal ordering not only exists, but also varies across discriminative
and generative tasks. Using the optimal training schedule within our training
framework, we demonstrate increased performance per memory footprint over
existing solutions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Human Pose Estimation for Free-form Activity Using WiFi Signals. (arXiv:2110.08314v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08314">
<div class="article-summary-box-inner">
<span><p>WiFi human sensing has become increasingly attractive in enabling emerging
human-computer interaction applications. The corresponding technique has
gradually evolved from the classification of multiple activity types to more
fine-grained tracking of 3D human poses. However, existing WiFi-based 3D human
pose tracking is limited to a set of predefined activities. In this work, we
present Winect, a 3D human pose tracking system for free-form activity using
commodity WiFi devices. Our system tracks free-form activity by estimating a 3D
skeleton pose that consists of a set of joints of the human body. In
particular, we combine signal separation and joint movement modeling to achieve
free-form activity tracking. Our system first identifies the moving limbs by
leveraging the two-dimensional angle of arrival of the signals reflected off
the human body and separates the entangled signals for each limb. Then, it
tracks each limb and constructs a 3D skeleton of the body by modeling the
inherent relationship between the movements of the limb and the corresponding
joints. Our evaluation results show that Winect is environment-independent and
achieves centimeter-level accuracy for free-form activity tracking under
various challenging environments including the none-line-of-sight (NLoS)
scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Solving Image PDEs with a Shallow Network. (arXiv:2110.08327v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08327">
<div class="article-summary-box-inner">
<span><p>Partial differential equations (PDEs) are typically used as models of
physical processes but are also of great interest in PDE-based image
processing. However, when it comes to their use in imaging, conventional
numerical methods for solving PDEs tend to require very fine grid resolution
for stability, and as a result have impractically high computational cost. This
work applies BLADE (Best Linear Adaptive Enhancement), a shallow learnable
filtering framework, to PDE solving, and shows that the resulting approach is
efficient and accurate, operating more reliably at coarse grid resolutions than
classical methods. As such, the model can be flexibly used for a wide variety
of problems in imaging.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Trigger Hunting with a Topological Prior for Trojan Detection. (arXiv:2110.08335v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08335">
<div class="article-summary-box-inner">
<span><p>Despite their success and popularity, deep neural networks (DNNs) are
vulnerable when facing backdoor attacks. This impedes their wider adoption,
especially in mission critical applications. This paper tackles the problem of
Trojan detection, namely, identifying Trojaned models -- models trained with
poisoned data. One popular approach is reverse engineering, i.e., recovering
the triggers on a clean image by manipulating the model's prediction. One major
challenge of reverse engineering approach is the enormous search space of
triggers. To this end, we propose innovative priors such as diversity and
topological simplicity to not only increase the chances of finding the
appropriate triggers but also improve the quality of the found triggers.
Moreover, by encouraging a diverse set of trigger candidates, our method can
perform effectively in cases with unknown target labels. We demonstrate that
these priors can significantly improve the quality of the recovered triggers,
resulting in substantially improved Trojan detection accuracy as validated on
both synthetic and publicly available TrojAI benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Counting Objects by Diffused Index: geometry-free and training-free approach. (arXiv:2110.08365v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08365">
<div class="article-summary-box-inner">
<span><p>Counting objects is a fundamental but challenging problem. In this paper, we
propose diffusion-based, geometry-free, and learning-free methodologies to
count the number of objects in images. The main idea is to represent each
object by a unique index value regardless of its intensity or size, and to
simply count the number of index values. First, we place different vectors,
refer to as seed vectors, uniformly throughout the mask image. The mask image
has boundary information of the objects to be counted. Secondly, the seeds are
diffused using an edge-weighted harmonic variational optimization model within
each object. We propose an efficient algorithm based on an operator splitting
approach and alternating direction minimization method, and theoretical
analysis of this algorithm is given. An optimal solution of the model is
obtained when the distributed seeds are completely diffused such that there is
a unique intensity within each object, which we refer to as an index. For
computational efficiency, we stop the diffusion process before a full
convergence, and propose to cluster these diffused index values. We refer to
this approach as Counting Objects by Diffused Index (CODI). We explore scalar
and multi-dimensional seed vectors. For Scalar seeds, we use Gaussian fitting
in histogram to count, while for vector seeds, we exploit a high-dimensional
clustering method for the final step of counting via clustering. The proposed
method is flexible even if the boundary of the object is not clear nor fully
enclosed. We present counting results in various applications such as
biological cells, agriculture, concert crowd, and transportation. Some
comparisons with existing methods are presented.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Starkit: RoboCup Humanoid KidSize 2021 Worldwide Champion Team Paper. (arXiv:2110.08377v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08377">
<div class="article-summary-box-inner">
<span><p>This article is devoted to the features that were under development between
RoboCup 2019 Sydney and RoboCup 2021 Worldwide. These features include
vision-related matters, such as detection and localization, mechanical and
algorithmic novelties. Since the competition was held virtually, the
simulation-specific features are also considered in the article. We give an
overview of the approaches that were tried out along with the analysis of their
preconditions, perspectives and the evaluation of their performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparing Human and Machine Bias in Face Recognition. (arXiv:2110.08396v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08396">
<div class="article-summary-box-inner">
<span><p>Much recent research has uncovered and discussed serious concerns of bias in
facial analysis technologies, finding performance disparities between groups of
people based on perceived gender, skin type, lighting condition, etc. These
audits are immensely important and successful at measuring algorithmic bias but
have two major challenges: the audits (1) use facial recognition datasets which
lack quality metadata, like LFW and CelebA, and (2) do not compare their
observed algorithmic bias to the biases of their human alternatives. In this
paper, we release improvements to the LFW and CelebA datasets which will enable
future researchers to obtain measurements of algorithmic bias that are not
tainted by major flaws in the dataset (e.g. identical images appearing in both
the gallery and test set). We also use these new data to develop a series of
challenging facial identification and verification questions that we
administered to various algorithms and a large, balanced sample of human
reviewers. We find that both computer models and human survey participants
perform significantly better at the verification task, generally obtain lower
accuracy rates on dark-skinned or female subjects for both tasks, and obtain
higher accuracy rates when their demographics match that of the question.
Computer models are observed to achieve a higher level of accuracy than the
survey participants on both tasks and exhibit bias to similar degrees as the
human survey participants.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mind the Gap: Domain Gap Control for Single Shot Domain Adaptation for Generative Adversarial Networks. (arXiv:2110.08398v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08398">
<div class="article-summary-box-inner">
<span><p>We present a new method for one shot domain adaptation. The input to our
method is trained GAN that can produce images in domain A and a single
reference image I_B from domain B. The proposed algorithm can translate any
output of the trained GAN from domain A to domain B. There are two main
advantages of our method compared to the current state of the art: First, our
solution achieves higher visual quality, e.g. by noticeably reducing
overfitting. Second, our solution allows for more degrees of freedom to control
the domain gap, i.e. what aspects of image I_B are used to define the domain B.
Technically, we realize the new method by building on a pre-trained StyleGAN
generator as GAN and a pre-trained CLIP model for representing the domain gap.
We propose several new regularizers for controlling the domain gap to optimize
the weights of the pre-trained StyleGAN generator to output images in domain B
instead of domain A. The regularizers prevent the optimization from taking on
too many attributes of the single reference image. Our results show significant
visual improvements over the state of the art as well as multiple applications
that highlight improved control.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridging the gap between paired and unpaired medical image translation. (arXiv:2110.08407v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08407">
<div class="article-summary-box-inner">
<span><p>Medical image translation has the potential to reduce the imaging workload,
by removing the need to capture some sequences, and to reduce the annotation
burden for developing machine learning methods. GANs have been used
successfully to translate images from one domain to another, such as MR to CT.
At present, paired data (registered MR and CT images) or extra supervision
(e.g. segmentation masks) is needed to learn good translation models.
Registering multiple modalities or annotating structures within each of them is
a tedious and laborious task. Thus, there is a need to develop improved
translation methods for unpaired data. Here, we introduce modified pix2pix
models for tasks CT$\rightarrow$MR and MR$\rightarrow$CT, trained with unpaired
CT and MR data, and MRCAT pairs generated from the MR scans. The proposed
modifications utilize the paired MR and MRCAT images to ensure good alignment
between input and translated images, and unpaired CT images ensure the
MR$\rightarrow$CT model produces realistic-looking CT and CT$\rightarrow$MR
model works well with real CT as input. The proposed pix2pix variants
outperform baseline pix2pix, pix2pixHD and CycleGAN in terms of FID and KID,
and generate more realistic looking CT and MR translations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dataset Knowledge Transfer for Class-Incremental Learning without Memory. (arXiv:2110.08421v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08421">
<div class="article-summary-box-inner">
<span><p>Incremental learning enables artificial agents to learn from sequential data.
While important progress was made by exploiting deep neural networks,
incremental learning remains very challenging. This is particularly the case
when no memory of past data is allowed and catastrophic forgetting has a strong
negative effect. We tackle class-incremental learning without memory by
adapting prediction bias correction, a method which makes predictions of past
and new classes more comparable. It was proposed when a memory is allowed and
cannot be directly used without memory, since samples of past classes are
required. We introduce a two-step learning process which allows the transfer of
bias correction parameters between reference and target datasets. Bias
correction is first optimized offline on reference datasets which have an
associated validation memory. The obtained correction parameters are then
transferred to target datasets, for which no memory is available. The second
contribution is to introduce a finer modeling of bias correction by learning
its parameters per incremental state instead of the usual past vs. new class
modeling. The proposed dataset knowledge transfer is applicable to any
incremental method which works without memory. We test its effectiveness by
applying it to four existing methods. Evaluation with four target datasets and
different configurations shows consistent improvement, with practically no
computational and memory overhead.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep learning-based detection of intravenous contrast in computed tomography scans. (arXiv:2110.08424v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08424">
<div class="article-summary-box-inner">
<span><p>Purpose: Identifying intravenous (IV) contrast use within CT scans is a key
component of data curation for model development and testing. Currently, IV
contrast is poorly documented in imaging metadata and necessitates manual
correction and annotation by clinician experts, presenting a major barrier to
imaging analyses and algorithm deployment. We sought to develop and validate a
convolutional neural network (CNN)-based deep learning (DL) platform to
identify IV contrast within CT scans. Methods: For model development and
evaluation, we used independent datasets of CT scans of head, neck (HN) and
lung cancer patients, totaling 133,480 axial 2D scan slices from 1,979 CT scans
manually annotated for contrast presence by clinical experts. Five different DL
models were adopted and trained in HN training datasets for slice-level
contrast detection. Model performances were evaluated on a hold-out set and on
an independent validation set from another institution. DL models was then
fine-tuned on chest CT data and externally validated on a separate chest CT
dataset. Results: Initial DICOM metadata tags for IV contrast were missing or
erroneous in 1,496 scans (75.6%). The EfficientNetB4-based model showed the
best overall detection performance. For HN scans, AUC was 0.996 in the internal
validation set (n = 216) and 1.0 in the external validation set (n = 595). The
fine-tuned model on chest CTs yielded an AUC: 1.0 for the internal validation
set (n = 53), and AUC: 0.980 for the external validation set (n = 402).
Conclusion: The DL model could accurately detect IV contrast in both HN and
chest CT scans with near-perfect performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COVID-19 Detection in Chest X-ray Images Using Swin-Transformer and Transformer in Transformer. (arXiv:2110.08427v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08427">
<div class="article-summary-box-inner">
<span><p>The Coronavirus Disease 2019 (COVID-19) has spread globally and caused
serious damages. Chest X-ray images are widely used for COVID-19 diagnosis and
Artificial Intelligence method can assist to increase the efficiency and
accuracy. In the Challenge of Chest XR COVID-19 detection in Ethics and
Explainability for Responsible Data Science (EE-RDS) conference 2021, we
proposed a method which combined Swin Transformer and Transformer in
Transformer to classify chest X-ray images as three classes: COVID-19,
Pneumonia and Normal (healthy) and achieved 0.9475 accuracy on test set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TorchEsegeta: Framework for Interpretability and Explainability of Image-based Deep Learning Models. (arXiv:2110.08429v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08429">
<div class="article-summary-box-inner">
<span><p>Clinicians are often very sceptical about applying automatic image processing
approaches, especially deep learning based methods, in practice. One main
reason for this is the black-box nature of these approaches and the inherent
problem of missing insights of the automatically derived decisions. In order to
increase trust in these methods, this paper presents approaches that help to
interpret and explain the results of deep learning algorithms by depicting the
anatomical areas which influence the decision of the algorithm most. Moreover,
this research presents a unified framework, TorchEsegeta, for applying various
interpretability and explainability techniques for deep learning models and
generate visual interpretations and explanations for clinicians to corroborate
their clinical findings. In addition, this will aid in gaining confidence in
such methods. The framework builds on existing interpretability and
explainability techniques that are currently focusing on classification models,
extending them to segmentation tasks. In addition, these methods have been
adapted to 3D models for volumetric analysis. The proposed framework provides
methods to quantitatively compare visual explanations using infidelity and
sensitivity metrics. This framework can be used by data scientists to perform
post-hoc interpretations and explanations of their models, develop more
explainable tools and present the findings to clinicians to increase their
faith in such models. The proposed framework was evaluated based on a use case
scenario of vessel segmentation models trained on Time-of-fight (TOF) Magnetic
Resonance Angiogram (MRA) images of the human brain. Quantitative and
qualitative results of a comparative study of different models and
interpretability methods are presented. Furthermore, this paper provides an
extensive overview of several existing interpretability and explainability
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Annotated Training for Controllable Image Captioning. (arXiv:2110.08446v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08446">
<div class="article-summary-box-inner">
<span><p>The Controllable Image Captioning (CIC) task aims to generate captions
conditioned on designated control signals. In this paper, we improve CIC from
two aspects: 1) Existing reinforcement training methods are not applicable to
structure-related CIC models due to the fact that the accuracy-based reward
focuses mainly on contents rather than semantic structures. The lack of
reinforcement training prevents the model from generating more accurate and
controllable sentences. To solve the problem above, we propose a novel
reinforcement training method for structure-related CIC models: Self-Annotated
Training (SAT), where a recursive sampling mechanism (RSM) is designed to force
the input control signal to match the actual output sentence. Extensive
experiments conducted on MSCOCO show that our SAT method improves C-Transformer
(XE) on CIDEr-D score from 118.6 to 130.1 in the length-control task and from
132.2 to 142.7 in the tense-control task, while maintaining more than 99$\%$
matching accuracy with the control signal. 2) We introduce a new control
signal: sentence quality. Equipped with it, CIC models are able to generate
captions of different quality levels as needed. Experiments show that without
additional information of ground truth captions, models controlled by the
highest level of sentence quality perform much better in accuracy than baseline
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint 3D Human Shape Recovery from A Single Imag with Bilayer-Graph. (arXiv:2110.08472v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08472">
<div class="article-summary-box-inner">
<span><p>The ability to estimate the 3D human shape and pose from images can be useful
in many contexts. Recent approaches have explored using graph convolutional
networks and achieved promising results. The fact that the 3D shape is
represented by a mesh, an undirected graph, makes graph convolutional networks
a natural fit for this problem. However, graph convolutional networks have
limited representation power. Information from nodes in the graph is passed to
connected neighbors, and propagation of information requires successive graph
convolutions. To overcome this limitation, we propose a dual-scale graph
approach. We use a coarse graph, derived from a dense graph, to estimate the
human's 3D pose, and the dense graph to estimate the 3D shape. Information in
coarse graphs can be propagated over longer distances compared to dense graphs.
In addition, information about pose can guide to recover local shape detail and
vice versa. We recognize that the connection between coarse and dense is itself
a graph, and introduce graph fusion blocks to exchange information between
graphs with different scales. We train our model end-to-end and show that we
can achieve state-of-the-art results for several evaluation datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Good Prompt Is Worth Millions of Parameters? Low-resource Prompt-based Learning for Vision-Language Models. (arXiv:2110.08484v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08484">
<div class="article-summary-box-inner">
<span><p>Large pretrained vision-language (VL) models can learn a new task with a
handful of examples or generalize to a new task without fine-tuning. However,
these gigantic VL models are hard to deploy for real-world applications due to
their impractically huge model size and slow inference speed. In this work, we
propose FewVLM, a few-shot prompt-based learner on vision-language tasks. We
pretrain a sequence-to-sequence Transformer model with both prefix language
modeling (PrefixLM) and masked language modeling (MaskedLM), and introduce
simple prompts to improve zero-shot and few-shot performance on VQA and image
captioning. Experimental results on five VQA and captioning datasets show that
\method\xspace outperforms Frozen which is 31 times larger than ours by 18.2%
point on zero-shot VQAv2 and achieves comparable results to a 246$\times$
larger model, PICa. We observe that (1) prompts significantly affect zero-shot
performance but marginally affect few-shot performance, (2) MaskedLM helps
few-shot VQA tasks while PrefixLM boosts captioning performance, and (3)
performance significantly increases when training set size is small.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Procedural Knowledge by Sequencing Multimodal Instructional Manuals. (arXiv:2110.08486v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08486">
<div class="article-summary-box-inner">
<span><p>The ability to sequence unordered events is an essential skill to comprehend
and reason about real world task procedures, which often requires thorough
understanding of temporal common sense and multimodal information, as these
procedures are often communicated through a combination of texts and images.
Such capability is essential for applications such as sequential task planning
and multi-source instruction summarization. While humans are capable of
reasoning about and sequencing unordered multimodal procedural instructions,
whether current machine learning models have such essential capability is still
an open question. In this work, we benchmark models' capability of reasoning
over and sequencing unordered multimodal instructions by curating datasets from
popular online instructional manuals and collecting comprehensive human
annotations. We find models not only perform significantly worse than humans
but also seem incapable of efficiently utilizing the multimodal information. To
improve machines' performance on multimodal event sequencing, we propose
sequentiality-aware pretraining techniques that exploit the sequential
alignment properties of both texts and images, resulting in &gt; 5% significant
improvements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grayscale Based Algorithm for Remote Sensing with Deep Learning. (arXiv:2110.08493v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08493">
<div class="article-summary-box-inner">
<span><p>Remote sensing is the image acquisition of a target without having physical
contact with it. Nowadays remote sensing data is widely preferred due to its
reduced image acquisition period. The remote sensing of ground targets is more
challenging because of the various factors that affect the propagation of light
through different mediums from a satellite acquisition. Several Convolutional
Neural Network-based algorithms are being implemented in the field of remote
sensing. Supervised learning is a machine learning technique where the data is
labelled according to their classes prior to the training. In order to detect
and classify the targets more accurately, YOLOv3, an algorithm based on
bounding and anchor boxes is adopted. In order to handle the various effects of
light travelling through the atmosphere, Grayscale based YOLOv3 configuration
is introduced. For better prediction and for solving the Rayleigh scattering
effect, RGB based grayscale algorithms are proposed. The acquired images are
analysed and trained with the grayscale based YOLO3 algorithm for target
detection. The results show that the grayscale-based method can sense the
target more accurately and effectively than the traditional YOLOv3 approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hybrid Mutimodal Fusion for Dimensional Emotion Recognition. (arXiv:2110.08495v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08495">
<div class="article-summary-box-inner">
<span><p>In this paper, we extensively present our solutions for the MuSe-Stress
sub-challenge and the MuSe-Physio sub-challenge of Multimodal Sentiment
Challenge (MuSe) 2021. The goal of MuSe-Stress sub-challenge is to predict the
level of emotional arousal and valence in a time-continuous manner from
audio-visual recordings and the goal of MuSe-Physio sub-challenge is to predict
the level of psycho-physiological arousal from a) human annotations fused with
b) galvanic skin response (also known as Electrodermal Activity (EDA)) signals
from the stressed people. The Ulm-TSST dataset which is a novel subset of the
audio-visual textual Ulm-Trier Social Stress dataset that features German
speakers in a Trier Social Stress Test (TSST) induced stress situation is used
in both sub-challenges. For the MuSe-Stress sub-challenge, we highlight our
solutions in three aspects: 1) the audio-visual features and the bio-signal
features are used for emotional state recognition. 2) the Long Short-Term
Memory (LSTM) with the self-attention mechanism is utilized to capture complex
temporal dependencies within the feature sequences. 3) the late fusion strategy
is adopted to further boost the model's recognition performance by exploiting
complementary information scattered across multimodal sequences. Our proposed
model achieves CCC of 0.6159 and 0.4609 for valence and arousal respectively on
the test set, which both rank in the top 3. For the MuSe-Physio sub-challenge,
we first extract the audio-visual features and the bio-signal features from
multiple modalities. Then, the LSTM module with the self-attention mechanism,
and the Gated Convolutional Neural Networks (GCNN) as well as the LSTM network
are utilized for modeling the complex temporal dependencies in the sequence.
Finally, the late fusion strategy is used. Our proposed method also achieves
CCC of 0.5412 on the test set, which ranks in the top 3.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BAPGAN: GAN-based Bone Age Progression of Femur and Phalange X-ray Images. (arXiv:2110.08509v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08509">
<div class="article-summary-box-inner">
<span><p>Convolutional Neural Networks play a key role in bone age assessment for
investigating endocrinology, genetic, and growth disorders under various
modalities and body regions. However, no researcher has tackled bone age
progression/regression despite its valuable potential applications:
bone-related disease diagnosis, clinical knowledge acquisition, and museum
education. Therefore, we propose Bone Age Progression Generative Adversarial
Network (BAPGAN) to progress/regress both femur/phalange X-ray images while
preserving identity and realism. We exhaustively confirm the BAPGAN's clinical
potential via Frechet Inception Distance, Visual Turing Test by two expert
orthopedists, and t-Distributed Stochastic Neighbor Embedding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Dialogue Response Generation. (arXiv:2110.08515v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08515">
<div class="article-summary-box-inner">
<span><p>Responsing with image has been recognized as an important capability for an
intelligent conversational agent. Yet existing works only focus on exploring
the multimodal dialogue models which depend on retrieval-based methods, but
neglecting generation methods. To fill in the gaps, we first present a
multimodal dialogue generation model, which takes the dialogue history as
input, then generates a textual sequence or an image as response. Learning such
a model often requires multimodal dialogues containing both texts and images
which are difficult to obtain. Motivated by the challenge in practice, we
consider multimodal dialogue generation under a natural assumption that only
limited training examples are available. In such a low-resource setting, we
devise a novel conversational agent, Divter, in order to isolate parameters
that depend on multimodal dialogues from the entire generation model. By this
means, the major part of the model can be learned from a large number of
text-only dialogues and text-image pairs respectively, then the whole
parameters can be well fitted using the limited training examples. Extensive
experiments demonstrate our method achieves state-of-the-art results in both
automatic and human evaluation, and can generate informative text and
high-resolution image responses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Locally Adaptive Structure and Texture Similarity for Image Quality Assessment. (arXiv:2110.08521v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08521">
<div class="article-summary-box-inner">
<span><p>The latest advances in full-reference image quality assessment (IQA) involve
unifying structure and texture similarity based on deep representations. The
resulting Deep Image Structure and Texture Similarity (DISTS) metric, however,
makes rather global quality measurements, ignoring the fact that natural
photographic images are locally structured and textured across space and scale.
In this paper, we describe a locally adaptive structure and texture similarity
index for full-reference IQA, which we term A-DISTS. Specifically, we rely on a
single statistical feature, namely the dispersion index, to localize texture
regions at different scales. The estimated probability (of one patch being
texture) is in turn used to adaptively pool local structure and texture
measurements. The resulting A-DISTS is adapted to local image content, and is
free of expensive human perceptual scores for supervised training. We
demonstrate the advantages of A-DISTS in terms of correlation with human data
on ten IQA databases and optimization of single image super-resolution methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-View Stereo Network with attention thin volume. (arXiv:2110.08556v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08556">
<div class="article-summary-box-inner">
<span><p>We propose an efficient multi-view stereo (MVS) network for infering depth
value from multiple RGB images. Recent studies have shown that mapping the
geometric relationship in real space to neural network is an essential topic of
the MVS problem. Specifically, these methods focus on how to express the
correspondence between different views by constructing a nice cost volume. In
this paper, we propose a more complete cost volume construction approach based
on absorbing previous experience. First of all, we introduce the self-attention
mechanism to fully aggregate the dominant information from input images and
accurately model the long-range dependency, so as to selectively aggregate
reference features. Secondly, we introduce the group-wise correlation to
feature aggregation, which greatly reduces the memory and calculation burden.
Meanwhile, this method enhances the information interaction between different
feature channels. With this approach, a more lightweight and efficient cost
volume is constructed. Finally we follow the coarse to fine strategy and refine
the depth sampling range scale by scale with the help of uncertainty
estimation. We further combine the previous steps to get the attention thin
volume. Quantitative and qualitative experiments are presented to demonstrate
the performance of our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Network Pruning Through Constrained Reinforcement Learning. (arXiv:2110.08558v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08558">
<div class="article-summary-box-inner">
<span><p>Network pruning reduces the size of neural networks by removing (pruning)
neurons such that the performance drop is minimal. Traditional pruning
approaches focus on designing metrics to quantify the usefulness of a neuron
which is often quite tedious and sub-optimal. More recent approaches have
instead focused on training auxiliary networks to automatically learn how
useful each neuron is however, they often do not take computational limitations
into account. In this work, we propose a general methodology for pruning neural
networks. Our proposed methodology can prune neural networks to respect
pre-defined computational budgets on arbitrary, possibly non-differentiable,
functions. Furthermore, we only assume the ability to be able to evaluate these
functions for different inputs, and hence they do not need to be fully
specified beforehand. We achieve this by proposing a novel pruning strategy via
constrained reinforcement learning algorithms. We prove the effectiveness of
our approach via comparison with state-of-the-art methods on standard image
classification datasets. Specifically, we reduce 83-92.90 of total parameters
on various variants of VGG while achieving comparable or better performance
than that of original networks. We also achieved 75.09 reduction in parameters
on ResNet18 without incurring any loss in accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BNAS v2: Learning Architectures for Binary Networks with Empirical Improvements. (arXiv:2110.08562v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08562">
<div class="article-summary-box-inner">
<span><p>Backbone architectures of most binary networks are well-known floating point
(FP) architectures such as the ResNet family. Questioning that the
architectures designed for FP networks might not be the best for binary
networks, we propose to search architectures for binary networks (BNAS) by
defining a new search space for binary architectures and a novel search
objective. Specifically, based on the cell based search method, we define the
new search space of binary layer types, design a new cell template, and
rediscover the utility of and propose to use the Zeroise layer instead of using
it as a placeholder. The novel search objective diversifies early search to
learn better performing binary architectures. We show that our method searches
architectures with stable training curves despite the quantization error
inherent in binary networks. Quantitative analyses demonstrate that our
searched architectures outperform the architectures used in state-of-the-art
binary networks and outperform or perform on par with state-of-the-art binary
networks that employ various techniques other than architectural changes. In
addition, we further propose improvements to the training scheme of our
searched architectures. With the new training scheme for our searched
architectures, we achieve the state-of-the-art performance by binary networks
by outperforming all previous methods by non-trivial margins.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ASFormer: Transformer for Action Segmentation. (arXiv:2110.08568v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08568">
<div class="article-summary-box-inner">
<span><p>Algorithms for the action segmentation task typically use temporal models to
predict what action is occurring at each frame for a minute-long daily
activity. Recent studies have shown the potential of Transformer in modeling
the relations among elements in sequential data. However, there are several
major concerns when directly applying the Transformer to the action
segmentation task, such as the lack of inductive biases with small training
sets, the deficit in processing long input sequence, and the limitation of the
decoder architecture to utilize temporal relations among multiple action
segments to refine the initial predictions. To address these concerns, we
design an efficient Transformer-based model for action segmentation task, named
ASFormer, with three distinctive characteristics: (i) We explicitly bring in
the local connectivity inductive priors because of the high locality of
features. It constrains the hypothesis space within a reliable scope, and is
beneficial for the action segmentation task to learn a proper target function
with small training sets. (ii) We apply a pre-defined hierarchical
representation pattern that efficiently handles long input sequences. (iii) We
carefully design the decoder to refine the initial predictions from the
encoder. Extensive experiments on three public datasets demonstrate that
effectiveness of our methods. Code is available at
\url{https://github.com/ChinaYi/ASFormer}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Image Debanding. (arXiv:2110.08569v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08569">
<div class="article-summary-box-inner">
<span><p>Banding or false contour is an annoying visual artifact whose impact is even
more pronounced in ultra high definition, high dynamic range, and wide colour
gamut visual content, which is becoming increasingly popular. Since users
associate a heightened expectation of quality with such content and banding
leads to deteriorated visual quality-of-experience, the area of banding removal
or debanding has taken paramount importance. Existing debanding approaches are
mostly knowledge-driven. Despite the widespread success of deep learning in
other areas of image processing and computer vision, data-driven debanding
approaches remain surprisingly missing. In this work, we make one of the first
attempts to develop a deep learning based banding artifact removal method for
images and name it deep debanding network (deepDeband). For its training, we
construct a large-scale dataset of 51,490 pairs of corresponding pristine and
banded image patches. Performance evaluation shows that deepDeband is
successful at greatly reducing banding artifacts in images, outperforming
existing methods both quantitatively and visually.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explore before Moving: A Feasible Path Estimation and Memory Recalling Framework for Embodied Navigation. (arXiv:2110.08571v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08571">
<div class="article-summary-box-inner">
<span><p>An embodied task such as embodied question answering (EmbodiedQA), requires
an agent to explore the environment and collect clues to answer a given
question that related with specific objects in the scene. The solution of such
task usually includes two stages, a navigator and a visual Q&amp;A module. In this
paper, we focus on the navigation and solve the problem of existing navigation
algorithms lacking experience and common sense, which essentially results in a
failure finding target when robot is spawn in unknown environments.
</p>
<p>Inspired by the human ability to think twice before moving and conceive
several feasible paths to seek a goal in unfamiliar scenes, we present a route
planning method named Path Estimation and Memory Recalling (PEMR) framework.
PEMR includes a "looking ahead" process, \textit{i.e.} a visual feature
extractor module that estimates feasible paths for gathering 3D navigational
information, which is mimicking the human sense of direction. PEMR contains
another process ``looking behind'' process that is a memory recall mechanism
aims at fully leveraging past experience collected by the feature extractor.
Last but not the least, to encourage the navigator to learn more accurate prior
expert experience, we improve the original benchmark dataset and provide a
family of evaluation metrics for diagnosing both navigation and question
answering modules. We show strong experimental results of PEMR on the
EmbodiedQA navigation task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visual-aware Attention Dual-stream Decoder for Video Captioning. (arXiv:2110.08578v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08578">
<div class="article-summary-box-inner">
<span><p>Video captioning is a challenging task that captures different visual parts
and describes them in sentences, for it requires visual and linguistic
coherence. The attention mechanism in the current video captioning method
learns to assign weight to each frame, promoting the decoder dynamically. This
may not explicitly model the correlation and the temporal coherence of the
visual features extracted in the sequence frames.To generate semantically
coherent sentences, we propose a new Visual-aware Attention (VA) model, which
concatenates dynamic changes of temporal sequence frames with the words at the
previous moment, as the input of attention mechanism to extract sequence
features.In addition, the prevalent approaches widely use the teacher-forcing
(TF) learning during training, where the next token is generated conditioned on
the previous ground-truth tokens. The semantic information in the previously
generated tokens is lost. Therefore, we design a self-forcing (SF) stream that
takes the semantic information in the probability distribution of the previous
token as input to enhance the current token.The Dual-stream Decoder (DD)
architecture unifies the TF and SF streams, generating sentences to promote the
annotated captioning for both streams.Meanwhile, with the Dual-stream Decoder
utilized, the exposure bias problem is alleviated, caused by the discrepancy
between the training and testing in the TF learning.The effectiveness of the
proposed Visual-aware Attention Dual-stream Decoder (VADD) is demonstrated
through the result of experimental studies on Microsoft video description
(MSVD) corpus and MSR-Video to text (MSR-VTT) datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Intelligent Video Editing: Incorporating Modern Talking Face Generation Algorithms in a Video Editor. (arXiv:2110.08580v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08580">
<div class="article-summary-box-inner">
<span><p>This paper proposes a video editor based on OpenShot with several
state-of-the-art facial video editing algorithms as added functionalities. Our
editor provides an easy-to-use interface to apply modern lip-syncing algorithms
interactively. Apart from lip-syncing, the editor also uses audio and facial
re-enactment to generate expressive talking faces. The manual control improves
the overall experience of video editing without missing out on the benefits of
modern synthetic video generation algorithms. This control enables us to
lip-sync complex dubbed movie scenes, interviews, television shows, and other
visual content. Furthermore, our editor provides features that automatically
translate lectures from spoken content, lip-sync of the professor, and
background content like slides. While doing so, we also tackle the critical
aspect of synchronizing background content with the translated speech. We
qualitatively evaluate the usefulness of the proposed editor by conducting
human evaluations. Our evaluations show a clear improvement in the efficiency
of using human editors and an improved video generation quality. We attach demo
videos with the supplementary material clearly explaining the tool and also
showcasing multiple results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pseudo-label refinement using superpixels for semi-supervised brain tumour segmentation. (arXiv:2110.08589v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08589">
<div class="article-summary-box-inner">
<span><p>Training neural networks using limited annotations is an important problem in
the medical domain. Deep Neural Networks (DNNs) typically require large,
annotated datasets to achieve acceptable performance which, in the medical
domain, are especially difficult to obtain as they require significant time
from expert radiologists. Semi-supervised learning aims to overcome this
problem by learning segmentations with very little annotated data, whilst
exploiting large amounts of unlabelled data. However, the best-known technique,
which utilises inferred pseudo-labels, is vulnerable to inaccurate
pseudo-labels degrading the performance. We propose a framework based on
superpixels - meaningful clusters of adjacent pixels - to improve the accuracy
of the pseudo labels and address this issue. Our framework combines superpixels
with semi-supervised learning, refining the pseudo-labels during training using
the features and edges of the superpixel maps. This method is evaluated on a
multimodal magnetic resonance imaging (MRI) dataset for the task of brain
tumour region segmentation. Our method demonstrates improved performance over
the standard semi-supervised pseudo-labelling baseline when there is a reduced
annotator burden and only 5 annotated patients are available. We report
DSC=0.824 and DSC=0.707 for the test set whole tumour and tumour core regions
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Remote Sensing Forest Inventory Using Satelite Imagery. (arXiv:2110.08590v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08590">
<div class="article-summary-box-inner">
<span><p>For many countries like Russia, Canada, or the USA, a robust and detailed
tree species inventory is essential to manage their forests sustainably. Since
one can not apply unmanned aerial vehicle (UAV) imagery-based approaches to
large-scale forest inventory applications, the utilization of machine learning
algorithms on satellite imagery is a rising topic of research. Although
satellite imagery quality is relatively low, additional spectral channels
provide a sufficient amount of information for tree crown classification tasks.
Assuming that tree crowns are detected already, we use embeddings of tree
crowns generated by Autoencoders as a data set to train classical Machine
Learning algorithms. We compare our Autoencoder (AE) based approach to
traditional convolutional neural networks (CNN) end-to-end classifiers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A MIMO Radar-based Few-Shot Learning Approach for Human-ID. (arXiv:2110.08595v1 [eess.SP])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08595">
<div class="article-summary-box-inner">
<span><p>Radar for deep learning-based human identification has become a research area
of increasing interest. It has been shown that micro-Doppler (\(\upmu\)-D) can
reflect the walking behavior through capturing the periodic limbs'
micro-motions. One of the main aspects is maximizing the number of included
classes while considering the real-time and training dataset size constraints.
In this paper, a multiple-input-multiple-output (MIMO) radar is used to
formulate micro-motion spectrograms of the elevation angular velocity
(\(\upmu\)-\(\omega\)). The effectiveness of concatenating this
newly-formulated spectrogram with the commonly used \(\upmu\)-D is
investigated. To accommodate for non-constrained real walking motion, an
adaptive cycle segmentation framework is utilized and a metric learning network
is trained on half gait cycles (\(\approx\) 0.5 s). Studies on the effects of
various numbers of classes (5--20), different dataset sizes, and varying
observation time windows 1--2 s are conducted. A non-constrained walking
dataset of 22 subjects is collected with different aspect angles with respect
to the radar. The proposed few-shot learning (FSL) approach achieves a
classification error of 11.3 % with only 2 min of training data per subject.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mapping illegal waste dumping sites with neural-network classification of satellite imagery. (arXiv:2110.08599v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08599">
<div class="article-summary-box-inner">
<span><p>Public health and habitat quality are crucial goals of urban planning. In
recent years, the severe social and environmental impact of illegal waste
dumping sites has made them one of the most serious problems faced by cities in
the Global South, in a context of scarce information available for decision
making. To help identify the location of dumping sites and track their
evolution over time we adopt a data-driven model from the machine learning
domain, analyzing satellite images. This allows us to take advantage of the
increasing availability of geo-spatial open-data, high-resolution satellite
imagery, and open source tools to train machine learning algorithms with a
small set of known waste dumping sites in Buenos Aires, and then predict the
location of other sites over vast areas at high speed and low cost. This case
study shows the results of a collaboration between Dymaxion Labs and
Fundaci\'on Bunge y Born to harness this technique in order to create a
comprehensive map of potential locations of illegal waste dumping sites in the
region.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MAAD: A Model and Dataset for "Attended Awareness" in Driving. (arXiv:2110.08610v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08610">
<div class="article-summary-box-inner">
<span><p>We propose a computational model to estimate a person's attended awareness of
their environment. We define attended awareness to be those parts of a
potentially dynamic scene which a person has attended to in recent history and
which they are still likely to be physically aware of. Our model takes as input
scene information in the form of a video and noisy gaze estimates, and outputs
visual saliency, a refined gaze estimate, and an estimate of the person's
attended awareness. In order to test our model, we capture a new dataset with a
high-precision gaze tracker including 24.5 hours of gaze sequences from 23
subjects attending to videos of driving scenes. The dataset also contains
third-party annotations of the subjects' attended awareness based on
observations of their scan path. Our results show that our model is able to
reasonably estimate attended awareness in a controlled setting, and in the
future could potentially be extended to real egocentric driving data to help
enable more effective ahead-of-time warnings in safety systems and thereby
augment driver performance. We also demonstrate our model's effectiveness on
the tasks of saliency, gaze calibration, and denoising, using both our dataset
and an existing saliency dataset. We make our model and dataset available at
https://github.com/ToyotaResearchInstitute/att-aware/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SAGAN: Adversarial Spatial-asymmetric Attention for Noisy Nona-Bayer Reconstruction. (arXiv:2110.08619v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08619">
<div class="article-summary-box-inner">
<span><p>Nona-Bayer colour filter array (CFA) pattern is considered one of the most
viable alternatives to traditional Bayer patterns. Despite the substantial
advantages, such non-Bayer CFA patterns are susceptible to produce visual
artefacts while reconstructing RGB images from noisy sensor data. This study
addresses the challenges of learning RGB image reconstruction from noisy
Nona-Bayer CFA comprehensively. We propose a novel spatial-asymmetric attention
module to jointly learn bi-direction transformation and large-kernel global
attention to reduce the visual artefacts. We combine our proposed module with
adversarial learning to produce plausible images from Nona-Bayer CFA. The
feasibility of the proposed method has been verified and compared with the
state-of-the-art image reconstruction method. The experiments reveal that the
proposed method can reconstruct RGB images from noisy Nona-Bayer CFA without
producing any visually disturbing artefacts. Also, it can outperform the
state-of-the-art image reconstruction method in both qualitative and
quantitative comparison. Code available:
https://github.com/sharif-apu/SAGAN_BMVC21.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DPC: Unsupervised Deep Point Correspondence via Cross and Self Construction. (arXiv:2110.08636v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08636">
<div class="article-summary-box-inner">
<span><p>We present a new method for real-time non-rigid dense correspondence between
point clouds based on structured shape construction. Our method, termed Deep
Point Correspondence (DPC), requires a fraction of the training data compared
to previous techniques and presents better generalization capabilities. Until
now, two main approaches have been suggested for the dense correspondence
problem. The first is a spectral-based approach that obtains great results on
synthetic datasets but requires mesh connectivity of the shapes and long
inference processing time while being unstable in real-world scenarios. The
second is a spatial approach that uses an encoder-decoder framework to regress
an ordered point cloud for the matching alignment from an irregular input.
Unfortunately, the decoder brings considerable disadvantages, as it requires a
large amount of training data and struggles to generalize well in cross-dataset
evaluations. DPC's novelty lies in its lack of a decoder component. Instead, we
use latent similarity and the input coordinates themselves to construct the
point cloud and determine correspondence, replacing the coordinate regression
done by the decoder. Extensive experiments show that our construction scheme
leads to a performance boost in comparison to recent state-of-the-art
correspondence methods. Our code is publicly available at
https://github.com/dvirginz/DPC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Face Verification with Challenging Imposters and Diversified Demographics. (arXiv:2110.08667v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08667">
<div class="article-summary-box-inner">
<span><p>Face verification aims to distinguish between genuine and imposter pairs of
faces, which include the same or different identities, respectively. The
performance reported in recent years gives the impression that the task is
practically solved. Here, we revisit the problem and argue that existing
evaluation datasets were built using two oversimplifying design choices. First,
the usual identity selection to form imposter pairs is not challenging enough
because, in practice, verification is needed to detect challenging imposters.
Second, the underlying demographics of existing datasets are often insufficient
to account for the wide diversity of facial characteristics of people from
across the world. To mitigate these limitations, we introduce the $FaVCI2D$
dataset. Imposter pairs are challenging because they include visually similar
faces selected from a large pool of demographically diversified identities. The
dataset also includes metadata related to gender, country and age to facilitate
fine-grained analysis of results. $FaVCI2D$ is generated from freely
distributable resources. Experiments with state-of-the-art deep models that
provide nearly 100\% performance on existing datasets show a significant
performance drop for $FaVCI2D$, confirming our starting hypothesis. Equally
important, we analyze legal and ethical challenges which appeared in recent
years and hindered the development of face analysis research. We introduce a
series of design choices which address these challenges and make the dataset
constitution and usage more sustainable and fairer. $FaVCI2D$ is available
at~\url{https://github.com/AIMultimediaLab/FaVCI2D-Face-Verification-with-Challenging-Imposters-and-Diversified-Demographics}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Resource Efficient 3D Convolutional Neural Networks. (arXiv:1904.02422v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1904.02422">
<div class="article-summary-box-inner">
<span><p>Recently, convolutional neural networks with 3D kernels (3D CNNs) have been
very popular in computer vision community as a result of their superior ability
of extracting spatio-temporal features within video frames compared to 2D CNNs.
Although there has been great advances recently to build resource efficient 2D
CNN architectures considering memory and power budget, there is hardly any
similar resource efficient architectures for 3D CNNs. In this paper, we have
converted various well-known resource efficient 2D CNNs to 3D CNNs and
evaluated their performance on three major benchmarks in terms of
classification accuracy for different complexity levels. We have experimented
on (1) Kinetics-600 dataset to inspect their capacity to learn, (2) Jester
dataset to inspect their ability to capture motion patterns, and (3) UCF-101 to
inspect the applicability of transfer learning. We have evaluated the run-time
performance of each model on a single Titan XP GPU and a Jetson TX2 embedded
system. The results of this study show that these models can be utilized for
different types of real-world applications since they provide real-time
performance with considerable accuracies and memory usage. Our analysis on
different complexity levels shows that the resource efficient 3D CNNs should
not be designed too shallow or narrow in order to save complexity. The codes
and pretrained models used in this work are publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">You Only Watch Once: A Unified CNN Architecture for Real-Time Spatiotemporal Action Localization. (arXiv:1911.06644v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1911.06644">
<div class="article-summary-box-inner">
<span><p>Spatiotemporal action localization requires the incorporation of two sources
of information into the designed architecture: (1) temporal information from
the previous frames and (2) spatial information from the key frame. Current
state-of-the-art approaches usually extract these information with separate
networks and use an extra mechanism for fusion to get detections. In this work,
we present YOWO, a unified CNN architecture for real-time spatiotemporal action
localization in video streams. YOWO is a single-stage architecture with two
branches to extract temporal and spatial information concurrently and predict
bounding boxes and action probabilities directly from video clips in one
evaluation. Since the whole architecture is unified, it can be optimized
end-to-end. The YOWO architecture is fast providing 34 frames-per-second on
16-frames input clips and 62 frames-per-second on 8-frames input clips, which
is currently the fastest state-of-the-art architecture on spatiotemporal action
localization task. Remarkably, YOWO outperforms the previous state-of-the art
results on J-HMDB-21 and UCF101-24 with an impressive improvement of ~3% and
~12%, respectively. Moreover, YOWO is the first and only single-stage
architecture that provides competitive results on AVA dataset. We make our code
and pretrained models publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nonparametric Continuous Sensor Registration. (arXiv:2001.04286v4 [math.OC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2001.04286">
<div class="article-summary-box-inner">
<span><p>This paper develops a new mathematical framework that enables nonparametric
joint semantic and geometric representation of continuous functions using data.
The joint embedding is modeled by representing the processes in a reproducing
kernel Hilbert space. The functions can be defined on arbitrary smooth
manifolds where the action of a Lie group aligns them. The continuous functions
allow the registration to be independent of a specific signal resolution. The
framework is fully analytical with a closed-form derivation of the Riemannian
gradient and Hessian. We study a more specialized but widely used case where
the Lie group acts on functions isometrically. We solve the problem by
maximizing the inner product between two functions defined over data, while the
continuous action of the rigid body motion Lie group is captured through the
integration of the flow in the corresponding Lie algebra. Low-dimensional cases
are derived with numerical examples to show the generality of the proposed
framework. The high-dimensional derivation for the special Euclidean group
acting on the Euclidean space showcases the point cloud registration and
bird's-eye view map registration abilities. An implementation of this framework
for RGB-D cameras outperforms the state-of-the-art robust visual odometry and
performs well in texture and structure-scarce environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the benefits of defining vicinal distributions in latent space. (arXiv:2003.06566v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.06566">
<div class="article-summary-box-inner">
<span><p>The vicinal risk minimization (VRM) principle is an empirical risk
minimization (ERM) variant that replaces Dirac masses with vicinal functions.
There is strong numerical and theoretical evidence showing that VRM outperforms
ERM in terms of generalization if appropriate vicinal functions are chosen.
Mixup Training (MT), a popular choice of vicinal distribution, improves the
generalization performance of models by introducing globally linear behavior in
between training examples. Apart from generalization, recent works have shown
that mixup trained models are relatively robust to input
perturbations/corruptions and at the same time are calibrated better than their
non-mixup counterparts. In this work, we investigate the benefits of defining
these vicinal distributions like mixup in latent space of generative models
rather than in input space itself. We propose a new approach - \textit{VarMixup
(Variational Mixup)} - to better sample mixup images by using the latent
manifold underlying the data. Our empirical studies on CIFAR-10, CIFAR-100, and
Tiny-ImageNet demonstrate that models trained by performing mixup in the latent
manifold learned by VAEs are inherently more robust to various input
corruptions/perturbations, are significantly better calibrated, and exhibit
more local-linear loss landscapes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modulating Bottom-Up and Top-Down Visual Processing via Language-Conditional Filters. (arXiv:2003.12739v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.12739">
<div class="article-summary-box-inner">
<span><p>How to best integrate linguistic and perceptual processing in multi-modal
tasks that involve language and vision is an important open problem. In this
work, we argue that the common practice of using language in a top-down manner,
to direct visual attention over high-level visual features, may not be optimal.
We hypothesize that the use of language to also condition the bottom-up
processing from pixels to high-level features can provide benefits to the
overall performance. To support our claim, we propose a model for
language-vision problems involving dense prediction, and perform experiments on
two different multi-modal tasks: image segmentation from referring expressions
and language-guided image colorization. We compare results where either one or
both of the top-down and bottom-up visual branches are conditioned on language.
Our experiments reveal that using language to control the filters for bottom-up
visual processing in addition to top-down attention leads to better results on
both tasks and achieves state-of-the-art performance. Our analysis of different
word types in input expressions suggest that the bottom-up conditioning is
especially helpful in the presence of low level visual concepts like color.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speak2Label: Using Domain Knowledge for Creating a Large Scale Driver Gaze Zone Estimation Dataset. (arXiv:2004.05973v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.05973">
<div class="article-summary-box-inner">
<span><p>Labelling of human behavior analysis data is a complex and time consuming
task. In this paper, a fully automatic technique for labelling an image based
gaze behavior dataset for driver gaze zone estimation is proposed. Domain
knowledge is added to the data recording paradigm and later labels are
generated in an automatic manner using Speech To Text conversion (STT). In
order to remove the noise in the STT process due to different illumination and
ethnicity of subjects in our data, the speech frequency and energy are
analysed. The resultant Driver Gaze in the Wild (DGW) dataset contains 586
recordings, captured during different times of the day including evenings. The
large scale dataset contains 338 subjects with an age range of 18-63 years. As
the data is recorded in different lighting conditions, an illumination robust
layer is proposed in the Convolutional Neural Network (CNN). The extensive
experiments show the variance in the dataset resembling real-world conditions
and the effectiveness of the proposed CNN pipeline. The proposed network is
also fine-tuned for the eye gaze prediction task, which shows the
discriminativeness of the representation learnt by our network on the proposed
DGW dataset. Project Page:
https://sites.google.com/view/drivergazeprediction/home
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Noisy Differentiable Architecture Search. (arXiv:2005.03566v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.03566">
<div class="article-summary-box-inner">
<span><p>Simplicity is the ultimate sophistication. Differentiable Architecture Search
(DARTS) has now become one of the mainstream paradigms of neural architecture
search. However, it largely suffers from the well-known performance collapse
issue due to the aggregation of skip connections. It is thought to have overly
benefited from the residual structure which accelerates the information flow.
To weaken this impact, we propose to inject unbiased random noise to impede the
flow. We name this novel approach NoisyDARTS. In effect, a network optimizer
should perceive this difficulty at each training step and refrain from
overshooting, especially on skip connections. In the long run, since we add no
bias to the gradient in terms of expectation, it is still likely to converge to
the right solution area. We also prove that the injected noise plays a role in
smoothing the loss landscape, which makes the optimization easier. Our method
features extreme simplicity and acts as a new strong baseline. We perform
extensive experiments across various search spaces, datasets, and tasks, where
we robustly achieve state-of-the-art results. Our code is available at
https://github.com/xiaomi-automl/NoisyDARTS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Generative Model for Texture Synthesis based on Optimal Transport between Feature Distributions. (arXiv:2007.03408v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.03408">
<div class="article-summary-box-inner">
<span><p>We propose GOTEX, a general framework for texture synthesis by optimization
that constrains the statistical distribution of local features. While our model
encompasses several existing texture models, we focus on the case where the
comparison between feature distributions relies on optimal transport distances.
We show that the semi-dual formulation of optimal transport allows to control
the distribution of various possible features, even if these features live in a
high-dimensional space. We then study the resulting minimax optimization
problem, which corresponds to a Wasserstein generative model, for which the
inner concave maximization problem can be solved with standard stochastic
gradient methods. The alternate optimization algorithm is shown to be versatile
in terms of applications, features and architecture; in particular it allows to
produce high-quality synthesized textures with different sets of features. We
analyze the results obtained by constraining the distribution of patches or the
distribution of responses to a pre-learned VGG neural network. We show that the
patch representation can retrieve the desired textural aspect in a more precise
manner. We also provide a detailed comparison with state-of-the-art texture
synthesis methods. The GOTEX model based on patch features is also adapted to
texture inpainting and texture interpolation. Finally, we show how to use our
framework to learn a feed-forward neural network that can synthesize on-the-fly
new textures of arbitrary size in a very fast manner. Experimental results and
comparisons with the mainstream methods from the literature illustrate the
relevance of the generative models learned with GOTEX.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dissected 3D CNNs: Temporal Skip Connections for Efficient Online Video Processing. (arXiv:2009.14639v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.14639">
<div class="article-summary-box-inner">
<span><p>Convolutional Neural Networks with 3D kernels (3D-CNNs) currently achieve
state-of-the-art results in video recognition tasks due to their supremacy in
extracting spatiotemporal features within video frames. There have been many
successful 3D-CNN architectures surpassing the state-of-the-art results
successively. However, nearly all of them are designed to operate offline
creating several serious handicaps during online operation. Firstly,
conventional 3D-CNNs are not dynamic since their output features represent the
complete input clip instead of the most recent frame in the clip. Secondly,
they are not temporal resolution-preserving due to their inherent temporal
downsampling. Lastly, 3D-CNNs are constrained to be used with fixed temporal
input size limiting their flexibility. In order to address these drawbacks, we
propose dissected 3D-CNNs, where the intermediate volumes of the network are
dissected and propagated over depth (time) dimension for future calculations,
substantially reducing the number of computations at online operation. For
action classification, the dissected version of ResNet models performs 77-90%
fewer computations at online operation while achieving ~5% better
classification accuracy on the Kinetics-600 dataset than conventional 3D-ResNet
models. Moreover, the advantages of dissected 3D-CNNs are demonstrated by
deploying our approach onto several vision tasks, which consistently improved
the performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Back to the Future: Cycle Encoding Prediction for Self-supervised Contrastive Video Representation Learning. (arXiv:2010.07217v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.07217">
<div class="article-summary-box-inner">
<span><p>In this paper we show that learning video feature spaces in which temporal
cycles are maximally predictable benefits action classification. In particular,
we propose a novel learning approach termed Cycle Encoding Prediction (CEP)
that is able to effectively represent high-level spatio-temporal structure of
unlabelled video content. CEP builds a latent space wherein the concept of
closed forward-backward as well as backward-forward temporal loops is
approximately preserved. As a self-supervision signal, CEP leverages the
bi-directional temporal coherence of the video stream and applies loss
functions that encourage both temporal cycle closure as well as contrastive
feature separation. Architecturally, the underpinning network structure
utilises a single feature encoder for all video snippets, adding two predictive
modules that learn temporal forward and backward transitions. We apply our
framework for pretext training of networks for action recognition tasks. We
report significantly improved results for the standard datasets UCF101 and
HMDB51. Detailed ablation studies support the effectiveness of the proposed
components. We publish source code for the CEP components in full with this
paper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Center-wise Local Image Mixture For Contrastive Representation Learning. (arXiv:2011.02697v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.02697">
<div class="article-summary-box-inner">
<span><p>Contrastive learning based on instance discrimination trains model to
discriminate different transformations of the anchor sample from other samples,
which does not consider the semantic similarity among samples. This paper
proposes a new kind of contrastive learning method, named CLIM, which uses
positives from other samples in the dataset. This is achieved by searching
local similar samples of the anchor, and selecting samples that are closer to
the corresponding cluster center, which we denote as center-wise local image
selection. The selected samples are instantiated via an data mixture strategy,
which performs as a smoothing regularization. As a result, CLIM encourages both
local similarity and global aggregation in a robust way, which we find is
beneficial for feature representation. Besides, we introduce
\emph{multi-resolution} augmentation, which enables the representation to be
scale invariant. We reach 75.5% top-1 accuracy with linear evaluation over
ResNet-50, and 59.3% top-1 accuracy when fine-tuned with only 1% labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Monitoring and Diagnosability of Perception Systems. (arXiv:2011.07010v5 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.07010">
<div class="article-summary-box-inner">
<span><p>Perception is a critical component of high-integrity applications of robotics
and autonomous systems, such as self-driving vehicles. In these applications,
failure of perception systems may put human life at risk, and a broad adoption
of these technologies requires the development of methodologies to guarantee
and monitor safe operation. Despite the paramount importance of perception
systems, currently there is no formal approach for system-level monitoring. In
this work, we propose a mathematical model for runtime monitoring and fault
detection and identification in perception systems. Towards this goal, we draw
connections with the literature on diagnosability in multiprocessor systems,
and generalize it to account for modules with heterogeneous outputs that
interact over time. The resulting temporal diagnostic graphs (i) provide a
framework to reason over the consistency of perception outputs -- across
modules and over time -- thus enabling fault detection, (ii) allow us to
establish formal guarantees on the maximum number of faults that can be
uniquely identified in a given perception system, and (iii) enable the design
of efficient algorithms for fault identification. We demonstrate our monitoring
system, dubbed PerSyS, in realistic simulations using the LGSVL self-driving
simulator and the Apollo Auto autonomy software stack, and show that PerSyS is
able to detect failures in challenging scenarios (including scenarios that have
caused self-driving car accidents in recent years), and is able to correctly
identify faults while entailing a minimal computation overhead (&lt; 5 ms on a
single-core CPU).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Object Detection with Adaptive Clustering Transformer. (arXiv:2011.09315v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.09315">
<div class="article-summary-box-inner">
<span><p>End-to-end Object Detection with Transformer (DETR)proposes to perform object
detection with Transformer and achieve comparable performance with two-stage
object detection like Faster-RCNN. However, DETR needs huge computational
resources for training and inference due to the high-resolution spatial input.
In this paper, a novel variant of transformer named Adaptive Clustering
Transformer(ACT) has been proposed to reduce the computation cost for
high-resolution input. ACT cluster the query features adaptively using Locality
Sensitive Hashing (LSH) and ap-proximate the query-key interaction using the
prototype-key interaction. ACT can reduce the quadratic O(N2) complexity inside
self-attention into O(NK) where K is the number of prototypes in each layer.
ACT can be a drop-in module replacing the original self-attention module
without any training. ACT achieves a good balance between accuracy and
computation cost (FLOPs). The code is available as supplementary for the ease
of experiment replication and verification. Code is released at
\url{https://github.com/gaopengcuhk/SMCA-DETR/}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">4D Human Body Capture from Egocentric Video via 3D Scene Grounding. (arXiv:2011.13341v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.13341">
<div class="article-summary-box-inner">
<span><p>We introduce a novel task of reconstructing a time series of second-person 3D
human body meshes from monocular egocentric videos. The unique viewpoint and
rapid embodied camera motion of egocentric videos raise additional technical
barriers for human body capture. To address those challenges, we propose a
simple yet effective optimization-based approach that leverages 2D observations
of the entire video sequence and human-scene interaction constraint to estimate
second-person human poses, shapes, and global motion that are grounded on the
3D environment captured from the egocentric view. We conduct detailed ablation
studies to validate our design choice. Moreover, we compare our method with the
previous state-of-the-art method on human motion capture from monocular video,
and show that our method estimates more accurate human-body poses and shapes
under the challenging egocentric setting. In addition, we demonstrate that our
approach produces more realistic human-scene interaction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Curiosity-driven 3D Object Detection Without Labels. (arXiv:2012.01230v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.01230">
<div class="article-summary-box-inner">
<span><p>In this paper we set out to solve the task of 6-DOF 3D object detection from
2D images, where the only supervision is a geometric representation of the
objects we aim to find. In doing so, we remove the need for 6-DOF labels (i.e.,
position, orientation etc.), allowing our network to be trained on unlabeled
images in a self-supervised manner. We achieve this through a neural network
which learns an explicit scene parameterization which is subsequently passed
into a differentiable renderer. We analyze why analysis-by-synthesis-like
losses for supervision of 3D scene structure using differentiable rendering is
not practical, as it almost always gets stuck in local minima of visual
ambiguities. This can be overcome by a novel form of training, where an
additional network is employed to steer the optimization itself to explore the
entire parameter space i.e., to be curious, and hence, to resolve those
ambiguities and find workable minima.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prototype-based Incremental Few-Shot Semantic Segmentation. (arXiv:2012.01415v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.01415">
<div class="article-summary-box-inner">
<span><p>Semantic segmentation models have two fundamental weaknesses: i) they require
large training sets with costly pixel-level annotations, and ii) they have a
static output space, constrained to the classes of the training set. Toward
addressing both problems, we introduce a new task, Incremental Few-Shot
Segmentation (iFSS). The goal of iFSS is to extend a pretrained segmentation
model with new classes from few annotated images and without access to old
training data. To overcome the limitations of existing models iniFSS, we
propose Prototype-based Incremental Few-Shot Segmentation (PIFS) that couples
prototype learning and knowledge distillation. PIFS exploits prototypes to
initialize the classifiers of new classes, fine-tuning the network to refine
its features representation. We design a prototype-based distillation loss on
the scores of both old and new class prototypes to avoid overfitting and
forgetting, and batch-renormalization to cope with non-i.i.d.few-shot data. We
create an extensive benchmark for iFSS showing that PIFS outperforms several
few-shot and incremental learning methods in all scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Object SLAM-Based Active Mapping and Robotic Grasping. (arXiv:2012.01788v3 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.01788">
<div class="article-summary-box-inner">
<span><p>This paper presents the first active object mapping framework for complex
robotic manipulation and autonomous perception tasks. The framework is built on
an object SLAM system integrated with a simultaneous multi-object pose
estimation process that is optimized for robotic grasping. Aiming to reduce the
observation uncertainty on target objects and increase their pose estimation
accuracy, we also design an object-driven exploration strategy to guide the
object mapping process, enabling autonomous mapping and high-level perception.
Combining the mapping module and the exploration strategy, an accurate object
map that is compatible with robotic grasping can be generated. Additionally,
quantitative evaluations also indicate that the proposed framework has a very
high mapping accuracy. Experiments with manipulation (including object grasping
and placement) and augmented reality significantly demonstrate the
effectiveness and advantages of our proposed framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wisdom of Committees: An Overlooked Approach To Faster and More Accurate Models. (arXiv:2012.01988v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.01988">
<div class="article-summary-box-inner">
<span><p>Committee-based models (ensembles or cascades) construct models by combining
existing pre-trained ones. While ensembles and cascades are well-known
techniques that were proposed before deep learning, they are not considered a
core building block of deep model architectures and are rarely compared to in
recent literature on developing efficient models. In this work, we go back to
basics and conduct a comprehensive analysis of the efficiency of
committee-based models. We find that even the most simplistic method for
building committees from existing, independently trained networks can match or
exceed the accuracy of state-of-the-art models while being drastically more
efficient. These simple committee-based models also outperform sophisticated
neural architecture search methods (e.g., BigNAS). These findings hold true for
several tasks, including image classification, video classification, and
semantic segmentation, and various architecture families, such as ViT,
EfficientNet, ResNet, MobileNetV2, and X3D. For example, an EfficientNet
cascade can achieve a 5.4x speedup over B7 and a ViT-based cascade can achieve
a 2.3x speedup over ViT-L-384 while being equally accurate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DIPPAS: A Deep Image Prior PRNU Anonymization Scheme. (arXiv:2012.03581v2 [cs.MM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.03581">
<div class="article-summary-box-inner">
<span><p>Source device identification is an important topic in image forensics since
it allows to trace back the origin of an image. Its forensics counter-part is
source device anonymization, that is, to mask any trace on the image that can
be useful for identifying the source device. A typical trace exploited for
source device identification is the Photo Response Non-Uniformity (PRNU), a
noise pattern left by the device on the acquired images. In this paper, we
devise a methodology for suppressing such a trace from natural images without
significant impact on image quality. Specifically, we turn PRNU anonymization
into an optimization problem in a Deep Image Prior (DIP) framework. In a
nutshell, a Convolutional Neural Network (CNN) acts as generator and returns an
image that is anonymized with respect to the source PRNU, still maintaining
high visual quality. With respect to widely-adopted deep learning paradigms,
our proposed CNN is not trained on a set of input-target pairs of images.
Instead, it is optimized to reconstruct the PRNU-free image from the original
image under analysis itself. This makes the approach particularly suitable in
scenarios where large heterogeneous databases are analyzed and prevents any
problem due to lack of generalization. Through numerical examples on publicly
available datasets, we prove our methodology to be effective compared to
state-of-the-art techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Asynchronous Kalman Filter for Hybrid Event Cameras. (arXiv:2012.05590v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.05590">
<div class="article-summary-box-inner">
<span><p>Event cameras are ideally suited to capture HDR visual information without
blur but perform poorly on static or slowly changing scenes. Conversely,
conventional image sensors measure absolute intensity of slowly changing scenes
effectively but do poorly on high dynamic range or quickly changing scenes. In
this paper, we present an event-based video reconstruction pipeline for High
Dynamic Range (HDR) scenarios. The proposed algorithm includes a frame
augmentation pre-processing step that deblurs and temporally interpolates frame
data using events. The augmented frame and event data are then fused using a
novel asynchronous Kalman filter under a unifying uncertainty model for both
sensors. Our experimental results are evaluated on both publicly available
datasets with challenging lighting conditions and fast motions and our new
dataset with HDR reference. The proposed algorithm outperforms state-of-the-art
methods in both absolute intensity error (48% reduction) and image similarity
indexes (average 11% improvement).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Geometric Adversarial Attacks and Defenses on 3D Point Clouds. (arXiv:2012.05657v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.05657">
<div class="article-summary-box-inner">
<span><p>Deep neural networks are prone to adversarial examples that maliciously alter
the network's outcome. Due to the increasing popularity of 3D sensors in
safety-critical systems and the vast deployment of deep learning models for 3D
point sets, there is a growing interest in adversarial attacks and defenses for
such models. So far, the research has focused on the semantic level, namely,
deep point cloud classifiers. However, point clouds are also widely used in a
geometric-related form that includes encoding and reconstructing the geometry.
In this work, we are the first to consider the problem of adversarial examples
at a geometric level. In this setting, the question is how to craft a small
change to a clean source point cloud that leads, after passing through an
autoencoder model, to the reconstruction of a different target shape. Our
attack is in sharp contrast to existing semantic attacks on 3D point clouds.
While such works aim to modify the predicted label by a classifier, we alter
the entire reconstructed geometry. Additionally, we demonstrate the robustness
of our attack in the case of defense, where we show that remnant
characteristics of the target shape are still present at the output after
applying the defense to the adversarial input. Our code is publicly available
at https://github.com/itailang/geometric_adv.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved StyleGAN Embedding: Where are the Good Latents?. (arXiv:2012.09036v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.09036">
<div class="article-summary-box-inner">
<span><p>StyleGAN is able to produce photorealistic images that are almost
indistinguishable from real photos. The reverse problem of finding an embedding
for a given image poses a challenge. Embeddings that reconstruct an image well
are not always robust to editing operations. In this paper, we address the
problem of finding an embedding that both reconstructs images and also supports
image editing tasks. First, we introduce a new normalized space to analyze the
diversity and the quality of the reconstructed latent codes. This space can
help answer the question of where good latent codes are located in latent
space. Second, we propose an improved embedding algorithm using a novel
regularization method based on our analysis. Finally, we analyze the quality of
different embedding algorithms. We compare our results with the current
state-of-the-art methods and achieve a better trade-off between reconstruction
quality and editing quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Power-SLIC: Fast Superpixel Segmentations by Diagrams. (arXiv:2012.11772v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.11772">
<div class="article-summary-box-inner">
<span><p>Superpixel algorithms grouping pixels with similar color and other low-level
properties are increasingly used for pre-processing in image segmentation. In
recent years, a focus has been placed on developing geometric superpixel
methods that facilitate the extraction and analysis of geometric image
features. Diagram-based superpixel methods are important among the geometric
methods as they generate compact and sparsely representable superpixels.
Introducing generalized balanced power diagrams to the field of superpixels, we
propose a diagram method called Power-SLIC. Power-SLIC is the first geometric
superpixel method to generate piecewise quadratic boundaries. Its speed,
competitive with fast state-of-the-art methods, is unprecedented for diagram
approaches. Extensive computational experiments show that Power-SLIC
outperforms existing diagram approaches in boundary recall, under segmentation
error, achievable segmentation accuracy, and compression quality. Moreover,
Power-SLIC is robust to Gaussian noise.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FaceX-Zoo: A PyTorch Toolbox for Face Recognition. (arXiv:2101.04407v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.04407">
<div class="article-summary-box-inner">
<span><p>Deep learning based face recognition has achieved significant progress in
recent years. Yet, the practical model production and further research of deep
face recognition are in great need of corresponding public support. For
example, the production of face representation network desires a modular
training scheme to consider the proper choice from various candidates of
state-of-the-art backbone and training supervision subject to the real-world
face recognition demand; for performance analysis and comparison, the standard
and automatic evaluation with a bunch of models on multiple benchmarks will be
a desired tool as well; besides, a public groundwork is welcomed for deploying
the face recognition in the shape of holistic pipeline. Furthermore, there are
some newly-emerged challenges, such as the masked face recognition caused by
the recent world-wide COVID-19 pandemic, which draws increasing attention in
practical applications. A feasible and elegant solution is to build an
easy-to-use unified framework to meet the above demands. To this end, we
introduce a novel open-source framework, named FaceX-Zoo, which is oriented to
the research-development community of face recognition. Resorting to the highly
modular and scalable design, FaceX-Zoo provides a training module with various
supervisory heads and backbones towards state-of-the-art face recognition, as
well as a standardized evaluation module which enables to evaluate the models
in most of the popular benchmarks just by editing a simple configuration. Also,
a simple yet fully functional face SDK is provided for the validation and
primary application of the trained models. Rather than including as many as
possible of the prior techniques, we enable FaceX-Zoo to easily upgrade and
extend along with the development of face related domains. The source code and
models are available at https://github.com/JDAI-CV/FaceX-Zoo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Makeup Face Verification by Exploring Part-Based Representations. (arXiv:2101.07338v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.07338">
<div class="article-summary-box-inner">
<span><p>Recently, we have seen an increase in the global facial recognition market
size. Despite significant advances in face recognition technology with the
adoption of convolutional neural networks, there are still open challenges,
such as when there is makeup in the face. To address this challenge, we propose
and evaluate the adoption of facial parts to fuse with current holistic
representations. We propose two strategies of facial parts: one with four
regions (left periocular, right periocular, nose and mouth) and another with
three facial thirds (upper, middle and lower). Experimental results obtained in
four public makeup face datasets and in a challenging cross-dataset protocol
show that the fusion of deep features extracted of facial parts with holistic
representation increases the accuracy of face verification systems and
decreases the error rates, even without any retraining of the CNN models. Our
proposed pipeline achieved competitive results for the four datasets (EMFD,
FAM, M501 and YMU).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diminishing Domain Bias by Leveraging Domain Labels in Object Detection on UAVs. (arXiv:2101.12677v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.12677">
<div class="article-summary-box-inner">
<span><p>Object detection from Unmanned Aerial Vehicles (UAVs) is of great importance
in many aerial vision-based applications. Despite the great success of generic
object detection methods, a significant performance drop is observed when
applied to images captured by UAVs. This is due to large variations in imaging
conditions, such as varying altitudes, dynamically changing viewing angles, and
different capture times. These variations lead to domain imbalances and, thus,
trained models suffering from domain bias. We demonstrate that domain knowledge
is a valuable source of information and thus propose domain-aware object
detectors by using freely accessible sensor data. By splitting the model into
cross-domain and domain-specific parts, substantial performance improvements
are achieved on multiple data sets across various models and metrics without
changing the architecture. In particular, we achieve a new state-of-the-art
performance on UAVDT for embedded real-time detectors. Furthermore, we create a
new airborne image data set by annotating 13,713 objects in 2,900 images
featuring precise altitude and viewing angle annotations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Global to Local Double Embedding Method for Multi-person Pose Estimation. (arXiv:2102.07318v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.07318">
<div class="article-summary-box-inner">
<span><p>Multi-person pose estimation is a fundamental and challenging problem to many
computer vision tasks. Most existing methods can be broadly categorized into
two classes: top-down and bottom-up methods. Both of the two types of methods
involve two stages, namely, person detection and joints detection.
Conventionally, the two stages are implemented separately without considering
their interactions between them, and this may inevitably cause some issue
intrinsically. In this paper, we present a novel method to simplify the
pipeline by implementing person detection and joints detection simultaneously.
We propose a Double Embedding (DE) method to complete the multi-person pose
estimation task in a global-to-local way. DE consists of Global Embedding (GE)
and Local Embedding (LE). GE encodes different person instances and processes
information covering the whole image and LE encodes the local limbs
information. GE functions for the person detection in top-down strategy while
LE connects the rest joints sequentially which functions for joint grouping and
information processing in A bottom-up strategy. Based on LE, we design the
Mutual Refine Machine (MRM) to reduce the prediction difficulty in complex
scenarios. MRM can effectively realize the information communicating between
keypoints and further improve the accuracy. We achieve the competitive results
on benchmarks MSCOCO, MPII and CrowdPose, demonstrating the effectiveness and
generalization ability of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HVAQ: A High-Resolution Vision-Based Air Quality Dataset. (arXiv:2102.09332v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.09332">
<div class="article-summary-box-inner">
<span><p>Air pollutants, such as particulate matter, negatively impact human health.
Most existing pollution monitoring techniques use stationary sensors, which are
typically sparsely deployed. However, real-world pollution distributions vary
rapidly with position and the visual effects of air pollution can be used to
estimate concentration, potentially at high spatial resolution. Accurate
pollution monitoring requires either densely deployed conventional point
sensors, at-a-distance vision-based pollution monitoring, or a combination of
both.
</p>
<p>The main contribution of this paper is that to the best of our knowledge, it
is the first publicly available, high temporal and spatial resolution air
quality dataset containing simultaneous point sensor measurements and
corresponding images. The dataset enables, for the first time, high spatial
resolution evaluation of image-based air pollution estimation algorithms. It
contains PM2.5, PM10, temperature, and humidity data. We evaluate several
state-of-art vision-based PM concentration estimation algorithms on our dataset
and quantify the increase in accuracy resulting from higher point sensor
density and the use of images. It is our intent and belief that this dataset
can enable advances by other research teams working on air quality estimation.
Our dataset is available at
https://github.com/implicitDeclaration/HVAQ-dataset/tree/master.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fixing Data Augmentation to Improve Adversarial Robustness. (arXiv:2103.01946v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.01946">
<div class="article-summary-box-inner">
<span><p>Adversarial training suffers from robust overfitting, a phenomenon where the
robust test accuracy starts to decrease during training. In this paper, we
focus on both heuristics-driven and data-driven augmentations as a means to
reduce robust overfitting. First, we demonstrate that, contrary to previous
findings, when combined with model weight averaging, data augmentation can
significantly boost robust accuracy. Second, we explore how state-of-the-art
generative models can be leveraged to artificially increase the size of the
training set and further improve adversarial robustness. Finally, we evaluate
our approach on CIFAR-10 against $\ell_\infty$ and $\ell_2$ norm-bounded
perturbations of size $\epsilon = 8/255$ and $\epsilon = 128/255$,
respectively. We show large absolute improvements of +7.06% and +5.88% in
robust accuracy compared to previous state-of-the-art methods. In particular,
against $\ell_\infty$ norm-bounded perturbations of size $\epsilon = 8/255$,
our model reaches 64.20% robust accuracy without using any external data,
beating most prior works that use external data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Local Patch AutoAugment with Multi-Agent Collaboration. (arXiv:2103.11099v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.11099">
<div class="article-summary-box-inner">
<span><p>Data augmentation (DA) plays a critical role in improving the generalization
of deep learning models. Recent works on automatically searching for DA
policies from data have achieved great success. However, existing automated DA
methods generally perform the search at the image level, which limits the
exploration of diversity in local regions. In this paper, we propose a more
fine-grained automated DA approach, dubbed Patch AutoAugment, to divide an
image into a grid of patches and search for the joint optimal augmentation
policies for the patches. We formulate it as a multi-agent reinforcement
learning (MARL) problem, where each agent learns an augmentation policy for
each patch based on its content together with the semantics of the whole image.
The agents cooperate with each other to achieve the optimal augmentation effect
of the entire image by sharing a team reward. We show the effectiveness of our
method on multiple benchmark datasets of image classification and fine-grained
image recognition (e.g., CIFAR-10, CIFAR-100, ImageNet, CUB-200-2011, Stanford
Cars and FGVC-Aircraft). Extensive experiments demonstrate that our method
outperforms the state-of-the-art DA methods while requiring fewer computational
resources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual Mesh Convolutional Networks for Human Shape Correspondence. (arXiv:2103.12459v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.12459">
<div class="article-summary-box-inner">
<span><p>Convolutional networks have been extremely successful for regular data
structures such as 2D images and 3D voxel grids. The transposition to meshes
is, however, not straight-forward due to their irregular structure. We explore
how the dual, face-based representation of triangular meshes can be leveraged
as a data structure for graph convolutional networks. In the dual mesh, each
node (face) has a fixed number of neighbors, which makes the networks less
susceptible to overfitting on the mesh topology, and also al-lows the use of
input features that are naturally defined over faces, such as surface normals
and face areas. We evaluate the dual approach on the shape correspondence task
on theFaust human shape dataset and variants of it with differ-ent mesh
topologies. Our experiments show that results of graph convolutional networks
improve when defined over the dual rather than primal mesh. Moreover, our
models that explicitly leverage the neighborhood regularity of dual meshes
allow improving results further while being more robust to changes in the mesh
topology.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Proxy-based Loss for Deep Metric Learning. (arXiv:2103.13538v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.13538">
<div class="article-summary-box-inner">
<span><p>Proxy-based metric learning losses are superior to pair-based losses due to
their fast convergence and low training complexity. However, existing
proxy-based losses focus on learning class-discriminative features while
overlooking the commonalities shared across classes which are potentially
useful in describing and matching samples. Moreover, they ignore the implicit
hierarchy of categories in real-world datasets, where similar subordinate
classes can be grouped together. In this paper, we present a framework that
leverages this implicit hierarchy by imposing a hierarchical structure on the
proxies and can be used with any existing proxy-based loss. This allows our
model to capture both class-discriminative features and class-shared
characteristics without breaking the implicit data hierarchy. We evaluate our
method on five established image retrieval datasets such as In-Shop and SOP.
Results demonstrate that our hierarchical proxy-based loss framework improves
the performance of existing proxy-based losses, especially on large datasets
which exhibit strong hierarchical structure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training a Task-Specific Image Reconstruction Loss. (arXiv:2103.14616v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.14616">
<div class="article-summary-box-inner">
<span><p>The choice of a loss function is an important factor when training neural
networks for image restoration problems, such as single image super resolution.
The loss function should encourage natural and perceptually pleasing results. A
popular choice for a loss is a pre-trained network, such as VGG, which is used
as a feature extractor for computing the difference between restored and
reference images. However, such an approach has multiple drawbacks: it is
computationally expensive, requires regularization and hyper-parameter tuning,
and involves a large network trained on an unrelated task. Furthermore, it has
been observed that there is no single loss function that works best across all
applications and across different datasets. In this work, we instead propose to
train a set of loss functions that are application specific in nature. Our loss
function comprises a series of discriminators that are trained to detect and
penalize the presence of application-specific artifacts. We show that a single
natural image and corresponding distortions are sufficient to train our feature
extractor that outperforms state-of-the-art loss functions in applications like
single image super resolution, denoising, and JPEG artifact removal. Finally,
we conclude that an effective loss function does not have to be a good
predictor of perceived image quality, but instead needs to be specialized in
identifying the distortions for a given restoration method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Composition Assessment with Saliency-augmented Multi-pattern Pooling. (arXiv:2104.03133v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.03133">
<div class="article-summary-box-inner">
<span><p>Image composition assessment is crucial in aesthetic assessment, which aims
to assess the overall composition quality of a given image. However, to the
best of our knowledge, there is neither dataset nor method specifically
proposed for this task. In this paper, we contribute the first composition
assessment dataset CADB with composition scores for each image provided by
multiple professional raters. Besides, we propose a composition assessment
network SAMP-Net with a novel Saliency-Augmented Multi-pattern Pooling (SAMP)
module, which analyses visual layout from the perspectives of multiple
composition patterns. We also leverage composition-relevant attributes to
further boost the performance, and extend Earth Mover's Distance (EMD) loss to
weighted EMD loss to eliminate the content bias. The experimental results show
that our SAMP-Net can perform more favorably than previous aesthetic assessment
approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Occlusion Guided Self-supervised Scene Flow Estimation on 3D Point Clouds. (arXiv:2104.04724v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.04724">
<div class="article-summary-box-inner">
<span><p>Understanding the flow in 3D space of sparsely sampled points between two
consecutive time frames is the core stone of modern geometric-driven systems
such as VR/AR, Robotics, and Autonomous driving. The lack of real,
non-simulated, labeled data for this task emphasizes the importance of self- or
un-supervised deep architectures. This work presents a new self-supervised
training method and an architecture for the 3D scene flow estimation under
occlusions. Here we show that smart multi-layer fusion between flow prediction
and occlusion detection outperforms traditional architectures by a large margin
for occluded and non-occluded scenarios. We report state-of-the-art results on
Flyingthings3D and KITTI datasets for both the supervised and self-supervised
training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge-Augmented Contrastive Learning for Abnormality Classification and Localization in Chest X-rays with Radiomics using a Feedback Loop. (arXiv:2104.04968v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.04968">
<div class="article-summary-box-inner">
<span><p>Building a highly accurate predictive model for classification and
localization of abnormalities in chest X-rays usually requires a large number
of manually annotated labels and pixel regions (bounding boxes) of
abnormalities. However, it is expensive to acquire such annotations, especially
the bounding boxes. Recently, contrastive learning has shown strong promise in
leveraging unlabeled natural images to produce highly generalizable and
discriminative features. However, extending its power to the medical image
domain is under-explored and highly non-trivial, since medical images are much
less amendable to data augmentations. In contrast, their prior knowledge, as
well as radiomic features, is often crucial. To bridge this gap, we propose an
end-to-end semi-supervised knowledge-augmented contrastive learning framework,
that simultaneously performs disease classification and localization tasks. The
key knob of our framework is a unique positive sampling approach tailored for
the medical images, by seamlessly integrating radiomic features as a knowledge
augmentation. Specifically, we first apply an image encoder to classify the
chest X-rays and to generate the image features. We next leverage Grad-CAM to
highlight the crucial (abnormal) regions for chest X-rays (even when
unannotated), from which we extract radiomic features. The radiomic features
are then passed through another dedicated encoder to act as the positive sample
for the image features generated from the same chest X-ray. In this way, our
framework constitutes a feedback loop for image and radiomic modality features
to mutually reinforce each other. Their contrasting yields knowledge-augmented
representations that are both robust and interpretable. Extensive experiments
on the NIH Chest X-ray dataset demonstrate that our approach outperforms
existing baselines in both classification and localization tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Task Generalization via Natural Language Crowdsourcing Instructions. (arXiv:2104.08773v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08773">
<div class="article-summary-box-inner">
<span><p>Humans (e.g., crowdworkers) have a remarkable ability in solving different
tasks, by simply reading textual instructions that define them and looking at a
few examples. NLP models built with the conventional paradigm, however, often
struggle with generalization across tasks (e.g., a question-answering system
cannot solve classification tasks). A long-standing challenge in AI is to build
a model that learns a new task by understanding the human-readable instructions
that define it. To study this, we introduce NATURAL INSTRUCTIONS, a dataset of
61 distinct tasks, their human-authored instructions and 193k task instances.
The instructions are obtained from crowdsourcing instructions used to create
existing NLP datasets and mapped to a unified schema. We adopt generative
pre-trained language models to encode task-specific instructions along with
input and generate task output. Our results indicate that models benefit from
instructions when evaluated in terms of generalization to unseen tasks. These
models, however, are far behind supervised task-specific models, indicating
significant room for more progress in this direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving state-of-the-art in Detecting Student Engagement with Resnet and TCN Hybrid Network. (arXiv:2104.10122v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.10122">
<div class="article-summary-box-inner">
<span><p>Automatic detection of students' engagement in online learning settings is a
key element to improve the quality of learning and to deliver personalized
learning materials to them. Varying levels of engagement exhibited by students
in an online classroom is an affective behavior that takes place over space and
time. Therefore, we formulate detecting levels of students' engagement from
videos as a spatio-temporal classification problem. In this paper, we present a
novel end-to-end Residual Network (ResNet) and Temporal Convolutional Network
(TCN) hybrid neural network architecture for students' engagement level
detection in videos. The 2D ResNet extracts spatial features from consecutive
video frames, and the TCN analyzes the temporal changes in video frames to
detect the level of engagement. The spatial and temporal arms of the hybrid
network are jointly trained on raw video frames of a large publicly available
students' engagement detection dataset, DAiSEE. We compared our method with
several competing students' engagement detection methods on this dataset. The
ResNet+TCN architecture outperforms all other studied methods, improves the
state-of-the-art engagement level detection accuracy, and sets a new baseline
for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FIERY: Future Instance Prediction in Bird's-Eye View from Surround Monocular Cameras. (arXiv:2104.10490v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.10490">
<div class="article-summary-box-inner">
<span><p>Driving requires interacting with road agents and predicting their future
behaviour in order to navigate safely. We present FIERY: a probabilistic future
prediction model in bird's-eye view from monocular cameras. Our model predicts
future instance segmentation and motion of dynamic agents that can be
transformed into non-parametric future trajectories. Our approach combines the
perception, sensor fusion and prediction components of a traditional autonomous
driving stack by estimating bird's-eye-view prediction directly from surround
RGB monocular camera inputs. FIERY learns to model the inherent stochastic
nature of the future solely from camera driving data in an end-to-end manner,
without relying on HD maps, and predicts multimodal future trajectories. We
show that our model outperforms previous prediction baselines on the NuScenes
and Lyft datasets. The code and trained models are available at
https://github.com/wayveai/fiery.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VidTr: Video Transformer Without Convolutions. (arXiv:2104.11746v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.11746">
<div class="article-summary-box-inner">
<span><p>We introduce Video Transformer (VidTr) with separable-attention for video
classification. Comparing with commonly used 3D networks, VidTr is able to
aggregate spatio-temporal information via stacked attentions and provide better
performance with higher efficiency. We first introduce the vanilla video
transformer and show that transformer module is able to perform spatio-temporal
modeling from raw pixels, but with heavy memory usage. We then present VidTr
which reduces the memory cost by 3.3$\times$ while keeping the same
performance. To further optimize the model, we propose the standard deviation
based topK pooling for attention ($pool_{topK\_std}$), which reduces the
computation by dropping non-informative features along temporal dimension.
VidTr achieves state-of-the-art performance on five commonly used datasets with
lower computational requirement, showing both the efficiency and effectiveness
of our design. Finally, error analysis and visualization show that VidTr is
especially good at predicting actions that require long-term temporal
reasoning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Collaborative Regression of Expressive Bodies using Moderation. (arXiv:2105.05301v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.05301">
<div class="article-summary-box-inner">
<span><p>Recovering expressive humans from images is essential for understanding human
behavior. Methods that estimate 3D bodies, faces, or hands have progressed
significantly, yet separately. Face methods recover accurate 3D shape and
geometric details, but need a tight crop and struggle with extreme views and
low resolution. Whole-body methods are robust to a wide range of poses and
resolutions, but provide only a rough 3D face shape without details like
wrinkles. To get the best of both worlds, we introduce PIXIE, which produces
animatable, whole-body 3D avatars with realistic facial detail, from a single
image. For this, PIXIE uses two key observations. First, existing work combines
independent estimates from body, face, and hand experts, by trusting them
equally. PIXIE introduces a novel moderator that merges the features of the
experts, weighted by their confidence. All part experts can contribute to the
whole, using SMPL-X's shared shape space across all body parts. Second, human
shape is highly correlated with gender, but existing work ignores this. We
label training images as male, female, or non-binary, and train PIXIE to infer
"gendered" 3D body shapes with a novel shape loss. In addition to 3D body pose
and shape parameters, PIXIE estimates expression, illumination, albedo and 3D
facial surface displacements. Quantitative and qualitative evaluation shows
that PIXIE estimates more accurate whole-body shape and detailed face shape
than the state of the art. Models and code are available at
https://pixie.is.tue.mpg.de.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SBEVNet: End-to-End Deep Stereo Layout Estimation. (arXiv:2105.11705v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.11705">
<div class="article-summary-box-inner">
<span><p>Accurate layout estimation is crucial for planning and navigation in robotics
applications, such as self-driving. In this paper, we introduce the Stereo
Bird's Eye ViewNetwork (SBEVNet), a novel supervised end-to-end framework for
estimation of bird's eye view layout from a pair of stereo images. Although our
network reuses some of the building blocks from the state-of-the-art deep
learning networks for disparity estimation, we show that explicit depth
estimation is neither sufficient nor necessary. Instead, the learning of a good
internal bird's eye view feature representation is effective for layout
estimation. Specifically, we first generate a disparity feature volume using
the features of the stereo images and then project it to the bird's eye view
coordinates. This gives us coarse-grained information about the scene
structure. We also apply inverse perspective mapping (IPM) to map the input
images and their features to the bird's eye view. This gives us fine-grained
texture information. Concatenating IPM features with the projected feature
volume creates a rich bird's eye view representation which is useful for
spatial reasoning. We use this representation to estimate the BEV semantic map.
Additionally, we show that using the IPM features as a supervisory signal for
stereo features can give an improvement in performance. We demonstrate our
approach on two datasets:the KITTI dataset and a synthetically generated
dataset from the CARLA simulator. For both of these datasets, we establish
state-of-the-art performance compared to baseline techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhanced Isotropy Maximization Loss: Seamless and High-Performance Out-of-Distribution Detection Simply Replacing the SoftMax Loss. (arXiv:2105.14399v7 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14399">
<div class="article-summary-box-inner">
<span><p>Current out-of-distribution detection approaches usually present special
requirements (e.g., collecting outlier data and hyperparameter validation) and
produce side effects (e.g., classification accuracy drop and slow/inefficient
inferences). Recently, entropic out-of-distribution detection has been proposed
as a seamless approach (i.e., a solution that avoids all of the previously
mentioned drawbacks). The entropic out-of-distribution detection solution uses
the IsoMax loss for training and the entropic score for out-of-distribution
detection. The IsoMax loss works as a SoftMax loss drop-in replacement because
swapping the SoftMax loss with the IsoMax loss requires no changes in the
model's architecture or training procedures/hyperparameters. In this paper, we
perform what we call an isometrization of the distances used in the IsoMax
loss. Additionally, we propose replacing the entropic score with the minimum
distance score. Experiments showed that these simple modifications increase
out-of-distribution detection performance while keeping the solution seamless.
Besides being competitive with or outperforming all major current approaches,
the proposed solution avoids all their current limitations in addition to being
much easier to use because only a simple loss replacement for training the
neural network is required.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analogous to Evolutionary Algorithm: Designing a Unified Sequence Model. (arXiv:2105.15089v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.15089">
<div class="article-summary-box-inner">
<span><p>Inspired by biological evolution, we explain the rationality of Vision
Transformer by analogy with the proven practical Evolutionary Algorithm (EA)
and derive that both of them have consistent mathematical representation.
Analogous to the dynamic local population in EA, we improve the existing
transformer structure and propose a more efficient EAT model, and design
task-related heads to deal with different tasks more flexibly. Moreover, we
introduce the spatial-filling curve into the current vision transformer to
sequence image data into a uniform sequential format. Thus we can design a
unified EAT framework to address multi-modal tasks, separating the network
architecture from the data format adaptation. Our approach achieves
state-of-the-art results on the ImageNet classification task compared with
recent vision transformer works while having smaller parameters and greater
throughput. We further conduct multi-modal tasks to demonstrate the superiority
of the unified EAT, e.g., Text-Based Image Retrieval, and our approach improves
the rank-1 by +3.7 points over the baseline on the CSS dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Container: Context Aggregation Network. (arXiv:2106.01401v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01401">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks (CNNs) are ubiquitous in computer vision, with
a myriad of effective and efficient variations. Recently, Transformers --
originally introduced in natural language processing -- have been increasingly
adopted in computer vision. While early adopters continue to employ CNN
backbones, the latest networks are end-to-end CNN-free Transformer solutions. A
recent surprising finding shows that a simple MLP based solution without any
traditional convolutional or Transformer components can produce effective
visual representations. While CNNs, Transformers and MLP-Mixers may be
considered as completely disparate architectures, we provide a unified view
showing that they are in fact special cases of a more general method to
aggregate spatial context in a neural network stack. We present the \model
(CONText AggregatIon NEtwoRk), a general-purpose building block for multi-head
context aggregation that can exploit long-range interactions \emph{a la}
Transformers while still exploiting the inductive bias of the local convolution
operation leading to faster convergence speeds, often seen in CNNs. In contrast
to Transformer-based methods that do not scale well to downstream tasks that
rely on larger input image resolutions, our efficient network, named
\modellight, can be employed in object detection and instance segmentation
networks such as DETR, RetinaNet and Mask-RCNN to obtain an impressive
detection mAP of 38.9, 43.8, 45.1 and mask mAP of 41.3, providing large
improvements of 6.6, 7.3, 6.9 and 6.6 pts respectively, compared to a ResNet-50
backbone with a comparable compute and parameter size. Our method also achieves
promising results on self-supervised learning compared to DeiT on the DINO
framework. Code is released at \url{https://github.com/allenai/container}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Barbershop: GAN-based Image Compositing using Segmentation Masks. (arXiv:2106.01505v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01505">
<div class="article-summary-box-inner">
<span><p>Seamlessly blending features from multiple images is extremely challenging
because of complex relationships in lighting, geometry, and partial occlusion
which cause coupling between different parts of the image. Even though recent
work on GANs enables synthesis of realistic hair or faces, it remains difficult
to combine them into a single, coherent, and plausible image rather than a
disjointed set of image patches. We present a novel solution to image blending,
particularly for the problem of hairstyle transfer, based on GAN-inversion. We
propose a novel latent space for image blending which is better at preserving
detail and encoding spatial information, and propose a new GAN-embedding
algorithm which is able to slightly modify images to conform to a common
segmentation mask. Our novel representation enables the transfer of the visual
properties from multiple reference images including specific details such as
moles and wrinkles, and because we do image blending in a latent-space we are
able to synthesize images that are coherent. Our approach avoids blending
artifacts present in other approaches and finds a globally consistent image.
Our results demonstrate a significant improvement over the current state of the
art in a user study, with users preferring our blending solution over 95
percent of the time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Image Local Autoregressive Transformer. (arXiv:2106.02514v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02514">
<div class="article-summary-box-inner">
<span><p>Recently, AutoRegressive (AR) models for the whole image generation empowered
by transformers have achieved comparable or even better performance to
Generative Adversarial Networks (GANs). Unfortunately, directly applying such
AR models to edit/change local image regions, may suffer from the problems of
missing global information, slow inference speed, and information leakage of
local guidance. To address these limitations, we propose a novel model -- image
Local Autoregressive Transformer (iLAT), to better facilitate the locally
guided image synthesis. Our iLAT learns the novel local discrete
representations, by the newly proposed local autoregressive (LA) transformer of
the attention mask and convolution mechanism. Thus iLAT can efficiently
synthesize the local image regions by key guidance information. Our iLAT is
evaluated on various locally guided image syntheses, such as pose-guided person
image synthesis and face editing. Both the quantitative and qualitative results
show the efficacy of our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Resolution Network. (arXiv:2106.02898v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02898">
<div class="article-summary-box-inner">
<span><p>Deep convolutional neural networks (CNNs) are often of sophisticated design
with numerous learnable parameters for the accuracy reason. To alleviate the
expensive costs of deploying them on mobile devices, recent works have made
huge efforts for excavating redundancy in pre-defined architectures.
Nevertheless, the redundancy on the input resolution of modern CNNs has not
been fully investigated, i.e., the resolution of input image is fixed. In this
paper, we observe that the smallest resolution for accurately predicting the
given image is different using the same neural network. To this end, we propose
a novel dynamic-resolution network (DRNet) in which the input resolution is
determined dynamically based on each input sample. Wherein, a resolution
predictor with negligible computational costs is explored and optimized jointly
with the desired network. Specifically, the predictor learns the smallest
resolution that can retain and even exceed the original recognition accuracy
for each image. During the inference, each input image will be resized to its
predicted resolution for minimizing the overall computation burden. We then
conduct extensive experiments on several benchmark networks and datasets. The
results show that our DRNet can be embedded in any off-the-shelf network
architecture to obtain a considerable reduction in computational complexity.
For instance, DR-ResNet-50 achieves similar performance with an about 34%
computation reduction, while gains 1.4% accuracy increase with 10% computation
reduction compared to the original ResNet-50 on ImageNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the relation between statistical learning and perceptual distances. (arXiv:2106.04427v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04427">
<div class="article-summary-box-inner">
<span><p>It has been demonstrated many times that the behavior of the human visual
system is connected to the statistics of natural images. Since machine learning
relies on the statistics of training data as well, the above connection has
interesting implications when using perceptual distances (which mimic the
behavior of the human visual system) as a loss function. In this paper, we aim
to unravel the non-trivial relationships between the probability distribution
of the data, perceptual distances, and unsupervised machine learning. To this
end, we show that perceptual sensitivity is correlated with the probability of
an image in its close neighborhood. We also explore the relation between
distances induced by autoencoders and the probability distribution of the
training data, as well as how these induced distances are correlated with human
perception. Finally, we find perceptual distances do not always lead to
noticeable gains in performance over Euclidean distance in common image
processing tasks, except when data is scarce and the perceptual distance
provides regularization. We propose this may be due to a \emph{double-counting}
effect of the image statistics, once in the perceptual distance and once in the
training procedure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse Training via Boosting Pruning Plasticity with Neuroregeneration. (arXiv:2106.10404v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.10404">
<div class="article-summary-box-inner">
<span><p>Works on lottery ticket hypothesis (LTH) and single-shot network pruning
(SNIP) have raised a lot of attention currently on post-training pruning
(iterative magnitude pruning), and before-training pruning (pruning at
initialization). The former method suffers from an extremely large computation
cost and the latter usually struggles with insufficient performance. In
comparison, during-training pruning, a class of pruning methods that
simultaneously enjoys the training/inference efficiency and the comparable
performance, temporarily, has been less explored. To better understand
during-training pruning, we quantitatively study the effect of pruning
throughout training from the perspective of pruning plasticity (the ability of
the pruned networks to recover the original performance). Pruning plasticity
can help explain several other empirical observations about neural network
pruning in literature. We further find that pruning plasticity can be
substantially improved by injecting a brain-inspired mechanism called
neuroregeneration, i.e., to regenerate the same number of connections as
pruned. We design a novel gradual magnitude pruning (GMP) method, named gradual
pruning with zero-cost neuroregeneration (\textbf{GraNet}), that advances state
of the art. Perhaps most impressively, its sparse-to-sparse version for the
first time boosts the sparse-to-sparse training performance over various
dense-to-sparse methods with ResNet-50 on ImageNet without extending the
training time. We release all codes in
https://github.com/Shiweiliuiiiiiii/GraNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Alias-Free Generative Adversarial Networks. (arXiv:2106.12423v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.12423">
<div class="article-summary-box-inner">
<span><p>We observe that despite their hierarchical convolutional nature, the
synthesis process of typical generative adversarial networks depends on
absolute pixel coordinates in an unhealthy manner. This manifests itself as,
e.g., detail appearing to be glued to image coordinates instead of the surfaces
of depicted objects. We trace the root cause to careless signal processing that
causes aliasing in the generator network. Interpreting all signals in the
network as continuous, we derive generally applicable, small architectural
changes that guarantee that unwanted information cannot leak into the
hierarchical synthesis process. The resulting networks match the FID of
StyleGAN2 but differ dramatically in their internal representations, and they
are fully equivariant to translation and rotation even at subpixel scales. Our
results pave the way for generative models better suited for video and
animation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hear Me Out: Fusional Approaches for Audio Augmented Temporal Action Localization. (arXiv:2106.14118v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.14118">
<div class="article-summary-box-inner">
<span><p>State of the art architectures for untrimmed video Temporal Action
Localization (TAL) have only considered RGB and Flow modalities, leaving the
information-rich audio modality totally unexploited. Audio fusion has been
explored for the related but arguably easier problem of trimmed (clip-level)
action recognition. However, TAL poses a unique set of challenges. In this
paper, we propose simple but effective fusion-based approaches for TAL. To the
best of our knowledge, our work is the first to jointly consider audio and
video modalities for supervised TAL. We experimentally show that our schemes
consistently improve performance for state of the art video-only TAL
approaches. Specifically, they help achieve new state of the art performance on
large-scale benchmark datasets - ActivityNet-1.3 (54.34 mAP@0.5) and THUMOS14
(57.18 mAP@0.5). Our experiments include ablations involving multiple fusion
schemes, modality combinations and TAL architectures. Our code, models and
associated data are available at https://github.com/skelemoa/tal-hmo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">JPGNet: Joint Predictive Filtering and Generative Network for Image Inpainting. (arXiv:2107.04281v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.04281">
<div class="article-summary-box-inner">
<span><p>Image inpainting aims to restore the missing regions of corrupted images and
make the recovery result identical to the originally complete image, which is
different from the common generative task emphasizing the naturalness or
realism of generated images. Nevertheless, existing works usually regard it as
a pure generation problem and employ cutting-edge deep generative techniques to
address it. The generative networks can fill the main missing parts with
realistic contents but usually distort the local structures or introduce
obvious artifacts. In this paper, for the first time, we formulate image
inpainting as a mix of two problems, predictive filtering and deep generation.
Predictive filtering is good at preserving local structures and removing
artifacts but falls short to complete the large missing regions. The deep
generative network can fill the numerous missing pixels based on the
understanding of the whole scene but hardly restores the details identical to
the original ones. To make use of their respective advantages, we propose the
joint predictive filtering and generative network (JPGNet) that contains three
branches: predictive filtering &amp; uncertainty network (PFUNet), deep generative
network, and uncertainty-aware fusion network (UAFNet). The PFUNet can
adaptively predict pixel-wise kernels for filtering-based inpainting according
to the input image and output an uncertainty map. This map indicates the pixels
should be processed by filtering or generative networks, which is further fed
to the UAFNet for a smart combination between filtering and generative results.
Note that, our method as a novel inpainting framework can benefit any existing
generation-based methods. We validate our method on three public datasets,
Dunhuang, Places2, and CelebA, and demonstrate that our method can enhance
three state-of-the-art generative methods significantly with slightly extra
time costs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AdvFilter: Predictive Perturbation-aware Filtering against Adversarial Attack via Multi-domain Learning. (arXiv:2107.06501v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.06501">
<div class="article-summary-box-inner">
<span><p>High-level representation-guided pixel denoising and adversarial training are
independent solutions to enhance the robustness of CNNs against adversarial
attacks by pre-processing input data and re-training models, respectively. Most
recently, adversarial training techniques have been widely studied and improved
while the pixel denoising-based method is getting less attractive. However, it
is still questionable whether there exists a more advanced pixel
denoising-based method and whether the combination of the two solutions
benefits each other. To this end, we first comprehensively investigate two
kinds of pixel denoising methods for adversarial robustness enhancement (i.e.,
existing additive-based and unexplored filtering-based methods) under the loss
functions of image-level and semantic-level, respectively, showing that
pixel-wise filtering can obtain much higher image quality (e.g., higher PSNR)
as well as higher robustness (e.g., higher accuracy on adversarial examples)
than existing pixel-wise additive-based method. However, we also observe that
the robustness results of the filtering-based method rely on the perturbation
amplitude of adversarial examples used for training. To address this problem,
we propose predictive perturbation-aware &amp; pixel-wise filtering}, where
dual-perturbation filtering and an uncertainty-aware fusion module are designed
and employed to automatically perceive the perturbation amplitude during the
training and testing process. The method is termed as AdvFilter. Moreover, we
combine adversarial pixel denoising methods with three adversarial
training-based methods, hinting that considering data and models jointly is
able to achieve more robust CNNs. The experiments conduct on NeurIPS-2017DEV,
SVHN and CIFAR10 datasets and show advantages over enhancing CNNs' robustness,
high generalization to different models and noise levels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diff-Net: Image Feature Difference based High-Definition Map Change Detection for Autonomous Driving. (arXiv:2107.07030v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.07030">
<div class="article-summary-box-inner">
<span><p>Up-to-date High-Definition (HD) maps are essential for self-driving cars. To
achieve constantly updated HD maps, we present a deep neural network (DNN),
Diff-Net, to detect changes in them. Compared to traditional methods based on
object detectors, the essential design in our work is a parallel feature
difference calculation structure that infers map changes by comparing features
extracted from the camera and rasterized images. To generate these rasterized
images, we project map elements onto images in the camera view, yielding
meaningful map representations that can be consumed by a DNN accordingly. As we
formulate the change detection task as an object detection problem, we leverage
the anchor-based structure that predicts bounding boxes with different change
status categories. To the best of our knowledge, the proposed method is the
first end-to-end network that tackles the high-definition map change detection
task, yielding a single stage solution. Furthermore, rather than relying on
single frame input, we introduce a spatio-temporal fusion module that fuses
features from history frames into the current, thus improving the overall
performance. Finally, we comprehensively validate our method's effectiveness
using freshly collected datasets. Results demonstrate that our Diff-Net
achieves better performance than the baseline methods and is ready to be
integrated into a map production pipeline maintaining an up-to-date HD map.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Adversarially Blur Visual Object Tracking. (arXiv:2107.12085v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.12085">
<div class="article-summary-box-inner">
<span><p>Motion blur caused by the moving of the object or camera during the exposure
can be a key challenge for visual object tracking, affecting tracking accuracy
significantly. In this work, we explore the robustness of visual object
trackers against motion blur from a new angle, i.e., adversarial blur attack
(ABA). Our main objective is to online transfer input frames to their natural
motion-blurred counterparts while misleading the state-of-the-art trackers
during the tracking process. To this end, we first design the motion blur
synthesizing method for visual tracking based on the generation principle of
motion blur, considering the motion information and the light accumulation
process. With this synthetic method, we propose optimization-based ABA (OP-ABA)
by iteratively optimizing an adversarial objective function against the
tracking w.r.t. the motion and light accumulation parameters. The OP-ABA is
able to produce natural adversarial examples but the iteration can cause heavy
time cost, making it unsuitable for attacking real-time trackers. To alleviate
this issue, we further propose one-step ABA (OS-ABA) where we design and train
a joint adversarial motion and accumulation predictive network (JAMANet) with
the guidance of OP-ABA, which is able to efficiently estimate the adversarial
motion and accumulation parameters in a one-step way. The experiments on four
popular datasets (e.g., OTB100, VOT2018, UAV123, and LaSOT) demonstrate that
our methods are able to cause significant accuracy drops on four
state-of-the-art trackers with high transferability. Please find the source
code at
\href{https://github.com/tsingqguo/ABA}{https://github.com/tsingqguo/ABA}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rain Removal and Illumination Enhancement Done in One Go. (arXiv:2108.03873v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03873">
<div class="article-summary-box-inner">
<span><p>Rain removal plays an important role in the restoration of degraded images.
Recently, data-driven methods have achieved remarkable success. However, these
approaches neglect that the appearance of rain is often accompanied by low
light conditions, which will further degrade the image quality. Therefore, it
is very indispensable to jointly remove the rain and enhance the light for
real-world rain image restoration. In this paper, we aim to address this
problem from two aspects. First, we proposed a novel entangled network, namely
EMNet, which can remove the rain and enhance illumination in one go.
Specifically, two encoder-decoder networks interact complementary information
through entanglement structure, and parallel rain removal and illumination
enhancement. Considering that the encoder-decoder structure is unreliable in
preserving spatial details, we employ a detail recovery network to restore the
desired fine texture. Second, we present a new synthetic dataset, namely
DarkRain, to boost the development of rain image restoration algorithms in
practical scenarios. DarkRain not only contains different degrees of rain, but
also considers different lighting conditions, and more realistically simulates
the rainfall in the real world. EMNet is extensively evaluated on the proposed
benchmark and achieves state-of-the-art results. In addition, after a simple
transformation, our method outshines existing methods in both rain removal and
low-light image enhancement. The source code and dataset will be made publicly
available later.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Iterative Distillation for Better Uncertainty Estimates in Multitask Emotion Recognition. (arXiv:2108.04228v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04228">
<div class="article-summary-box-inner">
<span><p>When recognizing emotions, subtle nuances in displays of emotion generate
ambiguity or uncertainty in emotion perception. Emotion uncertainty has been
previously interpreted as inter-rater disagreement among multiple annotators.
In this paper, we consider a more common and challenging scenario: modeling
emotion uncertainty when only single emotion labels are available. From a
Bayesian perspective, we propose to use deep ensembles to capture uncertainty
for multiple emotion descriptors, i.e., action units, discrete expression
labels and continuous descriptors. We further apply iterative
self-distillation. Iterative distillation over multiple generations
significantly improves performance in both emotion recognition and uncertainty
estimation. Our method generates single student models that provide accurate
estimates of uncertainty for in-domain samples and a student ensemble that can
detect out-of-domain samples. Our experiments on emotion recognition and
uncertainty estimation using the Aff-wild2 dataset demonstrate that our
algorithm gives more reliable uncertainty estimates than both Temperature
Scaling and Monte Carol Dropout.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Classification of Abnormal Hand Movement for Aiding in Autism Detection: Machine Learning Study. (arXiv:2108.07917v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07917">
<div class="article-summary-box-inner">
<span><p>A formal autism diagnosis is an inefficient and lengthy process. Families
often have to wait years before receiving a diagnosis for their child; some may
not receive one at all due to this delay. One approach to this problem is to
use digital technologies to detect the presence of behaviors related to autism,
which in aggregate may lead to remote and automated diagnostics. One of the
strongest indicators of autism is stimming, which is a set of repetitive,
self-stimulatory behaviors such as hand flapping, headbanging, and spinning.
Using computer vision to detect hand flapping is especially difficult due to
the sparsity of public training data in this space and excessive shakiness and
motion in such data. Our work demonstrates a novel method that overcomes these
issues: we use hand landmark detection over time as a feature representation
which is then fed into a Long Short-Term Memory (LSTM) model. We achieve a
validation accuracy and F1 Score of about 72% on detecting whether videos from
the Self-Stimulatory Behaviour Dataset (SSBD) contain hand flapping or not. Our
best model also predicts accurately on external videos we recorded of ourselves
outside of the dataset it was trained on. This model uses less than 26,000
parameters, providing promise for fast deployment into ubiquitous and wearable
digital settings for a remote autism diagnosis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LLVIP: A Visible-infrared Paired Dataset for Low-light Vision. (arXiv:2108.10831v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10831">
<div class="article-summary-box-inner">
<span><p>It is very challenging for various visual tasks such as image fusion,
pedestrian detection and image-to-image translation in low light conditions due
to the loss of effective target areas. In this case, infrared and visible
images can be used together to provide both rich detail information and
effective target areas. In this paper, we present LLVIP, a visible-infrared
paired dataset for low-light vision. This dataset contains 30976 images, or
15488 pairs, most of which were taken at very dark scenes, and all of the
images are strictly aligned in time and space. Pedestrians in the dataset are
labeled. We compare the dataset with other visible-infrared datasets and
evaluate the performance of some popular visual algorithms including image
fusion, pedestrian detection and image-to-image translation on the dataset. The
experimental results demonstrate the complementary effect of fusion on image
information, and find the deficiency of existing algorithms of the three visual
tasks in very low-light conditions. We believe the LLVIP dataset will
contribute to the community of computer vision by promoting image fusion,
pedestrian detection and image-to-image translation in very low-light
applications. The dataset is being released in
https://bupt-ai-cz.github.io/LLVIP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A realistic approach to generate masked faces applied on two novel masked face recognition data sets. (arXiv:2109.01745v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01745">
<div class="article-summary-box-inner">
<span><p>The COVID-19 pandemic raises the problem of adapting face recognition systems
to the new reality, where people may wear surgical masks to cover their noses
and mouths. Traditional data sets (e.g., CelebA, CASIA-WebFace) used for
training these systems were released before the pandemic, so they now seem
unsuited due to the lack of examples of people wearing masks. We propose a
method for enhancing data sets containing faces without masks by creating
synthetic masks and overlaying them on faces in the original images. Our method
relies on SparkAR Studio, a developer program made by Facebook that is used to
create Instagram face filters. In our approach, we use 9 masks of different
colors, shapes and fabrics. We employ our method to generate a number of
445,446 (90%) samples of masks for the CASIA-WebFace data set and 196,254
(96.8%) masks for the CelebA data set, releasing the mask images at
https://github.com/securifai/masked_faces. We show that our method produces
significantly more realistic training examples of masks overlaid on faces by
asking volunteers to qualitatively compare it to other methods or data sets
designed for the same task. We also demonstrate the usefulness of our method by
evaluating state-of-the-art face recognition systems (FaceNet, VGG-face,
ArcFace) trained on our enhanced data sets and showing that they outperform
equivalent systems trained on original data sets (containing faces without
masks) or competing data sets (containing masks generated by related methods),
when the test benchmarks contain masked faces.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fishr: Invariant Gradient Variances for Out-of-distribution Generalization. (arXiv:2109.02934v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02934">
<div class="article-summary-box-inner">
<span><p>Learning robust models that generalize well under changes in the data
distribution is critical for real-world applications. To this end, there has
been a growing surge of interest to learn simultaneously from multiple training
domains -- while enforcing different types of invariance across those domains.
Yet, all existing approaches fail to show systematic benefits under controlled
evaluation protocols. In this paper, we introduce a new regularization -- named
Fishr -- that enforces domain invariance in the space of the gradients of the
loss: specifically, the domain-level variances of gradients are matched across
training domains. Our approach is based on the close relations between the
gradient covariance, the Fisher Information and the Hessian of the loss: in
particular, we show that Fishr eventually aligns the domain-level loss
landscapes locally around the final weights. Extensive experiments demonstrate
the effectiveness of Fishr for out-of-distribution generalization. Notably,
Fishr improves the state of the art on the DomainBed benchmark and performs
consistently better than Empirical Risk Minimization. The code is released at
https://github.com/alexrame/fishr.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RobustART: Benchmarking Robustness on Architecture Design and Training Techniques. (arXiv:2109.05211v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05211">
<div class="article-summary-box-inner">
<span><p>Deep neural networks (DNNs) are vulnerable to adversarial noises, which
motivates the benchmark of model robustness. Existing benchmarks mainly focus
on evaluating the defenses, but there are no comprehensive studies of how
architecture design and general training techniques affect robustness.
Comprehensively benchmarking their relationships will be highly beneficial for
better understanding and developing robust DNNs. Thus, we propose RobustART,
the first comprehensive Robustness investigation benchmark on ImageNet
(including open-source toolkit, pre-trained model zoo, datasets, and analyses)
regarding ARchitecture design (44 human-designed off-the-shelf architectures
and 1200+ networks from neural architecture search) and Training techniques
(10+ general techniques, e.g., data augmentation) towards diverse noises
(adversarial, natural, and system noises). Extensive experiments revealed and
substantiated several insights for the first time, for example: (1) adversarial
training largely improves the clean accuracy and all types of robustness for
Transformers and MLP-Mixers; (2) with comparable sizes, CNNs &gt; Transformers &gt;
MLP-Mixers on robustness against natural and system noises; Transformers &gt;
MLP-Mixers &gt; CNNs on adversarial robustness; (3) for some light-weight
architectures (e.g., EfficientNet, MobileNetV2, and MobileNetV3), increasing
model sizes or using extra training data cannot improve robustness. Our
benchmark <a href="http://robust.art/">this http URL</a> : (1) presents an open-source platform for
conducting comprehensive evaluation on diverse robustness types; (2) provides a
variety of pre-trained models with different training techniques to facilitate
robustness evaluation; (3) proposes a new view to better understand the
mechanism towards designing robust DNN architectures, backed up by the
analysis. We will continuously contribute to building this ecosystem for the
community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Online Unsupervised Learning of Visual Representations and Categories. (arXiv:2109.05675v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05675">
<div class="article-summary-box-inner">
<span><p>Real world learning scenarios involve a nonstationary distribution of classes
with sequential dependencies among the samples, in contrast to the standard
machine learning formulation of drawing samples independently from a fixed,
typically uniform distribution. Furthermore, real world interactions demand
learning on-the-fly from few or no class labels. In this work, we propose an
unsupervised model that simultaneously performs online visual representation
learning and few-shot learning of new categories without relying on any class
labels. Our model is a prototype-based memory network with a control component
that determines when to form a new class prototype. We formulate it as an
online Gaussian mixture model, where components are created online with only a
single new example, and assignments do not have to be balanced, which permits
an approximation to natural imbalanced distributions from uncurated raw data.
Learning includes a contrastive loss that encourages different views of the
same image to be assigned to the same prototype. The result is a mechanism that
forms categorical representations of objects in nonstationary environments.
Experiments show that our method can learn from an online stream of visual
input data and is significantly better at category recognition compared to
state-of-the-art self-supervised learning methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BabelCalib: A Universal Approach to Calibrating Central Cameras. (arXiv:2109.09704v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09704">
<div class="article-summary-box-inner">
<span><p>Existing calibration methods occasionally fail for large field-of-view
cameras due to the non-linearity of the underlying problem and the lack of good
initial values for all parameters of the used camera model. This might occur
because a simpler projection model is assumed in an initial step, or a poor
initial guess for the internal parameters is pre-defined. A lot of the
difficulties of general camera calibration lie in the use of a forward
projection model. We side-step these challenges by first proposing a solver to
calibrate the parameters in terms of a back-projection model and then regress
the parameters for a target forward model. These steps are incorporated in a
robust estimation framework to cope with outlying detections. Extensive
experiments demonstrate that our approach is very reliable and returns the most
accurate calibration parameters as measured on the downstream task of absolute
pose estimation on test sets. The code is released at
https://github.com/ylochman/babelcalib.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated segmentation and extraction of posterior eye segment using OCT scans. (arXiv:2109.10000v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10000">
<div class="article-summary-box-inner">
<span><p>This paper proposes an automated method for the segmentation and extraction
of the posterior segment of the human eye, including the vitreous, retina,
choroid, and sclera compartments, using multi-vendor optical coherence
tomography (OCT) scans. The proposed method works in two phases. First extracts
the retinal pigment epithelium (RPE) layer by applying the adaptive
thresholding technique to identify the retina-choroid junction. Then, it
exploits the structure tensor guided approach to extract the inner limiting
membrane (ILM) and the choroidal stroma (CS) layers, locating the
vitreous-retina and choroid-sclera junctions in the candidate OCT scan.
Furthermore, these three junction boundaries are utilized to conduct posterior
eye compartmentalization effectively for both healthy and disease eye OCT
scans. The proposed framework is evaluated over 1000 OCT scans, where it
obtained the mean intersection over union (IoU) and mean Dice similarity
coefficient (DSC) scores of 0.874 and 0.930, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ProTo: Program-Guided Transformer for Program-Guided Tasks. (arXiv:2110.00804v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00804">
<div class="article-summary-box-inner">
<span><p>Programs, consisting of semantic and structural information, play an
important role in the communication between humans and agents. Towards learning
general program executors to unify perception, reasoning, and decision making,
we formulate program-guided tasks which require learning to execute a given
program on the observed task specification. Furthermore, we propose the
Program-guided Transformer (ProTo), which integrates both semantic and
structural guidance of a program by leveraging cross-attention and masked
self-attention to pass messages between the specification and routines in the
program. ProTo executes a program in a learned latent space and enjoys stronger
representation ability than previous neural-symbolic approaches. We demonstrate
that ProTo significantly outperforms the previous state-of-the-art methods on
GQA visual reasoning and 2D Minecraft policy learning datasets. Additionally,
ProTo demonstrates better generalization to unseen, complex, and human-written
programs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic-Guided Zero-Shot Learning for Low-Light Image/Video Enhancement. (arXiv:2110.00970v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00970">
<div class="article-summary-box-inner">
<span><p>Low-light images challenge both human perceptions and computer vision
algorithms. It is crucial to make algorithms robust to enlighten low-light
images for computational photography and computer vision applications such as
real-time detection and segmentation. This paper proposes a semantic-guided
zero-shot low-light enhancement network which is trained in the absence of
paired images, unpaired datasets, and segmentation annotation. Firstly, we
design an enhancement factor extraction network using depthwise separable
convolution for an efficient estimate of the pixel-wise light deficiency of a
low-light image. Secondly, we propose a recurrent image enhancement network to
progressively enhance the low-light image with affordable model size. Finally,
we introduce an unsupervised semantic segmentation network for preserving the
semantic information during intensive enhancement. Extensive experiments on
benchmark datasets and a low-light video demonstrate that our model outperforms
the previous state-of-the-art qualitatively and quantitatively. We further
discuss the benefits of the proposed method for low-light detection and
segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incremental Class Learning using Variational Autoencoders with Similarity Learning. (arXiv:2110.01303v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01303">
<div class="article-summary-box-inner">
<span><p>Catastrophic forgetting in neural networks during incremental learning
remains a challenging problem. Previous research investigated catastrophic
forgetting in fully connected networks, with some earlier work exploring
activation functions and learning algorithms. Applications of neural networks
have been extended to include similarity learning. It is of significant
interest to understand how similarity learning loss functions would be affected
by catastrophic forgetting. Our research investigates catastrophic forgetting
for four well-known similarity-based loss functions during incremental class
learning. The loss functions are angular, contrastive, centre, and triplet
loss. Our results show that the rate of catastrophic forgetting is different
across loss functions on multiple datasets. The angular loss was least
affected, followed by contrastive, triplet loss, and centre loss with good
mining techniques. We implemented three existing incremental learning
techniques, iCaRL, EWC, and EBLL. We further proposed our novel technique using
VAEs to generate representation as exemplars that are passed through
intermediate layers of the network. Our method outperformed the three existing
techniques. We have shown that we do not require stored images as exemplars for
incremental learning with similarity learning. The generated representations
can help preserve regions of the embedding space used by prior knowledge so
that new knowledge will not ``overwrite'' prior knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Causal Representation for Face Transfer across Large Appearance Gap. (arXiv:2110.01571v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01571">
<div class="article-summary-box-inner">
<span><p>Identity transfer often faces the challenge of generalizing to new situations
where large pose and expression or background gaps exist between source and
target face images. To improve generalization in such situations, biases take a
key role~\cite{mitchell_1980_bias}. This paper proposes an Errors-in-Variables
Adapter (EVA) model to induce learning of proper generalizations by explicitly
employing biases to identity estimation based on prior knowledge about the
target situation. To better match the source face with the target situation in
terms of pose, expression, and background factors, we model the bias as a
causal effect of the target situation on source identity and estimate this
effect through a controlled intervention trial. To achieve smoother transfer
for the target face across the identity gap, we eliminate the target face
specificity through multiple kernel regressions. The kernels are used to
constrain the regressions to operate only on identity information in the
internal representations of the target image, while leaving other perceptual
information invariant. Combining these post-regression representations with the
biased estimation for identity, EVA shows impressive performance even in the
presence of large gaps, providing empirical evidence supporting the utility of
the inductive biases in identity estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weak Novel Categories without Tears: A Survey on Weak-Shot Learning. (arXiv:2110.02651v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02651">
<div class="article-summary-box-inner">
<span><p>Deep learning is a data-hungry approach, which requires massive training
data. However, it is time-consuming and labor-intensive to collect abundant
fully-annotated training data for all categories. Assuming the existence of
base categories with adequate fully-annotated training samples, different
paradigms requiring fewer training samples or weaker annotations for novel
categories have attracted growing research interest. Among them, zero-shot
(resp., few-shot) learning explores using zero (resp., a few) training samples
for novel categories, which lowers the quantity requirement for novel
categories. Instead, weak-shot learning lowers the quality requirement for
novel categories. Specifically, sufficient training samples are collected for
novel categories but they only have weak annotations. In different tasks, weak
annotations are presented in different forms (e.g., noisy labels for image
classification, image labels for object detection, bounding boxes for
segmentation), similar to the definitions in weakly supervised learning.
Therefore, weak-shot learning can also be treated as weakly supervised learning
with auxiliary fully supervised categories. In this paper, we discuss the
existing weak-shot learning methodologies in different tasks and summarize the
codes at https://github.com/bcmi/Awesome-Weak-Shot-Learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepBBS: Deep Best Buddies for Point Cloud Registration. (arXiv:2110.03016v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03016">
<div class="article-summary-box-inner">
<span><p>Recently, several deep learning approaches have been proposed for point cloud
registration. These methods train a network to generate a representation that
helps finding matching points in two 3D point clouds. Finding good matches
allows them to calculate the transformation between the point clouds
accurately. Two challenges of these techniques are dealing with occlusions and
generalizing to objects of classes unseen during training. This work proposes
DeepBBS, a novel method for learning a representation that takes into account
the best buddy distance between points during training. Best Buddies (i.e.,
mutual nearest neighbors) are pairs of points nearest to each other. The Best
Buddies criterion is a strong indication for correct matches that, in turn,
leads to accurate registration. Our experiments show improved performance
compared to previous methods. In particular, our learned representation leads
to an accurate registration for partial shapes and in unseen categories.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Propagating State Uncertainty Through Trajectory Forecasting. (arXiv:2110.03267v3 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03267">
<div class="article-summary-box-inner">
<span><p>Uncertainty pervades through the modern robotic autonomy stack, with nearly
every component (e.g., sensors, detection, classification, tracking, behavior
prediction) producing continuous or discrete probabilistic distributions.
Trajectory forecasting, in particular, is surrounded by uncertainty as its
inputs are produced by (noisy) upstream perception and its outputs are
predictions that are often probabilistic for use in downstream planning.
However, most trajectory forecasting methods do not account for upstream
uncertainty, instead taking only the most-likely values. As a result,
perceptual uncertainties are not propagated through forecasting and predictions
are frequently overconfident. To address this, we present a novel method for
incorporating perceptual state uncertainty in trajectory forecasting, a key
component of which is a new statistical distance-based loss function which
encourages predicting uncertainties that better match upstream perception. We
evaluate our approach both in illustrative simulations and on large-scale,
real-world data, demonstrating its efficacy in propagating perceptual state
uncertainty through prediction and producing more calibrated predictions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta-Learning with Task-Adaptive Loss Function for Few-Shot Learning. (arXiv:2110.03909v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03909">
<div class="article-summary-box-inner">
<span><p>In few-shot learning scenarios, the challenge is to generalize and perform
well on new unseen examples when only very few labeled examples are available
for each task. Model-agnostic meta-learning (MAML) has gained the popularity as
one of the representative few-shot learning methods for its flexibility and
applicability to diverse problems. However, MAML and its variants often resort
to a simple loss function without any auxiliary loss function or regularization
terms that can help achieve better generalization. The problem lies in that
each application and task may require different auxiliary loss function,
especially when tasks are diverse and distinct. Instead of attempting to
hand-design an auxiliary loss function for each application and task, we
introduce a new meta-learning framework with a loss function that adapts to
each task. Our proposed framework, named Meta-Learning with Task-Adaptive Loss
Function (MeTAL), demonstrates the effectiveness and the flexibility across
various domains, such as few-shot classification and few-shot regression.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Point Cloud Prediction Using 3D Spatio-temporal Convolutional Networks. (arXiv:2110.04076v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04076">
<div class="article-summary-box-inner">
<span><p>Exploiting past 3D LiDAR scans to predict future point clouds is a promising
method for autonomous mobile systems to realize foresighted state estimation,
collision avoidance, and planning. In this paper, we address the problem of
predicting future 3D LiDAR point clouds given a sequence of past LiDAR scans.
Estimating the future scene on the sensor level does not require any preceding
steps as in localization or tracking systems and can be trained
self-supervised. We propose an end-to-end approach that exploits a 2D range
image representation of each 3D LiDAR scan and concatenates a sequence of range
images to obtain a 3D tensor. Based on such tensors, we develop an
encoder-decoder architecture using 3D convolutions to jointly aggregate spatial
and temporal information of the scene and to predict the future 3D point
clouds. We evaluate our method on multiple datasets and the experimental
results suggest that our method outperforms existing point cloud prediction
architectures and generalizes well to new, unseen environments without
additional fine-tuning. Our method operates online and is faster than the
common LiDAR frame rate of 10 Hz.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward a Human-Level Video Understanding Intelligence. (arXiv:2110.04203v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04203">
<div class="article-summary-box-inner">
<span><p>We aim to develop an AI agent that can watch video clips and have a
conversation with human about the video story. Developing video understanding
intelligence is a significantly challenging task, and evaluation methods for
adequately measuring and analyzing the progress of AI agent are lacking as
well. In this paper, we propose the Video Turing Test to provide effective and
practical assessments of video understanding intelligence as well as
human-likeness evaluation of AI agents. We define a general format and
procedure of the Video Turing Test and present a case study to confirm the
effectiveness and usefulness of the proposed test.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep learning-based person re-identification methods: A survey and outlook of recent works. (arXiv:2110.04764v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04764">
<div class="article-summary-box-inner">
<span><p>In recent years, with the increasing demand for public safety and the rapid
development of intelligent surveillance networks, person re-identification
(Re-ID) has become one of the hot research topics in the field of computer
vision. The main research goal of person Re-ID is to retrieve persons with the
same identity from different cameras. However, traditional person Re-ID methods
require manual marking of person targets, which consumes a lot of labor cost.
With the widespread application of deep neural networks in the field of
computer vision, a large number of deep learning-based person Re-ID methods
have emerged. Therefore, this paper is to facilitate researchers to better
understand the latest research results and the future trends in the field.
Firstly, we summarize the main study of several recently published person
re-identification surveys and try to fill the gaps between them. Secondly, We
propose a multi-dimensional taxonomy to categorize the most current deep
learning-based person Re-ID methods according to different characteristics,
including methods for deep metric learning, local feature learning, generate
adversarial networks, sequence feature learning and graph convolutional
networks. Furthermore, we subdivide the above five categories according to
their technique types, discussing and comparing the experimental performance of
part subcategories. Finally, we conclude this paper and discuss future research
directions for person Re-ID.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Training of 3D Seismic Image Fault Segmentation Network under Sparse Labels by Weakening Anomaly Annotation. (arXiv:2110.05319v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05319">
<div class="article-summary-box-inner">
<span><p>Seismic data fault detection has recently been regarded as a 3D image
segmentation task. The nature of fault structures in seismic image makes it
difficult to manually label faults. Manual labeling often has many false
negative labels (abnormal annotations), which will seriously harm the training
process. In this work, we find that region-based loss significantly outperforms
distribution-based loss when dealing with false negative labels, therefore we
proposed Mask Dice loss (MD loss), which is the first reported region-based
loss function for training 3D image segmentation networks using sparse 2D slice
labels. In addition, fault is an edge feature, and the current network widely
used for fault segmentation downsamples the features multiple times, which is
not conducive to edge representation and thus requires many parameters and
computational effort to preserve the features. We proposed Fault-Net, which
uses a high-resolution and shallow structure to propagate multi-scale features
in parallel, fully preserving edge features. Meanwhile, in order to efficiently
fuse multi-scale features, we decouple the convolution process into feature
selection and channel fusion, and proposed a lightweight feature fusion block,
Multi-Scale Compression Fusion (MCF). Because the Fault-Net always keeps the
edge features during propagation, only few parameters and computation are
required. Experimental results show that MD loss can clearly weaken the effect
of false negative labels. The Fault-Net parameter is only 0.42MB, support up to
528^3 (1.5x10^8, Float32) size cuboid inference on 16GB video ram, its
inference speed on CPU and GPU is significantly faster than other networks. It
works well on most of the open data seismic images, and the result of our
method is the state-of-the-art in the FORCE fault identification competition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NAS-Bench-360: Benchmarking Diverse Tasks for Neural Architecture Search. (arXiv:2110.05668v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05668">
<div class="article-summary-box-inner">
<span><p>Most existing neural architecture search (NAS) benchmarks and algorithms
prioritize performance on well-studied tasks, e.g., image classification on
CIFAR and ImageNet. This makes the applicability of NAS approaches in more
diverse areas inadequately understood. In this paper, we present NAS-Bench-360,
a benchmark suite for evaluating state-of-the-art NAS methods for convolutional
neural networks (CNNs). To construct it, we curate a collection of ten tasks
spanning a diverse array of application domains, dataset sizes, problem
dimensionalities, and learning objectives. By carefully selecting tasks that
can both interoperate with modern CNN-based search methods but that are also
far-afield from their original development domain, we can use NAS-Bench-360 to
investigate the following central question: do existing state-of-the-art NAS
methods perform well on diverse tasks? Our experiments show that a modern NAS
procedure designed for image classification can indeed find good architectures
for tasks with other dimensionalities and learning objectives; however, the
same method struggles against more task-specific methods and performs
catastrophically poorly on classification in non-vision domains. The case for
NAS robustness becomes even more dire in a resource-constrained setting, where
a recent NAS method provides little-to-no benefit over much simpler baselines.
These results demonstrate the need for a benchmark such as NAS-Bench-360 to
help develop NAS approaches that work well on a variety of tasks, a crucial
component of a truly robust and automated pipeline. We conclude with a
demonstration of the kind of future research our suite of tasks will enable.
All data and code is made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relation-aware Video Reading Comprehension for Temporal Language Grounding. (arXiv:2110.05717v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05717">
<div class="article-summary-box-inner">
<span><p>Temporal language grounding in videos aims to localize the temporal span
relevant to the given query sentence. Previous methods treat it either as a
boundary regression task or a span extraction task. This paper will formulate
temporal language grounding into video reading comprehension and propose a
Relation-aware Network (RaNet) to address it. This framework aims to select a
video moment choice from the predefined answer set with the aid of
coarse-and-fine choice-query interaction and choice-choice relation
construction. A choice-query interactor is proposed to match the visual and
textual information simultaneously in sentence-moment and token-moment levels,
leading to a coarse-and-fine cross-modal interaction. Moreover, a novel
multi-choice relation constructor is introduced by leveraging graph convolution
to capture the dependencies among video moment choices for the best choice
selection. Extensive experiments on ActivityNet-Captions, TACoS, and
Charades-STA demonstrate the effectiveness of our solution. Codes will be
released soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Open Source User Activity Traces with Applications to User Mobility Characterization and Modeling. (arXiv:2110.06382v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06382">
<div class="article-summary-box-inner">
<span><p>The current state-of-the-art in user mobility research has extensively relied
on open-source mobility traces captured from pedestrian and vehicular activity
through a variety of communication technologies as users engage in a wide-range
of applications, including connected healthcare, localization, social media,
e-commerce, etc. Most of these traces are feature-rich and diverse, not only in
the information they provide, but also in how they can be used and leveraged.
This diversity poses two main challenges for researchers and practitioners who
wish to make use of available mobility datasets. First, it is quite difficult
to get a bird's eye view of the available traces without spending considerable
time looking them up. Second, once they have found the traces, they still need
to figure out whether the traces are adequate to their needs.
</p>
<p>The purpose of this survey is three-fold. It proposes a taxonomy to classify
open-source mobility traces including their mobility mode, data source and
collection technology. It then uses the proposed taxonomy to classify existing
open-source mobility traces and finally, highlights three case studies using
popular publicly available datasets to showcase how our taxonomy can tease out
feature sets in traces to help determine their applicability to specific
use-cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">THOMAS: Trajectory Heatmap Output with learned Multi-Agent Sampling. (arXiv:2110.06607v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06607">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose THOMAS, a joint multi-agent trajectory prediction
framework allowing for efficient and consistent prediction of multi-agent
multi-modal trajectories. We present a unified model architecture for fast and
simultaneous agent future heatmap estimation leveraging hierarchical and sparse
image generation. We demonstrate that heatmap output enables a higher level of
control on the predicted trajectories compared to vanilla multi-modal
trajectory regression, allowing to incorporate additional constraints for
tighter sampling or collision-free predictions in a deterministic way. However,
we also highlight that generating scene-consistent predictions goes beyond the
mere generation of collision-free trajectories. We therefore propose a
learnable trajectory recombination model that takes as input a set of predicted
trajectories for each agent and outputs its consistent reordered recombination.
We report our results on the Interaction multi-agent prediction challenge and
rank $1^{st}$ on the online test leaderboard.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ADOP: Approximate Differentiable One-Pixel Point Rendering. (arXiv:2110.06635v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06635">
<div class="article-summary-box-inner">
<span><p>We present a novel point-based, differentiable neural rendering pipeline for
scene refinement and novel view synthesis. The input are an initial estimate of
the point cloud and the camera parameters. The output are synthesized images
from arbitrary camera poses. The point cloud rendering is performed by a
differentiable renderer using multi-resolution one-pixel point rasterization.
Spatial gradients of the discrete rasterization are approximated by the novel
concept of ghost geometry. After rendering, the neural image pyramid is passed
through a deep neural network for shading calculations and hole-filling. A
differentiable, physically-based tonemapper then converts the intermediate
output to the target image. Since all stages of the pipeline are
differentiable, we optimize all of the scene's parameters i.e. camera model,
camera pose, point position, point color, environment map, rendering network
weights, vignetting, camera response function, per image exposure, and per
image white balance. We show that our system is able to synthesize sharper and
more consistent novel views than existing approaches because the initial
reconstruction is refined during training. The efficient one-pixel point
rasterization allows us to use arbitrary camera models and display scenes with
well over 100M points in real time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Users' Mental Model with Attention-directed Counterfactual Edits. (arXiv:2110.06863v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06863">
<div class="article-summary-box-inner">
<span><p>In the domain of Visual Question Answering (VQA), studies have shown
improvement in users' mental model of the VQA system when they are exposed to
examples of how these systems answer certain Image-Question (IQ) pairs. In this
work, we show that showing controlled counterfactual image-question examples
are more effective at improving the mental model of users as compared to simply
showing random examples. We compare a generative approach and a retrieval-based
approach to show counterfactual examples. We use recent advances in generative
adversarial networks (GANs) to generate counterfactual images by deleting and
inpainting certain regions of interest in the image. We then expose users to
changes in the VQA system's answer on those altered images. To select the
region of interest for inpainting, we experiment with using both
human-annotated attention maps and a fully automatic method that uses the VQA
system's attention values. Finally, we test the user's mental model by asking
them to predict the model's performance on a test counterfactual image. We note
an overall improvement in users' accuracy to predict answer change when shown
counterfactual explanations. While realistic retrieved counterfactuals
obviously are the most effective at improving the mental model, we show that a
generative approach can also be equally effective.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking the Representational Continuity: Towards Unsupervised Continual Learning. (arXiv:2110.06976v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06976">
<div class="article-summary-box-inner">
<span><p>Continual learning (CL) aims to learn a sequence of tasks without forgetting
the previously acquired knowledge. However, recent advances in continual
learning are restricted to supervised continual learning (SCL) scenarios.
Consequently, they are not scalable to real-world applications where the data
distribution is often biased and unannotated. In this work, we focus on
unsupervised continual learning (UCL), where we learn the feature
representations on an unlabelled sequence of tasks and show that reliance on
annotated data is not necessary for continual learning. We conduct a systematic
study analyzing the learned feature representations and show that unsupervised
visual representations are surprisingly more robust to catastrophic forgetting,
consistently achieve better performance, and generalize better to
out-of-distribution tasks than SCL. Furthermore, we find that UCL achieves a
smoother loss landscape through qualitative analysis of the learned
representations and learns meaningful feature representations. Additionally, we
propose Lifelong Unsupervised Mixup (LUMP), a simple yet effective technique
that leverages the interpolation between the current task and previous tasks'
instances to alleviate catastrophic forgetting for unsupervised
representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Proposal Extension with LSTM Network for Weakly Supervised Object Detection. (arXiv:2110.07511v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07511">
<div class="article-summary-box-inner">
<span><p>Weakly supervised object detection (WSOD) has attracted more and more
attention since it only uses image-level labels and can save huge annotation
costs. Most of the WSOD methods use Multiple Instance Learning (MIL) as their
basic framework, which regard it as an instance classification problem.
However, these methods based on MIL tends to converge only on the most
discriminate regions of different instances, rather than their corresponding
complete regions, that is, insufficient integrity. Inspired by the habit of
observing things by the human, we propose a new method by comparing the initial
proposals and the extension ones to optimize those initial proposals.
Specifically, we propose one new strategy for WSOD by involving contrastive
proposal extension (CPE), which consists of multiple directional contrastive
proposal extensions (D-CPE), and each D-CPE contains encoders based on LSTM
network and corresponding decoders. Firstly, the boundary of initial proposals
in MIL is extended to different positions according to well-designed sequential
order. Then, CPE compares the extended proposal and the initial proposal by
extracting the feature semantics of them using the encoders, and calculates the
integrity of the initial proposal to optimize the score of the initial
proposal. These contrastive contextual semantics will guide the basic WSOD to
suppress bad proposals and improve the scores of good ones. In addition, a
simple two-stream network is designed as the decoder to constrain the temporal
coding of LSTM and improve the performance of WSOD further. Experiments on
PASCAL VOC 2007, VOC 2012 and MS-COCO datasets show that our method has
achieved the state-of-the-art results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeRS: Neural Reflectance Surfaces for Sparse-view 3D Reconstruction in the Wild. (arXiv:2110.07604v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07604">
<div class="article-summary-box-inner">
<span><p>Recent history has seen a tremendous growth of work exploring implicit
representations of geometry and radiance, popularized through Neural Radiance
Fields (NeRF). Such works are fundamentally based on a (implicit) volumetric
representation of occupancy, allowing them to model diverse scene structure
including translucent objects and atmospheric obscurants. But because the vast
majority of real-world scenes are composed of well-defined surfaces, we
introduce a surface analog of such implicit models called Neural Reflectance
Surfaces (NeRS). NeRS learns a neural shape representation of a closed surface
that is diffeomorphic to a sphere, guaranteeing water-tight reconstructions.
Even more importantly, surface parameterizations allow NeRS to learn (neural)
bidirectional surface reflectance functions (BRDFs) that factorize
view-dependent appearance into environmental illumination, diffuse color
(albedo), and specular "shininess." Finally, rather than illustrating our
results on synthetic scenes or controlled in-the-lab capture, we assemble a
novel dataset of multi-view images from online marketplaces for selling goods.
Such "in-the-wild" multi-view image sets pose a number of challenges, including
a small number of views with unknown/rough camera estimates. We demonstrate
that surface-based neural reconstructions enable learning from such data,
outperforming volumetric neural rendering-based reconstructions. We hope that
NeRS serves as a first step toward building scalable, high-quality libraries of
real-world shape, materials, and illumination. The project page with code and
video visualizations can be found at https://jasonyzhang.com/ners.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EMDS-7: Environmental Microorganism Image Dataset Seventh Version for Multiple Object Detection Evaluation. (arXiv:2110.07723v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07723">
<div class="article-summary-box-inner">
<span><p>The Environmental Microorganism Image Dataset Seventh Version (EMDS-7) is a
microscopic image data set, including the original Environmental Microorganism
images (EMs) and the corresponding object labeling files in ".XML" format file.
The EMDS-7 data set consists of 41 types of EMs, which has a total of 2365
images and 13216 labeled objects. The EMDS-7 database mainly focuses on the
object detection. In order to prove the effectiveness of EMDS-7, we select the
most commonly used deep learning methods (Faster-RCNN, YOLOv3, YOLOv4, SSD and
RetinaNet) and evaluation indices for testing and evaluation. EMDS-7 is freely
published for non-commercial purpose at: https://github.com/yanghechen/EMDS-7
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PTQ-SL: Exploring the Sub-layerwise Post-training Quantization. (arXiv:2110.07809v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07809">
<div class="article-summary-box-inner">
<span><p>Network quantization is a powerful technique to compress convolutional neural
networks. The quantization granularity determines how to share the scaling
factors in weights, which affects the performance of network quantization. Most
existing approaches share the scaling factors layerwisely or channelwisely for
quantization of convolutional layers. Channelwise quantization and layerwise
quantization have been widely used in various applications. However, other
quantization granularities are rarely explored. In this paper, we will explore
the sub-layerwise granularity that shares the scaling factor across multiple
input and output channels. We propose an efficient post-training quantization
method in sub-layerwise granularity (PTQ-SL). Then we systematically experiment
on various granularities and observe that the prediction accuracy of the
quantized neural network has a strong correlation with the granularity.
Moreover, we find that adjusting the position of the channels can improve the
performance of sub-layerwise quantization. Therefore, we propose a method to
reorder the channels for sub-layerwise quantization. The experiments
demonstrate that the sub-layerwise quantization with appropriate channel
reordering can outperform the channelwise quantization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MaGNET: Uniform Sampling from Deep Generative Network Manifolds Without Retraining. (arXiv:2110.08009v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08009">
<div class="article-summary-box-inner">
<span><p>Deep Generative Networks (DGNs) are extensively employed in Generative
Adversarial Networks (GANs), Variational Autoencoders (VAEs), and their
variants to approximate the data manifold, and data distribution on that
manifold. However, training samples are often obtained based on preferences,
costs, or convenience producing artifacts in the empirical data distribution
e.g., the large fraction of smiling faces in the CelebA dataset or the large
fraction of dark-haired individuals in FFHQ. These inconsistencies will be
reproduced when sampling from the trained DGN, which has far-reaching potential
implications for fairness, data augmentation, anomaly detection, domain
adaptation, and beyond. In response, we develop a differential geometry based
sampler -- coined MaGNET -- that, given any trained DGN, produces samples that
are uniformly distributed on the learned manifold. We prove theoretically and
empirically that our technique produces a uniform distribution on the manifold
regardless of the training set distribution. We perform a range of experiments
on various datasets and DGNs. One of them considers the state-of-the-art
StyleGAN2 trained on FFHQ dataset, where uniform sampling via MaGNET increases
distribution precision and recall by 4.1% &amp; 3.0% and decreases gender bias by
41.2%, without requiring labels or retraining.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FlexConv: Continuous Kernel Convolutions with Differentiable Kernel Sizes. (arXiv:2110.08059v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08059">
<div class="article-summary-box-inner">
<span><p>When designing Convolutional Neural Networks (CNNs), one must select the size
of the convolutional kernels before training. Recent works show CNNs benefit
from different kernel sizes at different layers, but exploring all possible
combinations is unfeasible in practice. A more efficient approach is to learn
the kernel size during training. However, existing works that learn the kernel
size have a limited bandwidth. These approaches scale kernels by dilation, and
thus the detail they can describe is limited. In this work, we propose
FlexConv, a novel convolutional operation with which high bandwidth
convolutional kernels of learnable kernel size can be learned at a fixed
parameter cost. FlexNets model long-term dependencies without the use of
pooling, achieve state-of-the-art performance on several sequential datasets,
outperform recent works with learned kernel sizes, and are competitive with
much deeper ResNets on image benchmark datasets. Additionally, FlexNets can be
deployed at higher resolutions than those seen during training. To avoid
aliasing, we propose a novel kernel parameterization with which the frequency
of the kernels can be analytically controlled. Our novel kernel
parameterization shows higher descriptive power and faster convergence speed
than existing parameterizations. This leads to important improvements in
classification accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MedAug: Contrastive learning leveraging patient metadata improves representations for chest X-ray interpretation. (arXiv:2102.10663v2 [eess.IV] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.10663">
<div class="article-summary-box-inner">
<span><p>Self-supervised contrastive learning between pairs of multiple views of the
same image has been shown to successfully leverage unlabeled data to produce
meaningful visual representations for both natural and medical images. However,
there has been limited work on determining how to select pairs for medical
images, where availability of patient metadata can be leveraged to improve
representations. In this work, we develop a method to select positive pairs
coming from views of possibly different images through the use of patient
metadata. We compare strategies for selecting positive pairs for chest X-ray
interpretation including requiring them to be from the same patient, imaging
study or laterality. We evaluate downstream task performance by fine-tuning the
linear layer on 1% of the labeled dataset for pleural effusion classification.
Our best performing positive pair selection strategy, which involves using
images from the same patient from the same study across all lateralities,
achieves a performance increase of 14.4% in mean AUC from the ImageNet
pretrained baseline. Our controlled experiments show that the keys to improving
downstream performance on disease classification are (1) using patient metadata
to appropriately create positive pairs from different images with the same
underlying pathologies, and (2) maximizing the number of different images used
in query pairing. In addition, we explore leveraging patient metadata to select
hard negative pairs for contrastive learning, but do not find improvement over
baselines that do not use metadata. Our method is broadly applicable to medical
image interpretation and allows flexibility for incorporating medical insights
in choosing pairs for contrastive learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Core Challenges in Embodied Vision-Language Planning. (arXiv:2106.13948v2 [cs.LG] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.13948">
<div class="article-summary-box-inner">
<span><p>Recent advances in the areas of multimodal machine learning and artificial
intelligence (AI) have led to the development of challenging tasks at the
intersection of Computer Vision, Natural Language Processing, and Embodied AI.
Whereas many approaches and previous survey pursuits have characterised one or
two of these dimensions, there has not been a holistic analysis at the center
of all three. Moreover, even when combinations of these topics are considered,
more focus is placed on describing, e.g., current architectural methods, as
opposed to also illustrating high-level challenges and opportunities for the
field. In this survey paper, we discuss Embodied Vision-Language Planning
(EVLP) tasks, a family of prominent embodied navigation and manipulation
problems that jointly use computer vision and natural language. We propose a
taxonomy to unify these tasks and provide an in-depth analysis and comparison
of the new and current algorithmic approaches, metrics, simulated environments,
as well as the datasets used for EVLP tasks. Finally, we present the core
challenges that we believe new EVLP works should seek to address, and we
advocate for task construction that enables model generalizability and furthers
real-world deployment.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-10-19 23:02:19.890198354 UTC">2021-10-19 23:02:19 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.4</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>