<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-01-20T01:30:00Z">01-20</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Annotating the Tweebank Corpus on Named Entity Recognition and Building NLP Models for Social Media Analysis. (arXiv:2201.07281v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07281">
<div class="article-summary-box-inner">
<span><p>Social media data such as Twitter messages ("tweets") pose a particular
challenge to NLP systems because of their short, noisy, and colloquial nature.
Tasks such as Named Entity Recognition (NER) and syntactic parsing require
highly domain-matched training data for good performance. While there are some
publicly available annotated datasets of tweets, they are all purpose-built for
solving one task at a time. As yet there is no complete training corpus for
both syntactic analysis (e.g., part of speech tagging, dependency parsing) and
NER of tweets. In this study, we aim to create Tweebank-NER, an NER corpus
based on Tweebank V2 (TB2), and we use these datasets to train state-of-the-art
NLP models. We first annotate named entities in TB2 using Amazon Mechanical
Turk and measure the quality of our annotations. We train a Stanza NER model on
the new benchmark, achieving competitive performance against other
non-transformer NER systems. Finally, we train other Twitter NLP models (a
tokenizer, lemmatizer, part of speech tagger, and dependency parser) on TB2
based on Stanza, and achieve state-of-the-art or competitive performance on
these tasks. We release the dataset and make the models available to use in an
"off-the-shelf" manner for future Tweet NLP research. Our source code, data,
and pre-trained models are available at:
\url{https://github.com/social-machines/TweebankNLP}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extending the Vocabulary of Fictional Languages using Neural Networks. (arXiv:2201.07288v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07288">
<div class="article-summary-box-inner">
<span><p>Fictional languages have become increasingly popular over the recent years
appearing in novels, movies, TV shows, comics, and video games. While some of
these fictional languages have a complete vocabulary, most do not. We propose a
deep learning solution to the problem. Using style transfer and machine
translation tools, we generate new words for a given target fictional language,
while maintaining the style of its creator, hence extending this language
vocabulary.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Datasheet for the Pile. (arXiv:2201.07311v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07311">
<div class="article-summary-box-inner">
<span><p>This datasheet describes the Pile, a 825 GiB dataset of human-authored text
compiled by EleutherAI for use in large-scale language modeling. The Pile is
comprised of 22 different text sources, ranging from original scrapes done for
this project, to text data made available by the data owners, to third-party
scrapes available online.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Privacy-Preserving Unsupervised Domain Adaptation Framework for Clinical Text Analysis. (arXiv:2201.07317v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07317">
<div class="article-summary-box-inner">
<span><p>Unsupervised domain adaptation (UDA) generally aligns the unlabeled target
domain data to the distribution of the source domain to mitigate the
distribution shift problem. The standard UDA requires sharing the source data
with the target, having potential data privacy leaking risks. To protect the
source data's privacy, we first propose to share the source feature
distribution instead of the source data. However, sharing only the source
feature distribution may still suffer from the membership inference attack who
can infer an individual's membership by the black-box access to the source
model. To resolve this privacy issue, we further study the under-explored
problem of privacy-preserving domain adaptation and propose a method with a
novel differential privacy training strategy to protect the source data
privacy. We model the source feature distribution by Gaussian Mixture Models
(GMMs) under the differential privacy setting and send it to the target client
for adaptation. The target client resamples differentially private source
features from GMMs and adapts on target data with several state-of-art UDA
backbones. With our proposed method, the source data provider could avoid
leaking source data privacy during domain adaptation as well as reserve the
utility. To evaluate our proposed method's utility and privacy loss, we apply
our model on a medical report disease label classification task using two noisy
challenging clinical text datasets. The results show that our proposed method
can preserve source data's privacy with a minor performance influence on the
text classification task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning grammar with a divide-and-concur neural network. (arXiv:2201.07341v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07341">
<div class="article-summary-box-inner">
<span><p>We implement a divide-and-concur iterative projection approach to
context-free grammar inference. Unlike most state-of-the-art models of natural
language processing, our method requires a relatively small number of discrete
parameters, making the inferred grammar directly interpretable -- one can read
off from a solution how to construct grammatically valid sentences. Another
advantage of our approach is the ability to infer meaningful grammatical rules
from just a few sentences, compared to the hundreds of gigabytes of training
data many other models employ. We demonstrate several ways of applying our
approach: classifying words and inferring a grammar from scratch, taking an
existing grammar and refining its categories and rules, and taking an existing
grammar and expanding its lexicon as it encounters new words in new data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Neural Machine Translation by Denoising Training. (arXiv:2201.07365v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07365">
<div class="article-summary-box-inner">
<span><p>We present a simple and effective pretraining strategy {D}en{o}ising
{T}raining DoT for neural machine translation. Specifically, we update the
model parameters with source- and target-side denoising tasks at the early
stage and then tune the model normally. Notably, our approach does not increase
any parameters or training steps, requiring the parallel data merely.
Experiments show that DoT consistently improves the neural machine translation
performance across 12 bilingual and 16 multilingual directions (data size
ranges from 80K to 20M). In addition, we show that DoT can complement existing
data manipulation strategies, i.e. curriculum learning, knowledge distillation,
data diversification, bidirectional training, and back-translation.
Encouragingly, we found that DoT outperforms costly pretrained model mBART in
high-resource settings. Analyses show DoT is a novel in-domain cross-lingual
pretraining strategy and could offer further improvements with task-relevant
self-supervisions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Language Models are Effective Plagiarists. (arXiv:2201.07406v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07406">
<div class="article-summary-box-inner">
<span><p>As artificial intelligence (AI) technologies become increasingly powerful and
prominent in society, their misuse is a growing concern. In educational
settings, AI technologies could be used by students to cheat on assignments and
exams. In this paper we explore whether transformers can be used to solve
introductory level programming assignments while bypassing commonly used AI
tools to detect plagiarism. We find that a student using GPT-J [Wang and
Komatsuzaki, 2021] can complete introductory level programming assignments
without triggering suspicion from MOSS [Aiken, 2000], a widely used plagiarism
detection tool. This holds despite the fact that GPT-J was not trained on the
problems in question and is not provided with any examples to work from. We
further find that the code written by GPT-J is diverse in structure, lacking
any particular tells that future plagiarism detection techniques may use to try
to identify algorithmically generated code. We conclude with a discussion of
the ethical and educational implications of large language models and
directions for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Many Ways to be Lonely: Fine-grained Characterization of Loneliness and its Potential Changes in COVID-19. (arXiv:2201.07423v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07423">
<div class="article-summary-box-inner">
<span><p>Loneliness has been associated with negative outcomes for physical and mental
health. Understanding how people express and cope with various forms of
loneliness is critical for early screening and targeted interventions to reduce
loneliness, particularly among vulnerable groups such as young adults. To
examine how different forms of loneliness and coping strategies manifest in
loneliness self-disclosure, we built a dataset, FIG-Loneliness (FIne-Grained
Loneliness) by using Reddit posts in two young adult-focused forums and two
loneliness related forums consisting of a diverse age group. We provide
annotations by trained human annotators for binary and fine-grained loneliness
classifications of the posts. Trained on FIG-Loneliness, two BERT-based models
were used to understand loneliness forms and authors' coping strategies in
these forums. Our binary loneliness classification archived an accuracy above
97%, and fine-grained loneliness category classification reached an average
accuracy of 77% across all labeled categories. With FIG-Loneliness and model
predictions, we found that loneliness expressions in the young adult related
forums are distinct from other forums. Those in young adult-focused forums are
more likely to express concerns pertaining to peer relationship, and are
potentially more sensitive to geographical isolation impacted by the COVID-19
pandemic lockdown. Also, we show that different forms of loneliness have
differential use in coping strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpreting Arabic Transformer Models. (arXiv:2201.07434v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07434">
<div class="article-summary-box-inner">
<span><p>Arabic is a Semitic language which is widely spoken with many dialects. Given
the success of pre-trained language models, many transformer models trained on
Arabic and its dialects have surfaced. While these models have been compared
with respect to downstream NLP tasks, no evaluation has been carried out to
directly compare the internal representations. We probe how linguistic
information is encoded in Arabic pretrained models, trained on different
varieties of Arabic language. We perform a layer and neuron analysis on the
models using three intrinsic tasks: two morphological tagging tasks based on
MSA (modern standard Arabic) and dialectal POS-tagging and a dialectal
identification task. Our analysis enlightens interesting findings such as: i)
word morphology is learned at the lower and middle layers ii) dialectal
identification necessitate more knowledge and hence preserved even in the final
layers, iii) despite a large overlap in their vocabulary, the MSA-based models
fail to capture the nuances of Arabic dialects, iv) we found that neurons in
embedding layers are polysemous in nature, while the neurons in middle layers
are exclusive to specific properties.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TourBERT: A pretrained language model for the tourism industry. (arXiv:2201.07449v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07449">
<div class="article-summary-box-inner">
<span><p>The Bidirectional Encoder Representations from Transformers (BERT) is
currently one of the most important and state-of-the-art models for natural
language. However, it has also been shown that for domain-specific tasks it is
helpful to pretrain BERT on a domain-specific corpus. In this paper, we present
TourBERT, a pretrained language model for tourism. We describe how TourBERT was
developed and evaluated. The evaluations show that TourBERT is outperforming
BERT in all tourism-specific tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Development of Fake News Model using Machine Learning through Natural Language Processing. (arXiv:2201.07489v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07489">
<div class="article-summary-box-inner">
<span><p>Fake news detection research is still in the early stage as this is a
relatively new phenomenon in the interest raised by society. Machine learning
helps to solve complex problems and to build AI systems nowadays and especially
in those cases where we have tacit knowledge or the knowledge that is not
known. We used machine learning algorithms and for identification of fake news;
we applied three classifiers; Passive Aggressive, Na\"ive Bayes, and Support
Vector Machine. Simple classification is not completely correct in fake news
detection because classification methods are not specialized for fake news.
With the integration of machine learning and text-based processing, we can
detect fake news and build classifiers that can classify the news data. Text
classification mainly focuses on extracting various features of text and after
that incorporating those features into classification. The big challenge in
this area is the lack of an efficient way to differentiate between fake and
non-fake due to the unavailability of corpora. We applied three different
machine learning classifiers on two publicly available datasets. Experimental
analysis based on the existing dataset indicates a very encouraging and
improved performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CM3: A Causal Masked Multimodal Model of the Internet. (arXiv:2201.07520v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07520">
<div class="article-summary-box-inner">
<span><p>We introduce CM3, a family of causally masked generative models trained over
a large corpus of structured multi-modal documents that can contain both text
and image tokens. Our new causally masked approach generates tokens left to
right while also masking out a small number of long token spans that are
generated at the end of the string, instead of their original positions. The
casual masking object provides a type of hybrid of the more common causal and
masked language models, by enabling full generative modeling while also
providing bidirectional context when generating the masked spans. We train
causally masked language-image models on large-scale web and Wikipedia
articles, where each document contains all of the text, hypertext markup,
hyperlinks, and image tokens (from a VQVAE-GAN), provided in the order they
appear in the original HTML source (before masking). The resulting CM3 models
can generate rich structured, multi-modal outputs while conditioning on
arbitrary masked document contexts, and thereby implicitly learn a wide range
of text, image, and cross modal tasks. They can be prompted to recover, in a
zero-shot fashion, the functionality of models such as DALL-E, GENRE, and HTLM.
We set the new state-of-the-art in zero-shot summarization, entity linking, and
entity disambiguation while maintaining competitive performance in the
fine-tuning setting. We can generate images unconditionally, conditioned on
text (like DALL-E) and do captioning all in a zero-shot setting with a single
model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Writing about COVID-19 vaccines: Emotional profiling unravels how mainstream and alternative press framed AstraZeneca, Pfizer and vaccination campaigns. (arXiv:2201.07538v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07538">
<div class="article-summary-box-inner">
<span><p>Since their announcement in November 2020, COVID-19 vaccines were largely
debated by the press and social media. With most studies focusing on COVID-19
disinformation in social media, little attention has been paid to how
mainstream news outlets framed COVID-19 narratives compared to alternative
sources. To fill this gap, we use cognitive network science and natural
language processing to reconstruct time-evolving semantic and emotional frames
of 5745 Italian news, that were massively re-shared on Facebook and Twitter,
about COVID-19 vaccines. We found consistently high levels of
trust/anticipation and less disgust in the way mainstream sources framed the
general idea of "vaccine/vaccino". These emotions were crucially missing in the
ways alternative sources framed COVID-19 vaccines. More differences were found
within specific instances of vaccines. Alternative news included titles framing
the AstraZeneca vaccine with strong levels of sadness, absent in mainstream
titles. Mainstream news initially framed "Pfizer" along more negative
associations with side effects than "AstraZeneca". With the temporary
suspension of the latter, on March 15th 2021, we identified a
semantic/emotional shift: Even mainstream article titles framed "AstraZeneca"
as semantically richer in negative associations with side effects, while
"Pfizer" underwent a positive shift in valence, mostly related to its higher
efficacy. "Thrombosis" entered the frame of vaccines together with fearful
conceptual associations, while "death" underwent an emotional shift, steering
towards fear in alternative titles and losing its hopeful connotation in
mainstream titles. Our findings expose crucial aspects of the emotional
narratives around COVID-19 vaccines adopted by the press, highlighting the need
to understand how alternative and mainstream media report vaccination news.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Supervised Clustering with Contrastive Learning for Discovering New Intents. (arXiv:2201.07604v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07604">
<div class="article-summary-box-inner">
<span><p>Most dialogue systems in real world rely on predefined intents and answers
for QA service, so discovering potential intents from large corpus previously
is really important for building such dialogue services. Considering that most
scenarios have few intents known already and most intents waiting to be
discovered, we focus on semi-supervised text clustering and try to make the
proposed method benefit from labeled samples for better overall clustering
performance. In this paper, we propose Deep Contrastive Semi-supervised
Clustering (DCSC), which aims to cluster text samples in a semi-supervised way
and provide grouped intents to operation staff. To make DCSC fully utilize the
limited known intents, we propose a two-stage training procedure for DCSC, in
which DCSC will be trained on both labeled samples and unlabeled samples, and
achieve better text representation and clustering performance. We conduct
experiments on two public datasets to compare our model with several popular
methods, and the results show DCSC achieve best performance across all datasets
and circumstances, indicating the effect of the improvements in our work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncovering More Shallow Heuristics: Probing the Natural Language Inference Capacities of Transformer-Based Pre-Trained Language Models Using Syllogistic Patterns. (arXiv:2201.07614v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07614">
<div class="article-summary-box-inner">
<span><p>In this article, we explore the shallow heuristics used by transformer-based
pre-trained language models (PLMs) that are fine-tuned for natural language
inference (NLI). To do so, we construct or own dataset based on syllogistic,
and we evaluate a number of models' performance on our dataset. We find
evidence that the models rely heavily on certain shallow heuristics, picking up
on symmetries and asymmetries between premise and hypothesis. We suggest that
the lack of generalization observable in our study, which is becoming a topic
of lively debate in the field, means that the PLMs are currently not learning
NLI, but rather spurious heuristics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Top-Down Influence? Predicting CEO Personality and Risk Impact from Speech Transcripts. (arXiv:2201.07670v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07670">
<div class="article-summary-box-inner">
<span><p>How much does a CEO's personality impact the performance of their company?
Management theory posits a great influence, but it is difficult to show
empirically -- there is a lack of publicly available self-reported personality
data of top managers. Instead, we propose a text-based personality regressor
using crowd-sourced Myers--Briggs Type Indicator (MBTI) assessments. The
ratings have a high internal and external validity and can be predicted with
moderate to strong correlations for three out of four dimensions. Providing
evidence for the upper echelons theory, we demonstrate that the predicted CEO
personalities have explanatory power of financial risk.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data-to-Value: An Evaluation-First Methodology for Natural Language Projects. (arXiv:2201.07725v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07725">
<div class="article-summary-box-inner">
<span><p>Big data, i.e. collecting, storing and processing of data at scale, has
recently been possible due to the arrival of clusters of commodity computers
powered by application-level distributed parallel operating systems like
HDFS/Hadoop/Spark, and such infrastructures have revolutionized data mining at
scale. For data mining project to succeed more consistently, some methodologies
were developed (e.g. CRISP-DM, SEMMA, KDD), but these do not account for (1)
very large scales of processing, (2) dealing with textual (unstructured) data
(i.e. Natural Language Processing (NLP, "text analytics"), and (3)
non-technical considerations (e.g. legal, ethical, project managerial aspects).
</p>
<p>To address these shortcomings, a new methodology, called "Data to Value"
(D2V), is introduced, which is guided by a detailed catalog of questions in
order to avoid a disconnect of big data text analytics project team with the
topic when facing rather abstract box-and-arrow diagrams commonly associated
with methodologies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Biomedical Information Retrieval with Neural Retrievers. (arXiv:2201.07745v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07745">
<div class="article-summary-box-inner">
<span><p>Information retrieval (IR) is essential in search engines and dialogue
systems as well as natural language processing tasks such as open-domain
question answering. IR serve an important function in the biomedical domain,
where content and sources of scientific knowledge may evolve rapidly. Although
neural retrievers have surpassed traditional IR approaches such as TF-IDF and
BM25 in standard open-domain question answering tasks, they are still found
lacking in the biomedical domain. In this paper, we seek to improve information
retrieval (IR) using neural retrievers (NR) in the biomedical domain, and
achieve this goal using a three-pronged approach. First, to tackle the relative
lack of data in the biomedical domain, we propose a template-based question
generation method that can be leveraged to train neural retriever models.
Second, we develop two novel pre-training tasks that are closely aligned to the
downstream task of information retrieval. Third, we introduce the ``Poly-DPR''
model which encodes each context into multiple context vectors. Extensive
experiments and analysis on the BioASQ challenge suggest that our proposed
method leads to large gains over existing neural approaches and beats BM25 in
the small-corpus setting. We show that BM25 and our method can complement each
other, and a simple hybrid model leads to further gains in the large corpus
setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attacking Neural Text Detectors. (arXiv:2002.11768v4 [cs.CR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.11768">
<div class="article-summary-box-inner">
<span><p>Machine learning based language models have recently made significant
progress, which introduces a danger to spread misinformation. To combat this
potential danger, several methods have been proposed for detecting text written
by these language models. This paper presents two classes of black-box attacks
on these detectors, one which randomly replaces characters with homoglyphs, and
the other a simple scheme to purposefully misspell words. The homoglyph and
misspelling attacks decrease a popular neural text detector's recall on neural
text from 97.44% to 0.26% and 22.68%, respectively. Results also indicate that
the attacks are transferable to other neural text detectors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PADA: Example-based Prompt Learning for on-the-fly Adaptation to Unseen Domains. (arXiv:2102.12206v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.12206">
<div class="article-summary-box-inner">
<span><p>Natural Language Processing algorithms have made incredible progress, but
they still struggle when applied to out-of-distribution examples. We address a
challenging and underexplored version of this domain adaptation problem, where
an algorithm is trained on several source domains, and then applied to examples
from unseen domains that are unknown at training time. Particularly, no
examples, labeled or unlabeled, or any other knowledge about the target domain
are available to the algorithm at training time. We present PADA: An
example-based autoregressive Prompt learning algorithm for on-the-fly
Any-Domain Adaptation, based on the T5 language model. Given a test example,
PADA first generates a unique prompt for it and then, conditioned on this
prompt, labels the example with respect to the NLP prediction task. PADA is
trained to generate a prompt which is a token sequence of unrestricted length,
consisting of Domain Related Features (DRFs) that characterize each of the
source domains. Intuitively, the generated prompt is a unique signature that
maps the test example to a semantic space spanned by the source domains. In
experiments with 3 tasks (text classification and sequence tagging), for a
total of 14 multi-source adaptation scenarios, PADA substantially outperforms
strong baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Search Engine for Discovery of Scientific Challenges and Directions. (arXiv:2108.13751v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13751">
<div class="article-summary-box-inner">
<span><p>Keeping track of scientific challenges, advances and emerging directions is a
fundamental part of research. However, researchers face a flood of papers that
hinders discovery of important knowledge. In biomedicine, this directly impacts
human lives. To address this problem, we present a novel task of extraction and
search of scientific challenges and directions, to facilitate rapid knowledge
discovery. We construct and release an expert-annotated corpus of texts sampled
from full-length papers, labeled with novel semantic categories that generalize
across many types of challenges and directions. We focus on a large corpus of
interdisciplinary work relating to the COVID-19 pandemic, ranging from
biomedicine to areas such as AI and economics. We apply a model trained on our
data to identify challenges and directions across the corpus and build a
dedicated search engine. In experiments with 19 researchers and clinicians
using our system, we outperform a popular scientific search engine in assisting
knowledge discovery. Finally, we show that models trained on our resource
generalize to the wider biomedical domain and to AI papers, highlighting its
broad utility. We make our data, model and search engine publicly available.
https://challenges.apps.allenai.org/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Multilingual Translation by Representation and Gradient Regularization. (arXiv:2109.04778v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04778">
<div class="article-summary-box-inner">
<span><p>Multilingual Neural Machine Translation (NMT) enables one model to serve all
translation directions, including ones that are unseen during training, i.e.
zero-shot translation. Despite being theoretically attractive, current models
often produce low quality translations -- commonly failing to even produce
outputs in the right target language. In this work, we observe that off-target
translation is dominant even in strong multilingual systems, trained on massive
multilingual corpora. To address this issue, we propose a joint approach to
regularize NMT models at both representation-level and gradient-level. At the
representation level, we leverage an auxiliary target language prediction task
to regularize decoder outputs to retain information about the target language.
At the gradient level, we leverage a small amount of direct data (in thousands
of sentence pairs) to regularize model gradients. Our results demonstrate that
our approach is highly effective in both reducing off-target translation
occurrences and improving zero-shot translation performance by +5.59 and +10.38
BLEU on WMT and OPUS datasets respectively. Moreover, experiments show that our
method also works well when the small amount of direct data is not available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RAFT: A Real-World Few-Shot Text Classification Benchmark. (arXiv:2109.14076v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14076">
<div class="article-summary-box-inner">
<span><p>Large pre-trained language models have shown promise for few-shot learning,
completing text-based tasks given only a few task-specific examples. Will
models soon solve classification tasks that have so far been reserved for human
research assistants? Existing benchmarks are not designed to measure progress
in applied settings, and so don't directly answer this question. The RAFT
benchmark (Real-world Annotated Few-shot Tasks) focuses on naturally occurring
tasks and uses an evaluation setup that mirrors deployment. Baseline
evaluations on RAFT reveal areas current techniques struggle with: reasoning
over long texts and tasks with many classes. Human baselines show that some
classification tasks are difficult for non-expert humans, reflecting that
real-world value sometimes depends on domain expertise. Yet even non-expert
human baseline F1 scores exceed GPT-3 by an average of 0.11. The RAFT datasets
and leaderboard will track which model improvements translate into real-world
benefits at https://raft.elicit.org .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SNCSE: Contrastive Learning for Unsupervised Sentence Embedding with Soft Negative Samples. (arXiv:2201.05979v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05979">
<div class="article-summary-box-inner">
<span><p>Unsupervised sentence embedding aims to obtain the most appropriate embedding
for a sentence to reflect its semantic. Contrastive learning has been
attracting developing attention. For a sentence, current models utilize diverse
data augmentation methods to generate positive samples, while consider other
independent sentences as negative samples. Then they adopt InfoNCE loss to pull
the embeddings of positive pairs gathered, and push those of negative pairs
scattered. Although these models have made great progress on sentence
embedding, we argue that they may suffer from feature suppression. The models
fail to distinguish and decouple textual similarity and semantic similarity.
And they may overestimate the semantic similarity of any pairs with similar
textual regardless of the actual semantic difference between them. This is
because positive pairs in unsupervised contrastive learning come with similar
and even the same textual through data augmentation. To alleviate feature
suppression, we propose contrastive learning for unsupervised sentence
embedding with soft negative samples (SNCSE). Soft negative samples share
highly similar textual but have surely and apparently different semantic with
the original samples. Specifically, we take the negation of original sentences
as soft negative samples, and propose Bidirectional Margin Loss (BML) to
introduce them into traditional contrastive learning framework, which merely
involves positive and negative samples. Our experimental results show that
SNCSE can obtain state-of-the-art performance on semantic textual similarity
(STS) task with average Spearman's correlation coefficient of 78.97% on
BERTbase and 79.23% on RoBERTabase. Besides, we adopt rank-based error analysis
method to detect the weakness of SNCSE for future study.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unintended Bias in Language Model-driven Conversational Recommendation. (arXiv:2201.06224v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06224">
<div class="article-summary-box-inner">
<span><p>Conversational Recommendation Systems (CRSs) have recently started to
leverage pretrained language models (LM) such as BERT for their ability to
semantically interpret a wide range of preference statement variations.
However, pretrained LMs are well-known to be prone to intrinsic biases in their
training data, which may be exacerbated by biases embedded in domain-specific
language data(e.g., user reviews) used to fine-tune LMs for CRSs. We study a
recently introduced LM-driven recommendation backbone (termed LMRec) of a CRS
to investigate how unintended bias i.e., language variations such as name
references or indirect indicators of sexual orientation or location that should
not affect recommendations manifests in significantly shifted price and
category distributions of restaurant recommendations. The alarming results we
observe strongly indicate that LMRec has learned to reinforce harmful
stereotypes through its recommendations. For example, offhand mention of names
associated with the black community significantly lowers the price distribution
of recommended restaurants, while offhand mentions of common male-associated
names lead to an increase in recommended alcohol-serving establishments. These
and many related results presented in this work raise a red flag that advances
in the language handling capability of LM-drivenCRSs do not come without
significant challenges related to mitigating unintended bias in future deployed
CRS assistants with a potential reach of hundreds of millions of end-users.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COPA-SSE: Semi-structured Explanations for Commonsense Reasoning. (arXiv:2201.06777v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06777">
<div class="article-summary-box-inner">
<span><p>We present Semi-Structured Explanations for COPA (COPA-SSE), a new
crowdsourced dataset of 9,747 semi-structured, English common sense
explanations for COPA questions. The explanations are formatted as a set of
triple-like common sense statements with ConceptNet relations but freely
written concepts. This semi-structured format strikes a balance between the
high quality but low coverage of structured data and the lower quality but high
coverage of free-form crowdsourcing. Each explanation also includes a set of
human-given quality ratings. With their familiar format, the explanations are
geared towards commonsense reasoners operating on knowledge graphs and serve as
a starting point for ongoing work on improving such systems.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Is Contrastive Learning Suitable for Left Ventricular Segmentation in Echocardiographic Images?. (arXiv:2201.07219v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07219">
<div class="article-summary-box-inner">
<span><p>Contrastive learning has proven useful in many applications where access to
labelled data is limited. The lack of annotated data is particularly
problematic in medical image segmentation as it is difficult to have clinical
experts manually annotate large volumes of data. One such task is the
segmentation of cardiac structures in ultrasound images of the heart. In this
paper, we argue whether or not contrastive pretraining is helpful for the
segmentation of the left ventricle in echocardiography images. Furthermore, we
study the effect of this on two segmentation networks, DeepLabV3, as well as
the commonly used segmentation network, UNet. Our results show that contrastive
pretraining helps improve the performance on left ventricle segmentation,
particularly when annotated data is scarce. We show how to achieve comparable
results to state-of-the-art fully supervised algorithms when we train our
models in a self-supervised fashion followed by fine-tuning on just 5% of the
data. We also show that our solution achieves better results than what is
currently published on a large public dataset (EchoNet-Dynamic) and we compare
the performance of our solution on another smaller dataset (CAMUS) as well.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explainable Ensemble Machine Learning for Breast Cancer Diagnosis based on Ultrasound Image Texture Features. (arXiv:2201.07227v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07227">
<div class="article-summary-box-inner">
<span><p>Image classification is widely used to build predictive models for breast
cancer diagnosis. Most existing approaches overwhelmingly rely on deep
convolutional networks to build such diagnosis pipelines. These model
architectures, although remarkable in performance, are black-box systems that
provide minimal insight into the inner logic behind their predictions. This is
a major drawback as the explainability of prediction is vital for applications
such as cancer diagnosis. In this paper, we address this issue by proposing an
explainable machine learning pipeline for breast cancer diagnosis based on
ultrasound images. We extract first- and second-order texture features of the
ultrasound images and use them to build a probabilistic ensemble of decision
tree classifiers. Each decision tree learns to classify the input ultrasound
image by learning a set of robust decision thresholds for texture features of
the image. The decision path of the model predictions can then be interpreted
by decomposing the learned decision trees. Our results show that our proposed
framework achieves high predictive performance while being explainable.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AI-based Carcinoma Detection and Classification Using Histopathological Images: A Systematic Review. (arXiv:2201.07231v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07231">
<div class="article-summary-box-inner">
<span><p>Histopathological image analysis is the gold standard to diagnose cancer.
Carcinoma is a subtype of cancer that constitutes more than 80% of all cancer
cases. Squamous cell carcinoma and adenocarcinoma are two major subtypes of
carcinoma, diagnosed by microscopic study of biopsy slides. However, manual
microscopic evaluation is a subjective and time-consuming process. Many
researchers have reported methods to automate carcinoma detection and
classification. The increasing use of artificial intelligence (AI) in the
automation of carcinoma diagnosis also reveals a significant rise in the use of
deep network models. In this systematic literature review, we present a
comprehensive review of the state-of-the-art approaches reported in carcinoma
diagnosis using histopathological images. Studies are selected from well-known
databases with strict inclusion/exclusion criteria. We have categorized the
articles and recapitulated their methods based on specific organs of carcinoma
origin. Further, we have summarized pertinent literature on AI methods,
highlighted critical challenges and limitations, and provided insights on
future research direction in automated carcinoma diagnosis. Out of 101 articles
selected, most of the studies experimented on private datasets with varied
image sizes, obtaining accuracy between 63% and 100%. Overall, this review
highlights the need for a generalized AI-based carcinoma diagnostic system.
Additionally, it is desirable to have accountable approaches to extract
microscopic features from images of multiple magnifications that should mimic
pathologists' evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Kervolutional Neural Networks. (arXiv:2201.07264v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07264">
<div class="article-summary-box-inner">
<span><p>A paper published in the CVPR 2019 conference outlines a new technique called
'kervolution' used in a new type of augmented convolutional neural network
(CNN) called a 'kervolutional neural network' (KNN). The paper asserts that
KNNs achieve faster convergence and higher accuracies than CNNs. This "mini
paper" will further examine the findings in the original paper and perform a
more in depth analysis of the KNN architecture. This will be done by analyzing
the impact of hyper parameters (specifically the learning rate) on KNNs versus
CNNs, experimenting with other types of kervolution operations not tested in
the original paper, a more rigourous statistical analysis of accuracies and
convergence times and additional theoretical analysis. The accompanying code is
publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OSSID: Online Self-Supervised Instance Detection by (and for) Pose Estimation. (arXiv:2201.07309v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07309">
<div class="article-summary-box-inner">
<span><p>Real-time object pose estimation is necessary for many robot manipulation
algorithms. However, state-of-the-art methods for object pose estimation are
trained for a specific set of objects; these methods thus need to be retrained
to estimate the pose of each new object, often requiring tens of GPU-days of
training for optimal performance. \revisef{In this paper, we propose the OSSID
framework,} leveraging a slow zero-shot pose estimator to self-supervise the
training of a fast detection algorithm. This fast detector can then be used to
filter the input to the pose estimator, drastically improving its inference
speed. We show that this self-supervised training exceeds the performance of
existing zero-shot detection methods on two widely used object pose estimation
and detection datasets, without requiring any human annotations. Further, we
show that the resulting method for pose estimation has a significantly faster
inference speed, due to the ability to filter out large parts of the image.
Thus, our method for self-supervised online learning of a detector (trained
using pseudo-labels from a slow pose estimator) leads to accurate pose
estimation at real-time speeds, without requiring human annotations.
Supplementary materials and code can be found at
https://georgegu1997.github.io/OSSID/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lung Swapping Autoencoder: Learning a Disentangled Structure-texture Representation of Chest Radiographs. (arXiv:2201.07344v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07344">
<div class="article-summary-box-inner">
<span><p>Well-labeled datasets of chest radiographs (CXRs) are difficult to acquire
due to the high cost of annotation. Thus, it is desirable to learn a robust and
transferable representation in an unsupervised manner to benefit tasks that
lack labeled data. Unlike natural images, medical images have their own domain
prior; e.g., we observe that many pulmonary diseases, such as the COVID-19,
manifest as changes in the lung tissue texture rather than the anatomical
structure. Therefore, we hypothesize that studying only the texture without the
influence of structure variations would be advantageous for downstream
prognostic and predictive modeling tasks. In this paper, we propose a
generative framework, the Lung Swapping Autoencoder (LSAE), that learns
factorized representations of a CXR to disentangle the texture factor from the
structure factor. Specifically, by adversarial training, the LSAE is optimized
to generate a hybrid image that preserves the lung shape in one image but
inherits the lung texture of another. To demonstrate the effectiveness of the
disentangled texture representation, we evaluate the texture encoder $Enc^t$ in
LSAE on ChestX-ray14 (N=112,120), and our own multi-institutional COVID-19
outcome prediction dataset, COVOC (N=340 (Subset-1) + 53 (Subset-2)). On both
datasets, we reach or surpass the state-of-the-art by finetuning $Enc^t$ in
LSAE that is 77% smaller than a baseline Inception v3. Additionally, in
semi-and-self supervised settings with a similar model budget, $Enc^t$ in LSAE
is also competitive with the state-of-the-art MoCo. By "re-mixing" the texture
and shape factors, we generate meaningful hybrid images that can augment the
training set. This data augmentation method can further improve COVOC
prediction performance. The improvement is consistent even when we directly
evaluate the Subset-1 trained model on Subset-2 without any fine-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Supervised Contrastive Learning for Better Severity Scoring of Lung Ultrasound. (arXiv:2201.07357v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07357">
<div class="article-summary-box-inner">
<span><p>With the onset of the COVID-19 pandemic, ultrasound has emerged as an
effective tool for bedside monitoring of patients. Due to this, a large amount
of lung ultrasound scans have been made available which can be used for AI
based diagnosis and analysis. Several AI-based patient severity scoring models
have been proposed that rely on scoring the appearance of the ultrasound scans.
AI models are trained using ultrasound-appearance severity scores that are
manually labeled based on standardized visual features. We address the
challenge of labeling every ultrasound frame in the video clips. Our
contrastive learning method treats the video clip severity labels as noisy weak
severity labels for individual frames, thus requiring only video-level labels.
We show that it performs better than the conventional cross-entropy loss based
training. We combine frame severity predictions to come up with video severity
predictions and show that the frame based model achieves comparable performance
to a video based TSM model, on a large dataset combining public and private
sources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TriCoLo: Trimodal Contrastive Loss for Fine-grained Text to Shape Retrieval. (arXiv:2201.07366v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07366">
<div class="article-summary-box-inner">
<span><p>Recent work on contrastive losses for learning joint embeddings over
multimodal data has been successful at downstream tasks such as retrieval and
classification. On the other hand, work on joint representation learning for 3D
shapes and text has thus far mostly focused on improving embeddings through
modeling of complex attention between representations , or multi-task learning
. We show that with large batch contrastive learning we achieve SoTA on
text-shape retrieval without complex attention mechanisms or losses. Prior work
in 3D and text representations has also focused on bimodal representation
learning using either voxels or multi-view images with text. To this end, we
propose a trimodal learning scheme to achieve even higher performance and
better representations for all modalities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Role of Pleura and Adipose in Lung Ultrasound AI. (arXiv:2201.07368v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07368">
<div class="article-summary-box-inner">
<span><p>In this paper, we study the significance of the pleura and adipose tissue in
lung ultrasound AI analysis. We highlight their more prominent appearance when
using high-frequency linear (HFL) instead of curvilinear ultrasound probes,
showing HFL reveals better pleura detail. We compare the diagnostic utility of
the pleura and adipose tissue using an HFL ultrasound probe. Masking the
adipose tissue during training and inference (while retaining the pleural line
and Merlin's space artifacts such as A-lines and B-lines) improved the AI
model's diagnostic accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Online Deep Learning based on Auto-Encoder. (arXiv:2201.07383v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07383">
<div class="article-summary-box-inner">
<span><p>Online learning is an important technical means for sketching massive
real-time and high-speed data. Although this direction has attracted intensive
attention, most of the literature in this area ignore the following three
issues: (1) they think little of the underlying abstract hierarchical latent
information existing in examples, even if extracting these abstract
hierarchical latent representations is useful to better predict the class
labels of examples; (2) the idea of preassigned model on unseen datapoints is
not suitable for modeling streaming data with evolving probability
distribution. This challenge is referred as model flexibility. And so, with
this in minds, the online deep learning model we need to design should have a
variable underlying structure; (3) moreover, it is of utmost importance to
fusion these abstract hierarchical latent representations to achieve better
classification performance, and we should give different weights to different
levels of implicit representation information when dealing with the data
streaming where the data distribution changes. To address these issues, we
propose a two-phase Online Deep Learning based on Auto-Encoder (ODLAE). Based
on auto-encoder, considering reconstruction loss, we extract abstract
hierarchical latent representations of instances; Based on predictive loss, we
devise two fusion strategies: the output-level fusion strategy, which is
obtained by fusing the classification results of encoder each hidden layer; and
feature-level fusion strategy, which is leveraged self-attention mechanism to
fusion every hidden layer output. Finally, in order to improve the robustness
of the algorithm, we also try to utilize the denoising auto-encoder to yield
hierarchical latent representations. Experimental results on different datasets
are presented to verify the validity of our proposed algorithm (ODLAE)
outperforms several baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Swin-Pose: Swin Transformer Based Human Pose Estimation. (arXiv:2201.07384v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07384">
<div class="article-summary-box-inner">
<span><p>Convolutional neural networks (CNNs) have been widely utilized in many
computer vision tasks. However, CNNs have a fixed reception field and lack the
ability of long-range perception, which is crucial to human pose estimation.
Due to its capability to capture long-range dependencies between pixels,
transformer architecture has been adopted to computer vision applications
recently and is proven to be a highly effective architecture. We are interested
in exploring its capability in human pose estimation, and thus propose a novel
model based on transformer architecture, enhanced with a feature pyramid fusion
structure. More specifically, we use pre-trained Swin Transformer as our
backbone and extract features from input images, we leverage a feature pyramid
structure to extract feature maps from different stages. By fusing the features
together, our model predicts the keypoint heatmap. The experiment results of
our study have demonstrated that the proposed transformer-based model can
achieve better performance compared to the state-of-the-art CNN-based models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KappaFace: Adaptive Additive Angular Margin Loss for Deep Face Recognition. (arXiv:2201.07394v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07394">
<div class="article-summary-box-inner">
<span><p>Feature learning is a widely used method employed for large-scale face
recognition. Recently, large-margin softmax loss methods have demonstrated
significant enhancements on deep face recognition. These methods propose fixed
positive margins in order to enforce intra-class compactness and inter-class
diversity. However, the majority of the proposed methods do not consider the
class imbalance issue, which is a major challenge in practice for developing
deep face recognition models. We hypothesize that it significantly affects the
generalization ability of the deep face models. Inspired by this observation,
we introduce a novel adaptive strategy, called KappaFace, to modulate the
relative importance based on class difficultness and imbalance. With the
support of the von Mises-Fisher distribution, our proposed KappaFace loss can
intensify the margin's magnitude for hard learning or low concentration classes
while relaxing it for counter classes. Experiments conducted on popular facial
benchmarks demonstrate that our proposed method achieves superior performance
to the state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Poseur: Direct Human Pose Regression with Transformers. (arXiv:2201.07412v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07412">
<div class="article-summary-box-inner">
<span><p>We propose a direct, regression-based approach to 2D human pose estimation
from single images. We formulate the problem as a sequence prediction task,
which we solve using a Transformer network. This network directly learns a
regression mapping from images to the keypoint coordinates, without resorting
to intermediate representations such as heatmaps. This approach avoids much of
the complexity associated with heatmap-based approaches. To overcome the
feature misalignment issues of previous regression-based methods, we propose an
attention mechanism that adaptively attends to the features that are most
relevant to the target keypoints, considerably improving the accuracy.
Importantly, our framework is end-to-end differentiable, and naturally learns
to exploit the dependencies between keypoints. Experiments on MS-COCO and MPII,
two predominant pose-estimation datasets, demonstrate that our method
significantly improves upon the state-of-the-art in regression-based pose
estimation. More notably, ours is the first regression-based approach to
perform favorably compared to the best heatmap-based pose estimation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Deep Blind Video Super-Resolution. (arXiv:2201.07422v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07422">
<div class="article-summary-box-inner">
<span><p>Existing deep learning-based video super-resolution (SR) methods usually
depend on the supervised learning approach, where the training data is usually
generated by the blurring operation with known or predefined kernels (e.g.,
Bicubic kernel) followed by a decimation operation. However, this does not hold
for real applications as the degradation process is complex and cannot be
approximated by these idea cases well. Moreover, obtaining high-resolution (HR)
videos and the corresponding low-resolution (LR) ones in real-world scenarios
is difficult. To overcome these problems, we propose a self-supervised learning
method to solve the blind video SR problem, which simultaneously estimates blur
kernels and HR videos from the LR videos. As directly using LR videos as
supervision usually leads to trivial solutions, we develop a simple and
effective method to generate auxiliary paired data from original LR videos
according to the image formation of video SR, so that the networks can be
better constrained by the generated paired data for both blur kernel estimation
and latent HR video restoration. In addition, we introduce an optical flow
estimation module to exploit the information from adjacent frames for HR video
restoration. Experiments show that our method performs favorably against
state-of-the-art ones on benchmarks and real-world videos.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WebUAV-3M: A Benchmark Unveiling the Power of Million-Scale Deep UAV Tracking. (arXiv:2201.07425v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07425">
<div class="article-summary-box-inner">
<span><p>In this work, we contribute a new million-scale Unmanned Aerial Vehicle (UAV)
tracking benchmark, called WebUAV-3M. Firstly, we collect 4,485 videos with
more than 3M frames from the Internet. Then, an efficient and scalable
Semi-Automatic Target Annotation (SATA) pipeline is devised to label the
tremendous WebUAV-3M in every frame. To the best of our knowledge, the densely
bounding box annotated WebUAV-3M is by far the largest public UAV tracking
benchmark. We expect to pave the way for the follow-up study in the UAV
tracking by establishing a million-scale annotated benchmark covering a wide
range of target categories. Moreover, considering the close connections among
visual appearance, natural language and audio, we enrich WebUAV-3M by providing
natural language specification and audio description, encouraging the
exploration of natural language features and audio cues for UAV tracking.
Equipped with this benchmark, we delve into million-scale deep UAV tracking
problems, aiming to provide the community with a dedicated large-scale
benchmark for training deep UAV trackers and evaluating UAV tracking
approaches. Extensive experiments on WebUAV-3M demonstrate that there is still
a big room for robust deep UAV tracking improvements. The dataset, toolkits and
baseline results will be available at
\url{https://github.<a href="/abs/com/9836328">com/9836328</a>47/WebUAV-3M}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Variable Augmented Network for Invertible MR Coil Compression. (arXiv:2201.07428v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07428">
<div class="article-summary-box-inner">
<span><p>A large number of coils are able to provide enhanced signal-to-noise ratio
and improve imaging performance in parallel imaging. As the increasingly grow
of coil number, however, it simultaneously aggravates the drawbacks of data
storage and reconstruction speed, especially in some iterative reconstructions.
Coil compression addresses these issues by generating fewer virtual coils. In
this work, a novel variable augmented network for invertible coil compression
(VAN-ICC) is presented, which utilizes inherent reversibility of
normalizing-flow-based models, for better compression and invertible recovery.
VAN-ICC trains invertible network by finding an invertible and bijective
function, which can map the original image to the compression image. In the
experiments, both fully-sampled images and under-sampled images were used to
verify the effectiveness of the model. Extensive quantitative and qualitative
evaluations demonstrated that, in comparison with SCC and GCC, VAN-ICC can
carry through better compression effect with equal number of virtual coils.
Additionally, its performance is not susceptible to different num-ber of
virtual coils.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth. (arXiv:2201.07436v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07436">
<div class="article-summary-box-inner">
<span><p>Depth estimation from a single image is an important task that can be applied
to various fields in computer vision, and has grown rapidly with the
development of convolutional neural networks. In this paper, we propose a novel
structure and training strategy for monocular depth estimation to further
improve the prediction accuracy of the network. We deploy a hierarchical
transformer encoder to capture and convey the global context, and design a
lightweight yet powerful decoder to generate an estimated depth map while
considering local connectivity. By constructing connected paths between
multi-scale local features and the global decoding stream with our proposed
selective feature fusion module, the network can integrate both representations
and recover fine details. In addition, the proposed decoder shows better
performance than the previously proposed decoders, with considerably less
computational complexity. Furthermore, we improve the depth-specific
augmentation method by utilizing an important observation in depth estimation
to enhance the model. Our network achieves state-of-the-art performance over
the challenging depth dataset NYU Depth V2. Extensive experiments have been
conducted to validate and show the effectiveness of the proposed approach.
Finally, our model shows better generalisation ability and robustness than
other comparative models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransFuse: A Unified Transformer-based Image Fusion Framework using Self-supervised Learning. (arXiv:2201.07451v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07451">
<div class="article-summary-box-inner">
<span><p>Image fusion is a technique to integrate information from multiple source
images with complementary information to improve the richness of a single
image. Due to insufficient task-specific training data and corresponding ground
truth, most existing end-to-end image fusion methods easily fall into
overfitting or tedious parameter optimization processes. Two-stage methods
avoid the need of large amount of task-specific training data by training
encoder-decoder network on large natural image datasets and utilizing the
extracted features for fusion, but the domain gap between natural images and
different fusion tasks results in limited performance. In this study, we design
a novel encoder-decoder based image fusion framework and propose a
destruction-reconstruction based self-supervised training scheme to encourage
the network to learn task-specific features. Specifically, we propose three
destruction-reconstruction self-supervised auxiliary tasks for multi-modal
image fusion, multi-exposure image fusion and multi-focus image fusion based on
pixel intensity non-linear transformation, brightness transformation and noise
transformation, respectively. In order to encourage different fusion tasks to
promote each other and increase the generalizability of the trained network, we
integrate the three self-supervised auxiliary tasks by randomly choosing one of
them to destroy a natural image in model training. In addition, we design a new
encoder that combines CNN and Transformer for feature extraction, so that the
trained model can exploit both local and global information. Extensive
experiments on multi-modal image fusion, multi-exposure image fusion and
multi-focus image fusion tasks demonstrate that our proposed method achieves
the state-of-the-art performance in both subjective and objective evaluations.
The code will be publicly available soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Self-Supervised Pretext Tasks for Active Learning. (arXiv:2201.07459v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07459">
<div class="article-summary-box-inner">
<span><p>Labeling a large set of data is expensive. Active learning aims to tackle
this problem by asking to annotate only the most informative data from the
unlabeled set. We propose a novel active learning approach that utilizes
self-supervised pretext tasks and a unique data sampler to select data that are
both difficult and representative. We discover that the loss of a simple
self-supervised pretext task, such as rotation prediction, is closely
correlated to the downstream task loss. The pretext task learner is trained on
the unlabeled set, and the unlabeled data are sorted and grouped into batches
by their pretext task losses. In each iteration, the main task model is used to
sample the most uncertain data in a batch to be annotated. We evaluate our
method on various image classification and segmentation benchmarks and achieve
compelling performances on CIFAR10, Caltech-101, ImageNet, and CityScapes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">High-fidelity 3D Model Compression based on Key Spheres. (arXiv:2201.07486v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07486">
<div class="article-summary-box-inner">
<span><p>In recent years, neural signed distance function (SDF) has become one of the
most effectiverepresentation methods for 3D models. By learning continuous SDFs
in 3D space, neuralnetworks can predict the distance from a given query space
point to its closest object surface,whose positive and negative signs denote
inside and outside of the object, respectively.Training a specific network for
each 3D model, which individually embeds its shape, canrealize compressed
representation of objects by storing fewer network (and possibly
latent)parameters. Consequently, reconstruction through network inference and
surface recoverycan be achieved. In this paper, we propose an SDF prediction
network using explicit keyspheres as input. Key spheres are extracted from the
internal space of objects, whosecenters either have relatively larger SDF
values (sphere radii), or are located at essentialpositions. By inputting the
spatial information of multiple spheres which imply differentlocal shapes, the
proposed method can significantly improve the reconstruction accuracywith a
negligible storage cost. Compared to previous works, our method achieves the
high-fidelity and high-compression 3D object coding and reconstruction.
Experiments conductedon three datasets verify the superior performance of our
method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Supervised Semantic Segmentation of Remote Sensing Images for Tree Species Classification Based on Explanation Methods. (arXiv:2201.07495v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07495">
<div class="article-summary-box-inner">
<span><p>The collection of a high number of pixel-based labeled training samples for
tree species identification is time consuming and costly in operational
forestry applications. To address this problem, in this paper we investigate
the effectiveness of explanation methods for deep neural networks in performing
weakly supervised semantic segmentation using only image-level labels.
Specifically, we consider four methods:i) class activation maps (CAM); ii)
gradient-based CAM; iii) pixel correlation module; and iv) self-enhancing maps
(SEM). We compare these methods with each other using both quantitative and
qualitative measures of their segmentation accuracy, as well as their
computational requirements. Experimental results obtained on an aerial image
archive show that:i) considered explanation techniques are highly relevant for
the identification of tree species with weak supervision; and ii) the SEM
outperforms the other considered methods. The code for this paper is publicly
available at https://git.tu-berlin.de/rsim/rs_wsss.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Virtual Coil Augmentation Technology for MRI via Deep Learning. (arXiv:2201.07540v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07540">
<div class="article-summary-box-inner">
<span><p>Magnetic resonance imaging (MRI) is a widely used medical imaging modality.
However, due to the limitations in hardware, scan time, and throughput, it is
often clinically challenging to obtain high-quality MR images. In this article,
we propose a method of using artificial intelligence to expand the channel to
achieve the effect of increasing the virtual coil. The main feature of our work
is utilizing dummy variable technology to expand the channel in both the image
and k-space domains. The high-dimensional information formed by channel
expansion is used as the prior information of parallel imaging to improve the
reconstruction effect of parallel imaging. Two features are introduced, namely
variable enhancement and sum of squares (SOS) objective function. Variable
argumentation provides the network with more high-dimensional prior
information, which is helpful for the network to extract the deep feature
in-formation of the image. The SOS objective function is employed to solve the
problem that k-space data is difficult to train while speeding up the
convergence speed. Ablation studies and experimental results demonstrate that
our method achieves significantly higher image reconstruction performance than
current state-of-the-art techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simpler is better: spectral regularization and up-sampling techniques for variational autoencoders. (arXiv:2201.07544v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07544">
<div class="article-summary-box-inner">
<span><p>Full characterization of the spectral behavior of generative models based on
neural networks remains an open issue. Recent research has focused heavily on
generative adversarial networks and the high-frequency discrepancies between
real and generated images. The current solution to avoid this is to either
replace transposed convolutions with bilinear up-sampling or add a spectral
regularization term in the generator. It is well known that Variational
Autoencoders (VAEs) also suffer from these issues. In this work, we propose a
simple 2D Fourier transform-based spectral regularization loss for the VAE and
show that it can achieve results equal to, or better than, the current
state-of-the-art in frequency-aware losses for generative models. In addition,
we experiment with altering the up-sampling procedure in the generator network
and investigate how it influences the spectral performance of the model. We
include experiments on synthetic and real data sets to demonstrate our results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learned Cone-Beam CT Reconstruction Using Neural Ordinary Differential Equations. (arXiv:2201.07562v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07562">
<div class="article-summary-box-inner">
<span><p>Learned iterative reconstruction algorithms for inverse problems offer the
flexibility to combine analytical knowledge about the problem with modules
learned from data. This way, they achieve high reconstruction performance while
ensuring consistency with the measured data. In computed tomography, extending
such approaches from 2D fan-beam to 3D cone-beam data is challenging due to the
prohibitively high GPU memory that would be needed to train such models. This
paper proposes to use neural ordinary differential equations to solve the
reconstruction problem in a residual formulation via numerical integration. For
training, there is no need to backpropagate through several unrolled network
blocks nor through the internals of the solver. Instead, the gradients are
obtained very memory-efficiently in the neural ODE setting allowing for
training on a single consumer graphics card. The method is able to reduce the
root mean squared error by over 30% compared to the best performing classical
iterative reconstruction algorithm and produces high quality cone-beam
reconstructions even in a sparse view scenario.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Superpixel Pre-Segmentation of HER2 Slides for Efficient Annotation. (arXiv:2201.07572v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07572">
<div class="article-summary-box-inner">
<span><p>Supervised deep learning has shown state-of-the-art performance for medical
image segmentation across different applications, including histopathology and
cancer research; however, the manual annotation of such data is extremely
laborious. In this work, we explore the use of superpixel approaches to compute
a pre-segmentation of HER2 stained images for breast cancer diagnosis that
facilitates faster manual annotation and correction in a second step. Four
methods are compared: Standard Simple Linear Iterative Clustering (SLIC) as a
baseline, a domain adapted SLIC, and superpixels based on feature embeddings of
a pretrained ResNet-50 and a denoising autoencoder. To tackle oversegmentation,
we propose to hierarchically merge superpixels, based on their content in the
respective feature space. When evaluating the approaches on fully manually
annotated images, we observe that the autoencoder-based superpixels achieve a
23% increase in boundary F1 score compared to the baseline SLIC superpixels.
Furthermore, the boundary F1 score increases by 73% when hierarchical
clustering is applied on the adapted SLIC and the autoencoder-based
superpixels. These evaluations show encouraging first results for a
pre-segmentation for efficient manual refinement without the need for an
initial set of annotated training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DMF-Net: Dual-Branch Multi-Scale Feature Fusion Network for copy forgery identification of anti-counterfeiting QR code. (arXiv:2201.07583v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07583">
<div class="article-summary-box-inner">
<span><p>Anti-counterfeiting QR codes are widely used in people's work and life,
especially in product packaging. However, the anti-counterfeiting QR code has
the risk of being copied and forged in the circulation process. In reality,
copying is usually based on genuine anti-counterfeiting QR codes, but the
brands and models of copiers are diverse, and it is extremely difficult to
determine which individual copier the forged anti-counterfeiting code come
from. In response to the above problems, this paper proposes a method for copy
forgery identification of anti-counterfeiting QR code based on deep learning.
We first analyze the production principle of anti-counterfeiting QR code, and
convert the identification of copy forgery to device category forensics, and
then a Dual-Branch Multi-Scale Feature Fusion network is proposed. During the
design of the network, we conducted a detailed analysis of the data
preprocessing layer, single-branch design, etc., combined with experiments, the
specific structure of the dual-branch multi-scale feature fusion network is
determined. The experimental results show that the proposed method has achieved
a high accuracy of copy forgery identification, which exceeds the current
series of methods in the field of image forensics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Real-time Recognition of Yoga Poses using computer Vision for Smart Health Care. (arXiv:2201.07594v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07594">
<div class="article-summary-box-inner">
<span><p>Nowadays, yoga has become a part of life for many people. Exercises and
sports technological assistance is implemented in yoga pose identification. In
this work, a self-assistance based yoga posture identification technique is
developed, which helps users to perform Yoga with the correction feature in
Real-time. The work also presents Yoga-hand mudra (hand gestures)
identification. The YOGI dataset has been developed which include 10 Yoga
postures with around 400-900 images of each pose and also contain 5 mudras for
identification of mudras postures. It contains around 500 images of each mudra.
The feature has been extracted by making a skeleton on the body for yoga poses
and hand for mudra poses. Two different algorithms have been used for creating
a skeleton one for yoga poses and the second for hand mudras. Angles of the
joints have been extracted as a features for different machine learning and
deep learning models. among all the models XGBoost with RandomSearch CV is most
accurate and gives 99.2\% accuracy. The complete design framework is described
in the present paper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Confidence-based Iterative Solver of Depths and Surface Normals for Deep Multi-view Stereo. (arXiv:2201.07609v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07609">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce a deep multi-view stereo (MVS) system that
jointly predicts depths, surface normals and per-view confidence maps. The key
to our approach is a novel solver that iteratively solves for per-view depth
map and normal map by optimizing an energy potential based on the locally
planar assumption. Specifically, the algorithm updates depth map by propagating
from neighboring pixels with slanted planes, and updates normal map with local
probabilistic plane fitting. Both two steps are monitored by a customized
confidence map. This solver is not only effective as a post-processing tool for
plane-based depth refinement and completion, but also differentiable such that
it can be efficiently integrated into deep learning pipelines. Our multi-view
stereo system employs multiple optimization steps of the solver over the
initial prediction of depths and surface normals. The whole system can be
trained end-to-end, decoupling the challenging problem of matching pixels
within poorly textured regions from the cost-volume based neural network.
Experimental results on ScanNet and RGB-D Scenes V2 demonstrate
state-of-the-art performance of the proposed deep MVS system on multi-view
depth estimation, with our proposed solver consistently improving the depth
quality over both conventional and deep learning based MVS pipelines. Code is
available at https://github.com/thuzhaowang/idn-solver.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nonlinear Unknown Input Observability and Unknown Input Reconstruction: The General Analytical Solution. (arXiv:2201.07610v1 [math.OC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07610">
<div class="article-summary-box-inner">
<span><p>Observability is a fundamental structural property of any dynamic system and
describes the possibility of reconstructing the state that characterizes the
system from observing its inputs and outputs. Despite the huge effort made to
study this property and to introduce analytical criteria able to check whether
a dynamic system satisfies this property or not, there is no general analytical
criterion to automatically check the state observability when the dynamics are
also driven by unknown inputs. Here, we introduce the general analytical
solution of this fundamental problem, often called the unknown input
observability problem. This paper provides the general analytical solution of
this problem, namely, it provides the systematic procedure, based on automatic
computation (differentiation and matrix rank determination), that allows us to
automatically check the state observability even in the presence of unknown
inputs. A first solution of this problem was presented in the second part of
the book: "Observability: A New Theory Based on the Group of Invariance" [45].
The solution presented by this paper completes the previous solution in [45].
In particular, the new solution exhaustively accounts for the systems that do
not belong to the category of the systems that are canonic with respect to
their unknown inputs. The new solution is also provided in the form of a new
algorithm. A further novelty with respect to the algorithm provided in [45]
consists of a new convergence criterion that holds in all the cases (the
convergence criterion of the algorithm provided in [45] can fail in some
cases). Finally, we also provide the answer to the problem of unknown input
reconstruction which is intimately related to the problem of state
observability. We illustrate the implementation of the new algorithm by
studying a nonlinear system in the framework of visual-inertial sensor fusion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CAST: Character labeling in Animation using Self-supervision by Tracking. (arXiv:2201.07619v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07619">
<div class="article-summary-box-inner">
<span><p>Cartoons and animation domain videos have very different characteristics
compared to real-life images and videos. In addition, this domain carries a
large variability in styles. Current computer vision and deep-learning
solutions often fail on animated content because they were trained on natural
images. In this paper we present a method to refine a semantic representation
suitable for specific animated content. We first train a neural network on a
large-scale set of animation videos and use the mapping to deep features as an
embedding space. Next, we use self-supervision to refine the representation for
any specific animation style by gathering many examples of animated characters
in this style, using a multi-object tracking. These examples are used to define
triplets for contrastive loss training. The refined semantic space allows
better clustering of animated characters even when they have diverse
manifestations. Using this space we can build dictionaries of characters in an
animation videos, and define specialized classifiers for specific stylistic
content (e.g., characters in a specific animation series) with very little user
effort. These classifiers are the basis for automatically labeling characters
in animation videos. We present results on a collection of characters in a
variety of animation styles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Training Challenges in Generative Adversarial Networks for Biomedical Image Analysis. (arXiv:2201.07646v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07646">
<div class="article-summary-box-inner">
<span><p>In biomedical image analysis, the applicability of deep learning methods is
directly impacted by the quantity of image data available. This is due to deep
learning models requiring large image datasets to provide high-level
performance. Generative Adversarial Networks (GANs) have been widely utilized
to address data limitations through the generation of synthetic biomedical
images. GANs consist of two models. The generator, a model that learns how to
produce synthetic images based on the feedback it receives. The discriminator,
a model that classifies an image as synthetic or real and provides feedback to
the generator. Throughout the training process, a GAN can experience several
technical challenges that impede the generation of suitable synthetic imagery.
First, the mode collapse problem whereby the generator either produces an
identical image or produces a uniform image from distinct input features.
Second, the non-convergence problem whereby the gradient descent optimizer
fails to reach a Nash equilibrium. Thirdly, the vanishing gradient problem
whereby unstable training behavior occurs due to the discriminator achieving
optimal classification performance resulting in no meaningful feedback being
provided to the generator. These problems result in the production of synthetic
imagery that is blurry, unrealistic, and less diverse. To date, there has been
no survey article outlining the impact of these technical challenges in the
context of the biomedical imagery domain. This work presents a review and
taxonomy based on solutions to the training problems of GANs in the biomedical
imaging domain. This survey highlights important challenges and outlines future
research directions about the training of GANs in the domain of biomedical
imagery.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open Source Handwritten Text Recognition on Medieval Manuscripts using Mixed Models and Document-Specific Finetuning. (arXiv:2201.07661v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07661">
<div class="article-summary-box-inner">
<span><p>This paper deals with the task of practical and open source Handwritten Text
Recognition (HTR) on German medieval manuscripts. We report on our efforts to
construct mixed recognition models which can be applied out-of-the-box without
any further document-specific training but also serve as a starting point for
finetuning by training a new model on a few pages of transcribed text (ground
truth). To train the mixed models we collected a corpus of 35 manuscripts and
ca. 12.5k text lines for two widely used handwriting styles, Gothic and
Bastarda cursives. Evaluating the mixed models out-of-the-box on four unseen
manuscripts resulted in an average Character Error Rate (CER) of 6.22%. After
training on 2, 4 and eventually 32 pages the CER dropped to 3.27%, 2.58%, and
1.65%, respectively. While the in-domain recognition and training of models
(Bastarda model to Bastarda material, Gothic to Gothic) unsurprisingly yielded
the best results, finetuning out-of-domain models to unseen scripts was still
shown to be superior to training from scratch.
</p>
<p>Our new mixed models have been made openly available to the community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-automatic 3D Object Keypoint Annotation and Detection for the Masses. (arXiv:2201.07665v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07665">
<div class="article-summary-box-inner">
<span><p>Creating computer vision datasets requires careful planning and lots of time
and effort. In robotics research, we often have to use standardized objects,
such as the YCB object set, for tasks such as object tracking, pose estimation,
grasping and manipulation, as there are datasets and pre-learned methods
available for these objects. This limits the impact of our research since
learning-based computer vision methods can only be used in scenarios that are
supported by existing datasets.
</p>
<p>In this work, we present a full object keypoint tracking toolkit,
encompassing the entire process from data collection, labeling, model learning
and evaluation. We present a semi-automatic way of collecting and labeling
datasets using a wrist mounted camera on a standard robotic arm. Using our
toolkit and method, we are able to obtain a working 3D object keypoint detector
and go through the whole process of data collection, annotation and learning in
just a couple hours of active time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neighborhood Spatial Aggregation MC Dropout for Efficient Uncertainty-aware Semantic Segmentation in Point Clouds. (arXiv:2201.07676v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07676">
<div class="article-summary-box-inner">
<span><p>Uncertainty-aware semantic segmentation of the point clouds includes the
predictive uncertainty estimation and the uncertainty-guided model
optimization. One key challenge in the task is the efficiency of point-wise
predictive distribution establishment. The widely-used MC dropout establishes
the distribution by computing the standard deviation of samples using multiple
stochastic forward propagations, which is time-consuming for tasks based on
point clouds containing massive points. Hence, a framework embedded with NSA-MC
dropout, a variant of MC dropout, is proposed to establish distributions in
just one forward pass. Specifically, the NSA-MC dropout samples the model many
times through a space-dependent way, outputting point-wise distribution by
aggregating stochastic inference results of neighbors. Based on this, aleatoric
and predictive uncertainties acquire from the predictive distribution. The
aleatoric uncertainty is integrated into the loss function to penalize noisy
points, avoiding the over-fitting of the model to some degree. Besides, the
predictive uncertainty quantifies the confidence degree of predictions.
Experimental results show that our framework obtains better segmentation
results of real-world point clouds and efficiently quantifies the credibility
of results. Our NSA-MC dropout is several times faster than MC dropout, and the
inference time does not establish a coupling relation with the sampling times.
The code will be available if the paper is accepted.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GroupGazer: A Tool to Compute the Gaze per Participant in Groups with integrated Calibration to Map the Gaze Online to a Screen or Beamer Projection. (arXiv:2201.07692v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07692">
<div class="article-summary-box-inner">
<span><p>In this paper we present GroupGaze. It is a tool that can be used to
calculate the gaze direction and the gaze position of whole groups. GroupGazer
calculates the gaze direction of every single person in the image and allows to
map these gaze vectors to a projection like a projector. In addition to the
person-specific gaze direction, the person affiliation of each gaze vector is
stored based on the position in the image. Also, it is possible to save the
group attention after a calibration. The software is free to use and requires a
simple webcam as well as an NVIDIA GPU and the operating system Windows or
Linux.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Visualization and Analysis of Wearable Health Data From COVID-19 Patients. (arXiv:2201.07698v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07698">
<div class="article-summary-box-inner">
<span><p>Effective visualizations were evaluated to reveal relevant health patterns
from multi-sensor real-time wearable devices that recorded vital signs from
patients admitted to hospital with COVID-19. Furthermore, specific challenges
associated with wearable health data visualizations, such as fluctuating data
quality resulting from compliance problems, time needed to charge the device
and technical problems are described. As a primary use case, we examined the
detection and communication of relevant health patterns visible in the vital
signs acquired by the technology. Customized heat maps and bar charts were used
to specifically highlight medically relevant patterns in vital signs. A survey
of two medical doctors, one clinical project manager and seven health data
science researchers was conducted to evaluate the visualization methods. From a
dataset of 84 hospitalized COVID-19 patients, we extracted one typical COVID-19
patient history and based on the visualizations showcased the health history of
two noteworthy patients. The visualizations were shown to be effective, simple
and intuitive in deducing the health status of patients. For clinical staff who
are time-constrained and responsible for numerous patients, such visualization
methods can be an effective tool to enable continuous acquisition and
monitoring of patients' health statuses even remotely.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Q-ViT: Fully Differentiable Quantization for Vision Transformer. (arXiv:2201.07703v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07703">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a fully differentiable quantization method for
vision transformer (ViT) named as Q-ViT, in which both of the quantization
scales and bit-widths are learnable parameters. Specifically, based on our
observation that heads in ViT display different quantization robustness, we
leverage head-wise bit-width to squeeze the size of Q-ViT while preserving
performance. In addition, we propose a novel technique named switchable scale
to resolve the convergence problem in the joint training of quantization scales
and bit-widths. In this way, Q-ViT pushes the limits of ViT quantization to
3-bit without heavy performance drop. Moreover, we analyze the quantization
robustness of every architecture component of ViT and show that the Multi-head
Self-Attention (MSA) and the Gaussian Error Linear Units (GELU) are the key
aspects for ViT quantization. This study provides some insights for further
research about ViT quantization. Extensive experiments on different ViT models,
such as DeiT and Swin Transformer show the effectiveness of our quantization
method. In particular, our method outperforms the state-of-the-art uniform
quantization method by 1.5% on DeiT-Tiny.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Object Detection in Autonomous Vehicles: Status and Open Challenges. (arXiv:2201.07706v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07706">
<div class="article-summary-box-inner">
<span><p>Object detection is a computer vision task that has become an integral part
of many consumer applications today such as surveillance and security systems,
mobile text recognition, and diagnosing diseases from MRI/CT scans. Object
detection is also one of the critical components to support autonomous driving.
Autonomous vehicles rely on the perception of their surroundings to ensure safe
and robust driving performance. This perception system uses object detection
algorithms to accurately determine objects such as pedestrians, vehicles,
traffic signs, and barriers in the vehicle's vicinity. Deep learning-based
object detectors play a vital role in finding and localizing these objects in
real-time. This article discusses the state-of-the-art in object detectors and
open challenges for their integration into autonomous vehicles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards holistic scene understanding: Semantic segmentation and beyond. (arXiv:2201.07734v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07734">
<div class="article-summary-box-inner">
<span><p>This dissertation addresses visual scene understanding and enhances
segmentation performance and generalization, training efficiency of networks,
and holistic understanding. First, we investigate semantic segmentation in the
context of street scenes and train semantic segmentation networks on
combinations of various datasets. In Chapter 2 we design a framework of
hierarchical classifiers over a single convolutional backbone, and train it
end-to-end on a combination of pixel-labeled datasets, improving
generalizability and the number of recognizable semantic concepts. Chapter 3
focuses on enriching semantic segmentation with weak supervision and proposes a
weakly-supervised algorithm for training with bounding box-level and
image-level supervision instead of only with per-pixel supervision. The memory
and computational load challenges that arise from simultaneous training on
multiple datasets are addressed in Chapter 4. We propose two methodologies for
selecting informative and diverse samples from datasets with weak supervision
to reduce our networks' ecological footprint without sacrificing performance.
Motivated by memory and computation efficiency requirements, in Chapter 5, we
rethink simultaneous training on heterogeneous datasets and propose a universal
semantic segmentation framework. This framework achieves consistent increases
in performance metrics and semantic knowledgeability by exploiting various
scene understanding datasets. Chapter 6 introduces the novel task of part-aware
panoptic segmentation, which extends our reasoning towards holistic scene
understanding. This task combines scene and parts-level semantics with
instance-level object detection. In conclusion, our contributions span over
convolutional network architectures, weakly-supervised learning, part and
panoptic segmentation, paving the way towards a holistic, rich, and sustainable
visual scene understanding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A pipeline for automated processing of Corona KH-4 (1962-1972) stereo imagery. (arXiv:2201.07756v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07756">
<div class="article-summary-box-inner">
<span><p>The Corona KH-4 reconnaissance satellite missions from 1962-1972 acquired
panoramic stereo imagery with high spatial resolution of 1.8-7.5 m. The
potential of 800,000+ declassified Corona images has not been leveraged due to
the complexities arising from handling of panoramic imaging geometry, film
distortions and limited availability of the metadata required for
georeferencing of the Corona imagery. This paper presents Corona Stereo
Pipeline (CoSP): A pipeline for processing of Corona KH-4 stereo panoramic
imagery. CoSP utlizes a deep learning based feature matcher SuperGlue to
automatically match features point between Corona KH-4 images and recent
satellite imagery to generate Ground Control Points (GCPs). To model the
imaging geometry and the scanning motion of the panoramic KH-4 cameras, a
rigorous camera model consisting of modified collinearity equations with time
dependent exterior orientation parameters is employed. The results show that
using the entire frame of the Corona image, bundle adjustment using
well-distributed GCPs results in an average standard deviation (SD) of less
than 2 pixels. The distortion pattern of image residuals of GCPs and y-parallax
in epipolar resampled images suggest that film distortions due to long term
storage as likely cause of systematic deviations. Compared to the SRTM DEM, the
Corona DEM computed using CoSP achieved a Normalized Median Absolute Deviation
(NMAD) of elevation differences of ~4 m over an area of approx. 4000 $km^2$. We
show that the proposed pipeline can be applied to sequence of complex scenes
involving high relief and glacierized terrain and that the resulting DEMs can
be used to compute long term glacier elevation changes over large areas.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Look Closer: Bridging Egocentric and Third-Person Views with Transformers for Robotic Manipulation. (arXiv:2201.07779v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07779">
<div class="article-summary-box-inner">
<span><p>Learning to solve precision-based manipulation tasks from visual feedback
using Reinforcement Learning (RL) could drastically reduce the engineering
efforts required by traditional robot systems. However, performing fine-grained
motor control from visual inputs alone is challenging, especially with a static
third-person camera as often used in previous work. We propose a setting for
robotic manipulation in which the agent receives visual feedback from both a
third-person camera and an egocentric camera mounted on the robot's wrist.
While the third-person camera is static, the egocentric camera enables the
robot to actively control its vision to aid in precise manipulation. To fuse
visual information from both cameras effectively, we additionally propose to
use Transformers with a cross-view attention mechanism that models spatial
attention from one view to another (and vice-versa), and use the learned
features as input to an RL policy. Our method improves learning over strong
single-view and multi-view baselines, and successfully transfers to a set of
challenging manipulation tasks on a real robot with uncalibrated cameras, no
access to state information, and a high degree of task variability. In a hammer
manipulation task, our method succeeds in 75% of trials versus 38% and 13% for
multi-view and single-view baselines, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards a General Deep Feature Extractor for Facial Expression Recognition. (arXiv:2201.07781v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07781">
<div class="article-summary-box-inner">
<span><p>The human face conveys a significant amount of information. Through facial
expressions, the face is able to communicate numerous sentiments without the
need for verbalisation. Visual emotion recognition has been extensively
studied. Recently several end-to-end trained deep neural networks have been
proposed for this task. However, such models often lack generalisation ability
across datasets. In this paper, we propose the Deep Facial Expression Vector
ExtractoR (DeepFEVER), a new deep learning-based approach that learns a visual
feature extractor general enough to be applied to any other facial emotion
recognition task or dataset. DeepFEVER outperforms state-of-the-art results on
the AffectNet and Google Facial Expression Comparison datasets. DeepFEVER's
extracted features also generalise extremely well to other datasets -- even
those unseen during training -- namely, the Real-World Affective Faces (RAF)
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ConDor: Self-Supervised Canonicalization of 3D Pose for Partial Shapes. (arXiv:2201.07788v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07788">
<div class="article-summary-box-inner">
<span><p>Progress in 3D object understanding has relied on manually canonicalized
shape datasets that contain instances with consistent position and orientation
(3D pose). This has made it hard to generalize these methods to in-the-wild
shapes, eg., from internet model collections or depth sensors. ConDor is a
self-supervised method that learns to Canonicalize the 3D orientation and
position for full and partial 3D point clouds. We build on top of Tensor Field
Networks (TFNs), a class of permutation- and rotation-equivariant, and
translation-invariant 3D networks. During inference, our method takes an unseen
full or partial 3D point cloud at an arbitrary pose and outputs an equivariant
canonical pose. During training, this network uses self-supervision losses to
learn the canonical pose from an un-canonicalized collection of full and
partial 3D point clouds. ConDor can also learn to consistently co-segment
object parts without any supervision. Extensive quantitative results on four
new metrics show that our approach outperforms existing methods while enabling
new applications such as operation on depth images and annotation transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GAMMA: A General Agent Motion Model for Autonomous Driving. (arXiv:1906.01566v5 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1906.01566">
<div class="article-summary-box-inner">
<span><p>This paper presents GAMMA, a general motion prediction model that enables
large-scale real-time simulation and planning for autonomous driving. GAMMA
models heterogeneous, interactive traffic agents. They operate under diverse
road conditions, with various geometric and kinematic constraints. GAMMA treats
the prediction task as constrained optimization in traffic agents' velocity
space. The objective is to optimize an agent's driving performance, while
obeying all the constraints resulting from the agent's kinematics, collision
avoidance with other agents, and the environmental context. Further, GAMMA
explicitly conditions the prediction on human behavioral states as parameters
of the optimization model, in order to account for versatile human behaviors.
We evaluated GAMMA on a set of real-world benchmark datasets. The results show
that GAMMA achieves high prediction accuracy on both homogeneous and
heterogeneous traffic datasets, with sub-millisecond execution time. Further,
the computational efficiency and the flexibility of GAMMA enable (i) simulation
of mixed urban traffic at many locations worldwide and (ii) planning for
autonomous driving in dense traffic with uncertain driver behaviors, both in
real-time. The open-source code of GAMMA is available online.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dilated Convolutions with Lateral Inhibitions for Semantic Image Segmentation. (arXiv:2006.03708v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.03708">
<div class="article-summary-box-inner">
<span><p>Dilated convolutions are widely used in deep semantic segmentation models as
they can enlarge the filters' receptive field without adding additional weights
nor sacrificing spatial resolution. However, as dilated convolutional filters
do not possess positional knowledge about the pixels on semantically meaningful
contours, they could lead to ambiguous predictions on object boundaries. In
addition, although dilating the filter can expand its receptive field, the
total number of sampled pixels remains unchanged, which usually comprises a
small fraction of the receptive field's total area. Inspired by the Lateral
Inhibition (LI) mechanisms in human visual systems, we propose the dilated
convolution with lateral inhibitions (LI-Convs) to overcome these limitations.
Introducing LI mechanisms improves the convolutional filter's sensitivity to
semantic object boundaries. Moreover, since LI-Convs also implicitly take the
pixels from the laterally inhibited zones into consideration, they can also
extract features at a denser scale. By integrating LI-Convs into the Deeplabv3+
architecture, we propose the Lateral Inhibited Atrous Spatial Pyramid Pooling
(LI-ASPP), the Lateral Inhibited MobileNet-V2 (LI-MNV2) and the Lateral
Inhibited ResNet (LI-ResNet). Experimental results on three benchmark datasets
(PASCAL VOC 2012, CelebAMask-HQ and ADE20K) show that our LI-based segmentation
models outperform the baseline on all of them, thus verify the effectiveness
and generality of the proposed LI-Convs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">3D Shape Reconstruction from Free-Hand Sketches. (arXiv:2006.09694v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.09694">
<div class="article-summary-box-inner">
<span><p>Sketches are the most abstract 2D representations of real-world objects.
Although a sketch usually has geometrical distortion and lacks visual cues,
humans can effortlessly envision a 3D object from it. This suggests that
sketches encode the information necessary for reconstructing 3D shapes. Despite
great progress achieved in 3D reconstruction from distortion-free line
drawings, such as CAD and edge maps, little effort has been made to reconstruct
3D shapes from free-hand sketches. We study this task and aim to enhance the
power of sketches in 3D-related applications such as interactive design and
VR/AR games.
</p>
<p>Unlike previous works, which mostly study distortion-free line drawings, our
3D shape reconstruction is based on free-hand sketches. A major challenge for
free-hand sketch 3D reconstruction comes from the insufficient training data
and free-hand sketch diversity, e.g. individualized sketching styles. We thus
propose data generation and standardization mechanisms. Instead of
distortion-free line drawings, synthesized sketches are adopted as input
training data. Additionally, we propose a sketch standardization module to
handle different sketch distortions and styles. Extensive experiments
demonstrate the effectiveness of our model and its strong generalizability to
various free-hand sketches. Our code is publicly available at
https://github.com/samaonline/3D-Shape-Reconstruction-from-Free-Hand-Sketches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards a Plug-and-Play Fully AutomatedUnsupervised 360-Degree Deep Learning VisualDefect Detection System. (arXiv:2012.06737v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.06737">
<div class="article-summary-box-inner">
<span><p>Visual defect detection is critical to ensure the quality of most products.
However, majority of small medium manufactures still rely on tedious and
error-prune human manual inspection. The main reasons include: 1) the existing
automated visual defect detection systems require altering production assembly
lines, which is time consuming and expensive 2) the existing systems require
manually collecting defective samples and labeling them for a comparison-based
algorithm or training a machine learning model. This introduces heavy burden
for Small and Medium-sized Enterprise (SME) manufactures as defects do not
happen often and are difficult and time-consuming to collect. Furthermore, we
cannot exhaustively collect or define all defect types as any new deviation
from acceptable products are defects. In this paper, we overcome these
challenges and design a three-stage plug-and-play fully automated unsupervised
360-degree defect detection system. In our system, products are freely placed
on an unaltered assembly line and receive 360 degree visual inspection with
multiple cameras from different angles. As such, the images collected from
real-world product assembly lines contain lots of background noise. The
products face different angles. The product sizes vary due to the distance to
cameras. All these make defect detection much more difficult. Our system use
object detection, background subtraction and unsupervised normalizing
flow-based defect detection techniques to tackle these difficulty. Experiments
show our system can achieve 0.90 AUROC in a real-world non-altered drink ware
production assembly line.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformers in Vision: A Survey. (arXiv:2101.01169v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.01169">
<div class="article-summary-box-inner">
<span><p>Astounding results from Transformer models on natural language tasks have
intrigued the vision community to study their application to computer vision
problems. Among their salient benefits, Transformers enable modeling long
dependencies between input sequence elements and support parallel processing of
sequence as compared to recurrent networks e.g., Long short-term memory (LSTM).
Different from convolutional networks, Transformers require minimal inductive
biases for their design and are naturally suited as set-functions. Furthermore,
the straightforward design of Transformers allows processing multiple
modalities (e.g., images, videos, text and speech) using similar processing
blocks and demonstrates excellent scalability to very large capacity networks
and huge datasets. These strengths have led to exciting progress on a number of
vision tasks using Transformer networks. This survey aims to provide a
comprehensive overview of the Transformer models in the computer vision
discipline. We start with an introduction to fundamental concepts behind the
success of Transformers i.e., self-attention, large-scale pre-training, and
bidirectional encoding. We then cover extensive applications of transformers in
vision including popular recognition tasks (e.g., image classification, object
detection, action recognition, and segmentation), generative modeling,
multi-modal tasks (e.g., visual-question answering, visual reasoning, and
visual grounding), video processing (e.g., activity recognition, video
forecasting), low-level vision (e.g., image super-resolution, image
enhancement, and colorization) and 3D analysis (e.g., point cloud
classification and segmentation). We compare the respective advantages and
limitations of popular techniques both in terms of architectural design and
their experimental value. Finally, we provide an analysis on open research
directions and possible future works.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Splicing Detection, Localization and Attribution via JPEG Primary Quantization Matrix Estimation and Clustering. (arXiv:2102.01439v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.01439">
<div class="article-summary-box-inner">
<span><p>Detection of inconsistencies of double JPEG artefacts across different image
regions is often used to detect local image manipulations, like image splicing,
and to localize them. In this paper, we move one step further, proposing an
end-to-end system that, in addition to detecting and localizing spliced
regions, can also distinguish regions coming from different donor images. We
assume that both the spliced regions and the background image have undergone a
double JPEG compression, and use a local estimate of the primary quantization
matrix to distinguish between spliced regions taken from different sources. To
do so, we cluster the image blocks according to the estimated primary
quantization matrix and refine the result by means of morphological
reconstruction. The proposed method can work in a wide variety of settings
including aligned and non-aligned double JPEG compression, and regardless of
whether the second compression is stronger or weaker than the first one. We
validated the proposed approach by means of extensive experiments showing its
superior performance with respect to baseline methods working in similar
conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wavelength-based Attributed Deep Neural Network for Underwater Image Restoration. (arXiv:2106.07910v3 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07910">
<div class="article-summary-box-inner">
<span><p>Background: Underwater images, in general, suffer from low contrast and high
color distortions due to the non-uniform attenuation of the light as it
propagates through the water. In addition, the degree of attenuation varies
with the wavelength resulting in the asymmetric traversing of colors. Despite
the prolific works for underwater image restoration (UIR) using deep learning,
the above asymmetricity has not been addressed in the respective network
engineering.
</p>
<p>Contributions: As the first novelty, this paper shows that attributing the
right receptive field size (context) based on the traversing range of the color
channel may lead to a substantial performance gain for the task of UIR.
Further, it is important to suppress the irrelevant multi-contextual features
and increase the representational power of the model. Therefore, as a second
novelty, we have incorporated an attentive skip mechanism to adaptively refine
the learned multi-contextual features. The proposed framework, called Deep
WaveNet, is optimized using the traditional pixel-wise and feature-based cost
functions. An extensive set of experiments have been carried out to show the
efficacy of the proposed scheme over existing best-published literature on
benchmark datasets. More importantly, we have demonstrated a comprehensive
validation of enhanced images across various high-level vision tasks, e.g.,
underwater image semantic segmentation, and diver's 2D pose estimation. A
sample video to exhibit our real-world performance is available at
\url{https://tinyurl.com/yzcrup9n}. Also, we have open-sourced our framework at
\url{https://github.com/pksvision/Deep-WaveNet-UnderwaterImage-Restoration}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Rule to Multi-Adapt: Generalized Multi-source Feature Learning Using Unsupervised Domain Adaptation for Colorectal Cancer Tissue Detection. (arXiv:2108.09178v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09178">
<div class="article-summary-box-inner">
<span><p>Supervised learning is constrained by the availability of labeled data, which
are especially expensive to acquire in the field of digital pathology. Making
use of open-source data for pre-training or using domain adaptation can be a
way to overcome this issue. However, pre-trained networks often fail to
generalize to new test domains that are not distributed identically due to
tissue stainings, types, and textures variations. Additionally, current domain
adaptation methods mainly rely on fully-labeled source datasets. In this work,
we propose Self-Rule to Multi-Adapt (SRMA), which takes advantage of
self-supervised learning to perform domain adaptation, and removes the
necessity of fully-labeled source datasets. SRMA can effectively transfer the
discriminative knowledge obtained from a few labeled source domain's data to a
new target domain without requiring additional tissue annotations. Our method
harnesses both domains' structures by capturing visual similarity with
intra-domain and cross-domain self-supervision. Moreover, we present a
generalized formulation of our approach that allows the framework to learn from
multiple source domains. We show that our proposed method outperforms baselines
for domain adaptation of colorectal tissue type classification \new{in single
and multi-source settings}, and further validate our approach on an in-house
clinical cohort. The code and trained models are available open-source:
https://github.com/christianabbet/SRA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reconstructing Cosmic Polarization Rotation with ResUNet-CMB. (arXiv:2109.09715v2 [astro-ph.CO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09715">
<div class="article-summary-box-inner">
<span><p>Cosmic polarization rotation, which may result from parity-violating new
physics or the presence of primordial magnetic fields, converts $E$-mode
polarization of the cosmic microwave background (CMB) into $B$-mode
polarization. Anisotropic cosmic polarization rotation leads to statistical
anisotropy in CMB polarization and can be reconstructed with quadratic
estimator techniques similar to those designed for gravitational lensing of the
CMB. At the sensitivity of upcoming CMB surveys, lensing-induced $B$-mode
polarization will act as a limiting factor in the search for anisotropic cosmic
polarization rotation, meaning that an analysis which incorporates some form of
delensing will be required to improve constraints on the effect with future
surveys. In this paper we extend the ResUNet-CMB convolutional neural network
to reconstruct anisotropic cosmic polarization rotation in the presence of
gravitational lensing and patchy reionization, and we show that the network
simultaneously reconstructs all three effects with variance that is lower than
that from the standard quadratic estimator nearly matching the performance of
an iterative reconstruction method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Map Update Using Dashcam Videos. (arXiv:2109.12131v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12131">
<div class="article-summary-box-inner">
<span><p>Autonomous driving requires 3D maps that provide accurate and up-to-date
information about semantic landmarks. Due to the wider availability and lower
cost of cameras compared with laser scanners, vision-based mapping solutions,
especially the ones using crowdsourced visual data, have attracted much
attention from academia and industry. However, previous works have mainly
focused on creating 3D point clouds, leaving automatic change detection as open
issues. We propose in this paper a pipeline for initiating and updating 3D maps
with dashcam videos, with a focus on automatic change detection based on
comparison of metadata (e.g., the types and locations of traffic signs). To
improve the performance of metadata generation, which depends on the accuracy
of 3D object detection and localization, we introduce a novel deep
learning-based pixel-wise 3D localization algorithm. The algorithm, trained
directly with SfM point cloud data, can locate objects detected from 2D images
in a 3D space with high accuracy by estimating not only depth from monocular
images but also lateral and height distances. In addition, we also propose a
point clustering and thresholding algorithm to improve the robustness of the
system to errors. We have performed experiments on two distinct areas - a
campus and a residential area - with different types of cameras, lighting, and
weather conditions. The changes were detected with 85% and 100% accuracy in the
campus and residential areas, respectively. The errors in the campus area were
mainly due to traffic signs seen from a far distance to the vehicle and
intended for pedestrians and cyclists only. We also conducted cause analysis of
the detection and localization errors to measure the impact from the
performance of the background technology in use.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generation of microbial colonies dataset with deep learning style transfer. (arXiv:2111.03789v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.03789">
<div class="article-summary-box-inner">
<span><p>We introduce an effective strategy to generate an annotated synthetic dataset
of microbiological images of Petri dishes that can be used to train deep
learning models in a fully supervised fashion. The developed generator employs
traditional computer vision algorithms together with a neural style transfer
method for data augmentation. We show that the method is able to synthesize a
dataset of realistic looking images that can be used to train a neural network
model capable of localising, segmenting, and classifying five different
microbial species. Our method requires significantly fewer resources to obtain
a useful dataset than collecting and labeling a whole large set of real images
with annotations. We show that starting with only 100 real images, we can
generate data to train a detector that achieves comparable results (detection
mAP = 0.416, and counting MAE = 4.49) to the same detector but trained on a
real, several dozen times bigger dataset (mAP = 0.520, MAE = 4.31), containing
over 7k images. We prove the usefulness of the method in microbe detection and
segmentation, but we expect that it is general and flexible and can also be
applicable in other domains of science and industry to detect various objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Medical Visual Question Answering: A Survey. (arXiv:2111.10056v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2111.10056">
<div class="article-summary-box-inner">
<span><p>Medical Visual Question Answering~(VQA) is a combination of medical
artificial intelligence and popular VQA challenges. Given a medical image and a
clinically relevant question in natural language, the medical VQA system is
expected to predict a plausible and convincing answer. Although the
general-domain VQA has been extensively studied, the medical VQA still needs
specific investigation and exploration due to its task features. In the first
part of this survey, we collect and discuss the publicly available medical VQA
datasets up to date about the data source, data quantity, and task feature. In
the second part, we review the approaches used in medical VQA tasks. We
summarize and discuss their techniques, innovation, and potential improvement.
In the last part, we analyze some medical-specific challenges for the field and
discuss future research directions. Our goal is to provide comprehensive
information for researchers interested in medical artificial intelligence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extrapolating from a Single Image to a Thousand Classes using Distillation. (arXiv:2112.00725v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.00725">
<div class="article-summary-box-inner">
<span><p>What can neural networks learn about the visual world from a single image?
While it obviously cannot contain the multitudes of possible objects, scenes
and lighting conditions that exist - within the space of all possible
256^(3x224x224) 224-sized square images, it might still provide a strong prior
for natural images. To analyze this hypothesis, we develop a framework for
training neural networks from scratch using a single image by means of
knowledge distillation from a supervisedly pretrained teacher. With this, we
find that the answer to the above question is: 'surprisingly, a lot'. In
quantitative terms, we find top-1 accuracies of 94%/74% on CIFAR-10/100, 59% on
ImageNet, and by extending this method to video and audio, 77% on UCF-101 and
84% on SpeechCommands. In extensive analyses we disentangle the effect of
augmentations, choice of source image and network architectures and also
discover "panda neurons" in networks that have never seen a panda. This work
shows that one image can be used to extrapolate to thousands of object classes
and motivates a renewed research agenda on the fundamental interplay of
augmentations and image.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Overcoming the Domain Gap in Neural Action Representations. (arXiv:2112.01176v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.01176">
<div class="article-summary-box-inner">
<span><p>Relating animal behaviors to brain activity is a fundamental goal in
neuroscience, with practical applications in building robust brain-machine
interfaces. However, the domain gap between individuals is a major issue that
prevents the training of general models that work on unlabeled subjects.
</p>
<p>Since 3D pose data can now be reliably extracted from multi-view video
sequences without manual intervention, we propose to use it to guide the
encoding of neural action representations together with a set of neural and
behavioral augmentations exploiting the properties of microscopy imaging. To
reduce the domain gap, during training, we swap neural and behavioral data
across animals that seem to be performing similar actions.
</p>
<p>To demonstrate this, we test our methods on three very different multimodal
datasets; one that features flies and their neural activity, one that contains
human neural Electrocorticography (ECoG) data, and lastly the RGB video data of
human activities from different viewpoints.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active learning with MaskAL reduces annotation effort for training Mask R-CNN. (arXiv:2112.06586v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.06586">
<div class="article-summary-box-inner">
<span><p>The generalisation performance of a convolutional neural network (CNN) is
influenced by the quantity, quality, and variety of the training images.
Training images must be annotated, and this is time consuming and expensive.
The goal of our work was to reduce the number of annotated images needed to
train a CNN while maintaining its performance. We hypothesised that the
performance of a CNN can be improved faster by ensuring that the set of
training images contains a large fraction of hard-to-classify images. The
objective of our study was to test this hypothesis with an active learning
method that can automatically select the hard-to-classify images. We developed
an active learning method for Mask Region-based CNN (Mask R-CNN) and named this
method MaskAL. MaskAL involved the iterative training of Mask R-CNN, after
which the trained model was used to select a set of unlabelled images about
which the model was uncertain. The selected images were then annotated and used
to retrain Mask R-CNN, and this was repeated for a number of sampling
iterations. In our study, Mask R-CNN was trained on 2500 broccoli images that
were selected through 12 sampling iterations by either MaskAL or a random
sampling method from a training set of 14,000 broccoli images. For all sampling
iterations, MaskAL performed significantly better than the random sampling.
Furthermore, MaskAL had the same performance after sampling 900 images as the
random sampling had after 2300 images. Compared to a Mask R-CNN model that was
trained on the entire training set (14,000 images), MaskAL achieved 93.9% of
its performance with 17.9% of its training data. The random sampling achieved
81.9% of its performance with 16.4% of its training data. We conclude that by
using MaskAL, the annotation effort can be reduced for training Mask R-CNN on a
broccoli dataset. Our software is available on
https://github.com/pieterblok/maskal.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When less is more: Simplifying inputs aids neural network understanding. (arXiv:2201.05610v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05610">
<div class="article-summary-box-inner">
<span><p>How do neural network image classifiers respond to simpler and simpler
inputs? And what do such responses reveal about the learning process? To answer
these questions, we need a clear measure of input simplicity (or inversely,
complexity), an optimization objective that correlates with simplification, and
a framework to incorporate such objective into training and inference. Lastly
we need a variety of testbeds to experiment and evaluate the impact of such
simplification on learning. In this work, we measure simplicity with the
encoding bit size given by a pretrained generative model, and minimize the bit
size to simplify inputs in training and inference. We investigate the effect of
such simplification in several scenarios: conventional training, dataset
condensation and post-hoc explanations. In all settings, inputs are simplified
along with the original classification task, and we investigate the trade-off
between input simplicity and task performance. For images with injected
distractors, such simplification naturally removes superfluous information. For
dataset condensation, we find that inputs can be simplified with almost no
accuracy degradation. When used in post-hoc explanation, our learning-based
simplification approach offers a valuable new tool to explore the basis of
network decisions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GradTail: Learning Long-Tailed Data Using Gradient-based Sample Weighting. (arXiv:2201.05938v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05938">
<div class="article-summary-box-inner">
<span><p>We propose GradTail, an algorithm that uses gradients to improve model
performance on the fly in the face of long-tailed training data distributions.
Unlike conventional long-tail classifiers which operate on converged - and
possibly overfit - models, we demonstrate that an approach based on gradient
dot product agreement can isolate long-tailed data early on during model
training and improve performance by dynamically picking higher sample weights
for that data. We show that such upweighting leads to model improvements for
both classification and regression models, the latter of which are relatively
unexplored in the long-tail literature, and that the long-tail examples found
by gradient alignment are consistent with our semantic expectations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">H&E-adversarial network: a convolutional neural network to learn stain-invariant features through Hematoxylin & Eosin regression. (arXiv:2201.06329v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06329">
<div class="article-summary-box-inner">
<span><p>Computational pathology is a domain that aims to develop algorithms to
automatically analyze large digitized histopathology images, called whole slide
images (WSI). WSIs are produced scanning thin tissue samples that are stained
to make specific structures visible. They show stain colour heterogeneity due
to different preparation and scanning settings applied across medical centers.
Stain colour heterogeneity is a problem to train convolutional neural networks
(CNN), the state-of-the-art algorithms for most computational pathology tasks,
since CNNs usually underperform when tested on images including different stain
variations than those within data used to train the CNN. Despite several
methods that were developed, stain colour heterogeneity is still an unsolved
challenge that limits the development of CNNs that can generalize on data from
several medical centers. This paper aims to present a novel method to train
CNNs that better generalize on data including several colour variations. The
method, called H&amp;E-adversarial CNN, exploits H&amp;E matrix information to learn
stain-invariant features during the training. The method is evaluated on the
classification of colon and prostate histopathology images, involving eleven
heterogeneous datasets, and compared with five other techniques used to handle
stain colour heterogeneity. H&amp;E-adversarial CNNs show an improvement in
performance compared to the other algorithms, demonstrating that it can help to
better deal with stain colour heterogeneous images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">STURE: Spatial-Temporal Mutual Representation Learning for Robust Data Association in Online Multi-Object Tracking. (arXiv:2201.06824v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06824">
<div class="article-summary-box-inner">
<span><p>Online multi-object tracking (MOT) is a longstanding task for computer vision
and intelligent vehicle platform. At present, the main paradigm is
tracking-by-detection, and the main difficulty of this paradigm is how to
associate the current candidate detection with the historical tracklets.
However, in the MOT scenarios, each historical tracklet is composed of an
object sequence, while each candidate detection is just a flat image, which
lacks the temporal features of the object sequence. The feature difference
between current candidate detection and historical tracklets makes the object
association much harder. Therefore, we propose a Spatial-Temporal Mutual
{Representation} Learning (STURE) approach which learns spatial-temporal
representations between current candidate detection and historical sequence in
a mutual representation space. For the historical trackelets, the detection
learning network is forced to match the representations of sequence learning
network in a mutual representation space. The proposed approach is capable of
extracting more distinguishing detection and sequence representations by using
various designed losses in object association. As a result, spatial-temporal
feature is learned mutually to reinforce the current detection features, and
the feature difference can be relieved. To prove the robustness of the STURE,
it is applied to the public MOT challenge benchmarks and performs well compared
with various state-of-the-art online MOT trackers based on identity-preserving
metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RePre: Improving Self-Supervised Vision Transformer with Reconstructive Pre-training. (arXiv:2201.06857v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06857">
<div class="article-summary-box-inner">
<span><p>Recently, self-supervised vision transformers have attracted unprecedented
attention for their impressive representation learning ability. However, the
dominant method, contrastive learning, mainly relies on an instance
discrimination pretext task, which learns a global understanding of the image.
This paper incorporates local feature learning into self-supervised vision
transformers via Reconstructive Pre-training (RePre). Our RePre extends
contrastive frameworks by adding a branch for reconstructing raw image pixels
in parallel with the existing contrastive objective. RePre is equipped with a
lightweight convolution-based decoder that fuses the multi-hierarchy features
from the transformer encoder. The multi-hierarchy features provide rich
supervisions from low to high semantic information, which are crucial for our
RePre. Our RePre brings decent improvements on various contrastive frameworks
with different vision transformer architectures. Transfer performance in
downstream tasks outperforms supervised pre-training and state-of-the-art
(SOTA) self-supervised counterparts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attentional Feature Refinement and Alignment Network for Aircraft Detection in SAR Imagery. (arXiv:2201.07124v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.07124">
<div class="article-summary-box-inner">
<span><p>Aircraft detection in Synthetic Aperture Radar (SAR) imagery is a challenging
task in SAR Automatic Target Recognition (SAR ATR) areas due to aircraft's
extremely discrete appearance, obvious intraclass variation, small size and
serious background's interference. In this paper, a single-shot detector namely
Attentional Feature Refinement and Alignment Network (AFRAN) is proposed for
detecting aircraft in SAR images with competitive accuracy and speed.
Specifically, three significant components including Attention Feature Fusion
Module (AFFM), Deformable Lateral Connection Module (DLCM) and Anchor-guided
Detection Module (ADM), are carefully designed in our method for refining and
aligning informative characteristics of aircraft. To represent characteristics
of aircraft with less interference, low-level textural and high-level semantic
features of aircraft are fused and refined in AFFM throughly. The alignment
between aircraft's discrete back-scatting points and convolutional sampling
spots is promoted in DLCM. Eventually, the locations of aircraft are predicted
precisely in ADM based on aligned features revised by refined anchors. To
evaluate the performance of our method, a self-built SAR aircraft sliced
dataset and a large scene SAR image are collected. Extensive quantitative and
qualitative experiments with detailed analysis illustrate the effectiveness of
the three proposed components. Furthermore, the topmost detection accuracy and
competitive speed are achieved by our method compared with other
domain-specific,e.g., DAPN, PADN, and general CNN-based methods,e.g., FPN,
Cascade R-CNN, SSD, RefineDet and RPDet.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-01-20 23:06:36.009710927 UTC">2022-01-20 23:06:36 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>