<!DOCTYPE html>
<html lang="en">
<head>
<title>M.D.Arxiv</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2022-02-28T01:30:00Z">02-28</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Oolong: Investigating What Makes Crosslingual Transfer Hard with Controlled Studies. (arXiv:2202.12312v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12312">
<div class="article-summary-box-inner">
<span><p>Little is known about what makes cross-lingual transfer hard, since factors
like tokenization, morphology, and syntax all change at once between languages.
To disentangle the impact of these factors, we propose a set of controlled
transfer studies: we systematically transform GLUE tasks to alter different
factors one at a time, then measure the resulting drops in a pretrained model's
downstream performance. In contrast to prior work suggesting little effect from
syntax on knowledge transfer, we find significant impacts from syntactic shifts
(3-6% drop), though models quickly adapt with continued pretraining on a small
dataset. However, we find that by far the most impactful factor for
crosslingual transfer is the challenge of aligning the new embeddings with the
existing transformer layers (18% drop), with little additional effect from
switching tokenizers (&lt;2% drop) or word morphologies (&lt;2% drop). Moreover,
continued pretraining with a small dataset is not very effective at closing
this gap - suggesting that new directions are needed for solving this problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Better Meta-Initialization with Task Augmentation for Kindergarten-aged Speech Recognition. (arXiv:2202.12326v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12326">
<div class="article-summary-box-inner">
<span><p>Children's automatic speech recognition (ASR) is always difficult due to, in
part, the data scarcity problem, especially for kindergarten-aged kids. When
data are scarce, the model might overfit to the training data, and hence good
starting points for training are essential. Recently, meta-learning was
proposed to learn model initialization (MI) for ASR tasks of different
languages. This method leads to good performance when the model is adapted to
an unseen language. However, MI is vulnerable to overfitting on training tasks
(learner overfitting). It is also unknown whether MI generalizes to other
low-resource tasks. In this paper, we validate the effectiveness of MI in
children's ASR and attempt to alleviate the problem of learner overfitting. To
achieve model-agnostic meta-learning (MAML), we regard children's speech at
each age as a different task. In terms of learner overfitting, we propose a
task-level augmentation method by simulating new ages using frequency warping
techniques. Detailed experiments are conducted to show the impact of task
augmentation on each age for kindergarten-aged speech. As a result, our
approach achieves a relative word error rate (WER) improvement of 51% over the
baseline system with no augmentation or initialization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DoCoGen: Domain Counterfactual Generation for Low Resource Domain Adaptation. (arXiv:2202.12350v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12350">
<div class="article-summary-box-inner">
<span><p>Natural language processing (NLP) algorithms have become very successful, but
they still struggle when applied to out-of-distribution examples. In this paper
we propose a controllable generation approach in order to deal with this domain
adaptation (DA) challenge. Given an input text example, our DoCoGen algorithm
generates a domain-counterfactual textual example (D-con) - that is similar to
the original in all aspects, including the task label, but its domain is
changed to a desired one. Importantly, DoCoGen is trained using only unlabeled
examples from multiple domains - no NLP task labels or parallel pairs of
textual examples and their domain-counterfactuals are required. We use the
D-cons generated by DoCoGen to augment a sentiment classifier in 20 DA setups,
where source-domain labeled data is scarce. Our model outperforms strong
baselines and improves the accuracy of a state-of-the-art unsupervised DA
algorithm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UnifiedQA-v2: Stronger Generalization via Broader Cross-Format Training. (arXiv:2202.12359v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12359">
<div class="article-summary-box-inner">
<span><p>We present UnifiedQA-v2, a QA model built with the same process as UnifiedQA,
except that it utilizes more supervision -- roughly 3x the number of datasets
used for UnifiedQA. This generally leads to better in-domain and cross-domain
results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Combine Instructions in LLVM Compiler. (arXiv:2202.12379v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12379">
<div class="article-summary-box-inner">
<span><p>Instruction combiner (IC) is a critical compiler optimization pass, which
replaces a sequence of instructions with an equivalent and optimized
instruction sequence at basic block level. There can be thousands of
instruction-combining patterns which need to be frequently updated as new
coding idioms/applications and novel hardware evolve over time. This results in
frequent updates to the IC optimization pass thereby incurring considerable
human effort and high software maintenance costs. To mitigate these challenges
associated with the traditional IC, we design and implement a Neural
Instruction Combiner (NIC) and demonstrate its feasibility by integrating it
into the standard LLVM compiler optimization pipeline. NIC leverages neural
sequence-to-sequence (Seq2Seq) models for generating optimized encoded IR
sequence from the unoptimized encoded IR sequence. To the best of our
knowledge, ours is the first work demonstrating the feasibility of a neural
instruction combiner built into a full-fledged compiler pipeline. Given the
novelty of this task, we built a new dataset for training our NIC neural model.
We show that NIC achieves exact match results percentage of 72% for optimized
sequences as compared to traditional IC and neural machine translation metric
Bleu precision score of 0.94, demonstrating its feasibility in a production
compiler pipeline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-Grained Prediction of Political Leaning on Social Media with Unsupervised Deep Learning. (arXiv:2202.12382v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12382">
<div class="article-summary-box-inner">
<span><p>Predicting the political leaning of social media users is an increasingly
popular task, given its usefulness for electoral forecasts, opinion dynamics
models and for studying the political dimension of polarization and
disinformation. Here, we propose a novel unsupervised technique for learning
fine-grained political leaning from the textual content of social media posts.
Our technique leverages a deep neural network for learning latent political
ideologies in a representation learning task. Then, users are projected in a
low-dimensional ideology space where they are subsequently clustered. The
political leaning of a user is automatically derived from the cluster to which
the user is assigned. We evaluated our technique in two challenging
classification tasks and we compared it to baselines and other state-of-the-art
approaches. Our technique obtains the best results among all unsupervised
techniques, with micro F1 = 0.426 in the 8-class task and micro F1 = 0.772 in
the 3-class task. Other than being interesting on their own, our results also
pave the way for the development of new and better unsupervised approaches for
the detection of fine-grained political leaning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TrimBERT: Tailoring BERT for Trade-offs. (arXiv:2202.12411v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12411">
<div class="article-summary-box-inner">
<span><p>Models based on BERT have been extremely successful in solving a variety of
natural language processing (NLP) tasks. Unfortunately, many of these large
models require a great deal of computational resources and/or time for
pre-training and fine-tuning which limits wider adoptability. While
self-attention layers have been well-studied, a strong justification for
inclusion of the intermediate layers which follow them remains missing in the
literature. In this work, we show that reducing the number of intermediate
layers in BERT-Base results in minimal fine-tuning accuracy loss of downstream
tasks while significantly decreasing model size and training time. We further
mitigate two key bottlenecks, by replacing all softmax operations in the
self-attention layers with a computationally simpler alternative and removing
half of all layernorm operations. This further decreases the training time
while maintaining a high level of fine-tuning accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Construction of Large-Scale Misinformation Labeled Datasets from Social Media Discourse using Label Refinement. (arXiv:2202.12413v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12413">
<div class="article-summary-box-inner">
<span><p>Malicious accounts spreading misinformation has led to widespread false and
misleading narratives in recent times, especially during the COVID-19 pandemic,
and social media platforms struggle to eliminate these contents rapidly. This
is because adapting to new domains requires human intensive fact-checking that
is slow and difficult to scale. To address this challenge, we propose to
leverage news-source credibility labels as weak labels for social media posts
and propose model-guided refinement of labels to construct large-scale, diverse
misinformation labeled datasets in new domains. The weak labels can be
inaccurate at the article or social media post level where the stance of the
user does not align with the news source or article credibility. We propose a
framework to use a detection model self-trained on the initial weak labels with
uncertainty sampling based on entropy in predictions of the model to identify
potentially inaccurate labels and correct for them using self-supervision or
relabeling. The framework will incorporate social context of the post in terms
of the community of its associated user for surfacing inaccurate labels towards
building a large-scale dataset with minimum human effort. To provide labeled
datasets with distinction of misleading narratives where information might be
missing significant context or has inaccurate ancillary details, the proposed
framework will use the few labeled samples as class prototypes to separate high
confidence samples into false, unproven, mixture, mostly false, mostly true,
true, and debunk information. The approach is demonstrated for providing a
large-scale misinformation dataset on COVID-19 vaccines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep neural networks for fine-grained surveillance of overdose mortality. (arXiv:2202.12448v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12448">
<div class="article-summary-box-inner">
<span><p>Surveillance of drug overdose deaths relies on death certificates for
identification of the substances that caused death. Drugs and drug classes can
be identified through the International Classification of Diseases, 10th
Revision (ICD-10) codes present on death certificates. However, ICD-10 codes do
not always provide high levels of specificity in drug identification. To
achieve more fine-grained identification of substances on a death certificate,
the free-text cause of death section, completed by the medical certifier, must
be analyzed. Current methods for analyzing free-text death certificates rely
solely on look-up tables for identifying specific substances, which must be
frequently updated and maintained. To improve identification of drugs on death
certificates, a deep learning named-entity recognition model was developed,
which achieved an F1-score of 99.13%. This model can identify new drug
misspellings and novel substances that are not present on current surveillance
look-up tables, enhancing the surveillance of drug overdose deaths.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">APEACH: Attacking Pejorative Expressions with Analysis on Crowd-Generated Hate Speech Evaluation Datasets. (arXiv:2202.12459v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12459">
<div class="article-summary-box-inner">
<span><p>Detecting toxic or pejorative expressions in online communities has become
one of the main concerns for preventing the users' mental harm. This led to the
development of large-scale hate speech detection datasets of various domains,
which are mainly built upon web-crawled texts with labels by crowd workers.
However, for languages other than English, researchers might have to rely on
only a small-sized corpus due to the lack of data-driven research of hate
speech detection. This sometimes misleads the evaluation of prevalently used
pretrained language models (PLMs) such as BERT, given that PLMs often share the
domain of pretraining corpus with the evaluation set, resulting in
over-representation of the detection performance. Also, the scope of pejorative
expressions might be restricted if the dataset is built on a single domain
text.
</p>
<p>To alleviate the above problems in Korean hate speech detection, we propose
APEACH,a method that allows the collection of hate speech generated by
unspecified users. By controlling the crowd-generation of hate speech and
adding only a minimum post-labeling, we create a corpus that enables the
generalizable and fair evaluation of hate speech detection regarding text
domain and topic. We Compare our outcome with prior work on an annotation-based
toxic news comment dataset using publicly available PLMs. We check that our
dataset is less sensitive to the lexical overlap between the evaluation set and
pretraining corpus of PLMs, showing that it helps mitigate the unexpected
under/over-representation of model performance. We distribute our dataset
publicly online to further facilitate the general-domain hate speech detection
in Korean.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PromDA: Prompt-based Data Augmentation for Low-Resource NLU Tasks. (arXiv:2202.12499v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12499">
<div class="article-summary-box-inner">
<span><p>This paper focuses on the Data Augmentation for low-resource Natural Language
Understanding (NLU) tasks. We propose Prompt-based D}ata Augmentation model
(PromDA) which only trains small-scale Soft Prompt (i.e., a set of trainable
vectors) in the frozen Pre-trained Language Models (PLMs). This avoids human
effort in collecting unlabeled in-domain data and maintains the quality of
generated synthetic data. In addition, PromDA generates synthetic data via two
different views and filters out the low-quality data using NLU models.
Experiments on four benchmarks show that synthetic data produced by PromDA
successfully boost up the performance of NLU models which consistently
outperform several competitive baseline models, including a state-of-the-art
semi-supervised model using unlabeled in-domain data. The synthetic data from
PromDA are also complementary with unlabeled in-domain data. The NLU models can
be further improved when they are combined for training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Asyncval: A Toolkit for Asynchronously Validating Dense Retriever Checkpoints during Training. (arXiv:2202.12510v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12510">
<div class="article-summary-box-inner">
<span><p>The process of model checkpoint validation refers to the evaluation of the
performance of a model checkpoint executed on a held-out portion of the
training data while learning the hyperparameters of the model, and is used to
avoid over-fitting and determine when the model has converged so as to stop
training. A simple and efficient strategy to validate deep learning checkpoints
is the addition of validation loops to execute during training. However, the
validation of dense retrievers (DR) checkpoints is not as trivial -- and the
addition of validation loops is not efficient. This is because, in order to
accurately evaluate the performance of a DR checkpoint, the whole document
corpus needs to be encoded into vectors using the current checkpoint before any
actual retrieval operation for checkpoint validation can be performed. This
corpus encoding process can be very time-consuming if the document corpus
contains millions of documents (e.g., 8.8m for MS MARCO and 21m for Natural
Questions). Thus, a naive use of validation loops during training will
significantly increase training time. To address this issue, in this demo
paper, we propose Asyncval: a Python-based toolkit for efficiently validating
DR checkpoints during training. Instead of pausing the training loop for
validating DR checkpoints, Asyncval decouples the validation loop from the
training loop, uses another GPU to automatically validate new DR checkpoints
and thus permits to perform validation asynchronously from training. Asyncval
also implements a range of different corpus subset sampling strategies for
validating DR checkpoints; these strategies allow to further speed up the
validation process. We provide an investigation of these methods in terms of
their impact on validation time and validation fidelity. Asyncval is made
available as an open-source project at \url{https://github.com/ielab/asyncval}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Screening Gender Transfer in Neural Machine Translation. (arXiv:2202.12568v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12568">
<div class="article-summary-box-inner">
<span><p>This paper aims at identifying the information flow in state-of-the-art
machine translation systems, taking as example the transfer of gender when
translating from French into English. Using a controlled set of examples, we
experiment several ways to investigate how gender information circulates in a
encoder-decoder architecture considering both probing techniques as well as
interventions on the internal representations used in the MT system. Our
results show that gender information can be found in all token representations
built by the encoder and the decoder and lead us to conclude that there are
multiple pathways for gender transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeuralKG: An Open Source Library for Diverse Representation Learning of Knowledge Graphs. (arXiv:2202.12571v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12571">
<div class="article-summary-box-inner">
<span><p>NeuralKG is an open-source Python-based library for diverse representation
learning of knowledge graphs. It implements three different series of Knowledge
Graph Embedding (KGE) methods, including conventional KGEs, GNN-based KGEs, and
Rule-based KGEs. With a unified framework, NeuralKG successfully reproduces
link prediction results of these methods on benchmarks, freeing users from the
laborious task of reimplementing them, especially for some methods originally
written in non-python programming languages. Besides, NeuralKG is highly
configurable and extensible. It provides various decoupled modules that can be
mixed and adapted to each other. Thus with NeuralKG, developers and researchers
can quickly implement their own designed models and obtain the optimal training
methods to achieve the best performance efficiently. We built an website in
<a href="http://neuralkg.zjukg.cn">this http URL</a> to organize an open and shared KG representation
learning community. The source code is all publicly released at
https://github.com/zjukg/NeuralKG.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mining Naturally-occurring Corrections and Paraphrases from Wikipedia's Revision History. (arXiv:2202.12575v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12575">
<div class="article-summary-box-inner">
<span><p>Naturally-occurring instances of linguistic phenomena are important both for
training and for evaluating automatic processes on text. When available in
large quantities, they also prove interesting material for linguistic studies.
In this article, we present a new resource built from Wikipedia's revision
history, called WiCoPaCo (Wikipedia Correction and Paraphrase Corpus), which
contains numerous editings by human contributors, including various corrections
and rewritings. We discuss the main motivations for building such a resource,
describe how it was built and present initial applications on French.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Multilingual Models for Automatic Speech Recognition. (arXiv:2202.12576v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12576">
<div class="article-summary-box-inner">
<span><p>Although Automatic Speech Recognition (ASR) systems have achieved human-like
performance for a few languages, the majority of the world's languages do not
have usable systems due to the lack of large speech datasets to train these
models. Cross-lingual transfer is an attractive solution to this problem,
because low-resource languages can potentially benefit from higher-resource
languages either through transfer learning, or being jointly trained in the
same multilingual model. The problem of cross-lingual transfer has been well
studied in ASR, however, recent advances in Self Supervised Learning are
opening up avenues for unlabeled speech data to be used in multilingual ASR
models, which can pave the way for improved performance on low-resource
languages. In this paper, we survey the state of the art in multilingual ASR
models that are built with cross-lingual transfer in mind. We present best
practices for building multilingual models from research across diverse
languages and techniques, discuss open questions and provide recommendations
for future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language technology practitioners as language managers: arbitrating data bias and predictive bias in ASR. (arXiv:2202.12603v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12603">
<div class="article-summary-box-inner">
<span><p>Despite the fact that variation is a fundamental characteristic of natural
language, automatic speech recognition systems perform systematically worse on
non-standardised and marginalised language varieties. In this paper we use the
lens of language policy to analyse how current practices in training and
testing ASR systems in industry lead to the data bias giving rise to these
systematic error differences. We believe that this is a useful perspective for
speech and language technology practitioners to understand the origins and
harms of algorithmic bias, and how they can mitigate it. We also propose a
re-framing of language resources as (public) infrastructure which should not
solely be designed for markets, but for, and with meaningful cooperation of,
speech communities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">JParaCrawl v3.0: A Large-scale English-Japanese Parallel Corpus. (arXiv:2202.12607v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12607">
<div class="article-summary-box-inner">
<span><p>Most current machine translation models are mainly trained with parallel
corpora, and their translation accuracy largely depends on the quality and
quantity of the corpora. Although there are billions of parallel sentences for
a few language pairs, effectively dealing with most language pairs is difficult
due to a lack of publicly available parallel corpora. This paper creates a
large parallel corpus for English-Japanese, a language pair for which only
limited resources are available, compared to such resource-rich languages as
English-German. It introduces a new web-based English-Japanese parallel corpus
named JParaCrawl v3.0. Our new corpus contains more than 21 million unique
parallel sentence pairs, which is more than twice as many as the previous
JParaCrawl v2.0 corpus. Through experiments, we empirically show how our new
corpus boosts the accuracy of machine translation models on various domains.
The JParaCrawl v3.0 corpus will eventually be publicly available online for
research purposes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Multi-Modal Representations for Ambiguity Detection & Coreference Resolution in the SIMMC 2.0 Challenge. (arXiv:2202.12645v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12645">
<div class="article-summary-box-inner">
<span><p>Anaphoric expressions, such as pronouns and referential descriptions, are
situated with respect to the linguistic context of prior turns, as well as, the
immediate visual environment. However, a speaker's referential descriptions do
not always uniquely identify the referent, leading to ambiguities in need of
resolution through subsequent clarificational exchanges. Thus, effective
Ambiguity Detection and Coreference Resolution are key to task success in
Conversational AI. In this paper, we present models for these two tasks as part
of the SIMMC 2.0 Challenge (Kottur et al. 2021). Specifically, we use TOD-BERT
and LXMERT based models, compare them to a number of baselines and provide
ablation experiments. Our results show that (1) language models are able to
exploit correlations in the data to detect ambiguity; and (2) unimodal
coreference resolution models can avoid the need for a vision component,
through the use of smart object representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning, Natural Language Processing, and Explainable Artificial Intelligence in the Biomedical Domain. (arXiv:2202.12678v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12678">
<div class="article-summary-box-inner">
<span><p>In this article, we first give an introduction to artificial intelligence and
its applications in biology and medicine in Section 1. Deep learning methods
are then described in Section 2. We narrow down the focus of the study on
textual data in Section 3, where natural language processing and its
applications in the biomedical domain are described. In Section 4, we give an
introduction to explainable artificial intelligence and discuss the importance
of explainability of artificial intelligence systems, especially in the
biomedical domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ask2Mask: Guided Data Selection for Masked Speech Modeling. (arXiv:2202.12719v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12719">
<div class="article-summary-box-inner">
<span><p>Masked speech modeling (MSM) methods such as wav2vec2 or w2v-BERT learn
representations over speech frames which are randomly masked within an
utterance. While these methods improve performance of Automatic Speech
Recognition (ASR) systems, they have one major limitation. They treat all
unsupervised speech samples with equal weight, which hinders learning as not
all samples have relevant information to learn meaningful representations. In
this work, we address this limitation. We propose ask2mask (ATM), a novel
approach to focus on specific samples during MSM pre-training. ATM employs an
external ASR model or \textit{scorer} to weight unsupervised input samples in
two different ways: 1) A fine-grained data selection is performed by masking
over the highly confident input frames as chosen by the scorer. This allows the
model to learn meaningful representations. 2) ATM is further extended to focus
at utterance-level by weighting the final MSM loss with the utterance-level
confidence score. We conduct fine-tuning experiments on two well-benchmarked
corpora: LibriSpeech (matching the pre-training data) and Commonvoice,
TED-LIUM, AMI and CHiME-6 (not matching the pre-training data). The results
substantiate the efficacy of ATM on significantly improving the recognition
performance under mismatched conditions (up to 11.6\% relative over published
results and upto 4.46\% relative over our internal baseline) while still
yielding modest improvements under matched conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the data requirements of probing. (arXiv:2202.12801v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12801">
<div class="article-summary-box-inner">
<span><p>As large and powerful neural language models are developed, researchers have
been increasingly interested in developing diagnostic tools to probe them.
There are many papers with conclusions of the form "observation X is found in
model Y", using their own datasets with varying sizes. Larger probing datasets
bring more reliability, but are also expensive to collect. There is yet to be a
quantitative method for estimating reasonable probing dataset sizes. We tackle
this omission in the context of comparing two probing configurations: after we
have collected a small dataset from a pilot study, how many additional data
samples are sufficient to distinguish two different configurations? We present
a novel method to estimate the required number of data samples in such
experiments and, across several case studies, we verify that our estimations
have sufficient statistical power. Our framework helps to systematically
construct probing datasets to diagnose neural NLP models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Reality of Multi-Lingual Machine Translation. (arXiv:2202.12814v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12814">
<div class="article-summary-box-inner">
<span><p>Our book "The Reality of Multi-Lingual Machine Translation" discusses the
benefits and perils of using more than two languages in machine translation
systems. While focused on the particular task of sequence-to-sequence
processing and multi-task learning, the book targets somewhat beyond the area
of natural language processing. Machine translation is for us a prime example
of deep learning applications where human skills and learning capabilities are
taken as a benchmark that many try to match and surpass. We document that some
of the gains observed in multi-lingual translation may result from simpler
effects than the assumed cross-lingual transfer of knowledge.
</p>
<p>In the first, rather general part, the book will lead you through the
motivation for multi-linguality, the versatility of deep neural networks
especially in sequence-to-sequence tasks to complications of this learning. We
conclude the general part with warnings against too optimistic and unjustified
explanations of the gains that neural networks demonstrate.
</p>
<p>In the second part, we fully delve into multi-lingual models, with a
particularly careful examination of transfer learning as one of the more
straightforward approaches utilizing additional languages. The recent
multi-lingual techniques, including massive models, are surveyed and practical
aspects of deploying systems for many languages are discussed. The conclusion
highlights the open problem of machine understanding and reminds of two ethical
aspects of building large-scale models: the inclusivity of research and its
ecological trace.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Systematic Literature Review about Idea Mining: The Use of Machine-driven Analytics to Generate Ideas. (arXiv:2202.12826v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12826">
<div class="article-summary-box-inner">
<span><p>Idea generation is the core activity of innovation. Digital data sources,
which are sources of innovation, such as patents, publications, social media,
websites, etc., are increasingly growing at unprecedented volume. Manual idea
generation is time-consuming and is affected by the subjectivity of the
individuals involved. Therefore, the use machine-driven data analytics
techniques to analyze data to generate ideas and support idea generation by
serving users is useful. The objective of this study is to study state-of
the-art machine-driven analytics for idea generation and data sources, hence
the result of this study will generally server as a guideline for choosing
techniques and data sources. A systematic literature review is conducted to
identify relevant scholarly literature from IEEE, Scopus, Web of Science and
Google Scholar. We selected a total of 71 articles and analyzed them
thematically. The results of this study indicate that idea generation through
machine-driven analytics applies text mining, information retrieval (IR),
artificial intelligence (AI), deep learning, machine learning, statistical
techniques, natural language processing (NLP), NLP-based morphological
analysis, network analysis, and bibliometric to support idea generation. The
results include a list of techniques and procedures in idea generation through
machine-driven idea analytics. Additionally, characterization and heuristics
used in idea generation are summarized. For the future, tools designed to
generate ideas could be explored.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Morphology Without Borders: Clause-Level Morphological Annotation. (arXiv:2202.12832v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12832">
<div class="article-summary-box-inner">
<span><p>Morphological tasks use large multi-lingual datasets that organize words into
inflection tables, which then serve as training and evaluation data for various
tasks. However, a closer inspection of these data reveals profound
cross-linguistic inconsistencies, that arise from the lack of a clear
linguistic and operational definition of what is a word, and that severely
impair the universality of the derived tasks. To overcome this deficiency, we
propose to view morphology as a clause-level phenomenon, rather than
word-level. It is anchored in a fixed yet inclusive set of features homogeneous
across languages, that encapsulates all functions realized in a saturated
clause. We deliver MightyMorph, a novel dataset for clause-level morphology
covering 4 typologically-different languages: English, German, Turkish and
Hebrew. We use this dataset to derive 3 clause-level morphological tasks:
inflection, reinflection and analysis. Our experiments show that the
clause-level tasks are substantially harder than the respective word-level
tasks, while having comparable complexity across languages. Furthermore,
redefining morphology to the clause-level provides a neat interface with
contextualized language models (LMs) and can be used to probe LMs capacity to
encode complex morphology. Taken together, this work opens up new horizons in
the study of computational morphology, leaving ample space for studying neural
morphological modeling cross-linguistically.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?. (arXiv:2202.12837v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12837">
<div class="article-summary-box-inner">
<span><p>Large language models (LMs) are able to in-context learn -- perform a new
task via inference alone by conditioning on a few input-label pairs
(demonstrations) and making predictions for new inputs. However, there has been
little understanding of how the model learns and which aspects of the
demonstrations contribute to end task performance. In this paper, we show that
ground truth demonstrations are in fact not required -- randomly replacing
labels in the demonstrations barely hurts performance, consistently over 12
different models including GPT-3. Instead, we find that other aspects of the
demonstrations are the key drivers of end task performance, including the fact
that they provide a few examples of (1) the label space, (2) the distribution
of the input text, and (3) the overall format of the sequence. Together, our
analysis provides a new way of understanding how and why in-context learning
works, while opening up new questions about how much can be learned from large
language models through inference alone.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DataLab: A Platform for Data Analysis and Intervention. (arXiv:2202.12875v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12875">
<div class="article-summary-box-inner">
<span><p>Despite data's crucial role in machine learning, most existing tools and
research tend to focus on systems on top of existing data rather than how to
interpret and manipulate data. In this paper, we propose DataLab, a unified
data-oriented platform that not only allows users to interactively analyze the
characteristics of data, but also provides a standardized interface for
different data processing operations. Additionally, in view of the ongoing
proliferation of datasets, \toolname has features for dataset recommendation
and global vision analysis that help researchers form a better view of the data
ecosystem. So far, DataLab covers 1,715 datasets and 3,583 of its transformed
version (e.g., hyponyms replacement), where 728 datasets support various
analyses (e.g., with respect to gender bias) with the help of 140M samples
annotated by 318 feature functions. DataLab is under active development and
will be supported going forward. We have released a web platform, web API,
Python SDK, PyPI published package and online documentation, which hopefully,
can meet the diverse needs of researchers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Negative Sampling for Handling Missing Entity Annotations. (arXiv:2108.11607v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11607">
<div class="article-summary-box-inner">
<span><p>Negative sampling is highly effective in handling missing annotations for
named entity recognition (NER). One of our contributions is an analysis on how
it makes sense through introducing two insightful concepts: missampling and
uncertainty. Empirical studies show low missampling rate and high uncertainty
are both essential for achieving promising performances with negative sampling.
Based on the sparsity of named entities, we also theoretically derive a lower
bound for the probability of zero missampling rate, which is only relevant to
sentence length. The other contribution is an adaptive and weighted sampling
distribution that further improves negative sampling via our former analysis.
Experiments on synthetic datasets and well-annotated datasets (e.g.,
CoNLL-2003) show that our proposed approach benefits negative sampling in terms
of F1 score and loss convergence. Besides, models with improved negative
sampling have achieved new state-of-the-art results on real-world datasets
(e.g., EC).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Situated Dialogue Learning through Procedural Environment Generation. (arXiv:2110.03262v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03262">
<div class="article-summary-box-inner">
<span><p>We teach goal-driven agents to interactively act and speak in situated
environments by training on generated curriculums. Our agents operate in LIGHT
(Urbanek et al. 2019) -- a large-scale crowd-sourced fantasy text adventure
game wherein an agent perceives and interacts with the world through textual
natural language. Goals in this environment take the form of character-based
quests, consisting of personas and motivations. We augment LIGHT by learning to
procedurally generate additional novel textual worlds and quests to create a
curriculum of steadily increasing difficulty for training agents to achieve
such goals. In particular, we measure curriculum difficulty in terms of the
rarity of the quest in the original training distribution -- an easier
environment is one that is more likely to have been found in the unaugmented
dataset. An ablation study shows that this method of learning from the tail of
a distribution results in significantly higher generalization abilities as
measured by zero-shot performance on never-before-seen quests.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Virtual Augmentation Supported Contrastive Learning of Sentence Representations. (arXiv:2110.08552v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.08552">
<div class="article-summary-box-inner">
<span><p>Despite profound successes, contrastive representation learning relies on
carefully designed data augmentations using domain specific knowledge. This
challenge is magnified in natural language processing where no general rules
exist for data augmentation due to the discrete nature of natural language. We
tackle this challenge by presenting a Virtual augmentation Supported
Contrastive Learning of sentence representations (VaSCL). Originating from the
interpretation that data augmentation essentially constructs the neighborhoods
of each training instance, we in turn utilize the neighborhood to generate
effective data augmentations. Leveraging the large training batch size of
contrastive learning, we approximate the neighborhood of an instance via its
K-nearest in-batch neighbors in the representation space. We then define an
instance discrimination task regarding this neighborhood and generate the
virtual augmentation in an adversarial training manner. We access the
performance of VaSCL on a wide range of downstream tasks, and set a new
state-of-the-art for unsupervised sentence representation learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt Learning for Few-Shot Dialogue State Tracking. (arXiv:2201.05780v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.05780">
<div class="article-summary-box-inner">
<span><p>Collecting dialogue state labels, slots and values, for learning dialogue
state tracking (DST) models can be costly, especially with the wide application
of dialogue systems in new-rising domains. In this paper, we focus on how to
learn a DST model efficiently with limited labeled data. We design a prompt
learning framework for few-shot DST, which consists of two main components:
value-based prompt and inverse prompt mechanism. This framework aims to utilize
the language understanding and generation ability of pre-trained language
models (PLM). First, we design value-based prompt functions to probe the
DST-related knowledge from PLM, which do not rely on the known ontology of
slots. Further, an inverse prompt mechanism is utilized to self-check the
"prompted" knowledge and help the PLM understand the essence of DST task
further. Experiments show that our model can generate unseen slots and
outperforms existing state-of-the-art few-shot methods. It indicates that
DST-related knowledge can be probed from PLM and utilized to address
low-resource DST efficiently with the help of prompt learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Memory-assisted prompt editing to improve GPT-3 after deployment. (arXiv:2201.06009v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.06009">
<div class="article-summary-box-inner">
<span><p>Large LMs such as GPT-3, while powerful, are not immune to mistakes, but are
prohibitively costly to retrain. One failure mode is misinterpreting a user's
instruction (e.g., GPT-3 interpreting "What word is similar to good?" to mean a
homonym, while the user intended a synonym). Our goal is to allow users to
correct such errors directly through interaction -- without retraining. Our
approach pairs GPT-3 with a growing memory of cases where the model
misunderstood the user's intent and was provided with feedback, clarifying the
instruction. Given a new query, our memory-enhanced GPT-3 uses feedback from
similar, prior queries to enrich the prompt. Through simple proof-of-concept
experiments, we show how a (simulated) user can interactively teach a deployed
GPT-3, doubling its accuracy on basic lexical tasks (e.g., generate a synonym)
where users query in different, novel (often misunderstood) ways. In such
scenarios, memory helps avoid repeating similar past mistakes. Our simple idea
is a first step towards strengthening deployed models, potentially broadening
their utility. All the code and data is available at
https://github.com/madaan/memprompt.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Retriever: Learning Content-Style Representation as a Token-Level Bipartite Graph. (arXiv:2202.12307v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12307">
<div class="article-summary-box-inner">
<span><p>This paper addresses the unsupervised learning of content-style decomposed
representation. We first give a definition of style and then model the
content-style representation as a token-level bipartite graph. An unsupervised
framework, named Retriever, is proposed to learn such representations. First, a
cross-attention module is employed to retrieve permutation invariant (P.I.)
information, defined as style, from the input data. Second, a vector
quantization (VQ) module is used, together with man-induced constraints, to
produce interpretable content tokens. Last, an innovative link attention module
serves as the decoder to reconstruct data from the decomposed content and
style, with the help of the linking keys. Being modal-agnostic, the proposed
Retriever is evaluated in both speech and image domains. The state-of-the-art
zero-shot voice conversion performance confirms the disentangling ability of
our framework. Top performance is also achieved in the part discovery task for
images, verifying the interpretability of our representation. In addition, the
vivid part-based style transfer quality demonstrates the potential of Retriever
to support various fascinating generative tasks. Project page at
https://ydcustc.github.io/retriever-demo/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Time Efficient Training of Progressive Generative Adversarial Network using Depthwise Separable Convolution and Super Resolution Generative Adversarial Network. (arXiv:2202.12337v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12337">
<div class="article-summary-box-inner">
<span><p>Generative Adversarial Networks have been employed successfully to generate
high-resolution augmented images of size 1024^2. Although the augmented images
generated are unprecedented, the training time of the model is exceptionally
high. Conventional GAN requires training of both Discriminator as well as the
Generator. In Progressive GAN, which is the current state-of-the-art GAN for
image augmentation, instead of training the GAN all at once, a new concept of
progressing growing of Discriminator and Generator simultaneously, was
proposed. Although the lower stages such as 4x4 and 8x8 train rather quickly,
the later stages consume a tremendous amount of time which could take days to
finish the model training. In our paper, we propose a novel pipeline that
combines Progressive GAN with slight modifications and Super Resolution GAN.
Super Resolution GAN up samples low-resolution images to high-resolution images
which can prove to be a useful resource to reduce the training time
exponentially.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RescueNet: A High Resolution UAV Semantic Segmentation Benchmark Dataset for Natural Disaster Damage Assessment. (arXiv:2202.12361v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12361">
<div class="article-summary-box-inner">
<span><p>Due to climate change, we can observe a recent surge of natural disasters all
around the world. These disasters are causing disastrous impact on both nature
and human lives. Economic losses are getting greater due to the hurricanes.
Quick and prompt response of the rescue teams are crucial in saving human lives
and reducing economic cost. Deep learning based computer vision techniques can
help in scene understanding, and help rescue teams with precise damage
assessment. Semantic segmentation, an active research area in computer vision,
can put labels to each pixel of an image, and therefore can be a valuable
arsenal in the effort of reducing the impacts of hurricanes. Unfortunately,
available datasets for natural disaster damage assessment lack detailed
annotation of the affected areas, and therefore do not support the deep
learning models in total damage assessment. To this end, we introduce the
RescueNet, a high resolution post disaster dataset, for semantic segmentation
to assess damages after natural disasters. The RescueNet consists of post
disaster images collected after Hurricane Michael. The data is collected using
Unmanned Aerial Vehicles (UAVs) from several areas impacted by the hurricane.
The uniqueness of the RescueNet comes from the fact that this dataset provides
high resolution post-disaster images and comprehensive annotation of each
image. While most of the existing dataset offer annotation of only part of the
scene, like building, road, or river, RescueNet provides pixel level annotation
of all the classes including building, road, pool, tree, debris, and so on. We
further analyze the usefulness of the dataset by implementing state-of-the-art
segmentation models on the RescueNet. The experiments demonstrate that our
dataset can be valuable in further improvement of the existing methodologies
for natural disaster damage assessment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StyleCLIPDraw: Coupling Content and Style in Text-to-Drawing Translation. (arXiv:2202.12362v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12362">
<div class="article-summary-box-inner">
<span><p>Generating images that fit a given text description using machine learning
has improved greatly with the release of technologies such as the CLIP
image-text encoder model; however, current methods lack artistic control of the
style of image to be generated. We present an approach for generating styled
drawings for a given text description where a user can specify a desired
drawing style using a sample image. Inspired by a theory in art that style and
content are generally inseparable during the creative process, we propose a
coupled approach, known here as StyleCLIPDraw, whereby the drawing is generated
by optimizing for style and content simultaneously throughout the process as
opposed to applying style transfer after creating content in a sequence. Based
on human evaluation, the styles of images generated by StyleCLIPDraw are
strongly preferred to those by the sequential approach. Although the quality of
content generation degrades for certain styles, overall considering both
content \textit{and} style, StyleCLIPDraw is found far more preferred,
indicating the importance of style, look, and feel of machine generated images
to people as well as indicating that style is coupled in the drawing process
itself. Our code (https://github.com/pschaldenbrand/StyleCLIPDraw), a
demonstration (https://replicate.com/pschaldenbrand/style-clip-draw), and style
evaluation data
(https://www.kaggle.com/pittsburghskeet/drawings-with-style-evaluation-styleclipdraw)
are publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Instantaneous Physiological Estimation using Video Transformers. (arXiv:2202.12368v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12368">
<div class="article-summary-box-inner">
<span><p>Video-based physiological signal estimation has been limited primarily to
predicting episodic scores in windowed intervals. While these intermittent
values are useful, they provide an incomplete picture of patients'
physiological status and may lead to late detection of critical conditions. We
propose a video Transformer for estimating instantaneous heart rate and
respiration rate from face videos. Physiological signals are typically
confounded by alignment errors in space and time. To overcome this, we
formulated the loss in the frequency domain. We evaluated the method on the
large scale Vision-for-Vitals (V4V) benchmark. It outperformed both shallow and
deep learning based methods for instantaneous respiration rate estimation. In
the case of heart-rate estimation, it achieved an instantaneous-MAE of 13.0
beats-per-minute.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Monocular Depth Estimation and Uncertainty Quantification using Classification Approaches for Regression. (arXiv:2202.12369v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12369">
<div class="article-summary-box-inner">
<span><p>Monocular depth is important in many tasks, such as 3D reconstruction and
autonomous driving. Deep learning based models achieve state-of-the-art
performance in this field. A set of novel approaches for estimating monocular
depth consists of transforming the regression task into a classification one.
However, there is a lack of detailed descriptions and comparisons for
Classification Approaches for Regression (CAR) in the community and no in-depth
exploration of their potential for uncertainty estimation. To this end, this
paper will introduce a taxonomy and summary of CAR approaches, a new
uncertainty estimation solution for CAR, and a set of experiments on depth
accuracy and uncertainty quantification for CAR-based models on KITTI dataset.
The experiments reflect the differences in the portability of various CAR
methods on two backbones. Meanwhile, the newly proposed method for uncertainty
estimation can outperform the ensembling method with only one forward
propagation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Highly-Efficient Binary Neural Networks for Visual Place Recognition. (arXiv:2202.12375v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12375">
<div class="article-summary-box-inner">
<span><p>VPR is a fundamental task for autonomous navigation as it enables a robot to
localize itself in the workspace when a known location is detected. Although
accuracy is an essential requirement for a VPR technique, computational and
energy efficiency are not less important for real-world applications. CNN-based
techniques archive state-of-the-art VPR performance but are computationally
intensive and energy demanding. Binary neural networks (BNN) have been recently
proposed to address VPR efficiently. Although a typical BNN is an order of
magnitude more efficient than a CNN, its processing time and energy usage can
be further improved. In a typical BNN, the first convolution is not completely
binarized for the sake of accuracy. Consequently, the first layer is the
slowest network stage, requiring a large share of the entire computational
effort. This paper presents a class of BNNs for VPR that combines depthwise
separable factorization and binarization to replace the first convolutional
layer to improve computational and energy efficiency. Our best model achieves
state-of-the-art VPR performance while spending considerably less time and
energy to process an image than a BNN using a non-binary convolution as a first
stage.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TwistSLAM: Constrained SLAM in Dynamic Environment. (arXiv:2202.12384v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12384">
<div class="article-summary-box-inner">
<span><p>Moving objects are present in most scenes of our life. However they can be
very problematic for classical SLAM algorithms that assume the scene to be
rigid. This assumption limits the applicability of those algorithms as they are
unable to accurately estimate the camera pose and world structure in many
scenarios. Some SLAM systems have been proposed to detect and mask out dynamic
objects, making the static scene assumption valid. However this information can
allow the system to track objects within the scene, while tracking the camera,
which can be crucial for some applications. In this paper we present TwistSLAM
a semantic, dynamic, stereo SLAM system that can track dynamic objects in the
scene. Our algorithm creates clusters of points according to their semantic
class. It uses the static parts of the environment to robustly localize the
camera and tracks the remaining objects. We propose a new formulation for the
tracking and the bundle adjustment to take in account the characteristics of
mechanical joints between clusters to constrain and improve their pose
estimation. We evaluate our approach on several sequences from a public dataset
and show that we improve camera and object tracking compared to state of the
art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Provable Stochastic Optimization for Global Contrastive Learning: Small Batch Does Not Harm Performance. (arXiv:2202.12387v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12387">
<div class="article-summary-box-inner">
<span><p>In this paper, we study contrastive learning from an optimization
perspective, aiming to analyze and address a fundamental issue of existing
contrastive learning methods that either rely on a large batch size or a large
dictionary. We consider a global objective for contrastive learning, which
contrasts each positive pair with all negative pairs for an anchor point. From
the optimization perspective, we explain why existing methods such as SimCLR
requires a large batch size in order to achieve a satisfactory result. In order
to remove such requirement, we propose a memory-efficient Stochastic
Optimization algorithm for solving the Global objective of Contrastive Learning
of Representations, named SogCLR. We show that its optimization error is
negligible under a reasonable condition after a sufficient number of iterations
or is diminishing for a slightly different global contrastive objective.
Empirically, we demonstrate that on ImageNet with a batch size 256, SogCLR
achieves a performance of 69.4% for top-1 linear evaluation accuracy using
ResNet-50, which is on par with SimCLR (69.3%) with a large batch size 8,192.
We also attempt to show that the proposed optimization technique is generic and
can be applied to solving other contrastive losses, e.g., two-way contrastive
losses for bimodal contrastive learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Transferable Reward for Query Object Localization with Policy Adaptation. (arXiv:2202.12403v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12403">
<div class="article-summary-box-inner">
<span><p>We propose a reinforcement learning based approach to \emph{query object
localization}, for which an agent is trained to localize objects of interest
specified by a small exemplary set. We learn a transferable reward signal
formulated using the exemplary set by ordinal metric learning. Our proposed
method enables test-time policy adaptation to new environments where the reward
signals are not readily available, and outperforms fine-tuning approaches that
are limited to annotated images. In addition, the transferable reward allows
repurposing the trained agent from one specific class to another class.
Experiments on corrupted MNIST, CU-Birds, and COCO datasets demonstrate the
effectiveness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fourier-Based Augmentations for Improved Robustness and Uncertainty Calibration. (arXiv:2202.12412v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12412">
<div class="article-summary-box-inner">
<span><p>Diverse data augmentation strategies are a natural approach to improving
robustness in computer vision models against unforeseen shifts in data
distribution. However, the ability to tailor such strategies to inoculate a
model against specific classes of corruptions or attacks -- without incurring
substantial losses in robustness against other classes of corruptions --
remains elusive. In this work, we successfully harden a model against
Fourier-based attacks, while producing superior-to-AugMix accuracy and
calibration results on both the CIFAR-10-C and CIFAR-100-C datasets;
classification error is reduced by over ten percentage points for some
high-severity noise and digital-type corruptions. We achieve this by
incorporating Fourier-basis perturbations in the AugMix image-augmentation
framework. Thus we demonstrate that the AugMix framework can be tailored to
effectively target particular distribution shifts, while boosting overall model
robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimal channel selection with discrete QCQP. (arXiv:2202.12417v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12417">
<div class="article-summary-box-inner">
<span><p>Reducing the high computational cost of large convolutional neural networks
is crucial when deploying the networks to resource-constrained environments. We
first show the greedy approach of recent channel pruning methods ignores the
inherent quadratic coupling between channels in the neighboring layers and
cannot safely remove inactive weights during the pruning procedure.
Furthermore, due to these inactive weights, the greedy methods cannot guarantee
to satisfy the given resource constraints and deviate with the true objective.
In this regard, we propose a novel channel selection method that optimally
selects channels via discrete QCQP, which provably prevents any inactive
weights and guarantees to meet the resource constraints tightly in terms of
FLOPs, memory usage, and network size. We also propose a quadratic model that
accurately estimates the actual inference time of the pruned network, which
allows us to adopt inference time as a resource constraint option. Furthermore,
we generalize our method to extend the selection granularity beyond channels
and handle non-sequential connections. Our experiments on CIFAR-10 and ImageNet
show our proposed pruning method outperforms other fixed-importance channel
pruning methods on various network architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing Human Observer Ability in Morphing Attack Detection -- Where Do We Stand?. (arXiv:2202.12426v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12426">
<div class="article-summary-box-inner">
<span><p>While several works have studied the vulnerability of automated FRS and have
proposed morphing attack detection (MAD) methods, very few have focused on
studying the human ability to detect morphing attacks. The examiner/observer's
face morph detection ability is based on their observation, domain knowledge,
experience, and familiarity with the problem, and no works report the detailed
findings from observers who check identity documents as a part of their
everyday professional life. This work creates a new benchmark database of
realistic morphing attacks from 48 unique subjects leading to 400 morphed
images presented to the observers in a Differential-MAD (D-MAD) setting. Unlike
the existing databases, the newly created morphed image database has been
created with careful considerations to age, gender and ethnicity to create
realistic morph attacks. Further, unlike the previous works, we also capture
ten images from Automated Border Control (ABC) gates to mimic the realistic
D-MAD setting leading to 400 probe images in border crossing scenarios. The
newly created dataset is further used to study the ability of human observers'
ability to detect morphed images. In addition, a new dataset of 180 morphed
images is also created using the FRGCv2 dataset under the Single Image-MAD
(S-MAD) setting. Further, to benchmark the human ability in detecting morphs, a
new evaluation platform is created to conduct S-MAD and D-MAD analysis. The
benchmark study employs 469 observers for D-MAD and 410 observers for S-MAD who
are primarily governmental employees from more than 40 countries. The analysis
provides interesting insights and points to expert observers' missing
competence and failure to detect a considerable amount of morphing attacks.
Human observers tend to detect morphed images to a lower accuracy as compared
to the automated MAD algorithms evaluated in this work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Video Segmentation Models with Per-frame Inference. (arXiv:2202.12427v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12427">
<div class="article-summary-box-inner">
<span><p>Most existing real-time deep models trained with each frame independently may
produce inconsistent results across the temporal axis when tested on a video
sequence. A few methods take the correlations in the video sequence into
account,e.g., by propagating the results to the neighboring frames using
optical flow or extracting frame representations using multi-frame information,
which may lead to inaccurate results or unbalanced latency. In this work, we
focus on improving the temporal consistency without introducing computation
overhead in inference. To this end, we perform inference at each frame.
Temporal consistency is achieved by learning from video frames with extra
constraints during the training phase. introduced for inference. We propose
several techniques to learn from the video sequence, including a temporal
consistency loss and online/offline knowledge distillation methods. On the task
of semantic video segmentation, weighing among accuracy, temporal smoothness,
and efficiency, our proposed method outperforms keyframe-based methods and a
few baseline methods that are trained with each frame independently, on
datasets including Cityscapes, Camvid, and 300VW-Mask. We further apply our
training method to video instance segmentation on YouTubeVISand develop an
application of portrait matting in video sequences, by segmenting temporally
consistent instance-level trimaps across frames. Experiments show superior
qualitative and quantitative results. Code is available at:
https://git.io/vidseg.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Adversarial Robustness from Feature Maps of Convolutional Layers. (arXiv:2202.12435v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12435">
<div class="article-summary-box-inner">
<span><p>The adversarial robustness of a neural network mainly relies on two factors,
one is the feature representation capacity of the network, and the other is its
resistance ability to perturbations. In this paper, we study the
anti-perturbation ability of the network from the feature maps of convolutional
layers. Our theoretical analysis discovers that larger convolutional features
before average pooling can contribute to better resistance to perturbations,
but the conclusion is not true for max pooling. Based on the theoretical
findings, we present two feasible ways to improve the robustness of existing
neural networks. The proposed approaches are very simple and only require
upsampling the inputs or modifying the stride configuration of convolution
operators. We test our approaches on several benchmark neural network
architectures, including AlexNet, VGG16, RestNet18 and PreActResNet18, and
achieve non-trivial improvements on both natural accuracy and robustness under
various attacks. Our study brings new insights into the design of robust neural
networks. The code is available at \url{https://github.com/MTandHJ/rcm}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structure-aware Unsupervised Tagged-to-Cine MRI Synthesis with Self Disentanglement. (arXiv:2202.12474v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12474">
<div class="article-summary-box-inner">
<span><p>Cycle reconstruction regularized adversarial training -- e.g., CycleGAN,
DiscoGAN, and DualGAN -- has been widely used for image style transfer with
unpaired training data. Several recent works, however, have shown that local
distortions are frequent, and structural consistency cannot be guaranteed.
Targeting this issue, prior works usually relied on additional segmentation or
consistent feature extraction steps that are task-specific. To counter this,
this work aims to learn a general add-on structural feature extractor, by
explicitly enforcing the structural alignment between an input and its
synthesized image. Specifically, we propose a novel input-output image patches
self-training scheme to achieve a disentanglement of underlying anatomical
structures and imaging modalities. The translator and structure encoder are
updated, following an alternating training protocol. In addition, the
information w.r.t. imaging modality can be eliminated with an asymmetric
adversarial game. We train, validate, and test our network on 1,768, 416, and
1,560 unpaired subject-independent slices of tagged and cine magnetic resonance
imaging from a total of twenty healthy subjects, respectively, demonstrating
superior performance over competing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learn From the Past: Experience Ensemble Knowledge Distillation. (arXiv:2202.12488v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12488">
<div class="article-summary-box-inner">
<span><p>Traditional knowledge distillation transfers "dark knowledge" of a
pre-trained teacher network to a student network, and ignores the knowledge in
the training process of the teacher, which we call teacher's experience.
However, in realistic educational scenarios, learning experience is often more
important than learning results. In this work, we propose a novel knowledge
distillation method by integrating the teacher's experience for knowledge
transfer, named experience ensemble knowledge distillation (EEKD). We save a
moderate number of intermediate models from the training process of the teacher
model uniformly, and then integrate the knowledge of these intermediate models
by ensemble technique. A self-attention module is used to adaptively assign
weights to different intermediate models in the process of knowledge transfer.
Three principles of constructing EEKD on the quality, weights and number of
intermediate models are explored. A surprising conclusion is found that strong
ensemble teachers do not necessarily produce strong students. The experimental
results on CIFAR-100 and ImageNet show that EEKD outperforms the mainstream
knowledge distillation methods and achieves the state-of-the-art. In
particular, EEKD even surpasses the standard ensemble distillation on the
premise of saving training cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Monogenic Wavelet Scattering Network for Texture Image Classification. (arXiv:2202.12491v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12491">
<div class="article-summary-box-inner">
<span><p>The scattering transform network (STN), which has a similar structure as that
of a popular convolutional neural network except its use of predefined
convolution filters and a small number of layers, can generates a robust
representation of an input signal relative to small deformations. We propose a
novel Monogenic Wavelet Scattering Network (MWSN) for 2D texture image
classification through a cascade of monogenic wavelet filtering with nonlinear
modulus and averaging operators by replacing the 2D Morlet wavelet filtering in
the standard STN. Our MWSN can extract useful hierarchical and directional
features with interpretable coefficients, which can be further compressed by
PCA and fed into a classifier. Using the CUReT texture image database, we
demonstrate the superior performance of our MWSN over the standard STN. This
performance improvement can be explained by the natural extension of 1D
analyticity to 2D monogenicity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Implicit Optimizer for Diffeomorphic Image Registration. (arXiv:2202.12498v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12498">
<div class="article-summary-box-inner">
<span><p>Diffeomorphic image registration is the underlying technology in medical
image processing which enables the invertibility and point-to-point
correspondence. Recently, numerous learning-based methods utilizing
convolutional neural networks (CNNs) have been proposed for registration
problems. Compared with the speed boosting, accuracy improvement brought by the
complicated CNN-based methods is minor. To tackle this problem, we propose a
rapid and accurate Implicit Optimizer for Diffeomorphic Image Registration
(IDIR) which utilizes the Deep Implicit Function as the neural velocity field
(NVF) whose input is the point coordinate p and output is velocity vector at
that point v. To reduce the huge memory consumption brought by NVF for 3D
volumes, a sparse sampling is employed to the framework. We evaluate our method
on two 3D large-scale MR brain scan datasets, the results show that our
proposed method provides faster and better registration results than
conventional image registration approaches and outperforms the learning-based
methods by a significant margin while maintaining the desired diffeomorphic
properties.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RRL:Regional Rotation Layer in Convolutional Neural Networks. (arXiv:2202.12509v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12509">
<div class="article-summary-box-inner">
<span><p>Convolutional Neural Networks (CNNs) perform very well in image
classification and object detection in recent years, but even the most advanced
models have limited rotation invariance. Known solutions include the
enhancement of training data and the increase of rotation invariance by
globally merging the rotation equivariant features. These methods either
increase the workload of training or increase the number of model parameters.
To address this problem, this paper proposes a module that can be inserted into
the existing networks, and directly incorporates the rotation invariance into
the feature extraction layers of the CNNs. This module does not have learnable
parameters and will not increase the complexity of the model. At the same time,
only by training the upright data, it can perform well on the rotated testing
set. These advantages will be suitable for fields such as biomedicine and
astronomy where it is difficult to obtain upright samples or the target has no
directionality. Evaluate our module with LeNet-5, ResNet-18 and tiny-yolov3, we
get impressive results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TeachAugment: Data Augmentation Optimization Using Teacher Knowledge. (arXiv:2202.12513v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12513">
<div class="article-summary-box-inner">
<span><p>Optimization of image transformation functions for the purpose of data
augmentation has been intensively studied. In particular, adversarial data
augmentation strategies, which search augmentation maximizing task loss, show
significant improvement in the model generalization for many tasks. However,
the existing methods require careful parameter tuning to avoid excessively
strong deformations that take away image features critical for acquiring
generalization. In this paper, we propose a data augmentation optimization
method based on the adversarial strategy called TeachAugment, which can produce
informative transformed images to the model without requiring careful tuning by
leveraging a teacher model. Specifically, the augmentation is searched so that
augmented images are adversarial for the target model and recognizable for the
teacher model. We also propose data augmentation using neural networks, which
simplifies the search space design and allows for updating of the data
augmentation using the gradient method. We show that TeachAugment outperforms
existing methods in experiments of image classification, semantic segmentation,
and unsupervised representation learning tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Faithful learning with sure data for lung nodule diagnosis. (arXiv:2202.12515v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12515">
<div class="article-summary-box-inner">
<span><p>Recent evolution in deep learning has proven its value for CT-based lung
nodule classification. Most current techniques are intrinsically black-box
systems, suffering from two generalizability issues in clinical practice.
First, benign-malignant discrimination is often assessed by human observers
without pathologic diagnoses at the nodule level. We termed these data as
"unsure data". Second, a classifier does not necessarily acquire reliable
nodule features for stable learning and robust prediction with patch-level
labels during learning. In this study, we construct a sure dataset with
pathologically-confirmed labels and propose a collaborative learning framework
to facilitate sure nodule classification by integrating unsure data knowledge
through nodule segmentation and malignancy score regression. A loss function is
designed to learn reliable features by introducing interpretability constraints
regulated with nodule segmentation maps. Furthermore, based on model inference
results that reflect the understanding from both machine and experts, we
explore a new nodule analysis method for similar historical nodule retrieval
and interpretable diagnosis. Detailed experimental results demonstrate that our
approach is beneficial for achieving improved performance coupled with faithful
model reasoning for lung cancer prediction. Extensive cross-evaluation results
further illustrate the effect of unsure data for deep-learning-based methods in
lung nodule classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Hand Gesture Detection and Recognition system based on ensemble-based Convolutional Neural Network. (arXiv:2202.12519v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12519">
<div class="article-summary-box-inner">
<span><p>Nowadays, hand gesture recognition has become an alternative for
human-machine interaction. It has covered a large area of applications like 3D
game technology, sign language interpreting, VR (virtual reality) environment,
and robotics. But detection of the hand portion has become a challenging task
in computer vision and pattern recognition communities. Deep learning algorithm
like convolutional neural network (CNN) architecture has become a very popular
choice for classification tasks, but CNN architectures suffer from some
problems like high variance during prediction, overfitting problem and also
prediction errors. To overcome these problems, an ensemble of CNN-based
approaches is presented in this paper. Firstly, the gesture portion is detected
by using the background separation method based on binary thresholding. After
that, the contour portion is extracted, and the hand region is segmented. Then,
the images have been resized and fed into three individual CNN models to train
them in parallel. In the last part, the output scores of CNN models are
averaged to construct an optimal ensemble model for the final prediction. Two
publicly available datasets (labeled as Dataset-1 and Dataset-2) containing
infrared images and one self-constructed dataset have been used to validate the
proposed system. Experimental results are compared with the existing
state-of-the-art approaches, and it is observed that our proposed ensemble
model outperforms other existing proposed methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved Dual Correlation Reduction Network. (arXiv:2202.12533v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12533">
<div class="article-summary-box-inner">
<span><p>Deep graph clustering, which aims to reveal the underlying graph structure
and divide the nodes into different clusters without human annotations, is a
fundamental yet challenging task. However, we observed that the existing
methods suffer from the representation collapse problem and easily tend to
encode samples with different classes into the same latent embedding.
Consequently, the discriminative capability of nodes is limited, resulting in
sub-optimal clustering performance. To address this problem, we propose a novel
deep graph clustering algorithm termed Improved Dual Correlation Reduction
Network (IDCRN) through improving the discriminative capability of samples.
Specifically, by approximating the cross-view feature correlation matrix to an
identity matrix, we reduce the redundancy between different dimensions of
features, thus improving the discriminative capability of the latent space
explicitly. Meanwhile, the cross-view sample correlation matrix is forced to
approximate the designed clustering-refined adjacency matrix to guide the
learned latent representation to recover the affinity matrix even across views,
thus enhancing the discriminative capability of features implicitly. Moreover,
we avoid the collapsed representation caused by the over-smoothing issue in
Graph Convolutional Networks (GCNs) through an introduced propagation
regularization term, enabling IDCRN to capture the long-range information with
the shallow network structure. Extensive experimental results on six benchmarks
have demonstrated the effectiveness and the efficiency of IDCRN compared to the
existing state-of-the-art deep graph clustering algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Ensemble Approach for Patient Prognosis of Head and Neck Tumor Using Multimodal Data. (arXiv:2202.12537v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12537">
<div class="article-summary-box-inner">
<span><p>Accurate prognosis of a tumor can help doctors provide a proper course of
treatment and, therefore, save the lives of many. Traditional machine learning
algorithms have been eminently useful in crafting prognostic models in the last
few decades. Recently, deep learning algorithms have shown significant
improvement when developing diagnosis and prognosis solutions to different
healthcare problems. However, most of these solutions rely solely on either
imaging or clinical data. Utilizing patient tabular data such as demographics
and patient medical history alongside imaging data in a multimodal approach to
solve a prognosis task has started to gain more interest recently and has the
potential to create more accurate solutions. The main issue when using clinical
and imaging data to train a deep learning model is to decide on how to combine
the information from these sources. We propose a multimodal network that
ensembles deep multi-task logistic regression (MTLR), Cox proportional hazard
(CoxPH) and CNN models to predict prognostic outcomes for patients with head
and neck tumors using patients' clinical and imaging (CT and PET) data.
Features from CT and PET scans are fused and then combined with patients'
electronic health records for the prediction. The proposed model is trained and
tested on 224 and 101 patient records respectively. Experimental results show
that our proposed ensemble solution achieves a C-index of 0.72 on The HECKTOR
test set that saved us the first place in prognosis task of the HECKTOR
challenge. The full implementation based on PyTorch is available on
\url{https://github.com/numanai/BioMedIA-Hecktor2021}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">6D Rotation Representation For Unconstrained Head Pose Estimation. (arXiv:2202.12555v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12555">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a method for unconstrained end-to-end head pose
estimation. We address the problem of ambiguous rotation labels by introducing
the rotation matrix formalism for our ground truth data and propose a
continuous 6D rotation matrix representation for efficient and robust direct
regression. This way, our method can learn the full rotation appearance which
is contrary to previous approaches that restrict the pose prediction to a
narrow-angle for satisfactory results. In addition, we propose a geodesic
distance-based loss to penalize our network with respect to the SO(3) manifold
geometry. Experiments on the public AFLW2000 and BIWI datasets demonstrate that
our proposed method significantly outperforms other state-of-the-art methods by
up to 20\%. We open-source our training and testing code along with our
pre-trained models: https://github.com/thohemp/6DRepNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An exploration of the performances achievable by combining unsupervised background subtraction algorithms. (arXiv:2202.12563v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12563">
<div class="article-summary-box-inner">
<span><p>Background subtraction (BGS) is a common choice for performing motion
detection in video. Hundreds of BGS algorithms are released every year, but
combining them to detect motion remains largely unexplored. We found that
combination strategies allow to capitalize on this massive amount of available
BGS algorithms, and offer significant space for performance improvement. In
this paper, we explore sets of performances achievable by 6 strategies
combining, pixelwise, the outputs of 26 unsupervised BGS algorithms, on the
CDnet 2014 dataset, both in the ROC space and in terms of the F1 score. The
chosen strategies are representative for a large panel of strategies, including
both deterministic and non-deterministic ones, voting and learning. In our
experiments, we compare our results with the state-of-the-art combinations
IUTIS-5 and CNN-SFC, and report six conclusions, among which the existence of
an important gap between the performances of the individual algorithms and the
best performances achievable by combining them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Local Intensity Order Transformation for Robust Curvilinear Object Segmentation. (arXiv:2202.12587v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12587">
<div class="article-summary-box-inner">
<span><p>Segmentation of curvilinear structures is important in many applications,
such as retinal blood vessel segmentation for early detection of vessel
diseases and pavement crack segmentation for road condition evaluation and
maintenance. Currently, deep learning-based methods have achieved impressive
performance on these tasks. Yet, most of them mainly focus on finding powerful
deep architectures but ignore capturing the inherent curvilinear structure
feature (e.g., the curvilinear structure is darker than the context) for a more
robust representation. In consequence, the performance usually drops a lot on
cross-datasets, which poses great challenges in practice. In this paper, we aim
to improve the generalizability by introducing a novel local intensity order
transformation (LIOT). Specifically, we transfer a gray-scale image into a
contrast-invariant four-channel image based on the intensity order between each
pixel and its nearby pixels along with the four (horizontal and vertical)
directions. This results in a representation that preserves the inherent
characteristic of the curvilinear structure while being robust to contrast
changes. Cross-dataset evaluation on three retinal blood vessel segmentation
datasets demonstrates that LIOT improves the generalizability of some
state-of-the-art methods. Additionally, the cross-dataset evaluation between
retinal blood vessel segmentation and pavement crack segmentation shows that
LIOT is able to preserve the inherent characteristic of curvilinear structure
with large appearance gaps. An implementation of the proposed method is
available at https://github.com/TY-Shi/LIOT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active Learning for Point Cloud Semantic Segmentation via Spatial-Structural Diversity Reasoning. (arXiv:2202.12588v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12588">
<div class="article-summary-box-inner">
<span><p>The expensive annotation cost is notoriously known as a main constraint for
the development of the point cloud semantic segmentation technique. In this
paper, we propose a novel active learning-based method to tackle this problem.
Dubbed SSDR-AL, our method groups the original point clouds into superpoints
and selects the most informative and representative ones for label acquisition.
We achieve the selection mechanism via a graph reasoning network that considers
both the spatial and structural diversity of the superpoints. To deploy SSDR-AL
in a more practical scenario, we design a noise aware iterative labeling scheme
to confront the "noisy annotation" problem introduced by previous dominant
labeling methods in superpoints. Extensive experiments on two point cloud
benchmarks demonstrate the effectiveness of SSDR-AL in the semantic
segmentation task. Particularly, SSDR-AL significantly outperforms the baseline
method when the labeled sets are small, where SSDR-AL requires only $5.7\%$ and
$1.9\%$ annotation costs to achieve the performance of $90\%$ fully supervised
learning on S3DIS and Semantic3D datasets, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LF-VIO: A Visual-Inertial-Odometry Framework for Large Field-of-View Cameras with Negative Plane. (arXiv:2202.12613v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12613">
<div class="article-summary-box-inner">
<span><p>Visual-inertial-odometry has attracted extensive attention in the field of
autonomous driving and robotics. The size of Field of View (FoV) plays an
important role in Visual-Odometry (VO) and Visual-Inertial-Odometry (VIO), as a
large FoV enables to perceive a wide range of surrounding scene elements and
features. However, when the field of the camera reaches the negative half
plane, one cannot simply use [u,v,1]^T to represent the image feature points
anymore. To tackle this issue, we propose LF-VIO, a real-time VIO framework for
cameras with extremely large FoV. We leverage a three-dimensional vector with
unit length to represent feature points, and design a series of algorithms to
overcome this challenge. To address the scarcity of panoramic visual odometry
datasets with ground-truth location and pose, we present the PALVIO dataset,
collected with a Panoramic Annular Lens (PAL) system with an entire FoV of
360x(40-120) degrees and an IMU sensor. With a comprehensive variety of
experiments, the proposed LF-VIO is verified on both the established PALVIO
benchmark and a public fisheye camera dataset with a FoV of 360x(0-93.5)
degrees. LF-VIO outperforms state-of-the-art visual-inertial-odometry methods.
Our dataset and code are made publicly available at
https://github.com/flysoaryun/LF-VIO
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Answering and Explanation for Visual Commonsense Reasoning. (arXiv:2202.12626v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12626">
<div class="article-summary-box-inner">
<span><p>Visual Commonsense Reasoning (VCR), deemed as one challenging extension of
the Visual Question Answering (VQA), endeavors to pursue a more high-level
visual comprehension. It is composed of two indispensable processes: question
answering over a given image and rationale inference for answer explanation.
Over the years, a variety of methods tackling VCR have advanced the performance
on the benchmark dataset. Despite significant as these methods are, they often
treat the two processes in a separate manner and hence decompose the VCR into
two irrelevant VQA instances. As a result, the pivotal connection between
question answering and rationale inference is interrupted, rendering existing
efforts less faithful on visual reasoning. To empirically study this issue, we
perform some in-depth explorations in terms of both language shortcuts and
generalization capability to verify the pitfalls of this treatment. Based on
our findings, in this paper, we present a plug-and-play knowledge distillation
enhanced framework to couple the question answering and rationale inference
processes. The key contribution is the introduction of a novel branch, which
serves as the bridge to conduct processes connecting. Given that our framework
is model-agnostic, we apply it to the existing popular baselines and validate
its effectiveness on the benchmark dataset. As detailed in the experimental
results, when equipped with our framework, these baselines achieve consistent
and significant performance improvements, demonstrating the viability of
processes coupling, as well as the superiority of the proposed framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting 4D Liver MRI for MR-guided Interventions. (arXiv:2202.12628v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12628">
<div class="article-summary-box-inner">
<span><p>Organ motion poses an unresolved challenge in image-guided interventions. In
the pursuit of solving this problem, the research field of time-resolved
volumetric magnetic resonance imaging (4D MRI) has evolved. However, current
techniques are unsuitable for most interventional settings because they lack
sufficient temporal and/or spatial resolution or have long acquisition times.
In this work, we propose a novel approach for real-time, high-resolution 4D MRI
with large fields of view for MR-guided interventions. To this end, we trained
a convolutional neural network (CNN) end-to-end to predict a 3D liver MRI that
correctly predicts the liver's respiratory state from a live 2D navigator MRI
of a subject. Our method can be used in two ways: First, it can reconstruct
near real-time 4D MRI with high quality and high resolution (209x128x128 matrix
size with isotropic 1.8mm voxel size and 0.6s/volume) given a dynamic
interventional 2D navigator slice for guidance during an intervention. Second,
it can be used for retrospective 4D reconstruction with a temporal resolution
of below 0.2s/volume for motion analysis and use in radiation therapy. We
report a mean target registration error (TRE) of 1.19 $\pm$0.74mm, which is
below voxel size. We compare our results with a state-of-the-art retrospective
4D MRI reconstruction. Visual evaluation shows comparable quality. We show that
small training sizes with short acquisition times down to 2min can already
achieve promising results and 24min are sufficient for high quality results.
Because our method can be readily combined with earlier methods, acquisition
time can be further decreased while also limiting quality loss. We show that an
end-to-end, deep learning formulation is highly promising for 4D MRI
reconstruction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Dirichlet uncertainty for unsupervised out-of-distribution detection of eye fundus photographs in glaucoma screening. (arXiv:2202.12634v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12634">
<div class="article-summary-box-inner">
<span><p>The development of automatic tools for early glaucoma diagnosis with color
fundus photographs can significantly reduce the impact of this disease.
However, current state-of-the-art solutions are not robust to real-world
scenarios, providing over-confident predictions for out-of-distribution cases.
With this in mind, we propose a model based on the Dirichlet distribution that
allows to obtain class-wise probabilities together with an uncertainty
estimation without exposure to out-of-distribution cases. We demonstrate our
approach on the AIROGS challenge. At the start of the final test phase (8 Feb.
2022), our method had the highest average score among all submissions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Amharic Handwritten Word Recognition Using Auxiliary Task. (arXiv:2202.12687v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12687">
<div class="article-summary-box-inner">
<span><p>Amharic is one of the official languages of the Federal Democratic Republic
of Ethiopia. It is one of the languages that use an Ethiopic script which is
derived from Gee'z, ancient and currently a liturgical language. Amharic is
also one of the most widely used literature-rich languages of Ethiopia. There
are very limited innovative and customized research works in Amharic optical
character recognition (OCR) in general and Amharic handwritten text recognition
in particular. In this study, Amharic handwritten word recognition will be
investigated. State-of-the-art deep learning techniques including convolutional
neural networks together with recurrent neural networks and connectionist
temporal classification (CTC) loss were used to make the recognition in an
end-to-end fashion. More importantly, an innovative way of complementing the
loss function using the auxiliary task from the row-wise similarities of the
Amharic alphabet was tested to show a significant recognition improvement over
a baseline method. Such findings will promote innovative problem-specific
solutions as well as will open insight to a generalized solution that emerges
from problem-specific domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Modality Bias Recognition and Reduction. (arXiv:2202.12690v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12690">
<div class="article-summary-box-inner">
<span><p>Making each modality in multi-modal data contribute is of vital importance to
learning a versatile multi-modal model. Existing methods, however, are often
dominated by one or few of modalities during model training, resulting in
sub-optimal performance. In this paper, we refer to this problem as modality
bias and attempt to study it in the context of multi-modal classification
systematically and comprehensively. After stepping into several empirical
analysis, we recognize that one modality affects the model prediction more just
because this modality has a spurious correlation with instance labels. In order
to primarily facilitate the evaluation on the modality bias problem, we
construct two datasets respectively for the colored digit recognition and video
action recognition tasks in line with the Out-of-Distribution (OoD) protocol.
Collaborating with the benchmarks in the visual question answering task, we
empirically justify the performance degradation of the existing methods on
these OoD datasets, which serves as evidence to justify the modality bias
learning. In addition, to overcome this problem, we propose a plug-and-play
loss function method, whereby the feature space for each label is adaptively
learned according to the training set statistics. Thereafter, we apply this
method on eight baselines in total to test its effectiveness. From the results
on four datasets regarding the above three tasks, our method yields remarkable
performance improvements compared with the baselines, demonstrating its
superiority on reducing the modality bias problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reconstruction of Perceived Images from fMRI Patterns and Semantic Brain Exploration using Instance-Conditioned GANs. (arXiv:2202.12692v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12692">
<div class="article-summary-box-inner">
<span><p>Reconstructing perceived natural images from fMRI signals is one of the most
engaging topics of neural decoding research. Prior studies had success in
reconstructing either the low-level image features or the semantic/high-level
aspects, but rarely both. In this study, we utilized an Instance-Conditioned
GAN (IC-GAN) model to reconstruct images from fMRI patterns with both accurate
semantic attributes and preserved low-level details. The IC-GAN model takes as
input a 119-dim noise vector and a 2048-dim instance feature vector extracted
from a target image via a self-supervised learning model (SwAV ResNet-50);
these instance features act as a conditioning for IC-GAN image generation,
while the noise vector introduces variability between samples. We trained ridge
regression models to predict instance features, noise vectors, and dense
vectors (the output of the first dense layer of the IC-GAN generator) of
stimuli from corresponding fMRI patterns. Then, we used the IC-GAN generator to
reconstruct novel test images based on these fMRI-predicted variables. The
generated images presented state-of-the-art results in terms of capturing the
semantic attributes of the original test images while remaining relatively
faithful to low-level image details. Finally, we use the learned regression
model and the IC-GAN generator to systematically explore and visualize the
semantic features that maximally drive each of several regions-of-interest in
the human brain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Online handwriting, signature and touch dynamics: tasks and potential applications in the field of security and health. (arXiv:2202.12693v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12693">
<div class="article-summary-box-inner">
<span><p>Background: An advantageous property of behavioural signals ,e.g.
handwriting, in contrast to morphological ones, such as iris, fingerprint, hand
geometry, etc., is the possibility to ask a user for a very rich amount of
different tasks. Methods: This article summarises recent findings and
applications of different handwriting and drawing tasks in the field of
security and health. More specifically, it is focused on on-line handwriting
and hand-based interaction, i.e. signals that utilise a digitizing device
(specific devoted or general-purpose tablet/smartphone) during the realization
of the tasks. Such devices permit the acquisition of on-surface dynamics as
well as in-air movements in time, thus providing complex and richer information
when compared to the conventional pen and paper method. Conclusions: Although
the scientific literature reports a wide range of tasks and applications, in
this paper, we summarize only those providing competitive results (e.g. in
terms of discrimination power) and having a significant impact in the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The effect of fatigue on the performance of online writer recognition. (arXiv:2202.12694v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12694">
<div class="article-summary-box-inner">
<span><p>Background: The performance of biometric modalities based on things done by
the subject, like signature and text-based recognition, may be affected by the
subject state. Fatigue is one of the conditions that can significantly affect
the outcome of handwriting tasks. Recent research has already shown that
physical fatigue produces measurable differences in some features extracted
from common writing and drawing tasks. It is important to establish to which
extent physical fatigue contributes to the intra-person variability observed in
these biometric modalities and also to know whether the performance of
recognition methods is affected by fatigue. Goal: In this paper we assess the
impact of fatigue on intra-user variability and on the performance of
signature-based and text-based writer recognition approaches encompassing both
identification and verification. Methods: Several signature and text
recognition methods are considered and applied to samples gathered after
different levels of induced fatigue, measured by metabolic and mechanical
assessment and, also by subjective perception. The recognition methods are
Dynamic Time Warping and Multi Section Vector Quantization, for signatures, and
Allographic Text-Dependent Recognition for text in capital letters. For each
fatigue level, the identification and verification performance of these methods
is measured. Results: Signature shows no statistically significant intra-user
impact, but text does. On the other hand, performance of signature-based
recognition approaches is negatively impacted by fatigue whereas the impact is
not noticeable in text-based recognition, provided long enough sequences are
considered.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synthesizing Photorealistic Images with Deep Generative Learning. (arXiv:2202.12752v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12752">
<div class="article-summary-box-inner">
<span><p>The goal of this thesis is to present my research contributions towards
solving various visual synthesis and generation tasks, comprising image
translation, image completion, and completed scene decomposition. This thesis
consists of five pieces of work, each of which presents a new learning-based
approach for synthesizing images with plausible content as well as visually
realistic appearance. Each work demonstrates the superiority of the proposed
approach on image synthesis, with some further contributing to other tasks,
such as depth estimation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data refinement for fully unsupervised visual inspection using pre-trained networks. (arXiv:2202.12759v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12759">
<div class="article-summary-box-inner">
<span><p>Anomaly detection has recently seen great progress in the field of visual
inspection. More specifically, the use of classical outlier detection
techniques on features extracted by deep pre-trained neural networks have been
shown to deliver remarkable performances on the MVTec Anomaly Detection (MVTec
AD) dataset. However, like most other anomaly detection strategies, these
pre-trained methods assume all training data to be normal. As a consequence,
they cannot be considered as fully unsupervised. There exists to our knowledge
no work studying these pre-trained methods under fully unsupervised setting. In
this work, we first assess the robustness of these pre-trained methods to fully
unsupervised context, using polluted training sets (i.e. containing defective
samples), and show that these methods are more robust to pollution compared to
methods such as CutPaste. We then propose SROC, a Simple Refinement strategy
for One Class classification. SROC enables to remove most of the polluted
images from the training set, and to recover some of the lost AUC. We further
show that our simple heuristic competes with, and even outperforms much more
complex strategies from the existing literature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Safe, Real-Time Systems: Stereo vs Images and LiDAR for 3D Object Detection. (arXiv:2202.12773v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12773">
<div class="article-summary-box-inner">
<span><p>As object detectors rapidly improve, attention has expanded past image-only
networks to include a range of 3D and multimodal frameworks, especially ones
that incorporate LiDAR. However, due to cost, logistics, and even some safety
considerations, stereo can be an appealing alternative. Towards understanding
the efficacy of stereo as a replacement for monocular input or LiDAR in object
detectors, we show that multimodal learning with traditional disparity
algorithms can improve image-based results without increasing the number of
parameters, and that learning over stereo error can impart similar 3D
localization power to LiDAR in certain contexts. Furthermore, doing so also has
calibration benefits with respect to image-only methods. We benchmark on the
public dataset KITTI, and in doing so, reveal a few small but common
algorithmic mistakes currently used in computing metrics on that set, and offer
efficient, provably correct alternatives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Confidence Calibration for Object Detection and Segmentation. (arXiv:2202.12785v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12785">
<div class="article-summary-box-inner">
<span><p>Calibrated confidence estimates obtained from neural networks are crucial,
particularly for safety-critical applications such as autonomous driving or
medical image diagnosis. However, although the task of confidence calibration
has been investigated on classification problems, thorough in\-ves\-tiga\-tions
on object detection and segmentation problems are still missing. Therefore, we
focus on the investigation of confidence calibration for object detection and
segmentation models in this chapter. We introduce the concept of multivariate
confidence calibration that is an extension of well-known calibration methods
to the task of object detection and segmentation. This allows for an extended
confidence calibration that is also aware of additional features such as
bounding box/pixel position, shape information, etc. Furthermore, we extend the
expected calibration error (ECE) to measure mis\-ca\-li\-bra\-tion of object
detection and segmentation models. We examine several network architectures on
MS COCO as well as on Cityscapes and show that especially object detection as
well as instance segmentation models are intrinsically miscalibrated given the
introduced definition of calibration. Using our proposed calibration methods,
we have been able to improve calibration so that it also has a positive impact
on the quality of segmentation masks as well.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sensing accident-prone features in urban scenes for proactive driving and accident prevention. (arXiv:2202.12788v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12788">
<div class="article-summary-box-inner">
<span><p>In urban cities, visual information along and on roadways is likely to
distract drivers and leads to missing traffic signs and other accident-prone
features. As a solution to avoid accidents due to missing these visual cues,
this paper proposes a visual notification of accident-prone features to
drivers, based on real-time images obtained via dashcam. For this purpose,
Google Street View images around accident hotspots (areas of dense accident
occurrence) identified by accident dataset are used to train a family of deep
convolutional neural networks (CNNs). Trained CNNs are able to detect
accident-prone features and classify a given urban scene into an accident
hotspot and a non-hotspot (area of sparse accident occurrence). For given
accident hotspot, the trained CNNs can classify it into an accident hotspot
with the accuracy up to 90%. The capability of detecting accident-prone
features by the family of CNNs is analyzed by a comparative study of four
different class activation map (CAM) methods, which are used to inspect
specific accident-prone features causing the decision of CNNs, and pixel-level
object class classification. The outputs of CAM methods are processed by an
image processing pipeline to extract only the accident-prone features that are
explainable to drivers with the help of visual notification system. To prove
the efficacy of accident-prone features, an ablation study is conducted.
Ablation of accident-prone features taking 7.7%, on average, of total area in
each image sample causes up to 13.7% more chance of given area to be classified
as a non-hotspot.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving generalization with synthetic training data for deep learning based quality inspection. (arXiv:2202.12818v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12818">
<div class="article-summary-box-inner">
<span><p>Automating quality inspection with computer vision techniques is often a very
data-demanding task. Specifically, supervised deep learning requires a large
amount of annotated images for training. In practice, collecting and annotating
such data is not only costly and laborious, but also inefficient, given the
fact that only a few instances may be available for certain defect classes. If
working with video frames can increase the number of these instances, it has a
major disadvantage: the resulting images will be highly correlated with one
another. As a consequence, models trained under such constraints are expected
to be very sensitive to input distribution changes, which may be caused in
practice by changes in the acquisition system (cameras, lights), in the parts
or in the defects aspect. In this work, we demonstrate the use of randomly
generated synthetic training images can help tackle domain instability issues,
making the trained models more robust to contextual changes. We detail both our
synthetic data generation pipeline and our deep learning methodology for
answering these questions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeuralFusion: Neural Volumetric Rendering under Human-object Interactions. (arXiv:2202.12825v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12825">
<div class="article-summary-box-inner">
<span><p>4D reconstruction and rendering of human activities is critical for immersive
VR/AR experience. Recent advances still fail to recover fine geometry and
texture results with the level of detail present in the input images from
sparse multi-view RGB cameras. In this paper, we propose NeuralHumanFVV, a
real-time neural human performance capture and rendering system to generate
both high-quality geometry and photo-realistic texture of human activities in
arbitrary novel views. We propose a neural geometry generation scheme with a
hierarchical sampling strategy for real-time implicit geometry inference, as
well as a novel neural blending scheme to generate high resolution (e.g., 1k)
and photo-realistic texture results in the novel views. Furthermore, we adopt
neural normal blending to enhance geometry details and formulate our neural
geometry and texture rendering into a multi-task learning framework. Extensive
experiments demonstrate the effectiveness of our approach to achieve
high-quality geometry and photo-realistic free view-point reconstruction for
challenging human performances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RELMOBNET: A Robust Two-Stage End-To-End Training Approach For MOBILENETV3 Based Relative Camera Pose Estimation. (arXiv:2202.12838v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12838">
<div class="article-summary-box-inner">
<span><p>Relative camera pose estimation plays a pivotal role in dealing with 3D
reconstruction and visual localization. To address this, we propose a Siamese
network based on MobileNetV3-Large for an end-to-end relative camera pose
regression independent of camera parameters. The proposed network uses pair of
images taken at different locations in the same scene to estimate the 3D
translation vector and rotation vector in unit quaternion. To increase the
generality of the model, rather than training it for a single scene, data for
four scenes are combined to train a single universal model to estimate the
relative pose. Further for independency of hyperparameter weighing between
translation and rotation loss is not used. Instead we use the novel two-stage
training procedure to learn the balance implicitly with faster convergence. We
compare the results obtained with the Cambridge Landmarks dataset, comprising
of different scenes, with existing CNN-based regression methods as baselines,
e.g., RPNet and RCPNet. The findings indicate that, when compared to RCPNet,
proposed model improves the estimation of the translation vector by a
percentage change of 16.11%, 28.88%, 52.27% on the Kings College, Old Hospital,
St Marys Church scenes from Cambridge Landmarks dataset, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ARIA: Adversarially Robust Image Attribution for Content Provenance. (arXiv:2202.12860v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12860">
<div class="article-summary-box-inner">
<span><p>Image attribution -- matching an image back to a trusted source -- is an
emerging tool in the fight against online misinformation. Deep visual
fingerprinting models have recently been explored for this purpose. However,
they are not robust to tiny input perturbations known as adversarial examples.
First we illustrate how to generate valid adversarial images that can easily
cause incorrect image attribution. Then we describe an approach to prevent
imperceptible adversarial attacks on deep visual fingerprinting models, via
robust contrastive learning. The proposed training procedure leverages training
on $\ell_\infty$-bounded adversarial examples, it is conceptually simple and
incurs only a small computational overhead. The resulting models are
substantially more robust, are accurate even on unperturbed images, and perform
well even over a database with millions of images. In particular, we achieve
91.6% standard and 85.1% adversarial recall under $\ell_\infty$-bounded
perturbations on manipulated images compared to 80.1% and 0.0% from prior work.
We also show that robustness generalizes to other types of imperceptible
perturbations unseen during training. Finally, we show how to train an
adversarially robust image comparator model for detecting editorial changes in
matched images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Identify Perceptual Bugs in 3D Video Games. (arXiv:2202.12884v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12884">
<div class="article-summary-box-inner">
<span><p>Automated Bug Detection (ABD) in video games is composed of two distinct but
complementary problems: automated game exploration and bug identification.
Automated game exploration has received much recent attention, spurred on by
developments in fields such as reinforcement learning. The complementary
problem of identifying the bugs present in a player's experience has for the
most part relied on the manual specification of rules. Although it is widely
recognised that many bugs of interest cannot be identified with such methods,
little progress has been made in this direction. In this work we show that it
is possible to identify a range of perceptual bugs using learning-based methods
by making use of only the rendered game screen as seen by the player. To
support our work, we have developed World of Bugs (WOB) an open platform for
testing ABD methods in 3D game environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ImageGCN: Multi-Relational Image Graph Convolutional Networks for Disease Identification with Chest X-rays. (arXiv:1904.00325v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1904.00325">
<div class="article-summary-box-inner">
<span><p>Image representation is a fundamental task in computer vision. However, most
of the existing approaches for image representation ignore the relations
between images and consider each input image independently. Intuitively,
relations between images can help to understand the images and maintain model
consistency over related images, leading to better explainability. In this
paper, we consider modeling the image-level relations to generate more
informative image representations, and propose ImageGCN, an end-to-end graph
convolutional network framework for inductive multi-relational image modeling.
We apply ImageGCN to chest X-ray images where rich relational information is
available for disease identification. Unlike previous image representation
models, ImageGCN learns the representation of an image using both its original
pixel features and its relationship with other images. Besides learning
informative representations for images, ImageGCN can also be used for object
detection in a weakly supervised manner. The experimental results on 3
open-source x-ray datasets, ChestX-ray14, CheXpert and MIMIC-CXR demonstrate
that ImageGCN can outperform respective baselines in both disease
identification and localization tasks and can achieve comparable and often
better results than the state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detection as Regression: Certified Object Detection by Median Smoothing. (arXiv:2007.03730v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.03730">
<div class="article-summary-box-inner">
<span><p>Despite the vulnerability of object detectors to adversarial attacks, very
few defenses are known to date. While adversarial training can improve the
empirical robustness of image classifiers, a direct extension to object
detection is very expensive. This work is motivated by recent progress on
certified classification by randomized smoothing. We start by presenting a
reduction from object detection to a regression problem. Then, to enable
certified regression, where standard mean smoothing fails, we propose median
smoothing, which is of independent interest. We obtain the first
model-agnostic, training-free, and certified defense for object detection
against $\ell_2$-bounded attacks. The code for all experiments in the paper is
available at <a href="http://github.com/Ping-C/CertifiedObjectDetection">this http URL</a> .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Patch-VQ: 'Patching Up' the Video Quality Problem. (arXiv:2011.13544v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.13544">
<div class="article-summary-box-inner">
<span><p>No-reference (NR) perceptual video quality assessment (VQA) is a complex,
unsolved, and important problem to social and streaming media applications.
Efficient and accurate video quality predictors are needed to monitor and guide
the processing of billions of shared, often imperfect, user-generated content
(UGC). Unfortunately, current NR models are limited in their prediction
capabilities on real-world, "in-the-wild" UGC video data. To advance progress
on this problem, we created the largest (by far) subjective video quality
dataset, containing 39, 000 realworld distorted videos and 117, 000 space-time
localized video patches ('v-patches'), and 5.5M human perceptual quality
annotations. Using this, we created two unique NR-VQA models: (a) a
local-to-global region-based NR VQA architecture (called PVQ) that learns to
predict global video quality and achieves state-of-the-art performance on 3 UGC
datasets, and (b) a first-of-a-kind space-time video quality mapping engine
(called PVQ Mapper) that helps localize and visualize perceptual distortions in
space and time. We will make the new database and prediction models available
immediately following the review process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fill-in-the-Blank: A Challenging Video Understanding Evaluation Framework. (arXiv:2104.04182v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.04182">
<div class="article-summary-box-inner">
<span><p>We propose fill-in-the-blanks as a video understanding evaluation framework.
The task tests a model's understanding of a video by requiring the model to
predict a masked noun phrase in the caption of the video, given the video and
the surrounding text. To this end, we introduce a novel dataset consisting of
28,000 videos and fill-in-the-blank tests with multiple correct answers. The
task and the dataset are challenging for the current state-of-the-art systems
to solve. This task also does not share the weaknesses of the current state of
the art language-informed video understanding tasks, namely: (1) video question
answering using multiple-choice questions, where models perform relatively well
because they exploit linguistic biases in the task formulation; and (2) video
captioning, which relies on an open-ended evaluation framework that is often
inaccurate because system answers may be perceived as incorrect if they differ
in form from the ground truth.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust fine-tuning of zero-shot models. (arXiv:2109.01903v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01903">
<div class="article-summary-box-inner">
<span><p>Large pre-trained models such as CLIP or ALIGN offer consistent accuracy
across a range of data distributions when performing zero-shot inference (i.e.,
without fine-tuning on a specific dataset). Although existing fine-tuning
methods substantially improve accuracy on a given target distribution, they
often reduce robustness to distribution shifts. We address this tension by
introducing a simple and effective method for improving robustness while
fine-tuning: ensembling the weights of the zero-shot and fine-tuned models
(WiSE-FT). Compared to standard fine-tuning, WiSE-FT provides large accuracy
improvements under distribution shift, while preserving high accuracy on the
target distribution. On ImageNet and five derived distribution shifts, WiSE-FT
improves accuracy under distribution shift by 4 to 6 percentage points (pp)
over prior work while increasing ImageNet accuracy by 1.6 pp. WiSE-FT achieves
similarly large robustness gains (2 to 23 pp) on a diverse set of six further
distribution shifts, and accuracy gains of 0.8 to 3.3 pp compared to standard
fine-tuning on seven commonly used transfer learning datasets. These
improvements come at no additional computational cost during fine-tuning or
inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Hilti SLAM Challenge Dataset. (arXiv:2109.11316v2 [cs.RO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11316">
<div class="article-summary-box-inner">
<span><p>Research in Simultaneous Localization and Mapping (SLAM) has made outstanding
progress over the past years. SLAM systems are nowadays transitioning from
academic to real-world applications. However, this transition has posed new
demanding challenges in terms of accuracy and robustness. To develop new SLAM
systems that are able to address these challenges, new datasets containing
cutting-edge hardware and realistic scenarios are required. We propose the
Hilti SLAM Challenge Dataset. Our dataset contains indoor sequences of offices,
labs, and construction environments and outdoor sequences of construction sites
and parking areas. All these sequences are characterized by featureless areas
and varying illumination conditions that are typical in real-world scenarios
and pose great challenges to SLAM algorithms that have been developed in
confined lab environments. Accurate ground truth, at millimeter level, is
provided for each sequence. The sensor platform used to record the data
includes a number of visual, lidar, and inertial sensors, which are spatially
and temporally calibrated. The purpose of this dataset is to foster the
research in sensor fusion to develop SLAM algorithms that can be deployed in
tasks where high accuracy and robustness are required, e.g., in construction
environments. Many academic and industrial groups tested their SLAM systems on
the proposed dataset in the Hilti SLAM Challenge. The results of the challenge,
which are summarized in this paper, show that the proposed dataset is an
important asset in the development of new SLAM algorithms that are ready to be
deployed in the real-world.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Hybrid Spatial-temporal Deep Learning Architecture for Lane Detection. (arXiv:2110.04079v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04079">
<div class="article-summary-box-inner">
<span><p>Accurate and reliable lane detection is vital for the safe performance of
lane-keeping assistance and lane departure warning systems. However, under
certain challenging circumstances, it is difficult to get satisfactory
performance in accurately detecting the lanes from one single image as mostly
done in current literature. Since lane markings are continuous lines, the lanes
that are difficult to be accurately detected in the current single image can
potentially be better deduced if information from previous frames is
incorporated. This study proposes a novel hybrid spatial-temporal (ST)
sequence-to-one deep learning architecture. This architecture makes full use of
the ST information in multiple continuous image frames to detect the lane
markings in the very last frame. Specifically, the hybrid model integrates the
following aspects: (a) the single image feature extraction module equipped with
the spatial convolutional neural network; (b) the ST feature integration module
constructed by ST recurrent neural network; (c) the encoder-decoder structure,
which makes this image segmentation problem work in an end-to-end supervised
learning format. Extensive experiments reveal that the proposed model
architecture can effectively handle challenging driving scenes and outperforms
available state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FIgLib & SmokeyNet: Dataset and Deep Learning Model for Real-Time Wildland Fire Smoke Detection. (arXiv:2112.08598v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2112.08598">
<div class="article-summary-box-inner">
<span><p>The size and frequency of wildland fires in the western United States have
dramatically increased in recent years. On high-fire-risk days, a small fire
ignition can rapidly grow and become out of control. Early detection of fire
ignitions from initial smoke can assist the response to such fires before they
become difficult to manage. Past deep learning approaches for wildfire smoke
detection have suffered from small or unreliable datasets that make it
difficult to extrapolate performance to real-world scenarios. In this work, we
present the Fire Ignition Library (FIgLib), a publicly available dataset of
nearly 25,000 labeled wildfire smoke images as seen from fixed-view cameras
deployed in Southern California. We also introduce SmokeyNet, a novel deep
learning architecture using spatiotemporal information from camera imagery for
real-time wildfire smoke detection. When trained on the FIgLib dataset,
SmokeyNet outperforms comparable baselines and rivals human performance. We
hope that the availability of the FIgLib dataset and the SmokeyNet architecture
will inspire further research into deep learning methods for wildfire smoke
detection, leading to automated notification systems that reduce the time to
wildfire response.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Filtering In Neural Implicit Functions. (arXiv:2201.13013v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2201.13013">
<div class="article-summary-box-inner">
<span><p>Neural implicit functions are highly effective for representing many kinds of
data, including images and 3D surfaces. However, the implicit functions learned
by neural networks usually include over-smoothed patches or noisy artifacts
into the results if the data has many scales of details or a wide range of
frequencies. Adapting the functions containing both noise and over-smoothed
regions may suffer from either over smoothing or noisy issues. To overcome this
challenge, we propose a new framework, coined FINN, that integrates a filtering
module into the neural network to perform data reconstruction while filtering
artifacts. The filtering module has a smoothing operator that acts on the
intermediate results of the network and a recovering operator that brings
distinct details from the input back to the regions overly smoothed. The
proposed method significantly alleviates over smoothing or noisy issues. We
demonstrate the advantage of the FINN on the tasks of image regression and
surface reconstruction and showcases significant improvement compared to
state-of-the-art methods. In addition, FINN also yields better performance in
both convergence speed and network stability. Source code is available at
https://github.com/yixin26/FINN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PARCEL: Physics-based unsupervised contrastive representation learning for parallel MR imaging. (arXiv:2202.01494v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.01494">
<div class="article-summary-box-inner">
<span><p>With the successful application of deep learning to magnetic resonance (MR)
imaging, parallel imaging techniques based on neural networks have attracted
wide attention. However, in the absence of high-quality, fully sampled datasets
for training, the performance of these methods is limited. To address this
issue, this paper proposes a Physics-bAsed unsupeRvised Contrastive
rEpresentation Learning (PARCEL) method to speed up parallel MR imaging.
Specifically, PARCEL has a parallel framework to contrastively learn two
branches of model-based unrolling networks directly from augmented undersampled
k-space data. A sophisticated co-training loss with three essential components
has been designed to guide the two networks in capturing the inherent features
and representations for MR images. And the final MR image is reconstructed with
the trained contrastive networks. PARCEL was evaluated on in vivo datasets and
compared to five state-of-the-art methods. The results show that PARCEL is able
to learn useful representations for more accurate MR reconstructions without
relying on fully sampled datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structured Prediction Problem Archive. (arXiv:2202.03574v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.03574">
<div class="article-summary-box-inner">
<span><p>Structured prediction problems are one of the fundamental tools in machine
learning. In order to facilitate algorithm development for their numerical
solution, we collect in one place a large number of datasets in easy to read
formats for a diverse set of problem classes. We provide archival links to
datasets, description of the considered problems and problem formats, and a
short summary of problem characteristics including size, number of instances
etc. For reference we also give a non-exhaustive selection of algorithms
proposed in the literature for their solution. We hope that this central
repository will make benchmarking and comparison to established works easier.
We welcome submission of interesting new datasets and algorithms for inclusion
in our archive.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task Specific Attention is one more thing you need for object detection. (arXiv:2202.09048v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09048">
<div class="article-summary-box-inner">
<span><p>Various models have been proposed to solve the object detection problem.
However, most of them require many hand-designed components to demonstrate good
performance. To mitigate these issues, Transformer based DETR and its variant
Deformable DETR were suggested. They solved much of the complex issue of
designing a head of object detection model but it has not been generally clear
that the Transformer-based models could be considered as the state-of-the-art
method in object detection without doubt. Furthermore, as DETR adapted
Transformer method only for the detection head, but still with including CNN
for the backbone body, it has not been certain that it would be possible to
build the competent end-to-end pipeline with the combination of attention
modules. In this paper, we propose that combining several attention modules
with our new Task Specific Split Transformer(TSST) is a fairly good enough
method to produce the best COCO results without traditionally hand-designed
components. By splitting generally purposed attention module into two separated
mission specific attention module, the proposed method addresses the way to
design simpler object detection models than before. Extensive experiments on
the COCO benchmark demonstrate the effectiveness of our approach. Code is
released at https://github.com/navervision/tsst
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modern Augmented Reality: Applications, Trends, and Future Directions. (arXiv:2202.09450v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.09450">
<div class="article-summary-box-inner">
<span><p>Augmented reality (AR) is one of the relatively old, yet trending areas in
the intersection of computer vision and computer graphics with numerous
applications in several areas, from gaming and entertainment, to education and
healthcare. Although it has been around for nearly fifty years, it has seen a
lot of interest by the research community in the recent years, mainly because
of the huge success of deep learning models for various computer vision and AR
applications, which made creating new generations of AR technologies possible.
This work tries to provide an overview of modern augmented reality, from both
application-level and technical perspective. We first give an overview of main
AR applications, grouped into more than ten categories. We then give an
overview of around 100 recent promising machine learning based works developed
for AR systems, such as deep learning works for AR shopping (clothing, makeup),
AR based image filters (such as Snapchat's lenses), AR animations, and more. In
the end we discuss about some of the current challenges in AR domain, and the
future directions in this area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Winning Solution to the iFLYTEK Challenge 2021 Cultivated Land Extraction from High-Resolution Remote Sensing Image. (arXiv:2202.10974v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.10974">
<div class="article-summary-box-inner">
<span><p>Extracting cultivated land accurately from high-resolution remote images is a
basic task for precision agriculture. This report introduces our solution to
the iFLYTEK challenge 2021 cultivated land extraction from high-resolution
remote sensing image. The challenge requires segmenting cultivated land objects
in very high-resolution multispectral remote sensing images. We established a
highly effective and efficient pipeline to solve this problem. We first divided
the original images into small tiles and separately performed instance
segmentation on each tile. We explored several instance segmentation algorithms
that work well on natural images and developed a set of effective methods that
are applicable to remote sensing images. Then we merged the prediction results
of all small tiles into seamless, continuous segmentation results through our
proposed overlap-tile fusion strategy. We achieved the first place among 486
teams in the challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Phrase-Based Affordance Detection via Cyclic Bilateral Interaction. (arXiv:2202.12076v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2202.12076">
<div class="article-summary-box-inner">
<span><p>Affordance detection, which refers to perceiving objects with potential
action possibilities in images, is a challenging task since the possible
affordance depends on the person's purpose in real-world application scenarios.
The existing works mainly extract the inherent human-object dependencies from
image/video to accommodate affordance properties that change dynamically. In
this paper, we explore to perceive affordance from a vision-language
perspective and consider the challenging phrase-based affordance detection
problem,i.e., given a set of phrases describing the action purposes, all the
object regions in a scene with the same affordance should be detected. To this
end, we propose a cyclic bilateral consistency enhancement network (CBCE-Net)
to align language and vision features progressively. Specifically, the
presented CBCE-Net consists of a mutual guided vision-language module that
updates the common features of vision and language in a progressive manner, and
a cyclic interaction module (CIM) that facilitates the perception of possible
interaction with objects in a cyclic manner. In addition, we extend the public
Purpose-driven Affordance Dataset (PAD) by annotating affordance categories
with short phrases. The contrastive experimental results demonstrate the
superiority of our method over nine typical methods from four relevant fields
in terms of both objective metrics and visual quality. The related code and
dataset will be released at \url{https://github.com/lulsheng/CBCE-Net}.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2022-02-28 23:08:26.072888745 UTC">2022-02-28 23:08:26 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.9</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>