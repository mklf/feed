<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-10-28T01:30:00Z">10-28</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Diachronic Text Mining Investigation of Therapeutic Candidates for COVID-19. (arXiv:2110.13971v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.13971">
<div class="article-summary-box-inner">
<span><p>Diachronic text mining has frequently been applied to long-term linguistic
surveys of word meaning and usage shifts over time. In this paper we apply
short-term diachronic text mining to a rapidly growing corpus of scientific
publications on COVID-19 captured in the CORD-19 dataset in order to identify
co-occurrences and analyze the behavior of potential candidate treatments. We
used a data set associated with a COVID-19 drug re-purposing study from Oak
Ridge National Laboratory. This study identified existing candidate coronavirus
treatments, including drugs and approved compounds, which had been analyzed and
ranked according to their potential for blocking the ability of the SARS-COV-2
virus to invade human cells. We investigated the occurrence of these candidates
in temporal instances of the CORD-19 corpus. We found that at least 25% of the
identified terms occurred in temporal instances of the corpus to the extent
that their frequency and contextual dynamics could be evaluated. We identified
three classes of behaviors: those where frequency and contextual shifts were
small and positively correlated; those where there was no correlation between
frequency and contextual changes; and those where there was a negative
correlation between frequency and contextual shift. We speculate that the
latter two patterns are indicative that a target candidate therapeutics is
undergoing active evaluation. The patterns we detected demonstrate the
potential benefits of using diachronic text mining techniques with a large
dynamic text corpus to track drug-repurposing activities across international
clinical and laboratory settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Attacks and Defenses for Social Network Text Processing Applications: Techniques, Challenges and Future Research Directions. (arXiv:2110.13980v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.13980">
<div class="article-summary-box-inner">
<span><p>The growing use of social media has led to the development of several Machine
Learning (ML) and Natural Language Processing(NLP) tools to process the
unprecedented amount of social media content to make actionable decisions.
However, these MLand NLP algorithms have been widely shown to be vulnerable to
adversarial attacks. These vulnerabilities allow adversaries to launch a
diversified set of adversarial attacks on these algorithms in different
applications of social media text processing. In this paper, we provide a
comprehensive review of the main approaches for adversarial attacks and
defenses in the context of social media applications with a particular focus on
key challenges and future research directions. In detail, we cover literature
on six key applications, namely (i) rumors detection, (ii) satires detection,
(iii) clickbait &amp; spams identification, (iv) hate speech detection,
(v)misinformation detection, and (vi) sentiment analysis. We then highlight the
concurrent and anticipated future research questions and provide
recommendations and directions for future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Connect-the-Dots: Bridging Semantics between Words and Definitions via Aligning Word Sense Inventories. (arXiv:2110.14091v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14091">
<div class="article-summary-box-inner">
<span><p>Word Sense Disambiguation (WSD) aims to automatically identify the exact
meaning of one word according to its context. Existing supervised models
struggle to make correct predictions on rare word senses due to limited
training data and can only select the best definition sentence from one
predefined word sense inventory (e.g., WordNet). To address the data sparsity
problem and generalize the model to be independent of one predefined inventory,
we propose a gloss alignment algorithm that can align definition sentences
(glosses) with the same meaning from different sense inventories to collect
rich lexical knowledge. We then train a model to identify semantic equivalence
between a target word in context and one of its glosses using these aligned
inventories, which exhibits strong transfer capability to many WSD tasks.
Experiments on benchmark datasets show that the proposed method improves
predictions on both frequent and rare word senses, outperforming prior work by
1.2% on the All-Words WSD Task and 4.3% on the Low-Shot WSD Task. Evaluation on
WiC Task also indicates that our method can better capture word meanings in
context.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training Verifiers to Solve Math Word Problems. (arXiv:2110.14168v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14168">
<div class="article-summary-box-inner">
<span><p>State-of-the-art language models can match human performance on many tasks,
but they still struggle to robustly perform multi-step mathematical reasoning.
To diagnose the failures of current models and support research, we introduce
GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math
word problems. We find that even the largest transformer models fail to achieve
high test performance, despite the conceptual simplicity of this problem
distribution. To increase performance, we propose training verifiers to judge
the correctness of model completions. At test time, we generate many candidate
solutions and select the one ranked highest by the verifier. We demonstrate
that verification significantly improves performance on GSM8K, and we provide
strong empirical evidence that verification scales more effectively with
increased data than a finetuning baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Standing on the Shoulders of Predecessors: Meta-Knowledge Transfer for Knowledge Graphs. (arXiv:2110.14170v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14170">
<div class="article-summary-box-inner">
<span><p>Knowledge graphs (KGs) have become widespread, and various knowledge graphs
are constructed incessantly to support many in-KG and out-of-KG applications.
During the construction of KGs, although new KGs may contain new entities with
respect to constructed KGs, some entity-independent knowledge can be
transferred from constructed KGs to new KGs. We call such knowledge
meta-knowledge, and refer to the problem of transferring meta-knowledge from
constructed (source) KGs to new (target) KGs to improve the performance of
tasks on target KGs as meta-knowledge transfer for knowledge graphs. However,
there is no available general framework that can tackle meta-knowledge transfer
for both in-KG and out-of-KG tasks uniformly. Therefore, in this paper, we
propose a framework, MorsE, which means conducting Meta-Learning for
Meta-Knowledge Transfer via Knowledge Graph Embedding. MorsE represents the
meta-knowledge via Knowledge Graph Embedding and learns the meta-knowledge by
Meta-Learning. Specifically, MorsE uses an entity initializer and a Graph
Neural Network (GNN) modulator to entity-independently obtain entity embeddings
given a KG and is trained following the meta-learning setting to gain the
ability of effectively obtaining embeddings. Experimental results on
meta-knowledge transfer for both in-KG and out-of-KG tasks show that MorsE is
able to learn and transfer meta-knowledge between KGs effectively, and
outperforms existing state-of-the-art models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diversity Enhanced Active Learning with Strictly Proper Scoring Rules. (arXiv:2110.14171v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14171">
<div class="article-summary-box-inner">
<span><p>We study acquisition functions for active learning (AL) for text
classification. The Expected Loss Reduction (ELR) method focuses on a Bayesian
estimate of the reduction in classification error, recently updated with Mean
Objective Cost of Uncertainty (MOCU). We convert the ELR framework to estimate
the increase in (strictly proper) scores like log probability or negative mean
square error, which we call Bayesian Estimate of Mean Proper Scores (BEMPS). We
also prove convergence results borrowing techniques used with MOCU. In order to
allow better experimentation with the new acquisition functions, we develop a
complementary batch AL algorithm, which encourages diversity in the vector of
expected changes in scores for unlabelled data. To allow high performance text
classifiers, we combine ensembling and dynamic validation set construction on
pretrained language models. Extensive experimental evaluation then explores how
these different acquisition functions perform. The results show that the use of
mean square error and log probability with BEMPS yields robust acquisition
functions, which consistently outperform the others tested.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evidential Softmax for Sparse Multimodal Distributions in Deep Generative Models. (arXiv:2110.14182v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14182">
<div class="article-summary-box-inner">
<span><p>Many applications of generative models rely on the marginalization of their
high-dimensional output probability distributions. Normalization functions that
yield sparse probability distributions can make exact marginalization more
computationally tractable. However, sparse normalization functions usually
require alternative loss functions for training since the log-likelihood is
undefined for sparse probability distributions. Furthermore, many sparse
normalization functions often collapse the multimodality of distributions. In
this work, we present $\textit{ev-softmax}$, a sparse normalization function
that preserves the multimodality of probability distributions. We derive its
properties, including its gradient in closed-form, and introduce a continuous
family of approximations to $\textit{ev-softmax}$ that have full support and
can be trained with probabilistic loss functions such as negative
log-likelihood and Kullback-Leibler divergence. We evaluate our method on a
variety of generative models, including variational autoencoders and
auto-regressive architectures. Our method outperforms existing dense and sparse
normalization techniques in distributional accuracy. We demonstrate that
$\textit{ev-softmax}$ successfully reduces the dimensionality of probability
distributions while maintaining multimodality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Syllabic Quantity Patterns as Rhythmic Features for Latin Authorship Attribution. (arXiv:2110.14203v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14203">
<div class="article-summary-box-inner">
<span><p>It is well known that, within the Latin production of written text, peculiar
metric schemes were followed not only in poetic compositions, but also in many
prose works. Such metric patterns were based on so-called syllabic quantity,
i.e., on the length of the involved syllables, and there is substantial
evidence suggesting that certain authors had a preference for certain metric
patterns over others. In this research we investigate the possibility to employ
syllabic quantity as a base for deriving rhythmic features for the task of
computational authorship attribution of Latin prose texts. We test the impact
of these features on the authorship attribution task when combined with other
topic-agnostic features. Our experiments, carried out on three different
datasets, using two different machine learning methods, show that rhythmic
features based on syllabic quantity are beneficial in discriminating among
Latin prose authors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Much Coffee Was Consumed During EMNLP 2019? Fermi Problems: A New Reasoning Challenge for AI. (arXiv:2110.14207v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14207">
<div class="article-summary-box-inner">
<span><p>Many real-world problems require the combined application of multiple
reasoning abilities employing suitable abstractions, commonsense knowledge, and
creative synthesis of problem-solving strategies. To help advance AI systems
towards such capabilities, we propose a new reasoning challenge, namely Fermi
Problems (FPs), which are questions whose answers can only be approximately
estimated because their precise computation is either impractical or
impossible. For example, "How much would the sea level rise if all ice in the
world melted?" FPs are commonly used in quizzes and interviews to bring out and
evaluate the creative reasoning abilities of humans. To do the same for AI
systems, we present two datasets: 1) A collection of 1k real-world FPs sourced
from quizzes and olympiads; and 2) a bank of 10k synthetic FPs of intermediate
complexity to serve as a sandbox for the harder real-world challenge. In
addition to question answer pairs, the datasets contain detailed solutions in
the form of an executable program and supporting facts, helping in supervision
and evaluation of intermediate steps. We demonstrate that even extensively
fine-tuned large scale language models perform poorly on these datasets, on
average making estimates that are off by two orders of magnitude. Our
contribution is thus the crystallization of several unsolved AI problems into a
single, new challenge that we hope will spur further advances in building
systems that can reason.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emoji-based Co-attention Network for Microblog Sentiment Analysis. (arXiv:2110.14227v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14227">
<div class="article-summary-box-inner">
<span><p>Emojis are widely used in online social networks to express emotions,
attitudes, and opinions. As emotional-oriented characters, emojis can be
modeled as important features of emotions towards the recipient or subject for
sentiment analysis. However, existing methods mainly take emojis as heuristic
information that fails to resolve the problem of ambiguity noise. Recent
researches have utilized emojis as an independent input to classify text
sentiment but they ignore the emotional impact of the interaction between text
and emojis. It results that the emotional semantics of emojis cannot be fully
explored. In this paper, we propose an emoji-based co-attention network that
learns the mutual emotional semantics between text and emojis on microblogs.
Our model adopts the co-attention mechanism based on bidirectional long
short-term memory incorporating the text and emojis, and integrates a
squeeze-and-excitation block in a convolutional neural network classifier to
increase its sensitivity to emotional semantic features. Experimental results
show that the proposed method can significantly outperform several baselines
for sentiment analysis on short texts of social media.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic population-based meta-learning for multi-agent communication with natural language. (arXiv:2110.14241v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14241">
<div class="article-summary-box-inner">
<span><p>In this work, our goal is to train agents that can coordinate with seen,
unseen as well as human partners in a multi-agent communication environment
involving natural language. Previous work using a single set of agents has
shown great progress in generalizing to known partners, however it struggles
when coordinating with unfamiliar agents. To mitigate that, recent work
explored the use of population-based approaches, where multiple agents interact
with each other with the goal of learning more generic protocols. These
methods, while able to result in good coordination between unseen partners,
still only achieve so in cases of simple languages, thus failing to adapt to
human partners using natural language. We attribute this to the use of static
populations and instead propose a dynamic population-based meta-learning
approach that builds such a population in an iterative manner. We perform a
holistic evaluation of our method on two different referential games, and show
that our agents outperform all prior work when communicating with seen partners
and humans. Furthermore, we analyze the natural language generation skills of
our agents, where we find that our agents also outperform strong baselines.
Finally, we test the robustness of our agents when communicating with
out-of-population agents and carefully test the importance of each component of
our method through ablation studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SQALER: Scaling Question Answering by Decoupling Multi-Hop and Logical Reasoning. (arXiv:2110.14266v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14266">
<div class="article-summary-box-inner">
<span><p>State-of-the-art approaches to reasoning and question answering over
knowledge graphs (KGs) usually scale with the number of edges and can only be
applied effectively on small instance-dependent subgraphs. In this paper, we
address this issue by showing that multi-hop and more complex logical reasoning
can be accomplished separately without losing expressive power. Motivated by
this insight, we propose an approach to multi-hop reasoning that scales
linearly with the number of relation types in the graph, which is usually
significantly smaller than the number of edges or nodes. This produces a set of
candidate solutions that can be provably refined to recover the solution to the
original problem. Our experiments on knowledge-based question answering show
that our approach solves the multi-hop MetaQA dataset, achieves a new
state-of-the-art on the more challenging WebQuestionsSP, is orders of magnitude
more scalable than competitive approaches, and can achieve compositional
generalization out of the training distribution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Learning For Prominence Detection In Children's Read Speech. (arXiv:2110.14273v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14273">
<div class="article-summary-box-inner">
<span><p>The detection of perceived prominence in speech has attracted approaches
ranging from the design of linguistic knowledge-based acoustic features to the
automatic feature learning from suprasegmental attributes such as pitch and
intensity contours. We present here, in contrast, a system that operates
directly on segmented speech waveforms to learn features relevant to prominent
word detection for children's oral fluency assessment. The chosen CRNN
(convolutional recurrent neural network) framework, incorporating both
word-level features and sequence information, is found to benefit from the
perceptually motivated SincNet filters as the first convolutional layer. We
further explore the benefits of the linguistic association between the prosodic
events of phrase boundary and prominence with different multi-task
architectures. Matching the previously reported performance on the same dataset
of a random forest ensemble predictor trained on carefully chosen hand-crafted
acoustic features, we evaluate further the possibly complementary information
from hand-crafted acoustic and pre-trained lexical features.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Linguistic Distance help Language Classification? Assessing Hawrami-Zaza and Kurmanji-Sorani. (arXiv:2110.14398v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14398">
<div class="article-summary-box-inner">
<span><p>To consider Hawrami and Zaza (Zazaki) standalone languages or dialects of a
language have been discussed and debated for a while among linguists active in
studying Iranian languages. The question of whether those languages/dialects
belong to the Kurdish language or if they are independent descendants of
Iranian languages was answered by MacKenzie (1961). However, a majority of
people who speak the dialects are against that answer. Their disapproval mainly
seems to be based on the sociological, cultural, and historical relationship
among the speakers of the dialects. While the case of Hawrami and Zaza has
remained unexplored and under-examined, an almost unanimous agreement exists
about the classification of Kurmanji and Sorani as Kurdish dialects. The
related studies to address the mentioned cases are primarily qualitative.
However, computational linguistics could approach the question from a
quantitative perspective. In this research, we look into three questions from a
linguistic distance point of view. First, how similar or dissimilar Hawrami and
Zaza are, considering no common geographical coexistence between the two.
Second, what about Kurmanji and Sorani that have geographical overlap. Finally,
what is the distance among all these dialects, pair by pair? We base our
computation on phonetic presentations of these dialects (languages), and we
calculate various linguistic distances among the pairs. We analyze the data and
discuss the results to conclude.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FacTeR-Check: Semi-automated fact-checking through Semantic Similarity and Natural Language Inference. (arXiv:2110.14532v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14532">
<div class="article-summary-box-inner">
<span><p>Our society produces and shares overwhelming amounts of information through
the Online Social Networks (OSNs). Within this environment, misinformation and
disinformation have proliferated, becoming a public safety concern on every
country. Allowing the public and professionals to efficiently find reliable
evidence about the factual veracity of a claim is crucial to mitigate this
harmful spread. To this end, we propose FacTeR-Check, a multilingual
architecture for semi-automated fact-checking that can be used for either the
general public but also useful for fact-checking organisations. FacTeR-Check
enables retrieving fact-checked information, unchecked claims verification and
tracking dangerous information over social media. This architectures involves
several modules developed to evaluate semantic similarity, to calculate natural
language inference and to retrieve information from Online Social Networks. The
union of all these modules builds a semi-automated fact-checking tool able of
verifying new claims, to extract related evidence, and to track the evolution
of a hoax on a OSN. While individual modules are validated on related
benchmarks (mainly MSTS and SICK), the complete architecture is validated using
a new dataset called NLI19-SP that is publicly released with COVID-19 related
hoaxes and tweets from Spanish social media. Our results show state-of-the-art
performance on the individual benchmarks, as well as producing useful analysis
of the evolution over time of 61 different hoaxes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IndoNLI: A Natural Language Inference Dataset for Indonesian. (arXiv:2110.14566v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14566">
<div class="article-summary-box-inner">
<span><p>We present IndoNLI, the first human-elicited NLI dataset for Indonesian. We
adapt the data collection protocol for MNLI and collect nearly 18K sentence
pairs annotated by crowd workers and experts. The expert-annotated data is used
exclusively as a test set. It is designed to provide a challenging test-bed for
Indonesian NLI by explicitly incorporating various linguistic phenomena such as
numerical reasoning, structural changes, idioms, or temporal and spatial
reasoning. Experiment results show that XLM-R outperforms other pre-trained
models in our data. The best performance on the expert-annotated data is still
far below human performance (13.4% accuracy gap), suggesting that this test set
is especially challenging. Furthermore, our analysis shows that our
expert-annotated data is more diverse and contains fewer annotation artifacts
than the crowd-annotated data. We hope this dataset can help accelerate
progress in Indonesian NLP research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GradInit: Learning to Initialize Neural Networks for Stable and Efficient Training. (arXiv:2102.08098v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.08098">
<div class="article-summary-box-inner">
<span><p>Innovations in neural architectures have fostered significant breakthroughs
in language modeling and computer vision. Unfortunately, novel architectures
often result in challenging hyper-parameter choices and training instability if
the network parameters are not properly initialized. A number of
architecture-specific initialization schemes have been proposed, but these
schemes are not always portable to new architectures. This paper presents
GradInit, an automated and architecture agnostic method for initializing neural
networks. GradInit is based on a simple heuristic; the norm of each network
layer is adjusted so that a single step of SGD or Adam with prescribed
hyperparameters results in the smallest possible loss value. This adjustment is
done by introducing a scalar multiplier variable in front of each parameter
block, and then optimizing these variables using a simple numerical scheme.
GradInit accelerates the convergence and test performance of many convolutional
architectures, both with or without skip connections, and even without
normalization layers. It also improves the stability of the original
Transformer architecture for machine translation, enabling training it without
learning rate warmup using either Adam or SGD under a wide range of learning
rates and momentum coefficients. Code is available at
https://github.com/zhuchen03/gradinit.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining. (arXiv:2102.08473v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.08473">
<div class="article-summary-box-inner">
<span><p>We present a self-supervised learning framework, COCO-LM, that pretrains
Language Models by COrrecting and COntrasting corrupted text sequences.
Following ELECTRA-style pretraining, COCO-LM employs an auxiliary language
model to corrupt text sequences, upon which it constructs two new tasks for
pretraining the main model. The first token-level task, Corrective Language
Modeling, is to detect and correct tokens replaced by the auxiliary model, in
order to better capture token-level semantics. The second sequence-level task,
Sequence Contrastive Learning, is to align text sequences originated from the
same source input while ensuring uniformity in the representation space.
Experiments on GLUE and SQuAD demonstrate that COCO-LM not only outperforms
recent state-of-the-art pretrained models in accuracy, but also improves
pretraining efficiency. It achieves the MNLI accuracy of ELECTRA with 50% of
its pretraining GPU hours. With the same pretraining steps of standard
base/large-sized models, COCO-LM outperforms the previous best models by 1+
GLUE average points.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From partners to populations: A hierarchical Bayesian account of coordination and convention. (arXiv:2104.05857v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05857">
<div class="article-summary-box-inner">
<span><p>Languages are powerful solutions to coordination problems: they provide
stable, shared expectations about how the words we say correspond to the
beliefs and intentions in our heads. Yet language use in a variable and
non-stationary social environment requires linguistic representations to be
flexible: old words acquire new ad hoc or partner-specific meanings on the fly.
In this paper, we introduce CHAI (Continual Hierarchical Adaptation through
Inference), a hierarchical Bayesian theory of coordination and convention
formation that aims to reconcile the long-standing tension between these two
basic observations. We argue that the central computational problem of
communication is not simply transmission, as in classical formulations, but
continual learning and adaptation over multiple timescales. Partner-specific
common ground quickly emerges from social inferences within dyadic
interactions, while community-wide social conventions are stable priors that
have been abstracted away from interactions with multiple partners. We present
new empirical data alongside simulations showing how our model provides a
computational foundation for several phenomena that have posed a challenge for
previous accounts: (1) the convergence to more efficient referring expressions
across repeated interaction with the same partner, (2) the gradual transfer of
partner-specific common ground to strangers, and (3) the influence of
communicative context on which conventions eventually form.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structured Reordering for Modeling Latent Alignments in Sequence Transduction. (arXiv:2106.03257v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03257">
<div class="article-summary-box-inner">
<span><p>Despite success in many domains, neural models struggle in settings where
train and test examples are drawn from different distributions. In particular,
in contrast to humans, conventional sequence-to-sequence (seq2seq) models fail
to generalize systematically, i.e., interpret sentences representing novel
combinations of concepts (e.g., text segments) seen in training. Traditional
grammar formalisms excel in such settings by implicitly encoding alignments
between input and output segments, but are hard to scale and maintain. Instead
of engineering a grammar, we directly model segment-to-segment alignments as
discrete structured latent variables within a neural seq2seq model. To
efficiently explore the large space of alignments, we introduce a reorder-first
align-later framework whose central component is a neural reordering module
producing {\it separable} permutations. We present an efficient dynamic
programming algorithm performing exact marginal inference of separable
permutations, and, thus, enabling end-to-end differentiable training of our
model. The resulting seq2seq model exhibits better systematic generalization
than standard models on synthetic problems and NLP tasks (i.e., semantic
parsing and machine translation).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Kaizen: Continuously improving teacher using Exponential Moving Average for semi-supervised speech recognition. (arXiv:2106.07759v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07759">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce the Kaizen framework that uses a continuously
improving teacher to generate pseudo-labels for semi-supervised speech
recognition (ASR). The proposed approach uses a teacher model which is updated
as the exponential moving average (EMA) of the student model parameters. We
demonstrate that it is critical for EMA to be accumulated with full-precision
floating point. The Kaizen framework can be seen as a continuous version of the
iterative pseudo-labeling approach for semi-supervised training. It is
applicable for different training criteria, and in this paper we demonstrate
its effectiveness for frame-level hybrid hidden Markov model-deep neural
network (HMM-DNN) systems as well as sequence-level Connectionist Temporal
Classification (CTC) based models.
</p>
<p>For large scale real-world unsupervised public videos in UK English and
Italian languages the proposed approach i) shows more than 10% relative word
error rate (WER) reduction over standard teacher-student training; ii) using
just 10 hours of supervised data and a large amount of unsupervised data closes
the gap to the upper-bound supervised ASR system that uses 650h or 2700h
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BARTScore: Evaluating Generated Text as Text Generation. (arXiv:2106.11520v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.11520">
<div class="article-summary-box-inner">
<span><p>A wide variety of NLP applications, such as machine translation,
summarization, and dialog, involve text generation. One major challenge for
these applications is how to evaluate whether such generated texts are actually
fluent, accurate, or effective. In this work, we conceptualize the evaluation
of generated text as a text generation problem, modeled using pre-trained
sequence-to-sequence models. The general idea is that models trained to convert
the generated text to/from a reference output or the source text will achieve
higher scores when the generated text is better. We operationalize this idea
using BART, an encoder-decoder based pre-trained model, and propose a metric
BARTScore with a number of variants that can be flexibly applied in an
unsupervised fashion to evaluation of text from different perspectives (e.g.
informativeness, fluency, or factuality). BARTScore is conceptually simple and
empirically effective. It can outperform existing top-scoring metrics in 16 of
22 test settings, covering evaluation of 16 datasets (e.g., machine
translation, text summarization) and 7 different perspectives (e.g.,
informativeness, factuality). Code to calculate BARTScore is available at
https://github.com/neulab/BARTScore, and we have released an interactive
leaderboard for meta-evaluation at
<a href="http://explainaboard.nlpedia.ai/leaderboard/task-meval/">this http URL</a> on the ExplainaBoard
platform, which allows us to interactively understand the strengths,
weaknesses, and complementarity of each metric.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active Learning for Massively Parallel Translation of Constrained Text into Low Resource Languages. (arXiv:2108.07127v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07127">
<div class="article-summary-box-inner">
<span><p>We translate a closed text that is known in advance and available in many
languages into a new and severely low resource language. Most human translation
efforts adopt a portion-based approach to translate consecutive pages/chapters
in order, which may not suit machine translation. We compare the portion-based
approach that optimizes coherence of the text locally with the random sampling
approach that increases coverage of the text globally. Our results show that
the random sampling approach performs better. When training on a seed corpus of
~1,000 lines from the Bible and testing on the rest of the Bible (~30,000
lines), random sampling gives a performance gain of +11.0 BLEU using English as
a simulated low resource language, and +4.9 BLEU using Eastern Pokomchi, a
Mayan language. Furthermore, we compare three ways of updating machine
translation models with increasing amount of human post-edited data through
iterations. We find that adding newly post-edited data to training after
vocabulary update without self-supervision performs the best. We propose an
algorithm for human and machine to work together seamlessly to translate a
closed text into a severely low resource language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Combinatorial Optimization for Word-level Adversarial Textual Attack. (arXiv:2109.02229v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02229">
<div class="article-summary-box-inner">
<span><p>Over the past few years, various word-level textual attack approaches have
been proposed to reveal the vulnerability of deep neural networks used in
natural language processing. Typically, these approaches involve an important
optimization step to determine which substitute to be used for each word in the
original input. However, current research on this step is still rather limited,
from the perspectives of both problem-understanding and problem-solving. In
this paper, we address these issues by uncovering the theoretical properties of
the problem and proposing an efficient local search algorithm (LS) to solve it.
We establish the first provable approximation guarantee on solving the problem
in general cases.Extensive experiments involving 5 NLP tasks, 8 datasets and 26
NLP models show that LS can largely reduce the number of queries usually by an
order of magnitude to achieve high attack success rates. Further experiments
show that the adversarial examples crafted by LS usually have higher quality,
exhibit better transferability, and can bring more robustness improvement to
victim models by adversarial training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LightSeq2: Accelerated Training for Transformer-based Models on GPUs. (arXiv:2110.05722v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05722">
<div class="article-summary-box-inner">
<span><p>Transformer-based models have proven to be powerful in many natural language,
computer vision, and speech recognition applications. It is expensive to train
these types of models due to unfixed input length, complex computation, and
large numbers of parameters. Existing systems either only focus on efficient
inference or optimize only BERT-like encoder models. In this paper, we present
LightSeq2, a system for efficient training of Transformer-based models on GPUs.
We propose a series of GPU optimization techniques tailored to computation flow
and memory access patterns of neural layers in Transformers. LightSeq2 supports
a variety of network architectures, including BERT (encoder-only), GPT
(decoder-only), and Transformer (encoder-decoder). Our experiments on GPUs with
varying models and datasets show that LightSeq2 is 1.4-3.5x faster than
previous systems. In particular, it gains 308% training speedup compared with
existing systems on a large public machine translation benchmark (WMT14
English-German).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Myelin: An asynchronous, message-driven parallel framework for extreme-scale deep learning. (arXiv:2110.13005v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.13005">
<div class="article-summary-box-inner">
<span><p>In the last few years, the memory requirements to train state-of-the-art
neural networks have far exceeded the DRAM capacities of modern hardware
accelerators. This has necessitated the development of efficient algorithms to
train these neural networks in parallel on large-scale GPU-based clusters.
Since computation is relatively inexpensive on modern GPUs, designing and
implementing extremely efficient communication in these parallel training
algorithms is critical for extracting the maximum performance. This paper
presents Myelin, a parallel deep learning framework that exploits asynchrony
and message-driven execution to schedule neural network operations on each GPU,
thereby reducing GPU idle time and maximizing hardware efficiency. By using the
CPU memory as a scratch space for offloading data periodically during training,
Myelin is able to reduce GPU memory consumption by four times. This allows us
to increase the number of parameters per GPU by four times, thus reducing the
amount of communication and increasing performance by over 13%. When tested
against large transformer models with 12-100 billion parameters on 48-384
NVIDIA Tesla V100 GPUs, Myelin achieves a per-GPU throughput of 49.4-54.78% of
theoretical peak and reduces the training time by 22-37 days (15-25% speedup)
as compared to the state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
<li class="source">
<section>
<h3 class="source-name">cs.CV updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Frequency Centric Defense Mechanisms against Adversarial Examples. (arXiv:2110.13935v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.13935">
<div class="article-summary-box-inner">
<span><p>Adversarial example (AE) aims at fooling a Convolution Neural Network by
introducing small perturbations in the input image.The proposed work uses the
magnitude and phase of the Fourier Spectrum and the entropy of the image to
defend against AE. We demonstrate the defense in two ways: by training an
adversarial detector and denoising the adversarial effect. Experiments were
conducted on the low-resolution CIFAR-10 and high-resolution ImageNet datasets.
The adversarial detector has 99% accuracy for FGSM and PGD attacks on the
CIFAR-10 dataset. However, the detection accuracy falls to 50% for
sophisticated DeepFool and Carlini &amp; Wagner attacks on ImageNet. We overcome
the limitation by using autoencoder and show that 70% of AEs are correctly
classified after denoising.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CausalAF: Causal Autoregressive Flow for Goal-Directed Safety-Critical Scenes Generation. (arXiv:2110.13939v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.13939">
<div class="article-summary-box-inner">
<span><p>Goal-directed generation, aiming for solving downstream tasks by generating
diverse data, has a potentially wide range of applications in the real world.
Previous works tend to formulate goal-directed generation as a purely
data-driven problem, which directly searches or approximates the distribution
of samples satisfying the goal. However, the generation ability of preexisting
work is heavily restricted by inefficient sampling, especially for sparse goals
that rarely show up in off-the-shelf datasets. For instance, generating
safety-critical traffic scenes with the goal of increasing the risk of
collision is critical to evaluate autonomous vehicles, but the rareness of such
scenes is the biggest resistance. In this paper, we integrate causality as a
prior into the safety-critical scene generation process and propose a
flow-based generative framework - Causal Autoregressive Flow (CausalAF).
CausalAF encourages the generative model to uncover and follow the causal
relationship among generated objects via novel causal masking operations
instead of searching the sample only from observational data. By learning the
cause-and-effect mechanism of how the generated scene achieves the goal rather
than just learning correlations from data, CausalAF significantly improves the
learning efficiency. Extensive experiments on three heterogeneous traffic
scenes illustrate that CausalAF requires much fewer optimization resources to
effectively generate goal-directed scenes for safety evaluation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Collaborative Uncertainty in Multi-Agent Trajectory Forecasting. (arXiv:2110.13947v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.13947">
<div class="article-summary-box-inner">
<span><p>Uncertainty modeling is critical in trajectory forecasting systems for both
interpretation and safety reasons. To better predict the future trajectories of
multiple agents, recent works have introduced interaction modules to capture
interactions among agents. This approach leads to correlations among the
predicted trajectories. However, the uncertainty brought by such correlations
is neglected. To fill this gap, we propose a novel concept, collaborative
uncertainty(CU), which models the uncertainty resulting from the interaction
module. We build a general CU-based framework to make a prediction model to
learn the future trajectory and the corresponding uncertainty. The CU-based
framework is integrated as a plugin module to current state-of-the-art (SOTA)
systems and deployed in two special cases based on multivariate Gaussian and
Laplace distributions. In each case, we conduct extensive experiments on two
synthetic datasets and two public, large-scale benchmarks of trajectory
forecasting. The results are promising: 1) The results of synthetic datasets
show that CU-based framework allows the model to appropriately approximate the
ground-truth distribution. 2) The results of trajectory forecasting benchmarks
demonstrate that the CU-based framework steadily helps SOTA systems improve
their performances. Especially, the proposed CU-based framework helps VectorNet
improve by 57cm regarding Final Displacement Error on nuScenes dataset. 3) The
visualization results of CU illustrate that the value of CU is highly related
to the amount of the interactive information among agents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can't Fool Me: Adversarially Robust Transformer for Video Understanding. (arXiv:2110.13950v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.13950">
<div class="article-summary-box-inner">
<span><p>Deep neural networks have been shown to perform poorly on adversarial
examples. To address this, several techniques have been proposed to increase
robustness of a model for image classification tasks. However, in video
understanding tasks, developing adversarially robust models is still
unexplored. In this paper, we aim to bridge this gap. We first show that simple
extensions of image based adversarially robust models slightly improve the
worst-case performance. Further, we propose a temporal attention regularization
scheme in Transformer to improve the robustness of attention modules to
adversarial examples. We illustrate using a large-scale video data set
YouTube-8M that the final model (A-ART) achieves close to non-adversarial
performance on its adversarial example set. We achieve 91% GAP on adversarial
examples, whereas baseline Transformer and simple adversarial extensions
achieve 72.9% and 82% respectively, showing significant improvement in
robustness over the state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Video-based fully automatic assessment of open surgery suturing skills. (arXiv:2110.13972v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.13972">
<div class="article-summary-box-inner">
<span><p>The goal of this study was to develop new reliable open surgery suturing
simulation system for training medical students in situation where resources
are limited or in the domestic setup. Namely, we developed an algorithm for
tools and hands localization as well as identifying the interactions between
them based on simple webcam video data, calculating motion metrics for
assessment of surgical skill. Twenty-five participants performed multiple
suturing tasks using our simulator. The YOLO network has been modified to a
multi-task network, for the purpose of tool localization and tool-hand
interaction detection. This was accomplished by splitting the YOLO detection
heads so that they supported both tasks with minimal addition to computer
run-time. Furthermore, based on the outcome of the system, motion metrics were
calculated. These metrics included traditional metrics such as time and path
length as well as new metrics assessing the technique participants use for
holding the tools. The dual-task network performance was similar to that of two
networks, while computational load was only slightly bigger than one network.
In addition, the motion metrics showed significant differences between experts
and novices. While video capture is an essential part of minimally invasive
surgery, it is not an integral component of open surgery. Thus, new algorithms,
focusing on the unique challenges open surgery videos present, are required. In
this study, a dual-task network was developed to solve both a localization task
and a hand-tool interaction task. The dual network may be easily expanded to a
multi-task network, which may be useful for images with multiple layers and for
evaluating the interaction between these different layers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CHIP: CHannel Independence-based Pruning for Compact Neural Networks. (arXiv:2110.13981v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.13981">
<div class="article-summary-box-inner">
<span><p>Filter pruning has been widely used for neural network compression because of
its enabled practical acceleration. To date, most of the existing filter
pruning works explore the importance of filters via using intra-channel
information. In this paper, starting from an inter-channel perspective, we
propose to perform efficient filter pruning using Channel Independence, a
metric that measures the correlations among different feature maps. The less
independent feature map is interpreted as containing less useful
information$/$knowledge, and hence its corresponding filter can be pruned
without affecting model capacity. We systematically investigate the
quantification metric, measuring scheme and sensitiveness$/$reliability of
channel independence in the context of filter pruning. Our evaluation results
for different models on various datasets show the superior performance of our
approach. Notably, on CIFAR-10 dataset our solution can bring $0.75\%$ and
$0.94\%$ accuracy increase over baseline ResNet-56 and ResNet-110 models,
respectively, and meanwhile the model size and FLOPs are reduced by $42.8\%$
and $47.4\%$ (for ResNet-56) and $48.3\%$ and $52.1\%$ (for ResNet-110),
respectively. On ImageNet dataset, our approach can achieve $40.8\%$ and
$44.8\%$ storage and computation reductions, respectively, with $0.15\%$
accuracy increase over the baseline ResNet-50 model. The code is available at
https://github.com/Eclipsess/CHIP_NeurIPS2021.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Batch Normalization. (arXiv:2110.13989v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.13989">
<div class="article-summary-box-inner">
<span><p>Batch normalization (BN) is comprised of a normalization component followed
by an affine transformation and has become essential for training deep neural
networks. Standard initialization of each BN in a network sets the affine
transformation scale and shift to 1 and 0, respectively. However, after
training we have observed that these parameters do not alter much from their
initialization. Furthermore, we have noticed that the normalization process can
still yield overly large values, which is undesirable for training. We revisit
the BN formulation and present a new initialization method and update approach
for BN to address the aforementioned issues. Experimental results using the
proposed alterations to BN show statistically significant performance gains in
a variety of scenarios. The approach can be used with existing implementations
at no additional computational cost. We also present a new online BN-based
input data normalization technique to alleviate the need for other offline or
fixed methods. Source code is available at
https://github.com/osu-cvl/revisiting-bn.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging Local Temporal Information for Multimodal Scene Classification. (arXiv:2110.13992v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.13992">
<div class="article-summary-box-inner">
<span><p>Robust video scene classification models should capture the spatial
(pixel-wise) and temporal (frame-wise) characteristics of a video effectively.
Transformer models with self-attention which are designed to get contextualized
representations for individual tokens given a sequence of tokens, are becoming
increasingly popular in many computer vision tasks. However, the use of
Transformer based models for video understanding is still relatively
unexplored. Moreover, these models fail to exploit the strong temporal
relationships between the neighboring video frames to get potent frame-level
representations. In this paper, we propose a novel self-attention block that
leverages both local and global temporal relationships between the video frames
to obtain better contextualized representations for the individual frames. This
enables the model to understand the video at various granularities. We
illustrate the performance of our models on the large scale YoutTube-8M data
set on the task of video categorization and further analyze the results to
showcase improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controllable Data Augmentation Through Deep Relighting. (arXiv:2110.13996v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.13996">
<div class="article-summary-box-inner">
<span><p>At the heart of the success of deep learning is the quality of the data.
Through data augmentation, one can train models with better generalization
capabilities and thus achieve greater results in their field of interest. In
this work, we explore how to augment a varied set of image datasets through
relighting so as to improve the ability of existing models to be invariant to
illumination changes, namely for learned descriptors. We develop a tool, based
on an encoder-decoder network, that is able to quickly generate multiple
variations of the illumination of various input scenes whilst also allowing the
user to define parameters such as the angle of incidence and intensity. We
demonstrate that by training models on datasets that have been augmented with
our pipeline, it is possible to achieve higher performance on localization
benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MisConv: Convolutional Neural Networks for Missing Data. (arXiv:2110.14010v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14010">
<div class="article-summary-box-inner">
<span><p>Processing of missing data by modern neural networks, such as CNNs, remains a
fundamental, yet unsolved challenge, which naturally arises in many practical
applications, like image inpainting or autonomous vehicles and robots. While
imputation-based techniques are still one of the most popular solutions, they
frequently introduce unreliable information to the data and do not take into
account the uncertainty of estimation, which may be destructive for a machine
learning model. In this paper, we present MisConv, a general mechanism, for
adapting various CNN architectures to process incomplete images. By modeling
the distribution of missing values by the Mixture of Factor Analyzers, we cover
the spectrum of possible replacements and find an analytical formula for the
expected value of convolution operator applied to the incomplete image. The
whole framework is realized by matrix operations, which makes MisConv extremely
efficient in practice. Experiments performed on various image processing tasks
demonstrate that MisConv achieves superior or comparable performance to the
state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Integrated Pipeline of Segmentation Leading to Classification for Automated Detection of Breast Cancer from Breast Ultrasound Images. (arXiv:2110.14013v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14013">
<div class="article-summary-box-inner">
<span><p>Breast cancer has become a symbol of tremendous concern in the modern world,
as it is one of the major causes of cancer mortality worldwide. In this
concern, many people are frequently screening for breast cancer in order to be
identified early and avert mortality from the disease by receiving treatment.
Breast Ultrasonography Images are frequently utilized by doctors to diagnose
breast cancer at an early stage. However, the complex artifacts and heavily
noised Breast Ultrasonography Images make detecting Breast Cancer a tough
challenge. Furthermore, the ever-increasing number of patients being screened
for Breast Cancer necessitates the use of automated Computer Aided Technology
for high accuracy diagnosis at a cheap cost and in a short period of time. The
current progress of Artificial Intelligence (AI) in the fields of Medical Image
Analysis and Health Care is a boon to humanity. In this study, we have proposed
a compact integrated automated pipelining framework which integrates
ultrasonography image preprocessing with Simple Linear Iterative Clustering
(SLIC) to tackle the complex artifact of Breast Ultrasonography Images
complementing semantic segmentation with Modified U-Net leading to Breast Tumor
classification with robust feature extraction using a transfer learning
approach with pretrained VGG 16 model and densely connected neural network
architecture. The proposed automated pipeline can be effectively implemented to
assist medical practitioners in making more accurate and timely diagnoses of
breast cancer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Local Effectiveness for Global robust training. (arXiv:2110.14030v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14030">
<div class="article-summary-box-inner">
<span><p>Despite its popularity, deep neural networks are easily fooled. To alleviate
this deficiency, researchers are actively developing new training strategies,
which encourage models that are robust to small input perturbations. Several
successful robust training methods have been proposed. However, many of them
rely on strong adversaries, which can be prohibitively expensive to generate
when the input dimension is high and the model structure is complicated. We
adopt a new perspective on robustness and propose a novel training algorithm
that allows a more effective use of adversaries. Our method improves the model
robustness at each local ball centered around an adversary and then, by
combining these local balls through a global term, achieves overall robustness.
We demonstrate that, by maximizing the use of adversaries via focusing on local
balls, we achieve high robust accuracy with weak adversaries. Specifically, our
method reaches a similar robust accuracy level to the state of the art
approaches trained on strong adversaries on MNIST, CIFAR-10 and CIFAR-100. As a
result, the overall training time is reduced. Furthermore, when trained with
strong adversaries, our method matches with the current state of the art on
MNIST and outperforms them on CIFAR-10 and CIFAR-100.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MEST: Accurate and Fast Memory-Economic Sparse Training Framework on the Edge. (arXiv:2110.14032v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14032">
<div class="article-summary-box-inner">
<span><p>Recently, a new trend of exploring sparsity for accelerating neural network
training has emerged, embracing the paradigm of training on the edge. This
paper proposes a novel Memory-Economic Sparse Training (MEST) framework
targeting for accurate and fast execution on edge devices. The proposed MEST
framework consists of enhancements by Elastic Mutation (EM) and Soft Memory
Bound (&amp;S) that ensure superior accuracy at high sparsity ratios. Different
from the existing works for sparse training, this current work reveals the
importance of sparsity schemes on the performance of sparse training in terms
of accuracy as well as training speed on real edge devices. On top of that, the
paper proposes to employ data efficiency for further acceleration of sparse
training. Our results suggest that unforgettable examples can be identified
in-situ even during the dynamic exploration of sparsity masks in the sparse
training process, and therefore can be removed for further training speedup on
edge devices. Comparing with state-of-the-art (SOTA) works on accuracy, our
MEST increases Top-1 accuracy significantly on ImageNet when using the same
unstructured sparsity scheme. Systematical evaluation on accuracy, training
speed, and memory footprint are conducted, where the proposed MEST framework
consistently outperforms representative SOTA works. A reviewer strongly against
our work based on his false assumptions and misunderstandings. On top of the
previous submission, we employ data efficiency for further acceleration of
sparse training. And we explore the impact of model sparsity, sparsity schemes,
and sparse training algorithms on the number of removable training examples.
Our codes are publicly available at: https://github.com/boone891214/MEST.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Unified Survey on Anomaly, Novelty, Open-Set, and Out-of-Distribution Detection: Solutions and Future Challenges. (arXiv:2110.14051v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14051">
<div class="article-summary-box-inner">
<span><p>Machine learning models often encounter samples that are diverged from the
training distribution. Failure to recognize an out-of-distribution (OOD)
sample, and consequently assign that sample to an in-class label significantly
compromises the reliability of a model. The problem has gained significant
attention due to its importance for safety deploying models in open-world
settings. Detecting OOD samples is challenging due to the intractability of
modeling all possible unknown distributions. To date, several research domains
tackle the problem of detecting unfamiliar samples, including anomaly
detection, novelty detection, one-class learning, open set recognition, and
out-of-distribution detection. Despite having similar and shared concepts,
out-of-distribution, open-set, and anomaly detection have been investigated
independently. Accordingly, these research avenues have not cross-pollinated,
creating research barriers. While some surveys intend to provide an overview of
these approaches, they seem to only focus on a specific domain without
examining the relationship between different domains. This survey aims to
provide a cross-domain and comprehensive review of numerous eminent works in
respective areas while identifying their commonalities. Researchers can benefit
from the overview of research advances in different fields and develop future
methodology synergistically. Furthermore, to the best of our knowledge, while
there are surveys in anomaly detection or one-class learning, there is no
comprehensive or up-to-date survey on out-of-distribution detection, which our
survey covers extensively. Finally, having a unified cross-domain perspective,
we discuss and shed light on future lines of research, intending to bring these
fields closer together.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CoFiNet: Reliable Coarse-to-fine Correspondences for Robust Point Cloud Registration. (arXiv:2110.14076v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14076">
<div class="article-summary-box-inner">
<span><p>We study the problem of extracting correspondences between a pair of point
clouds for registration. For correspondence retrieval, existing works benefit
from matching sparse keypoints detected from dense points but usually struggle
to guarantee their repeatability. To address this issue, we present CoFiNet -
Coarse-to-Fine Network which extracts hierarchical correspondences from coarse
to fine without keypoint detection. On a coarse scale and guided by a weighting
scheme, our model firstly learns to match down-sampled nodes whose vicinity
points share more overlap, which significantly shrinks the search space of a
consecutive stage. On a finer scale, node proposals are consecutively expanded
to patches that consist of groups of points together with associated
descriptors. Point correspondences are then refined from the overlap areas of
corresponding patches, by a density-adaptive matching module capable to deal
with varying point density. Extensive evaluation of CoFiNet on both indoor and
outdoor standard benchmarks shows our superiority over existing methods.
Especially on 3DLoMatch where point clouds share less overlap, CoFiNet
significantly outperforms state-of-the-art approaches by at least 5% on
Registration Recall, with at most two-third of their parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Robust Bisimulation Metric Learning. (arXiv:2110.14096v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14096">
<div class="article-summary-box-inner">
<span><p>Learned representations in deep reinforcement learning (DRL) have to extract
task-relevant information from complex observations, balancing between
robustness to distraction and informativeness to the policy. Such stable and
rich representations, often learned via modern function approximation
techniques, can enable practical application of the policy improvement theorem,
even in high-dimensional continuous state-action spaces. Bisimulation metrics
offer one solution to this representation learning problem, by collapsing
functionally similar states together in representation space, which promotes
invariance to noise and distractors. In this work, we generalize value function
approximation bounds for on-policy bisimulation metrics to non-optimal policies
and approximate environment dynamics. Our theoretical results help us identify
embedding pathologies that may occur in practical use. In particular, we find
that these issues stem from an underconstrained dynamics model and an unstable
dependence of the embedding norm on the reward signal in environments with
sparse rewards. Further, we propose a set of practical remedies: (i) a norm
constraint on the representation space, and (ii) an extension of prior
approaches with intrinsic rewards and latent space regularization. Finally, we
provide evidence that the resulting method is not only more robust to sparse
reward functions, but also able to solve challenging continuous control tasks
with observational distractions, where prior methods fail.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ScaleCert: Scalable Certified Defense against Adversarial Patches with Sparse Superficial Layers. (arXiv:2110.14120v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14120">
<div class="article-summary-box-inner">
<span><p>Adversarial patch attacks that craft the pixels in a confined region of the
input images show their powerful attack effectiveness in physical environments
even with noises or deformations. Existing certified defenses towards
adversarial patch attacks work well on small images like MNIST and CIFAR-10
datasets, but achieve very poor certified accuracy on higher-resolution images
like ImageNet. It is urgent to design both robust and effective defenses
against such a practical and harmful attack in industry-level larger images. In
this work, we propose the certified defense methodology that achieves high
provable robustness for high-resolution images and largely improves the
practicality for real adoption of the certified defense. The basic insight of
our work is that the adversarial patch intends to leverage localized
superficial important neurons (SIN) to manipulate the prediction results.
Hence, we leverage the SIN-based DNN compression techniques to significantly
improve the certified accuracy, by reducing the adversarial region searching
overhead and filtering the prediction noises. Our experimental results show
that the certified accuracy is increased from 36.3% (the state-of-the-art
certified detection) to 60.4% on the ImageNet dataset, largely pushing the
certified defenses for practical use.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SOAT: A Scene- and Object-Aware Transformer for Vision-and-Language Navigation. (arXiv:2110.14143v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14143">
<div class="article-summary-box-inner">
<span><p>Natural language instructions for visual navigation often use scene
descriptions (e.g., "bedroom") and object references (e.g., "green chairs") to
provide a breadcrumb trail to a goal location. This work presents a
transformer-based vision-and-language navigation (VLN) agent that uses two
different visual encoders -- a scene classification network and an object
detector -- which produce features that match these two distinct types of
visual cues. In our method, scene features contribute high-level contextual
information that supports object-level processing. With this design, our model
is able to use vision-and-language pretraining (i.e., learning the alignment
between images and text from large-scale web data) to substantially improve
performance on the Room-to-Room (R2R) and Room-Across-Room (RxR) benchmarks.
Specifically, our approach leads to improvements of 1.8% absolute in SPL on R2R
and 3.7% absolute in SR on RxR. Our analysis reveals even larger gains for
navigation instructions that contain six or more object references, which
further suggests that our approach is better able to use object features and
align them to references in the instructions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Physically Explainable CNN for SAR Image Classification. (arXiv:2110.14144v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14144">
<div class="article-summary-box-inner">
<span><p>Integrating the special electromagnetic characteristics of Synthetic Aperture
Radar (SAR) in deep neural networks is essential in order to enhance the
explainability and physics awareness of deep learning. In this paper, we
firstly propose a novel physics guided and injected neural network for SAR
image classification, which is mainly guided by explainable physics models and
can be learned with very limited labeled data. The proposed framework comprises
three parts: (1) generating physics guided signals using existing explainable
models, (2) learning physics-aware features with physics guided network, and
(3) injecting the physics-aware features adaptively to the conventional
classification deep learning model for prediction. The prior knowledge,
physical scattering characteristic of SAR in this paper, is injected into the
deep neural network in the form of physics-aware features which is more
conducive to understanding the semantic labels of SAR image patches. A hybrid
Image-Physics SAR dataset format is proposed, and both Sentinel-1 and Gaofen-3
SAR data are taken for evaluation. The experimental results show that our
proposed method substantially improve the classification performance compared
with the counterpart data-driven CNN. Moreover, the guidance of explainable
physics signals leads to explainability of physics-aware features and the
physics consistency of features are also preserved in the predictions. We deem
the proposed method would promote the development of physically explainable
deep learning in SAR image interpretation field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Image Comes Dancing with Collaborative Parsing-Flow Video Synthesis. (arXiv:2110.14147v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14147">
<div class="article-summary-box-inner">
<span><p>Transferring human motion from a source to a target person poses great
potential in computer vision and graphics applications. A crucial step is to
manipulate sequential future motion while retaining the appearance
characteristic.Previous work has either relied on crafted 3D human models or
trained a separate model specifically for each target person, which is not
scalable in practice.This work studies a more general setting, in which we aim
to learn a \emph{single} model to parsimoniously transfer motion from a source
video to any target person given only one image of the person, named as
Collaborative Parsing-Flow Network (CPF-Net). The paucity of information
regarding the target person makes the task particularly challenging to
faithfully preserve the appearance in varying designated poses.To address this
issue, CPF-Net integrates the structured human parsing and appearance flow to
guide the realistic foreground synthesis which is merged into the background by
a spatio-temporal fusion module.In particular, CPF-Net decouples the problem
into stages of human parsing sequence generation, foreground sequence
generation and final video generation. The human parsing generation stage
captures both the pose and the body structure of the target. The appearance
flow is beneficial to keep details in synthesized frames. The integration of
human parsing and appearance flow effectively guides the generation of video
frames with realistic appearance. Finally, the dedicated designed fusion
network ensure the temporal coherence. We further collect a large set of human
dancing videos to push forward this research field. Both quantitative and
qualitative results show our method substantially improves over previous
approaches and is able to generate appealing and photo-realistic target videos
given any input person image. All source code and dataset will be released at
https://github.com/xiezhy6/CPF-Net.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training Wasserstein GANs without gradient penalties. (arXiv:2110.14150v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14150">
<div class="article-summary-box-inner">
<span><p>We propose a stable method to train Wasserstein generative adversarial
networks. In order to enhance stability, we consider two objective functions
using the $c$-transform based on Kantorovich duality which arises in the theory
of optimal transport. We experimentally show that this algorithm can
effectively enforce the Lipschitz constraint on the discriminator while other
standard methods fail to do so. As a consequence, our method yields an accurate
estimation for the optimal discriminator and also for the Wasserstein distance
between the true distribution and the generated one. Our method requires no
gradient penalties nor corresponding hyperparameter tuning and is
computationally more efficient than other methods. At the same time, it yields
competitive generators of synthetic images based on the MNIST, F-MNIST, and
CIFAR-10 datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying the key components in ResNet-50 for diabetic retinopathy grading from fundus images: a systematic investigation. (arXiv:2110.14160v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14160">
<div class="article-summary-box-inner">
<span><p>Although deep learning based diabetic retinopathy (DR) classification methods
typically benefit from well-designed architectures of convolutional neural
networks, the training setting also has a non-negligible impact on the
prediction performance. The training setting includes various interdependent
components, such as objective function, data sampling strategy and data
augmentation approach. To identify the key components in a standard deep
learning framework (ResNet-50) for DR grading, we systematically analyze the
impact of several major components. Extensive experiments are conducted on a
publicly-available dataset EyePACS. We demonstrate that (1) the ResNet-50
framework for DR grading is sensitive to input resolution, objective function,
and composition of data augmentation, (2) using mean square error as the loss
function can effectively improve the performance with respect to a
task-specific evaluation metric, namely the quadratically-weighted Kappa, (3)
utilizing eye pairs boosts the performance of DR grading and (4) using data
resampling to address the problem of imbalanced data distribution in EyePACS
hurts the performance. Based on these observations and an optimal combination
of the investigated components, our framework, without any specialized network
design, achieves the state-of-the-art result (0.8631 for Kappa) on the EyePACS
test set (a total of 42670 fundus images) with only image-level labels. Our
codes and pre-trained model are available at
https://github.com/YijinHuang/pytorch-classification
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QU-net++: Image Quality Detection Framework for Segmentation of 3D Medical Image Stacks. (arXiv:2110.14181v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14181">
<div class="article-summary-box-inner">
<span><p>Automated segmentation of pathological regions of interest has been shown to
aid prognosis and follow up treatment. However, accurate pathological
segmentations require high quality of annotated data that can be both cost and
time intensive to generate. In this work, we propose an automated two-step
method that evaluates the quality of medical images from 3D image stacks using
a U-net++ model, such that images that can aid further training of the U-net++
model can be detected based on the disagreement in segmentations produced from
the final two layers. Images thus detected can then be used to further fine
tune the U-net++ model for semantic segmentation. The proposed QU-net++ model
isolates around 10\% of images per 3D stack and can scale across imaging
modalities to segment cysts in OCT images and ground glass opacity in Lung CT
images with Dice cores in the range 0.56-0.72. Thus, the proposed method can be
applied for multi-modal binary segmentation of pathology.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evidential Softmax for Sparse Multimodal Distributions in Deep Generative Models. (arXiv:2110.14182v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14182">
<div class="article-summary-box-inner">
<span><p>Many applications of generative models rely on the marginalization of their
high-dimensional output probability distributions. Normalization functions that
yield sparse probability distributions can make exact marginalization more
computationally tractable. However, sparse normalization functions usually
require alternative loss functions for training since the log-likelihood is
undefined for sparse probability distributions. Furthermore, many sparse
normalization functions often collapse the multimodality of distributions. In
this work, we present $\textit{ev-softmax}$, a sparse normalization function
that preserves the multimodality of probability distributions. We derive its
properties, including its gradient in closed-form, and introduce a continuous
family of approximations to $\textit{ev-softmax}$ that have full support and
can be trained with probabilistic loss functions such as negative
log-likelihood and Kullback-Leibler divergence. We evaluate our method on a
variety of generative models, including variational autoencoders and
auto-regressive architectures. Our method outperforms existing dense and sparse
normalization techniques in distributional accuracy. We demonstrate that
$\textit{ev-softmax}$ successfully reduces the dimensionality of probability
distributions while maintaining multimodality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Contrastive Learning Using Negative Samples with Diminished Semantics. (arXiv:2110.14189v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14189">
<div class="article-summary-box-inner">
<span><p>Unsupervised learning has recently made exceptional progress because of the
development of more effective contrastive learning methods. However, CNNs are
prone to depend on low-level features that humans deem non-semantic. This
dependency has been conjectured to induce a lack of robustness to image
perturbations or domain shift. In this paper, we show that by generating
carefully designed negative samples, contrastive learning can learn more robust
representations with less dependence on such features. Contrastive learning
utilizes positive pairs that preserve semantic information while perturbing
superficial features in the training images. Similarly, we propose to generate
negative samples in a reversed way, where only the superfluous instead of the
semantic features are preserved. We develop two methods, texture-based and
patch-based augmentations, to generate negative samples. These samples achieve
better generalization, especially under out-of-domain settings. We also analyze
our method and the generated texture-based samples, showing that texture
features are indispensable in classifying particular ImageNet classes and
especially finer classes. We also show that model bias favors texture and shape
features differently under different test settings. Our code, trained models,
and ImageNet-Texture dataset can be found at
https://github.com/SongweiGe/Contrastive-Learning-with-Non-Semantic-Negatives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mixed Supervised Object Detection by Transferring Mask Prior and Semantic Similarity. (arXiv:2110.14191v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14191">
<div class="article-summary-box-inner">
<span><p>Object detection has achieved promising success, but requires large-scale
fully-annotated data, which is time-consuming and labor-extensive. Therefore,
we consider object detection with mixed supervision, which learns novel object
categories using weak annotations with the help of full annotations of existing
base object categories. Previous works using mixed supervision mainly learn the
class-agnostic objectness from fully-annotated categories, which can be
transferred to upgrade the weak annotations to pseudo full annotations for
novel categories. In this paper, we further transfer mask prior and semantic
similarity to bridge the gap between novel categories and base categories.
Specifically, the ability of using mask prior to help detect objects is learned
from base categories and transferred to novel categories. Moreover, the
semantic similarity between objects learned from base categories is transferred
to denoise the pseudo full annotations for novel categories. Experimental
results on three benchmark datasets demonstrate the effectiveness of our method
over existing methods. Codes are available at
https://github.com/bcmi/TraMaS-Weak-Shot-Object-Detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Smooth head tracking for virtual reality applications. (arXiv:2110.14193v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14193">
<div class="article-summary-box-inner">
<span><p>In this work, we propose a new head-tracking solution for human-machine
real-time interaction with virtual 3D environments. This solution leverages
RGBD data to compute virtual camera pose according to the movements of the
user's head. The process starts with the extraction of a set of facial features
from the images delivered by the sensor. Such features are matched against
their respective counterparts in a reference image for the computation of the
current head pose. Afterwards, a prediction approach is used to guess the most
likely next head move (final pose). Pythagorean Hodograph interpolation is then
adapted to determine the path and local frames taken between the two poses. The
result is a smooth head trajectory that serves as an input to set the camera in
virtual scenes according to the user's gaze. The resulting motion model has the
advantage of being: continuous in time, it adapts to any frame rate of
rendering; it is ergonomic, as it frees the user from wearing tracking markers;
it is smooth and free from rendering jerks; and it is also torsion and
curvature minimizing as it produces a path with minimum bending energy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Image to Imuge: Immunized Image Generation. (arXiv:2110.14196v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14196">
<div class="article-summary-box-inner">
<span><p>We introduce Imuge, an image tamper resilient generative scheme for image
self-recovery. The traditional manner of concealing image content within the
image are inflexible and fragile to diverse digital attack, i.e. image cropping
and JPEG compression. To address this issue, we jointly train a U-Net backboned
encoder, a tamper localization network and a decoder for image recovery. Given
an original image, the encoder produces a visually indistinguishable immunized
image. At the recipient's side, the verifying network localizes the malicious
modifications, and the original content can be approximately recovered by the
decoder, despite the presence of the attacks. Several strategies are proposed
to boost the training efficiency. We demonstrate that our method can recover
the details of the tampered regions with a high quality despite the presence of
various kinds of attacks. Comprehensive ablation studies are conducted to
validate our network designs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Denoised Non-Local Neural Network for Semantic Segmentation. (arXiv:2110.14200v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14200">
<div class="article-summary-box-inner">
<span><p>The non-local network has become a widely used technique for semantic
segmentation, which computes an attention map to measure the relationships of
each pixel pair. However, most of the current popular non-local models tend to
ignore the phenomenon that the calculated attention map appears to be very
noisy, containing inter-class and intra-class inconsistencies, which lowers the
accuracy and reliability of the non-local methods. In this paper, we
figuratively denote these inconsistencies as attention noises and explore the
solutions to denoise them. Specifically, we inventively propose a Denoised
Non-Local Network (Denoised NL), which consists of two primary modules, i.e.,
the Global Rectifying (GR) block and the Local Retention (LR) block, to
eliminate the inter-class and intra-class noises respectively. First, GR adopts
the class-level predictions to capture a binary map to distinguish whether the
selected two pixels belong to the same category. Second, LR captures the
ignored local dependencies and further uses them to rectify the unwanted
hollows in the attention map. The experimental results on two challenging
semantic segmentation datasets demonstrate the superior performance of our
model. Without any external training data, our proposed Denoised NL can achieve
the state-of-the-art performance of 83.5\% and 46.69\% mIoU on Cityscapes and
ADE20K, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisit Multimodal Meta-Learning through the Lens of Multi-Task Learning. (arXiv:2110.14202v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14202">
<div class="article-summary-box-inner">
<span><p>Multimodal meta-learning is a recent problem that extends conventional
few-shot meta-learning by generalizing its setup to diverse multimodal task
distributions. This setup makes a step towards mimicking how humans make use of
a diverse set of prior skills to learn new skills. Previous work has achieved
encouraging performance. In particular, in spite of the diversity of the
multimodal tasks, previous work claims that a single meta-learner trained on a
multimodal distribution can sometimes outperform multiple specialized
meta-learners trained on individual unimodal distributions. The improvement is
attributed to knowledge transfer between different modes of task distributions.
However, there is no deep investigation to verify and understand the knowledge
transfer between multimodal tasks. Our work makes two contributions to
multimodal meta-learning. First, we propose a method to quantify knowledge
transfer between tasks of different modes at a micro-level. Our quantitative,
task-level analysis is inspired by the recent transference idea from multi-task
learning. Second, inspired by hard parameter sharing in multi-task learning and
a new interpretation of related work, we propose a new multimodal meta-learner
that outperforms existing work by considerable margins. While the major focus
is on multimodal meta-learning, our work also attempts to shed light on task
interaction in conventional meta-learning. The code for this project is
available at https://miladabd.github.io/KML.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural View Synthesis and Matching for Semi-Supervised Few-Shot Learning of 3D Pose. (arXiv:2110.14213v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14213">
<div class="article-summary-box-inner">
<span><p>We study the problem of learning to estimate the 3D object pose from a few
labelled examples and a collection of unlabelled data. Our main contribution is
a learning framework, neural view synthesis and matching, that can transfer the
3D pose annotation from the labelled to unlabelled images reliably, despite
unseen 3D views and nuisance variations such as the object shape, texture,
illumination or scene context. In our approach, objects are represented as 3D
cuboid meshes composed of feature vectors at each mesh vertex. The model is
initialized from a few labelled images and is subsequently used to synthesize
feature representations of unseen 3D views. The synthesized views are matched
with the feature representations of unlabelled images to generate pseudo-labels
of the 3D pose. The pseudo-labelled data is, in turn, used to train the feature
extractor such that the features at each mesh vertex are more invariant across
varying 3D views of the object. Our model is trained in an EM-type manner
alternating between increasing the 3D pose invariance of the feature extractor
and annotating unlabelled data through neural view synthesis and matching. We
demonstrate the effectiveness of the proposed semi-supervised learning
framework for 3D pose estimation on the PASCAL3D+ and KITTI datasets. We find
that our approach outperforms all baselines by a wide margin, particularly in
an extreme few-shot setting where only 7 annotated images are given.
Remarkably, we observe that our model also achieves an exceptional robustness
in out-of-distribution scenarios that involve partial occlusion.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Classification: Knowledge Distillation using Multi-Object Impressions. (arXiv:2110.14215v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14215">
<div class="article-summary-box-inner">
<span><p>Knowledge Distillation (KD) utilizes training data as a transfer set to
transfer knowledge from a complex network (Teacher) to a smaller network
(Student). Several works have recently identified many scenarios where the
training data may not be available due to data privacy or sensitivity concerns
and have proposed solutions under this restrictive constraint for the
classification task. Unlike existing works, we, for the first time, solve a
much more challenging problem, i.e., "KD for object detection with zero
knowledge about the training data and its statistics". Our proposed approach
prepares pseudo-targets and synthesizes corresponding samples (termed as
"Multi-Object Impressions"), using only the pretrained Faster RCNN Teacher
network. We use this pseudo-dataset as a transfer set to conduct zero-shot KD
for object detection. We demonstrate the efficacy of our proposed method
through several ablations and extensive experiments on benchmark datasets like
KITTI, Pascal and COCO. Our approach with no training samples, achieves a
respectable mAP of 64.2% and 55.5% on the student with same and half capacity
while performing distillation from a Resnet-18 Teacher of 73.3% mAP on KITTI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dex-NeRF: Using a Neural Radiance Field to Grasp Transparent Objects. (arXiv:2110.14217v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14217">
<div class="article-summary-box-inner">
<span><p>The ability to grasp and manipulate transparent objects is a major challenge
for robots. Existing depth cameras have difficulty detecting, localizing, and
inferring the geometry of such objects. We propose using neural radiance fields
(NeRF) to detect, localize, and infer the geometry of transparent objects with
sufficient accuracy to find and grasp them securely. We leverage NeRF's
view-independent learned density, place lights to increase specular
reflections, and perform a transparency-aware depth-rendering that we feed into
the Dex-Net grasp planner. We show how additional lights create specular
reflections that improve the quality of the depth map, and test a setup for a
robot workcell equipped with an array of cameras to perform transparent object
manipulation. We also create synthetic and real datasets of transparent objects
in real-world settings, including singulated objects, cluttered tables, and the
top rack of a dishwasher. In each setting we show that NeRF and Dex-Net are
able to reliably compute robust grasps on transparent objects, achieving 90%
and 100% grasp success rates in physical experiments on an ABB YuMi, on objects
where baseline methods fail.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RRNet: Relational Reasoning Network with Parallel Multi-scale Attention for Salient Object Detection in Optical Remote Sensing Images. (arXiv:2110.14223v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14223">
<div class="article-summary-box-inner">
<span><p>Salient object detection (SOD) for optical remote sensing images (RSIs) aims
at locating and extracting visually distinctive objects/regions from the
optical RSIs. Despite some saliency models were proposed to solve the intrinsic
problem of optical RSIs (such as complex background and scale-variant objects),
the accuracy and completeness are still unsatisfactory. To this end, we propose
a relational reasoning network with parallel multi-scale attention for SOD in
optical RSIs in this paper. The relational reasoning module that integrates the
spatial and the channel dimensions is designed to infer the semantic
relationship by utilizing high-level encoder features, thereby promoting the
generation of more complete detection results. The parallel multi-scale
attention module is proposed to effectively restore the detail information and
address the scale variation of salient objects by using the low-level features
refined by multi-scale attention. Extensive experiments on two datasets
demonstrate that our proposed RRNet outperforms the existing state-of-the-art
SOD competitors both qualitatively and quantitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">2nd Place Solution for VisDA 2021 Challenge -- Universally Domain Adaptive Image Recognition. (arXiv:2110.14240v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14240">
<div class="article-summary-box-inner">
<span><p>The Visual Domain Adaptation (VisDA) 2021 Challenge calls for unsupervised
domain adaptation (UDA) methods that can deal with both input distribution
shift and label set variance between the source and target domains. In this
report, we introduce a universal domain adaptation (UniDA) method by
aggregating several popular feature extraction and domain adaptation schemes.
First, we utilize VOLO, a Transformer-based architecture with state-of-the-art
performance in several visual tasks, as the backbone to extract effective
feature representations. Second, we modify the open-set classifier of OVANet to
recognize the unknown class with competitive accuracy and robustness. As shown
in the leaderboard, our proposed UniDA method ranks the 2nd place with 48.56%
ACC and 70.72% AUROC in the VisDA 2021 Challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilayer Lookahead: a Nested Version of Lookahead. (arXiv:2110.14254v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14254">
<div class="article-summary-box-inner">
<span><p>In recent years, SGD and its variants have become the standard tool to train
Deep Neural Networks. In this paper, we focus on the recently proposed variant
Lookahead, which improves upon SGD in a wide range of applications. Following
this success, we study an extension of this algorithm, the \emph{Multilayer
Lookahead} optimizer, which recursively wraps Lookahead around itself. We prove
the convergence of Multilayer Lookahead with two layers to a stationary point
of smooth non-convex functions with $O(\frac{1}{\sqrt{T}})$ rate. We also
justify the improved generalization of both Lookahead over SGD, and of
Multilayer Lookahead over Lookahead, by showing how they amplify the implicit
regularization effect of SGD. We empirically verify our results and show that
Multilayer Lookahead outperforms Lookahead on CIFAR-10 and CIFAR-100
classification tasks, and on GANs training on the MNIST dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Important is Importance Sampling for Deep Budgeted Training?. (arXiv:2110.14283v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14283">
<div class="article-summary-box-inner">
<span><p>Long iterative training processes for Deep Neural Networks (DNNs) are
commonly required to achieve state-of-the-art performance in many computer
vision tasks. Importance sampling approaches might play a key role in budgeted
training regimes, i.e. when limiting the number of training iterations. These
approaches aim at dynamically estimating the importance of each sample to focus
on the most relevant and speed up convergence. This work explores this paradigm
and how a budget constraint interacts with importance sampling approaches and
data augmentation techniques. We show that under budget restrictions,
importance sampling approaches do not provide a consistent improvement over
uniform sampling. We suggest that, given a specific budget, the best course of
action is to disregard the importance and introduce adequate data augmentation;
e.g. when reducing the budget to a 30% in CIFAR-10/100, RICAP data augmentation
maintains accuracy, while importance sampling does not. We conclude from our
work that DNNs under budget restrictions benefit greatly from variety in the
training set and that finding the right samples to train on is not the most
effective strategy when balancing high performance with low computational
requirements. Source code available at https://git.io/JKHa3 .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Sanity Checks for Saliency Maps. (arXiv:2110.14297v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14297">
<div class="article-summary-box-inner">
<span><p>Saliency methods are a popular approach for model debugging and
explainability. However, in the absence of ground-truth data for what the
correct maps should be, evaluating and comparing different approaches remains a
long-standing challenge. The sanity checks methodology of Adebayo et al
[Neurips 2018] has sought to address this challenge. They argue that some
popular saliency methods should not be used for explainability purposes since
the maps they produce are not sensitive to the underlying model that is to be
explained. Through a causal re-framing of their objective, we argue that their
empirical evaluation does not fully establish these conclusions, due to a form
of confounding introduced by the tasks they evaluate on. Through various
experiments on simple custom tasks we demonstrate that some of their
conclusions may indeed be artifacts of the tasks more than a criticism of the
saliency methods themselves. More broadly, our work challenges the utility of
the sanity check methodology, and further highlights that saliency map
evaluation beyond ad-hoc visual examination remains a fundamental challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inferring the Class Conditional Response Map for Weakly Supervised Semantic Segmentation. (arXiv:2110.14309v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14309">
<div class="article-summary-box-inner">
<span><p>Image-level weakly supervised semantic segmentation (WSSS) relies on class
activation maps (CAMs) for pseudo labels generation. As CAMs only highlight the
most discriminative regions of objects, the generated pseudo labels are usually
unsatisfactory to serve directly as supervision. To solve this, most existing
approaches follow a multi-training pipeline to refine CAMs for better
pseudo-labels, which includes: 1) re-training the classification model to
generate CAMs; 2) post-processing CAMs to obtain pseudo labels; and 3) training
a semantic segmentation model with the obtained pseudo labels. However, this
multi-training pipeline requires complicated adjustment and additional time. To
address this, we propose a class-conditional inference strategy and an
activation aware mask refinement loss function to generate better pseudo labels
without re-training the classifier. The class conditional inference-time
approach is presented to separately and iteratively reveal the classification
network's hidden object activation to generate more complete response maps.
Further, our activation aware mask refinement loss function introduces a novel
way to exploit saliency maps during segmentation training and refine the
foreground object masks without suppressing background objects. Our method
achieves superior WSSS results without requiring re-training of the classifier.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-frequency image completion via a biologically-inspired sub-Riemannian model with frequency and phase. (arXiv:2110.14330v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14330">
<div class="article-summary-box-inner">
<span><p>We present a novel cortically-inspired image completion algorithm. It uses a
five dimensional sub-Riemannian cortical geometry modelling the orientation,
spatial frequency and phase selective behavior of the cells in the visual
cortex. The algorithm extracts the orientation, frequency and phase information
existing in a given two dimensional corrupted input image via a Gabor transform
and represent those values in terms of cortical cell output responses in the
model geometry. Then it performs completion via a diffusion concentrated in a
neighbourhood along the neural connections within the model geometry. The
diffusion models the activity propagation integrating orientation, frequency
and phase features along the neural connections. Finally, the algorithm
transforms back the diffused and completed output responses back to the two
dimensional image plane.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature and Label Embedding Spaces Matter in Addressing Image Classifier Bias. (arXiv:2110.14336v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14336">
<div class="article-summary-box-inner">
<span><p>This paper strives to address image classifier bias, with a focus on both
feature and label embedding spaces. Previous works have shown that spurious
correlations from protected attributes, such as age, gender, or skin tone, can
cause adverse decisions. To balance potential harms, there is a growing need to
identify and mitigate image classifier bias. First, we identify in the feature
space a bias direction. We compute class prototypes of each protected attribute
value for every class, and reveal an existing subspace that captures the
maximum variance of the bias. Second, we mitigate biases by mapping image
inputs to label embedding spaces. Each value of the protected attribute has its
projection head where classes are embedded through a latent vector
representation rather than a common one-hot encoding. Once trained, we further
reduce in the feature space the bias effect by removing its direction.
Evaluation on biased image datasets, for multi-class, multi-label and binary
classifications, shows the effectiveness of tackling both feature and label
embedding spaces in improving the fairness of the classifier predictions, while
preserving classification performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CamLessMonoDepth: Monocular Depth Estimation with Unknown Camera Parameters. (arXiv:2110.14347v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14347">
<div class="article-summary-box-inner">
<span><p>Perceiving 3D information is of paramount importance in many applications of
computer vision. Recent advances in monocular depth estimation have shown that
gaining such knowledge from a single camera input is possible by training deep
neural networks to predict inverse depth and pose, without the necessity of
ground truth data. The majority of such approaches, however, require camera
parameters to be fed explicitly during training. As a result, image sequences
from wild cannot be used during training. While there exist methods which also
predict camera intrinsics, their performance is not on par with novel methods
taking camera parameters as input. In this work, we propose a method for
implicit estimation of pinhole camera intrinsics along with depth and pose, by
learning from monocular image sequences alone. In addition, by utilizing
efficient sub-pixel convolutions, we show that high fidelity depth estimates
can be obtained. We also embed pixel-wise uncertainty estimation into the
framework, to emphasize the possible applicability of this work in practical
domain. Finally, we demonstrate the possibility of accurate prediction of depth
information without prior knowledge of camera intrinsics, while outperforming
the existing state-of-the-art approaches on KITTI benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ConAM: Confidence Attention Module for Convolutional Neural Networks. (arXiv:2110.14369v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14369">
<div class="article-summary-box-inner">
<span><p>The so-called ``attention'' is an efficient mechanism to improve the
performance of convolutional neural networks. It uses contextual information to
recalibrate the input to strengthen the propagation of informative features.
However, the majority of the attention mechanisms only consider either local or
global contextual information, which is singular to extract features. Moreover,
many existing mechanisms directly use the contextual information to recalibrate
the input, which unilaterally enhances the propagation of the informative
features, but does not suppress the useless ones. This paper proposes a new
attention mechanism module based on the correlation between local and global
contextual information and we name this correlation as confidence. The novel
attention mechanism extracts the local and global contextual information
simultaneously, and calculates the confidence between them, then uses this
confidence to recalibrate the input pixels. The extraction of local and global
contextual information increases the diversity of features. The recalibration
with confidence suppresses useless information while enhancing the informative
one with fewer parameters. We use CIFAR-10 and CIFAR-100 in our experiments and
explore the performance of our method's components by sufficient ablation
studies. Finally, we compare our method with a various state-of-the-art
convolutional neural networks and the results show that our method completely
surpasses these models. We implement ConAM with the Python library, Pytorch,
and the code and models will be publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural-PIL: Neural Pre-Integrated Lighting for Reflectance Decomposition. (arXiv:2110.14373v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14373">
<div class="article-summary-box-inner">
<span><p>Decomposing a scene into its shape, reflectance and illumination is a
fundamental problem in computer vision and graphics. Neural approaches such as
NeRF have achieved remarkable success in view synthesis, but do not explicitly
perform decomposition and instead operate exclusively on radiance (the product
of reflectance and illumination). Extensions to NeRF, such as NeRD, can perform
decomposition but struggle to accurately recover detailed illumination, thereby
significantly limiting realism. We propose a novel reflectance decomposition
network that can estimate shape, BRDF, and per-image illumination given a set
of object images captured under varying illumination. Our key technique is a
novel illumination integration network called Neural-PIL that replaces a costly
illumination integral operation in the rendering with a simple network query.
In addition, we also learn deep low-dimensional priors on BRDF and illumination
representations using novel smooth manifold auto-encoders. Our decompositions
can result in considerably better BRDF and light estimates enabling more
accurate novel view-synthesis and relighting compared to prior art. Project
page: https://markboss.me/publication/2021-neural-pil/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perceptual Score: What Data Modalities Does Your Model Perceive?. (arXiv:2110.14375v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14375">
<div class="article-summary-box-inner">
<span><p>Machine learning advances in the last decade have relied significantly on
large-scale datasets that continue to grow in size. Increasingly, those
datasets also contain different data modalities. However, large multi-modal
datasets are hard to annotate, and annotations may contain biases that we are
often unaware of. Deep-net-based classifiers, in turn, are prone to exploit
those biases and to find shortcuts. To study and quantify this concern, we
introduce the perceptual score, a metric that assesses the degree to which a
model relies on the different subsets of the input features, i.e., modalities.
Using the perceptual score, we find a surprisingly consistent trend across four
popular datasets: recent, more accurate state-of-the-art multi-modal models for
visual question-answering or visual dialog tend to perceive the visual data
less than their predecessors. This trend is concerning as answers are hence
increasingly inferred from textual cues only. Using the perceptual score also
helps to analyze model biases by decomposing the score into data subset
contributions. We hope to spur a discussion on the perceptiveness of
multi-modal models and also hope to encourage the community working on
multi-modal classifiers to start quantifying perceptiveness via the proposed
perceptual score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Temporal-attentive Covariance Pooling Networks for Video Recognition. (arXiv:2110.14381v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14381">
<div class="article-summary-box-inner">
<span><p>For video recognition task, a global representation summarizing the whole
contents of the video snippets plays an important role for the final
performance. However, existing video architectures usually generate it by using
a simple, global average pooling (GAP) method, which has limited ability to
capture complex dynamics of videos. For image recognition task, there exist
evidences showing that covariance pooling has stronger representation ability
than GAP. Unfortunately, such plain covariance pooling used in image
recognition is an orderless representative, which cannot model spatio-temporal
structure inherent in videos. Therefore, this paper proposes a
Temporal-attentive Covariance Pooling(TCP), inserted at the end of deep
architectures, to produce powerful video representations. Specifically, our TCP
first develops a temporal attention module to adaptively calibrate
spatio-temporal features for the succeeding covariance pooling, approximatively
producing attentive covariance representations. Then, a temporal covariance
pooling performs temporal pooling of the attentive covariance representations
to characterize both intra-frame correlations and inter-frame
cross-correlations of the calibrated features. As such, the proposed TCP can
capture complex temporal dynamics. Finally, a fast matrix power normalization
is introduced to exploit geometry of covariance representations. Note that our
TCP is model-agnostic and can be flexibly integrated into any video
architectures, resulting in TCPNet for effective video recognition. The
extensive experiments on six benchmarks using various video architectures show
our TCPNet is clearly superior to its counterparts, while having strong
generalization
ability.$\href{https://github.com/ZilinGao/Temporal-attentive-Covariance-Pooling-Networks-for-Video-Recognition}{\textit{The
source code is publicly available.}}$
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Traffic Forecasting on Traffic Moving Snippets. (arXiv:2110.14383v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14383">
<div class="article-summary-box-inner">
<span><p>Advances in traffic forecasting technology can greatly impact urban mobility.
In the traffic4cast competition, the task of short-term traffic prediction is
tackled in unprecedented detail, with traffic volume and speed information
available at 5 minute intervals and high spatial resolution. To improve
generalization to unknown cities, as required in the 2021 extended challenge,
we propose to predict small quadratic city sections, rather than processing a
full-city-raster at once. At test time, breaking down the test data into
spatially-cropped overlapping snippets improves stability and robustness of the
final predictions, since multiple patches covering one cell can be processed
independently. With the performance on the traffic4cast test data and further
experiments on a validation set it is shown that patch-wise prediction indeed
improves accuracy. Further advantages can be gained with a Unet++ architecture
and with an increasing number of patches per sample processed at test time. We
conclude that our snippet-based method, combined with other successful network
architectures proposed in the competition, can leverage performance, in
particular on unseen cities. All source code is available at
https://github.com/NinaWie/NeurIPS2021-traffic4cast.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Taylor Swift: Taylor Driven Temporal Modeling for Swift Future Frame Prediction. (arXiv:2110.14392v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14392">
<div class="article-summary-box-inner">
<span><p>While recurrent neural networks (RNNs) demonstrate outstanding capabilities
in future video frame prediction, they model dynamics in a discrete time space
and sequentially go through all frames until the desired future temporal step
is reached. RNNs are therefore prone to accumulate the error as the number of
future frames increases. In contrast, partial differential equations (PDEs)
model physical phenomena like dynamics in continuous time space, however,
current PDE-based approaches discretize the PDEs using e.g., the forward Euler
method. In this work, we therefore propose to approximate the motion in a video
by a continuous function using the Taylor series. To this end, we introduce
TayloSwiftNet, a novel convolutional neural network that learns to estimate the
higher order terms of the Taylor series for a given input video. TayloSwiftNet
can swiftly predict any desired future frame in just one forward pass and
change the temporal resolution on-the-fly. The experimental results on various
datasets demonstrate the superiority of our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Separating Content and Style for Unsupervised Image-to-Image Translation. (arXiv:2110.14404v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14404">
<div class="article-summary-box-inner">
<span><p>Unsupervised image-to-image translation aims to learn the mapping between two
visual domains with unpaired samples. Existing works focus on disentangling
domain-invariant content code and domain-specific style code individually for
multimodal purposes. However, less attention has been paid to interpreting and
manipulating the translated image. In this paper, we propose to separate the
content code and style code simultaneously in a unified framework. Based on the
correlation between the latent features and the high-level domain-invariant
tasks, the proposed framework demonstrates superior performance in multimodal
translation, interpretability and manipulation of the translated image.
Experimental results show that the proposed approach outperforms the existing
unsupervised image translation methods in terms of visual quality and
diversity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Localized Super Resolution for Foreground Images using U-Net and MR-CNN. (arXiv:2110.14413v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14413">
<div class="article-summary-box-inner">
<span><p>Images play a vital role in understanding data through visual representation.
It gives a clear representation of the object in context. But if this image is
not clear it might not be of much use. Thus, the topic of Image Super
Resolution arose and many researchers have been working towards applying
Computer Vision and Deep Learning Techniques to increase the quality of images.
One of the applications of Super Resolution is to increase the quality of
Portrait Images. Portrait Images are images which mainly focus on capturing the
essence of the main object in the frame, where the object in context is
highlighted whereas the background is occluded. When performing Super
Resolution the model tries to increase the overall resolution of the image. But
in portrait images the foreground resolution is more important than that of the
background. In this paper, the performance of a Convolutional Neural Network
(CNN) architecture known as U-Net for Super Resolution combined with Mask
Region Based CNN (MR-CNN) for foreground super resolution is analysed. This
analysis is carried out based on Localized Super Resolution i.e. We pass the LR
Images to a pre-trained Image Segmentation model (MR-CNN) and perform super
resolution inference on the foreground or Segmented Images and compute the
Structural Similarity Index (SSIM) and Peak Signal-to-Noise Ratio (PSNR)
metrics for comparisons.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Neuron Pruning Purifies Backdoored Deep Models. (arXiv:2110.14430v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14430">
<div class="article-summary-box-inner">
<span><p>As deep neural networks (DNNs) are growing larger, their requirements for
computational resources become huge, which makes outsourcing training more
popular. Training in a third-party platform, however, may introduce potential
risks that a malicious trainer will return backdoored DNNs, which behave
normally on clean samples but output targeted misclassifications whenever a
trigger appears at the test time. Without any knowledge of the trigger, it is
difficult to distinguish or recover benign DNNs from backdoored ones. In this
paper, we first identify an unexpected sensitivity of backdoored DNNs, that is,
they are much easier to collapse and tend to predict the target label on clean
samples when their neurons are adversarially perturbed. Based on these
observations, we propose a novel model repairing method, termed Adversarial
Neuron Pruning (ANP), which prunes some sensitive neurons to purify the
injected backdoor. Experiments show, even with only an extremely small amount
of clean data (e.g., 1%), ANP effectively removes the injected backdoor without
causing obvious performance degradation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Iterative Teaching by Label Synthesis. (arXiv:2110.14432v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14432">
<div class="article-summary-box-inner">
<span><p>In this paper, we consider the problem of iterative machine teaching, where a
teacher provides examples sequentially based on the current iterative learner.
In contrast to previous methods that have to scan over the entire pool and
select teaching examples from it in each iteration, we propose a label
synthesis teaching framework where the teacher randomly selects input teaching
examples (e.g., images) and then synthesizes suitable outputs (e.g., labels)
for them. We show that this framework can avoid costly example selection while
still provably achieving exponential teachability. We propose multiple novel
teaching algorithms in this framework. Finally, we empirically demonstrate the
value of our framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Discriminator in GAN Compression: A Generator-discriminator Cooperative Compression Scheme. (arXiv:2110.14439v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14439">
<div class="article-summary-box-inner">
<span><p>Recently, a series of algorithms have been explored for GAN compression,
which aims to reduce tremendous computational overhead and memory usages when
deploying GANs on resource-constrained edge devices. However, most of the
existing GAN compression work only focuses on how to compress the generator,
while fails to take the discriminator into account. In this work, we revisit
the role of discriminator in GAN compression and design a novel
generator-discriminator cooperative compression scheme for GAN compression,
termed GCC. Within GCC, a selective activation discriminator automatically
selects and activates convolutional channels according to a local capacity
constraint and a global coordination constraint, which help maintain the Nash
equilibrium with the lightweight generator during the adversarial training and
avoid mode collapse. The original generator and discriminator are also
optimized from scratch, to play as a teacher model to progressively refine the
pruned generator and the selective activation discriminator. A novel online
collaborative distillation scheme is designed to take full advantage of the
intermediate feature of the teacher generator and discriminator to further
boost the performance of the lightweight generator. Extensive experiments on
various GAN-based generation tasks demonstrate the effectiveness and
generalization of GCC. Among them, GCC contributes to reducing 80%
computational costs while maintains comparable performance in image translation
tasks. Our code and models are available at \url{https://github.com/SJLeo/GCC}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CBIR using Pre-Trained Neural Networks. (arXiv:2110.14455v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14455">
<div class="article-summary-box-inner">
<span><p>Much of the recent research work in image retrieval, has been focused around
using Neural Networks as the core component. Many of the papers in other domain
have shown that training multiple models, and then combining their outcomes,
provide good results. This is since, a single Neural Network model, may not
extract sufficient information from the input. In this paper, we aim to follow
a different approach. Instead of the using a single model, we use a pretrained
Inception V3 model, and extract activation of its last fully connected layer,
which forms a low dimensional representation of the image. This feature matrix,
is then divided into branches and separate feature extraction is done for each
branch, to obtain multiple features flattened into a vector. Such individual
vectors are then combined, to get a single combined feature. We make use of
CUB200-2011 Dataset, which comprises of 200 birds classes to train the model
on. We achieved a training accuracy of 99.46% and validation accuracy of 84.56%
for the same. On further use of 3 branched global descriptors, we improve the
validation accuracy to 88.89%. For this, we made use of MS-RMAC feature
extraction method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hand gesture detection in the hand movement test for the early diagnosis of dementia. (arXiv:2110.14461v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14461">
<div class="article-summary-box-inner">
<span><p>Collecting hands data is important for many cognitive studies, especially for
senior participants who has no IT background. For example, alternating hand
movements and imitation of gestures are formal cognitive assessment in the
early detection of dementia. During data collection process, one of the key
steps is to detect whether the participants is following the instruction
correctly to do the correct gestures. Meanwhile, re-searchers found a lot of
problems in TAS Test hand movement data collection process, where is
challenging to detect similar gestures and guarantee the quality of the
collect-ed images. We have implemented a hand gesture detector to detect the
gestures per-formed in the hand movement tests, which enables us to monitor if
the participants are following the instructions correctly. In this research, we
have processed 20,000 images collected from TAS Test and labelled 6,450 images
to detect different hand poses in the hand movement tests. This paper has the
following three contributions. Firstly, we compared the performance of
different network structures for hand poses detection. Secondly, we introduced
a transformer block in the state of art network and increased the
classification performance of the similar gestures. Thirdly, we have created
two datasets and included 20 percent of blurred images in the dataset to
investigate how different network structures were impacted by noisy data, then
we proposed a novel net-work to increase the detection accuracy to mediate the
influence of the noisy data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Arbitrary Scale Super-Resolution Approach for 3-Dimensional Magnetic Resonance Image using Implicit Neural Representation. (arXiv:2110.14476v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14476">
<div class="article-summary-box-inner">
<span><p>High Resolution (HR) medical images provide rich anatomical structure details
to facilitate early and accurate diagnosis. In MRI, restricted by hardware
capacity, scan time, and patient cooperation ability, isotropic 3D HR image
acquisition typically requests long scan time and, results in small spatial
coverage and low SNR. Recent studies showed that, with deep convolutional
neural networks, isotropic HR MR images could be recovered from low-resolution
(LR) input via single image super-resolution (SISR) algorithms. However, most
existing SISR methods tend to approach a scale-specific projection between LR
and HR images, thus these methods can only deal with a fixed up-sampling rate.
For achieving different up-sampling rates, multiple SR networks have to be
built up respectively, which is very time-consuming and resource-intensive. In
this paper, we propose ArSSR, an Arbitrary Scale Super-Resolution approach for
recovering 3D HR MR images. In the ArSSR model, the reconstruction of HR images
with different up-scaling rates is defined as learning a continuous implicit
voxel function from the observed LR images. Then the SR task is converted to
represent the implicit voxel function via deep neural networks from a set of
paired HR-LR training examples. The ArSSR model consists of an encoder network
and a decoder network. Specifically, the convolutional encoder network is to
extract feature maps from the LR input images and the fully-connected decoder
network is to approximate the implicit voxel function. Due to the continuity of
the learned function, a single ArSSR model can achieve arbitrary up-sampling
rate reconstruction of HR images from any input LR image after training.
Experimental results on three datasets show that the ArSSR model can achieve
state-of-the-art SR performance for 3D HR MR image reconstruction while using a
single trained model to achieve arbitrary up-sampling scales.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PL-Net: Progressive Learning Network for Medical Image Segmentation. (arXiv:2110.14484v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14484">
<div class="article-summary-box-inner">
<span><p>In recent years, segmentation methods based on deep convolutional neural
networks (CNNs) have made state-of-the-art achievements for many medical
analysis tasks. However, most of these approaches improve performance by
optimizing the structure or adding new functional modules of the U-Net, which
ignoring the complementation and fusion of the coarse-grained and fine-grained
semantic information. To solve the above problems, we propose a medical image
segmentation framework called progressive learning network (PL-Net), which
includes internal progressive learning (IPL) and external progressive learning
(EPL). PL-Net has the following advantages: (1) IPL divides feature extraction
into two "steps", which can mix different size receptive fields and capture
semantic information from coarse to fine granularity without introducing
additional parameters; (2) EPL divides the training process into two "stages"
to optimize parameters, and realizes the fusion of coarse-grained information
in the previous stage and fine-grained information in the latter stage. We
evaluate our method in different medical image analysis tasks, and the results
show that the segmentation performance of PL-Net is better than the
state-of-the-art methods of U-Net and its variants.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training Lightweight CNNs for Human-Nanodrone Proximity Interaction from Small Datasets using Background Randomization. (arXiv:2110.14491v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14491">
<div class="article-summary-box-inner">
<span><p>We consider the task of visually estimating the pose of a human from images
acquired by a nearby nano-drone; in this context, we propose a data
augmentation approach based on synthetic background substitution to learn a
lightweight CNN model from a small real-world training set. Experimental
results on data from two different labs proves that the approach improves
generalization to unseen environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GenURL: A General Framework for Unsupervised Representation Learning. (arXiv:2110.14553v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14553">
<div class="article-summary-box-inner">
<span><p>Recently unsupervised representation learning (URL) has achieved remarkable
progress in various scenarios. However, most methods are specifically designed
based on specific data characters or task assumptions. Based on the manifold
assumption, we regard most URL problems as an embedding problem that seeks an
optimal low-dimensional representation of the given high-dimensional data. We
split the embedding process into two steps, data structural modeling and
low-dimensional embedding, and propose a general similarity-based framework
called GenURL. Specifically, we provide a general method to model data
structures by adaptively combining graph distances on the feature space and
predefined graphs, then propose robust loss functions to learn the
low-dimensional embedding. Combining with a specific pretext task, we can adapt
GenURL to various URL tasks in a unified manner and achieve state-of-the-art
performance, including self-supervised visual representation learning,
unsupervised knowledge distillation, graph embeddings, and dimension reduction.
Moreover, ablation studies of loss functions and basic hyper-parameter settings
in GenURL illustrate the data characters of various tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Geometric Perspective towards Neural Calibration via Sensitivity Decomposition. (arXiv:2110.14577v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14577">
<div class="article-summary-box-inner">
<span><p>It is well known that vision classification models suffer from poor
calibration in the face of data distribution shifts. In this paper, we take a
geometric approach to this problem. We propose Geometric Sensitivity
Decomposition (GSD) which decomposes the norm of a sample feature embedding and
the angular similarity to a target classifier into an instance-dependent and an
instance-independent component. The instance-dependent component captures the
sensitive information about changes in the input while the instance-independent
component represents the insensitive information serving solely to minimize the
loss on the training dataset. Inspired by the decomposition, we analytically
derive a simple extension to current softmax-linear models, which learns to
disentangle the two components during training. On several common vision
models, the disentangled model outperforms other calibration methods on
standard calibration metrics in the face of out-of-distribution (OOD) data and
corruption with significantly less complexity. Specifically, we surpass the
current state of the art by 30.8% relative improvement on corrupted CIFAR100 in
Expected Calibration Error. Code available at
https://github.com/GT-RIPL/Geometric-Sensitivity-Decomposition.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boundary Guided Context Aggregation for Semantic Segmentation. (arXiv:2110.14587v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14587">
<div class="article-summary-box-inner">
<span><p>The recent studies on semantic segmentation are starting to notice the
significance of the boundary information, where most approaches see boundaries
as the supplement of semantic details. However, simply combing boundaries and
the mainstream features cannot ensure a holistic improvement of semantics
modeling. In contrast to the previous studies, we exploit boundary as a
significant guidance for context aggregation to promote the overall semantic
understanding of an image. To this end, we propose a Boundary guided Context
Aggregation Network (BCANet), where a Multi-Scale Boundary extractor (MSB)
borrowing the backbone features at multiple scales is specifically designed for
accurate boundary detection. Based on which, a Boundary guided Context
Aggregation module (BCA) improved from Non-local network is further proposed to
capture long-range dependencies between the pixels in the boundary regions and
the ones inside the objects. By aggregating the context information along the
boundaries, the inner pixels of the same category achieve mutual gains and
therefore the intra-class consistency is enhanced. We conduct extensive
experiments on the Cityscapes and ADE20K databases, and comparable results are
achieved with the state-of-the-art methods, clearly demonstrating the
effectiveness of the proposed one.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TMBuD: A dataset for urban scene building detection. (arXiv:2110.14590v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14590">
<div class="article-summary-box-inner">
<span><p>Building recognition and 3D reconstruction of human made structures in urban
scenarios has become an interesting and actual topic in the image processing
domain. For this research topic the Computer Vision and Augmented Reality areas
intersect for creating a better understanding of the urban scenario for various
topics. In this paper we aim to introduce a dataset solution, the TMBuD, that
is better fitted for image processing on human made structures for urban scene
scenarios. The proposed dataset will allow proper evaluation of salient edges
and semantic segmentation of images focusing on the street view perspective of
buildings. The images that form our dataset offer various street view
perspectives of buildings from urban scenarios, which allows for evaluating
complex algorithms. The dataset features 160 images of buildings from
Timisoara, Romania, with a resolution of 768 x 1024 pixels each.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TA-Net: Topology-Aware Network for Gland Segmentation. (arXiv:2110.14593v1 [eess.IV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14593">
<div class="article-summary-box-inner">
<span><p>Gland segmentation is a critical step to quantitatively assess the morphology
of glands in histopathology image analysis. However, it is challenging to
separate densely clustered glands accurately. Existing deep learning-based
approaches attempted to use contour-based techniques to alleviate this issue
but only achieved limited success. To address this challenge, we propose a
novel topology-aware network (TA-Net) to accurately separate densely clustered
and severely deformed glands. The proposed TA-Net has a multitask learning
architecture and enhances the generalization of gland segmentation by learning
shared representation from two tasks: instance segmentation and gland topology
estimation. The proposed topology loss computes gland topology using gland
skeletons and markers. It drives the network to generate segmentation results
that comply with the true gland topology. We validate the proposed approach on
the GlaS and CRAG datasets using three quantitative metrics, F1-score,
object-level Dice coefficient, and object-level Hausdorff distance. Extensive
experiments demonstrate that TA-Net achieves state-of-the-art performance on
the two datasets. TA-Net outperforms other approaches in the presence of
densely clustered glands.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">International Workshop on Continual Semi-Supervised Learning: Introduction, Benchmarks and Baselines. (arXiv:2110.14613v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.14613">
<div class="article-summary-box-inner">
<span><p>The aim of this paper is to formalize a new continual semi-supervised
learning (CSSL) paradigm, proposed to the attention of the machine learning
community via the IJCAI 2021 International Workshop on Continual
Semi-Supervised Learning (CSSL-IJCAI), with the aim of raising field awareness
about this problem and mobilizing its effort in this direction. After a formal
definition of continual semi-supervised learning and the appropriate training
and testing protocols, the paper introduces two new benchmarks specifically
designed to assess CSSL on two important computer vision tasks: activity
recognition and crowd counting. We describe the Continual Activity Recognition
(CAR) and Continual Crowd Counting (CCC) challenges built upon those
benchmarks, the baseline models proposed for the challenges, and describe a
simple CSSL baseline which consists in applying batch self-training in temporal
sessions, for a limited number of rounds. The results show that learning from
unlabelled data streams is extremely challenging, and stimulate the search for
methods that can encode the dynamics of the data stream.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Edge Detection with Diverse Deep Supervision. (arXiv:1804.02864v5 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1804.02864">
<div class="article-summary-box-inner">
<span><p>Semantic edge detection (SED), which aims at jointly extracting edges as well
as their category information, has far-reaching applications in domains such as
semantic segmentation, object proposal generation, and object recognition. SED
naturally requires achieving two distinct supervision targets: locating fine
detailed edges and identifying high-level semantics. Our motivation comes from
the hypothesis that such distinct targets prevent state-of-the-art SED methods
from effectively using deep supervision to improve results. To this end, we
propose a novel fully convolutional neural network using diverse deep
supervision (DDS) within a multi-task framework where bottom layers aim at
generating category-agnostic edges, while top layers are responsible for the
detection of category-aware semantic edges. To overcome the hypothesized
supervision challenge, a novel information converter unit is introduced, whose
effectiveness has been extensively evaluated on SBD and Cityscapes datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CRIC: A VQA Dataset for Compositional Reasoning on Vision and Commonsense. (arXiv:1908.02962v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1908.02962">
<div class="article-summary-box-inner">
<span><p>Alternatively inferring on the visual facts and commonsense is fundamental
for an advanced VQA system. This ability requires models to go beyond the
literal understanding of commonsense. The system should not just treat objects
as the entrance to query background knowledge, but fully ground commonsense to
the visual world and imagine the possible relationships between objects, e.g.,
"fork, can lift, food". To comprehensively evaluate such abilities, we propose
a VQA benchmark, CRIC, which introduces new types of questions about
Compositional Reasoning on vIsion and Commonsense, and an evaluation metric
integrating the correctness of answering and commonsense grounding. To collect
such questions and rich additional annotations to support the metric, we also
propose an automatic algorithm to generate question samples from the scene
graph associated with the images and the relevant knowledge graph. We further
analyze several representative types of VQA models on the CRIC dataset.
Experimental results show that grounding the commonsense to the image region
and joint reasoning on vision and commonsense are still challenging for current
approaches. The dataset is available at https://cricvqa.github.io.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Compositions of Transformations in Contrastive Self-Supervised Learning. (arXiv:2003.04298v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.04298">
<div class="article-summary-box-inner">
<span><p>In the image domain, excellent representations can be learned by inducing
invariance to content-preserving transformations via noise contrastive
learning. In this paper, we generalize contrastive learning to a wider set of
transformations, and their compositions, for which either invariance or
distinctiveness is sought. We show that it is not immediately obvious how
existing methods such as SimCLR can be extended to do so. Instead, we introduce
a number of formal requirements that all contrastive formulations must satisfy,
and propose a practical construction which satisfies these requirements. In
order to maximise the reach of this analysis, we express all components of
noise contrastive formulations as the choice of certain generalized
transformations of the data (GDTs), including data sampling. We then consider
videos as an example of data in which a large variety of transformations are
applicable, accounting for the extra modalities -- for which we analyze audio
and text -- and the dimension of time. We find that being invariant to certain
transformations and distinctive to others is critical to learning effective
video representations, improving the state-of-the-art for multiple benchmarks
by a large margin, and even surpassing supervised pretraining.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Visual Analytics Framework for Reviewing Multivariate Time-Series Data with Dimensionality Reduction. (arXiv:2008.01645v3 [cs.HC] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.01645">
<div class="article-summary-box-inner">
<span><p>Data-driven problem solving in many real-world applications involves analysis
of time-dependent multivariate data, for which dimensionality reduction (DR)
methods are often used to uncover the intrinsic structure and features of the
data. However, DR is usually applied to a subset of data that is either
single-time-point multivariate or univariate time-series, resulting in the need
to manually examine and correlate the DR results out of different data subsets.
When the number of dimensions is large either in terms of the number of time
points or attributes, this manual task becomes too tedious and infeasible. In
this paper, we present MulTiDR, a new DR framework that enables processing of
time-dependent multivariate data as a whole to provide a comprehensive overview
of the data. With the framework, we employ DR in two steps. When treating the
instances, time points, and attributes of the data as a 3D array, the first DR
step reduces the three axes of the array to two, and the second DR step
visualizes the data in a lower-dimensional space. In addition, by coupling with
a contrastive learning method and interactive visualizations, our framework
enhances analysts' ability to interpret DR results. We demonstrate the
effectiveness of our framework with four case studies using real-world
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bridging Composite and Real: Towards End-to-end Deep Image Matting. (arXiv:2010.16188v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.16188">
<div class="article-summary-box-inner">
<span><p>Extracting accurate foregrounds from natural images benefits many downstream
applications such as film production and augmented reality. However, the furry
characteristics and various appearance of the foregrounds, e.g., animal and
portrait, challenge existing matting methods, which usually require extra user
inputs such as trimap or scribbles. To resolve these problems, we study the
distinct roles of semantics and details for image matting and decompose the
task into two parallel sub-tasks: high-level semantic segmentation and
low-level details matting. Specifically, we propose a novel Glance and Focus
Matting network (GFM), which employs a shared encoder and two separate decoders
to learn both tasks in a collaborative manner for end-to-end natural image
matting. Besides, due to the limitation of available natural images in the
matting task, previous methods typically adopt composite images for training
and evaluation, which result in limited generalization ability on real-world
images. In this paper, we investigate the domain gap issue between composite
images and real-world images systematically by conducting comprehensive
analyses of various discrepancies between the foreground and background images.
We find that a carefully designed composition route RSSN that aims to reduce
the discrepancies can lead to a better model with remarkable generalization
ability. Furthermore, we provide a benchmark containing 2,000 high-resolution
real-world animal images and 10,000 portrait images along with their manually
labeled alpha mattes to serve as a test bed for evaluating matting model's
generalization ability on real-world images. Comprehensive empirical studies
have demonstrated that GFM outperforms state-of-the-art methods and effectively
reduces the generalization error. The code and the datasets will be released at
https://github.com/JizhiziLi/GFM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Strict Enforcement of Conservation Laws and Invertibility in CNN-Based Super Resolution for Scientific Datasets. (arXiv:2011.05586v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.05586">
<div class="article-summary-box-inner">
<span><p>Recently, deep Convolutional Neural Networks (CNNs) have revolutionized image
super-resolution (SR), dramatically outperforming past methods for enhancing
image resolution. They could be a boon for the many scientific fields that
involve image or gridded datasets: satellite remote sensing, radar meteorology,
medical imaging, numerical modeling etc. Unfortunately, while SR-CNNs produce
visually compelling outputs, they may break physical conservation laws when
applied to scientific datasets. Here, a method for ``Downsampling Enforcement"
in SR-CNNs is proposed. A differentiable operator is derived that, when applied
as the final transfer function of a CNN, ensures the high resolution outputs
exactly reproduce the low resolution inputs under 2D-average downsampling while
improving performance of the SR schemes. The method is demonstrated across
seven modern CNN-based SR schemes on several benchmark image datasets, and
applications to weather radar, satellite imager, and climate model data are
also shown. The approach improves training time and performance while ensuring
physical consistency between the super-resolved and low resolution data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Seismic Facies Analysis: A Deep Domain Adaptation Approach. (arXiv:2011.10510v3 [physics.geo-ph] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.10510">
<div class="article-summary-box-inner">
<span><p>Deep neural networks (DNNs) can learn accurately from large quantities of
labeled input data, but often fail to do so when labelled data are scarce. DNNs
sometimes fail to generalize ontest data sampled from different input
distributions. Unsupervised Deep Domain Adaptation (DDA)techniques have been
proven useful when no labels are available, and when distribution shifts are
observed in the target domain (TD). In the present study, experiments are
performed on seismic images of the F3 block 3D dataset from offshore
Netherlands (source domain; SD) and Penobscot 3D survey data from Canada
(target domain; TD). Three geological classes from SD and TD that have similar
reflection patterns are considered. A deep neural network architecture named
EarthAdaptNet (EAN) is proposed to semantically segment the seismic images when
few classes have data scarcity, and we use a transposed residual unit to
replace the traditional dilated convolution in the decoder block. The EAN
achieved a pixel-level accuracy &gt;84% and an accuracy of ~70% for the minority
classes, showing improved performance compared to existing architectures. In
addition, we introduce the CORAL (Correlation Alignment) method to the EAN to
create an unsupervised deep domain adaptation network (EAN-DDA) for the
classification of seismic reflections from F3 and Penobscot, to demonstrate
possible approaches when labelled data are unavailable. Maximum class accuracy
achieved was ~99% for class 2 of Penobscot, with an overall accuracy&gt;50%. Taken
together, the EAN-DDA has the potential to classify target domain seismic
facies classes with high accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Success and Simplicity: A Second Look at Transferable Targeted Attacks. (arXiv:2012.11207v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.11207">
<div class="article-summary-box-inner">
<span><p>Achieving transferability of targeted attacks is reputed to be remarkably
difficult. Currently, state-of-the-art approaches are resource-intensive
because they necessitate training model(s) for each target class with
additional data. In our investigation, we find, however, that simple
transferable attacks which require neither additional data nor model training
can achieve surprisingly high targeted transferability. This insight has been
overlooked until now, mainly due to the widespread practice of unreasonably
restricting attack optimization to a limited number of iterations. In
particular, we, for the first time, identify that a simple logit loss can yield
competitive results with the state of the arts. Our analysis spans a variety of
transfer settings, especially including three new, realistic settings: an
ensemble transfer setting with little model similarity, a worse-case setting
with low-ranked target classes, and also a real-world attack against the Google
Cloud Vision API. Results in these new settings demonstrate that the commonly
adopted, easy settings cannot fully reveal the actual properties of different
attacks and may cause misleading comparisons. We also show the usefulness of
the simple logit loss for generating targeted universal adversarial
perturbations in a data-free and training-free manner. Overall, the aim of our
analysis is to inspire a more meaningful evaluation on targeted
transferability. Code is available at
https://github.com/ZhengyuZhao/Targeted-Tansfer
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DetectorGuard: Provably Securing Object Detectors against Localized Patch Hiding Attacks. (arXiv:2102.02956v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.02956">
<div class="article-summary-box-inner">
<span><p>State-of-the-art object detectors are vulnerable to localized patch hiding
attacks, where an adversary introduces a small adversarial patch to make
detectors miss the detection of salient objects. The patch attacker can carry
out a physical-world attack by printing and attaching an adversarial patch to
the victim object. In this paper, we propose DetectorGuard as the first general
framework for building provably robust object detectors against localized patch
hiding attacks. DetectorGuard is inspired by recent advancements in robust
image classification research; we ask: can we adapt robust image classifiers
for robust object detection? Unfortunately, due to their task difference, an
object detector naively adapted from a robust image classifier 1) may not
necessarily be robust in the adversarial setting or 2) even maintain decent
performance in the clean setting. To build a high-performance robust object
detector, we propose an objectness explaining strategy: we adapt a robust image
classifier to predict objectness for every image location and then explain each
objectness using the bounding boxes predicted by a conventional object
detector. If all objectness is well explained, we output the predictions made
by the conventional object detector; otherwise, we issue an attack alert.
Notably, 1) in the adversarial setting, we formally prove the end-to-end
robustness of DetectorGuard on certified objects, i.e., it either detects the
object or triggers an alert, against any patch hiding attacker within our
threat model; 2) in the clean setting, we have almost the same performance as
state-of-the-art object detectors. Our evaluation on the PASCAL VOC, MS COCO,
and KITTI datasets further demonstrates that DetectorGuard achieves the first
provable robustness against localized patch hiding attacks at a negligible cost
(&lt;1%) of clean performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GradInit: Learning to Initialize Neural Networks for Stable and Efficient Training. (arXiv:2102.08098v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.08098">
<div class="article-summary-box-inner">
<span><p>Innovations in neural architectures have fostered significant breakthroughs
in language modeling and computer vision. Unfortunately, novel architectures
often result in challenging hyper-parameter choices and training instability if
the network parameters are not properly initialized. A number of
architecture-specific initialization schemes have been proposed, but these
schemes are not always portable to new architectures. This paper presents
GradInit, an automated and architecture agnostic method for initializing neural
networks. GradInit is based on a simple heuristic; the norm of each network
layer is adjusted so that a single step of SGD or Adam with prescribed
hyperparameters results in the smallest possible loss value. This adjustment is
done by introducing a scalar multiplier variable in front of each parameter
block, and then optimizing these variables using a simple numerical scheme.
GradInit accelerates the convergence and test performance of many convolutional
architectures, both with or without skip connections, and even without
normalization layers. It also improves the stability of the original
Transformer architecture for machine translation, enabling training it without
learning rate warmup using either Adam or SGD under a wide range of learning
rates and momentum coefficients. Code is available at
https://github.com/zhuchen03/gradinit.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Discovery of Adaptive Attacks on Adversarial Defenses. (arXiv:2102.11860v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.11860">
<div class="article-summary-box-inner">
<span><p>Reliable evaluation of adversarial defenses is a challenging task, currently
limited to an expert who manually crafts attacks that exploit the defense's
inner workings or approaches based on an ensemble of fixed attacks, none of
which may be effective for the specific defense at hand. Our key observation is
that adaptive attacks are composed of reusable building blocks that can be
formalized in a search space and used to automatically discover attacks for
unknown defenses. We evaluated our approach on 24 adversarial defenses and show
that it outperforms AutoAttack, the current state-of-the-art tool for reliable
evaluation of adversarial defenses: our tool discovered significantly stronger
attacks by producing 3.0\%-50.8\% additional adversarial examples for 10
models, while obtaining attacks with slightly stronger or similar strength for
the remaining models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Space-Time Crop & Attend: Improving Cross-modal Video Representation Learning. (arXiv:2103.10211v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.10211">
<div class="article-summary-box-inner">
<span><p>The quality of the image representations obtained from self-supervised
learning depends strongly on the type of data augmentations used in the
learning formulation. Recent papers have ported these methods from still images
to videos and found that leveraging both audio and video signals yields strong
gains; however, they did not find that spatial augmentations such as cropping,
which are very important for still images, work as well for videos. In this
paper, we improve these formulations in two ways unique to the spatio-temporal
aspect of videos. First, for space, we show that spatial augmentations such as
cropping do work well for videos too, but that previous implementations, due to
the high processing and memory cost, could not do this at a scale sufficient
for it to work well. To address this issue, we first introduce Feature Crop, a
method to simulate such augmentations much more efficiently directly in feature
space. Second, we show that as opposed to naive average pooling, the use of
transformer-based attention improves performance significantly, and is well
suited for processing feature crops. Combining both of our discoveries into a
new method, Space-Time Crop &amp; Attend (STiCA) we achieve state-of-the-art
performance across multiple video-representation learning benchmarks. In
particular, we achieve new state-of-the-art accuracies of 67.0% on HMDB-51 and
93.1% on UCF-101 when pre-training on Kinetics-400.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CMA-Net: A Cascaded Mutual Attention Network for Light Field Salient Object Detection. (arXiv:2105.00949v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.00949">
<div class="article-summary-box-inner">
<span><p>In the past few years, numerous deep learning methods have been proposed to
address the task of segmenting salient objects from RGB images. However, these
approaches depending on single modality fail to achieve the state-of-the-art
performance on widely used light field salient object detection (SOD) datasets,
which collect large-scale natural images and provide multiple modalities such
as multi-view, micro-lens images and depth maps. Most recently proposed light
field SOD methods have acquired improving detecting accuracy, yet still predict
rough objects' structures and perform slow inference speed. To this end, we
propose CMA-Net, which consists of two novel cascaded mutual attention modules
aiming at fusing the high level features from the modalities of all-in-focus
and depth. Our proposed CMA-Net outperforms 30 SOD methods on two widely
applied light field benchmark datasets. Besides, the proposed CMA-Net is able
to inference at the speed of 53 fps, thus being much faster than the
state-of-the-art multi-modal SOD methods. Extensive quantitative and
qualitative experiments illustrate both the effectiveness and efficiency of our
CMA-Net, inspiring future development of multi-modal learning for both the
RGB-D and light field SOD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PoseContrast: Class-Agnostic Object Viewpoint Estimation in the Wild with Pose-Aware Contrastive Learning. (arXiv:2105.05643v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.05643">
<div class="article-summary-box-inner">
<span><p>Motivated by the need for estimating the 3D pose of arbitrary objects, we
consider the challenging problem of class-agnostic object viewpoint estimation
from images only, without CAD model knowledge. The idea is to leverage features
learned on seen classes to estimate the pose for classes that are unseen, yet
that share similar geometries and canonical frames with seen classes. We train
a direct pose estimator in a class-agnostic way by sharing weights across all
object classes, and we introduce a contrastive learning method that has three
main ingredients: (i) the use of pre-trained, self-supervised, contrast-based
features; (ii) pose-aware data augmentations; (iii) a pose-aware contrastive
loss. We experimented on Pascal3D+, ObjectNet3D and Pix3D in a cross-dataset
fashion, with both seen and unseen classes. We report state-of-the-art results,
including against methods that additionally use CAD models as input.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dual-stream Network for Visual Recognition. (arXiv:2105.14734v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14734">
<div class="article-summary-box-inner">
<span><p>Transformers with remarkable global representation capacities achieve
competitive results for visual tasks, but fail to consider high-level local
pattern information in input images. In this paper, we present a generic
Dual-stream Network (DS-Net) to fully explore the representation capacity of
local and global pattern features for image classification. Our DS-Net can
simultaneously calculate fine-grained and integrated features and efficiently
fuse them. Specifically, we propose an Intra-scale Propagation module to
process two different resolutions in each block and an Inter-Scale Alignment
module to perform information interaction across features at dual scales.
Besides, we also design a Dual-stream FPN (DS-FPN) to further enhance
contextual information for downstream dense predictions. Without bells and
whistles, the proposed DS-Net outperforms DeiT-Small by 2.4% in terms of top-1
accuracy on ImageNet-1k and achieves state-of-the-art performance over other
Vision Transformers and ResNets. For object detection and instance
segmentation, DS-Net-Small respectively outperforms ResNet-50 by 6.4% and 5.5%
in terms of mAP on MSCOCO 2017, and surpasses the previous state-of-the-art
scheme, which significantly demonstrates its potential to be a general backbone
in vision tasks. The code will be released soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection. (arXiv:2106.00666v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.00666">
<div class="article-summary-box-inner">
<span><p>Can Transformer perform 2D object- and region-level recognition from a pure
sequence-to-sequence perspective with minimal knowledge about the 2D spatial
structure? To answer this question, we present You Only Look at One Sequence
(YOLOS), a series of object detection models based on the vanilla Vision
Transformer with the fewest possible modifications, region priors, as well as
inductive biases of the target task. We find that YOLOS pre-trained on the
mid-sized ImageNet-1k dataset only can already achieve quite competitive
performance on the challenging COCO object detection benchmark, e.g.,
YOLOS-Base directly adopted from BERT-Base architecture can obtain 42.0 box AP
on COCO val. We also discuss the impacts as well as limitations of current
pre-train schemes and model scaling strategies for Transformer in vision
through YOLOS. Code and pre-trained models are available at
https://github.com/hustvl/YOLOS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Noise2Score: Tweedie's Approach to Self-Supervised Image Denoising without Clean Images. (arXiv:2106.07009v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07009">
<div class="article-summary-box-inner">
<span><p>Recently, there has been extensive research interest in training deep
networks to denoise images without clean reference. However, the representative
approaches such as Noise2Noise, Noise2Void, Stein's unbiased risk estimator
(SURE), etc. seem to differ from one another and it is difficult to find the
coherent mathematical structure. To address this, here we present a novel
approach, called Noise2Score, which reveals a missing link in order to unite
these seemingly different approaches. Specifically, we show that image
denoising problems without clean images can be addressed by finding the mode of
the posterior distribution and that the Tweedie's formula offers an explicit
solution through the score function (i.e. the gradient of log likelihood). Our
method then uses the recent finding that the score function can be stably
estimated from the noisy images using the amortized residual denoising
autoencoder, the method of which is closely related to Noise2Noise or
Nose2Void. Our Noise2Score approach is so universal that the same network
training can be used to remove noises from images that are corrupted by any
exponential family distributions and noise parameters. Using extensive
experiments with Gaussian, Poisson, and Gamma noises, we show that Noise2Score
significantly outperforms the state-of-the-art self-supervised denoising
methods in the benchmark data set such as (C)BSD68, Set12, and Kodak, etc.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised GANs with Label Augmentation. (arXiv:2106.08601v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.08601">
<div class="article-summary-box-inner">
<span><p>Recently, transformation-based self-supervised learning has been applied to
generative adversarial networks (GANs) to mitigate catastrophic forgetting in
the discriminator by introducing a stationary learning environment. However,
the separate self-supervised tasks in existing self-supervised GANs cause a
goal inconsistent with generative modeling due to the fact that their
self-supervised classifiers are agnostic to the generator distribution. To
address this problem, we propose a novel self-supervised GAN that unifies the
GAN task with the self-supervised task by augmenting the GAN labels (real or
fake) via self-supervision of data transformation. Specifically, the original
discriminator and self-supervised classifier are unified into a label-augmented
discriminator that predicts the augmented labels to be aware of both the
generator distribution and the data distribution under every transformation,
and then provide the discrepancy between them to optimize the generator.
Theoretically, we prove that the optimal generator could converge to replicate
the real data distribution. Empirically, we show that the proposed method
significantly outperforms previous self-supervised and data augmentation GANs
on both generative modeling and representation learning across benchmark
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IA-RED$^2$: Interpretability-Aware Redundancy Reduction for Vision Transformers. (arXiv:2106.12620v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.12620">
<div class="article-summary-box-inner">
<span><p>The self-attention-based model, transformer, is recently becoming the leading
backbone in the field of computer vision. In spite of the impressive success
made by transformers in a variety of vision tasks, it still suffers from heavy
computation and intensive memory costs. To address this limitation, this paper
presents an Interpretability-Aware REDundancy REDuction framework (IA-RED$^2$).
We start by observing a large amount of redundant computation, mainly spent on
uncorrelated input patches, and then introduce an interpretable module to
dynamically and gracefully drop these redundant patches. This novel framework
is then extended to a hierarchical structure, where uncorrelated tokens at
different stages are gradually removed, resulting in a considerable shrinkage
of computational cost. We include extensive experiments on both image and video
tasks, where our method could deliver up to 1.4x speed-up for state-of-the-art
models like DeiT and TimeSformer, by only sacrificing less than 0.7% accuracy.
More importantly, contrary to other acceleration approaches, our method is
inherently interpretable with substantial visual evidence, making vision
transformer closer to a more human-understandable architecture while being
lighter. We demonstrate that the interpretability that naturally emerged in our
framework can outperform the raw attention learned by the original visual
transformer, as well as those generated by off-the-shelf interpretation
methods, with both qualitative and quantitative results. Project Page:
<a href="http://people.csail.mit.edu/bpan/ia-red/.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast Training of Neural Lumigraph Representations using Meta Learning. (arXiv:2106.14942v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.14942">
<div class="article-summary-box-inner">
<span><p>Novel view synthesis is a long-standing problem in machine learning and
computer vision. Significant progress has recently been made in developing
neural scene representations and rendering techniques that synthesize
photorealistic images from arbitrary views. These representations, however, are
extremely slow to train and often also slow to render. Inspired by neural
variants of image-based rendering, we develop a new neural rendering approach
with the goal of quickly learning a high-quality representation which can also
be rendered in real-time. Our approach, MetaNLR++, accomplishes this by using a
unique combination of a neural shape representation and 2D CNN-based image
feature extraction, aggregation, and re-projection. To push representation
convergence times down to minutes, we leverage meta learning to learn neural
shape and image feature priors which accelerate training. The optimized shape
and image features can then be extracted using traditional graphics techniques
and rendered in real time. We show that MetaNLR++ achieves similar or better
novel view synthesis results in a fraction of the time that competing methods
require.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sanity Checks for Lottery Tickets: Does Your Winning Ticket Really Win the Jackpot?. (arXiv:2107.00166v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.00166">
<div class="article-summary-box-inner">
<span><p>There have been long-standing controversies and inconsistencies over the
experiment setup and criteria for identifying the "winning ticket" in
literature. To reconcile such, we revisit the definition of lottery ticket
hypothesis, with comprehensive and more rigorous conditions. Under our new
definition, we show concrete evidence to clarify whether the winning ticket
exists across the major DNN architectures and/or applications. Through
extensive experiments, we perform quantitative analysis on the correlations
between winning tickets and various experimental factors, and empirically study
the patterns of our observations. We find that the key training
hyperparameters, such as learning rate and training epochs, as well as the
architecture characteristics such as capacities and residual connections, are
all highly correlated with whether and when the winning tickets can be
identified. Based on our analysis, we summarize a guideline for parameter
settings in regards of specific architecture characteristics, which we hope to
catalyze the research progress on the topic of lottery ticket hypothesis. Our
codes are publicly available at:
https://github.com/boone891214/sanity-check-LTH.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synth-by-Reg (SbR): Contrastive learning for synthesis-based registration of paired images. (arXiv:2107.14449v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.14449">
<div class="article-summary-box-inner">
<span><p>Nonlinear inter-modality registration is often challenging due to the lack of
objective functions that are good proxies for alignment. Here we propose a
synthesis-by-registration method to convert this problem into an easier
intra-modality task. We introduce a registration loss for weakly supervised
image translation between domains that does not require perfectly aligned
training data. This loss capitalises on a registration U-Net with frozen
weights, to drive a synthesis CNN towards the desired translation. We
complement this loss with a structure preserving constraint based on
contrastive learning, which prevents blurring and content shifts due to
overfitting. We apply this method to the registration of histological sections
to MRI slices, a key step in 3D histology reconstruction. Results on two
different public datasets show improvements over registration based on mutual
information (13% reduction in landmark error) and synthesis-based algorithms
such as CycleGAN (11% reduction), and are comparable to a registration CNN with
label supervision. Code and data are publicly available at
\url{https://github.com/acasamitjana/SynthByReg}
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Object-aware Contrastive Learning for Debiased Scene Representation. (arXiv:2108.00049v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.00049">
<div class="article-summary-box-inner">
<span><p>Contrastive self-supervised learning has shown impressive results in learning
visual representations from unlabeled images by enforcing invariance against
different data augmentations. However, the learned representations are often
contextually biased to the spurious scene correlations of different objects or
object and background, which may harm their generalization on the downstream
tasks. To tackle the issue, we develop a novel object-aware contrastive
learning framework that first (a) localizes objects in a self-supervised manner
and then (b) debias scene correlations via appropriate data augmentations
considering the inferred object locations. For (a), we propose the contrastive
class activation map (ContraCAM), which finds the most discriminative regions
(e.g., objects) in the image compared to the other images using the
contrastively trained models. We further improve the ContraCAM to detect
multiple objects and entire shapes via an iterative refinement procedure. For
(b), we introduce two data augmentations based on ContraCAM, object-aware
random crop and background mixup, which reduce contextual and background biases
during contrastive self-supervised learning, respectively. Our experiments
demonstrate the effectiveness of our representation learning framework,
particularly when trained under multi-object images or evaluated under the
background (and distribution) shifted images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PDE-GCN: Novel Architectures for Graph Neural Networks Motivated by Partial Differential Equations. (arXiv:2108.01938v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.01938">
<div class="article-summary-box-inner">
<span><p>Graph neural networks are increasingly becoming the go-to approach in various
fields such as computer vision, computational biology and chemistry, where data
are naturally explained by graphs. However, unlike traditional convolutional
neural networks, deep graph networks do not necessarily yield better
performance than shallow graph networks. This behavior usually stems from the
over-smoothing phenomenon. In this work, we propose a family of architectures
to control this behavior by design. Our networks are motivated by numerical
methods for solving Partial Differential Equations (PDEs) on manifolds, and as
such, their behavior can be explained by similar analysis. Moreover, as we
demonstrate using an extensive set of experiments, our PDE-motivated networks
can generalize and be effective for various types of problems from different
fields. Our architectures obtain better or on par with the current
state-of-the-art results for problems that are typically approached using
different architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Road Scenes Segmentation Across Different Domains by Disentangling Latent Representations. (arXiv:2108.03021v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03021">
<div class="article-summary-box-inner">
<span><p>Deep learning models obtain impressive accuracy in road scenes understanding,
however they need a large quantity of labeled samples for their training.
Additionally, such models do not generalise well to environments where the
statistical properties of data do not perfectly match those of training scenes,
and this can be a significant problem for intelligent vehicles. Hence, domain
adaptation approaches have been introduced to transfer knowledge acquired on a
label-abundant source domain to a related label-scarce target domain. In this
work, we design and carefully analyse multiple latent space-shaping
regularisation strategies that work together to reduce the domain shift. More
in detail, we devise a feature clustering strategy to increase domain
alignment, a feature perpendicularity constraint to space apart features
belonging to different semantic classes, including those not present in the
current batch, and a feature norm alignment strategy to separate active and
inactive channels. In addition, we propose a novel evaluation metric to capture
the relative performance of an adapted model with respect to supervised
training. We validate our framework in driving scenarios, considering both
synthetic-to-real and real-to-real adaptation, outperforming previous
feature-level state-of-the-art methods on multiple road scenes benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SnowflakeNet: Point Cloud Completion by Snowflake Point Deconvolution with Skip-Transformer. (arXiv:2108.04444v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04444">
<div class="article-summary-box-inner">
<span><p>Point cloud completion aims to predict a complete shape in high accuracy from
its partial observation. However, previous methods usually suffered from
discrete nature of point cloud and unstructured prediction of points in local
regions, which makes it hard to reveal fine local geometric details on the
complete shape. To resolve this issue, we propose SnowflakeNet with Snowflake
Point Deconvolution (SPD) to generate the complete point clouds. The
SnowflakeNet models the generation of complete point clouds as the
snowflake-like growth of points in 3D space, where the child points are
progressively generated by splitting their parent points after each SPD. Our
insight of revealing detailed geometry is to introduce skip-transformer in SPD
to learn point splitting patterns which can fit local regions the best.
Skip-transformer leverages attention mechanism to summarize the splitting
patterns used in the previous SPD layer to produce the splitting in the current
SPD layer. The locally compact and structured point cloud generated by SPD is
able to precisely capture the structure characteristic of 3D shape in local
patches, which enables the network to predict highly detailed geometries, such
as smooth regions, sharp edges and corners. Our experimental results outperform
the state-of-the-art point cloud completion methods under widely used
benchmarks. Code will be available at
https://github.com/AllenXiangX/SnowflakeNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CPNet: Cross-Parallel Network for Efficient Anomaly Detection. (arXiv:2108.04454v4 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04454">
<div class="article-summary-box-inner">
<span><p>Anomaly detection in video streams is a challenging problem because of the
scarcity of abnormal events and the difficulty of accurately annotating them.
To alleviate these issues, unsupervised learning-based prediction methods have
been previously applied. These approaches train the model with only normal
events and predict a future frame from a sequence of preceding frames by use of
encoder-decoder architectures so that they result in small prediction errors on
normal events but large errors on abnormal events. The architecture, however,
comes with the computational burden as some anomaly detection tasks require low
computational cost without sacrificing performance. In this paper,
Cross-Parallel Network (CPNet) for efficient anomaly detection is proposed here
to minimize computations without performance drops. It consists of N smaller
parallel U-Net, each of which is designed to handle a single input frame, to
make the calculations significantly more efficient. Additionally, an
inter-network shift module is incorporated to capture temporal relationships
among sequential frames to enable more accurate future predictions.The
quantitative results show that our model requires less computational cost than
the baseline U-Net while delivering equivalent performance in anomaly
detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly-Supervised Surface Crack Segmentation by Generating Pseudo-Labels using Localization with a Classifier and Thresholding. (arXiv:2109.00456v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00456">
<div class="article-summary-box-inner">
<span><p>Surface cracks are a common sight on public infrastructure nowadays. Recent
work has been addressing this problem by supporting structural maintenance
measures using machine learning methods. Those methods are used to segment
surface cracks from their background, making them easier to localize. However,
a common issue is that to create a well-functioning algorithm, the training
data needs to have detailed annotations of pixels that belong to cracks. Our
work proposes a weakly supervised approach that leverages a CNN classifier in a
novel way to create surface crack pseudo labels. First, we use the classifier
to create a rough crack localization map by using its class activation maps and
a patch based classification approach and fuse this with a thresholding based
approach to segment the mostly darker crack pixels. The classifier assists in
suppressing noise from the background regions, which commonly are incorrectly
highlighted as cracks by standard thresholding methods. Then, the pseudo labels
can be used in an end-to-end approach when training a standard CNN for surface
crack segmentation. Our method is shown to yield sufficiently accurate pseudo
labels. Those labels, incorporated into segmentation CNN training using
multiple recent crack segmentation architectures, achieve comparable
performance to fully supervised methods on four popular crack segmentation
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Compositional Feature Embedding and Similarity Metric for Ultra-Fine-Grained Visual Categorization. (arXiv:2109.12380v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12380">
<div class="article-summary-box-inner">
<span><p>Fine-grained visual categorization (FGVC), which aims at classifying objects
with small inter-class variances, has been significantly advanced in recent
years. However, ultra-fine-grained visual categorization (ultra-FGVC), which
targets at identifying subclasses with extremely similar patterns, has not
received much attention. In ultra-FGVC datasets, the samples per category are
always scarce as the granularity moves down, which will lead to overfitting
problems. Moreover, the difference among different categories is too subtle to
distinguish even for professional experts. Motivated by these issues, this
paper proposes a novel compositional feature embedding and similarity metric
(CECS). Specifically, in the compositional feature embedding module, we
randomly select patches in the original input image, and these patches are then
replaced by patches from the images of different categories or masked out. Then
the replaced and masked images are used to augment the original input images,
which can provide more diverse samples and thus largely alleviate overfitting
problem resulted from limited training samples. Besides, learning with diverse
samples forces the model to learn not only the most discriminative features but
also other informative features in remaining regions, enhancing the
generalization and robustness of the model. In the compositional similarity
metric module, a new similarity metric is developed to improve the
classification performance by narrowing the intra-category distance and
enlarging the inter-category distance. Experimental results on two ultra-FGVC
datasets and one FGVC dataset with recent benchmark methods consistently
demonstrate that the proposed CECS method achieves the state of-the-art
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Homography Estimation in Dynamic Surgical Scenes for Laparoscopic Camera Motion Extraction. (arXiv:2109.15098v2 [eess.IV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15098">
<div class="article-summary-box-inner">
<span><p>Current laparoscopic camera motion automation relies on rule-based approaches
or only focuses on surgical tools. Imitation Learning (IL) methods could
alleviate these shortcomings, but have so far been applied to oversimplified
setups. Instead of extracting actions from oversimplified setups, in this work
we introduce a method that allows to extract a laparoscope holder's actions
from videos of laparoscopic interventions. We synthetically add camera motion
to a newly acquired dataset of camera motion free da Vinci surgery image
sequences through a novel homography generation algorithm. The synthetic camera
motion serves as a supervisory signal for camera motion estimation that is
invariant to object and tool motion. We perform an extensive evaluation of
state-of-the-art (SOTA) Deep Neural Networks (DNNs) across multiple compute
regimes, finding our method transfers from our camera motion free da Vinci
surgery dataset to videos of laparoscopic interventions, outperforming
classical homography estimation approaches in both, precision by 41%, and
runtime on a CPU by 43%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Attacks on Black Box Video Classifiers: Leveraging the Power of Geometric Transformations. (arXiv:2110.01823v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01823">
<div class="article-summary-box-inner">
<span><p>When compared to the image classification models, black-box adversarial
attacks against video classification models have been largely understudied.
This could be possible because, with video, the temporal dimension poses
significant additional challenges in gradient estimation. Query-efficient
black-box attacks rely on effectively estimated gradients towards maximizing
the probability of misclassifying the target video. In this work, we
demonstrate that such effective gradients can be searched for by parameterizing
the temporal structure of the search space with geometric transformations.
Specifically, we design a novel iterative algorithm Geometric TRAnsformed
Perturbations (GEO-TRAP), for attacking video classification models. GEO-TRAP
employs standard geometric transformation operations to reduce the search space
for effective gradients into searching for a small group of parameters that
define these operations. This group of parameters describes the geometric
progression of gradients, resulting in a reduced and structured search space.
Our algorithm inherently leads to successful perturbations with surprisingly
few queries. For example, adversarial examples generated from GEO-TRAP have
better attack success rates with ~73.55% fewer queries compared to the
state-of-the-art method for video adversarial attacks on the widely used Jester
dataset. Overall, our algorithm exposes vulnerabilities of diverse video
classification models and achieves new state-of-the-art results under black-box
settings on two large datasets. Code is available here:
https://github.com/sli057/Geo-TRAP
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient large-scale image retrieval with deep feature orthogonality and Hybrid-Swin-Transformers. (arXiv:2110.03786v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03786">
<div class="article-summary-box-inner">
<span><p>We present an efficient end-to-end pipeline for largescale landmark
recognition and retrieval. We show how to combine and enhance concepts from
recent research in image retrieval and introduce two architectures especially
suited for large-scale landmark identification. A model with deep orthogonal
fusion of local and global features (DOLG) using an EfficientNet backbone as
well as a novel Hybrid-Swin-Transformer is discussed and details how to train
both architectures efficiently using a step-wise approach and a sub-center
arcface loss with dynamic margins are provided. Furthermore, we elaborate a
novel discriminative re-ranking methodology for image retrieval. The
superiority of our approach was demonstrated by winning the recognition and
retrieval track of the Google Landmark Competition 2021.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NAS-Bench-360: Benchmarking Diverse Tasks for Neural Architecture Search. (arXiv:2110.05668v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05668">
<div class="article-summary-box-inner">
<span><p>Most existing neural architecture search (NAS) benchmarks and algorithms
prioritize performance on well-studied tasks, e.g., image classification on
CIFAR and ImageNet. This makes the applicability of NAS approaches in more
diverse areas inadequately understood. In this paper, we present NAS-Bench-360,
a benchmark suite for evaluating state-of-the-art NAS methods for convolutional
neural networks (CNNs). To construct it, we curate a collection of ten tasks
spanning a diverse array of application domains, dataset sizes, problem
dimensionalities, and learning objectives. By carefully selecting tasks that
can both interoperate with modern CNN-based search methods but that are also
far-afield from their original development domain, we can use NAS-Bench-360 to
investigate the following central question: do existing state-of-the-art NAS
methods perform well on diverse tasks? Our experiments show that a modern NAS
procedure designed for image classification can indeed find good architectures
for tasks with other dimensionalities and learning objectives; however, the
same method struggles against more task-specific methods and performs
catastrophically poorly on classification in non-vision domains. The case for
NAS robustness becomes even more dire in a resource-constrained setting, where
a recent NAS method provides little-to-no benefit over much simpler baselines.
These results demonstrate the need for a benchmark such as NAS-Bench-360 to
help develop NAS approaches that work well on a variety of tasks, a crucial
component of a truly robust and automated pipeline. We conclude with a
demonstration of the kind of future research our suite of tasks will enable.
All data and code is made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Top 3 in FG 2021 Families In the Wild Kinship Verification Challenge. (arXiv:2110.07020v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07020">
<div class="article-summary-box-inner">
<span><p>Kinship verification is the task of determining whether a parent-child,
sibling, or grandparent-grandchild relationship exists between two people and
is important in social media applications, forensic investigations, finding
missing children, and reuniting families. We demonstrate high quality kinship
verification by participating in the 2021 Recognizing Families in the Wild
challenge which provides the largest publicly available dataset in the field.
Our approach is among the top 3 winning entries in the competition. We ensemble
models written by both human experts and OpenAI Codex. We make our models and
code publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inverse Problems Leveraging Pre-trained Contrastive Representations. (arXiv:2110.07439v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07439">
<div class="article-summary-box-inner">
<span><p>We study a new family of inverse problems for recovering representations of
corrupted data. We assume access to a pre-trained representation learning
network R(x) that operates on clean images, like CLIP. The problem is to
recover the representation of an image R(x), if we are only given a corrupted
version A(x), for some known forward operator A. We propose a supervised
inversion method that uses a contrastive objective to obtain excellent
representations for highly corrupted images. Using a linear probe on our robust
representations, we achieve a higher accuracy than end-to-end supervised
baselines when classifying images with various types of distortions, including
blurring, additive noise, and random pixel masking. We evaluate on a subset of
ImageNet and observe that our method is robust to varying levels of distortion.
Our method outperforms end-to-end baselines even with a fraction of the labeled
data in a wide range of forward operators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continuation of Famous Art with AI: A Conditional Adversarial Network Inpainting Approach. (arXiv:2110.09170v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.09170">
<div class="article-summary-box-inner">
<span><p>Much of the state-of-the-art in image synthesis inspired by real artwork are
either entirely generative by filtered random noise or inspired by the transfer
of style. This work explores the application of image inpainting to continue
famous artworks and produce generative art with a Conditional GAN. During the
training stage of the process, the borders of images are cropped, leaving only
the centre. An inpainting GAN is then tasked with learning to reconstruct the
original image from the centre crop by way of minimising both adversarial and
absolute difference losses, which are analysed by both their Fr\'echet
Inception Distances and manual observations which are presented. Once the
network is trained, images are then resized rather than cropped and presented
as input to the generator. Following the learning process, the generator then
creates new images by continuing from the edges of the original piece. Three
experiments are performed with datasets of 4766 landscape paintings
(impressionism and romanticism), 1167 Ukiyo-e works from the Japanese Edo
period, and 4968 abstract artworks. Results show that geometry and texture
(including canvas and paint) as well as scenery such as sky, clouds, water,
land (including hills and mountains), grass, and flowers are implemented by the
generator when extending real artworks. In the Ukiyo-e experiments, it was
observed that features such as written text were generated even in cases where
the original image did not have any, due to the presence of an unpainted border
within the input image.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ensemble of Averages: Improving Model Selection and Boosting Performance in Domain Generalization. (arXiv:2110.10832v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10832">
<div class="article-summary-box-inner">
<span><p>In Domain Generalization (DG) settings, models trained on a given set of
training domains have notoriously chaotic performance on distribution shifted
test domains, and stochasticity in optimization (e.g. seed) plays a big role.
This makes deep learning models unreliable in real world settings. We first
show that a simple protocol for averaging model parameters along the
optimization path, starting early during training, both significantly boosts
domain generalization and diminishes the impact of stochasticity by improving
the rank correlation between the in-domain validation accuracy and out-domain
test accuracy, which is crucial for reliable model selection. Next, we show
that an ensemble of independently trained models also has a chaotic behavior in
the DG setting. Taking advantage of our observation, we show that instead of
ensembling unaveraged models, ensembling moving average models (EoA) from
different runs does increase stability and further boosts performance. On the
DomainBed benchmark, when using a ResNet-50 pre-trained on ImageNet, this
ensemble of averages achieves $88.6\%$ on PACS, $79.1\%$ on VLCS, $72.5\%$ on
OfficeHome, $52.3\%$ on TerraIncognita, and $47.4\%$ on DomainNet, an average
of $68.0\%$, beating ERM (w/o model averaging) by $\sim 4\%$. We also evaluate
a model that is pre-trained on a larger dataset, where we show EoA achieves an
average accuracy of $72.7\%$, beating its corresponding ERM baseline by $5\%$.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LARNet: Latent Action Representation for Human Action Synthesis. (arXiv:2110.10899v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.10899">
<div class="article-summary-box-inner">
<span><p>We present LARNet, a novel end-to-end approach for generating human action
videos. A joint generative modeling of appearance and dynamics to synthesize a
video is very challenging and therefore recent works in video synthesis have
proposed to decompose these two factors. However, these methods require a
driving video to model the video dynamics. In this work, we propose a
generative approach instead, which explicitly learns action dynamics in latent
space avoiding the need of a driving video during inference. The generated
action dynamics is integrated with the appearance using a recurrent
hierarchical structure which induces motion at different scales to focus on
both coarse as well as fine level action details. In addition, we propose a
novel mix-adversarial loss function which aims at improving the temporal
coherency of synthesized videos. We evaluate the proposed approach on four
real-world human action datasets demonstrating the effectiveness of the
proposed approach in generating human actions. Code available at
https://github.com/aayushjr/larnet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Per-Pixel Lung Thickness and Lung Capacity Estimation on Chest X-Rays using Convolutional Neural Networks. (arXiv:2110.12509v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.12509">
<div class="article-summary-box-inner">
<span><p>Estimating the lung depth on x-ray images could provide both an accurate
opportunistic lung volume estimation during clinical routine and improve image
contrast in modern structural chest imaging techniques like x-ray dark-field
imaging. We present a method based on a convolutional neural network that
allows a per-pixel lung thickness estimation and subsequent total lung capacity
estimation. The network was trained and validated using 5250 simulated
radiographs generated from 525 real CT scans. Furthermore, we are able to infer
the model trained with simulation data on real radiographs.
</p>
<p>For 35 patients, quantitative and qualitative evaluation was performed on
standard clinical radiographs. The ground-truth for each patient's total lung
volume was defined based on the patients' corresponding CT scan. The
mean-absolute error between the estimated lung volume on the 35 real
radiographs and groundtruth volume was 0.73 liter. Additionally, we predicted
the lung thicknesses on a synthetic dataset of 131 radiographs, where the
mean-absolute error was 0.27 liter. The results show, that it is possible to
transfer the knowledge obtained in a simulation model to real x-ray images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Graph Representation of Person-specific Cognitive Processes from Audio-visual Behaviours for Automatic Personality Recognition. (arXiv:2110.13570v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.13570">
<div class="article-summary-box-inner">
<span><p>This approach builds on two following findings in cognitive science: (i)
human cognition partially determines expressed behaviour and is directly linked
to true personality traits; and (ii) in dyadic interactions individuals'
nonverbal behaviours are influenced by their conversational partner behaviours.
In this context, we hypothesise that during a dyadic interaction, a target
subject's facial reactions are driven by two main factors, i.e. their internal
(person-specific) cognitive process, and the externalised nonverbal behaviours
of their conversational partner. Consequently, we propose to represent the
target subjects (defined as the listener) person-specific cognition in the form
of a person-specific CNN architecture that has unique architectural parameters
and depth, which takes audio-visual non-verbal cues displayed by the
conversational partner (defined as the speaker) as input, and is able to
reproduce the target subject's facial reactions. Each person-specific CNN is
explored by the Neural Architecture Search (NAS) and a novel adaptive loss
function, which is then represented as a graph representation for recognising
the target subject's true personality. Experimental results not only show that
the produced graph representations are well associated with target subjects'
personality traits in both human-human and human-machine interaction scenarios,
and outperform the existing approaches with significant advantages, but also
demonstrate that the proposed novel strategies such as adaptive loss, and the
end-to-end vertices/edges feature learning, help the proposed approach in
learning more reliable personality representations.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-10-28 23:02:31.687533631 UTC">2021-10-28 23:02:31 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.6</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>